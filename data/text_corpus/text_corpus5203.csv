index,text
26015,in hydrologic research there is a need to manage archive and publish data in a discoverable way to increase data reuse transparency and reproducibility multidimensional space time data are commonly used in hydrologic research and systems are needed for sharing and exchanging such data simply exchanging files may result in loss of metadata information and can be challenging when files are large we developed an approach to manage share and publish multidimensional space time data in hydroshare a next generation hydrologic information system and domain specific repository this paper presents the design development and testing of this approach we selected the network common data form netcdf as the underlying data model we defined specific metadata elements to store and manage multidimensional space time data we adopted and adapted existing software to automatically harvest support entry of metadata and establish standardized data services to serve and enhance access to the datasets shared in hydroshare keywords multidimensional space time data netcdf collaborative data sharing hydroshare cyberinfrastructure 1 introduction with advances in hydrologic monitoring and model simulation technologies hydrologic research has become data and computationally intensive resulting in large volumes of scientific data generated or collected by individual researchers and organizations advances in hydrologic understanding now tends to require discovery access and integration of heterogeneous and dispersed data from multiple sources moreover large scale hydrologic problems often need to be solved by collaboration among researchers thus working as a team to collaborate around data has become indispensable these emerging trends in hydrologic research are key drivers that demand new tools to support the entire research cycle of data creation discovery access curation publication and analysis to help achieve new scientific breakthroughs horsburgh et al 2015 rajib et al 2016 morsy et al 2017 the consortium of universities for the advancement of hydrologic science inc cuahsi has devoted great effort to the development of cyberinfrastructure ci to satisfy this need including hydroshare http www hydroshare org a next generation internet based hydrologic information system his tarboton et al 2014 hydroshare was developed to extend the capability of the earlier server based cuahsi his which focused on the sharing of point observation time series data horsburgh et al 2008 2009 tarboton et al 2009 given that the needs of hydrology researchers go well beyond time series data hydroshare was established to add support for sharing a broader range of hydrologic datasets and models that are widely used in the hydrologic science community these include data of varying dimensionality such as time series i e observations varying over time at a single fixed location geographic raster i e a regular raster grid representing a spatial field at a single time geographic feature i e geospatial data represented by points lines or polygons representing a single point in time and multidimensional space time data i e data that may vary in both space and time as well as model instances and model programs morsy et al 2017 as a first step in hydroshare development a resource data model was designed that enabled storing transmitting and cataloging of resources comprised of these diverse hydrologic data types and models to facilitate discovery horsburgh et al 2015 details for each of hydroshare s supported data types were specified including required data format and content metadata elements and functions for data processing analysis or visualization hydroshare s resource data model was designed to generalize the way datasets and models were managed and shared while at the same time supporting specific metadata elements and functions required for enhancing the hydrologic analysis capability and interoperability for different data types and models multidimensional space time data referred to as md data in this paper is widely used in hydrology for both observations and model results examples include weather and climate data such as spatially distributed precipitation temperature wind speed and humidity used as model inputs or snow water equivalent and soil moisture output from models while commonly used there are several challenges associated with md data that can make data sharing more difficult one challenge is that there is no single accepted data format for storing this type of data to support the interoperability needed for data sharing and analysis formats used include tagged image file format tiff https www adobe io open standards tiff html gridded binary grib https www wmo int pages prog www wmocodes guides grib common data format cdf http cdf gsfc nasa gov hierarchical data format hdf https www hdfgroup org and network common data form netcdf http www unidata ucar edu software netcdf file size can be another challenge md datasets can be large making it inefficient to download and exchange entire large files when users may need only a subset or slice of them for visualization or analysis it is also important to effectively capture expose and support the recording and editing of metadata associated with the md data files recognizing these challenges this paper describes our efforts to establish functionality to support the sharing of md data in hydroshare currently several websites and software tools can be used for sharing md data and each has its own strengths and limitations for example figshare http figshare com is a website that enables users to manage their research output in the cloud to be stored shared published and discovered it supports permanent data publication and provides citation information for shared datasets to give the data provider credit and make their datasets citable figshare supports social functions such as commenting and access control to facilitate collaboration around the datasets however although figshare has functions to capture simple metadata and preview file contents for commonly used formats such as microsoft word pdf and microsoft excel no functions are provided to preview or edit the metadata or contents of the more advanced scientific data formats used for md data e g netcdf and cdf these limitations hinder users ability to describe preview access and interpret file contents through the website which can be a barrier to data sharing inhibiting data reuse and the reproducibility of scientific analyses the thematic real time environmental distributed data services thredds and hyrax data servers can provide cataloging functionality and support access to metadata and data for scientific datasets through various data access protocols opendap 2017 unidata 2017a however this does require that a data provider have access to or be able to install and maintain server software and hardware the thredds catalog and the open source project for a network data access protocol opendap service are common services supported by these two data servers thredds catalogs are logical directories of available online datasets that help in data discovery the opendap services enable users to subset or preview the contents of remote datasets and metadata moreover opendap client software programs exist that can help retrieve remote datasets for analysis and visualization these include the netcdf4 python package the rnetcdf package for the r statistical computing environment netcdf operators nco zender 2008 integrated data viewer idv unidata 2017b and panoply nasa 2018 etc thus there is important value to data consumers made available through these services motivating the need for this serving capability to be available for individual researchers or small research groups that do not have the capacity to set up thredds or hyrax servers another issue with existing thredds and hyrax server functionality is limited exposure to search capabilities which may prevent or impede scientists from discovering datasets using search terms or the geolocation of the dataset etc the repository for archiving managing and accessing diverse data ramadda http ramadda org is another web based application framework that provides a broad suite of services for content and data management publishing and collaboration with ramadda users can search access upload or comment on datasets the system incorporates the opendap service and data analysis tools to provide functions for file content preview metadata capture and curation and data subsetting and analysis for md data however as with the thredds and hyrax data servers sharing md data with ramadda requires setting up and maintaining the services which may make sharing research datasets impractical for individual researchers or small research groups to address some of these limitations in this paper we report the hydroshare md data representation design and implementation it provides functionality to help share md data to promote data curation publication and reuse we present a use case that demonstrates the new capabilities and contrasts them with capabilities of existing systems when researchers store and share md data in a format not widely used by the scientific community current data sharing systems treat these files as generic file objects which makes it difficult for others to know about the detailed metadata they contain it also makes it challenging to directly subset visualize or process the datasets through the data sharing system in contrast our approach enables users to share md data in the netcdf data format metadata from the file are exposed and presented in hydroshare users can collaboratively edit metadata in hydroshare and have these edits easily updated in the netcdf file when made public the md data are automatically registered in a hydroshare connected thredds server that enables remote access using the opendap service without data providers being required to provision any server hardware or install or configure any software with hydroshare s inherent data discovery versioning publication and social functions users can collaborate around datasets from initial data preparation to final data publication and the sharing discovery and reuse of md data is simplified in this paper we describe the design development and testing of this approach section 2 provides a brief introduction of hydroshare system section 3 describes the functional design and implementation for the md data type in hydroshare section 4 presents a use case that demonstrates functions in hydroshare that facilitate collaboration among users for data preparation publication and reuse discussion and conclusions are provided in the final sections 2 background hydroshare is a web based hydrologic information system that provides functionality for metadata capture and curation data manipulation data publication data discovery and collaboration tarboton et al 2014 horsburgh et al 2015 these functions represent a new paradigm in data sharing systems supporting discovery through the integration of information from multiple sources team work collaboration reuse of data and transparency to enhance trust in research findings fig 1 shows a high level view of hydroshare s system architecture the resource sharing functionality provides a web user interface to help users store and manage shared datasets and models in hydroshare the actions on resources functionality includes web applications or web services from hydroshare or third party organizations that enhance the capability for data analysis visualization or model simulation the interactions between resource sharing and actions on resources are through hydroshare s representational state transfer rest application programming interface api and irods client interface e g irods python api heard et al 2014 provide additional details of each open source component e g django and irods the hydroshare resource data model was designed and implemented to manage various types of hydrologic datasets and models horsburgh et al 2015 a hydroshare resource is the granular unit of shared content for access control serialization for transport over the internet and cataloging for discovery within the system major concepts for hydroshare resource are listed below and fig 2 shows an example to explain their relationships resource content files single or multiple files uploaded into hydroshare by users that make up different hydrologic datasets and models or additional informational files resource level metadata file a file encoded using extensible markup language xml and generated by hydroshare for metadata that documents the whole resource content type a widely used and well known hydrologic data type e g time series geographic feature and md data which is supported in hydroshare with advanced functions for metadata management and data analysis content type files single or multiple resource content files within a resource that represent a dataset of a supported content type content type metadata file a file encoded using xml and generated by hydroshare for metadata that documents a dataset of a supported content type in fig 2 an example hydroshare resource consists of a resource level metadata file and multiple resource content files these resource content files include an informational file a microsoft word document and multiple groups of files to represent different hydrologic datasets e g time series and md data each group of files or aggregation includes a content type metadata file and one or more content type files to represent a hydrologic dataset for a supported content type in hydroshare hydroshare s resource data model allows for definition of a new content type through specification of a content type data model the content type data model defines the data format and contents of the files along with content type metadata elements resource content files associated with a supported content type are grouped together into an aggregation by following the open archives initiative object reuse oai ore standard which is used for the description and exchange of aggregations of web resources lagoze et al 2008 thus an instance of a content type is referred to as an aggregation in hydroshare e g md aggregation in fig 2 the advantage of the resource data model design is that hydroshare can manage e g storage on disk packaging for delivery over the internet access control and cataloging for discovery multiple types of datasets and models in the same way regardless of the data formats and contents meanwhile a content type data model enables users to standardize data formats and syntax and to add additional metadata to describe the hydrologic datasets developers can then use the standardized data formats and metadata to create advanced functions to facilitate metadata management data analysis or data visualization this paper specifically reports the design of the content type data model for md data and serves as an example demonstrating how to extend the hydroshare resource data model with a new content type 3 methods we designed the md content type in hydroshare to support the sharing of md data using the following steps 1 we designed and implemented a content type data model for md data 2 we developed automated functions to support metadata extraction and editing for md data and 3 we set up the opendap web services to facilitate remote data access for data subsetting analysis and visualization detailed methods we used for each step are described in the following sections 3 1 md content type data model 3 1 1 content type files since there are many scientific data formats capable of storing md data we evaluated the benefits and tradeoffs of these data formats and chose the one that we felt was most suitable for data storage and management in hydroshare we established the following criteria to decide the data format used to represent md data in hydroshare first the data stored in the file needed to be organized in a way that helps users understand the data structure and retrieve a subset for analysis second the data format needed to be widely recognized and used in hydrologic research with available open source software or libraries to help analyze or visualize the data third widely accepted standards needed to be available to guide users in how to organize the data contents and metadata in the file to promote interoperability for data processing and sharing based on these criteria we compared several data formats that are widely used in the hydrologic science community including tiff grib2 cdf hdf5 and netcdf and adopted the netcdf data format to store md data in hydroshare tiff format is often used to store md data with each file representing a physical phenomenon at a time slice over a spatial coverage however this format is inconvenient for data transfer and data subsetting when a md dataset involves a large number of files grib2 cdf hdf5 and netcdf data formats are able to store md data and associated metadata within one file open source software programs for these data formats are available to support data analysis or visualization the reasons for selecting the netcdf data format were its wide use in modeling research in hydrology and aligned fields such as atmospheric science its adoption as a standard ogc 2011 and support for standards for its metadata eaton et al 2017 esip 2017 netcdf is a set of software libraries and self describing machine independent data formats that support the creation access and sharing of array oriented scientific data the netcdf software includes c fortran77 fortran 90 and c interfaces for accessing netcdf data programming interfaces are also available for python r java and other languages netcdf utilities such as ncdump are available to facilitate simple data management tasks the netcdf data format can actually refer to multiple formats in this paper we specifically refer to the netcdf classic formats and the netcdf 4 hdf5 format which are based on the netcdf classic model and the enhanced data model the netcdf user s guide provided additional details https www unidata ucar edu software netcdf docs user guide html the netcdf data format is widely used to represent md datasets as the input or output for hydrologic models david et al 2011 sen gupta et al 2015 thornton et al 1997 it has also been used for data management and curation of data converted from other data formats guo et al 2015 moreover many software programs and libraries available for netcdf data processing analysis or visualization are widely applied among the research community unidata 2017c thus researchers with these tools can easily manipulate netcdf files this capability was an advantage that enabled us to develop the functions in hydroshare without starting from scratch furthermore several conventions https www unidata ucar edu software netcdf conventions html are available to promote the processing and sharing of data in the netcdf data format for example the netcdf climate and forecast netcdf cf convention eaton et al 2017 specifies how to define the dimensions variables and attributes to represent md data as regular grid data or point time series the attribute conventions for data discovery acdd esip 2017 were designed to define the metadata attributes needed to describe the whole netcdf dataset to a discovery system with the selection of the netcdf data format we specified that a md aggregation should include only one netcdf file uploaded by the user and one metadata header information text file automatically generated by the system from the uploaded file to provide a brief summary of the contents in the netcdf file a hydroshare resource may contain one or many md aggregations for the netcdf file it is recommended that users define the dimensions variables and attributes by following the netcdf cf and the acdd conventions hydroshare does not prevent users from sharing md data in netcdf files that do not follow these conventions however the functions developed to harvest metadata were based on these conventions when they are not followed in the netcdf file metadata will not be automatically extracted and users will need to enter it manually the metadata header information text file is represented in a form called network common data form language cdl it is a human readable text representation of the metadata contained within the netcdf file this file includes information about the defined attributes and data structures extracted from the netcdf file to provide users a brief summary of the file s contents 3 1 2 content type metadata a hydroshare resource holding a md aggregation has two sets of metadata elements fig 3 a shows the resource level metadata elements they are based on the standard dublin core metadata elements which are common to all hydroshare resources describing the general attributes of a resource e g title creator abstract fig 3 b shows content type metadata elements or aggregation level metadata designed to describe the md aggregation the content type metadata includes general elements which are based on dublin core metadata elements to capture the basic information of any content type e g keywords and coverage and extended elements which are designed by content type developers to capture the data features in the netcdf file e g spatial reference and variable information some metadata elements were designed to contain sub elements for example the creator metadata element in fig 3 a has sub elements such as name organization and email that apply to the creator similarly the netcdfvariable metadata element in fig 3 b includes sub elements to describe the name data type units etc for a given variable in designing the md content type we chose to extract metadata elements held within the netcdf file and explicitly list them as the resource level or content type metadata for two reasons first this made it easier to present the full metadata description on a resource s landing page in hydroshare which is the web page where users view and manage the resource making it more accessible to potential users of the data e g potential users are not required to download or open the netcdf file to learn about its contents second the resource level metadata and content type metadata help hydroshare and potentially other web services catalogue the information to enable data discovery which allows users to search datasets in hydroshare based on desired data attributes 3 1 3 content type implementation in hydroshare a new content type can be created by inheriting from the abstract content type given this general extensibility pattern we implemented a new content type to manage md data in hydroshare a uml diagram of the logical database design for the md content type is shown in fig 4 which presents only major classes attributes and methods to demonstrate the organization of the md content type in hydroshare in the uml diagram there are two categories of classes 1 the abstract classes that are inherited by any new content type including the abstractcontenttype class the abstractcontenttypemetadata class and the abstractmetadataelement class and 2 the classes that define the md content type including the mdcontenttype class mdmetadata class spatialreference class and netcdfvariable class a brief description of each class is listed as follows abstractcontenttype an abstract class that provides the interface to represent a content type it includes the properties and methods for the system to manage a content type and provides a common interface to enable content type related functions for example the set file type method is used to check the data format of uploaded datasets if the data format is for a supported content type in hydroshare an aggregation will be created abstractcontenttypemetadata an abstract class used to manage the content type metadata this class by default contains general metadata elements e g keywords coverages the get xml method is used to generate the content type metadata xml file the has all required elements method is used to check if the required metadata elements for a content type are provided by the user before the resource is shared to the public abstractmetadataelement an abstract class used to represent a metadata element and define its sub elements and methods this class can be used to define extended metadata elements for any content type mdcontenttype a class that manages the md content type and inherits from the abstractcontenttype class functionality specific to the md content type had to be developed by overriding some methods of abstractcontenttype class for example the set file type method was overridden to check if the uploaded md data is in netcdf data format to create a md aggregation mdmetadata a class that manages the md content type metadata and inherits from the abstractcontenttypemetadata class this class is composed of general metadata elements and extended metadata elements e g variables and spatial reference the get xml method and has all required elements methods in the mdmetadata class override the corresponding methods from the abstract class for the md content type spatialreference a class that manages the spatial reference extended metadata element for the md content type this class inherits from the abstractmetadataelement and is contained in the mdmetadata class it includes attributes for storing the sub elements for spatial reference metadata e g projection name projection string and value netcdfvariable a class that manages the variable extended metadata element for the md content type this class inherits from the abstractmetadataelement class and is contained in the mdmetadata class it includes sub elements to describe a variable stored in the netcdf file e g name unit and type 3 2 additional content type functions as described above hydroshare provides a base set of functionality for each resource that includes access control publication social functions etc however one of the advantages of the design and implementation we describe here is that additional functionality can be developed for a specific content type to support specialized metadata management and sharing of the data via content type specific web services without affecting other content types in the following subsections we describe how this functionality was created for the md content type 3 2 1 metadata management functionality two functions were designed to 1 extract information where it exists from the netcdf file to populate the resource level and content type metadata elements and 2 generate the metadata header information text file when a user uploads a file with the nc extension hydroshare will test whether the file holds valid netcdf content and if successful execute these functions to create a md aggregation from the file aside from metadata extraction functions we designed functionality for editing metadata in the netcdf file through hydroshare we established a mapping between hydroshare s metadata elements and the acdd and netcdf cf conventions table 1 when a user edits the metadata in hydroshare the system utilizes the metadata mapping to check for consistency between the netcdf file and the hydroshare metadata if there is a need to add or update the metadata in the netcdf file the system will notify the user and the user can have the system update the file based on the new metadata edits this functionality helps a user easily update the netcdf file without having to download and manually edit it when the initial file includes little metadata this functionality makes it easy to create metadata in the file that follows netcdf conventions we used the netcdf4 python library and netcdf utility ncdump to implement the metadata extraction functions for files that follow acdd or netcdf cf conventions the automated metadata extraction function retrieves and populates matched hydroshare metadata elements based on the metadata mapping table 1 for files without acdd metadata elements but with spatial or temporal coordinate variables given following the netcdf cf conventions spatial and temporal coverage metadata elements determined by reading these data variables are populated in the content type and resource coverage metadata the metadata editing functionality was implemented using the netcdf4 python library and hydroshare irods client interface fig 1 when metadata needs to be updated the system first copies the original netcdf file within the irods file storage system to a temporary folder second the system writes hydroshare s metadata into the copied file using the netcdf4 python library then the system generates a new metadata header information text file from the updated copied file finally the system replaces the original netcdf file and the metadata header information text file in irods with these newly created files 3 2 2 opendap service opendap services help users learn about and work with the contents of the datasets without being required to download them first users are able to retrieve a subset of the data for use cases that require smaller spatial or temporal extent to provide this capability for hydroshare users we automated the process of creating an opendap web service for all publicly shared md data in hydroshare users can access and subset the dataset stored in hydroshare through an opendap data access form in a web browser or through existing opendap client software for data visualization or processing in hydroshare support for opendap services was created by setting up a thredds data server to interact with hydroshare s irods file storage system in the system architecture shown in fig 1 the data server plays the role of providing web services to enhance the capability for data analysis orange frame the data server requires direct file system access to the netcdf files for its opendap services thus we used existing irods client software to interface to the irods network file system yellow arrow connecting the orange and purple frames in fig 1 we developed a script that copies hydroshare public resources containing md aggregations efficiently using the irods multi thread parallel transfer iget command to a directory on the data server this copying occurs 1 when access control for a private resource is changed to public and 2 when the time stamp of a public resource on the data server is older than that in hydroshare and a data update is needed this takes advantage of irods high performance parallel data transfers but in the present implementation does require duplicate storage of netcdf files moreover since the data server does not support file level user access control as would be required for access to private files in hydroshare the opendap service is limited to netcdf files stored in public or formally published resources in hydroshare this functionality saves users from the work that would be required to set up a server to host opendap services for their datasets and gives them the freedom to decide when to make their datasets accessible via opendap services by using hydroshare s access control settings 4 results we validated the design with an experimental use case to demonstrate how sharing md data in hydroshare can help users collaborate around datasets and to facilitate the activities involved in the data management life cycle shown in fig 5 we considered a use case where a researcher simulated the snowmelt process for the dolores river watershed in the colorado river basin from 1988 to 2010 and shared model results in hydroshare this was part of a study that the authors were involved in on snowmelt modeling and operational water supply forecasting within the colorado river basin gan 2019a we present the results to demonstrate how the researcher shared the model output of snow water equivalent as md data in hydroshare to support the activities from data creation data publication to data analysis this use case involved multiple hypothetical users and was implemented by the first author acting as these users from separate hydroshare accounts 4 1 data creation and preparation the simulated snow water equivalent datasets were initially stored as separate two dimensional geospatial data files for each 6 h time step by the model this results in thousands of model output files for a 22 year simulation sharing of these original model output files has limitations that make data management and reuse difficult first information may be lost if any file is missed during the file transfer process second when the original model output files are in a format not widely used by the research community it is inconvenient to extract subsets that involve thousands of files and difficult to find available software for data analysis or visualization thus the researcher developed a python script to reorganize and convert the multiple original model output files into one netcdf file fig 6 shows the visualization of the use case md data upon uploading the use case md data into an empty hydroshare resource the type of data file was automatically recognized and a md aggregation was created in the resource hydroshare generated a resource landing page which provides different functions for the user to manage the resource fig 7 a and shows the content type files and content type metadata for the md aggregation fig 7 b for data preparation the user can use the data access control and the data versioning functionality fig 7 a to collaborate with trusted users to prepare the shared datasets with multiple versions if the original dataset evolves users can also edit delete copy and formally publish the resource via its landing page 4 2 data description and publication when the md aggregation was created in hydroshare two metadata extraction functions were executed a metadata header information text file was automatically created and stored in the content type file folder left panel in fig 7 b the content type metadata such as title keywords spatial temporal coverage spatial reference and variable metadata were automatically created by extracting metadata from the netcdf file right panel in fig 7 b the researcher collaborated with a trusted colleague referred to as user 1 to update the content type metadata in hydroshare to better describe the data hydroshare s metadata editing function was then used to update the metadata into the netcdf file for instance when user 1 added a new keyword in the metadata panel hydroshare s consistency check identified the presence of newly added metadata and showed an update netcdf file button fig 7 b to inform the user that the netcdf file could be updated with the new information then user 1 clicked the button to have hydroshare update the metadata in the netcdf file this is an example of how using hydroshare multiple users can collaborate to annotate the resource with metadata this metadata editing function enhances netcdf files to have more attributes that follow netcdf conventions after the data description was completed the researcher formally published the final data product with an assigned digital object identifier doi in hydroshare gan 2019b the suggested citation information was generated in hydroshare to encourage proper citation of this dataset fig 7 a 4 3 data discovery and analysis after the resource was formally published anyone can discover this dataset using hydroshare s search and filter functions fig 8 a hydroshare user referred to as user 2 provided a search term colorado river basin and hydroshare listed matching resources by querying the hydroshare metadata elements such as title abstract and keywords the search results were filtered based on different metadata facets such as content type author and subject user 2 identified the use case resource and used hydroshare s map search function to determine the geographic location associated with this dataset after discovering this resource user 2 decided to reuse a subset of the use case md data for data analysis user 2 used the opendap service from hydroshare and the panoply client software for data visualization without downloading the use case netcdf file to a local computer fig 6 fig 9 shows the nco commands used to access subset and process the use case dataset using the opendap service the code first subsets the data from january 1st to may 31st 2009 to identify the maximum snow water equivalent for each grid cell which provides the maximum snow accumulation assumed to occur within this period for that year max nc the code then retrieves the data for april 1st and april 15th and evaluates the snow water equivalent difference between the two dates which provides the analysis result for accumulation increase or ablation decrease during the period diff nc water managers often track such snow water equivalent changes in water supply forecasts user 2 then uploaded the data analysis code and the derived netcdf files into hydroshare as a new resource gan 2019c which started a new cycle of activities involved in collaborative data publication and reuse to improve research reproducibility 5 discussion the use case illustrated how organizing md data using the netcdf data format and sharing it in hydroshare provided added value in terms of functionality for metadata management and data analysis when compared with other md data sharing methods this approach has the following advantages it provides functionality to capture expose and edit the metadata stored in the netcdf file the manage access function enables users to collaborate on metadata editing and thus improve its description of the data by following metadata standards other data sharing methods either do not automatically expose the metadata from a netcdf file or make it difficult to collaboratively edit the metadata in the netcdf file it provides automated opendap services with access control for shared datasets to support data analysis that enhance opportunities for collaboration around the data other data sharing methods either do not provide an opendap service or require effort to set up and maintain a server and service it provides better data discovery functionality for the shared datasets it supports keyword and geolocation searches based on a catalog of metadata extracted from the netcdf file or input by the data provider other data sharing methods provide limited discovery capability to search the md data based on their attributes in our approach several key factors make this advantageous functionality available 1 we adopted a standard data format netcdf to organize md data which has conventions that standardize how data and metadata are organized in the file to improve the interoperability of datasets 2 we utilized existing tools and standard data services to develop automated functions for metadata management and data analysis to promote md data sharing and reuse and 3 hydroshare s resource data model design helps improve consistent data discovery access and publishing across the broad range of data types used by scientists in the hydrology domain while at the same time allowing value added functionality for specific data types however there are limitations that need further improvements for sharing md data in hydroshare one limitation is that some users may not be familiar with the netcdf data format users need to learn how to organize md data in this data format for data sharing another limitation is web based visualization there is a need for additional functionality that provides researchers with greater capacity to process and visualize datasets directly without transferring the data or subsets of the data between the data sharing system and their local computers to address these limitations one possible solution is to create and share jupyter notebook examples in hydroshare to support md data visualization and demonstrate how to convert md data in various formats into the netcdf format in the future we are considering support for other widely used standard variable ontologies such as the community surface dynamics modeling system csdms standard names peckham 2014 in metadata management functions another enhancement we are considering is enabling web services beyond the opendap service for shared md datasets to increase the value and usability of netcdf data including open geospatial consortium ogc web map service wms ogc web coverage service wcs netcdf subsetting service ncss etc 6 conclusions hydroshare is a web based hydrologic information system that provides researchers with a platform to share their hydrologic data and models as md data is one of the widely used data types in hydrologic research we developed an approach to support sharing of this data type within hydroshare our approach was aimed at overcoming challenges for sharing md data including 1 lack of a single accepted standard data format to support the interoperability needed for data sharing and analysis 2 lack of advanced functions to preview or edit the metadata in the file and 3 difficulty in subsetting data from large datasets for data visualization and processing to address these challenges we adopted a standard data format netcdf and standard metadata elements to manage md data in hydroshare and we implemented value added functionality to manage metadata and support data reuse the use case presented demonstrates the new capabilities in hydroshare and shows that researchers can share md data in a netcdf file with the metadata automatically exposed in the system metadata can be edited collaboratively in hydroshare and automatically updated in the netcdf file once publicly shared users can subset the data with the automatically configured opendap service for visualization and analysis without effort to set up and maintain a server in concert with existing hydroshare functionality e g data discovery data publishing and access control the work described here enables relatively straightforward sharing and formal publication of md data this increases transparency and reproducibility of the associated research and promotes reuse of data and the derivation of additional value from research data investments beyond the context of the new functionality we have demonstrated another contribution of this work is that the methods we developed for improving sharing of md data can be used as examples for supporting other data types in hydroshare or for better supporting md data in other systems cyberinfrastructure developers who are going to build or have built a data sharing system to support md data sharing can use the recommendations of this work to organize data in a standard data format and document the datasets using the standards based metadata using the patterns we established they may be able to create standard data services or develop new functionality to facilitate metadata management data analysis or visualization adopting standard formats and techniques across data repositories could lead to a level of interoperability that is worth considering in the future software availability the software created in this research is free and open source as part of the larger hydroshare software repository the hydroshare software repository is managed through github and is available at https github com hydroshare hydroshare funding this work was supported by the national science foundation oci 1148453 oci 1148090 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national science foundation under collaborative grants oci 1148453 and oci 1148090 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation we are also thankful to others on the hydroshare development team for providing suggestions on function design and implementation for hydroshare system 
26015,in hydrologic research there is a need to manage archive and publish data in a discoverable way to increase data reuse transparency and reproducibility multidimensional space time data are commonly used in hydrologic research and systems are needed for sharing and exchanging such data simply exchanging files may result in loss of metadata information and can be challenging when files are large we developed an approach to manage share and publish multidimensional space time data in hydroshare a next generation hydrologic information system and domain specific repository this paper presents the design development and testing of this approach we selected the network common data form netcdf as the underlying data model we defined specific metadata elements to store and manage multidimensional space time data we adopted and adapted existing software to automatically harvest support entry of metadata and establish standardized data services to serve and enhance access to the datasets shared in hydroshare keywords multidimensional space time data netcdf collaborative data sharing hydroshare cyberinfrastructure 1 introduction with advances in hydrologic monitoring and model simulation technologies hydrologic research has become data and computationally intensive resulting in large volumes of scientific data generated or collected by individual researchers and organizations advances in hydrologic understanding now tends to require discovery access and integration of heterogeneous and dispersed data from multiple sources moreover large scale hydrologic problems often need to be solved by collaboration among researchers thus working as a team to collaborate around data has become indispensable these emerging trends in hydrologic research are key drivers that demand new tools to support the entire research cycle of data creation discovery access curation publication and analysis to help achieve new scientific breakthroughs horsburgh et al 2015 rajib et al 2016 morsy et al 2017 the consortium of universities for the advancement of hydrologic science inc cuahsi has devoted great effort to the development of cyberinfrastructure ci to satisfy this need including hydroshare http www hydroshare org a next generation internet based hydrologic information system his tarboton et al 2014 hydroshare was developed to extend the capability of the earlier server based cuahsi his which focused on the sharing of point observation time series data horsburgh et al 2008 2009 tarboton et al 2009 given that the needs of hydrology researchers go well beyond time series data hydroshare was established to add support for sharing a broader range of hydrologic datasets and models that are widely used in the hydrologic science community these include data of varying dimensionality such as time series i e observations varying over time at a single fixed location geographic raster i e a regular raster grid representing a spatial field at a single time geographic feature i e geospatial data represented by points lines or polygons representing a single point in time and multidimensional space time data i e data that may vary in both space and time as well as model instances and model programs morsy et al 2017 as a first step in hydroshare development a resource data model was designed that enabled storing transmitting and cataloging of resources comprised of these diverse hydrologic data types and models to facilitate discovery horsburgh et al 2015 details for each of hydroshare s supported data types were specified including required data format and content metadata elements and functions for data processing analysis or visualization hydroshare s resource data model was designed to generalize the way datasets and models were managed and shared while at the same time supporting specific metadata elements and functions required for enhancing the hydrologic analysis capability and interoperability for different data types and models multidimensional space time data referred to as md data in this paper is widely used in hydrology for both observations and model results examples include weather and climate data such as spatially distributed precipitation temperature wind speed and humidity used as model inputs or snow water equivalent and soil moisture output from models while commonly used there are several challenges associated with md data that can make data sharing more difficult one challenge is that there is no single accepted data format for storing this type of data to support the interoperability needed for data sharing and analysis formats used include tagged image file format tiff https www adobe io open standards tiff html gridded binary grib https www wmo int pages prog www wmocodes guides grib common data format cdf http cdf gsfc nasa gov hierarchical data format hdf https www hdfgroup org and network common data form netcdf http www unidata ucar edu software netcdf file size can be another challenge md datasets can be large making it inefficient to download and exchange entire large files when users may need only a subset or slice of them for visualization or analysis it is also important to effectively capture expose and support the recording and editing of metadata associated with the md data files recognizing these challenges this paper describes our efforts to establish functionality to support the sharing of md data in hydroshare currently several websites and software tools can be used for sharing md data and each has its own strengths and limitations for example figshare http figshare com is a website that enables users to manage their research output in the cloud to be stored shared published and discovered it supports permanent data publication and provides citation information for shared datasets to give the data provider credit and make their datasets citable figshare supports social functions such as commenting and access control to facilitate collaboration around the datasets however although figshare has functions to capture simple metadata and preview file contents for commonly used formats such as microsoft word pdf and microsoft excel no functions are provided to preview or edit the metadata or contents of the more advanced scientific data formats used for md data e g netcdf and cdf these limitations hinder users ability to describe preview access and interpret file contents through the website which can be a barrier to data sharing inhibiting data reuse and the reproducibility of scientific analyses the thematic real time environmental distributed data services thredds and hyrax data servers can provide cataloging functionality and support access to metadata and data for scientific datasets through various data access protocols opendap 2017 unidata 2017a however this does require that a data provider have access to or be able to install and maintain server software and hardware the thredds catalog and the open source project for a network data access protocol opendap service are common services supported by these two data servers thredds catalogs are logical directories of available online datasets that help in data discovery the opendap services enable users to subset or preview the contents of remote datasets and metadata moreover opendap client software programs exist that can help retrieve remote datasets for analysis and visualization these include the netcdf4 python package the rnetcdf package for the r statistical computing environment netcdf operators nco zender 2008 integrated data viewer idv unidata 2017b and panoply nasa 2018 etc thus there is important value to data consumers made available through these services motivating the need for this serving capability to be available for individual researchers or small research groups that do not have the capacity to set up thredds or hyrax servers another issue with existing thredds and hyrax server functionality is limited exposure to search capabilities which may prevent or impede scientists from discovering datasets using search terms or the geolocation of the dataset etc the repository for archiving managing and accessing diverse data ramadda http ramadda org is another web based application framework that provides a broad suite of services for content and data management publishing and collaboration with ramadda users can search access upload or comment on datasets the system incorporates the opendap service and data analysis tools to provide functions for file content preview metadata capture and curation and data subsetting and analysis for md data however as with the thredds and hyrax data servers sharing md data with ramadda requires setting up and maintaining the services which may make sharing research datasets impractical for individual researchers or small research groups to address some of these limitations in this paper we report the hydroshare md data representation design and implementation it provides functionality to help share md data to promote data curation publication and reuse we present a use case that demonstrates the new capabilities and contrasts them with capabilities of existing systems when researchers store and share md data in a format not widely used by the scientific community current data sharing systems treat these files as generic file objects which makes it difficult for others to know about the detailed metadata they contain it also makes it challenging to directly subset visualize or process the datasets through the data sharing system in contrast our approach enables users to share md data in the netcdf data format metadata from the file are exposed and presented in hydroshare users can collaboratively edit metadata in hydroshare and have these edits easily updated in the netcdf file when made public the md data are automatically registered in a hydroshare connected thredds server that enables remote access using the opendap service without data providers being required to provision any server hardware or install or configure any software with hydroshare s inherent data discovery versioning publication and social functions users can collaborate around datasets from initial data preparation to final data publication and the sharing discovery and reuse of md data is simplified in this paper we describe the design development and testing of this approach section 2 provides a brief introduction of hydroshare system section 3 describes the functional design and implementation for the md data type in hydroshare section 4 presents a use case that demonstrates functions in hydroshare that facilitate collaboration among users for data preparation publication and reuse discussion and conclusions are provided in the final sections 2 background hydroshare is a web based hydrologic information system that provides functionality for metadata capture and curation data manipulation data publication data discovery and collaboration tarboton et al 2014 horsburgh et al 2015 these functions represent a new paradigm in data sharing systems supporting discovery through the integration of information from multiple sources team work collaboration reuse of data and transparency to enhance trust in research findings fig 1 shows a high level view of hydroshare s system architecture the resource sharing functionality provides a web user interface to help users store and manage shared datasets and models in hydroshare the actions on resources functionality includes web applications or web services from hydroshare or third party organizations that enhance the capability for data analysis visualization or model simulation the interactions between resource sharing and actions on resources are through hydroshare s representational state transfer rest application programming interface api and irods client interface e g irods python api heard et al 2014 provide additional details of each open source component e g django and irods the hydroshare resource data model was designed and implemented to manage various types of hydrologic datasets and models horsburgh et al 2015 a hydroshare resource is the granular unit of shared content for access control serialization for transport over the internet and cataloging for discovery within the system major concepts for hydroshare resource are listed below and fig 2 shows an example to explain their relationships resource content files single or multiple files uploaded into hydroshare by users that make up different hydrologic datasets and models or additional informational files resource level metadata file a file encoded using extensible markup language xml and generated by hydroshare for metadata that documents the whole resource content type a widely used and well known hydrologic data type e g time series geographic feature and md data which is supported in hydroshare with advanced functions for metadata management and data analysis content type files single or multiple resource content files within a resource that represent a dataset of a supported content type content type metadata file a file encoded using xml and generated by hydroshare for metadata that documents a dataset of a supported content type in fig 2 an example hydroshare resource consists of a resource level metadata file and multiple resource content files these resource content files include an informational file a microsoft word document and multiple groups of files to represent different hydrologic datasets e g time series and md data each group of files or aggregation includes a content type metadata file and one or more content type files to represent a hydrologic dataset for a supported content type in hydroshare hydroshare s resource data model allows for definition of a new content type through specification of a content type data model the content type data model defines the data format and contents of the files along with content type metadata elements resource content files associated with a supported content type are grouped together into an aggregation by following the open archives initiative object reuse oai ore standard which is used for the description and exchange of aggregations of web resources lagoze et al 2008 thus an instance of a content type is referred to as an aggregation in hydroshare e g md aggregation in fig 2 the advantage of the resource data model design is that hydroshare can manage e g storage on disk packaging for delivery over the internet access control and cataloging for discovery multiple types of datasets and models in the same way regardless of the data formats and contents meanwhile a content type data model enables users to standardize data formats and syntax and to add additional metadata to describe the hydrologic datasets developers can then use the standardized data formats and metadata to create advanced functions to facilitate metadata management data analysis or data visualization this paper specifically reports the design of the content type data model for md data and serves as an example demonstrating how to extend the hydroshare resource data model with a new content type 3 methods we designed the md content type in hydroshare to support the sharing of md data using the following steps 1 we designed and implemented a content type data model for md data 2 we developed automated functions to support metadata extraction and editing for md data and 3 we set up the opendap web services to facilitate remote data access for data subsetting analysis and visualization detailed methods we used for each step are described in the following sections 3 1 md content type data model 3 1 1 content type files since there are many scientific data formats capable of storing md data we evaluated the benefits and tradeoffs of these data formats and chose the one that we felt was most suitable for data storage and management in hydroshare we established the following criteria to decide the data format used to represent md data in hydroshare first the data stored in the file needed to be organized in a way that helps users understand the data structure and retrieve a subset for analysis second the data format needed to be widely recognized and used in hydrologic research with available open source software or libraries to help analyze or visualize the data third widely accepted standards needed to be available to guide users in how to organize the data contents and metadata in the file to promote interoperability for data processing and sharing based on these criteria we compared several data formats that are widely used in the hydrologic science community including tiff grib2 cdf hdf5 and netcdf and adopted the netcdf data format to store md data in hydroshare tiff format is often used to store md data with each file representing a physical phenomenon at a time slice over a spatial coverage however this format is inconvenient for data transfer and data subsetting when a md dataset involves a large number of files grib2 cdf hdf5 and netcdf data formats are able to store md data and associated metadata within one file open source software programs for these data formats are available to support data analysis or visualization the reasons for selecting the netcdf data format were its wide use in modeling research in hydrology and aligned fields such as atmospheric science its adoption as a standard ogc 2011 and support for standards for its metadata eaton et al 2017 esip 2017 netcdf is a set of software libraries and self describing machine independent data formats that support the creation access and sharing of array oriented scientific data the netcdf software includes c fortran77 fortran 90 and c interfaces for accessing netcdf data programming interfaces are also available for python r java and other languages netcdf utilities such as ncdump are available to facilitate simple data management tasks the netcdf data format can actually refer to multiple formats in this paper we specifically refer to the netcdf classic formats and the netcdf 4 hdf5 format which are based on the netcdf classic model and the enhanced data model the netcdf user s guide provided additional details https www unidata ucar edu software netcdf docs user guide html the netcdf data format is widely used to represent md datasets as the input or output for hydrologic models david et al 2011 sen gupta et al 2015 thornton et al 1997 it has also been used for data management and curation of data converted from other data formats guo et al 2015 moreover many software programs and libraries available for netcdf data processing analysis or visualization are widely applied among the research community unidata 2017c thus researchers with these tools can easily manipulate netcdf files this capability was an advantage that enabled us to develop the functions in hydroshare without starting from scratch furthermore several conventions https www unidata ucar edu software netcdf conventions html are available to promote the processing and sharing of data in the netcdf data format for example the netcdf climate and forecast netcdf cf convention eaton et al 2017 specifies how to define the dimensions variables and attributes to represent md data as regular grid data or point time series the attribute conventions for data discovery acdd esip 2017 were designed to define the metadata attributes needed to describe the whole netcdf dataset to a discovery system with the selection of the netcdf data format we specified that a md aggregation should include only one netcdf file uploaded by the user and one metadata header information text file automatically generated by the system from the uploaded file to provide a brief summary of the contents in the netcdf file a hydroshare resource may contain one or many md aggregations for the netcdf file it is recommended that users define the dimensions variables and attributes by following the netcdf cf and the acdd conventions hydroshare does not prevent users from sharing md data in netcdf files that do not follow these conventions however the functions developed to harvest metadata were based on these conventions when they are not followed in the netcdf file metadata will not be automatically extracted and users will need to enter it manually the metadata header information text file is represented in a form called network common data form language cdl it is a human readable text representation of the metadata contained within the netcdf file this file includes information about the defined attributes and data structures extracted from the netcdf file to provide users a brief summary of the file s contents 3 1 2 content type metadata a hydroshare resource holding a md aggregation has two sets of metadata elements fig 3 a shows the resource level metadata elements they are based on the standard dublin core metadata elements which are common to all hydroshare resources describing the general attributes of a resource e g title creator abstract fig 3 b shows content type metadata elements or aggregation level metadata designed to describe the md aggregation the content type metadata includes general elements which are based on dublin core metadata elements to capture the basic information of any content type e g keywords and coverage and extended elements which are designed by content type developers to capture the data features in the netcdf file e g spatial reference and variable information some metadata elements were designed to contain sub elements for example the creator metadata element in fig 3 a has sub elements such as name organization and email that apply to the creator similarly the netcdfvariable metadata element in fig 3 b includes sub elements to describe the name data type units etc for a given variable in designing the md content type we chose to extract metadata elements held within the netcdf file and explicitly list them as the resource level or content type metadata for two reasons first this made it easier to present the full metadata description on a resource s landing page in hydroshare which is the web page where users view and manage the resource making it more accessible to potential users of the data e g potential users are not required to download or open the netcdf file to learn about its contents second the resource level metadata and content type metadata help hydroshare and potentially other web services catalogue the information to enable data discovery which allows users to search datasets in hydroshare based on desired data attributes 3 1 3 content type implementation in hydroshare a new content type can be created by inheriting from the abstract content type given this general extensibility pattern we implemented a new content type to manage md data in hydroshare a uml diagram of the logical database design for the md content type is shown in fig 4 which presents only major classes attributes and methods to demonstrate the organization of the md content type in hydroshare in the uml diagram there are two categories of classes 1 the abstract classes that are inherited by any new content type including the abstractcontenttype class the abstractcontenttypemetadata class and the abstractmetadataelement class and 2 the classes that define the md content type including the mdcontenttype class mdmetadata class spatialreference class and netcdfvariable class a brief description of each class is listed as follows abstractcontenttype an abstract class that provides the interface to represent a content type it includes the properties and methods for the system to manage a content type and provides a common interface to enable content type related functions for example the set file type method is used to check the data format of uploaded datasets if the data format is for a supported content type in hydroshare an aggregation will be created abstractcontenttypemetadata an abstract class used to manage the content type metadata this class by default contains general metadata elements e g keywords coverages the get xml method is used to generate the content type metadata xml file the has all required elements method is used to check if the required metadata elements for a content type are provided by the user before the resource is shared to the public abstractmetadataelement an abstract class used to represent a metadata element and define its sub elements and methods this class can be used to define extended metadata elements for any content type mdcontenttype a class that manages the md content type and inherits from the abstractcontenttype class functionality specific to the md content type had to be developed by overriding some methods of abstractcontenttype class for example the set file type method was overridden to check if the uploaded md data is in netcdf data format to create a md aggregation mdmetadata a class that manages the md content type metadata and inherits from the abstractcontenttypemetadata class this class is composed of general metadata elements and extended metadata elements e g variables and spatial reference the get xml method and has all required elements methods in the mdmetadata class override the corresponding methods from the abstract class for the md content type spatialreference a class that manages the spatial reference extended metadata element for the md content type this class inherits from the abstractmetadataelement and is contained in the mdmetadata class it includes attributes for storing the sub elements for spatial reference metadata e g projection name projection string and value netcdfvariable a class that manages the variable extended metadata element for the md content type this class inherits from the abstractmetadataelement class and is contained in the mdmetadata class it includes sub elements to describe a variable stored in the netcdf file e g name unit and type 3 2 additional content type functions as described above hydroshare provides a base set of functionality for each resource that includes access control publication social functions etc however one of the advantages of the design and implementation we describe here is that additional functionality can be developed for a specific content type to support specialized metadata management and sharing of the data via content type specific web services without affecting other content types in the following subsections we describe how this functionality was created for the md content type 3 2 1 metadata management functionality two functions were designed to 1 extract information where it exists from the netcdf file to populate the resource level and content type metadata elements and 2 generate the metadata header information text file when a user uploads a file with the nc extension hydroshare will test whether the file holds valid netcdf content and if successful execute these functions to create a md aggregation from the file aside from metadata extraction functions we designed functionality for editing metadata in the netcdf file through hydroshare we established a mapping between hydroshare s metadata elements and the acdd and netcdf cf conventions table 1 when a user edits the metadata in hydroshare the system utilizes the metadata mapping to check for consistency between the netcdf file and the hydroshare metadata if there is a need to add or update the metadata in the netcdf file the system will notify the user and the user can have the system update the file based on the new metadata edits this functionality helps a user easily update the netcdf file without having to download and manually edit it when the initial file includes little metadata this functionality makes it easy to create metadata in the file that follows netcdf conventions we used the netcdf4 python library and netcdf utility ncdump to implement the metadata extraction functions for files that follow acdd or netcdf cf conventions the automated metadata extraction function retrieves and populates matched hydroshare metadata elements based on the metadata mapping table 1 for files without acdd metadata elements but with spatial or temporal coordinate variables given following the netcdf cf conventions spatial and temporal coverage metadata elements determined by reading these data variables are populated in the content type and resource coverage metadata the metadata editing functionality was implemented using the netcdf4 python library and hydroshare irods client interface fig 1 when metadata needs to be updated the system first copies the original netcdf file within the irods file storage system to a temporary folder second the system writes hydroshare s metadata into the copied file using the netcdf4 python library then the system generates a new metadata header information text file from the updated copied file finally the system replaces the original netcdf file and the metadata header information text file in irods with these newly created files 3 2 2 opendap service opendap services help users learn about and work with the contents of the datasets without being required to download them first users are able to retrieve a subset of the data for use cases that require smaller spatial or temporal extent to provide this capability for hydroshare users we automated the process of creating an opendap web service for all publicly shared md data in hydroshare users can access and subset the dataset stored in hydroshare through an opendap data access form in a web browser or through existing opendap client software for data visualization or processing in hydroshare support for opendap services was created by setting up a thredds data server to interact with hydroshare s irods file storage system in the system architecture shown in fig 1 the data server plays the role of providing web services to enhance the capability for data analysis orange frame the data server requires direct file system access to the netcdf files for its opendap services thus we used existing irods client software to interface to the irods network file system yellow arrow connecting the orange and purple frames in fig 1 we developed a script that copies hydroshare public resources containing md aggregations efficiently using the irods multi thread parallel transfer iget command to a directory on the data server this copying occurs 1 when access control for a private resource is changed to public and 2 when the time stamp of a public resource on the data server is older than that in hydroshare and a data update is needed this takes advantage of irods high performance parallel data transfers but in the present implementation does require duplicate storage of netcdf files moreover since the data server does not support file level user access control as would be required for access to private files in hydroshare the opendap service is limited to netcdf files stored in public or formally published resources in hydroshare this functionality saves users from the work that would be required to set up a server to host opendap services for their datasets and gives them the freedom to decide when to make their datasets accessible via opendap services by using hydroshare s access control settings 4 results we validated the design with an experimental use case to demonstrate how sharing md data in hydroshare can help users collaborate around datasets and to facilitate the activities involved in the data management life cycle shown in fig 5 we considered a use case where a researcher simulated the snowmelt process for the dolores river watershed in the colorado river basin from 1988 to 2010 and shared model results in hydroshare this was part of a study that the authors were involved in on snowmelt modeling and operational water supply forecasting within the colorado river basin gan 2019a we present the results to demonstrate how the researcher shared the model output of snow water equivalent as md data in hydroshare to support the activities from data creation data publication to data analysis this use case involved multiple hypothetical users and was implemented by the first author acting as these users from separate hydroshare accounts 4 1 data creation and preparation the simulated snow water equivalent datasets were initially stored as separate two dimensional geospatial data files for each 6 h time step by the model this results in thousands of model output files for a 22 year simulation sharing of these original model output files has limitations that make data management and reuse difficult first information may be lost if any file is missed during the file transfer process second when the original model output files are in a format not widely used by the research community it is inconvenient to extract subsets that involve thousands of files and difficult to find available software for data analysis or visualization thus the researcher developed a python script to reorganize and convert the multiple original model output files into one netcdf file fig 6 shows the visualization of the use case md data upon uploading the use case md data into an empty hydroshare resource the type of data file was automatically recognized and a md aggregation was created in the resource hydroshare generated a resource landing page which provides different functions for the user to manage the resource fig 7 a and shows the content type files and content type metadata for the md aggregation fig 7 b for data preparation the user can use the data access control and the data versioning functionality fig 7 a to collaborate with trusted users to prepare the shared datasets with multiple versions if the original dataset evolves users can also edit delete copy and formally publish the resource via its landing page 4 2 data description and publication when the md aggregation was created in hydroshare two metadata extraction functions were executed a metadata header information text file was automatically created and stored in the content type file folder left panel in fig 7 b the content type metadata such as title keywords spatial temporal coverage spatial reference and variable metadata were automatically created by extracting metadata from the netcdf file right panel in fig 7 b the researcher collaborated with a trusted colleague referred to as user 1 to update the content type metadata in hydroshare to better describe the data hydroshare s metadata editing function was then used to update the metadata into the netcdf file for instance when user 1 added a new keyword in the metadata panel hydroshare s consistency check identified the presence of newly added metadata and showed an update netcdf file button fig 7 b to inform the user that the netcdf file could be updated with the new information then user 1 clicked the button to have hydroshare update the metadata in the netcdf file this is an example of how using hydroshare multiple users can collaborate to annotate the resource with metadata this metadata editing function enhances netcdf files to have more attributes that follow netcdf conventions after the data description was completed the researcher formally published the final data product with an assigned digital object identifier doi in hydroshare gan 2019b the suggested citation information was generated in hydroshare to encourage proper citation of this dataset fig 7 a 4 3 data discovery and analysis after the resource was formally published anyone can discover this dataset using hydroshare s search and filter functions fig 8 a hydroshare user referred to as user 2 provided a search term colorado river basin and hydroshare listed matching resources by querying the hydroshare metadata elements such as title abstract and keywords the search results were filtered based on different metadata facets such as content type author and subject user 2 identified the use case resource and used hydroshare s map search function to determine the geographic location associated with this dataset after discovering this resource user 2 decided to reuse a subset of the use case md data for data analysis user 2 used the opendap service from hydroshare and the panoply client software for data visualization without downloading the use case netcdf file to a local computer fig 6 fig 9 shows the nco commands used to access subset and process the use case dataset using the opendap service the code first subsets the data from january 1st to may 31st 2009 to identify the maximum snow water equivalent for each grid cell which provides the maximum snow accumulation assumed to occur within this period for that year max nc the code then retrieves the data for april 1st and april 15th and evaluates the snow water equivalent difference between the two dates which provides the analysis result for accumulation increase or ablation decrease during the period diff nc water managers often track such snow water equivalent changes in water supply forecasts user 2 then uploaded the data analysis code and the derived netcdf files into hydroshare as a new resource gan 2019c which started a new cycle of activities involved in collaborative data publication and reuse to improve research reproducibility 5 discussion the use case illustrated how organizing md data using the netcdf data format and sharing it in hydroshare provided added value in terms of functionality for metadata management and data analysis when compared with other md data sharing methods this approach has the following advantages it provides functionality to capture expose and edit the metadata stored in the netcdf file the manage access function enables users to collaborate on metadata editing and thus improve its description of the data by following metadata standards other data sharing methods either do not automatically expose the metadata from a netcdf file or make it difficult to collaboratively edit the metadata in the netcdf file it provides automated opendap services with access control for shared datasets to support data analysis that enhance opportunities for collaboration around the data other data sharing methods either do not provide an opendap service or require effort to set up and maintain a server and service it provides better data discovery functionality for the shared datasets it supports keyword and geolocation searches based on a catalog of metadata extracted from the netcdf file or input by the data provider other data sharing methods provide limited discovery capability to search the md data based on their attributes in our approach several key factors make this advantageous functionality available 1 we adopted a standard data format netcdf to organize md data which has conventions that standardize how data and metadata are organized in the file to improve the interoperability of datasets 2 we utilized existing tools and standard data services to develop automated functions for metadata management and data analysis to promote md data sharing and reuse and 3 hydroshare s resource data model design helps improve consistent data discovery access and publishing across the broad range of data types used by scientists in the hydrology domain while at the same time allowing value added functionality for specific data types however there are limitations that need further improvements for sharing md data in hydroshare one limitation is that some users may not be familiar with the netcdf data format users need to learn how to organize md data in this data format for data sharing another limitation is web based visualization there is a need for additional functionality that provides researchers with greater capacity to process and visualize datasets directly without transferring the data or subsets of the data between the data sharing system and their local computers to address these limitations one possible solution is to create and share jupyter notebook examples in hydroshare to support md data visualization and demonstrate how to convert md data in various formats into the netcdf format in the future we are considering support for other widely used standard variable ontologies such as the community surface dynamics modeling system csdms standard names peckham 2014 in metadata management functions another enhancement we are considering is enabling web services beyond the opendap service for shared md datasets to increase the value and usability of netcdf data including open geospatial consortium ogc web map service wms ogc web coverage service wcs netcdf subsetting service ncss etc 6 conclusions hydroshare is a web based hydrologic information system that provides researchers with a platform to share their hydrologic data and models as md data is one of the widely used data types in hydrologic research we developed an approach to support sharing of this data type within hydroshare our approach was aimed at overcoming challenges for sharing md data including 1 lack of a single accepted standard data format to support the interoperability needed for data sharing and analysis 2 lack of advanced functions to preview or edit the metadata in the file and 3 difficulty in subsetting data from large datasets for data visualization and processing to address these challenges we adopted a standard data format netcdf and standard metadata elements to manage md data in hydroshare and we implemented value added functionality to manage metadata and support data reuse the use case presented demonstrates the new capabilities in hydroshare and shows that researchers can share md data in a netcdf file with the metadata automatically exposed in the system metadata can be edited collaboratively in hydroshare and automatically updated in the netcdf file once publicly shared users can subset the data with the automatically configured opendap service for visualization and analysis without effort to set up and maintain a server in concert with existing hydroshare functionality e g data discovery data publishing and access control the work described here enables relatively straightforward sharing and formal publication of md data this increases transparency and reproducibility of the associated research and promotes reuse of data and the derivation of additional value from research data investments beyond the context of the new functionality we have demonstrated another contribution of this work is that the methods we developed for improving sharing of md data can be used as examples for supporting other data types in hydroshare or for better supporting md data in other systems cyberinfrastructure developers who are going to build or have built a data sharing system to support md data sharing can use the recommendations of this work to organize data in a standard data format and document the datasets using the standards based metadata using the patterns we established they may be able to create standard data services or develop new functionality to facilitate metadata management data analysis or visualization adopting standard formats and techniques across data repositories could lead to a level of interoperability that is worth considering in the future software availability the software created in this research is free and open source as part of the larger hydroshare software repository the hydroshare software repository is managed through github and is available at https github com hydroshare hydroshare funding this work was supported by the national science foundation oci 1148453 oci 1148090 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national science foundation under collaborative grants oci 1148453 and oci 1148090 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation we are also thankful to others on the hydroshare development team for providing suggestions on function design and implementation for hydroshare system 
26016,mechanistic phytoplankton functional group pfg models are used to develop water quality targets designed to mitigate cyanobacteria blooms but it remains unclear whether pfg models adequately simulate cyanobacteria dynamics as most are evaluated against observations of chlorophyll a instead of pfg biomass to address this challenge we analyzed an application of ce qual icm a 3d mechanistic pfg model used by water managers and modelers global sensitivity analysis was employed to assess the sensitivity of modeled chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass to 42 uncertain input factors in ce qual icm s pfg growth and loss functions results revealed that parameterization of ce qual icm captured bloom variation but underpredicted bloom peaks and simulated chlorophyll a with greater skill than pfg biomass additionally when run across realistic ranges of pfg parameter values model outputs were highly sensitive to chlorophyll to carbon ratios and phosphorus uptake parameters indicating that these factors should be the focus of targeted parameterization efforts 1 introduction research organizations and government agencies conduct long term water quality sampling to monitor threats posed by phytoplankton blooms which are intensifying in magnitude frequency and duration in many cases because of the impacts of human activities such as cultural eutrophication and climate change carey et al 2012 glibert et al 2005 o neil et al 2012 paerl et al 2011 chlorophyll a is classically measured as a proxy of phytoplankton biomass within these monitoring programs and used as a response variable in models designed to explain and predict blooms and their secondary effects e g changes in dissolved oxygen however chlorophyll a is an aggregate measure and its dynamics may not fully reflect shifts in biomass of key functional groups e g n longphuirt et al 2019 additionally changes in phytoplankton community composition and eco physiological responses to changing environmental conditions can alter the relationship between chlorophyll a and biomass jakobsen and markager 2016 to capture phytoplankton community dynamics underlying changes in chlorophyll a scientists and engineers have developed models that simulate phytoplankton functional groups pfgs which are groups of phytoplankton with shared characteristics such as morphological and eco physiological traits kruk et al 2002 c s reynolds et al 2002 in theory these models quantify phytoplankton growth kinetics and temporal variations in the ecological processes underpinning phytoplankton community structure thereby representing pelagic productivity dynamics more realistically as compared to models that solely simulate chlorophyll a in practice pfg models can be difficult to parameterize and validate because the processes they simulate are contextualized by significant ecological and biological complexity arhonditsis and brett 2004 rigosi et al 2010 robson 2014 shimoda and arhonditsis 2016 consequently pfg parameterization often occurs subjectively or through calibration frede thingstad et al 2010 these complexities result in pfg model algorithms and their parameters being largely uncertain many pfg model parameters such as rates associated with growth kinetics are not extensively measured or documented in existing literature anderson 2005 franks 2009 shimoda and arhonditsis 2016 although uncertainties propagate through direct and interactive effects to model outputs it remains difficult to assess the degree to which parameter uncertainties impact pfg model simulation skill in part because there are relatively few published pfg modeling studies that include quantified performance metrics a recent review of pfg modeling analyses by shimoda and arhonditsis 2016 revealed that only around 30 of published studies applying pfg models within freshwater systems reported pfg goodness of fit measures the lack of information on pfg performance may partly result from limitations in the availability of detailed and long term observational pfg data shimoda and arhonditsis 2016 among the pfg modeling studies that have presented observations to support simulated outputs the performances are often poor for example shimoda and arhonditsis 2016 discovered that published pfg modeling studies in which cyanobacteria was included as a state variable n 68 simulatedcyanobacteria dynamics with a median model efficiency of 0 06 calculated as 1 o p 2 o o 2 where o observations p predictions and o the mean of observations a level of performance not ideal for supporting management and policymaking ritter and muoz carpena 2013 increasing demands on water managers to mitigate the effects of harmful algal blooms such as those composed of toxic cyanobacteria will be met by a growing need for pfg models that can support managers decision making thus it is important that efforts be made to improve pfg model structure and performance by taking a critical look at the processes and parameters contributing uncertainty to simulated pfg dynamics one approach that allows for systematic assessment of pfg model structure and performance is global sensitivity analysis gsa as with any type of sensitivity analysis gsa is used to quantify the degree to which model outputs are sensitive to changes in input factors e g parameter values boundary conditions algorithm structures etc saltelli et al 2008 gsa contrasts with other sensitivity analysis approaches due to its capacity to assess nonlinear and interactive effects in high dimensional models saltelli et al 2004 2008 results produced from applications of gsa reveal the relative importance of model input factors model input factors that are found to dictate model performance should be addressed in order to ensure they are being assigned the most accurate values conversely factors identified as unimportant could potentially be removed from the model thereby helping to reduce model complexity and uncertainty jakeman et al 2006 muoz carpena et al 2007 saltelli et al 2004 2008 in addition gsa can help identify processes involving important factors that could be targeted for model improvement thus by methodically informing ways to improve upon the algorithmic structures of pfg models gsa can markedly increase their utility the overall goal of this study was to address the need for further quantitative scrutiny of state of the art pfg models our primary objective was to quantify the global sensitivity of pfg model outputs to parameters related to phytoplankton growth and loss processes to meet this objective we applied gsa to the corps of engineers integrated compartment water quality model ce qual icm a highly specified and state of the art 3d mechanistic water quality and pfg model that couples with the environmental fluid dynamics code efdc a 3d mechanistic hydrodynamic model ce qual icm is supported by the united states environmental protection agency and used to inform the development of water quality targets for regulatory purposes cerco and cole 1995 specifically the gsa was used to assess ce qual icm s sensitivity to a large number of input factors when simulating chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass over an 8 year period in a flow through lake located in florida usa the availability of detailed and long term phytoplankton monitoring data allowed for model goodness of fit to be quantified for all outputs and for application of gsa parameters were varied across realistic value ranges supported by a survey of the literature thus making the findings regarding model sensitivity of broad interest to our knowledge this is the first application of gsa to a highly complex pfg model 2 methods 2 1 ce qual icm an application of ce qual icm a 3d mechanistic water quality and pfg model was used for this study ce qual icm was developed by the u s army corps of engineers cerco and cole 1995 and is described by the u s environmental protection agency epa as a model that can support nutrient target development epa 2001 ce qual icm couples with the environmental fluid dynamics code efdc a 3d process based hydrodynamic and transport model capable of simulating complex physical processes pertinent to aquatic system dynamics hamrick 1992 the epa specifically outlines ce qual icm as a model suited for dynamic simulation of estuarine and coastal marine water quality but it can also be parameterized for freshwater applications ce qual icm was most notably applied to the chesapeake bay during the development of total maximum daily load standards cerco and noel 2013 ce qual icm can simulate the carbon biomass of up to three pfgs as well as many other water quality factors such as chlorophyll a dissolved oxygen nitrogen phosphorus and silica pfg growth dynamics are governed by equation 1 1 b t p r p r w z b where b phytoplankton biomass mg c l 1 t time day p production day 1 r respiration day 1 pr predation day 1 w settling velocity m day 1 and z depth m production is calculated as the product of an optimal production rate pm and the effects of inorganic nitrogen f n phosphate f p silica f s illumination f i and temperature f t production is mediated by f t and f i as well as the minimum of f n f p f s equation 2 2 p p m f t f i min f n f p f s the effects of nitrogen and silica limitation are modeled by monod functions whereas the effect of phosphorus limitation is modeled with droop kinetics cerco et al 2004 f i is calculated as a productivity intensity curve jassby and platt 1976 using an optimized light extinction model developed for our study system gallegos 2005 f t is an approximately gaussian probability curve that peaks at a user specified optimal temperature salinity toxicity is also considered within the governing growth equation through an empirical function the rate of basal respiration is increased when salinity reaches toxic levels the predation function can be specified to assume that the rate of predation is either linearly or quadratically related to phytoplankton biomass both predation and respiration are corrected for the effects of temperature in total ce qual icm s pfg growth and loss equations include 20 22 parameters and these equations are parameterized for each pfg included in the model e g to dynamically simulate two pfgs 40 42 parameters need to be specified details on these and other ce qual icm equations are outlined in cerco and cole 1995 and sucsy and hendrickson 2003 2 2 global sensitivity analysis elementary effects morris method the morris method morris 1991 is one of the most commonly used gsa approaches this method measures sensitivity through two metrics the 1 mean  and 2 standard deviation  of each model input factor s elementary effects elementary effects are the resulting model output changes that occur with a change in factor value equation 3 said another way elementary effects equal y x i across a multifactor space where y is the output and x i is an individual model input factor within a i factor space more specifically a factor s elementary effects ee are calculated as 3 e e i y x 1 x 2 x i x i 1 x k y x x 2 x i  x i 1 x k  where y is the model output x is a model input factor k is the total number of model input factors and  is the fixed amount by which the model input factor xi is varied to determine  the pre defined model input factor value ranges are divided into a discrete number of specified levels p researchers have commonly used four levels p 4 e g campolongo et al 2007 khare et al 2018 model input factor ranges are discretized into p levels to calculate each model input factor s  equation 4 4  0 1 p 1 2 p 1 1 after discretizing the factors value ranges into levels the resulting domain of model parameter factor values is sampled and these samples are used to parameterize model runs several sampling strategies exist the sampling strategy first proposed by morris involves randomly throwing trajectories across the discretized hyperspace the starting points for each trajectory are random but the trajectory itself will move one at a time across the hyperspace to take into account the  changes in different model input factors each trajectory includes k 1 points selection of the total number of trajectories r is subjective but r 10 is generally considered to produce satisfactory results muoz carpena et al 2007 the number of model runs n required by this sampling strategy is equation 5 5 n r k 1 trajectory based sampling strategies other than those of morris 1991 exist and include the methods of optimized trajectories campolongo et al 2007 modified optimized trajectories ruano et al 2012 and sampling for uniformity khare et al 2013 these methodologies offer a variety of benefits such as enhanced sample generation runtime efficiencies that prove useful for running gsa with high dimensional models in particular the enhanced sampling for uniformity esu strategy chitale et al 2017 khare et al 2015 which ensures sampling uniformity maximizes trajectory spread and computational efficiency is used herein once the model is run n times the corresponding outputs are collated to calculate the elementary effects resulting from varying model input factors the mean  and standard deviation  of a factor s elementary effects reflect the degree to which the factor influences the output and interacts with other factors respectively campolongo et al 2007 found that the absolute values of a factor s mean elementary effects  better represent the sensitivity of the factor by avoiding the possibility of positive and negative elementary effects cancelling each other out during the calculation of the  when  is used the approach is described as the modified morris method each factor s sensitivity metrics   and   are plotted on a cartesian plane to evaluate the relative influence of each model input factor on the output a high value of  suggests a large effect on the output a high value of  suggests that a factor has high order interactions with other model input factors and or behaves nonlinearly campolongo et al 2007 morris 1991 according to the positions of the input factors on the   and   cartesian planes model input factors can be generally classified as linear additive mixed and nonlinear non additive nonlinear non additive factors are denoted by a position above the 45 line in the   plot and in between the  2sem sem standard error of the mean reference lines in the   plot notably these measures provide information on the qualitative importance ranking of model input factors the comparison of   and   cartesian plots for a given output allows for the identification of directionality sign of  and non monotonicity   of the input factor effects 2 3 study site lake george florida usa phytoplankton dynamics in lake george a flow through lake of the st johns river florida usa fig 1 were examined in this study due to the regular occurrence of nuisance algal blooms the phytoplankton community in lake george has been the focus of targeted monitoring efforts from 2002 to 2010 which resulted in the curation of an unusually rich dataset composed of monthly phytoplankton species counts srifa et al 2016 and monthly to thrice monthly water chemistry observations access to species level phytoplankton data created the opportunity to evaluate how a high dimensional management model represented pfg dynamics as opposed to just chlorophyll a typically due to the absence of dynamic pfg data this level of detail is rarely considered in pfg model evaluations shimoda and arhonditsis 2016 cyanobacteria were by far the most abundant group of phytoplankton in lake george over 87 of total phytoplankton carbon biomass observed in this system from 2002 to 2010 reaching annual peaks in the range of 3 11 mg c l 1 fig 2 in contrast eukaryotes only reached annual peaks of about 0 6 mg c l 1 oscillatoria spp dominated the cyanobacteria population in lake george during the study period 47 of total cyanobacteria carbon biomass followed by aphanizomenon spp cylindrospermopsis spp planktolyngbya spp and dolichospernum spp 15 14 10 and 6 of total cyanobacteria carbon biomass respectively by carbon biomass the eukaryotic community is largely composed of diatoms followed by chlorophytes and other eukaryotes 2 4 simulating lake george pfgs with ce qual icm ce qual icm and efdc were previously calibrated and applied by the st johns water management district to support the establishment of total maximum daily load standards for lake george magley 2018 and the lower st johns river sucsy and hendrickson 2003 for this study outputs from calibrated efdc simulations were used as inputs for ce qual icm the simulation period covered january 1 2002 through december 30 2010 this is the same time period that the model was originally calibrated to simulate outputs were tabulated daily for time 12 00 p m this time point was chosen due to the observed data having been manually collected in the time frame of approximately 9 00 a m to 3 00 p m as part of the st john river water management district s water quality monitoring program in situ water samples were taken with integrated pole samplers so the outputs were also vertically averaged to align with sampling protocols only outputs from the location corresponding to monitoring site lg1 2 fig 1 were evaluated due to this site also being the point of data collection within the water quality monitoring program for this study ce qual icm was parameterized to simulate two pfgs pelagic cyanobacteria and eukaryotic phytoplankton cyanobacteria and eukaryotic phytoplankton were divided into separate pfgs for this study given the differences in their ecophysiology i e nutrient uptake kinetics palatability and priority from a management perspective i e cyanobacteria are the focus of ongoing management efforts in lake george further division of these pfgs was considered impractical given the paucity of parameter values available for narrower groupings e g individual genera cyanobacteria dominate the lake george phytoplankton community nelson et al 2018 srifa et al 2016 and include a diversity of genera including nitrogen fixers e g aphanizomenon spp cylindrospermopsis spp dolichospernum spp and non fixers e g oscillatoria spp considering the importance of nitrogen fixers in the lake george cyanobacteria community specifically as related to nitrogen uptake kinetics the proportion of nitrogen fixers in the cyanobacteria pfg was used to mediate the nitrogen uptake kinetic equation f n nitrogen limitation was assumed to occur as a function of the proportion of nitrogen fixers in the cyanobacteria community equation 6 6 f n 1 f n f i x d i n k h n c d i n f n f i x d i n k h n f i x k h n c d i n k h n f i x in equation 6 fnfix observed fraction of nitrogen fixers in the cyanobacteria community din dissolved inorganic nitrogen concentration khnc half saturation constant for nitrogen uptake for cyanobacteria and khnfix dissolved inorganic nitrogen concentration threshold fnfix was varied temporally in the model based on observed phytoplankton community composition fig s1 supporting information this contrasts with the original model formulation in which nitrogen fixers were assumed to occupy a constant proportion of total cyanobacteria biomass the inclusion of khnfix is used to increase the value of f n when nitrogen fixers are present in the cyanobacteria community in turn diminishing the likelihood of nitrogen limitation in the production equation equation 2 additionally rates of predation were assumed to be linear for cyanobacteria and quadratic for eukaryotes due to their differences in palatability wilson et al 2006 further details on equations in the lake george application of ce qual icm are provided in sucsy and hendrickson 2003 2 5 application of gsa to the lake george ce qual icm 1 define model input factor hyperspace gsa requires the construction of a parameter space or hyperspace that includes all possible model parameterizations this hyperspace is defined by assigning probability density functions pdfs to each of the model inputs saltelli et al 2004 factor pdfs represent the variability or uncertainty observed in real world settings given that parameter sensitivity is calculated as a function of output variance relative to the corresponding input parameter values variances equation 3 sensitivity is related to the magnitude of individual parameters variabilities and the results of a gsa are a property of the application and not the model itself the extent to which model input pdfs are symmetrical or skewed may impact gsa results but the means value ranges and variances have a greater impact on output variability than the type of pdfs haan et al 1995 wallach et al 2014 thus a uniform pdf can be used as a conservative approximation of an input factor s range of variation for this study parameters associated with ce qual icm s algal growth and loss processes k 42 were sampled for the gsa each parameter was assigned a uniform pdf to impose the least restrictive assumptions on the analysis the uniform distributions were bound by published value ranges when available parameters for which published values were not available were varied by 20 from their calibrated values table 1 when evaluating the considered ranges it is important to acknowledge that many of the analyzed factors are essentially effective parameters in the context of the model application due to constraints associated with domain discretization and mathematical characterization of ecological processes beven 2000 and some have never been measured in a lab or field setting anderson 2005 franks 2009 shimoda and arhonditsis 2016 2 generate samples samples were generated using the enhanced sampling for uniformity strategy which maximizes trajectory spread and computational efficiency chitale et al 2017 khare et al 2015 factors were discretized at 4 levels p 4 and samples were generated using 12 trajectories r 12 thus 12 42 1 516 samples were generated n equation 5 sample generation was performed with matlab the mathworks inc 2015 3 run ce qual icm ce qual icm was run 516 times at the university of florida high performance computing hipergator facility each model run required approximately 5 h to complete amounting to around 107 days of computational time for all 516 simulations simulated time points that coincided with those of the observed dataset n 178 for chlorophyll a n 84 for cyanobacteria and eukaryote biomass were tabulated 4 summarize model outputs for each run chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass were summed across the study period to generate total values e g total chlorophyll a simulated from 2002 to 2010 total cyanobacteria biomass simulated from 2002 to 2010 etc these summations referred to hereafter as total simulated values serve as an output metric by which to evaluate how parameter variability translates into changes in model output quantities 5 quantify goodness of fit for each of the runs n 516 ce qual icm s performance was evaluated by quantifying the model s ability to simulate cyanobacteria biomass eukaryote biomass and chlorophyll a model performance was quantified with the nash sutcliffe efficiency nse coefficient nash and sutcliffe 1970 7 n s e 1 t 1 n o t p t 2 t 1 n o t o 2 where t time point n total number of time points ot observation at t mean of observed data and pt predicted data at t when nse equals 1 the predicted data perfectly match the observations when nse equals 0 the mean of the observations serves as an equally effective predictor as the model when nse is less than 0 the mean of the observations is a more effective predictor than the model meaning that the model is performing poorly 6 calculate elementary effects elementary effects were quantified in matlab using the ee sensitivity measures package khare et al 2015 and corresponding visualizations were produced in r r core team 2019 elementary effects were calculated in relation to i total simulated values and ii nse for chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass recent research challenges the use of performance based metrics e g nse to assess model sensitivity as performance based metrics measure the degree to which a model s outputs converge to observed data and not necessarily how well the model represents the dynamic nature of a system gupta and razavi 2018 here we consider both performance based metrics and the magnitude of the system response i e total chlorophyll a total cyanobacteria biomass and total eukaryotic phytoplankton biomass to permit for assessment of the importance of model parameters in driving simulated system dynamics as well as understand the sensitivity of the calibration space 3 results 3 1 model efficiency the nse values produced from the 516 simulations for each of the considered outputs were largely negative though this is unsurprising given that the range of parameter values considered may not have been well specified for the studied system despite using realistic ranges of variation from the literature the median nse values for chlorophyll a cyanobacteria biomass and eukaryote biomass were 3 0 0 53 and 8 26 respectively the maximum nse values respectively were 0 36 0 34 and 0 88 simulated outputs associated with the maximum nse values are presented in fig 2 visualizations of simulated outputs reveal that the model generally predicted the annual pattern though tended to underestimate blooms peaks as quantified by chlorophyll a parameter ranges corresponding to simulations for which the nse values were positive for simulated chlorophyll a n 27 and cyanobacteria n 17 are visualized in fig 3 the model was also run in its initial calibrated form in order to quantify the baseline performance of the ce qual icm model for lake george nse values associated with this baseline run were 0 50 0 34 and 10 47 for chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass respectively the model was originally calibrated to optimize prediction of a chlorophyll a target fraction of days per year when chlorophyll a exceeds 40 g l and the discrepancy in performance between the baseline simulation nsechla 0 50 and best simulation from the gsa nsechla 0 36 for chlorophyll a is likely explained by our having used literature defined values instead of calibrated values for many of the model parameters when running the gsa the morris method is used to explore model sensitivities not to perform an exhaustive search designed to optimize outputs against an objective function therefore the combinations of parameter values considered here allowed for exploration of the parameter space but without seeking to identify an ideal calibration parameter set 3 2 output sensitivity the   and   plots hereafter referred to as morris plots illustrate the sensitivity of total simulated chlorophyll a cyanobacteria carbon biomass and eukaryotic phytoplankton carbon biomass fig 4 and nse fig 5 to changes in the growth kinetic parameters for a tabulated list of   and  values for each of the considered outputs see table s1 supporting information in the presented morris plots figs 4 and 5 points vary in color by the parameter type and in shape by phytoplankton classification the general phytoplankton classification is used to describe parameters that are not specific to a phytoplankton functional group the parameters to which the outputs were most sensitive are labeled label definitions are included in table 1 dashed lines in the plots indicate thresholds for predominance of higher order effects e g parameters that fell above the 45 line in the   plots affected the output through interactive and higher order effects as did parameters that fell inside the 2sem v in the   plots in contrast parameters that fell below the 45 line in the   plots and outside of the 2sem v in the   plots impacted the output largely through direct effects chlorophyll a nse fig 5a and total simulated chlorophyll fig 4a were most sensitive to parameters related to cyanobacteria growth dynamics particularly the chlorophyll to carbon ratio of cyanobacteria chlcmnc after chlcmnc the basal metabolism rate of cyanobacteria bmrc and rate of cyanobacteria production under optimal growth conditions pmc were the most important parameters followed by p uptake parameters for cyanobacteria and eukaryotes minimum cyanobacteria cell quota for p q0c maximum p uptake for cyanobacteria vmaxc and eukaryotes vmaxe and the half saturation constant for eukaryote p uptake khpe and the temperature at which optimal cyanobacteria growth occurs tmpc the effects of the uncertainty of these parameters on total simulated chlorophyll a and chlorophyll a nse were largely nonlinear and interactive as demonstrated by their positions above the 1 to 1 line in the   plots and within the 2sem lines in the   plots the nonlinear and interactive effects of these parameters on nse and total simulated chlorophyll a is generally to be expected considering the complexity of ce qual icm muller et al 2011 the morris plots revealed that total simulated cyanobacteria biomass nse fig 5b was most sensitive to parameters related to cyanobacteria p uptake specifically the minimum cell quota for p q0c and vmaxc in addition to these two parameters cyanobacteria biomass nse was sensitive to bmrc total simulated cyanobacteria biomass was also sensitive to these factors fig 4b as well as chlcmnc half saturation constant for cyanobacteria p uptake khpc and p uptake parameters for eukaryotic phytoplankton vmaxe khpe parameters to which cyanobacteria nse was most sensitive propagated through the model primarily via interactive effects parameter uncertainty also influenced total simulated cyanobacteria through interactive effects but unlike cyanobacteria nse several of the parameters that total simulated cyanobacteria was most sensitive to e g q0c vmaxc bmrc pmc chlcmnc influenced simulated cyanobacteria biomass through direct effects as evidenced by their positions outside of the 2sem lines in the   plots specifically vmaxc pmc and chlcmnc drive monotonic increases in total simulated cyanobacteria via interactive effects whereas q0c bmrc and khpe drive monotonic decreases in total simulated cyanobacteria via interactive effects however uncertainty in q0c and bmrc non monotonically drove increases in cyanobacteria biomass nse eukaryotic phytoplankton biomass nse fig 5c was most sensitive to the eukaryotic phytoplankton predation rate multiplier bpre chlorophyll to carbon ratio for eukaryotes chlcmne rate of eukaryote production under optimal growth conditions pme the eukaryote i e diatom silica to carbon ratio asce and maximum rate of p uptake by eukaryotes vmaxe total simulated eukaryote biomass during the study period was only sensitive to a subset of these factors specifically chlcmne bpre pme and vmaxe fig 4c all of these parameters influenced eukaryote biomass nse and total simulated biomass through a mix of direct and interactive effects in particular chlcmne and pme influenced total eukaryote biomass via nonlinear but positively monotonic effects and eukaryote nse via nonlinear but negatively monotonic effects similarly bpre influenced eukaryote biomass via direct and negatively monotonic effects but it influenced eukaryote nse via direct and positively monotonic effects these relationships suggest that changes in bpre or the eukaryote predation function could dramatically influence the model s ability to represent eukaryotic phytoplankton biomass dynamics 4 discussion to address the need for greater in depth evaluation of mechanistic phytoplankton functional group pfg management models global sensitivity analysis gsa was applied to evaluate the importance of 42 parameters on the outputs of an application of ce qual icm a state of the art process based pfg and biogeochemical model used to inform the development of water quality targets for the management of algal blooms in fresh and estuarine waters notably this study focused on parameters used to specify pfg growth and loss equations but ce qual icm includes many other equations for simulating related processes such as those driving biogeochemical cycling that could also influence pfg biomass the parameters used to specify equations dictating other cycles e g nutrients carbon in the model could be of equal or greater importance in driving pfg model outcomes so our results should be interpreted in the context of this additional source of uncertainty for the 516 simulations needed to evaluate the influence of 42 input factors model efficiency varied and many simulations were associated with very low nash sutcliffe efficiency nse values although the morris method is not applied for optimization purposes understanding the range of nse values associated with ranges of realistic parameter values allows for assessment of the degree to which model performance metrics may vary solely as a function of parameter uncertainty all eukaryotic phytoplankton biomass nse values were negative thus indicating that observed mean biomass of eukaryotic phytoplankton would have served as a more suitable predictor of eukaryotic phytoplankton biomass than ce qual icm however in comparison to simulations from other published studies the model efficiencies for eukaryotic biomass reported herein are common shimoda and arhonditsis 2016 reported that published pfg modeling studies in which aggregated phytoplankton was simulated as a state variable had a median model efficiency of 0 20 with the minimum model efficiency among this group of peer reviewed publications being 8 02 the low nse values for eukaryotic biomass was unsurprising in this context but also could be explained by the eukaryotic phytoplankton group including a range of diverse species with disparate growth strategies and characteristics the simulations produced mostly low model efficiencies for cyanobacteria biomass and chlorophyll a but low model efficiencies do not necessarily indicate poor model efficacy as our analysis did not aim to optimize ce qual icm rather these results highlight how parameter values of phytoplankton growth and loss equations can largely influence outputs of interest i e cyanobacteria biomass and chlorophyll a the nse values for simulated chlorophyll a were higher than those for cyanobacteria biomass which was expected based on trends reported in the literature rigosi et al 2010 shimoda and arhonditsis 2016 the discrepancy between cyanobacteria and chlorophyll a nse values for the lake george ce qual icm model despite cyanobacteria accounting for nearly 90 of the phytoplankton biomass in this system during the study period highlights a disconnect in the way that pfg models simulate phytoplankton i e as dynamic and interactive functional groups versus how they are often calibrated and tested i e for chlorophyll a in ce qual icm and other commonly used pfg models chlorophyll a is calculated as a product of simulated phytoplankton functional group carbon and a user specified chlorophyll to carbon ratio therefore pfg biomass prediction may suffer when chlorophyll a calibration is prioritized yet the disparity between nse values for chlorophyll a and cyanobacteria biomass in the case presented here suggests that the way chlorophyll to carbon ratio parameters are assigned may obscure whether a pfg model is skillfully simulating underlying pfg dynamics at least for some systems further the range of parameter values associated with simulations corresponding to positive nse values for chlorophyll a n 27 and cyanobacteria biomass n 17 are different for several of the parameters considered fig 3 which underlines how ce qual icm may be calibrated differently based on whether the modeling and management objective targets chlorophyll a or cyanobacteria for example the median chlcmnc values associated with simulations for which nsechla 0 were at the bottom end of the range considered here whereas the median chlcmnc values associated with simulations for which nsecyano 0 were closer to the midpoint of the considered range the potential for chlorophyll to carbon ratios to mask pfg dynamics is important given that the gsa results presented for the lake george model point to the chlorophyll to carbon ratios as major factors to which the model s chlorophyll a outputs are most sensitive published values for chlcmnc vary widely table 1 and can fluctuate temporally as a function of changes in environmental conditions e g light and nutrient levels and the state of the phytoplankton community e g species composition cell age structure jakobsen and markager 2016 reynolds 2006 richardson et al 1983 song et al 2016 for example major differences in growth light levels can result in large differences in chlorophyll a concentrations per cell volume sometimes up to 5 to 10 fold richardson et al 1983 yang et al 2015 therefore the use of temporally invariant chlcmn values is likely not reflective of reality and can be a source of uncertainty to modeled chlorophyll a ce qual icm is capable of dynamically simulating pfg chlorophyll to carbon ratios based on the strong sensitivity of model outputs to chlcmnc in this analysis and well known temporal variability of this ratio incorporating dynamic simulation of chlorophyll to carbon ratios is warranted alternatively instead of increasing the structural complexity of the model a potentially more parsimonious approach to pfg model calibration and testing would be to shift focus away from chlorophyll a and towards cyanobacteria biomass or other system relevant pfgs historically algal bloom modeling efforts have focused on chlorophyll a due to the widespread availability of chlorophyll a observations however considering how a greater proportion of the parameter space fit chlorophyll a more effectively than cyanobacteria biomass in the lake george model findings from this study highlight the need for modelers to consider using data that more closely approximates cyanobacteria biomass currently microscopic methodologies which generate biovolume values and species identifications for cyanobacteria remain the most accurate option for measuring cyanobacteria biomass menden deuer and lessard 2000 sacc 2016 smayda 1978 while there is a growing list of long term studies which have monitored for cyanobacteria abundance through microscopy e g davies et al 2016 phlips et al 2015 srifa et al 2016 the time commitment cost and extensive expertise needed are obstacles to widespread application alternatively there have been recent improvements in cost effective methods for quantifying concentrations of phycobiliproteins pigments found in all cyanobacteria both in situ and remotely using satellite imagery mishra et al 2009 data collected through these in situ instruments and remote sensors could potentially be used to calibrate and validate mechanistic pfg models however while these methods are a significant step closer to defining cyanobacteria biomass than chlorophyll a they are still subject to variability in phycobiliprotein to biomass ratios related to environmental conditions and the state of the phytoplankton community allen and smith 1969 boussiba and richmond 1980 reynolds 2006 szwarc and zieliski 2018 other options include flow cytometry methods some of which include imaging capabilities that can provide information on biovolume and species composition and relatively high rates of water sample processing the downsides of the latter option include high instrumentation costs purchase and maintenance and the need for high level operational expertise current flow imaging technologies are also not ideal for quantifying very small size classes of phytoplankton i e pico and small nano plankton 5 m which can be a major component of phytoplankton community biomass in some ecosystems still all of these methods will help facilitate the broad adoption of the proposed calibration and testing strategy based on pfg biomass rather than chlorophyll a a need underlined by this study further as the u s environmental protection agency recently recommended recreational ambient water quality criteria for two cyanotoxins microcystins and cylindrospermopsin epa 2019 it is likely that future modeling efforts will require consideration of not only pfgs but also the toxins they produce thus adopting a pfg centric approach to modeling and testing models would help address uncertainty associated with chlorophyll to carbon ratios better align model testing with model structure i e validating pfg models based on pfg dynamics not proxy chlorophyll a dynamics and allow for further model development with regards to simulation of cyanobacteria toxin production importantly the mechanisms controlling toxin production are complex o neil et al 2012 and further research is needed to identify how pfg models may be used to parsimoniously simulate toxin production additionally the morris plots demonstrated relatively high sensitivity of the lake george model to parameters related to phosphorus uptake of the phosphorus uptake parameters results indicate that q0 minimum cell quota for phosphorus and vmax maximum p uptake rate are the most influential cerco et al 2004 incorporated q0 within ce qual icm to account for the ability of bloom forming phytoplankton i e cyanobacteria to assimilate phosphorus in excess of immediate growth needs and concluded that the use of droop kinetics for phosphorus uptake did not appreciably impact chlorophyll a simulations the results presented in this study showed that parameters in the droop equation were important in driving model outcomes and thus do not support the previous finding of cerco et al 2004 cerco et al 2004 arrived at their conclusion through the use of a one at a time sensitivity analysis and acknowledged that insight into the mechanisms that induce the response of the model to changes in droop parameter values are confounded by the immense number of interacting processes in the complete model therefore the incongruity between their findings and ours may be a product of their having used a local as opposed to global analytical approach further model simulations may also suffer from imperfect understanding of the availability and utilization of varied phosphorus sources by different phytoplankton species carey et al 2012 cottingham et al 2015 previous research on algal blooms in lake george highlight the importance of both phosphorus and nitrogen in describing cyanobacteria dynamics doron 2010 nelson et al 2018 paerl et al 2002 piehler et al 2009 srifa et al 2016 so simulated cyanobacteria concentrations lack of sensitivity to nitrogen uptake parameters was unexpected the apparent lack of sensitivity to nitrogen may be an important sign that further investigation of model formulations especially in freshwater systems where phytoplankton production is more often limited by nitrogen availability than phosphorus as is the case for lake george for example since ce qual icm models nitrogen and phosphorus uptake differently i e monod for nitrogen droop for phosphorus it suggests that the use of droop or monod kinetics may disparately sway model outcomes an interesting subsequent step in this analysis would be to assess how the model may perform differently depending on whether nitrogen phosphorus and silica uptake equations take the form of droop or monod kinetics at present it is difficult to determine whether the use of droop kinetics for phosphorus uptake appropriately represents this process and its evaluation is worthy of future research additionally the use of alternative nutrient uptake kinetic formulations should also be explored in the context of the equation for algal production equation 2 the production equation used in this study assumed that resource limitation occurred as a function of the most limiting factor i e nutrients light availability so the use of alternative nutrient uptake kinetic formulations could result in consequential changes to how and when nitrogen phosphorus and silica drive model outcomes a broad parameter space was explored in this study using gsa but none of the model runs produced simulations that effectively captured the magnitude of blooms or reached the nse value nsechla 0 5 that was achieved under the calibration of the model s original application though the intention of this study was not to optimize the model to maximize nse the range of nse values produced from this analysis raises two questions 1 to what extent will changes to the parameterization of pfg models elevate their performance and 2 do current values reported in the literature adequately capture real ecological conditions changes in model algorithm structure particularly those that promote greater model parsimony e g han et al 2019 malve et al 2007 rigosi et al 2014 shimoda et al 2016 may be needed to improve pfg simulation skill future research efforts should evaluate alternative equation formulations and explore new ways of quantifying phytoplankton growth kinetics additionally research is also needed to constrain parameter value ranges robson et al 2018 recently produced an online tool that allows users to interact with collated parameter values contributed by pfg modeling researchers which is a key first step towards developing a more robust library of pfg model parameters the continued development of this tool and library will further facilitate the refining of parameter values and incorporation of uncertainty in pfg models e g through parsimonious bayesian models robson et al 2018 5 conclusions in the present study we identified pfg model parameters that drive uncertainty in modeled chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass in a large shallow subtropical flow through lake and identified opportunities for pfg model improvement in particular the considered model outputs were sensitive to chlorophyll to carbon ratios p uptake parameters basal phytoplankton metabolism rates and phytoplankton production under optimal growth conditions among these factors simulated chlorophyll a was most sensitive to the cyanobacteria chlorophyll to carbon ratio though these findings were specific to the model and study site analyzed herein the revealed sensitivities highlight broad challenges associated with pfg model structures that could be addressed through changes in common calibration and validation practices in particular to reduce reliance on chlorophyll to carbon ratios which are subject to high variability pfg modelers may benefit from moving away from calibrating and testing based on chlorophyll a and instead target pfg biomass directly this transition is becoming more feasible with continually advancing and cost effective approaches for monitoring pfg biomass additionally our results point to a need for further research on the validity of using different equations e g droop monod to capture pfg specific nutrient uptake kinetics we screened all ce qual icm parameters included in pfg growth and loss functions as our focus was specifically on how pfg specific parameters influence simulated chlorophyll a and pfg biomass importantly ce qual icm also includes dozens of other parameters related to biogeochemical cycling that were not evaluated here due to computational limitations associated with varying all parameters in the model thus our results should be interpreted as only capturing the influence of pfg growth and loss parameters on ce qual icm s output uncertainty other parameters in the model such as those associated with sediment diagenesis processes may play equal or greater roles in driving output uncertainty than those considered here the challenges associated with comprehensively evaluating how all ce qual icm parameters contribute to uncertainty in simulated chlorophyll and pfg biomass underscores the need for parsimonious models that are more transparent and readily testable pfg model outputs inform a wide range of management decisions that have far reaching impacts for example pfg models are commonly used to determine specific water quality thresholds e g total maximum daily loads in the united states that are codified and result in modifications to permitting protocols development of incentives for adoption of best management practices and implementation of aquatic restoration projects given the importance of pfg model outcomes minimizing uncertainty in their outputs is integral to effective decision making in water management declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements and data the authors report no conflicts of interest and thank peter sucsy and john hendrickson of the st johns river water management district for providing access to information ngn is supported by the usda national institute of food and agriculture hatch project 1016068 and rmc by hatch project fla abe 005556 additionally this material is based upon work supported by the national science foundation graduate research fellowship under grant no dge 0802270 usda nifa hatch project 1011481 and st john river water management district contract no 28650 the authors acknowledge university of florida research computing for providing computational resources and support that have contributed to the research results reported in this publication water quality data analyzed here are publicly available through the st johns river water management district appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104708 
26016,mechanistic phytoplankton functional group pfg models are used to develop water quality targets designed to mitigate cyanobacteria blooms but it remains unclear whether pfg models adequately simulate cyanobacteria dynamics as most are evaluated against observations of chlorophyll a instead of pfg biomass to address this challenge we analyzed an application of ce qual icm a 3d mechanistic pfg model used by water managers and modelers global sensitivity analysis was employed to assess the sensitivity of modeled chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass to 42 uncertain input factors in ce qual icm s pfg growth and loss functions results revealed that parameterization of ce qual icm captured bloom variation but underpredicted bloom peaks and simulated chlorophyll a with greater skill than pfg biomass additionally when run across realistic ranges of pfg parameter values model outputs were highly sensitive to chlorophyll to carbon ratios and phosphorus uptake parameters indicating that these factors should be the focus of targeted parameterization efforts 1 introduction research organizations and government agencies conduct long term water quality sampling to monitor threats posed by phytoplankton blooms which are intensifying in magnitude frequency and duration in many cases because of the impacts of human activities such as cultural eutrophication and climate change carey et al 2012 glibert et al 2005 o neil et al 2012 paerl et al 2011 chlorophyll a is classically measured as a proxy of phytoplankton biomass within these monitoring programs and used as a response variable in models designed to explain and predict blooms and their secondary effects e g changes in dissolved oxygen however chlorophyll a is an aggregate measure and its dynamics may not fully reflect shifts in biomass of key functional groups e g n longphuirt et al 2019 additionally changes in phytoplankton community composition and eco physiological responses to changing environmental conditions can alter the relationship between chlorophyll a and biomass jakobsen and markager 2016 to capture phytoplankton community dynamics underlying changes in chlorophyll a scientists and engineers have developed models that simulate phytoplankton functional groups pfgs which are groups of phytoplankton with shared characteristics such as morphological and eco physiological traits kruk et al 2002 c s reynolds et al 2002 in theory these models quantify phytoplankton growth kinetics and temporal variations in the ecological processes underpinning phytoplankton community structure thereby representing pelagic productivity dynamics more realistically as compared to models that solely simulate chlorophyll a in practice pfg models can be difficult to parameterize and validate because the processes they simulate are contextualized by significant ecological and biological complexity arhonditsis and brett 2004 rigosi et al 2010 robson 2014 shimoda and arhonditsis 2016 consequently pfg parameterization often occurs subjectively or through calibration frede thingstad et al 2010 these complexities result in pfg model algorithms and their parameters being largely uncertain many pfg model parameters such as rates associated with growth kinetics are not extensively measured or documented in existing literature anderson 2005 franks 2009 shimoda and arhonditsis 2016 although uncertainties propagate through direct and interactive effects to model outputs it remains difficult to assess the degree to which parameter uncertainties impact pfg model simulation skill in part because there are relatively few published pfg modeling studies that include quantified performance metrics a recent review of pfg modeling analyses by shimoda and arhonditsis 2016 revealed that only around 30 of published studies applying pfg models within freshwater systems reported pfg goodness of fit measures the lack of information on pfg performance may partly result from limitations in the availability of detailed and long term observational pfg data shimoda and arhonditsis 2016 among the pfg modeling studies that have presented observations to support simulated outputs the performances are often poor for example shimoda and arhonditsis 2016 discovered that published pfg modeling studies in which cyanobacteria was included as a state variable n 68 simulatedcyanobacteria dynamics with a median model efficiency of 0 06 calculated as 1 o p 2 o o 2 where o observations p predictions and o the mean of observations a level of performance not ideal for supporting management and policymaking ritter and muoz carpena 2013 increasing demands on water managers to mitigate the effects of harmful algal blooms such as those composed of toxic cyanobacteria will be met by a growing need for pfg models that can support managers decision making thus it is important that efforts be made to improve pfg model structure and performance by taking a critical look at the processes and parameters contributing uncertainty to simulated pfg dynamics one approach that allows for systematic assessment of pfg model structure and performance is global sensitivity analysis gsa as with any type of sensitivity analysis gsa is used to quantify the degree to which model outputs are sensitive to changes in input factors e g parameter values boundary conditions algorithm structures etc saltelli et al 2008 gsa contrasts with other sensitivity analysis approaches due to its capacity to assess nonlinear and interactive effects in high dimensional models saltelli et al 2004 2008 results produced from applications of gsa reveal the relative importance of model input factors model input factors that are found to dictate model performance should be addressed in order to ensure they are being assigned the most accurate values conversely factors identified as unimportant could potentially be removed from the model thereby helping to reduce model complexity and uncertainty jakeman et al 2006 muoz carpena et al 2007 saltelli et al 2004 2008 in addition gsa can help identify processes involving important factors that could be targeted for model improvement thus by methodically informing ways to improve upon the algorithmic structures of pfg models gsa can markedly increase their utility the overall goal of this study was to address the need for further quantitative scrutiny of state of the art pfg models our primary objective was to quantify the global sensitivity of pfg model outputs to parameters related to phytoplankton growth and loss processes to meet this objective we applied gsa to the corps of engineers integrated compartment water quality model ce qual icm a highly specified and state of the art 3d mechanistic water quality and pfg model that couples with the environmental fluid dynamics code efdc a 3d mechanistic hydrodynamic model ce qual icm is supported by the united states environmental protection agency and used to inform the development of water quality targets for regulatory purposes cerco and cole 1995 specifically the gsa was used to assess ce qual icm s sensitivity to a large number of input factors when simulating chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass over an 8 year period in a flow through lake located in florida usa the availability of detailed and long term phytoplankton monitoring data allowed for model goodness of fit to be quantified for all outputs and for application of gsa parameters were varied across realistic value ranges supported by a survey of the literature thus making the findings regarding model sensitivity of broad interest to our knowledge this is the first application of gsa to a highly complex pfg model 2 methods 2 1 ce qual icm an application of ce qual icm a 3d mechanistic water quality and pfg model was used for this study ce qual icm was developed by the u s army corps of engineers cerco and cole 1995 and is described by the u s environmental protection agency epa as a model that can support nutrient target development epa 2001 ce qual icm couples with the environmental fluid dynamics code efdc a 3d process based hydrodynamic and transport model capable of simulating complex physical processes pertinent to aquatic system dynamics hamrick 1992 the epa specifically outlines ce qual icm as a model suited for dynamic simulation of estuarine and coastal marine water quality but it can also be parameterized for freshwater applications ce qual icm was most notably applied to the chesapeake bay during the development of total maximum daily load standards cerco and noel 2013 ce qual icm can simulate the carbon biomass of up to three pfgs as well as many other water quality factors such as chlorophyll a dissolved oxygen nitrogen phosphorus and silica pfg growth dynamics are governed by equation 1 1 b t p r p r w z b where b phytoplankton biomass mg c l 1 t time day p production day 1 r respiration day 1 pr predation day 1 w settling velocity m day 1 and z depth m production is calculated as the product of an optimal production rate pm and the effects of inorganic nitrogen f n phosphate f p silica f s illumination f i and temperature f t production is mediated by f t and f i as well as the minimum of f n f p f s equation 2 2 p p m f t f i min f n f p f s the effects of nitrogen and silica limitation are modeled by monod functions whereas the effect of phosphorus limitation is modeled with droop kinetics cerco et al 2004 f i is calculated as a productivity intensity curve jassby and platt 1976 using an optimized light extinction model developed for our study system gallegos 2005 f t is an approximately gaussian probability curve that peaks at a user specified optimal temperature salinity toxicity is also considered within the governing growth equation through an empirical function the rate of basal respiration is increased when salinity reaches toxic levels the predation function can be specified to assume that the rate of predation is either linearly or quadratically related to phytoplankton biomass both predation and respiration are corrected for the effects of temperature in total ce qual icm s pfg growth and loss equations include 20 22 parameters and these equations are parameterized for each pfg included in the model e g to dynamically simulate two pfgs 40 42 parameters need to be specified details on these and other ce qual icm equations are outlined in cerco and cole 1995 and sucsy and hendrickson 2003 2 2 global sensitivity analysis elementary effects morris method the morris method morris 1991 is one of the most commonly used gsa approaches this method measures sensitivity through two metrics the 1 mean  and 2 standard deviation  of each model input factor s elementary effects elementary effects are the resulting model output changes that occur with a change in factor value equation 3 said another way elementary effects equal y x i across a multifactor space where y is the output and x i is an individual model input factor within a i factor space more specifically a factor s elementary effects ee are calculated as 3 e e i y x 1 x 2 x i x i 1 x k y x x 2 x i  x i 1 x k  where y is the model output x is a model input factor k is the total number of model input factors and  is the fixed amount by which the model input factor xi is varied to determine  the pre defined model input factor value ranges are divided into a discrete number of specified levels p researchers have commonly used four levels p 4 e g campolongo et al 2007 khare et al 2018 model input factor ranges are discretized into p levels to calculate each model input factor s  equation 4 4  0 1 p 1 2 p 1 1 after discretizing the factors value ranges into levels the resulting domain of model parameter factor values is sampled and these samples are used to parameterize model runs several sampling strategies exist the sampling strategy first proposed by morris involves randomly throwing trajectories across the discretized hyperspace the starting points for each trajectory are random but the trajectory itself will move one at a time across the hyperspace to take into account the  changes in different model input factors each trajectory includes k 1 points selection of the total number of trajectories r is subjective but r 10 is generally considered to produce satisfactory results muoz carpena et al 2007 the number of model runs n required by this sampling strategy is equation 5 5 n r k 1 trajectory based sampling strategies other than those of morris 1991 exist and include the methods of optimized trajectories campolongo et al 2007 modified optimized trajectories ruano et al 2012 and sampling for uniformity khare et al 2013 these methodologies offer a variety of benefits such as enhanced sample generation runtime efficiencies that prove useful for running gsa with high dimensional models in particular the enhanced sampling for uniformity esu strategy chitale et al 2017 khare et al 2015 which ensures sampling uniformity maximizes trajectory spread and computational efficiency is used herein once the model is run n times the corresponding outputs are collated to calculate the elementary effects resulting from varying model input factors the mean  and standard deviation  of a factor s elementary effects reflect the degree to which the factor influences the output and interacts with other factors respectively campolongo et al 2007 found that the absolute values of a factor s mean elementary effects  better represent the sensitivity of the factor by avoiding the possibility of positive and negative elementary effects cancelling each other out during the calculation of the  when  is used the approach is described as the modified morris method each factor s sensitivity metrics   and   are plotted on a cartesian plane to evaluate the relative influence of each model input factor on the output a high value of  suggests a large effect on the output a high value of  suggests that a factor has high order interactions with other model input factors and or behaves nonlinearly campolongo et al 2007 morris 1991 according to the positions of the input factors on the   and   cartesian planes model input factors can be generally classified as linear additive mixed and nonlinear non additive nonlinear non additive factors are denoted by a position above the 45 line in the   plot and in between the  2sem sem standard error of the mean reference lines in the   plot notably these measures provide information on the qualitative importance ranking of model input factors the comparison of   and   cartesian plots for a given output allows for the identification of directionality sign of  and non monotonicity   of the input factor effects 2 3 study site lake george florida usa phytoplankton dynamics in lake george a flow through lake of the st johns river florida usa fig 1 were examined in this study due to the regular occurrence of nuisance algal blooms the phytoplankton community in lake george has been the focus of targeted monitoring efforts from 2002 to 2010 which resulted in the curation of an unusually rich dataset composed of monthly phytoplankton species counts srifa et al 2016 and monthly to thrice monthly water chemistry observations access to species level phytoplankton data created the opportunity to evaluate how a high dimensional management model represented pfg dynamics as opposed to just chlorophyll a typically due to the absence of dynamic pfg data this level of detail is rarely considered in pfg model evaluations shimoda and arhonditsis 2016 cyanobacteria were by far the most abundant group of phytoplankton in lake george over 87 of total phytoplankton carbon biomass observed in this system from 2002 to 2010 reaching annual peaks in the range of 3 11 mg c l 1 fig 2 in contrast eukaryotes only reached annual peaks of about 0 6 mg c l 1 oscillatoria spp dominated the cyanobacteria population in lake george during the study period 47 of total cyanobacteria carbon biomass followed by aphanizomenon spp cylindrospermopsis spp planktolyngbya spp and dolichospernum spp 15 14 10 and 6 of total cyanobacteria carbon biomass respectively by carbon biomass the eukaryotic community is largely composed of diatoms followed by chlorophytes and other eukaryotes 2 4 simulating lake george pfgs with ce qual icm ce qual icm and efdc were previously calibrated and applied by the st johns water management district to support the establishment of total maximum daily load standards for lake george magley 2018 and the lower st johns river sucsy and hendrickson 2003 for this study outputs from calibrated efdc simulations were used as inputs for ce qual icm the simulation period covered january 1 2002 through december 30 2010 this is the same time period that the model was originally calibrated to simulate outputs were tabulated daily for time 12 00 p m this time point was chosen due to the observed data having been manually collected in the time frame of approximately 9 00 a m to 3 00 p m as part of the st john river water management district s water quality monitoring program in situ water samples were taken with integrated pole samplers so the outputs were also vertically averaged to align with sampling protocols only outputs from the location corresponding to monitoring site lg1 2 fig 1 were evaluated due to this site also being the point of data collection within the water quality monitoring program for this study ce qual icm was parameterized to simulate two pfgs pelagic cyanobacteria and eukaryotic phytoplankton cyanobacteria and eukaryotic phytoplankton were divided into separate pfgs for this study given the differences in their ecophysiology i e nutrient uptake kinetics palatability and priority from a management perspective i e cyanobacteria are the focus of ongoing management efforts in lake george further division of these pfgs was considered impractical given the paucity of parameter values available for narrower groupings e g individual genera cyanobacteria dominate the lake george phytoplankton community nelson et al 2018 srifa et al 2016 and include a diversity of genera including nitrogen fixers e g aphanizomenon spp cylindrospermopsis spp dolichospernum spp and non fixers e g oscillatoria spp considering the importance of nitrogen fixers in the lake george cyanobacteria community specifically as related to nitrogen uptake kinetics the proportion of nitrogen fixers in the cyanobacteria pfg was used to mediate the nitrogen uptake kinetic equation f n nitrogen limitation was assumed to occur as a function of the proportion of nitrogen fixers in the cyanobacteria community equation 6 6 f n 1 f n f i x d i n k h n c d i n f n f i x d i n k h n f i x k h n c d i n k h n f i x in equation 6 fnfix observed fraction of nitrogen fixers in the cyanobacteria community din dissolved inorganic nitrogen concentration khnc half saturation constant for nitrogen uptake for cyanobacteria and khnfix dissolved inorganic nitrogen concentration threshold fnfix was varied temporally in the model based on observed phytoplankton community composition fig s1 supporting information this contrasts with the original model formulation in which nitrogen fixers were assumed to occupy a constant proportion of total cyanobacteria biomass the inclusion of khnfix is used to increase the value of f n when nitrogen fixers are present in the cyanobacteria community in turn diminishing the likelihood of nitrogen limitation in the production equation equation 2 additionally rates of predation were assumed to be linear for cyanobacteria and quadratic for eukaryotes due to their differences in palatability wilson et al 2006 further details on equations in the lake george application of ce qual icm are provided in sucsy and hendrickson 2003 2 5 application of gsa to the lake george ce qual icm 1 define model input factor hyperspace gsa requires the construction of a parameter space or hyperspace that includes all possible model parameterizations this hyperspace is defined by assigning probability density functions pdfs to each of the model inputs saltelli et al 2004 factor pdfs represent the variability or uncertainty observed in real world settings given that parameter sensitivity is calculated as a function of output variance relative to the corresponding input parameter values variances equation 3 sensitivity is related to the magnitude of individual parameters variabilities and the results of a gsa are a property of the application and not the model itself the extent to which model input pdfs are symmetrical or skewed may impact gsa results but the means value ranges and variances have a greater impact on output variability than the type of pdfs haan et al 1995 wallach et al 2014 thus a uniform pdf can be used as a conservative approximation of an input factor s range of variation for this study parameters associated with ce qual icm s algal growth and loss processes k 42 were sampled for the gsa each parameter was assigned a uniform pdf to impose the least restrictive assumptions on the analysis the uniform distributions were bound by published value ranges when available parameters for which published values were not available were varied by 20 from their calibrated values table 1 when evaluating the considered ranges it is important to acknowledge that many of the analyzed factors are essentially effective parameters in the context of the model application due to constraints associated with domain discretization and mathematical characterization of ecological processes beven 2000 and some have never been measured in a lab or field setting anderson 2005 franks 2009 shimoda and arhonditsis 2016 2 generate samples samples were generated using the enhanced sampling for uniformity strategy which maximizes trajectory spread and computational efficiency chitale et al 2017 khare et al 2015 factors were discretized at 4 levels p 4 and samples were generated using 12 trajectories r 12 thus 12 42 1 516 samples were generated n equation 5 sample generation was performed with matlab the mathworks inc 2015 3 run ce qual icm ce qual icm was run 516 times at the university of florida high performance computing hipergator facility each model run required approximately 5 h to complete amounting to around 107 days of computational time for all 516 simulations simulated time points that coincided with those of the observed dataset n 178 for chlorophyll a n 84 for cyanobacteria and eukaryote biomass were tabulated 4 summarize model outputs for each run chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass were summed across the study period to generate total values e g total chlorophyll a simulated from 2002 to 2010 total cyanobacteria biomass simulated from 2002 to 2010 etc these summations referred to hereafter as total simulated values serve as an output metric by which to evaluate how parameter variability translates into changes in model output quantities 5 quantify goodness of fit for each of the runs n 516 ce qual icm s performance was evaluated by quantifying the model s ability to simulate cyanobacteria biomass eukaryote biomass and chlorophyll a model performance was quantified with the nash sutcliffe efficiency nse coefficient nash and sutcliffe 1970 7 n s e 1 t 1 n o t p t 2 t 1 n o t o 2 where t time point n total number of time points ot observation at t mean of observed data and pt predicted data at t when nse equals 1 the predicted data perfectly match the observations when nse equals 0 the mean of the observations serves as an equally effective predictor as the model when nse is less than 0 the mean of the observations is a more effective predictor than the model meaning that the model is performing poorly 6 calculate elementary effects elementary effects were quantified in matlab using the ee sensitivity measures package khare et al 2015 and corresponding visualizations were produced in r r core team 2019 elementary effects were calculated in relation to i total simulated values and ii nse for chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass recent research challenges the use of performance based metrics e g nse to assess model sensitivity as performance based metrics measure the degree to which a model s outputs converge to observed data and not necessarily how well the model represents the dynamic nature of a system gupta and razavi 2018 here we consider both performance based metrics and the magnitude of the system response i e total chlorophyll a total cyanobacteria biomass and total eukaryotic phytoplankton biomass to permit for assessment of the importance of model parameters in driving simulated system dynamics as well as understand the sensitivity of the calibration space 3 results 3 1 model efficiency the nse values produced from the 516 simulations for each of the considered outputs were largely negative though this is unsurprising given that the range of parameter values considered may not have been well specified for the studied system despite using realistic ranges of variation from the literature the median nse values for chlorophyll a cyanobacteria biomass and eukaryote biomass were 3 0 0 53 and 8 26 respectively the maximum nse values respectively were 0 36 0 34 and 0 88 simulated outputs associated with the maximum nse values are presented in fig 2 visualizations of simulated outputs reveal that the model generally predicted the annual pattern though tended to underestimate blooms peaks as quantified by chlorophyll a parameter ranges corresponding to simulations for which the nse values were positive for simulated chlorophyll a n 27 and cyanobacteria n 17 are visualized in fig 3 the model was also run in its initial calibrated form in order to quantify the baseline performance of the ce qual icm model for lake george nse values associated with this baseline run were 0 50 0 34 and 10 47 for chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass respectively the model was originally calibrated to optimize prediction of a chlorophyll a target fraction of days per year when chlorophyll a exceeds 40 g l and the discrepancy in performance between the baseline simulation nsechla 0 50 and best simulation from the gsa nsechla 0 36 for chlorophyll a is likely explained by our having used literature defined values instead of calibrated values for many of the model parameters when running the gsa the morris method is used to explore model sensitivities not to perform an exhaustive search designed to optimize outputs against an objective function therefore the combinations of parameter values considered here allowed for exploration of the parameter space but without seeking to identify an ideal calibration parameter set 3 2 output sensitivity the   and   plots hereafter referred to as morris plots illustrate the sensitivity of total simulated chlorophyll a cyanobacteria carbon biomass and eukaryotic phytoplankton carbon biomass fig 4 and nse fig 5 to changes in the growth kinetic parameters for a tabulated list of   and  values for each of the considered outputs see table s1 supporting information in the presented morris plots figs 4 and 5 points vary in color by the parameter type and in shape by phytoplankton classification the general phytoplankton classification is used to describe parameters that are not specific to a phytoplankton functional group the parameters to which the outputs were most sensitive are labeled label definitions are included in table 1 dashed lines in the plots indicate thresholds for predominance of higher order effects e g parameters that fell above the 45 line in the   plots affected the output through interactive and higher order effects as did parameters that fell inside the 2sem v in the   plots in contrast parameters that fell below the 45 line in the   plots and outside of the 2sem v in the   plots impacted the output largely through direct effects chlorophyll a nse fig 5a and total simulated chlorophyll fig 4a were most sensitive to parameters related to cyanobacteria growth dynamics particularly the chlorophyll to carbon ratio of cyanobacteria chlcmnc after chlcmnc the basal metabolism rate of cyanobacteria bmrc and rate of cyanobacteria production under optimal growth conditions pmc were the most important parameters followed by p uptake parameters for cyanobacteria and eukaryotes minimum cyanobacteria cell quota for p q0c maximum p uptake for cyanobacteria vmaxc and eukaryotes vmaxe and the half saturation constant for eukaryote p uptake khpe and the temperature at which optimal cyanobacteria growth occurs tmpc the effects of the uncertainty of these parameters on total simulated chlorophyll a and chlorophyll a nse were largely nonlinear and interactive as demonstrated by their positions above the 1 to 1 line in the   plots and within the 2sem lines in the   plots the nonlinear and interactive effects of these parameters on nse and total simulated chlorophyll a is generally to be expected considering the complexity of ce qual icm muller et al 2011 the morris plots revealed that total simulated cyanobacteria biomass nse fig 5b was most sensitive to parameters related to cyanobacteria p uptake specifically the minimum cell quota for p q0c and vmaxc in addition to these two parameters cyanobacteria biomass nse was sensitive to bmrc total simulated cyanobacteria biomass was also sensitive to these factors fig 4b as well as chlcmnc half saturation constant for cyanobacteria p uptake khpc and p uptake parameters for eukaryotic phytoplankton vmaxe khpe parameters to which cyanobacteria nse was most sensitive propagated through the model primarily via interactive effects parameter uncertainty also influenced total simulated cyanobacteria through interactive effects but unlike cyanobacteria nse several of the parameters that total simulated cyanobacteria was most sensitive to e g q0c vmaxc bmrc pmc chlcmnc influenced simulated cyanobacteria biomass through direct effects as evidenced by their positions outside of the 2sem lines in the   plots specifically vmaxc pmc and chlcmnc drive monotonic increases in total simulated cyanobacteria via interactive effects whereas q0c bmrc and khpe drive monotonic decreases in total simulated cyanobacteria via interactive effects however uncertainty in q0c and bmrc non monotonically drove increases in cyanobacteria biomass nse eukaryotic phytoplankton biomass nse fig 5c was most sensitive to the eukaryotic phytoplankton predation rate multiplier bpre chlorophyll to carbon ratio for eukaryotes chlcmne rate of eukaryote production under optimal growth conditions pme the eukaryote i e diatom silica to carbon ratio asce and maximum rate of p uptake by eukaryotes vmaxe total simulated eukaryote biomass during the study period was only sensitive to a subset of these factors specifically chlcmne bpre pme and vmaxe fig 4c all of these parameters influenced eukaryote biomass nse and total simulated biomass through a mix of direct and interactive effects in particular chlcmne and pme influenced total eukaryote biomass via nonlinear but positively monotonic effects and eukaryote nse via nonlinear but negatively monotonic effects similarly bpre influenced eukaryote biomass via direct and negatively monotonic effects but it influenced eukaryote nse via direct and positively monotonic effects these relationships suggest that changes in bpre or the eukaryote predation function could dramatically influence the model s ability to represent eukaryotic phytoplankton biomass dynamics 4 discussion to address the need for greater in depth evaluation of mechanistic phytoplankton functional group pfg management models global sensitivity analysis gsa was applied to evaluate the importance of 42 parameters on the outputs of an application of ce qual icm a state of the art process based pfg and biogeochemical model used to inform the development of water quality targets for the management of algal blooms in fresh and estuarine waters notably this study focused on parameters used to specify pfg growth and loss equations but ce qual icm includes many other equations for simulating related processes such as those driving biogeochemical cycling that could also influence pfg biomass the parameters used to specify equations dictating other cycles e g nutrients carbon in the model could be of equal or greater importance in driving pfg model outcomes so our results should be interpreted in the context of this additional source of uncertainty for the 516 simulations needed to evaluate the influence of 42 input factors model efficiency varied and many simulations were associated with very low nash sutcliffe efficiency nse values although the morris method is not applied for optimization purposes understanding the range of nse values associated with ranges of realistic parameter values allows for assessment of the degree to which model performance metrics may vary solely as a function of parameter uncertainty all eukaryotic phytoplankton biomass nse values were negative thus indicating that observed mean biomass of eukaryotic phytoplankton would have served as a more suitable predictor of eukaryotic phytoplankton biomass than ce qual icm however in comparison to simulations from other published studies the model efficiencies for eukaryotic biomass reported herein are common shimoda and arhonditsis 2016 reported that published pfg modeling studies in which aggregated phytoplankton was simulated as a state variable had a median model efficiency of 0 20 with the minimum model efficiency among this group of peer reviewed publications being 8 02 the low nse values for eukaryotic biomass was unsurprising in this context but also could be explained by the eukaryotic phytoplankton group including a range of diverse species with disparate growth strategies and characteristics the simulations produced mostly low model efficiencies for cyanobacteria biomass and chlorophyll a but low model efficiencies do not necessarily indicate poor model efficacy as our analysis did not aim to optimize ce qual icm rather these results highlight how parameter values of phytoplankton growth and loss equations can largely influence outputs of interest i e cyanobacteria biomass and chlorophyll a the nse values for simulated chlorophyll a were higher than those for cyanobacteria biomass which was expected based on trends reported in the literature rigosi et al 2010 shimoda and arhonditsis 2016 the discrepancy between cyanobacteria and chlorophyll a nse values for the lake george ce qual icm model despite cyanobacteria accounting for nearly 90 of the phytoplankton biomass in this system during the study period highlights a disconnect in the way that pfg models simulate phytoplankton i e as dynamic and interactive functional groups versus how they are often calibrated and tested i e for chlorophyll a in ce qual icm and other commonly used pfg models chlorophyll a is calculated as a product of simulated phytoplankton functional group carbon and a user specified chlorophyll to carbon ratio therefore pfg biomass prediction may suffer when chlorophyll a calibration is prioritized yet the disparity between nse values for chlorophyll a and cyanobacteria biomass in the case presented here suggests that the way chlorophyll to carbon ratio parameters are assigned may obscure whether a pfg model is skillfully simulating underlying pfg dynamics at least for some systems further the range of parameter values associated with simulations corresponding to positive nse values for chlorophyll a n 27 and cyanobacteria biomass n 17 are different for several of the parameters considered fig 3 which underlines how ce qual icm may be calibrated differently based on whether the modeling and management objective targets chlorophyll a or cyanobacteria for example the median chlcmnc values associated with simulations for which nsechla 0 were at the bottom end of the range considered here whereas the median chlcmnc values associated with simulations for which nsecyano 0 were closer to the midpoint of the considered range the potential for chlorophyll to carbon ratios to mask pfg dynamics is important given that the gsa results presented for the lake george model point to the chlorophyll to carbon ratios as major factors to which the model s chlorophyll a outputs are most sensitive published values for chlcmnc vary widely table 1 and can fluctuate temporally as a function of changes in environmental conditions e g light and nutrient levels and the state of the phytoplankton community e g species composition cell age structure jakobsen and markager 2016 reynolds 2006 richardson et al 1983 song et al 2016 for example major differences in growth light levels can result in large differences in chlorophyll a concentrations per cell volume sometimes up to 5 to 10 fold richardson et al 1983 yang et al 2015 therefore the use of temporally invariant chlcmn values is likely not reflective of reality and can be a source of uncertainty to modeled chlorophyll a ce qual icm is capable of dynamically simulating pfg chlorophyll to carbon ratios based on the strong sensitivity of model outputs to chlcmnc in this analysis and well known temporal variability of this ratio incorporating dynamic simulation of chlorophyll to carbon ratios is warranted alternatively instead of increasing the structural complexity of the model a potentially more parsimonious approach to pfg model calibration and testing would be to shift focus away from chlorophyll a and towards cyanobacteria biomass or other system relevant pfgs historically algal bloom modeling efforts have focused on chlorophyll a due to the widespread availability of chlorophyll a observations however considering how a greater proportion of the parameter space fit chlorophyll a more effectively than cyanobacteria biomass in the lake george model findings from this study highlight the need for modelers to consider using data that more closely approximates cyanobacteria biomass currently microscopic methodologies which generate biovolume values and species identifications for cyanobacteria remain the most accurate option for measuring cyanobacteria biomass menden deuer and lessard 2000 sacc 2016 smayda 1978 while there is a growing list of long term studies which have monitored for cyanobacteria abundance through microscopy e g davies et al 2016 phlips et al 2015 srifa et al 2016 the time commitment cost and extensive expertise needed are obstacles to widespread application alternatively there have been recent improvements in cost effective methods for quantifying concentrations of phycobiliproteins pigments found in all cyanobacteria both in situ and remotely using satellite imagery mishra et al 2009 data collected through these in situ instruments and remote sensors could potentially be used to calibrate and validate mechanistic pfg models however while these methods are a significant step closer to defining cyanobacteria biomass than chlorophyll a they are still subject to variability in phycobiliprotein to biomass ratios related to environmental conditions and the state of the phytoplankton community allen and smith 1969 boussiba and richmond 1980 reynolds 2006 szwarc and zieliski 2018 other options include flow cytometry methods some of which include imaging capabilities that can provide information on biovolume and species composition and relatively high rates of water sample processing the downsides of the latter option include high instrumentation costs purchase and maintenance and the need for high level operational expertise current flow imaging technologies are also not ideal for quantifying very small size classes of phytoplankton i e pico and small nano plankton 5 m which can be a major component of phytoplankton community biomass in some ecosystems still all of these methods will help facilitate the broad adoption of the proposed calibration and testing strategy based on pfg biomass rather than chlorophyll a a need underlined by this study further as the u s environmental protection agency recently recommended recreational ambient water quality criteria for two cyanotoxins microcystins and cylindrospermopsin epa 2019 it is likely that future modeling efforts will require consideration of not only pfgs but also the toxins they produce thus adopting a pfg centric approach to modeling and testing models would help address uncertainty associated with chlorophyll to carbon ratios better align model testing with model structure i e validating pfg models based on pfg dynamics not proxy chlorophyll a dynamics and allow for further model development with regards to simulation of cyanobacteria toxin production importantly the mechanisms controlling toxin production are complex o neil et al 2012 and further research is needed to identify how pfg models may be used to parsimoniously simulate toxin production additionally the morris plots demonstrated relatively high sensitivity of the lake george model to parameters related to phosphorus uptake of the phosphorus uptake parameters results indicate that q0 minimum cell quota for phosphorus and vmax maximum p uptake rate are the most influential cerco et al 2004 incorporated q0 within ce qual icm to account for the ability of bloom forming phytoplankton i e cyanobacteria to assimilate phosphorus in excess of immediate growth needs and concluded that the use of droop kinetics for phosphorus uptake did not appreciably impact chlorophyll a simulations the results presented in this study showed that parameters in the droop equation were important in driving model outcomes and thus do not support the previous finding of cerco et al 2004 cerco et al 2004 arrived at their conclusion through the use of a one at a time sensitivity analysis and acknowledged that insight into the mechanisms that induce the response of the model to changes in droop parameter values are confounded by the immense number of interacting processes in the complete model therefore the incongruity between their findings and ours may be a product of their having used a local as opposed to global analytical approach further model simulations may also suffer from imperfect understanding of the availability and utilization of varied phosphorus sources by different phytoplankton species carey et al 2012 cottingham et al 2015 previous research on algal blooms in lake george highlight the importance of both phosphorus and nitrogen in describing cyanobacteria dynamics doron 2010 nelson et al 2018 paerl et al 2002 piehler et al 2009 srifa et al 2016 so simulated cyanobacteria concentrations lack of sensitivity to nitrogen uptake parameters was unexpected the apparent lack of sensitivity to nitrogen may be an important sign that further investigation of model formulations especially in freshwater systems where phytoplankton production is more often limited by nitrogen availability than phosphorus as is the case for lake george for example since ce qual icm models nitrogen and phosphorus uptake differently i e monod for nitrogen droop for phosphorus it suggests that the use of droop or monod kinetics may disparately sway model outcomes an interesting subsequent step in this analysis would be to assess how the model may perform differently depending on whether nitrogen phosphorus and silica uptake equations take the form of droop or monod kinetics at present it is difficult to determine whether the use of droop kinetics for phosphorus uptake appropriately represents this process and its evaluation is worthy of future research additionally the use of alternative nutrient uptake kinetic formulations should also be explored in the context of the equation for algal production equation 2 the production equation used in this study assumed that resource limitation occurred as a function of the most limiting factor i e nutrients light availability so the use of alternative nutrient uptake kinetic formulations could result in consequential changes to how and when nitrogen phosphorus and silica drive model outcomes a broad parameter space was explored in this study using gsa but none of the model runs produced simulations that effectively captured the magnitude of blooms or reached the nse value nsechla 0 5 that was achieved under the calibration of the model s original application though the intention of this study was not to optimize the model to maximize nse the range of nse values produced from this analysis raises two questions 1 to what extent will changes to the parameterization of pfg models elevate their performance and 2 do current values reported in the literature adequately capture real ecological conditions changes in model algorithm structure particularly those that promote greater model parsimony e g han et al 2019 malve et al 2007 rigosi et al 2014 shimoda et al 2016 may be needed to improve pfg simulation skill future research efforts should evaluate alternative equation formulations and explore new ways of quantifying phytoplankton growth kinetics additionally research is also needed to constrain parameter value ranges robson et al 2018 recently produced an online tool that allows users to interact with collated parameter values contributed by pfg modeling researchers which is a key first step towards developing a more robust library of pfg model parameters the continued development of this tool and library will further facilitate the refining of parameter values and incorporation of uncertainty in pfg models e g through parsimonious bayesian models robson et al 2018 5 conclusions in the present study we identified pfg model parameters that drive uncertainty in modeled chlorophyll a cyanobacteria biomass and eukaryotic phytoplankton biomass in a large shallow subtropical flow through lake and identified opportunities for pfg model improvement in particular the considered model outputs were sensitive to chlorophyll to carbon ratios p uptake parameters basal phytoplankton metabolism rates and phytoplankton production under optimal growth conditions among these factors simulated chlorophyll a was most sensitive to the cyanobacteria chlorophyll to carbon ratio though these findings were specific to the model and study site analyzed herein the revealed sensitivities highlight broad challenges associated with pfg model structures that could be addressed through changes in common calibration and validation practices in particular to reduce reliance on chlorophyll to carbon ratios which are subject to high variability pfg modelers may benefit from moving away from calibrating and testing based on chlorophyll a and instead target pfg biomass directly this transition is becoming more feasible with continually advancing and cost effective approaches for monitoring pfg biomass additionally our results point to a need for further research on the validity of using different equations e g droop monod to capture pfg specific nutrient uptake kinetics we screened all ce qual icm parameters included in pfg growth and loss functions as our focus was specifically on how pfg specific parameters influence simulated chlorophyll a and pfg biomass importantly ce qual icm also includes dozens of other parameters related to biogeochemical cycling that were not evaluated here due to computational limitations associated with varying all parameters in the model thus our results should be interpreted as only capturing the influence of pfg growth and loss parameters on ce qual icm s output uncertainty other parameters in the model such as those associated with sediment diagenesis processes may play equal or greater roles in driving output uncertainty than those considered here the challenges associated with comprehensively evaluating how all ce qual icm parameters contribute to uncertainty in simulated chlorophyll and pfg biomass underscores the need for parsimonious models that are more transparent and readily testable pfg model outputs inform a wide range of management decisions that have far reaching impacts for example pfg models are commonly used to determine specific water quality thresholds e g total maximum daily loads in the united states that are codified and result in modifications to permitting protocols development of incentives for adoption of best management practices and implementation of aquatic restoration projects given the importance of pfg model outcomes minimizing uncertainty in their outputs is integral to effective decision making in water management declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements and data the authors report no conflicts of interest and thank peter sucsy and john hendrickson of the st johns river water management district for providing access to information ngn is supported by the usda national institute of food and agriculture hatch project 1016068 and rmc by hatch project fla abe 005556 additionally this material is based upon work supported by the national science foundation graduate research fellowship under grant no dge 0802270 usda nifa hatch project 1011481 and st john river water management district contract no 28650 the authors acknowledge university of florida research computing for providing computational resources and support that have contributed to the research results reported in this publication water quality data analyzed here are publicly available through the st johns river water management district appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104708 
26017,there is a growing need to address water pollution that demands advanced tools to predict the fate and transport of water quality constituents existing stream solute transport models use simple first order kinetics to evaluate nutrient loss however these ignore biochemical reactions and lack a user friendly interface to address this shortcoming we integrated the one dimensional transport with inflow and storage otis model and the enhanced stream water quality model qual2e and developed an improved interface for a physically based solute transport model with background algal concentration as the only calibration parameter a generalized model was developed and evaluated the new model performed reasonably well in predicting nutrient uptake of newly collected experimental data and published data from 32 other datasets r2 0 76 nse 0 47 and percent bias 4 3 inclusion of biochemical reactions from qual2e improves model confidence and provides options for incorporating actual process based data which is unfeasible in existing first order decay based models keywords stream solute transport nutrient uptake water quality otis qual2e 1 introduction streams and rivers are complex ecohydrological systems under persistent human pressure carpenter et al 1998 johnson et al 1997 societies need clean water for their activities but use waterways to dispose of their waste a costly dilemma requiring expensive engineering solutions solute transport models are frequently used to simulate transport of reactive and non reactive solutes in streams predicting the extent and timing of contaminant spills for example or the export of pollutants during extreme events mueller price et al 2014 ani et al 2009 in these models advection dispersion and chemical and or biological reactions are the basic processes considered another key feature unique to lotic systems is the concept of transient storage which is defined as water that is moving more slowly than water in the channel harvey et al 1996 hall et al 2002 these storage zones can include surface pools hyporheic flow vegetation etc runkel and broshears 1991 transient storage models tsm quantify conservative and reactive solute transport with a one dimensional advection dispersion reaction equation to route water and solutes downstream and capacity coefficients to store water in one or more transient storage zones each characterized by a specific residence time distribution and biogeochemical activity bencala and walters 1983 stream solute workshop 1990 ward et al 2017 harvey et al 1996 the one dimensional transport with inflow and storage model otis runkel 1998 is one of the most commonly used implementations of tsms bencala and walters 1983 sheibley et al 2014 mueller price et al 2016 originally otis was developed for headwater mountain streams but it has been widely used in streams with small to moderate width and depth where one dimensional transport can be assumed fischer et al 1979 in otis conservative solutes move by advection and dispersion in the main channel and exchange with a single transient storage zone while reactive solutes decay according to a first order reaction rate assigned to each domain otis is primarily used in conjunction with field scale tracer experiments where conservative or reactive tracers are injected into a given stream reach to monitor tracer concentrations over time otis parameters are calibrated by fitting observed and simulated tracer concentration data with nutrient transport represented by two calibrated decay parameters therefore two of the primary limitations of using otis to simulate in channel processes are 1 the need for calibration when field data is scarce and 2 the empirical nature of first order decay equations at the watershed scale models often do not include a detailed stream solute transport module owing to the large scale of simulation however studies have suggested the need for refining and improving nutrient transport representation in one of the most widely used watershed models the soil and water assessment tool swat due to the large deviation of modeled values of water quality constituents from the observed values gassman et al 2007 femeena et al 2018 in large watersheds it becomes challenging to use otis where uptake parameters should be calibrated separately for each stream reach another limitation is the use of first order rate kinetics potter et al 2010 o connor et al 2010 and the degree to which nutrient limitation affects nutrient uptake tank et al 2017 the relationship between uptake and limiting nutrient concentration is frequently described by michaelis menten asymptotic equation with many studies emphasizing the link between decreasing biological nitrogen uptake with increasing no3 concentration e g o brien et al 2007 subsequent studies also supported this concept by identifying a partial saturation effect in nutrient uptake mulholland et al 2008 2009 hall et al 2009 however some studies have reported discrepancies like bernot et al 2006 which showed that biological uptake of nitrate was saturated at higher concentrations whereas ammonium and phosphorus uptake continued to increase at higher concentrations nutrient uptake dynamics are complex and change in space and time and it is inappropriate to assume that a single first order uptake parameter can universally simulate nutrient transport hence the need for a comprehensive process based model to simulate nutrient uptake stream water quality models such as the enhanced stream water quality models qual2e qual2k brown and barnwell 1987 chapra et al 2008 and water quality analysis simulation program wasp di toro et al 1983 connolly and winfield 1984 ambrose et al 1988 are examples of process based models that consider biochemical reactions in addition to the advection dispersion equations for transport however these models neglect the strong influence that transient storage can have on pollutant dynamics marsalek et al 2003 and are used as water quality planning and management tools rather than solute transport models an ideal nutrient solute transport model must represent all four key processes advection dispersion transient storage and in stream biogeochemistry to address this gap we combined the advection dispersion transient storage processes used in otis solute transport model and reaction processes used in qual2e water quality model to develop an enhanced solute transport model we calibrated and validated our new model using both experimental data collected for this work and 32 datasets from peer reviewed literature we assessed its performance as a predictive tool and built an improved user interface for running otis and enhanced otis model to provide users with better data visualization and modeling options 2 materials and methods 2 1 otis and qual2e models the proposed solute transport model was created in matlab by combining the algorithms of otis and qual2e the otis model bencala and walters 1983 uses a finite difference approach to calculate solute concentration at different times along the stream length advection dispersion transient storage and decay processes are modeled in otis using equations 1 and 2 below storage zones are immobile and thus do not include advection and dispersion processes the distributed lateral flow component of otis is ignored in this study and therefore we recommend this model for stream reaches where negligible lateral inflow outflow is observed the five major calibration parameters in the otis model include dispersion coefficient d stream cross sectional area a storage zone area as storage exchange coefficient  and first order decay parameters in main channel and storage zone  s 1 c t d 2 c x 2 u c x  c s c  c 2 c s t  a a s c s c  s c s where a stream channel cross sectional area m2 as storage zone cross sectional area m2 c in stream solute concentration mass m3 cs storage zone solute concentration mass m3 d dispersion coefficient m2 s u average flow velocity m s  storage zone exchange coefficient s 1  main channel decay coefficient s 1 s storage zone decay coefficient s 1 the qual2e model brown and barnwell 1987 is a steady state model that simulates up to 15 water quality parameters in branching streams and well mixed lakes it uses a finite difference solution of advective dispersive mass transport and reaction equations to compute steady state water profiles the major water quality parameters simulated in qual2e include dissolved oxygen algae nitrogen as organic n no3 no2 and nh4 and phosphorus as organic and inorganic p algal growth is the key process affecting nutrient transport in qual2e which is further influenced by growth limiting factors such as light nitrogen and phosphorus recent water quality models like wasp di toro et al 1983 have the functionality to model different types of algae such complex modeling considering the natural complexity and the variability of the chl a to biomass ratio was not included in this study owing to the uncertainty induced by an increased number of parameters in the model algal biomass concentration in qual2e is expressed in terms of chlorophyll a concentration using a simple relationship equation 3 major equations affecting nitrate and phosphate uptake by algae in streams are given in equations 4 and 5 see brown and barnwell 1987 for the complete set equations and parameters 3 c h l a  0 a 4 d n o 3 d t  2 n o 2 1 f  1  a 5 d i n o r g p d t  4 o r g p  2 d  2  a where c h l a chlorophyll a concentration g chl a l  0 a conversion factor g chl a mg a a algal concentration mg a l n o 3 concentration of nitrate nitrogen mg n l  2 rate constant for oxidation of nitrite nitrogen day 1 n o 2 concentration of nitrite nitrogen mg n l f fraction of algal nitrogen taken from ammonia pool  1 fraction of algal biomass that is nitrogen mg n mg a  local specific growth rate of algae day 1 inorgp concentration of inorganic or dissolved phosphorus mg p l  4 organic phosphorus decay rate day 1 org p concentration of organic phosphorus mg p l  2 benthos source rate of dissolved phosphorus day 1 d mean stream depth ft and  2 fraction of algal biomass that is phosphorus mg p mg a 2 2 model development for a non reactive tracer the parameters a as d and  will affect solute concentrations in both the main channel and storage zone for a reactive tracer these parameters along with qual2e parameters will affect main channel and storage zone concentrations in this study biochemical reaction equations used in qual2e were incorporated into otis algorithm using matlab platform to develop the enhanced otis model see fig 1 the biochemical reactions are modeled at the same time step as in otis since streams have varying geomorphological conditions and processes in order to model a reactive solute in any given stream it is essential to appropriately calibrate both tsm a as d  and reactive parameters instead of the existing first order decay based approach used in otis we used qual2e equations to estimate the change in solute concentration qual2e includes several types of decay kinetics for each water quality constituent including first order decay and michaelis menten asymptotic decay to minimize calibration complexity the proposed merged model will only use one reaction parameter background algal concentration for calibration to substitute the decay parameters  s of otis the enhanced otis model has the capability to calibrate the storage and reactive parameters using an optimization module when provided with an observed breakthrough curve solute concentration versus time curve model inputs required are the values of all reaction parameters constants stream hydraulic data such as streamflow length cross sectional data and boundary conditions such as background and injection concentrations of algae and other water quality variables this model is a time space dynamic model that uses finite difference approach to estimate concentration of water quality variables at any given time and distance the assumptions considered in otis and qual2e models are also applicable to the proposed model this includes the assumption that solute concentration varies only in longitudinal direction the model is valid for reach lengths where the model parameters are spatially considered constant and therefore for longer reaches multiple model simulations with varying inputs are recommended other improved water quality models like qual2k and wasp were considered but ultimately not selected because of the optimal balance of minimal complexity and improved reach scale process representation in qual2e in addition it is one of the most popular models used in larger watershed scale models for nutrient prediction similar to an approach used by audet et al 2018 we propose a four step model generalization process in this study 1 initial stream specific calibration of transient storage parameters 2 sensitivity analysis to identify sensitive reaction parameters 3 calibration of sensitive reaction parameters using experimental data to obtain a universal reaction parameter set and 4 validation of the selected universal parameter set using literature data a fully functional interface was created using matlab guide gui development environment in which users can input all data related to the tracer tests provide upper and lower bounds of calibration parameters and simulate solute transport for both conservative and reactive tracers fig 2 the user friendly interface is a significant improvement over the existing otis and qual2e models that are based on text files and microsoft excel files the interface is split into two sections for users to be able to model non reactive and reactive tracers separately users may exclusively use the tsm part of the interface to replicate existing otis model simulations but with better data visualization viewing the optimization runs with curve matches and option to plot breakthrough curves concentration distance and concentration time plots and save data files are additional enhancements of the new interface for simulating reactive tracer transport the interface also provides users with the option to select otis based first order decay method or qual2e based reaction method depending on the study duration and conditions although short tracer experiments are expected to be simulated well with existing otis model longer experiments are better simulated with a biochemical reaction based model that accounts for long term algal uptake and nutrient limitation factors 2 3 study area and data the enhanced otis model was calibrated and validated using two different sets of data a field data collected in two separate stream sections in kielstau catchment fohrer and schmalz 2012 schmalz and fohrer 2010 wagner et al 2018 located in northern germany fig 3 and b literature data gathered from 5 published studies for a total of 32 sets of experimental data nutrient uptake was modeled for all the data and compared with measured uptake to validate the model among the different uptake metrics available in this study we use the longitudinal uptake coefficient rate kx which is measured as the decrease of nutrient concentration per unit length fellows et al 2006 brookshire et al 2005 2 4 experimental data the kielstau river in germany is 17 km long and the catchment covers an area of about 50 km2 two instantaneous tracer injections were conducted in october 2017 in two similar order stream reaches towards the outlet of the watershed a a 120 m long reach at soltfeld gauging station and b a 135 m long reach at freienwill fig 3 table 1 the experimental locations were chosen considering stream morphology storage potential and accessibility for tracer injection and monitoring at the time of the experiment streamflow at freienwill was 306 l s and that at soltfeld was 124 l s a conservative tracer sodium chloride and a reactive tracer potassium phosphate were used in the study sodium chloride is a typical conservative tracer used in tracer experiments since the measured chloride in relatively inert and not used for biological uptake it is therefore used as a control tracer in conjunction with n and p tracers to study nutrient uptake for phosphate test a salt solution prepared with 8 kg of nacl 250 g of kh2po4 and 30 l stream water was injected instantaneously at the upstream point of the reach at the downstream point specific conductivity was measured at 5 s intervals using ysi 6600 v2 ysi incorporated usa water quality probe and salt concentrations calculated based on laboratory calibrations phosphate concentrations were measured using water samples collected at the same location in intervals ranging from 30s to 5 min using the phosphate samples 23 data points for soltfeld station and 18 data points for freienwill station were obtained during the time of experiment soltfeld and freienwill reaches had background po4 p concentrations of 0 17 mg l and 0 27 mg l respectively background algal concentration in the streams were approximately 5 95 mg l 2 5 literature data in order to enhance confidence in the developed model additional model validation was done using data assembled from 5 studies that conducted tracer tests in stream reaches ranging from 50 to 2700 m in length table 1 request for data were sent to several authors who conducted tracer tests in different streams around the world among the responses received we only selected studies in which both conservative and reactive tracers were used to obtain calibrated transient storage parameters and longitudinal uptake coefficients two studies demars 2008 tank et al 2008 conducted pulse instantaneous injections and the remaining three schroer 2011 baker et al 2012 burrows et al 2013 conducted continuous constant injections by injecting tracer solution over a period of time nitrate phosphate and ammonium uptake were considered in these studies by injecting salts of these ions along with a conservative tracer altogether literature data provided 32 sets of tracer data for validating our model 2 6 sensitivity analysis qual2e includes 45 biochemical reaction parameters and 17 stream parameters relating to stream characteristics and background solute concentrations that affect reactive solute transport since this study aims for a 1 parameter calibration of the reactive part of the model it was critical to verify that values of all the remaining reaction parameters were acceptable to be used in all streams globally qual2e manual provides a range of values that can be used for each reaction parameter but the typical default values used vary across different models that implement qual2e based solute transport processes we used a manual parameter calibration technique to arrive at a universal set of values for all the reaction parameters using the experimental data at soltfeld and freienwill considering the large number of parameters a one at a time sensitivity analysis was conducted by changing a single parameter value at a time within the typical range and evaluating the change in model results this analysis was used to shortlist the most sensitive parameters that affect nutrient uptake using kielstau experimental data from freienwill station as a test case sensitivity analysis was performed to evaluate the sensitivity of four major parameters affecting n and p uptake ratio of chlorophyll a to algal biomass 0 fraction of algal biomass that is nitrogen n fraction of algal biomass that are phosphorus p and background algal concentration calg in mg l were considered for sensitivity analysis since n has no effect on p uptake in freienwill test case it was ignored in this analysis change in phosphate kx with changing values of 0 p and calg was used to determine the sensitivity of these three parameters the parameter values of 0 n and p are usually provided as a default in the model and their typical range adopted from brown and barnwell 1987 and bowie et al 1985 are reported in table 2 to study the full extent of parameter sensitivity the entire feasible range 0 01 1 for 0 and p was considered for sensitivity analysis background algal concentration on the other hand is generally given as an input to the model and extensively changes depending upon stream conditions therefore we analyzed change in kx over a wide range of algal concentrations 0 500 mg a l which corresponds to 0 5 mg chla l when 0 10 the main objective behind the sensitivity analysis study was to extract the parameters that would go into the model generalization process and to ultimately obtain a universal parameter set highly changing background algal concentrations for different streams is the reason behind using it as a calibration parameter in our newly proposed model and simultaneously finding generalized values for the remaining 3 sensitive parameters 0 n p the major goal behind adapting the qual2e based reaction model is to improve confidence in the model by employing real world scenarios and considering all major stream processes to demonstrate the benefits of using a physically based model a further analysis was conducted to visualize the varying pattern of stream uptake simulated by the proposed model this analysis is expected to show the spatial and temporal differences in stream uptake when using a single value for kx in the main channel as in otis versus a dynamic kx as in our model 2 7 model generalization and validation the primary aim of this part of the study was to check if certain generalizations could be drawn in terms of parameter values in order to move towards a general nutrient transport model for future studies model generalization was done by determining a universal set of values for the three parameters obtained through sensitivity analysis described in section 2 6 this was done using data collected from soltfeld and freienwill stream reaches the enhanced otis model requires calibration of both transient storage model tsm and reaction module for soltfeld and freienwill data the transient storage parameters were automatically calibrated by the model with the help of observed breakthrough curve of conservative tracer best fit parameters were derived by using a malab optimization function fminsearch with objective function to minimize root mean square error rmse between observed and modeled points along the breakthrough curves the otis model fitting procedure converges to a chl a to biomass minimum to ensure that this was indeed the global minimum the optimization was run iteratively with different initial guesses to come to an optimal solution calibration was attained when the rmse values converged to a minimum and the change in rmse between consecutive iterations is less than 0 01 the same calibrated set of storage parameters were used to model the reactive p tracer when simulating phosphate the parameters 0 n and p were manually changed to get the best fit with the observed breakthrough curve the obtained final parameter set was then considered the universal i e generalized reaction parameter values for our new model for the experimental data background algal concentration was a known value from field measurements and hence was not required to be calibrated validation of the universal reaction parameter set was done using the 32 sets of literature tracer test data tsm parameter values for these tests were either taken directly from the papers or determined with the help of published conservative tracer breakthrough curves reactive tracers for the same literature studies were modeled using the universal reaction parameter set obtained from our experimental data simulation in this case background algal concentration had to be calibrated since it was an unknown matching observed and simulated breakthrough curves and longitudinal uptake coefficients would mean that the universal parameter set developed for our experimental streams is applicable for other streams as well when the values of the reaction parameters 0 n p from kielstau study did not give accurate prediction of kx for literature studies other values were tested we assessed model performance by comparing observed and modeled kx values a good fit was assumed if the difference between modeled and observed kx was within 0 0001 m 1 the longitudinal uptake coefficient for pulse injections tank et al 2008 demars 2008 were calculated by plotting the log ratio of reactive and conservative tracer masses over distance from injection point for continuous injections kx was calculated directly by plotting plateau concentrations versus distance the slope of the linear regression line in both cases gives the main channel nutrient uptake per unit length kx which is the inverse of uptake length besides visual interpretation performance indicators such as r2 nash sutcliffe efficiency nse and percent bias were evaluated for better assessment ideally models are considered best performing if they have a value close to 1 for r2 and nse and a value close to zero for percent bias 3 results and discussions the model interface created in matlab was used to simulate tracer tests for both experimental and literature data besides improving the interface scientific reliability of the new model was evaluated in the following sections through model calibration and validation 3 1 experimental tracer test nutrient transport in soltfeld and freienwill stream reaches was modeled using the developed model and calibrated for the tracer injections carried out in these reaches a as d and  values were calibrated for both reaches using conservative tracer data table s 1 and the same parameter values were applied to phosphate breakthrough curves in both streams the model predicted po4 p concentrations with high accuracy fig 4 kolmogorov smirnov test indicated very close match between the curves with the test statistic d maximum deviation of 0 13 for soltfeld and 0 11 for freienwill at 1 significance level null hypothesis was accepted indicating that the difference between modeled and observed curves was not significant in either case comparing individual data points along the breakthrough curves both reaches demonstrated a very high r2 0 99 for soltfeld and 0 95 for freienwill a mass balance approach yielded phosphate kx values in soltfeld and freienwill of 0 00054 m 1 and 0 00029 m 1 corresponding kx values obtained from our model were 0 00076 m 1 and 0 00024 m 1 the ability of our model to simulate dynamic uptake was demonstrated by expressing change in po4 p concentration as a function of solute concentration this would overcome one of the existing limitations with first order solute transport models in which nutrient uptake is modeled at a single solute concentration covino et al 2010 the model was run for freienwill data and uptake was estimated in terms of decrease in po4 p concentration p p is the change in po4 p concentration from the previous time step according to the finite difference approach used in the model eq 1 c t and c s t change with c and c s respectively in other words solute concentration and consequently nutrient uptake varies at every time step in both main channel and storage zone fig 5 showing greater p as po4 p concentration increases this is also in agreement with experimental work done by bernot et al 2006 which reported an increase in phosphorus uptake with higher concentrations a slight lag can be observed between main channel and storage zone uptake curve which is expected in reality where solute entry to storage zones is delayed compared to the main channel fig 5a first order decay approach in otis also simulates uptake as a function of solute concentration c t k c however with the enhanced model c t is not just a function of c and other interactions and factors affecting algal growth and uptake such as light and nutrient limitations are also considered while otis and many other solute transport models represents uptake by two values  or s the enhanced model utilizes a hidden dynamic uptake parameter that gets updated during each time and distance step of the model fig 5b the model also accounts for other existing nutrients within the stream that affects limiting factors of algal uptake which is not considered in the first order decay base method of otis for instance a stream with high background nitrate concentration and limited phosphate concentration may not show any uptake for additional nitrate tracer but will have high phosphate tracer uptake such dynamics are explicitly included in the new model using qual2e equations experimental tracer test data used in this study typically uses one nutrient eg nitrate or phosphate at a time to avoid interactions and to understand the uptake of each under ambient conditions qual2e incorporates all the nutrient reactions in the algal growth model and accounts for their interactions and limitations therefore the collected experimental data may not explicitly represent the limiting nutrient conditions in the stream as modeled by qual2e for steady state studies in streams with minimal solute interactions a single uptake parameter may be sufficient however for real world scenarios where pollutant input may vary over time and distance resulting in shifting nutrient limitations over time the enhanced model provides better and more realistic options to model nutrient transport examples of such scenarios include pollutant loads coming from industries that are mostly injected for certain time of the day or non point source agricultural runoff that gets distributed over a specified reach length even though the studies presented here did not provide opportunity to demonstrate these cases the ability to simulate dynamic uptake is expected to be beneficial for users simulating such scenarios with temporally and spatially varying pollutant inputs 3 2 sensitivity analysis to understand the influence of background algal concentration on sensitivity of other parameters the model was run with two levels of algal concentration the phosphate tracer test at freienwill was used as the test case for sensitivity analysis longitudinal uptake coefficient for p changed from 0 00023 m 1 to 0 00028 m 1 when algal concentration was changed from 0 to 500 mg l fig 6 a at low algal concentration 1 mg l stream kx showed negligible change 0 13 when 0 was increased from 10 to 100 fig 6c similarly within the considered range of p 0 1 1 kx showed only 5 4 increase at low algal concentration at high algal concentration both 0 and p demonstrated high sensitivity with 10 5 and 114 5 change in kx respectively these results show the significance of background algal concentration on phosphate uptake although similar behavior is expected for n when studying n uptake nitrate removal is stream is a much more complicated process owing to other factors like denitrification and algal preference for ammonia kemp and dodds 2002 bernhardt et al 2002 3 3 model generalization and validation the values of 0 n and p can change according to different types of algae phytoplankton periphyton and benthic autotrophs present in the streams but unlike algal concentration these parameters are rarely measured in field in an attempt to avoid extensive calibration and to generalize the values of 0 n and p model generalization was carried out with kielstau experimental data and validated for literature data this ensures that background algal concentration remains as the only parameter required for model parameterization that could be derived through calibration or accurate field measurement for all of the 32 sets of tracer data obtained from literature storage parameters a as d and  were either directly fed to the model in case these were reported in the paper or calibrated using observed conservative tracer breakthrough curves table s 1 using the derived set of storage parameters reactive solutes were modeled for each test data concentrations of algae were calibrated for all test cases to obtain best fit between observed and modeled breakthrough curves and longitudinal uptake coefficients chlorophyll a concentration calculated as calg 0 ranged from 0 01 to 5 mg l for the streams considered manually calibrated values of reaction parameters indicate that values of 0 10 n 0 2 and p 0 1 were good estimates in predicting nutrient uptake except for the experimental data at soltfeld and freienwill this parameter set was validated for data from the five literature studies a value of 0 1 for p was validated for all test cases three stream reaches in burrows et al 2013 exhibited maximum model performance with n 0 5 instead of the generalized value n 0 2 suggesting a probable difference in algal species that results in higher fraction of nitrogen in algae in these streams ratio of chl a to algal biomass can have a wide range of values from 10 to 100 according to bowie et al 1985 for the tracer tests considered here the generalized value of 10 obtained with the experimental data simulated the models well in all test cases except for burrows et al 2013 and tank et al 2008 overall using the generalized values of the three reaction parameters 0 10 n 0 2 and p 0 1 the modeled kx values were similar to observed values in over 70 of case studies including the experimental data from kielstau catchment we conducted additional tests to determine optimal 0 values for burrows et al 2013 and tank et al 2008 data for these two studies a slightly higher calibrated value for chl a to algal biomass ratio 0 20 would have yielded a better performing model when compared to using the value of 0 10 even though we obtained a relatively lower r2 0 77 between modeled and observed kx values with our universal parameter set 0 10 n 0 2 and p 0 1 it still didn t show significant difference according to a two sample t test at 0 05 significance level this reinforces our recommendation that these values would act as approximate estimates of these parameters when field data is unavailable for calibration the generalized parameter set is developed here for a base case that can be optimized at particular study sites proposed model is set up in such a way that users are free to calibrate any reaction parameter or input field measured values of the parameters for the study reach in consideration selected data from demars 2008 burrows et al 2013 and tank et al 2008 are shown in fig 7 to demonstrate the capability of developed model to accurately simulate nutrient uptake for continuous tracer injection conducted in cairn stream demars 2008 and pc023c stream reach burrows et al 2013 measured plateau concentrations when plotted against distance yielded a phosphate kx of 0 0033 m 1 and 0 011 m 1 respectively corresponding simulated values were 0 0034 m 1 and 0 012 m 1 indicating a very close prediction for pulse injection conducted in upper snake river tank et al 2008 log ratio of reactive and conservative tracer masses at different downstream locations when plotted against distance to estimate nitrate kx in this case with generalized parameters we obtained a slightly higher simulated kx values of 0 0023 m 1 when compared to measured value of 0 0017 m 1 overall for the entire dataset modeled kx values for nitrate phosphate and ammonium closely matched the measured values fig 8 a good model performance r2 0 76 nse 0 47 percent bias 4 3 was thus achieved in terms of kx most of the inconsistencies between observed and modeled values were observed for 5 streams in burrows et al 2013 study fig 8b lack of sufficient breakthrough curve data and relying solely on plateau concentrations to calibrate storage parameters could be one reason behind this the only variable in the reactive part of the model that required significant calibration was background algal concentration calg instead of calibrating empirical parameters  and s in otis the proposed model emphasizes calibrating a physically based variable calg that is expected to affect nutrient uptake in addition the process based nature of our model provides opportunity to feed field measured values of algal concentration into the model like the example of kielstau experimental data used in this study considering the widely varying values of the calibration parameters used in the model in future applications of the new model users are recommended to conduct identifiability and uncertainty analysis of these five parameters when the model is used for decision making kelleher et al 2013 besides simulating short term breakthrough curves like the ones from soltfeld and freienwill the developed model also estimated steady state longitudinal uptake coefficients for nitrate ammonium and phosphate for multiple stream segments in scotland georgia colorado and australia hence for large scale studies this enhancement in nutrient transport representation would assist in realistically predicting water quality without the need for empirical models 4 conclusions this paper presented a finite difference approach based modeling framework to simulate stream solute transport by integrating otis and qual2e models the framework created in matlab known as enhanced otis model includes an improved user interface with additional modeling and data visualization options our model enables users to simulate solute transport using the existing simple first order decay approach or using our biochemical reaction based algorithms a case study performed with experimental tracer test data showed that with actual biochemical reactions the new model takes the dynamic nature of nutrient uptake with changing concentrations into account this is an added benefit over existing first order decay models that calibrate empirical parameters with the help of field measured tracer data including these real world in stream dynamics and process based reactions will not only improve confidence amongst researchers in model predictions but also aid in accurate decision making with regard to water quality management sensitivity analysis of a few key model parameters indicated the significance of background algal concentration on nutrient uptake as well as on the sensitivity of other parameters with high levels of algal concentration in freienwill stream reach 100 mg l the model forecasted a 114 5 increase in longitudinal uptake coefficient kx value when p fraction of algal biomass that is phosphorus was increased from 0 to 1 a 10 5 decrease in kx was also observed at this level of algal concentration when 0 ratio of chl a to algal biomass increased from 10 to 100 the developed model calibrated and validated using data from kielstau and five other published studies gave promising results in terms of ability to predict transport and uptake soltfeld and freienwill breakthrough curves and kx values were accurately modeled by using measured algal data considering all test data with minimal calibration the model estimated longitudinal uptake coefficients with good accuracy r2 0 76 nse 0 47 percent bias 4 3 although the model provides option to calibrate all the parameters we generalized values of 0 10 n 0 2 and p 0 1 to achieve reasonable model performance for more than 70 of the published cases tested here these values are thus proposed as reasonable parameter estimates when field measured data is unavailable for calibration however we recommend calibrating or measuring background algal concentration in streams for precise model predictions although calibrating first order decay rates can provide reasonable estimates of nutrient uptake in many cases this study aims to propose a more realistic approach for simulating nutrient transport that is representative of real world scenarios especially when studying long term nutrient uptake software data availability the proposed enhanced otis model requires matlab software to run the program a beta version of the model along with all the matlab scripts and associated example data is available at https github com fpandara e otis program title e otis developer femeena pandara valappil contact address fpandara alumni purdue edu software access https github com fpandara e otis year first available 2019 software required matlab program language matlab availability and cost open source declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funding for this study was provided by united state department of agriculture national institute of food and agriculture regional research project s 1063 we would like to thank dr benoit demars dr dan baker dr jennifer tank and dr ryan burrows for generously sharing tracer test data from their studies help provided by johannes fischer oliver tank and the lab staff at kiel university to p v femeena in carrying out the tracer experiments is greatly appreciated appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104715 
26017,there is a growing need to address water pollution that demands advanced tools to predict the fate and transport of water quality constituents existing stream solute transport models use simple first order kinetics to evaluate nutrient loss however these ignore biochemical reactions and lack a user friendly interface to address this shortcoming we integrated the one dimensional transport with inflow and storage otis model and the enhanced stream water quality model qual2e and developed an improved interface for a physically based solute transport model with background algal concentration as the only calibration parameter a generalized model was developed and evaluated the new model performed reasonably well in predicting nutrient uptake of newly collected experimental data and published data from 32 other datasets r2 0 76 nse 0 47 and percent bias 4 3 inclusion of biochemical reactions from qual2e improves model confidence and provides options for incorporating actual process based data which is unfeasible in existing first order decay based models keywords stream solute transport nutrient uptake water quality otis qual2e 1 introduction streams and rivers are complex ecohydrological systems under persistent human pressure carpenter et al 1998 johnson et al 1997 societies need clean water for their activities but use waterways to dispose of their waste a costly dilemma requiring expensive engineering solutions solute transport models are frequently used to simulate transport of reactive and non reactive solutes in streams predicting the extent and timing of contaminant spills for example or the export of pollutants during extreme events mueller price et al 2014 ani et al 2009 in these models advection dispersion and chemical and or biological reactions are the basic processes considered another key feature unique to lotic systems is the concept of transient storage which is defined as water that is moving more slowly than water in the channel harvey et al 1996 hall et al 2002 these storage zones can include surface pools hyporheic flow vegetation etc runkel and broshears 1991 transient storage models tsm quantify conservative and reactive solute transport with a one dimensional advection dispersion reaction equation to route water and solutes downstream and capacity coefficients to store water in one or more transient storage zones each characterized by a specific residence time distribution and biogeochemical activity bencala and walters 1983 stream solute workshop 1990 ward et al 2017 harvey et al 1996 the one dimensional transport with inflow and storage model otis runkel 1998 is one of the most commonly used implementations of tsms bencala and walters 1983 sheibley et al 2014 mueller price et al 2016 originally otis was developed for headwater mountain streams but it has been widely used in streams with small to moderate width and depth where one dimensional transport can be assumed fischer et al 1979 in otis conservative solutes move by advection and dispersion in the main channel and exchange with a single transient storage zone while reactive solutes decay according to a first order reaction rate assigned to each domain otis is primarily used in conjunction with field scale tracer experiments where conservative or reactive tracers are injected into a given stream reach to monitor tracer concentrations over time otis parameters are calibrated by fitting observed and simulated tracer concentration data with nutrient transport represented by two calibrated decay parameters therefore two of the primary limitations of using otis to simulate in channel processes are 1 the need for calibration when field data is scarce and 2 the empirical nature of first order decay equations at the watershed scale models often do not include a detailed stream solute transport module owing to the large scale of simulation however studies have suggested the need for refining and improving nutrient transport representation in one of the most widely used watershed models the soil and water assessment tool swat due to the large deviation of modeled values of water quality constituents from the observed values gassman et al 2007 femeena et al 2018 in large watersheds it becomes challenging to use otis where uptake parameters should be calibrated separately for each stream reach another limitation is the use of first order rate kinetics potter et al 2010 o connor et al 2010 and the degree to which nutrient limitation affects nutrient uptake tank et al 2017 the relationship between uptake and limiting nutrient concentration is frequently described by michaelis menten asymptotic equation with many studies emphasizing the link between decreasing biological nitrogen uptake with increasing no3 concentration e g o brien et al 2007 subsequent studies also supported this concept by identifying a partial saturation effect in nutrient uptake mulholland et al 2008 2009 hall et al 2009 however some studies have reported discrepancies like bernot et al 2006 which showed that biological uptake of nitrate was saturated at higher concentrations whereas ammonium and phosphorus uptake continued to increase at higher concentrations nutrient uptake dynamics are complex and change in space and time and it is inappropriate to assume that a single first order uptake parameter can universally simulate nutrient transport hence the need for a comprehensive process based model to simulate nutrient uptake stream water quality models such as the enhanced stream water quality models qual2e qual2k brown and barnwell 1987 chapra et al 2008 and water quality analysis simulation program wasp di toro et al 1983 connolly and winfield 1984 ambrose et al 1988 are examples of process based models that consider biochemical reactions in addition to the advection dispersion equations for transport however these models neglect the strong influence that transient storage can have on pollutant dynamics marsalek et al 2003 and are used as water quality planning and management tools rather than solute transport models an ideal nutrient solute transport model must represent all four key processes advection dispersion transient storage and in stream biogeochemistry to address this gap we combined the advection dispersion transient storage processes used in otis solute transport model and reaction processes used in qual2e water quality model to develop an enhanced solute transport model we calibrated and validated our new model using both experimental data collected for this work and 32 datasets from peer reviewed literature we assessed its performance as a predictive tool and built an improved user interface for running otis and enhanced otis model to provide users with better data visualization and modeling options 2 materials and methods 2 1 otis and qual2e models the proposed solute transport model was created in matlab by combining the algorithms of otis and qual2e the otis model bencala and walters 1983 uses a finite difference approach to calculate solute concentration at different times along the stream length advection dispersion transient storage and decay processes are modeled in otis using equations 1 and 2 below storage zones are immobile and thus do not include advection and dispersion processes the distributed lateral flow component of otis is ignored in this study and therefore we recommend this model for stream reaches where negligible lateral inflow outflow is observed the five major calibration parameters in the otis model include dispersion coefficient d stream cross sectional area a storage zone area as storage exchange coefficient  and first order decay parameters in main channel and storage zone  s 1 c t d 2 c x 2 u c x  c s c  c 2 c s t  a a s c s c  s c s where a stream channel cross sectional area m2 as storage zone cross sectional area m2 c in stream solute concentration mass m3 cs storage zone solute concentration mass m3 d dispersion coefficient m2 s u average flow velocity m s  storage zone exchange coefficient s 1  main channel decay coefficient s 1 s storage zone decay coefficient s 1 the qual2e model brown and barnwell 1987 is a steady state model that simulates up to 15 water quality parameters in branching streams and well mixed lakes it uses a finite difference solution of advective dispersive mass transport and reaction equations to compute steady state water profiles the major water quality parameters simulated in qual2e include dissolved oxygen algae nitrogen as organic n no3 no2 and nh4 and phosphorus as organic and inorganic p algal growth is the key process affecting nutrient transport in qual2e which is further influenced by growth limiting factors such as light nitrogen and phosphorus recent water quality models like wasp di toro et al 1983 have the functionality to model different types of algae such complex modeling considering the natural complexity and the variability of the chl a to biomass ratio was not included in this study owing to the uncertainty induced by an increased number of parameters in the model algal biomass concentration in qual2e is expressed in terms of chlorophyll a concentration using a simple relationship equation 3 major equations affecting nitrate and phosphate uptake by algae in streams are given in equations 4 and 5 see brown and barnwell 1987 for the complete set equations and parameters 3 c h l a  0 a 4 d n o 3 d t  2 n o 2 1 f  1  a 5 d i n o r g p d t  4 o r g p  2 d  2  a where c h l a chlorophyll a concentration g chl a l  0 a conversion factor g chl a mg a a algal concentration mg a l n o 3 concentration of nitrate nitrogen mg n l  2 rate constant for oxidation of nitrite nitrogen day 1 n o 2 concentration of nitrite nitrogen mg n l f fraction of algal nitrogen taken from ammonia pool  1 fraction of algal biomass that is nitrogen mg n mg a  local specific growth rate of algae day 1 inorgp concentration of inorganic or dissolved phosphorus mg p l  4 organic phosphorus decay rate day 1 org p concentration of organic phosphorus mg p l  2 benthos source rate of dissolved phosphorus day 1 d mean stream depth ft and  2 fraction of algal biomass that is phosphorus mg p mg a 2 2 model development for a non reactive tracer the parameters a as d and  will affect solute concentrations in both the main channel and storage zone for a reactive tracer these parameters along with qual2e parameters will affect main channel and storage zone concentrations in this study biochemical reaction equations used in qual2e were incorporated into otis algorithm using matlab platform to develop the enhanced otis model see fig 1 the biochemical reactions are modeled at the same time step as in otis since streams have varying geomorphological conditions and processes in order to model a reactive solute in any given stream it is essential to appropriately calibrate both tsm a as d  and reactive parameters instead of the existing first order decay based approach used in otis we used qual2e equations to estimate the change in solute concentration qual2e includes several types of decay kinetics for each water quality constituent including first order decay and michaelis menten asymptotic decay to minimize calibration complexity the proposed merged model will only use one reaction parameter background algal concentration for calibration to substitute the decay parameters  s of otis the enhanced otis model has the capability to calibrate the storage and reactive parameters using an optimization module when provided with an observed breakthrough curve solute concentration versus time curve model inputs required are the values of all reaction parameters constants stream hydraulic data such as streamflow length cross sectional data and boundary conditions such as background and injection concentrations of algae and other water quality variables this model is a time space dynamic model that uses finite difference approach to estimate concentration of water quality variables at any given time and distance the assumptions considered in otis and qual2e models are also applicable to the proposed model this includes the assumption that solute concentration varies only in longitudinal direction the model is valid for reach lengths where the model parameters are spatially considered constant and therefore for longer reaches multiple model simulations with varying inputs are recommended other improved water quality models like qual2k and wasp were considered but ultimately not selected because of the optimal balance of minimal complexity and improved reach scale process representation in qual2e in addition it is one of the most popular models used in larger watershed scale models for nutrient prediction similar to an approach used by audet et al 2018 we propose a four step model generalization process in this study 1 initial stream specific calibration of transient storage parameters 2 sensitivity analysis to identify sensitive reaction parameters 3 calibration of sensitive reaction parameters using experimental data to obtain a universal reaction parameter set and 4 validation of the selected universal parameter set using literature data a fully functional interface was created using matlab guide gui development environment in which users can input all data related to the tracer tests provide upper and lower bounds of calibration parameters and simulate solute transport for both conservative and reactive tracers fig 2 the user friendly interface is a significant improvement over the existing otis and qual2e models that are based on text files and microsoft excel files the interface is split into two sections for users to be able to model non reactive and reactive tracers separately users may exclusively use the tsm part of the interface to replicate existing otis model simulations but with better data visualization viewing the optimization runs with curve matches and option to plot breakthrough curves concentration distance and concentration time plots and save data files are additional enhancements of the new interface for simulating reactive tracer transport the interface also provides users with the option to select otis based first order decay method or qual2e based reaction method depending on the study duration and conditions although short tracer experiments are expected to be simulated well with existing otis model longer experiments are better simulated with a biochemical reaction based model that accounts for long term algal uptake and nutrient limitation factors 2 3 study area and data the enhanced otis model was calibrated and validated using two different sets of data a field data collected in two separate stream sections in kielstau catchment fohrer and schmalz 2012 schmalz and fohrer 2010 wagner et al 2018 located in northern germany fig 3 and b literature data gathered from 5 published studies for a total of 32 sets of experimental data nutrient uptake was modeled for all the data and compared with measured uptake to validate the model among the different uptake metrics available in this study we use the longitudinal uptake coefficient rate kx which is measured as the decrease of nutrient concentration per unit length fellows et al 2006 brookshire et al 2005 2 4 experimental data the kielstau river in germany is 17 km long and the catchment covers an area of about 50 km2 two instantaneous tracer injections were conducted in october 2017 in two similar order stream reaches towards the outlet of the watershed a a 120 m long reach at soltfeld gauging station and b a 135 m long reach at freienwill fig 3 table 1 the experimental locations were chosen considering stream morphology storage potential and accessibility for tracer injection and monitoring at the time of the experiment streamflow at freienwill was 306 l s and that at soltfeld was 124 l s a conservative tracer sodium chloride and a reactive tracer potassium phosphate were used in the study sodium chloride is a typical conservative tracer used in tracer experiments since the measured chloride in relatively inert and not used for biological uptake it is therefore used as a control tracer in conjunction with n and p tracers to study nutrient uptake for phosphate test a salt solution prepared with 8 kg of nacl 250 g of kh2po4 and 30 l stream water was injected instantaneously at the upstream point of the reach at the downstream point specific conductivity was measured at 5 s intervals using ysi 6600 v2 ysi incorporated usa water quality probe and salt concentrations calculated based on laboratory calibrations phosphate concentrations were measured using water samples collected at the same location in intervals ranging from 30s to 5 min using the phosphate samples 23 data points for soltfeld station and 18 data points for freienwill station were obtained during the time of experiment soltfeld and freienwill reaches had background po4 p concentrations of 0 17 mg l and 0 27 mg l respectively background algal concentration in the streams were approximately 5 95 mg l 2 5 literature data in order to enhance confidence in the developed model additional model validation was done using data assembled from 5 studies that conducted tracer tests in stream reaches ranging from 50 to 2700 m in length table 1 request for data were sent to several authors who conducted tracer tests in different streams around the world among the responses received we only selected studies in which both conservative and reactive tracers were used to obtain calibrated transient storage parameters and longitudinal uptake coefficients two studies demars 2008 tank et al 2008 conducted pulse instantaneous injections and the remaining three schroer 2011 baker et al 2012 burrows et al 2013 conducted continuous constant injections by injecting tracer solution over a period of time nitrate phosphate and ammonium uptake were considered in these studies by injecting salts of these ions along with a conservative tracer altogether literature data provided 32 sets of tracer data for validating our model 2 6 sensitivity analysis qual2e includes 45 biochemical reaction parameters and 17 stream parameters relating to stream characteristics and background solute concentrations that affect reactive solute transport since this study aims for a 1 parameter calibration of the reactive part of the model it was critical to verify that values of all the remaining reaction parameters were acceptable to be used in all streams globally qual2e manual provides a range of values that can be used for each reaction parameter but the typical default values used vary across different models that implement qual2e based solute transport processes we used a manual parameter calibration technique to arrive at a universal set of values for all the reaction parameters using the experimental data at soltfeld and freienwill considering the large number of parameters a one at a time sensitivity analysis was conducted by changing a single parameter value at a time within the typical range and evaluating the change in model results this analysis was used to shortlist the most sensitive parameters that affect nutrient uptake using kielstau experimental data from freienwill station as a test case sensitivity analysis was performed to evaluate the sensitivity of four major parameters affecting n and p uptake ratio of chlorophyll a to algal biomass 0 fraction of algal biomass that is nitrogen n fraction of algal biomass that are phosphorus p and background algal concentration calg in mg l were considered for sensitivity analysis since n has no effect on p uptake in freienwill test case it was ignored in this analysis change in phosphate kx with changing values of 0 p and calg was used to determine the sensitivity of these three parameters the parameter values of 0 n and p are usually provided as a default in the model and their typical range adopted from brown and barnwell 1987 and bowie et al 1985 are reported in table 2 to study the full extent of parameter sensitivity the entire feasible range 0 01 1 for 0 and p was considered for sensitivity analysis background algal concentration on the other hand is generally given as an input to the model and extensively changes depending upon stream conditions therefore we analyzed change in kx over a wide range of algal concentrations 0 500 mg a l which corresponds to 0 5 mg chla l when 0 10 the main objective behind the sensitivity analysis study was to extract the parameters that would go into the model generalization process and to ultimately obtain a universal parameter set highly changing background algal concentrations for different streams is the reason behind using it as a calibration parameter in our newly proposed model and simultaneously finding generalized values for the remaining 3 sensitive parameters 0 n p the major goal behind adapting the qual2e based reaction model is to improve confidence in the model by employing real world scenarios and considering all major stream processes to demonstrate the benefits of using a physically based model a further analysis was conducted to visualize the varying pattern of stream uptake simulated by the proposed model this analysis is expected to show the spatial and temporal differences in stream uptake when using a single value for kx in the main channel as in otis versus a dynamic kx as in our model 2 7 model generalization and validation the primary aim of this part of the study was to check if certain generalizations could be drawn in terms of parameter values in order to move towards a general nutrient transport model for future studies model generalization was done by determining a universal set of values for the three parameters obtained through sensitivity analysis described in section 2 6 this was done using data collected from soltfeld and freienwill stream reaches the enhanced otis model requires calibration of both transient storage model tsm and reaction module for soltfeld and freienwill data the transient storage parameters were automatically calibrated by the model with the help of observed breakthrough curve of conservative tracer best fit parameters were derived by using a malab optimization function fminsearch with objective function to minimize root mean square error rmse between observed and modeled points along the breakthrough curves the otis model fitting procedure converges to a chl a to biomass minimum to ensure that this was indeed the global minimum the optimization was run iteratively with different initial guesses to come to an optimal solution calibration was attained when the rmse values converged to a minimum and the change in rmse between consecutive iterations is less than 0 01 the same calibrated set of storage parameters were used to model the reactive p tracer when simulating phosphate the parameters 0 n and p were manually changed to get the best fit with the observed breakthrough curve the obtained final parameter set was then considered the universal i e generalized reaction parameter values for our new model for the experimental data background algal concentration was a known value from field measurements and hence was not required to be calibrated validation of the universal reaction parameter set was done using the 32 sets of literature tracer test data tsm parameter values for these tests were either taken directly from the papers or determined with the help of published conservative tracer breakthrough curves reactive tracers for the same literature studies were modeled using the universal reaction parameter set obtained from our experimental data simulation in this case background algal concentration had to be calibrated since it was an unknown matching observed and simulated breakthrough curves and longitudinal uptake coefficients would mean that the universal parameter set developed for our experimental streams is applicable for other streams as well when the values of the reaction parameters 0 n p from kielstau study did not give accurate prediction of kx for literature studies other values were tested we assessed model performance by comparing observed and modeled kx values a good fit was assumed if the difference between modeled and observed kx was within 0 0001 m 1 the longitudinal uptake coefficient for pulse injections tank et al 2008 demars 2008 were calculated by plotting the log ratio of reactive and conservative tracer masses over distance from injection point for continuous injections kx was calculated directly by plotting plateau concentrations versus distance the slope of the linear regression line in both cases gives the main channel nutrient uptake per unit length kx which is the inverse of uptake length besides visual interpretation performance indicators such as r2 nash sutcliffe efficiency nse and percent bias were evaluated for better assessment ideally models are considered best performing if they have a value close to 1 for r2 and nse and a value close to zero for percent bias 3 results and discussions the model interface created in matlab was used to simulate tracer tests for both experimental and literature data besides improving the interface scientific reliability of the new model was evaluated in the following sections through model calibration and validation 3 1 experimental tracer test nutrient transport in soltfeld and freienwill stream reaches was modeled using the developed model and calibrated for the tracer injections carried out in these reaches a as d and  values were calibrated for both reaches using conservative tracer data table s 1 and the same parameter values were applied to phosphate breakthrough curves in both streams the model predicted po4 p concentrations with high accuracy fig 4 kolmogorov smirnov test indicated very close match between the curves with the test statistic d maximum deviation of 0 13 for soltfeld and 0 11 for freienwill at 1 significance level null hypothesis was accepted indicating that the difference between modeled and observed curves was not significant in either case comparing individual data points along the breakthrough curves both reaches demonstrated a very high r2 0 99 for soltfeld and 0 95 for freienwill a mass balance approach yielded phosphate kx values in soltfeld and freienwill of 0 00054 m 1 and 0 00029 m 1 corresponding kx values obtained from our model were 0 00076 m 1 and 0 00024 m 1 the ability of our model to simulate dynamic uptake was demonstrated by expressing change in po4 p concentration as a function of solute concentration this would overcome one of the existing limitations with first order solute transport models in which nutrient uptake is modeled at a single solute concentration covino et al 2010 the model was run for freienwill data and uptake was estimated in terms of decrease in po4 p concentration p p is the change in po4 p concentration from the previous time step according to the finite difference approach used in the model eq 1 c t and c s t change with c and c s respectively in other words solute concentration and consequently nutrient uptake varies at every time step in both main channel and storage zone fig 5 showing greater p as po4 p concentration increases this is also in agreement with experimental work done by bernot et al 2006 which reported an increase in phosphorus uptake with higher concentrations a slight lag can be observed between main channel and storage zone uptake curve which is expected in reality where solute entry to storage zones is delayed compared to the main channel fig 5a first order decay approach in otis also simulates uptake as a function of solute concentration c t k c however with the enhanced model c t is not just a function of c and other interactions and factors affecting algal growth and uptake such as light and nutrient limitations are also considered while otis and many other solute transport models represents uptake by two values  or s the enhanced model utilizes a hidden dynamic uptake parameter that gets updated during each time and distance step of the model fig 5b the model also accounts for other existing nutrients within the stream that affects limiting factors of algal uptake which is not considered in the first order decay base method of otis for instance a stream with high background nitrate concentration and limited phosphate concentration may not show any uptake for additional nitrate tracer but will have high phosphate tracer uptake such dynamics are explicitly included in the new model using qual2e equations experimental tracer test data used in this study typically uses one nutrient eg nitrate or phosphate at a time to avoid interactions and to understand the uptake of each under ambient conditions qual2e incorporates all the nutrient reactions in the algal growth model and accounts for their interactions and limitations therefore the collected experimental data may not explicitly represent the limiting nutrient conditions in the stream as modeled by qual2e for steady state studies in streams with minimal solute interactions a single uptake parameter may be sufficient however for real world scenarios where pollutant input may vary over time and distance resulting in shifting nutrient limitations over time the enhanced model provides better and more realistic options to model nutrient transport examples of such scenarios include pollutant loads coming from industries that are mostly injected for certain time of the day or non point source agricultural runoff that gets distributed over a specified reach length even though the studies presented here did not provide opportunity to demonstrate these cases the ability to simulate dynamic uptake is expected to be beneficial for users simulating such scenarios with temporally and spatially varying pollutant inputs 3 2 sensitivity analysis to understand the influence of background algal concentration on sensitivity of other parameters the model was run with two levels of algal concentration the phosphate tracer test at freienwill was used as the test case for sensitivity analysis longitudinal uptake coefficient for p changed from 0 00023 m 1 to 0 00028 m 1 when algal concentration was changed from 0 to 500 mg l fig 6 a at low algal concentration 1 mg l stream kx showed negligible change 0 13 when 0 was increased from 10 to 100 fig 6c similarly within the considered range of p 0 1 1 kx showed only 5 4 increase at low algal concentration at high algal concentration both 0 and p demonstrated high sensitivity with 10 5 and 114 5 change in kx respectively these results show the significance of background algal concentration on phosphate uptake although similar behavior is expected for n when studying n uptake nitrate removal is stream is a much more complicated process owing to other factors like denitrification and algal preference for ammonia kemp and dodds 2002 bernhardt et al 2002 3 3 model generalization and validation the values of 0 n and p can change according to different types of algae phytoplankton periphyton and benthic autotrophs present in the streams but unlike algal concentration these parameters are rarely measured in field in an attempt to avoid extensive calibration and to generalize the values of 0 n and p model generalization was carried out with kielstau experimental data and validated for literature data this ensures that background algal concentration remains as the only parameter required for model parameterization that could be derived through calibration or accurate field measurement for all of the 32 sets of tracer data obtained from literature storage parameters a as d and  were either directly fed to the model in case these were reported in the paper or calibrated using observed conservative tracer breakthrough curves table s 1 using the derived set of storage parameters reactive solutes were modeled for each test data concentrations of algae were calibrated for all test cases to obtain best fit between observed and modeled breakthrough curves and longitudinal uptake coefficients chlorophyll a concentration calculated as calg 0 ranged from 0 01 to 5 mg l for the streams considered manually calibrated values of reaction parameters indicate that values of 0 10 n 0 2 and p 0 1 were good estimates in predicting nutrient uptake except for the experimental data at soltfeld and freienwill this parameter set was validated for data from the five literature studies a value of 0 1 for p was validated for all test cases three stream reaches in burrows et al 2013 exhibited maximum model performance with n 0 5 instead of the generalized value n 0 2 suggesting a probable difference in algal species that results in higher fraction of nitrogen in algae in these streams ratio of chl a to algal biomass can have a wide range of values from 10 to 100 according to bowie et al 1985 for the tracer tests considered here the generalized value of 10 obtained with the experimental data simulated the models well in all test cases except for burrows et al 2013 and tank et al 2008 overall using the generalized values of the three reaction parameters 0 10 n 0 2 and p 0 1 the modeled kx values were similar to observed values in over 70 of case studies including the experimental data from kielstau catchment we conducted additional tests to determine optimal 0 values for burrows et al 2013 and tank et al 2008 data for these two studies a slightly higher calibrated value for chl a to algal biomass ratio 0 20 would have yielded a better performing model when compared to using the value of 0 10 even though we obtained a relatively lower r2 0 77 between modeled and observed kx values with our universal parameter set 0 10 n 0 2 and p 0 1 it still didn t show significant difference according to a two sample t test at 0 05 significance level this reinforces our recommendation that these values would act as approximate estimates of these parameters when field data is unavailable for calibration the generalized parameter set is developed here for a base case that can be optimized at particular study sites proposed model is set up in such a way that users are free to calibrate any reaction parameter or input field measured values of the parameters for the study reach in consideration selected data from demars 2008 burrows et al 2013 and tank et al 2008 are shown in fig 7 to demonstrate the capability of developed model to accurately simulate nutrient uptake for continuous tracer injection conducted in cairn stream demars 2008 and pc023c stream reach burrows et al 2013 measured plateau concentrations when plotted against distance yielded a phosphate kx of 0 0033 m 1 and 0 011 m 1 respectively corresponding simulated values were 0 0034 m 1 and 0 012 m 1 indicating a very close prediction for pulse injection conducted in upper snake river tank et al 2008 log ratio of reactive and conservative tracer masses at different downstream locations when plotted against distance to estimate nitrate kx in this case with generalized parameters we obtained a slightly higher simulated kx values of 0 0023 m 1 when compared to measured value of 0 0017 m 1 overall for the entire dataset modeled kx values for nitrate phosphate and ammonium closely matched the measured values fig 8 a good model performance r2 0 76 nse 0 47 percent bias 4 3 was thus achieved in terms of kx most of the inconsistencies between observed and modeled values were observed for 5 streams in burrows et al 2013 study fig 8b lack of sufficient breakthrough curve data and relying solely on plateau concentrations to calibrate storage parameters could be one reason behind this the only variable in the reactive part of the model that required significant calibration was background algal concentration calg instead of calibrating empirical parameters  and s in otis the proposed model emphasizes calibrating a physically based variable calg that is expected to affect nutrient uptake in addition the process based nature of our model provides opportunity to feed field measured values of algal concentration into the model like the example of kielstau experimental data used in this study considering the widely varying values of the calibration parameters used in the model in future applications of the new model users are recommended to conduct identifiability and uncertainty analysis of these five parameters when the model is used for decision making kelleher et al 2013 besides simulating short term breakthrough curves like the ones from soltfeld and freienwill the developed model also estimated steady state longitudinal uptake coefficients for nitrate ammonium and phosphate for multiple stream segments in scotland georgia colorado and australia hence for large scale studies this enhancement in nutrient transport representation would assist in realistically predicting water quality without the need for empirical models 4 conclusions this paper presented a finite difference approach based modeling framework to simulate stream solute transport by integrating otis and qual2e models the framework created in matlab known as enhanced otis model includes an improved user interface with additional modeling and data visualization options our model enables users to simulate solute transport using the existing simple first order decay approach or using our biochemical reaction based algorithms a case study performed with experimental tracer test data showed that with actual biochemical reactions the new model takes the dynamic nature of nutrient uptake with changing concentrations into account this is an added benefit over existing first order decay models that calibrate empirical parameters with the help of field measured tracer data including these real world in stream dynamics and process based reactions will not only improve confidence amongst researchers in model predictions but also aid in accurate decision making with regard to water quality management sensitivity analysis of a few key model parameters indicated the significance of background algal concentration on nutrient uptake as well as on the sensitivity of other parameters with high levels of algal concentration in freienwill stream reach 100 mg l the model forecasted a 114 5 increase in longitudinal uptake coefficient kx value when p fraction of algal biomass that is phosphorus was increased from 0 to 1 a 10 5 decrease in kx was also observed at this level of algal concentration when 0 ratio of chl a to algal biomass increased from 10 to 100 the developed model calibrated and validated using data from kielstau and five other published studies gave promising results in terms of ability to predict transport and uptake soltfeld and freienwill breakthrough curves and kx values were accurately modeled by using measured algal data considering all test data with minimal calibration the model estimated longitudinal uptake coefficients with good accuracy r2 0 76 nse 0 47 percent bias 4 3 although the model provides option to calibrate all the parameters we generalized values of 0 10 n 0 2 and p 0 1 to achieve reasonable model performance for more than 70 of the published cases tested here these values are thus proposed as reasonable parameter estimates when field measured data is unavailable for calibration however we recommend calibrating or measuring background algal concentration in streams for precise model predictions although calibrating first order decay rates can provide reasonable estimates of nutrient uptake in many cases this study aims to propose a more realistic approach for simulating nutrient transport that is representative of real world scenarios especially when studying long term nutrient uptake software data availability the proposed enhanced otis model requires matlab software to run the program a beta version of the model along with all the matlab scripts and associated example data is available at https github com fpandara e otis program title e otis developer femeena pandara valappil contact address fpandara alumni purdue edu software access https github com fpandara e otis year first available 2019 software required matlab program language matlab availability and cost open source declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funding for this study was provided by united state department of agriculture national institute of food and agriculture regional research project s 1063 we would like to thank dr benoit demars dr dan baker dr jennifer tank and dr ryan burrows for generously sharing tracer test data from their studies help provided by johannes fischer oliver tank and the lab staff at kiel university to p v femeena in carrying out the tracer experiments is greatly appreciated appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104715 
26018,the u s pacific northwest has a history of frequent and occasionally deadly landslides caused by various factors using a multivariate machine learning approach we combined a pacific northwest landslide inventory with a 36 year gridded hydrologic dataset from the national climate assessment land data assimilation system to produce a landslide hazard indicator lhi on a daily 0 125 degree grid the lhi identified where and when landslides were most probable over the years 1979 2016 addressing issues of bias and completeness that muddy the analysis of multi decadal landslide inventories the seasonal cycle was strong along the west coast with a peak in the winter but weaker east of the cascade range this lagging indicator can fill gaps in the observational record to identify the seasonality of landslides over a large spatiotemporal domain and show how landslide hazard has responded to a changing climate keywords xgboost washington oregon land data assimilation system gradient boosting machine national climate assessment 1 introduction landslides have caused substantial property losses and fatalities in the pacific northwest of the united states fortunately the states of washington and oregon have invested in landslide research as have federal and local agencies an important fruit of this effort landslide inventories have already been of great assistance to urban planning testing of early warning systems and prioritizing future research in this manuscript we show how machine learning can describe the seasonality of landslides over a broad region by fusing landslide inventories and land data assimilation systems in the city of seattle most landslides happen in winter especially the month of january laprade and tubbs 2008 a much shorter record of landslides in the city of portland oregon showed frequent landslide occurrence from december to march mirus et al 2018b a similar pattern was found in a pacific northwest landslide inventory pnli that included data from portland seattle and many other locations kirschbaum et al 2016 however data on the timing of landslides are rare outside of metropolitan areas furthermore major temporal gaps can be seen in the pnli especially in the years 1979 1995 and 1998 2005 stanley et al 2019 given the diversity of terrains and climates across the pacific northwest it seems unwise to assume that all locations follow the seasonal pattern established within two well documented metropolitan regions instead landslide models must fill gaps in the recorded history of the pacific northwest previous research has identified atmospheric rivers long plumes of water vapor in the troposphere as the principal cause of major floods in western washington neiman et al 2011 and landslides along the west coast biasutti et al 2016 cordeira et al 2019 however atmospheric rivers reaching the coast of washington or oregon are most frequent in november gershunov et al 2017 which is quite early seasonally relative to the historical landslide climatology although the duration intensity and frequency of atmospheric rivers along the u s west coast do not follow the same climatology gershunov et al 2017 vertically integrated horizontal vapor transport also peaks early in the season during december and atmospheric rivers last longer early in the season recent research suggests that families temporal clusters of atmospheric rivers are substantially more hazardous than lone atmospheric rivers ogle et al 2018 taken together these findings suggest that individual storms are not sufficient to explain patterns of landslide occurrence antecedent conditions must also be considered researchers have long been aware of the importance of antecedent rainfall to hydrologic response in this region istok and boersma 1986 in the city of seattle landslide occurrence was estimated by combining 3 days of recent and 15 days of antecedent precipitation chleborad 2000 more recently an exhaustive analysis revealed that 32 days of antecedent precipitation better identified landslide events in much of the seattle metro area scheevel et al 2017 applying a similar recent antecedent ra rainfall threshold across the pacific northwest resulted in numerous successfully identified landslides but the model s false positive rate fpr was quite high in some locations and the monthly climatology overemphasized the beginning of landslide season in november stanley et al 2017 since antecedent rainfall functions not as the direct cause of landslides but as a proxy for groundwater state an antecedent wetness index was created to describe the accumulation of water in a slope godt et al 2006 this index was combined with the intensity and duration of rainstorms to predict most major landslide events in seattle washington subsequent research showed that replacing the antecedent rainfall component of a ra threshold with soil saturation measured by subsurface probes reduced the frequency of false positives especially in the summer and fall mirus et al 2018a melting snow can also be a risk factor for landslides chleborad 2000 snow depth has been incorporated in a prototype landslide early warning system for seattle chleborad et al 2008 in particular rain on snow ros can generate massive runoff and landslide events chleborad 2000 marks et al 1998 beyond these well established variables other factors such as air temperature chleborad 2000 1998 1997 soil temperature tubbs 1974 and wind buma and johnson 2015 have been identified as predictors of landslides in mountainous or coastal areas of the western united states a global analysis of satellite precipitation estimates indicated that landslide behavior of the western united states is not well explained by rainfall alone which suggests that other factors are important in this region jia et al 2019 although intense rainfall is an important trigger of landslides in the pacific northwest many other factors help to explain the spatiotemporal patterns of landslide occurrence prior landslide models rarely considered all of these time varying factors but the failure to do so can result in outputs that are climatologically incorrect or confined to a specific place machine learning offers a way to model landslide occurrence in a consistent objective manner without omitting major hydrological variables machine learning has been applied in landslide research for over a decade lee et al 2002 marjanovic et al 2009 the vast majority of this research focused on static landslide susceptibility korup and stolle 2014 but machine learning was also applied to dynamic landslide prediction farahmand and aghakouchak 2013 prediction of landslide displacement from in situ data lian et al 2014 zhao et al 2018 and the remote sensing of landslides chen et al 2014 ghorbanzadeh et al 2019 stumpf and kerle 2011 most scientists reported good to excellent predictive performance korup and stolle 2014 lun et al 2017 however researchers should take steps to avoid overfitting brenning 2005 and evaluate the geomorphic plausibility of model outputs steger et al 2016 researchers must also consider the potential effects of incomplete or inaccurate landslide inventories upon both the training and testing of empirical models stanley and kirschbaum 2017 steger et al 2017 despite these notes of caution machine learning is a powerful tool for understanding complex climate data and predicting rare events like landslides we describe the seasonality of landslides with a landslide hazard indicator lhi that scales well across space and time here lhi is defined as the daily probability that a landslide will occur averaged over a given space time domain at coarser temporal resolutions this lagging indicator is similar but not identical to landslide hazard the yearly probability of landslide occurrence is greater than the mean of the daily probabilities lhi offers a few benefits for landslide research first it is directly comparable to observed frequencies of a boolean variable this means that for a well calibrated model the mean of predictions and observations will be equal second the simplicity of calculation leads to a simplicity of interpretation the values shown are always the mean daily probability over a given area in this case 0 125 degree grid cells in contrast using the number of predicted landslides as an indicator might result in numerous cases of fractional landslides while return periods are notorious for misinterpretation read and vogel 2015 sayers 2016 vogel et al 2014 third lhi enables comparisons across space time and even other variables even geographic areas of unequal size or time periods of unequal length could be compared lhi identifies the times and places at the greatest hazard with a consistent quantitative metric that facilitates the search for patterns across space and time 2 data 2 1 national climate assessment land data assimilation system nca ldas the nca ldas is a terrestrial hydrology satellite data assimilation system that was created to support the national climate assessment nca jasinski et al 2019 kumar et al 2018 xia et al 2012 developed within the nasa land information system lis modeling framework kumar et al 2006 nca ldas integrates input data from multiple sources including a 36 year record of satellite observations to provide gridded daily time series over the continental u s of land surface variables including precipitation runoff latent and sensible heat fluxes soil moisture at multiple depths and snow water equivalent nca ldas version 2 0 employs the noah version 3 3 land surface model ek et al 2003 and is driven by atmospheric forcing data from the north american land data assimilation system phase 2 nldas 2 xia et al 2012 nca ldas data products are provided at 0 125 degree resolution for the satellite era water years 1980 2015 jasinski et al 2018 the nldas 2 precipitation forcing field is a product of a temporal disaggregation of a gauge only climate prediction center analysis of daily precipitation chen et al 2008 higgins et al 2000 and includes an orographic adjustment daly et al 1994 the rest of the forcing fields used by nca ldas including surface temperature radiation wind and humidity were derived from the analysis fields of the national centers for climate protection north american regional reanalysis lis generated a variety of hydrological variables from these forcings including soil moisture and snow depth lis performed simultaneous assimilation of satellite data with a 1 d ensemble kalman filter this data assimilation process systematically improved the accuracy of soil moisture and snow depth especially when data from newer sensors were available kumar et al 2018 analysis of these data revealed predominantly declining trends across the pacific northwest in several variables relevant to landslide activity including heavy precipitation snow cover and runoff jasinski et al 2019 because precipitation thresholds for landslide warning have been shown to vary widely over space baum and godt 2009 we rescaled rainfall and all other nca ldas variables in proportion to each grid cell s full range r e s c a l e d g r i d c e l l v a l u e g r i d c e l l v a l u e g r i d c e l l m i n i m u m g r i d c e l l m a x i m u m g r i d c e l l m i n i m u m 10 20 a small value was added to the denominator to avoid divide by zero errors in rare cases for some machine learning techniques rescaling or normalizing data greatly improves performance but this is not required for the tree based method presented below this preprocessing provides a strong localization effect that respects both the effects of rainfall frequency and maximum intensity and it is easily reversed interpreted and replicated nevertheless we do not claim that this is the optimal method for model localization future research may show that a more sophisticated approach delivers a benefit to model performance 2 2 3dep although very high resolution digital elevation models dem have been derived from lidar over much of the pacific northwest these data did not provide complete coverage the coarse resolution of the nca ldas forcing variables implied that a relatively coarse dem with a high level of consistency would be more appropriate the 3dep dataset with a resolution of 0 333 arcseconds u s geological survey 2016 combines a high level of consistency and trust with a resolution that is still relevant to landslide behavior in order to represent terrain at the modeling resolution of 0 125 we calculated a relief value by subtracting the lowest from the highest elevation within each 0 125 degree grid cell this range of elevations does not capture the geomorphological nuances useful for landslide modeling at the local scale but it does provide a simple topographic index to modulate the response to rainfall and other triggers as our primary goal was to examine the role of climate in landslide activity additional topographic variables were not added to the model 2 3 ecoregions in order to summarize and disaggregate landslide seasonality over the pacific northwest we adopted the level i ecoregions dataset u s environmental protection agency 2010 the commission for environmental cooperation developed this ecological classification to address environmental issues shared by canada mexico and the united states level i is the coarsest classification with 15 ecoregions that cover the entirety of north america only three of these zones occur in the states of oregon and washington the marine west coast forest the northwestern forested mountains and the north american deserts fig 1 2 4 pnli a landslide inventory for the pacific northwest was created by merging data from several existing inventories kirschbaum et al 2016 as well as collecting new landslide reports from news media and other sources most of the landslides in the pnli originated in the global landslide catalog kirschbaum et al 2015 2010 the statewide landslide information database for oregon burns 2014 and landslide inventories from the washington department of natural resources sarikhan et al 2009 sarikhan et al 2008 washington division of geology and earth resources 2016 over 85 of the inventoried landslides were considered shallow landslides or debris flows in order to determine relationships between historical landslide occurrence and possible explanatory variables from nca ldas we converted the 14 495 landslides with known dates to a gridded dataset with the same temporal and spatial resolution as nca ldas unfortunately this process required omission of most data in the source inventories because these often prehistoric landslides do not have specific known dates in this paper we shall refer to this gridded record of landslide occurrence as the pnli although pnli has previously been used to describe the underlying vector data the pnli is affected by a reporting bias towards populated areas and highways as the date of landslide events is much more likely to be known when humans are impacted kirschbaum et al 2016 this problem is most salient in washington s olympic peninsula where the pnli does not reflect the large number of landslides with known locations but unknown dates of occurrence fig 2 in addition the pnli is systematically biased towards the present since data was collected more consistently over the past decade thus the pnli alone is inadequate to provide a record of the climate s effect on landslide behavior over multiple decades we sought to address this problem by developing a landslide model that responds to changes in a multivariate spatially consistent gridded climate record despite these limitations some general patterns can be observed in the pnli landslides have been reported much more frequently to the west of the cascade mountains fig 2 furthermore landslide activity followed a clear seasonal cycle fig 3 with the most frequent landslide activity in the winter months of december january and february landslides were also common in march and november the north american deserts did not follow this cycle but very few landslides were reported in this ecoregion landslides were most frequent in the marine west coast forest ecoregion although the difference was less pronounced from april to september the distribution of landslides was highly skewed fig 4 just eight days over the years 1996 2015 accounted for 87 of landslide grid cells as well as almost 90 of historical landslides from 1996 to 2016 stanley et al 2017 no landslides were reported on the vast majority of days 6 820 during this period the dominance of a handful of landslide events suggests the importance of long records to a good understanding of landslide hazard 3 methods 3 1 gradient boosting from the wide variety of machine learning software we selected xgboost a form of decision tree model that uses gradient boosting to achieve a good empirical fit xgboost has become the first choice of many data scientists due to its well documented speed and accuracy chen and guestrin 2016 furthermore landslide scientists have applied it to both susceptibility mapping chakraborty et al 2019 and displacement monitoring zhao et al 2018 xgboost is a gradient boosting machine gbm a class of models that optimizes classification performance by combining many weakly predictive models into a single highly accurate ensemble the algorithm relies upon multiple iterations each of which corrects to the model produced in the previous step a process known as boosting a gbm optimizes performance by following the steepest gradient along a differentiable loss function friedman 2001 with this method scientists can fit a model that is highly predictive of a training dataset however overfitting fig 5 can reduce the performance of the model on new data this is commonly addressed by random subsampling of the training data a technique which can greatly improve the ability of a gbm to generalize over new data friedman 2002 in addition xgboost addresses this issue by regularizing the objective function chen and guestrin 2016 i e it adds a term that represents model complexity to the loss function that describes classification error l  i l y i y  i k  f k here l is the loss function that compares the predicted value y  i in this case the probability that a landslide has occurred to the observed value y i 1 if a landslide was recorded otherwise 0 and  represents the complexity of each tree added to the model f k in xgboost this regularization is primarily applied through the  parameter which requires each new split to improve the existing tree by a minimum amount although we utilized both subsampling and objective regularization when training the model these were inadequate to obtain a stable and generally applicable prediction of landslide activity fortunately xgboost offers several additional parameters the rate at which machine learning proceeds can be directly controlled with the  parameter also known as shrinkage or learning rate this number represents the ratio by which the output of each iteration will be reduced thus increasing  will speed up machine learning and result in a model with a small number of strongly predictive trees in contrast decreasing  will slow learning and produce more trees each of which is less fitted to the data but can have improved prediction as a whole in practice larger ensembles of trees produce more accurate and reliable outputs the default  of 0 3 is excellent for exploratory work but data scientists often lower the value to achieve better generalization we set  to 0 01 for the final model in our application although xgboost offers several parameters to mitigate overfitting monotonicity constraints were especially effective although this parameter has been used in cybersecurity research incer et al 2018 we believe that landslide scientists have not applied it previously this may be because it nullifies an important advantage of machine learning over linear regression the ability to model non monotonic relationships this loss may reduce performance on existing data but it can increase the model s reliability for new conditions incer et al 2018 fortunately researchers can control the presence and direction of monotonic constraints for every input variable even very conservative values for  and other parameters failed to eliminate overfitting so we applied monotonic constraints to all predictors for which a direction could be assigned with confidence from prior knowledge table 1 other variables were not included in the model we also made a simplification of soil temperature which is available for each soil layer in the nca ldas record rather than include four related variables that might not have a monotonic relationship with landslide probability we checked if the temperature of any layer was below 0 c although variable selection and monotonicity introduced a certain amount of subjectivity into the modeling process the increase in reliability made it the best choice for obtaining a regional landslide climatology the difference between the number of grid cells with and without recorded landslides known as class imbalance is an important challenge in landslide research stumpf and kerle 2011 van den eeckhaut et al 2009 as well as many other domains dynamic landslide modelers must contend with a much greater imbalance than is typically encountered in susceptibility mapping because exact dates are not available for most inventoried landslides thus the inventories of dated landslides are much smaller than the undated landslide inventories used for susceptibility mapping furthermore the limited stockpile of landslide events must be divided across thousands of time steps in this case the ratio of landslides to other pixels is 7 x 10 5 the strength of this class imbalance destabilized gradient descent even with a low  fortunately xgboost provides two tools to address class imbalance the first scale pos weight can raise the importance of rare events during model training this can increase predictive ability song et al 2018 but will produce incorrect probability estimates xgboost developers 2016 in order to avoid this problem we focused on the second parameter max delta step which limits the strength of updates performed in each iteration of training this differs from  in that it applies a hard limit to each update rather than rescaling the value a value of 10 produced a much more stable series of updates when training this landslide model table 2 shows the full list of parameter choices used to generate a landslide climatology of the pacific northwest in order to achieve the most accurate model we limited the training data to the most recent section of the pnli water years 2011 2015 we withheld water years 1996 2010 to evaluate the performance of the trained model this time period contains two very large landslide events and relatively few small landslide events since these events are outside the range of data for 2011 2015 they represent a difficult test for the model to overcome most of these early events were recorded in oregon the remainder of the pnli is not a complete record of water year 2016 so it was not included in either the training or validation datasets the entirety of the states of washington and oregon were utilized but the coarse resolution of the nca ldas dataset meant that a few landslides were located within water pixels and did not contribute to either training or testing the model 3 2 landslide climatology while the process of building machine learning models can in itself be informative we produced a landslide hazard indicator in order to understand the historical patterns of landslide behavior across the pacific northwest at the most basic level we averaged the lhi over the full time series 1979 2016 at each location to produce a map that shows where landslides would be expected more frequently we applied a similar analysis over time by averaging lhi for each month across the whole study area we also produced a climatology for each ecoregion by determining the long term mean lhi within each zone and month these methods could be repeated for any subset of the study domain in order to understand the climatology of landslides in more detail we calculated a seasonality index si that rates the temporal concentration of the lhi at each grid cell although this can be produced on a monthly basis markham 1970 it can also be calculated from daily data burn 1997 the si is determined by treating each unit of time as an angle on the unit circle and assigning a length equal to the property of interest in this case lhi next the resultant of these vectors is divided by the annual total to produce the si value thus a uniform distribution of lhi across the year would produce a si of zero while a perfect concentration of all landslides in a single month or day would produce a si of one in addition the mean direction of the vectors can be determined and converted back to the relevant unit of time landslide activity does not necessarily peak on the resulting date rather it is the center of the landslide season as a whole mean direction is more reliable for sites with high si values low values may indicate a lack of consistent seasonality and thus the average date might be uninformative for these sites this range of climatological measures shows where and when landslides were likely to happen 3 3 co occurrence of rain on snow and landslides since rain falling on snow has been identified as a major cause of landslides in this region we also analyzed the historical frequency of these events drawing on a previously published model chleborad et al 2008 we defined ros as days in every grid cell where greater than 25 4 mm of rain fell on a layer of snow deeper than 150 mm the historical frequency of landslides 3 0 10 3 in these events is roughly two orders of magnitude higher than the overall baseline for the study area 8 5 10 5 it is also slightly higher than the frequency of landslides when rainfall is greater than 25 4 mm and snow depth is not greater than 150 mm 2 8 10 3 in addition a threshold of 25 mm recent rainfall has been identified as a rough minimum trigger for landslide activity across this region stanley et al 2017 we computed the long term frequency of ros days at each location 3 4 trends in rain on snow and landslide seasonality in addition to the primary analyses we also calculated the trends over time in ros the lhi and the si derived from lhi the rate of change was calculated with the theil sen method for slope estimation theil 1992 with mann kendall significance testing kendall 1948 mann 1945 4 results 4 1 gradient boosting model evaluation the model produced maps of lhi with a daily 0 125 degree resolution typically the western third of the study area showed the highest probability of landslide occurrence fig 6 but this was not always the case fig 7 this geographic pattern broadly matches the historical record of landslides fig 3 standard quantitative measures also indicate success in reproducing the observations in the pnli binary predictions including landslide maps are often judged with the curve from the receiver operating characteristic petschko et al 2014 although the shape of the curve can convey more complex information the area under this curve auc serves as a standard measure of performance results of the model comparison have an auc of 0 91 for the model training period 2011 2015 and an auc of 0 95 for the evaluation period 1996 2010 suggesting excellent overall performance the process of model development is iterative and we tested multiple models against the validation data in order to ensure complete independence we did not test predictions for the year 2016 during the model development process therefore we obtained two small inventories of dated landslides nasa s global landslide catalog glc and recent landslides in washington state washington dnr 2019 rock falls were removed from both inventories and landslides with a spatial accuracy worse than 10 km were removed from the glc we benchmarked the new xgboost model against a simple recent antecedent ra rainfall threshold that was fitted to this study area stanley et al 2017 the ra threshold predicted 37 of the glc landslides and 31 of the recent landslides in washington state with a fpr of 6 in order to compare these binary results to the probabilistic outputs of xgboost we applied a probability threshold of 0 00033126 which generates a binary output with the same fpr as the ra threshold but a different spatiotemporal structure with this binary output xgboost correctly predicted 55 of the glc and 69 of the washington landslides since quantitative metrics can give false confidence in empirical models steger et al 2016 we also sought to examine the plausibility of the relationships between model inputs and outputs xgboost provides a few default measures of variable importance the most useful is gain the average error reduction made by introducing each variable to the model chen and guestrin 2016 this measure shows that rainfall is the most useful predictor of landslides while snowfall is the least fig 8 soil moisture and temperature also play important roles in modulating the output 4 2 landslide climatology lhi provides an overview of landslide seasonality across the whole of the pacific northwest as shown in fig 6 landslides were more probable along the west coast landslide triggering conditions also occur frequently on the west side of the cascades the temporal pattern of landslide behavior was revealed by calculating the monthly mean lhi of all grid cells in the study area fig 9 the highest monthly lhi from 1979 to 2016 was recorded for february 1996 which was also the peak of landslide activity recorded in the pnli the pnli does not contain any landslides with precise dates reported before 1996 other notable peaks in lhi can be seen in december 1981 february 1986 december 1996 february 1999 and january 2006 lhi typically peaked in winter fell to very low levels in the summer and did not rise until late in the fall we also analyzed the monthly climatology within each major ecoregion fig 10 lhi was highest in the marine west coast forest ecoregion and lowest in the north american deserts ecoregion lhi peaked in the marine west coast forest and northwestern forest mountains during december and january but march was the peak month in north american deserts calculation of si and mean direction of lhi at each grid cell corroborated this climatology fig 11 the average date of the local landslide season was typically in the winter months of december january and february for locations west of the cascades landslide season was less consistent east of the cascades the average dates were spread over fall winter or spring however lhi did not peak during summer anywhere in the study area si was very high along the west coast indicating a strong seasonal pattern to lhi in this area fig 12 in some locations si exceeded 0 7 east of the cascades si was more variable in some of these areas si exceeded 0 6 while si 0 1 at many locations the largest area without a consistent landslide season was located in central washington si was also low over much of eastern oregon overall this climatological analysis showed that landslide hazard was most severe in winter that lhi was declining but highly variable and that the seasonal cycle was more consistent along the west coast 4 3 co occurrence of rain on snow and landslides rain falls on snow most frequently during the winter months of december january and february fig 13 the months of march and november represent a transition between summer and winter conditions with a lesser number of ros days this season matches the landslide cycle quite well fig 3 ros was most common in the olympic and cascade ranges particularly the north cascades fig 14 the remainder of the study area experiences ros only on rare occasions this geographic distribution does not match that of historically recorded landslides fig 2 specifically no dated landslides were recorded in the olympic mountains even though this area experiences ros frequently in addition large numbers of landslides have been recorded in the coast range and cascades of oregon ros occurs rarely in most of these areas 4 4 trends we found statistically significant declining trends in the annual number of ros days at many locations fig 15 a the rate of change was no more than a half day per year over the period 1979 2016 in most locations it was much slower very few locations experienced increased ros statistical significance was largely confined to the areas with frequent ros the overall picture is one of broadly declining numbers of ros days at higher elevations and infrequent ros at lower elevations and in drier regions although statistically significant trends were not found at most grid cells lhi declined at most locations on both sides of the cascade range and increased in relatively few locations fig 15b statistically significant trends in si were relatively uncommon even at the 0 1 level fig 15c si increased and decreased in roughly equal numbers of locations nearly all of these were located east of the cascades however neither the increases nor the declines were clustered in a specific area 5 discussion while we implicitly or explicitly modeled a wide range of triggers pre conditioning factors and landslide types some topics were not analyzed rock falls were explicitly omitted from the pnli and wave action on coastal bluffs was not modeled we did not consider the cascading impacts from volcanoes which means the results do not show potentially substantial if infrequent lahar hazards from the cascades while wildfires are probably not a major destabilizing factor in this region the omission of burned areas as a predictor might have a confounding effect on model performance in some locations perhaps the most important omission was the absence of anthropogenic terrain modification from the list of predictors human activities can have a profoundly destabilizing effect and they have played a major role in landslide activity from local chleborad et al 2008 to global scales froude and petley 2018 the challenge here was differentiating the true effects of human activity from the apparent effects that are due to inventory bias landslides were much more commonly recorded near highways in the pnli kirschbaum et al 2016 but this probably reflects the chance that a landslide was recorded more than the chance that a landslide occurred including any anthropogenic activity index such as distance to road would probably have acted as a strong confounding factor on the analysis of hazard nevertheless we acknowledge that it would be desirable to consider anthropogenic slope modification as soon as improved inventories are available the incomplete nature of the pnli implies some uncertainty in the lhi values the relatively low modeled probabilities associated with historical landslide events might be due not only to the low frequency of historical landslides but also the uncertainly due to false negative labels in the training data it also implies that the probabilities of landslide occurrence represent a minimum level of hazard this is one reason that the lhi cannot be interpreted as a hazard map of the pacific northwest another reason is that the dominance of a handful of major events fig 4 suggests that risks from the extreme tail of the distribution are very important this implies that the 20 year inventory may be too short to accurately estimate landslide hazard for now models are a preferable way to assess hazard although the current model does underestimate the impact of two major events although ros is rare fig 6 in the areas with the most landslide reports fig 2 this does not mean that ros plays no role in triggering landslides since major landslide events are rare fig 4 infrequent ros could still be a critical factor in slope failure although we have used the pnli to evaluate the model a more holistic comparison is often warranted for example fig 1 shows a surprising lack of dated landslides in the olympic peninsula of washington frequent ros events imply a high landslide rate fig 14 as does the rugged terrain in fact numerous landslides have been catalogued in this area kirschbaum et al 2016 but the record rarely included dates by one interpretation the absence of human populations in the olympic mountains has reduced the number of landslides reported by the news media a major source of dated records this seems probable but the spatial pattern of the lhi suggests that other factors were in play fig 6 landslide triggering conditions occurred far less often in the northeast quadrant of the peninsula than along the pacific coastline it seems possible that part of the explanation for the gap in fig 1 is that landslides in this area are simply infrequent another interpretation is that the absence of human activity on the olympic peninsula has reduced the number of destabilized slopes relative to more populous regions of the pacific northwest in this hypothesis much of the reported landslide activity in the pnli has been affected by anthropogenic influence finally the model explicitly omitted rock falls which are a major process at higher elevations if these were included lhi in rugged mountains like the olympic range might have been higher we cannot resolve the issue without a more detailed investigation but we are inclined to attribute most of the gap to an inventory bias towards populated areas the divergence between model and observations from 1999 to 2005 could be interpreted as a failure of the model to generalize outside of the training period in this view the model should have been trained with a randomized train test data split instead of the division by water years however that approach would have introduced numerous false negatives from years that probably experienced numerous landslides we attribute the gaps in the inventory prior to 2007 to lack of reporting rather than any physical phenomenon if this assumption is correct the current train test split should produce a more accurate model by including fewer labeling errors in the training data of course a much larger and more complete inventory of dated landslides would obviate this approach and allow a conventional randomized training data split dividing the data by water year also has the benefit of challenging the model s ability to function well under more extreme circumstances than those in the training data splitting the data at 2011 leaves the largest landslide events fig 4 out of the training sample if these extreme conditions were handled well that would indicate a robust model that is capable of extrapolation outside of its training data for the most part this has been achieved the model shows temporal peaks in 1996 2006 2008 and 2009 that correspond to months with more observed landslides it also identifies the unseasonably low level of landslide activity in the winter of 2009 2010 it did underestimate the peaks of landslide activity in the winters of 1995 1996 and 2007 2008 however it succeeded in identifying the month of february 1996 as the most hazardous in the entire time series the results include good predictions of most out of sample major events which should reinforce confidence in the predictions made for years with no historical landslide data xgboost is a powerful tool for prediction but it requires careful supervision to avoid spurious associations between landslides and predictors in the course of this research a preliminary model accurately predicted every grid cell for the training period 2011 2015 unfortunately it was severely overfitted fig 5 so we were forced to revise both the model parameters and input variables nevertheless the preliminary gbm performed well against independent data auc 0 95 our experience suggests that landslide scientists should be very cautious when evaluating the results of empirical modeling particularly when numerous predictors are available furthermore it demonstrates that the need for common sense in model evaluation is not limited to landslide susceptibility steger et al 2016 but also applies to dynamic landslide models while we did not attempt to develop an early warning system for the pacific northwest machine learning with multivariate inputs appears likely to advance that field of research as well uncertainty and sensitivity analyses of machine learning models could also help identify what climatic variables are most valuable for understanding landslide behavior in turn this can inform decisions about future climate research the downwards trend in ros fig 15a can be attributed to declines in both snow cover and very heavy 20 mm precipitation over the pacific northwest jasinski et al 2019 ros appears to follow the same temporal distribution as landslides fig 2 but its spatial distribution suggests that it accounts for only a small portion of landslide hazard fig 14 since ros primarily affects mountainous areas changes in snowfall have the highest impact on these locations when evaluated across the entire record lhi showed a declining trend over time fig 15b which paralleled consistent decreasing trends in the underlying hydrologic variables such as extreme rainfall jasinski et al 2019 as with ros fig 15a lhi decreased across the pacific northwest unlike ros these declines occurred in all major regions not just one climate zone although mann kendall analysis showed statistically significant trends in si fig 15c many false trend detections would occur through random chance combined with the fact that roughly equal numbers of decreasing and increasing trends were observed and the fact that no association between climatic zones and si trends was obvious other than the absence of trends along the coast we suspect that the trends in si are spurious and do not reflect a major regime shift over the past 4 decades although there has been a decreasing trend in ros and lhi it is possible that anthropogenic disturbances have increased both landslide hazard and risk in the states of washington and oregon population has been growing steadily in the pacific northwest for decades ciesin 2018 ciesin 2005 this growth has occurred broadly but the rate of growth is fastest in the most hazardous quarter of the study area fig 16 although this increase in exposed population does not guarantee an increase in risk we did not analyze vulnerability it suggests the need to enforce construction standards that can mitigate or avoid landslide damages and injuries fortunately washington and oregon have embraced landslide research and disaster preparedness more generally analyzing the nca ldas dataset produced a multi decadal overview of landslide hazard across the pacific northwest but this lagging indicator is not without limitations the lhi emphasizes the geographic or temporal extent of landslide triggering events rather than the number arrangement speed or volume of landslides at each location this emphasis as well as the coarse resolution limits its utility for deriving landslide risk and the lhi probably has little relevance to determining sediment budgets the lhi is not appropriate for calculating a landslide return period nor is it a substitute for site specific investigations prior to any construction project however lhi does serve an important purpose by enabling the comparison of landslide hazard across broad spatial and temporal scales it can also inform the inventory mapping process by identifying dates and places to look for historical landslides in the supplement we have included a listing of grid cells that could form the basis for inventory expansion through historical imagery analysis or searches of media archives lhi also serves to correct or inform the historical record by providing a view into the conditions at the time of failure for example many landslides not considered in this analysis were reported without a known trigger a high lhi value for that date and location implies that the slide was triggered by one of the predictors from the nca ldas data and a detailed examination of the model s response to inputs could identify which factors were most important for that event lhi can probably be extended to projections of future hazard and new regions although these may require additional predictors 6 conclusions antecedent conditions help describe landslide behavior not predicted solely by rainfall intensity and duration or recorded in historical inventories machine learning is an effective method for incorporating rainfall intensity atmospheric rivers antecedent soil moisture and melting snow from land data assimilation systems into a unified indicator of rainfall triggered landslide hazard multivariate modeling reveals that landslides follow general geographic and temporal patterns but with substantial interannual and local variability west of the cascade range landslide frequency was high and the season was well defined with activity centered on the month of january east of the cascades landslide season was not as strongly defined and the frequency of hazardous conditions was much lower although some landslides do occur in summer this is not the dominant period for any location in the pacific northwest the lhi serves well as a guide to landslide seasonality because it acts as a consistent measure across time and space that can be easily compared to the frequency of historical reports software accessibility we used free open source software to perform this analysis we have also provided the model dump file to ensure full replicability of this study s results xgboost has been documented thoroughly in an academic paper chen and guestrin 2016 as well as an online manual at https xgboost readthedocs io although xgboost is written in c a wide variety of interfaces are now available we accessed its functions through scripts written in the python language on a linux virtual machine with 10 central processing units declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was made possible thanks to the contributions of those at nasa goddard space flight center usgs oregon water science center dogami odot wadnr and the other federal state and local groups responsible for rigorous efforts in comprehensively inventorying landslides in the pacific northwest we would like to specifically acknowledge those individuals whose insights efforts and participation greatly improved our research including jordan psaltakis nasa for assembling the pnli bill burns dogami for providing insight into historical landslides in oregon and benjamin mirus usgs for providing critical feedback on the aims modeling approach and limitations of this research this work was supported by the nasa national climate assessment project grant nnh14zda001n inca climate indicators and data products for future national climate assessments and by the nasa science mission directorate earth science division support of the us global change research program usgcrp national climate assessment nca ldas daily data products were obtained from https disc gsfc nasa gov datasets ncaldas noah0125 d 2 0 summary nca ldas trends products were obtained from https disc gsfc nasa gov datasets ncaldas noah0125 trends 2 0 summary appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104692 
26018,the u s pacific northwest has a history of frequent and occasionally deadly landslides caused by various factors using a multivariate machine learning approach we combined a pacific northwest landslide inventory with a 36 year gridded hydrologic dataset from the national climate assessment land data assimilation system to produce a landslide hazard indicator lhi on a daily 0 125 degree grid the lhi identified where and when landslides were most probable over the years 1979 2016 addressing issues of bias and completeness that muddy the analysis of multi decadal landslide inventories the seasonal cycle was strong along the west coast with a peak in the winter but weaker east of the cascade range this lagging indicator can fill gaps in the observational record to identify the seasonality of landslides over a large spatiotemporal domain and show how landslide hazard has responded to a changing climate keywords xgboost washington oregon land data assimilation system gradient boosting machine national climate assessment 1 introduction landslides have caused substantial property losses and fatalities in the pacific northwest of the united states fortunately the states of washington and oregon have invested in landslide research as have federal and local agencies an important fruit of this effort landslide inventories have already been of great assistance to urban planning testing of early warning systems and prioritizing future research in this manuscript we show how machine learning can describe the seasonality of landslides over a broad region by fusing landslide inventories and land data assimilation systems in the city of seattle most landslides happen in winter especially the month of january laprade and tubbs 2008 a much shorter record of landslides in the city of portland oregon showed frequent landslide occurrence from december to march mirus et al 2018b a similar pattern was found in a pacific northwest landslide inventory pnli that included data from portland seattle and many other locations kirschbaum et al 2016 however data on the timing of landslides are rare outside of metropolitan areas furthermore major temporal gaps can be seen in the pnli especially in the years 1979 1995 and 1998 2005 stanley et al 2019 given the diversity of terrains and climates across the pacific northwest it seems unwise to assume that all locations follow the seasonal pattern established within two well documented metropolitan regions instead landslide models must fill gaps in the recorded history of the pacific northwest previous research has identified atmospheric rivers long plumes of water vapor in the troposphere as the principal cause of major floods in western washington neiman et al 2011 and landslides along the west coast biasutti et al 2016 cordeira et al 2019 however atmospheric rivers reaching the coast of washington or oregon are most frequent in november gershunov et al 2017 which is quite early seasonally relative to the historical landslide climatology although the duration intensity and frequency of atmospheric rivers along the u s west coast do not follow the same climatology gershunov et al 2017 vertically integrated horizontal vapor transport also peaks early in the season during december and atmospheric rivers last longer early in the season recent research suggests that families temporal clusters of atmospheric rivers are substantially more hazardous than lone atmospheric rivers ogle et al 2018 taken together these findings suggest that individual storms are not sufficient to explain patterns of landslide occurrence antecedent conditions must also be considered researchers have long been aware of the importance of antecedent rainfall to hydrologic response in this region istok and boersma 1986 in the city of seattle landslide occurrence was estimated by combining 3 days of recent and 15 days of antecedent precipitation chleborad 2000 more recently an exhaustive analysis revealed that 32 days of antecedent precipitation better identified landslide events in much of the seattle metro area scheevel et al 2017 applying a similar recent antecedent ra rainfall threshold across the pacific northwest resulted in numerous successfully identified landslides but the model s false positive rate fpr was quite high in some locations and the monthly climatology overemphasized the beginning of landslide season in november stanley et al 2017 since antecedent rainfall functions not as the direct cause of landslides but as a proxy for groundwater state an antecedent wetness index was created to describe the accumulation of water in a slope godt et al 2006 this index was combined with the intensity and duration of rainstorms to predict most major landslide events in seattle washington subsequent research showed that replacing the antecedent rainfall component of a ra threshold with soil saturation measured by subsurface probes reduced the frequency of false positives especially in the summer and fall mirus et al 2018a melting snow can also be a risk factor for landslides chleborad 2000 snow depth has been incorporated in a prototype landslide early warning system for seattle chleborad et al 2008 in particular rain on snow ros can generate massive runoff and landslide events chleborad 2000 marks et al 1998 beyond these well established variables other factors such as air temperature chleborad 2000 1998 1997 soil temperature tubbs 1974 and wind buma and johnson 2015 have been identified as predictors of landslides in mountainous or coastal areas of the western united states a global analysis of satellite precipitation estimates indicated that landslide behavior of the western united states is not well explained by rainfall alone which suggests that other factors are important in this region jia et al 2019 although intense rainfall is an important trigger of landslides in the pacific northwest many other factors help to explain the spatiotemporal patterns of landslide occurrence prior landslide models rarely considered all of these time varying factors but the failure to do so can result in outputs that are climatologically incorrect or confined to a specific place machine learning offers a way to model landslide occurrence in a consistent objective manner without omitting major hydrological variables machine learning has been applied in landslide research for over a decade lee et al 2002 marjanovic et al 2009 the vast majority of this research focused on static landslide susceptibility korup and stolle 2014 but machine learning was also applied to dynamic landslide prediction farahmand and aghakouchak 2013 prediction of landslide displacement from in situ data lian et al 2014 zhao et al 2018 and the remote sensing of landslides chen et al 2014 ghorbanzadeh et al 2019 stumpf and kerle 2011 most scientists reported good to excellent predictive performance korup and stolle 2014 lun et al 2017 however researchers should take steps to avoid overfitting brenning 2005 and evaluate the geomorphic plausibility of model outputs steger et al 2016 researchers must also consider the potential effects of incomplete or inaccurate landslide inventories upon both the training and testing of empirical models stanley and kirschbaum 2017 steger et al 2017 despite these notes of caution machine learning is a powerful tool for understanding complex climate data and predicting rare events like landslides we describe the seasonality of landslides with a landslide hazard indicator lhi that scales well across space and time here lhi is defined as the daily probability that a landslide will occur averaged over a given space time domain at coarser temporal resolutions this lagging indicator is similar but not identical to landslide hazard the yearly probability of landslide occurrence is greater than the mean of the daily probabilities lhi offers a few benefits for landslide research first it is directly comparable to observed frequencies of a boolean variable this means that for a well calibrated model the mean of predictions and observations will be equal second the simplicity of calculation leads to a simplicity of interpretation the values shown are always the mean daily probability over a given area in this case 0 125 degree grid cells in contrast using the number of predicted landslides as an indicator might result in numerous cases of fractional landslides while return periods are notorious for misinterpretation read and vogel 2015 sayers 2016 vogel et al 2014 third lhi enables comparisons across space time and even other variables even geographic areas of unequal size or time periods of unequal length could be compared lhi identifies the times and places at the greatest hazard with a consistent quantitative metric that facilitates the search for patterns across space and time 2 data 2 1 national climate assessment land data assimilation system nca ldas the nca ldas is a terrestrial hydrology satellite data assimilation system that was created to support the national climate assessment nca jasinski et al 2019 kumar et al 2018 xia et al 2012 developed within the nasa land information system lis modeling framework kumar et al 2006 nca ldas integrates input data from multiple sources including a 36 year record of satellite observations to provide gridded daily time series over the continental u s of land surface variables including precipitation runoff latent and sensible heat fluxes soil moisture at multiple depths and snow water equivalent nca ldas version 2 0 employs the noah version 3 3 land surface model ek et al 2003 and is driven by atmospheric forcing data from the north american land data assimilation system phase 2 nldas 2 xia et al 2012 nca ldas data products are provided at 0 125 degree resolution for the satellite era water years 1980 2015 jasinski et al 2018 the nldas 2 precipitation forcing field is a product of a temporal disaggregation of a gauge only climate prediction center analysis of daily precipitation chen et al 2008 higgins et al 2000 and includes an orographic adjustment daly et al 1994 the rest of the forcing fields used by nca ldas including surface temperature radiation wind and humidity were derived from the analysis fields of the national centers for climate protection north american regional reanalysis lis generated a variety of hydrological variables from these forcings including soil moisture and snow depth lis performed simultaneous assimilation of satellite data with a 1 d ensemble kalman filter this data assimilation process systematically improved the accuracy of soil moisture and snow depth especially when data from newer sensors were available kumar et al 2018 analysis of these data revealed predominantly declining trends across the pacific northwest in several variables relevant to landslide activity including heavy precipitation snow cover and runoff jasinski et al 2019 because precipitation thresholds for landslide warning have been shown to vary widely over space baum and godt 2009 we rescaled rainfall and all other nca ldas variables in proportion to each grid cell s full range r e s c a l e d g r i d c e l l v a l u e g r i d c e l l v a l u e g r i d c e l l m i n i m u m g r i d c e l l m a x i m u m g r i d c e l l m i n i m u m 10 20 a small value was added to the denominator to avoid divide by zero errors in rare cases for some machine learning techniques rescaling or normalizing data greatly improves performance but this is not required for the tree based method presented below this preprocessing provides a strong localization effect that respects both the effects of rainfall frequency and maximum intensity and it is easily reversed interpreted and replicated nevertheless we do not claim that this is the optimal method for model localization future research may show that a more sophisticated approach delivers a benefit to model performance 2 2 3dep although very high resolution digital elevation models dem have been derived from lidar over much of the pacific northwest these data did not provide complete coverage the coarse resolution of the nca ldas forcing variables implied that a relatively coarse dem with a high level of consistency would be more appropriate the 3dep dataset with a resolution of 0 333 arcseconds u s geological survey 2016 combines a high level of consistency and trust with a resolution that is still relevant to landslide behavior in order to represent terrain at the modeling resolution of 0 125 we calculated a relief value by subtracting the lowest from the highest elevation within each 0 125 degree grid cell this range of elevations does not capture the geomorphological nuances useful for landslide modeling at the local scale but it does provide a simple topographic index to modulate the response to rainfall and other triggers as our primary goal was to examine the role of climate in landslide activity additional topographic variables were not added to the model 2 3 ecoregions in order to summarize and disaggregate landslide seasonality over the pacific northwest we adopted the level i ecoregions dataset u s environmental protection agency 2010 the commission for environmental cooperation developed this ecological classification to address environmental issues shared by canada mexico and the united states level i is the coarsest classification with 15 ecoregions that cover the entirety of north america only three of these zones occur in the states of oregon and washington the marine west coast forest the northwestern forested mountains and the north american deserts fig 1 2 4 pnli a landslide inventory for the pacific northwest was created by merging data from several existing inventories kirschbaum et al 2016 as well as collecting new landslide reports from news media and other sources most of the landslides in the pnli originated in the global landslide catalog kirschbaum et al 2015 2010 the statewide landslide information database for oregon burns 2014 and landslide inventories from the washington department of natural resources sarikhan et al 2009 sarikhan et al 2008 washington division of geology and earth resources 2016 over 85 of the inventoried landslides were considered shallow landslides or debris flows in order to determine relationships between historical landslide occurrence and possible explanatory variables from nca ldas we converted the 14 495 landslides with known dates to a gridded dataset with the same temporal and spatial resolution as nca ldas unfortunately this process required omission of most data in the source inventories because these often prehistoric landslides do not have specific known dates in this paper we shall refer to this gridded record of landslide occurrence as the pnli although pnli has previously been used to describe the underlying vector data the pnli is affected by a reporting bias towards populated areas and highways as the date of landslide events is much more likely to be known when humans are impacted kirschbaum et al 2016 this problem is most salient in washington s olympic peninsula where the pnli does not reflect the large number of landslides with known locations but unknown dates of occurrence fig 2 in addition the pnli is systematically biased towards the present since data was collected more consistently over the past decade thus the pnli alone is inadequate to provide a record of the climate s effect on landslide behavior over multiple decades we sought to address this problem by developing a landslide model that responds to changes in a multivariate spatially consistent gridded climate record despite these limitations some general patterns can be observed in the pnli landslides have been reported much more frequently to the west of the cascade mountains fig 2 furthermore landslide activity followed a clear seasonal cycle fig 3 with the most frequent landslide activity in the winter months of december january and february landslides were also common in march and november the north american deserts did not follow this cycle but very few landslides were reported in this ecoregion landslides were most frequent in the marine west coast forest ecoregion although the difference was less pronounced from april to september the distribution of landslides was highly skewed fig 4 just eight days over the years 1996 2015 accounted for 87 of landslide grid cells as well as almost 90 of historical landslides from 1996 to 2016 stanley et al 2017 no landslides were reported on the vast majority of days 6 820 during this period the dominance of a handful of landslide events suggests the importance of long records to a good understanding of landslide hazard 3 methods 3 1 gradient boosting from the wide variety of machine learning software we selected xgboost a form of decision tree model that uses gradient boosting to achieve a good empirical fit xgboost has become the first choice of many data scientists due to its well documented speed and accuracy chen and guestrin 2016 furthermore landslide scientists have applied it to both susceptibility mapping chakraborty et al 2019 and displacement monitoring zhao et al 2018 xgboost is a gradient boosting machine gbm a class of models that optimizes classification performance by combining many weakly predictive models into a single highly accurate ensemble the algorithm relies upon multiple iterations each of which corrects to the model produced in the previous step a process known as boosting a gbm optimizes performance by following the steepest gradient along a differentiable loss function friedman 2001 with this method scientists can fit a model that is highly predictive of a training dataset however overfitting fig 5 can reduce the performance of the model on new data this is commonly addressed by random subsampling of the training data a technique which can greatly improve the ability of a gbm to generalize over new data friedman 2002 in addition xgboost addresses this issue by regularizing the objective function chen and guestrin 2016 i e it adds a term that represents model complexity to the loss function that describes classification error l  i l y i y  i k  f k here l is the loss function that compares the predicted value y  i in this case the probability that a landslide has occurred to the observed value y i 1 if a landslide was recorded otherwise 0 and  represents the complexity of each tree added to the model f k in xgboost this regularization is primarily applied through the  parameter which requires each new split to improve the existing tree by a minimum amount although we utilized both subsampling and objective regularization when training the model these were inadequate to obtain a stable and generally applicable prediction of landslide activity fortunately xgboost offers several additional parameters the rate at which machine learning proceeds can be directly controlled with the  parameter also known as shrinkage or learning rate this number represents the ratio by which the output of each iteration will be reduced thus increasing  will speed up machine learning and result in a model with a small number of strongly predictive trees in contrast decreasing  will slow learning and produce more trees each of which is less fitted to the data but can have improved prediction as a whole in practice larger ensembles of trees produce more accurate and reliable outputs the default  of 0 3 is excellent for exploratory work but data scientists often lower the value to achieve better generalization we set  to 0 01 for the final model in our application although xgboost offers several parameters to mitigate overfitting monotonicity constraints were especially effective although this parameter has been used in cybersecurity research incer et al 2018 we believe that landslide scientists have not applied it previously this may be because it nullifies an important advantage of machine learning over linear regression the ability to model non monotonic relationships this loss may reduce performance on existing data but it can increase the model s reliability for new conditions incer et al 2018 fortunately researchers can control the presence and direction of monotonic constraints for every input variable even very conservative values for  and other parameters failed to eliminate overfitting so we applied monotonic constraints to all predictors for which a direction could be assigned with confidence from prior knowledge table 1 other variables were not included in the model we also made a simplification of soil temperature which is available for each soil layer in the nca ldas record rather than include four related variables that might not have a monotonic relationship with landslide probability we checked if the temperature of any layer was below 0 c although variable selection and monotonicity introduced a certain amount of subjectivity into the modeling process the increase in reliability made it the best choice for obtaining a regional landslide climatology the difference between the number of grid cells with and without recorded landslides known as class imbalance is an important challenge in landslide research stumpf and kerle 2011 van den eeckhaut et al 2009 as well as many other domains dynamic landslide modelers must contend with a much greater imbalance than is typically encountered in susceptibility mapping because exact dates are not available for most inventoried landslides thus the inventories of dated landslides are much smaller than the undated landslide inventories used for susceptibility mapping furthermore the limited stockpile of landslide events must be divided across thousands of time steps in this case the ratio of landslides to other pixels is 7 x 10 5 the strength of this class imbalance destabilized gradient descent even with a low  fortunately xgboost provides two tools to address class imbalance the first scale pos weight can raise the importance of rare events during model training this can increase predictive ability song et al 2018 but will produce incorrect probability estimates xgboost developers 2016 in order to avoid this problem we focused on the second parameter max delta step which limits the strength of updates performed in each iteration of training this differs from  in that it applies a hard limit to each update rather than rescaling the value a value of 10 produced a much more stable series of updates when training this landslide model table 2 shows the full list of parameter choices used to generate a landslide climatology of the pacific northwest in order to achieve the most accurate model we limited the training data to the most recent section of the pnli water years 2011 2015 we withheld water years 1996 2010 to evaluate the performance of the trained model this time period contains two very large landslide events and relatively few small landslide events since these events are outside the range of data for 2011 2015 they represent a difficult test for the model to overcome most of these early events were recorded in oregon the remainder of the pnli is not a complete record of water year 2016 so it was not included in either the training or validation datasets the entirety of the states of washington and oregon were utilized but the coarse resolution of the nca ldas dataset meant that a few landslides were located within water pixels and did not contribute to either training or testing the model 3 2 landslide climatology while the process of building machine learning models can in itself be informative we produced a landslide hazard indicator in order to understand the historical patterns of landslide behavior across the pacific northwest at the most basic level we averaged the lhi over the full time series 1979 2016 at each location to produce a map that shows where landslides would be expected more frequently we applied a similar analysis over time by averaging lhi for each month across the whole study area we also produced a climatology for each ecoregion by determining the long term mean lhi within each zone and month these methods could be repeated for any subset of the study domain in order to understand the climatology of landslides in more detail we calculated a seasonality index si that rates the temporal concentration of the lhi at each grid cell although this can be produced on a monthly basis markham 1970 it can also be calculated from daily data burn 1997 the si is determined by treating each unit of time as an angle on the unit circle and assigning a length equal to the property of interest in this case lhi next the resultant of these vectors is divided by the annual total to produce the si value thus a uniform distribution of lhi across the year would produce a si of zero while a perfect concentration of all landslides in a single month or day would produce a si of one in addition the mean direction of the vectors can be determined and converted back to the relevant unit of time landslide activity does not necessarily peak on the resulting date rather it is the center of the landslide season as a whole mean direction is more reliable for sites with high si values low values may indicate a lack of consistent seasonality and thus the average date might be uninformative for these sites this range of climatological measures shows where and when landslides were likely to happen 3 3 co occurrence of rain on snow and landslides since rain falling on snow has been identified as a major cause of landslides in this region we also analyzed the historical frequency of these events drawing on a previously published model chleborad et al 2008 we defined ros as days in every grid cell where greater than 25 4 mm of rain fell on a layer of snow deeper than 150 mm the historical frequency of landslides 3 0 10 3 in these events is roughly two orders of magnitude higher than the overall baseline for the study area 8 5 10 5 it is also slightly higher than the frequency of landslides when rainfall is greater than 25 4 mm and snow depth is not greater than 150 mm 2 8 10 3 in addition a threshold of 25 mm recent rainfall has been identified as a rough minimum trigger for landslide activity across this region stanley et al 2017 we computed the long term frequency of ros days at each location 3 4 trends in rain on snow and landslide seasonality in addition to the primary analyses we also calculated the trends over time in ros the lhi and the si derived from lhi the rate of change was calculated with the theil sen method for slope estimation theil 1992 with mann kendall significance testing kendall 1948 mann 1945 4 results 4 1 gradient boosting model evaluation the model produced maps of lhi with a daily 0 125 degree resolution typically the western third of the study area showed the highest probability of landslide occurrence fig 6 but this was not always the case fig 7 this geographic pattern broadly matches the historical record of landslides fig 3 standard quantitative measures also indicate success in reproducing the observations in the pnli binary predictions including landslide maps are often judged with the curve from the receiver operating characteristic petschko et al 2014 although the shape of the curve can convey more complex information the area under this curve auc serves as a standard measure of performance results of the model comparison have an auc of 0 91 for the model training period 2011 2015 and an auc of 0 95 for the evaluation period 1996 2010 suggesting excellent overall performance the process of model development is iterative and we tested multiple models against the validation data in order to ensure complete independence we did not test predictions for the year 2016 during the model development process therefore we obtained two small inventories of dated landslides nasa s global landslide catalog glc and recent landslides in washington state washington dnr 2019 rock falls were removed from both inventories and landslides with a spatial accuracy worse than 10 km were removed from the glc we benchmarked the new xgboost model against a simple recent antecedent ra rainfall threshold that was fitted to this study area stanley et al 2017 the ra threshold predicted 37 of the glc landslides and 31 of the recent landslides in washington state with a fpr of 6 in order to compare these binary results to the probabilistic outputs of xgboost we applied a probability threshold of 0 00033126 which generates a binary output with the same fpr as the ra threshold but a different spatiotemporal structure with this binary output xgboost correctly predicted 55 of the glc and 69 of the washington landslides since quantitative metrics can give false confidence in empirical models steger et al 2016 we also sought to examine the plausibility of the relationships between model inputs and outputs xgboost provides a few default measures of variable importance the most useful is gain the average error reduction made by introducing each variable to the model chen and guestrin 2016 this measure shows that rainfall is the most useful predictor of landslides while snowfall is the least fig 8 soil moisture and temperature also play important roles in modulating the output 4 2 landslide climatology lhi provides an overview of landslide seasonality across the whole of the pacific northwest as shown in fig 6 landslides were more probable along the west coast landslide triggering conditions also occur frequently on the west side of the cascades the temporal pattern of landslide behavior was revealed by calculating the monthly mean lhi of all grid cells in the study area fig 9 the highest monthly lhi from 1979 to 2016 was recorded for february 1996 which was also the peak of landslide activity recorded in the pnli the pnli does not contain any landslides with precise dates reported before 1996 other notable peaks in lhi can be seen in december 1981 february 1986 december 1996 february 1999 and january 2006 lhi typically peaked in winter fell to very low levels in the summer and did not rise until late in the fall we also analyzed the monthly climatology within each major ecoregion fig 10 lhi was highest in the marine west coast forest ecoregion and lowest in the north american deserts ecoregion lhi peaked in the marine west coast forest and northwestern forest mountains during december and january but march was the peak month in north american deserts calculation of si and mean direction of lhi at each grid cell corroborated this climatology fig 11 the average date of the local landslide season was typically in the winter months of december january and february for locations west of the cascades landslide season was less consistent east of the cascades the average dates were spread over fall winter or spring however lhi did not peak during summer anywhere in the study area si was very high along the west coast indicating a strong seasonal pattern to lhi in this area fig 12 in some locations si exceeded 0 7 east of the cascades si was more variable in some of these areas si exceeded 0 6 while si 0 1 at many locations the largest area without a consistent landslide season was located in central washington si was also low over much of eastern oregon overall this climatological analysis showed that landslide hazard was most severe in winter that lhi was declining but highly variable and that the seasonal cycle was more consistent along the west coast 4 3 co occurrence of rain on snow and landslides rain falls on snow most frequently during the winter months of december january and february fig 13 the months of march and november represent a transition between summer and winter conditions with a lesser number of ros days this season matches the landslide cycle quite well fig 3 ros was most common in the olympic and cascade ranges particularly the north cascades fig 14 the remainder of the study area experiences ros only on rare occasions this geographic distribution does not match that of historically recorded landslides fig 2 specifically no dated landslides were recorded in the olympic mountains even though this area experiences ros frequently in addition large numbers of landslides have been recorded in the coast range and cascades of oregon ros occurs rarely in most of these areas 4 4 trends we found statistically significant declining trends in the annual number of ros days at many locations fig 15 a the rate of change was no more than a half day per year over the period 1979 2016 in most locations it was much slower very few locations experienced increased ros statistical significance was largely confined to the areas with frequent ros the overall picture is one of broadly declining numbers of ros days at higher elevations and infrequent ros at lower elevations and in drier regions although statistically significant trends were not found at most grid cells lhi declined at most locations on both sides of the cascade range and increased in relatively few locations fig 15b statistically significant trends in si were relatively uncommon even at the 0 1 level fig 15c si increased and decreased in roughly equal numbers of locations nearly all of these were located east of the cascades however neither the increases nor the declines were clustered in a specific area 5 discussion while we implicitly or explicitly modeled a wide range of triggers pre conditioning factors and landslide types some topics were not analyzed rock falls were explicitly omitted from the pnli and wave action on coastal bluffs was not modeled we did not consider the cascading impacts from volcanoes which means the results do not show potentially substantial if infrequent lahar hazards from the cascades while wildfires are probably not a major destabilizing factor in this region the omission of burned areas as a predictor might have a confounding effect on model performance in some locations perhaps the most important omission was the absence of anthropogenic terrain modification from the list of predictors human activities can have a profoundly destabilizing effect and they have played a major role in landslide activity from local chleborad et al 2008 to global scales froude and petley 2018 the challenge here was differentiating the true effects of human activity from the apparent effects that are due to inventory bias landslides were much more commonly recorded near highways in the pnli kirschbaum et al 2016 but this probably reflects the chance that a landslide was recorded more than the chance that a landslide occurred including any anthropogenic activity index such as distance to road would probably have acted as a strong confounding factor on the analysis of hazard nevertheless we acknowledge that it would be desirable to consider anthropogenic slope modification as soon as improved inventories are available the incomplete nature of the pnli implies some uncertainty in the lhi values the relatively low modeled probabilities associated with historical landslide events might be due not only to the low frequency of historical landslides but also the uncertainly due to false negative labels in the training data it also implies that the probabilities of landslide occurrence represent a minimum level of hazard this is one reason that the lhi cannot be interpreted as a hazard map of the pacific northwest another reason is that the dominance of a handful of major events fig 4 suggests that risks from the extreme tail of the distribution are very important this implies that the 20 year inventory may be too short to accurately estimate landslide hazard for now models are a preferable way to assess hazard although the current model does underestimate the impact of two major events although ros is rare fig 6 in the areas with the most landslide reports fig 2 this does not mean that ros plays no role in triggering landslides since major landslide events are rare fig 4 infrequent ros could still be a critical factor in slope failure although we have used the pnli to evaluate the model a more holistic comparison is often warranted for example fig 1 shows a surprising lack of dated landslides in the olympic peninsula of washington frequent ros events imply a high landslide rate fig 14 as does the rugged terrain in fact numerous landslides have been catalogued in this area kirschbaum et al 2016 but the record rarely included dates by one interpretation the absence of human populations in the olympic mountains has reduced the number of landslides reported by the news media a major source of dated records this seems probable but the spatial pattern of the lhi suggests that other factors were in play fig 6 landslide triggering conditions occurred far less often in the northeast quadrant of the peninsula than along the pacific coastline it seems possible that part of the explanation for the gap in fig 1 is that landslides in this area are simply infrequent another interpretation is that the absence of human activity on the olympic peninsula has reduced the number of destabilized slopes relative to more populous regions of the pacific northwest in this hypothesis much of the reported landslide activity in the pnli has been affected by anthropogenic influence finally the model explicitly omitted rock falls which are a major process at higher elevations if these were included lhi in rugged mountains like the olympic range might have been higher we cannot resolve the issue without a more detailed investigation but we are inclined to attribute most of the gap to an inventory bias towards populated areas the divergence between model and observations from 1999 to 2005 could be interpreted as a failure of the model to generalize outside of the training period in this view the model should have been trained with a randomized train test data split instead of the division by water years however that approach would have introduced numerous false negatives from years that probably experienced numerous landslides we attribute the gaps in the inventory prior to 2007 to lack of reporting rather than any physical phenomenon if this assumption is correct the current train test split should produce a more accurate model by including fewer labeling errors in the training data of course a much larger and more complete inventory of dated landslides would obviate this approach and allow a conventional randomized training data split dividing the data by water year also has the benefit of challenging the model s ability to function well under more extreme circumstances than those in the training data splitting the data at 2011 leaves the largest landslide events fig 4 out of the training sample if these extreme conditions were handled well that would indicate a robust model that is capable of extrapolation outside of its training data for the most part this has been achieved the model shows temporal peaks in 1996 2006 2008 and 2009 that correspond to months with more observed landslides it also identifies the unseasonably low level of landslide activity in the winter of 2009 2010 it did underestimate the peaks of landslide activity in the winters of 1995 1996 and 2007 2008 however it succeeded in identifying the month of february 1996 as the most hazardous in the entire time series the results include good predictions of most out of sample major events which should reinforce confidence in the predictions made for years with no historical landslide data xgboost is a powerful tool for prediction but it requires careful supervision to avoid spurious associations between landslides and predictors in the course of this research a preliminary model accurately predicted every grid cell for the training period 2011 2015 unfortunately it was severely overfitted fig 5 so we were forced to revise both the model parameters and input variables nevertheless the preliminary gbm performed well against independent data auc 0 95 our experience suggests that landslide scientists should be very cautious when evaluating the results of empirical modeling particularly when numerous predictors are available furthermore it demonstrates that the need for common sense in model evaluation is not limited to landslide susceptibility steger et al 2016 but also applies to dynamic landslide models while we did not attempt to develop an early warning system for the pacific northwest machine learning with multivariate inputs appears likely to advance that field of research as well uncertainty and sensitivity analyses of machine learning models could also help identify what climatic variables are most valuable for understanding landslide behavior in turn this can inform decisions about future climate research the downwards trend in ros fig 15a can be attributed to declines in both snow cover and very heavy 20 mm precipitation over the pacific northwest jasinski et al 2019 ros appears to follow the same temporal distribution as landslides fig 2 but its spatial distribution suggests that it accounts for only a small portion of landslide hazard fig 14 since ros primarily affects mountainous areas changes in snowfall have the highest impact on these locations when evaluated across the entire record lhi showed a declining trend over time fig 15b which paralleled consistent decreasing trends in the underlying hydrologic variables such as extreme rainfall jasinski et al 2019 as with ros fig 15a lhi decreased across the pacific northwest unlike ros these declines occurred in all major regions not just one climate zone although mann kendall analysis showed statistically significant trends in si fig 15c many false trend detections would occur through random chance combined with the fact that roughly equal numbers of decreasing and increasing trends were observed and the fact that no association between climatic zones and si trends was obvious other than the absence of trends along the coast we suspect that the trends in si are spurious and do not reflect a major regime shift over the past 4 decades although there has been a decreasing trend in ros and lhi it is possible that anthropogenic disturbances have increased both landslide hazard and risk in the states of washington and oregon population has been growing steadily in the pacific northwest for decades ciesin 2018 ciesin 2005 this growth has occurred broadly but the rate of growth is fastest in the most hazardous quarter of the study area fig 16 although this increase in exposed population does not guarantee an increase in risk we did not analyze vulnerability it suggests the need to enforce construction standards that can mitigate or avoid landslide damages and injuries fortunately washington and oregon have embraced landslide research and disaster preparedness more generally analyzing the nca ldas dataset produced a multi decadal overview of landslide hazard across the pacific northwest but this lagging indicator is not without limitations the lhi emphasizes the geographic or temporal extent of landslide triggering events rather than the number arrangement speed or volume of landslides at each location this emphasis as well as the coarse resolution limits its utility for deriving landslide risk and the lhi probably has little relevance to determining sediment budgets the lhi is not appropriate for calculating a landslide return period nor is it a substitute for site specific investigations prior to any construction project however lhi does serve an important purpose by enabling the comparison of landslide hazard across broad spatial and temporal scales it can also inform the inventory mapping process by identifying dates and places to look for historical landslides in the supplement we have included a listing of grid cells that could form the basis for inventory expansion through historical imagery analysis or searches of media archives lhi also serves to correct or inform the historical record by providing a view into the conditions at the time of failure for example many landslides not considered in this analysis were reported without a known trigger a high lhi value for that date and location implies that the slide was triggered by one of the predictors from the nca ldas data and a detailed examination of the model s response to inputs could identify which factors were most important for that event lhi can probably be extended to projections of future hazard and new regions although these may require additional predictors 6 conclusions antecedent conditions help describe landslide behavior not predicted solely by rainfall intensity and duration or recorded in historical inventories machine learning is an effective method for incorporating rainfall intensity atmospheric rivers antecedent soil moisture and melting snow from land data assimilation systems into a unified indicator of rainfall triggered landslide hazard multivariate modeling reveals that landslides follow general geographic and temporal patterns but with substantial interannual and local variability west of the cascade range landslide frequency was high and the season was well defined with activity centered on the month of january east of the cascades landslide season was not as strongly defined and the frequency of hazardous conditions was much lower although some landslides do occur in summer this is not the dominant period for any location in the pacific northwest the lhi serves well as a guide to landslide seasonality because it acts as a consistent measure across time and space that can be easily compared to the frequency of historical reports software accessibility we used free open source software to perform this analysis we have also provided the model dump file to ensure full replicability of this study s results xgboost has been documented thoroughly in an academic paper chen and guestrin 2016 as well as an online manual at https xgboost readthedocs io although xgboost is written in c a wide variety of interfaces are now available we accessed its functions through scripts written in the python language on a linux virtual machine with 10 central processing units declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was made possible thanks to the contributions of those at nasa goddard space flight center usgs oregon water science center dogami odot wadnr and the other federal state and local groups responsible for rigorous efforts in comprehensively inventorying landslides in the pacific northwest we would like to specifically acknowledge those individuals whose insights efforts and participation greatly improved our research including jordan psaltakis nasa for assembling the pnli bill burns dogami for providing insight into historical landslides in oregon and benjamin mirus usgs for providing critical feedback on the aims modeling approach and limitations of this research this work was supported by the nasa national climate assessment project grant nnh14zda001n inca climate indicators and data products for future national climate assessments and by the nasa science mission directorate earth science division support of the us global change research program usgcrp national climate assessment nca ldas daily data products were obtained from https disc gsfc nasa gov datasets ncaldas noah0125 d 2 0 summary nca ldas trends products were obtained from https disc gsfc nasa gov datasets ncaldas noah0125 trends 2 0 summary appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104692 
26019,google earth engine gee is an ideal platform for large scale geospatial agricultural and environmental modeling based on its diverse geospatial datasets easy to use application programming interface api rich reusable library and high performance computational capacity however using gee to prepare geospatial data requires not only the skills of programming languages like javascript and python but also the knowledge of gee apis and data catalog this paper presents the agkit4ee toolkit to facilitate the use of the cropland data layer cdl product over the gee platform this toolkit contains a variety of frequently used functions for use of cdl including crop sequence modeling crop frequency modeling confidence layer modeling and land use change analysis the experimental results suggest that the proposed software can significantly reduce the workload for modelers who conduct geospatial agricultural and environmental modeling with cdl data as well as developers who build the gee enabled geospatial cyberinfrastructure for agricultural land use modeling of the conterminous united states agkit4ee is an open source and it is free to use modify and distribute the latest release of agkit4ee can be imported to any modeling workflow developed using gee code editor https code earthengine google com accept repo users czhang11 agkit4ee the source code examples documentation user community and wiki pages are available on github https github com czhang11 agkit4ee keywords google earth engine cropland data layer land use modeling crop mapping geospatial cyberinfrastructure 1 introduction the conterminous united states conus which has the largest production areas of corn soybeans and sorghum in the world is an ideal study area for scientists to model natural environment and human activities in agriculture feng and hu 2004 mccarty et al 2009 li et al 2016 feng et al 2019 flynn 2019 as the only annual crop specific land use and land cover lulc data product of the conus the cropland data layer cdl of the u s department of agriculture usda national agricultural statistics service nass has been widely used by farmers educators students researchers and government officers in agricultural business universities research institutes and governments worldwide for agricultural production planning and management education government policy formulation and decision making and various research activities boryan et al 2011 stern et al 2012 mueller and harris 2013 di et al 2015 the cdl product has been disseminated through cropscape since 2011 according to the report from google analytics more than 208 000 unique users around the world have visited and interacted with cropscape as of may 2019 as an one of a kind one stop platform to visualize retrieve and analyze cdl cropscape provides not only various geospatial functionalities such as data customization and downloading crop acreage statistics charting and graphing and multi temporal change analysis but also the highly interoperable open standard based geospatial web services such as web map service wms web coverage service wcs and web processing service wps han et al 2012 zhang et al 2019d these functionalities and web services of cropscape have been utilized in many geospatial environmental models frameworks and software han et al 2014 feng et al 2015 mcnider et al 2015 groff et al 2016 tasdighi et al 2018 however due to the limited computing resources it is difficult to perform advanced modeling and mathematical operations on the multi year cdl data directly using either the cropscape web portal or its web services to fill the gap and further facilitate the use of cdl data a new web based tool for the conus scale agricultural land use modeling is needed modeling multi year cdl data for the entire conus requires massive computational capacity therefore the development of the software should be based on a high performance geospatial cyberinfrastructure ci which supports the collection management share analysis visualization and dissemination of the geospatial big data over the high speed network yang et al 2010 yue et al 2015 during the past decade many geospatial applications and tools have been transformed from the traditional geographic information system gis software to the geospatial ci with the rapid advancements in web service technologies castronova et al 2013 lin et al 2017 zhang et al 2019c geospatial information interoperability zhao and di 2010 goodall et al 2013 nativi et al 2013 sun et al 2017 geospatial cloud computing yang and huang 2013 zhang et al 2017 high performance computing lee et al 2011 and geospatial big data analytics deng and di 2014 vitolo et al 2015 di 2016 among those geospatial applications there are many geospatial cis serving for the environmental modeling community the self adaptive earth predictive systems seps adopts the service oriented architecture soa to bridge earth observation eo data and earth system models using ogc iso sensor web standards and geospatial interoperability protocols di 2007 cybergis wang 2010 a gis framework based on the advanced ci has been integrated into many gis applications and software wang et al 2013 padmanabhan et al 2014 lin et al 2015 global earth observation system of systems geoss platform links a set of eo systems around the world to facilitate the monitoring of the state of the earth and the sharing of environmental data nativi et al 2013 santoro et al 2016 as a collaboration between the division of advanced cyberinfrastructure and the geosciences directorate of national science foundation nsf earthcube a community driven organization for geoscience ci has funded many projects to improve data access sharing visualization and analysis across geoscience disciplines katz 2015 these projects include geolink krisnadhi et al 2015 cloud hosted real time data services for the geosciences chords kerkez et al 2016 brokering building block bcube khalsa 2017 cyberconnector di et al 2017 sun et al 2017 cyberway di et al 2019 hydroshare and geotrust essawy et al 2018 xue et al 2019 as one of the major players in the cloud computing business google unveiled google earth engine gee in 2010 gee is a cloud based platform for planetary scale geospatial data analysis with diverse geospatial datasets and a variety of ready to use application programming interface api gorelick et al 2017 it has been used as the major computing platform in many earth system science studies including lulc change detection hansen et al 2013 huang et al 2017 midekisa et al 2017 yu et al 2018 crop mapping shelestov et al 2017 teluguntla et al 2018 digital soil mapping padarian et al 2015 forest mapping chen et al 2017 koskinen et al 2019 and wetland mapping hird et al 2017 however prototyping a complicated modeling algorithm with gee involves the cost of learning javascript and gee apis which is a time consuming job for modelers especially those who do not have a strong technical background to address this issue gee provides the javascript python client library allowing users to develop gee enabled applications and tools as needed for example agrisuit provides a web based framework for supporting land use suitability analysis which integrating spatial datasets algorithms and computing capabilities of gee platform yalew et al 2016 collect earth online ceo offers an open source tool for systematic reference data collection in land cover and use applications bey et al 2016 flood prevention and emergency response system a gee powered web based platform for supporting flood event prevention and emergency response has been applied in 19 typhoons and torrential rain events from 2013 to 2016 in taiwan liu et al 2018 the biomass estimation platform is a cloud based application for aboveground biomass mapping and estimation which integrated gee yang et al 2018 coastsat is a gee toolkit to extract shorelines from satellite imagery in large scale vos et al 2019 li et al 2019 presents a gee enabled toolbox of generating high quality user ready landsat mosaic images in this paper we present the agkit4ee toolkit to a simplify the the conus scale agricultural land use modeling on gee b derive cdl based land use data products on the fly with gee and c boost the development of gee enabled web applications for agricultural land use modeling the rest of the paper is organized as follow section 2 introduces the architectural context data development flow and core functions section 3 presents examples to demonstrate features and capabilities of the toolkit section 4 discusses the application scenario the advantages and the limitations of the current implementation the conclusion and future works are given in section 5 2 software design 2 1 architectural context fig 1 illustrates the architectural context of the proposed software the development of the agkit4ee toolkit is fully based on gee client library and gee data catalog which are powered by the high performance computation and data stores of google s cloud infrastructure the agkit4ee library contains a suite of modules such as modeling modules statistics modules and data processing modules the library is developed using javascript which is consistent with gee code editor users can directly import the library to any project in gee code editor and gee enabled web applications currently we have enabled agkit4ee in two web application prototypes the cropland explorer and the crop frequency explorer which are published over the earth engine apps platform as the core component in the architectural context the agkit4ee library includes a variety of frequently used functions for the retrieval process modeling and statistics of cdl data all functions are developed based on the native gee apis and wrapped as the ready to use apis with required and optional arguments the code example of retrieving a cdl image collection of corn and soybeans for selected years using the getcdlcollection function as well as the equivalent code using native gee apis are compared in appendix a fig a1 this example presents a workflow of cdl data retrieval with the readable input arguments in one line of code where the product option refers to the band name of the original cdl data in gee data catalog the remap option includes a list of the value for the desired crop type codes 1 and 5 refer to corn and soybeans respectively the years option specifies the years of interest as a control the same functionality is equivalent to a series of native gee apis including image blending band selection value remapping and mapping over images it is obvious that such a simplification would significantly reduce the burdens of gee coding for modelers who are not familiar with gee apis and cdl products 2 2 data the objective of the agkit4ee toolkit is to facilitate the use of cdl data on gee the complete collection of historical cdl data has been archived in the gee data catalog https developers google com earth engine datasets catalog usda nass cdl table 1 summarizes the information of three cdl products including the cropland layer the confidence layer and the cultivated layer the cropland layer covers the entire conus from 2008 to 2018 and some states from 1997 to 2007 which is composed of over 140 land cover classes with 30m spatial resolution the confidence layer covers the entire conus from 2008 to 2018 which reflects the percentage 0 100 of confidence for each cropland pixel liu et al 2004 the cultivated layer covers the entire conus from 2013 to present which is produced based on the most recent five years of cropland layer the current year cdl data would be first released through cropscape and the usda nass website in the early next year for example the 2019 cdl which is the latest cdl data as of the writing of this article was published on january 2020 however the collection of some datasets on gee might be delayed the 2019 cdl has not been archived in gee data catalog as of march 2020 besides the cdl data the toolkit integrated a collection of boundary data that frequently used in agricultural and environmental modeling including u s county boundary u s state boundary usda agricultural statistics district asd boundary landsat world reference system 2 wrs 2 scene boundary and sentinel 2 tile boundary these boundary data will improve efficiency while preparing data for region of interest for example users can export the cdl data of specific u s state county or asd by using the exportcdlbyfips function with the specific federal information processing standards fips code 2 3 implementation gee provides two basic geospatial data structures image and imagecollection to manipulate raster data the image is a single raster image data composed by one or multiple bands the imagecollection is a stack of the image as of december 2019 the gee data catalog has archived all historical cdl data from 1997 to 2018 the cdl product of each year is saved as an image depending on the data availability table 1 each cdl image contains either one two or three bands based on image and imagecollection we defined and implemented five extended data structure optionsto manupulate cdl images they are s ingle band cdl image s ingle band cdl imagecollection s tacked cdl image m ulti bands cdl i mage and m ulti bands cdl i mage c ollection table 2 summarizes all data structures defined in agkit4ee fig 2 illustrates the architecture and the development flow of the core modules in agkit4ee the development is composed of a suite of modules and each module contains a group of functions the getcdl module provides the capability of getting original cdl data according to user requirements such as the product type year or crop type meanwhile all modeling functions are implemented based on the getcdl module according to the data product type the modeling functions are implemented as croplandmodeling module and confidencemodeling module respectively the croplandmodeling module consists of functions related to the cropland layer such as crop sequence modeling crop rotation modeling and crop frequency modeling the confidencemodeling module handles the functions of pixel level confidence percentage modeling based on the confidence layer additionally there are several miscellaneous modules offering common geospatial functions the getroi module manages the u s boundary files the export module allows user to batch export of the on demand cdl data the statistics module provides the statistical functions for agricultual land use change analyisis 2 4 functions and capabilities all functions in the agkit4ee toolkit contain one or more arguments besides the required arguments that users must assign before use most functions also have several optional arguments which are passed through the options object for example the optional arguments of the getcdlcollection function in the getcdl module consist of product years remap and defaultvalue the product option is the product name of the desired cdl layer the years option is a list of years of interest the remap option is a list of the codes of target crops the defaultvalue option is the code value of no data pixels table 3 summaries all functions offered by the current release of the toolkit 3 examples this section gives several examples of agricultural land use modeling and analysis using the agkit4ee toolkit first we demonstrate the capability of crop mapping for the entire conus using the crop sequence modeling function section 3 1 then we present an example of crop frequency modeling and compare the result with the official crop frequency map produced by usda nass section 3 2 in addition we also illustrate how to extract the high confidence cdl pixels by stratifying the historical confidence layers section 3 3 finally two agkit4ee enabled web applications are prototypically implemented and published through the earth engine apps platform section 3 4 3 1 mapping cropland by crop sequence the cropping sequence can affect crop yield edwards et al 1988 crookston et al 1991 berzsenyi et al 2000 as well as soil quality soil fertility and soil physical chemical properties janzen et al 1992 karlen et al 2006 van eerd et al 2014 triberti et al 2016 for instance the corn soybean rotation a widely adopted common cropping practice helps preserve the croplands productivity in the u s corn belt based on the reliable crop sequence information the types of crops to be planted can be predicted before the growing season starts zhang et al 2019a zhang et al 2019b these prediction and pre season crop planting information are critical to many early season environmental modeling and applications to facilitate the pre season crop mapping agkit4ee offers an innovative function to extract pixels that following the specific cropping sequence in the recent years the code example of mapping major crops based on the common crop rotation patterns using the modelingbycropsequence function can be found in appendix a fig a2 in this example the variable corn mono and corn rotation refer to the corn pixels following the monocropping and corn soybeans rotated cropping pattern the variable soybean mono soybean rotation and soybean rotation alt refer to the soybeans pixels following the monocropping corn soybeans and rice soybean rotated cropping pattern the variable cotton mono refers to the cotton pixels following the monocropping pattern the variable rice mono and rice rotation refer to the rice pixels following the monocropping pattern and rice soybeans rotated cropping pattern the variable durumwheat mono springwheat mono and winterwheat mono refer to the wheat pixels following the monocropping pattern fig 3 shows the spatial distribution of major crops extracted from the historical cdl data with the above mentioned crop sequences 3 2 mapping cropland by crop frequency usda nass releases the crop frequency data layers accompanying with the release of annual cropland layer this product identifies the planting frequency for the specific crop type based on the cdl from 2008 to present boryan et al 2014 the crop frequency data layers for four major crops in the u s corn cotton soybeans and wheat are available to the public the latest crop frequency data layer products are available on cropscape and the usda nass website the agkit4ee toolkit provides the capability of modeling crop frequency based on the historical cropland layers the code example of mapping the frequency of major crops i e corn and soybeans for the state of nebraska can be found in appendix a fig a3 the variable freq corn refers to the planting frequency map of corn and freq soybean refers to the planting frequency map of soybeans fig 4 compares the crop planting frequency maps with the nass official crop frequency data layers as shown in fig 4a and c the agkit4ee derived crop planting frequency maps contain more information than the nass crop frequency data layers as shown in fig 4b and d more importantly the toolkit provides the capability of modeling the frequency of any crop type from cdl within any available year range for example users can model the frequency of major crops for the state of arkansas in the recent five years 2014 2018 by assigning the years option of the modelingfrequencybycrop function see appendix a figure a4 fig 5 shows the mapping results of corn fig 5a cotton fig 5b rice fig 5c soybeans fig 5d winter wheat fig 5e and double cropping of winter wheat and soybeans fig 5f 3 3 mapping confidence layer by threshold the cdl data are produced using c5 0 see5 decision tree algorithm for every classified cdl pixel there is an associated classification confidence measure which measures the confidence percentage of the corresponding cdl pixel a map of the cdl pixels that are higher or lower than a specific confidence level threshold can produced by binarizing the confidence layer with the specific threshold furthermore by modeling the multi years of confidence layers we can observe the spatial distribution of the trusted cropland pixels maintaining high confidence for identifying the crop type of the corresponding pixel the code example of modeling confidence layers with the modelingconfidencebythreshold function can be found in appendix a fig a5 the variable conf 70 conf 80 conf 90 and conf 100 represent the maps of cropland pixels which the confidence value is consistently greater than 70 80 90 and 100 respectively for the given time period fig 6 shows the binarized maps of the modeling result in which the bright pixels represent the pixels higher than the threshold value 3 4 enabling agkit4ee in web applications in this study we enabled agkit4ee in two web application prototypes the cropland explorer https czhang11 users earthengine app view agkit4ee cdl explorer and the crop frequency explorer https czhang11 users earthengine app view agkit4ee crop frequency explorer these prototypes are published as the gee app over the earth engine apps platform https www earthengine app as shown in fig 7 the graphic user interface includes a configuration panel and a map explorer the app calls the specific modeling functions of the toolkit based on the user s choice and dynamically reload the on demand results on the map explorer with the cropland explorer fig 7a users can select the product layer year crop types and boundary layer on the configuration panel the prototype currently provides the crop area statistics and crop sequence statistics which are created using statisticsbyroi and statisticsbypoi functions of the statistics module for a selected region county asd state of interest and point of interest on the map explorer the time series chart of changes of crop area for the selected region of interest and crop sequence for the point of interest will be plotted on the panel similarly the crop frequency explorer fig 7b will produce the crop frequency map according to the crop type and years of interest 4 discussion 4 1 contribution of the study the agkit4ee toolkit makes the conus scale agricultural land use modeling more effectively and efficiently first of all it remedies the limitation of cropscape and provides various ready to use modeling functions such as crop sequence modeling crop frequency modeling and confidence layer modeling these functions can be coupled with many modeling workflows for example the pixels derived from the common crop sequence can be potentially used as training samples to train the classification model which would be a low cost but reliable way to produce reference data for the early season crop mapping over a large geographic area the crop frequency map can provide information regarding the potential geospatial distribution of crop planting in the future by modeling the confidence layer the current cdl users can easily find out the trusted cdl pixels over time thus better assessing the modeling result computing resource is critical to the performance of geospatial environmental modeling because of the limited computing capacity the conus scale modeling might take a few hours or days to run on a single server or personal computer by taking advantage of gee s powerful cloud computing environment it takes only a few seconds to process the cdl data for the entire conus this will save a considerable amount of time for modelers we believe that the environmental modeling community lulc community and agricultural sectors will be benefited from using gee along with agkit4ee 4 2 application scenarios the agkit4ee toolkit is designed for users who deal with cdl data on the gee platform and gee enabled web applications here are some common application scenarios first all functions can be directly requested to visualize explore and export the on demand cdl products through gee code editor another application scenario is integrating the toolkit with other modeling workflow while cdl is one of the data sources moreover the agkit4ee library can be imported as javascript module into the web frameworks in this way the implementation of gee based web applications and geospatial ci can be significantly accelerated developers can just focus on the implementation of the high level architecture without figuring out the deatils of gee apis and cdl data 4 3 data derived from cdl the agkit4ee toolkit is more than a gee extension for cdl data visualization it is characterized by many modeling functions and options the original cdl data can be fully utilized and various agricultural land use products can be derived using the toolkit table 4 compares the data offered by cropscape gee data catalog and agkit4ee the original cdl products cropland layer confidence layer and cultivated layer can be directly exported using the export module for example user can start a job to batch download the cropland layer maps from 2010 to 2018 of corn and soybeans for iowa by calling the exportcdlbyfips function in the export module all other products that derived from the cdl data e g crop frequency map and crop sequence map can be retrieved through the modeling functions summarized in table 3 then exported to the local path with the gee s built in export functions agkit4ee is an open source software with a fully extensible structure it is easy and free to extend the modules and develop more cdl based agricultural land use products as needed 4 4 limitations the current release of agkit4ee still has some limitations on the one hand computing capacity of some geospatial functions are restricted by gee for examlpe we currently only support the county level statistics due to the limited number of pixels per each process 10 million pixels allowed by gee when scaling up to the state level or larger geographic area the process will be stopped with the too many pixels in the region error a solution to bypass the limitation is to increase the scale for the reducer operation however this tweak would affect the accuracy of the statistics result a better solution is to sum up the county level results to asd or state level but it would take more processing time moreover the earth engine apps platform does not support the data export function in the current phase to export the modeling result the user must run the toolkit through the gee code editor on the other hand there were many misclassified pixels in the early year cdl data because of cloud cover and lack of satellite images also the coverage of the early year cdl was incomplete only a few states were fully covered before 2008 these quality and coverage issue of the early year cdl data can potentially affect the follow on studies zhang et al 2020 5 conclusion and future works this paper presents the design implementation and use examples of agkit4ee as a gee enabled toolkit for the conus scale agricultural land use modeling agkit4ee contains a variety of frequently used functions for retrieving visualizing modeling and analyzing cdl data the major functions and capabilities of the proposed software including crop sequence modeling crop frequency modeling confidence layer modeling and geospatial statistics were demonstrated additionally two agkit4ee enabled web applications the cropland explorer and the crop frequency explorer were prototyped and published over the earth engine apps platform the result suggests this toolkit would greatly reduce the workload of modelers and developers who deal with cdl data in the next phase of development we will enhance and extend the agkit4ee toolkit integrate more modeling functions develop the machine learning module and support more lulc data products such as the national land cover database nlcd of u s geological survey usgs a geospatial ci with all features of the agkit4ee toolkit is under development currently the toolkit is implemented in javascript only we will implement the core modules in python to support more third party applications developed with gee python api software availability software name agkit4ee developer center for spatial information science and systems george mason university technical support chen zhang czhang11 gmu edu programming language javascript license mit software required google earth engine project https code earthengine google com accept repo users czhang11 agkit4ee earth engine repository https earthengine googlesource com users czhang11 agkit4ee github repository https github com czhang11 agkit4ee cropland explorer https czhang11 users earthengine app view agkit4ee cdl explorer crop frequency explorer https czhang11 users earthengine app view agkit4ee crop frequency explorer declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is supported by a grant from national science foundation infews program grant cns 1739705 pi dr liping di the authors would like to thank two anonymous reviewers for their valuable comments appendix a code examples fig a1 example of retrieving cdl image collection using agkit4ee fig a1 fig a2 mapping major crops based on common crop rotation patterns for the entire conus fig a2 fig a3 mapping frequency of corn and soybeans for the state of nebraska fig a3 fig a4 mapping crop frequency for the state of arkansas in the recent five years fig a4 fig a4 modeling confidence layer for the entire conus fig a4 
26019,google earth engine gee is an ideal platform for large scale geospatial agricultural and environmental modeling based on its diverse geospatial datasets easy to use application programming interface api rich reusable library and high performance computational capacity however using gee to prepare geospatial data requires not only the skills of programming languages like javascript and python but also the knowledge of gee apis and data catalog this paper presents the agkit4ee toolkit to facilitate the use of the cropland data layer cdl product over the gee platform this toolkit contains a variety of frequently used functions for use of cdl including crop sequence modeling crop frequency modeling confidence layer modeling and land use change analysis the experimental results suggest that the proposed software can significantly reduce the workload for modelers who conduct geospatial agricultural and environmental modeling with cdl data as well as developers who build the gee enabled geospatial cyberinfrastructure for agricultural land use modeling of the conterminous united states agkit4ee is an open source and it is free to use modify and distribute the latest release of agkit4ee can be imported to any modeling workflow developed using gee code editor https code earthengine google com accept repo users czhang11 agkit4ee the source code examples documentation user community and wiki pages are available on github https github com czhang11 agkit4ee keywords google earth engine cropland data layer land use modeling crop mapping geospatial cyberinfrastructure 1 introduction the conterminous united states conus which has the largest production areas of corn soybeans and sorghum in the world is an ideal study area for scientists to model natural environment and human activities in agriculture feng and hu 2004 mccarty et al 2009 li et al 2016 feng et al 2019 flynn 2019 as the only annual crop specific land use and land cover lulc data product of the conus the cropland data layer cdl of the u s department of agriculture usda national agricultural statistics service nass has been widely used by farmers educators students researchers and government officers in agricultural business universities research institutes and governments worldwide for agricultural production planning and management education government policy formulation and decision making and various research activities boryan et al 2011 stern et al 2012 mueller and harris 2013 di et al 2015 the cdl product has been disseminated through cropscape since 2011 according to the report from google analytics more than 208 000 unique users around the world have visited and interacted with cropscape as of may 2019 as an one of a kind one stop platform to visualize retrieve and analyze cdl cropscape provides not only various geospatial functionalities such as data customization and downloading crop acreage statistics charting and graphing and multi temporal change analysis but also the highly interoperable open standard based geospatial web services such as web map service wms web coverage service wcs and web processing service wps han et al 2012 zhang et al 2019d these functionalities and web services of cropscape have been utilized in many geospatial environmental models frameworks and software han et al 2014 feng et al 2015 mcnider et al 2015 groff et al 2016 tasdighi et al 2018 however due to the limited computing resources it is difficult to perform advanced modeling and mathematical operations on the multi year cdl data directly using either the cropscape web portal or its web services to fill the gap and further facilitate the use of cdl data a new web based tool for the conus scale agricultural land use modeling is needed modeling multi year cdl data for the entire conus requires massive computational capacity therefore the development of the software should be based on a high performance geospatial cyberinfrastructure ci which supports the collection management share analysis visualization and dissemination of the geospatial big data over the high speed network yang et al 2010 yue et al 2015 during the past decade many geospatial applications and tools have been transformed from the traditional geographic information system gis software to the geospatial ci with the rapid advancements in web service technologies castronova et al 2013 lin et al 2017 zhang et al 2019c geospatial information interoperability zhao and di 2010 goodall et al 2013 nativi et al 2013 sun et al 2017 geospatial cloud computing yang and huang 2013 zhang et al 2017 high performance computing lee et al 2011 and geospatial big data analytics deng and di 2014 vitolo et al 2015 di 2016 among those geospatial applications there are many geospatial cis serving for the environmental modeling community the self adaptive earth predictive systems seps adopts the service oriented architecture soa to bridge earth observation eo data and earth system models using ogc iso sensor web standards and geospatial interoperability protocols di 2007 cybergis wang 2010 a gis framework based on the advanced ci has been integrated into many gis applications and software wang et al 2013 padmanabhan et al 2014 lin et al 2015 global earth observation system of systems geoss platform links a set of eo systems around the world to facilitate the monitoring of the state of the earth and the sharing of environmental data nativi et al 2013 santoro et al 2016 as a collaboration between the division of advanced cyberinfrastructure and the geosciences directorate of national science foundation nsf earthcube a community driven organization for geoscience ci has funded many projects to improve data access sharing visualization and analysis across geoscience disciplines katz 2015 these projects include geolink krisnadhi et al 2015 cloud hosted real time data services for the geosciences chords kerkez et al 2016 brokering building block bcube khalsa 2017 cyberconnector di et al 2017 sun et al 2017 cyberway di et al 2019 hydroshare and geotrust essawy et al 2018 xue et al 2019 as one of the major players in the cloud computing business google unveiled google earth engine gee in 2010 gee is a cloud based platform for planetary scale geospatial data analysis with diverse geospatial datasets and a variety of ready to use application programming interface api gorelick et al 2017 it has been used as the major computing platform in many earth system science studies including lulc change detection hansen et al 2013 huang et al 2017 midekisa et al 2017 yu et al 2018 crop mapping shelestov et al 2017 teluguntla et al 2018 digital soil mapping padarian et al 2015 forest mapping chen et al 2017 koskinen et al 2019 and wetland mapping hird et al 2017 however prototyping a complicated modeling algorithm with gee involves the cost of learning javascript and gee apis which is a time consuming job for modelers especially those who do not have a strong technical background to address this issue gee provides the javascript python client library allowing users to develop gee enabled applications and tools as needed for example agrisuit provides a web based framework for supporting land use suitability analysis which integrating spatial datasets algorithms and computing capabilities of gee platform yalew et al 2016 collect earth online ceo offers an open source tool for systematic reference data collection in land cover and use applications bey et al 2016 flood prevention and emergency response system a gee powered web based platform for supporting flood event prevention and emergency response has been applied in 19 typhoons and torrential rain events from 2013 to 2016 in taiwan liu et al 2018 the biomass estimation platform is a cloud based application for aboveground biomass mapping and estimation which integrated gee yang et al 2018 coastsat is a gee toolkit to extract shorelines from satellite imagery in large scale vos et al 2019 li et al 2019 presents a gee enabled toolbox of generating high quality user ready landsat mosaic images in this paper we present the agkit4ee toolkit to a simplify the the conus scale agricultural land use modeling on gee b derive cdl based land use data products on the fly with gee and c boost the development of gee enabled web applications for agricultural land use modeling the rest of the paper is organized as follow section 2 introduces the architectural context data development flow and core functions section 3 presents examples to demonstrate features and capabilities of the toolkit section 4 discusses the application scenario the advantages and the limitations of the current implementation the conclusion and future works are given in section 5 2 software design 2 1 architectural context fig 1 illustrates the architectural context of the proposed software the development of the agkit4ee toolkit is fully based on gee client library and gee data catalog which are powered by the high performance computation and data stores of google s cloud infrastructure the agkit4ee library contains a suite of modules such as modeling modules statistics modules and data processing modules the library is developed using javascript which is consistent with gee code editor users can directly import the library to any project in gee code editor and gee enabled web applications currently we have enabled agkit4ee in two web application prototypes the cropland explorer and the crop frequency explorer which are published over the earth engine apps platform as the core component in the architectural context the agkit4ee library includes a variety of frequently used functions for the retrieval process modeling and statistics of cdl data all functions are developed based on the native gee apis and wrapped as the ready to use apis with required and optional arguments the code example of retrieving a cdl image collection of corn and soybeans for selected years using the getcdlcollection function as well as the equivalent code using native gee apis are compared in appendix a fig a1 this example presents a workflow of cdl data retrieval with the readable input arguments in one line of code where the product option refers to the band name of the original cdl data in gee data catalog the remap option includes a list of the value for the desired crop type codes 1 and 5 refer to corn and soybeans respectively the years option specifies the years of interest as a control the same functionality is equivalent to a series of native gee apis including image blending band selection value remapping and mapping over images it is obvious that such a simplification would significantly reduce the burdens of gee coding for modelers who are not familiar with gee apis and cdl products 2 2 data the objective of the agkit4ee toolkit is to facilitate the use of cdl data on gee the complete collection of historical cdl data has been archived in the gee data catalog https developers google com earth engine datasets catalog usda nass cdl table 1 summarizes the information of three cdl products including the cropland layer the confidence layer and the cultivated layer the cropland layer covers the entire conus from 2008 to 2018 and some states from 1997 to 2007 which is composed of over 140 land cover classes with 30m spatial resolution the confidence layer covers the entire conus from 2008 to 2018 which reflects the percentage 0 100 of confidence for each cropland pixel liu et al 2004 the cultivated layer covers the entire conus from 2013 to present which is produced based on the most recent five years of cropland layer the current year cdl data would be first released through cropscape and the usda nass website in the early next year for example the 2019 cdl which is the latest cdl data as of the writing of this article was published on january 2020 however the collection of some datasets on gee might be delayed the 2019 cdl has not been archived in gee data catalog as of march 2020 besides the cdl data the toolkit integrated a collection of boundary data that frequently used in agricultural and environmental modeling including u s county boundary u s state boundary usda agricultural statistics district asd boundary landsat world reference system 2 wrs 2 scene boundary and sentinel 2 tile boundary these boundary data will improve efficiency while preparing data for region of interest for example users can export the cdl data of specific u s state county or asd by using the exportcdlbyfips function with the specific federal information processing standards fips code 2 3 implementation gee provides two basic geospatial data structures image and imagecollection to manipulate raster data the image is a single raster image data composed by one or multiple bands the imagecollection is a stack of the image as of december 2019 the gee data catalog has archived all historical cdl data from 1997 to 2018 the cdl product of each year is saved as an image depending on the data availability table 1 each cdl image contains either one two or three bands based on image and imagecollection we defined and implemented five extended data structure optionsto manupulate cdl images they are s ingle band cdl image s ingle band cdl imagecollection s tacked cdl image m ulti bands cdl i mage and m ulti bands cdl i mage c ollection table 2 summarizes all data structures defined in agkit4ee fig 2 illustrates the architecture and the development flow of the core modules in agkit4ee the development is composed of a suite of modules and each module contains a group of functions the getcdl module provides the capability of getting original cdl data according to user requirements such as the product type year or crop type meanwhile all modeling functions are implemented based on the getcdl module according to the data product type the modeling functions are implemented as croplandmodeling module and confidencemodeling module respectively the croplandmodeling module consists of functions related to the cropland layer such as crop sequence modeling crop rotation modeling and crop frequency modeling the confidencemodeling module handles the functions of pixel level confidence percentage modeling based on the confidence layer additionally there are several miscellaneous modules offering common geospatial functions the getroi module manages the u s boundary files the export module allows user to batch export of the on demand cdl data the statistics module provides the statistical functions for agricultual land use change analyisis 2 4 functions and capabilities all functions in the agkit4ee toolkit contain one or more arguments besides the required arguments that users must assign before use most functions also have several optional arguments which are passed through the options object for example the optional arguments of the getcdlcollection function in the getcdl module consist of product years remap and defaultvalue the product option is the product name of the desired cdl layer the years option is a list of years of interest the remap option is a list of the codes of target crops the defaultvalue option is the code value of no data pixels table 3 summaries all functions offered by the current release of the toolkit 3 examples this section gives several examples of agricultural land use modeling and analysis using the agkit4ee toolkit first we demonstrate the capability of crop mapping for the entire conus using the crop sequence modeling function section 3 1 then we present an example of crop frequency modeling and compare the result with the official crop frequency map produced by usda nass section 3 2 in addition we also illustrate how to extract the high confidence cdl pixels by stratifying the historical confidence layers section 3 3 finally two agkit4ee enabled web applications are prototypically implemented and published through the earth engine apps platform section 3 4 3 1 mapping cropland by crop sequence the cropping sequence can affect crop yield edwards et al 1988 crookston et al 1991 berzsenyi et al 2000 as well as soil quality soil fertility and soil physical chemical properties janzen et al 1992 karlen et al 2006 van eerd et al 2014 triberti et al 2016 for instance the corn soybean rotation a widely adopted common cropping practice helps preserve the croplands productivity in the u s corn belt based on the reliable crop sequence information the types of crops to be planted can be predicted before the growing season starts zhang et al 2019a zhang et al 2019b these prediction and pre season crop planting information are critical to many early season environmental modeling and applications to facilitate the pre season crop mapping agkit4ee offers an innovative function to extract pixels that following the specific cropping sequence in the recent years the code example of mapping major crops based on the common crop rotation patterns using the modelingbycropsequence function can be found in appendix a fig a2 in this example the variable corn mono and corn rotation refer to the corn pixels following the monocropping and corn soybeans rotated cropping pattern the variable soybean mono soybean rotation and soybean rotation alt refer to the soybeans pixels following the monocropping corn soybeans and rice soybean rotated cropping pattern the variable cotton mono refers to the cotton pixels following the monocropping pattern the variable rice mono and rice rotation refer to the rice pixels following the monocropping pattern and rice soybeans rotated cropping pattern the variable durumwheat mono springwheat mono and winterwheat mono refer to the wheat pixels following the monocropping pattern fig 3 shows the spatial distribution of major crops extracted from the historical cdl data with the above mentioned crop sequences 3 2 mapping cropland by crop frequency usda nass releases the crop frequency data layers accompanying with the release of annual cropland layer this product identifies the planting frequency for the specific crop type based on the cdl from 2008 to present boryan et al 2014 the crop frequency data layers for four major crops in the u s corn cotton soybeans and wheat are available to the public the latest crop frequency data layer products are available on cropscape and the usda nass website the agkit4ee toolkit provides the capability of modeling crop frequency based on the historical cropland layers the code example of mapping the frequency of major crops i e corn and soybeans for the state of nebraska can be found in appendix a fig a3 the variable freq corn refers to the planting frequency map of corn and freq soybean refers to the planting frequency map of soybeans fig 4 compares the crop planting frequency maps with the nass official crop frequency data layers as shown in fig 4a and c the agkit4ee derived crop planting frequency maps contain more information than the nass crop frequency data layers as shown in fig 4b and d more importantly the toolkit provides the capability of modeling the frequency of any crop type from cdl within any available year range for example users can model the frequency of major crops for the state of arkansas in the recent five years 2014 2018 by assigning the years option of the modelingfrequencybycrop function see appendix a figure a4 fig 5 shows the mapping results of corn fig 5a cotton fig 5b rice fig 5c soybeans fig 5d winter wheat fig 5e and double cropping of winter wheat and soybeans fig 5f 3 3 mapping confidence layer by threshold the cdl data are produced using c5 0 see5 decision tree algorithm for every classified cdl pixel there is an associated classification confidence measure which measures the confidence percentage of the corresponding cdl pixel a map of the cdl pixels that are higher or lower than a specific confidence level threshold can produced by binarizing the confidence layer with the specific threshold furthermore by modeling the multi years of confidence layers we can observe the spatial distribution of the trusted cropland pixels maintaining high confidence for identifying the crop type of the corresponding pixel the code example of modeling confidence layers with the modelingconfidencebythreshold function can be found in appendix a fig a5 the variable conf 70 conf 80 conf 90 and conf 100 represent the maps of cropland pixels which the confidence value is consistently greater than 70 80 90 and 100 respectively for the given time period fig 6 shows the binarized maps of the modeling result in which the bright pixels represent the pixels higher than the threshold value 3 4 enabling agkit4ee in web applications in this study we enabled agkit4ee in two web application prototypes the cropland explorer https czhang11 users earthengine app view agkit4ee cdl explorer and the crop frequency explorer https czhang11 users earthengine app view agkit4ee crop frequency explorer these prototypes are published as the gee app over the earth engine apps platform https www earthengine app as shown in fig 7 the graphic user interface includes a configuration panel and a map explorer the app calls the specific modeling functions of the toolkit based on the user s choice and dynamically reload the on demand results on the map explorer with the cropland explorer fig 7a users can select the product layer year crop types and boundary layer on the configuration panel the prototype currently provides the crop area statistics and crop sequence statistics which are created using statisticsbyroi and statisticsbypoi functions of the statistics module for a selected region county asd state of interest and point of interest on the map explorer the time series chart of changes of crop area for the selected region of interest and crop sequence for the point of interest will be plotted on the panel similarly the crop frequency explorer fig 7b will produce the crop frequency map according to the crop type and years of interest 4 discussion 4 1 contribution of the study the agkit4ee toolkit makes the conus scale agricultural land use modeling more effectively and efficiently first of all it remedies the limitation of cropscape and provides various ready to use modeling functions such as crop sequence modeling crop frequency modeling and confidence layer modeling these functions can be coupled with many modeling workflows for example the pixels derived from the common crop sequence can be potentially used as training samples to train the classification model which would be a low cost but reliable way to produce reference data for the early season crop mapping over a large geographic area the crop frequency map can provide information regarding the potential geospatial distribution of crop planting in the future by modeling the confidence layer the current cdl users can easily find out the trusted cdl pixels over time thus better assessing the modeling result computing resource is critical to the performance of geospatial environmental modeling because of the limited computing capacity the conus scale modeling might take a few hours or days to run on a single server or personal computer by taking advantage of gee s powerful cloud computing environment it takes only a few seconds to process the cdl data for the entire conus this will save a considerable amount of time for modelers we believe that the environmental modeling community lulc community and agricultural sectors will be benefited from using gee along with agkit4ee 4 2 application scenarios the agkit4ee toolkit is designed for users who deal with cdl data on the gee platform and gee enabled web applications here are some common application scenarios first all functions can be directly requested to visualize explore and export the on demand cdl products through gee code editor another application scenario is integrating the toolkit with other modeling workflow while cdl is one of the data sources moreover the agkit4ee library can be imported as javascript module into the web frameworks in this way the implementation of gee based web applications and geospatial ci can be significantly accelerated developers can just focus on the implementation of the high level architecture without figuring out the deatils of gee apis and cdl data 4 3 data derived from cdl the agkit4ee toolkit is more than a gee extension for cdl data visualization it is characterized by many modeling functions and options the original cdl data can be fully utilized and various agricultural land use products can be derived using the toolkit table 4 compares the data offered by cropscape gee data catalog and agkit4ee the original cdl products cropland layer confidence layer and cultivated layer can be directly exported using the export module for example user can start a job to batch download the cropland layer maps from 2010 to 2018 of corn and soybeans for iowa by calling the exportcdlbyfips function in the export module all other products that derived from the cdl data e g crop frequency map and crop sequence map can be retrieved through the modeling functions summarized in table 3 then exported to the local path with the gee s built in export functions agkit4ee is an open source software with a fully extensible structure it is easy and free to extend the modules and develop more cdl based agricultural land use products as needed 4 4 limitations the current release of agkit4ee still has some limitations on the one hand computing capacity of some geospatial functions are restricted by gee for examlpe we currently only support the county level statistics due to the limited number of pixels per each process 10 million pixels allowed by gee when scaling up to the state level or larger geographic area the process will be stopped with the too many pixels in the region error a solution to bypass the limitation is to increase the scale for the reducer operation however this tweak would affect the accuracy of the statistics result a better solution is to sum up the county level results to asd or state level but it would take more processing time moreover the earth engine apps platform does not support the data export function in the current phase to export the modeling result the user must run the toolkit through the gee code editor on the other hand there were many misclassified pixels in the early year cdl data because of cloud cover and lack of satellite images also the coverage of the early year cdl was incomplete only a few states were fully covered before 2008 these quality and coverage issue of the early year cdl data can potentially affect the follow on studies zhang et al 2020 5 conclusion and future works this paper presents the design implementation and use examples of agkit4ee as a gee enabled toolkit for the conus scale agricultural land use modeling agkit4ee contains a variety of frequently used functions for retrieving visualizing modeling and analyzing cdl data the major functions and capabilities of the proposed software including crop sequence modeling crop frequency modeling confidence layer modeling and geospatial statistics were demonstrated additionally two agkit4ee enabled web applications the cropland explorer and the crop frequency explorer were prototyped and published over the earth engine apps platform the result suggests this toolkit would greatly reduce the workload of modelers and developers who deal with cdl data in the next phase of development we will enhance and extend the agkit4ee toolkit integrate more modeling functions develop the machine learning module and support more lulc data products such as the national land cover database nlcd of u s geological survey usgs a geospatial ci with all features of the agkit4ee toolkit is under development currently the toolkit is implemented in javascript only we will implement the core modules in python to support more third party applications developed with gee python api software availability software name agkit4ee developer center for spatial information science and systems george mason university technical support chen zhang czhang11 gmu edu programming language javascript license mit software required google earth engine project https code earthengine google com accept repo users czhang11 agkit4ee earth engine repository https earthengine googlesource com users czhang11 agkit4ee github repository https github com czhang11 agkit4ee cropland explorer https czhang11 users earthengine app view agkit4ee cdl explorer crop frequency explorer https czhang11 users earthengine app view agkit4ee crop frequency explorer declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is supported by a grant from national science foundation infews program grant cns 1739705 pi dr liping di the authors would like to thank two anonymous reviewers for their valuable comments appendix a code examples fig a1 example of retrieving cdl image collection using agkit4ee fig a1 fig a2 mapping major crops based on common crop rotation patterns for the entire conus fig a2 fig a3 mapping frequency of corn and soybeans for the state of nebraska fig a3 fig a4 mapping crop frequency for the state of arkansas in the recent five years fig a4 fig a4 modeling confidence layer for the entire conus fig a4 
