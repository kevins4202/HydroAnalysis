index,text
25915,salinity is a complex process in watersheds and there is a need for data and modeling salinity in arid and semi arid watersheds development and application of a salinity model to accurately predict the salinity processes are difficult tasks at the watershed scale available salinity models are limited in sufficiently modeling natural salinity processes in watersheds in this study a salinity module is proposed for the soil and water assessment tool swat utilizing new salinity equations to model natural salinity processes in watershed systems data related to the distribution maps of saline formations is used in the modeling process an advantage of this module is that it can use low quality maps for salinity modeling of different salts the proposed methodology can be used to assess the impacts of management practices climate change and land use changes on non point source pollution at a watershed scale graphical abstract image 1 keywords iswat salinity modeling sce ua swat watershed modeling 1 introduction salts are highly soluble in surface water and groundwater and can be mobilized in the landscape by water movement salinity a global challenge to water and soil quality management is a measure of salts content in soil or water and may be of primary or secondary origin kaushal et al 2018 primary salinity occurs naturally in soils and water bodies due to natural salinity processes such as weathering of rocks with salts dissolution from saline formations through hydrologic processes naderi et al 2016 also wind and rain can deposit salts over a lengthy period the dissolution of salts from saline formations is a complicated physical and chemical process the dissolution process is dependent on the dissolved salt concentration of water when the solute concentration increases to a certain degree the precipitation phenomenon stops the solute from dissolving as the solution gets saturated yang et al 2018 secondary salinity is the result of human activities such as deforestation urbanization irrigation and surface mining cañedo argüelles et al 2013 the topography and age of landscapes affect the nature of salinity processes furthermore hydrologic pathways evaporative concentration vegetation patterns nature and depth of regolith all interact to determine salts storage location within landscapes moore et al 2018 excessive salinity makes water unsuitable for domestic use al ansari et al 2019 negatively affects irrigated agriculture majeed and muhammad 2019 damages wildlife habitat lawlor and arellano 2020 limits the ability to recycle water and limits recharge of groundwater given the high cost of treatment saline water is often discharged into water bodies rather than reused the monitoring and modelling of the quantity and salinity of water are vital in water resource management analysis of the transfer of salt and the ratio of salt along the hydrologic pathways to the river is crucial for watershed scale water resource management while there is a lack of information and salinity observed data in many watersheds around the world the reasonable determination of causes and origins of salinity problems is increasingly complex with the influence of land use change water management biodiversity climate change and agricultural production cheng et al 2014 besides water composition particularly in a large watershed reflects multiple salinity sources and processes that mobilize salts and change salt concentrations gitau and chaubey 2010 since the study of all salinity processes of a watershed is an enormous task it is challenging to establish a general salinity model to provide accurate predictions therefore salinity studies historically have been focused on the most crucial salinity processes at the watershed scale salt generation processes salt delivery processes through landscapes and from a landscape into a river and in stream salt transport processes are generally essential processes that can be considered for modelling the fate and transport of salts through a watershed for example the salt mass balance of a watershed the salt export import e i ratio is an efficient mechanism for encapsulating all salinity processes biggs et al 2013 many salinity models have been developed to determine the source transformation and transportation of salts through different media of watersheds these salinity models vary depending on process representations process complexities data requirements scales of the application and tend to focus on particular salt mobilization processes at others expense they can predict the salinity of areas which have no observed data or to evaluate the effects of different management practices and climate regimes on states of salinity issues based on the scale of the application some salinity models such as hydrus šimůnek et al 2013 leachm hutson 2003 swap kroes et al 2017 mt3dms zheng et al 2012 and saltmod oosterbaan 2001 have been used for salinity studies at a local scale also there are several salinity models such as dsm2 anderson and mierzwa 2002 oteq runkel 2010 msm bigmod close et al 2004 iqqm podger 2004 and realm perera et al 2005 that specifically simulate in stream salinity semi distributed watershed scale salinity models such as 2csalt stenson et al 2011 and bc2c gilfedder et al 2009 divide watersheds properties of land use soil type and topography each spatial unit represents similar hydrological behavior the bc2c model which uses a simple water balance approach can be used for regional prioritization and examining land use impacts in watersheds the bc2c model is a rapid assessment tool for the first approximations from hydrogeological information beverly et al 2006 the 2csalt model is a lumped parameter model that quantifies salt export through surface and subsurface contributions into the river and predicts land use change impacts the 2csalt model was exclusively designed for application to the murray darling basin the application of the model is highly questionable in other basins fully distributed watershed scale salinity models such as class teng et al 2008 catsalt tuteja et al 2003 and wec c croton and barry 2001 divide watersheds into hydraulically connected elements such as grids or triangular elements fully distributed salinity models require considerable amounts of data at finer scales and typically have much larger computational requirements than semi distributed salinity models therefore they are not considered feasible for large watersheds currently no model simulates salt transport in all significant hydrologic pathways rain surface runoff soil percolation and leaching lateral flow groundwater flow streamflow at the watershed scale and incorporates major solution reaction chemistry bailey et al 2019 the geographic information system gis allows spatial and exploratory analyses of landscapes dehghani et al 2019 determining the most critical salt sources and salt transport mechanisms in a watershed can be assessed using gis maps a comprehensive salinity model is needed to simulate the fate and transport of salts along all significant hydrologic pathways and consider crucial solution reaction chemistry including the dissolution precipitation gis maps of saline formations can improve salinity modeling at the watershed scale for assessing spatial and temporal distributions of salts contents of saline formations however this has not been well done yet among catchment scale hydrological models soil and water assessment tool swat is a popular semi distributed physically based agro hydrological model used to analyze the impact of water and land management measures on water sediment and chemical fluxes at a daily time step neitsch et al 2011 while swat incorporates modules to deal with different pollutants and chemicals including nitrogen phosphorous pesticides sediments nutrients bacteria and heavy metal it does not include a specific salinity module for transport and transformation of salts throughout a watershed bailey et al 2019 present a watershed scale salt ion fate and transport model by developing a salinity module for the swat model the module accounts for salt loading for each major hydrologic pathway in a watershed setting stream groundwater lateral flow surface runoff tile drain flow for each major salt ion so4 ca mg na k cl co3 and hco3 the module also accounts for principal equilibrium chemistry reactions precipitation dissolution complexation cation exchange however natural salinity sources and processes were disregarded in the model also bailey et al 2019 study applied the model to an irrigated area and chemical reactions do not change in stream salt ion concentrations in the salinity module therefore there is a need to develop new internal salinity modules for the swat model using new salinity equations to include natural salinity sources and processes in soil and river environments at a watershed scale a new watershed scale natural salinity model can consider main salinity processes related to the fate of salts in a watershed that includes the generation of salts transportation of salts through the landscapes transportation of salts from the landscapes into rivers and in stream transport of salts in this study an internal computational module is developed for modeling primary salinity in swat using the fortran language furthermore saline formations have been categorized into shallow deep and reach bed saline formations and included using gis the salinity module accounts for the transport of salts based on the hydrological processes of the swat model also the dissolution and precipitation of salts have been considered for the surface runoff soil water shallow aquifer and river flow 2 methodology the natural sources of salts were incorporated into the proposed swat s model the following sections explain details of the proposed swat s model for watershed scale salinity routing 2 1 hydrological and salinity models a water salt balance model swat s module to simulate discharge and salinity at the catchment scale was developed by modifying the swat model the swat includes subbasin and channel flow routing and each subbasin is further divided into hydrologic response units hrus neitsch et al 2011 the swat s considers the dissolution precipitation and transportation of salts in the watershed the interactions between different cations and anions related to different salts have not been directly assessed however the swat s assumes that the mass of the salt is conservative and that steady state conditions can simulate long term transient conditions approximately since the maximum dissolved concentration of different salts is not unique it is required to model the dissolution precipitation and transportation processes of each salt separately in any desired location as the output of the reaches tds total dissolved solids can be calculated by the summing of different dissolved salt concentrations fig 1 briefly illustrates various components of the swat s blue arrows indicate the displacement of water and black arrows indicate the displacement of the salt and salt mobilization the salt in the solution phase is transported through the root zone with the surface runoff lateral flow and percolation shallow aquifer and the stream reach there is a need to propose appropriate methods and salinity equations to model natural salinity at the watershed scale the main equations used for hydrological modeling and derived for salinity modeling are discussed in the following subsections equations presented in this section are generic to represent each salt type 2 1 1 root zone the amount of precipitation that contributes to surface runoff can be calculated using the scs curve number equation for each hru neitsch et al 2011 1 q s r f r t i a 2 r t i a s where q s r f is the accumulated surface runoff for the hru in a time step daily mm h2o r t is the precipitation depth for the hru in a time step daily mm h2o i a is the initial abstractions which includes the surface storage interception and infiltration before the surface runoff for the hru mm h2o and s is the retention parameter for the hru mm h2o the amount of water that percolates to the next layer is neitsch et al 2011 2 w l y p r c s w l y e x c 1 e x p δ t t t p r c where w l y p r c is the amount of water percolating to the underlying soil layer for the hru in a time step daily mm h2o s w l y e x c is the drainable volume of water in the soil layer in a time step daily mm h2o δ t is the length of the time step hrs and t t p r c is the travel time for the percolation hrs when the soil water of the soil layer is greater than the soil water of the field capacity the lateral flow is calculated as bellow neitsch et al 2011 3 q l a t 0 024 2 s w l y e x c k s a t s l p φ d l h i l l where q l a t is the lateral flow discharged from the hillslope for the hru in a time step one daily mm h2o k s a t is the saturated hydraulic conductivity for the layer mm hrs s l p is the increase in the elevation per unit distance mm mm φ d is the drainable porosity of the soil layer mm mm and l h i l l is the hillslope length m the evaporation of water from a soil layer increases the dissolved salt concentration of the soil layer if the dissolved salt concentration of a soil layer reaches the maximum dissolved salt concentration extra evaporation will cause the salt precipitation therefore the calculated dissolved salt concentration of a soil layer without considering the salt precipitation maybe become higher than the maximum dissolved salt concentration of the soil layer hence an initial dissolved salt concentration of the soil layer at the end of the time step without the consideration of the salt precipitation can be derived using equation 4 afterward as can be seen in equation 5 a final dissolved salt concentration of the soil layer can be derived with the consideration of the salt precipitation 4 c l y s w t s w l y t c l y s w t 1 s w l y t 1 λ c p t r t 1 λ c l y 1 s w t w l y 1 p r c t c s h t c s h t 1 2 w l y u p f t m l y d e p t 1 c l y s w t c l y s w t 1 2 β λ q s r f t q l y l a t t w l y p r c t η τ c l y s w t 1 e l y s o l t where c l y s w t is the initial dissolved salt concentration of the soil layer for the hru at the end of the time step kg salt mm h2o s w l y t is the water content of the soil layer for the hru at the end of the time step mm h2o c l y s w t 1 is the final dissolved salt concentration of the soil layer for the hru at the beginning of the time step kg salt mm h2o s w l y t 1 is the water content of the soil layer for the hru at the beginning of the time step mm h2o λ is a dimensionless factor λ 1 for the first soil layer and λ 0 for other soil layers c p t is the dissolved salt concentration of precipitation for the hru during the time step kg salt mm h2o r t is the precipitation depth for the hru during the time step mm h2o c l y 1 s w t is the final dissolved salt concentration of the upper soil layer for the hru at the end of the time step kg salt mm h2o w l y 1 p r c is the amount of water percolating to the soil layer from the upper soil layer for the hru during the time step mm h2o c s h t is the final dissolved salt concentration of the shallow aquifer for the hru at the end of the time step kg salt mm h2o c s h t 1 is the final dissolved salt concentration of the shallow aquifer for the hru at the beginning of the time step kg salt mm h2o w l y u p f t is the amount of upward flux from the shallow aquifer to the soil layer for the hru during the time step mm h2o m l y d e p t 1 is the amount of the precipitated salt in the soil layer for the hru at the beginning of the time step kg salt ha β is the salt percolation coefficient β 0 for the first soil layer and β 1 for other soil layers η is a dimensionless factor η 1 for the first soil layer η 1 for the second soil layer and η 0 for other soil layers τ is the coefficient related to the upward movement of salt from the first the soil layer to the top 10 mm of the soil profile and e l y s o l t is the amount of water removed from the first soil layer due to the evaporation for the hru during the time step mm h2o with the consideration of the salt precipitation the final dissolved salt concentration of the soil layer at the end of the time step can be derived by comparing the initial dissolved salt concentration of the soil layer at the end of the time step with the maximum dissolved salt concentration for the hru as follow 5 c l y s w t c h r u m a x i f c l y s w t c h r u m a x c l y s w t c l y s w t i f c l y s w t c h r u m a x where c l y s w t is the final dissolved salt concentration of the soil layer for the hru at the end of the time step kg salt mm h2o and c h r u m a x is the maximum dissolved salt concentration for the hru kg salt mm h2o the precipitated salt mass in the soil layer at the end of the time step can be derived as follow 6 m l y d e p t c l y s w t c l y s w t s w l y t 0 5 β λ q s r f t q l y l a t t w l y p r c t where m l y d e p t is the amount of the precipitated salt in the soil layer for the hru at the end of the time step kg salt ha 2 1 2 shallow aquifer the water balance in the shallow aquifer is neitsch et al 2011 7 q s h t q s h t 1 w r g s h t w r g d p q r f t w r e v t where q s h t is the amount of water stored in the shallow aquifer for the hru at the end of the time step mm h2o q s h t 1 is the amount of water stored in the shallow aquifer for the hru at the beginning of the time step mm h2o w r g s h t is the amount of the recharge entering the shallow aquifer for the hru during the time step mm h2o w r g d p t is the amount of the recharge entering the deep aquifer for the hru during the time step mm h2o q r f t is the groundwater flow or baseflow into the main channel for the hru during the time step mm h2o and w r e v t is the amount of water moving from the shallow aquifer to the soil zone in response to water deficiencies for the hru during the time step mm h2o similar to the procedure that was used for the calculation of the dissolved salt concentration of the soil layers an initial dissolved salt concentration of the shallow aquifer at the end of the time step without the consideration of the salt precipitation can be derived using equation 8 a final dissolved salt concentration of the shallow aquifer can be derived with the consideration of the salt precipitation in equation 9 8 c s h t q s h t c s h t 1 q s h t 1 c p e r t 1 w r g s h t m s h d e p t 1 c s h t c s h t 1 2 q r f t w r e v t w r g d p t where c s h t is the initial dissolved salt concentration of the shallow aquifer for the hru during the time step kg salt mm h2o c s h t 1 is the final dissolved salt concentration of the shallow aquifer for the hru at the beginning of the time step kg salt mm h2o c p e r t 1 is the final dissolved salt concentration of the lowest soil layer for the hru at the beginning of the time step kg salt mm h2o and m s h d e p t 1 is the amount of precipitated salt in the shallow aquifer for the hru at the beginning of the time step kg salt ha with the consideration of the salt precipitation the final dissolved salt concentration of the shallow aquifer at the end of the time step can be derived by comparing the initial dissolved salt concentration of the shallow aquifer at the end of the time step with the maximum dissolved salt concentration for the hru as follow 9 c s h t c h r u m a x i f c s h t c h r u m a x c s h t c c h t i f c s h t c h r u m a x where c s h t is the final dissolved salt concentration of the shallow aquifer for the hru at the end of the time step kg salt mm h2o and c h r u m a x is the maximum dissolved salt concentration for the hru kg salt mm h2o the precipitated salt mass in the shallow aquifer at the end of the time step can be derived as follow 10 m s h d e p t c s h t c s h t q s h t 0 5 q r f t w r e v t w r g d p t where m s h d e p t is the amount of the precipitated salt in the shallow aquifer for the hru at the end of the time step kg salt ha 2 1 3 stream reach the swat model calculates the net water yield y l d w in mm to the stream channel for each subbasin by using the following equation neitsch et al 2011 11 y l d w q s r f q l a t q r f q t l where q s r f is the surface runoff mm h2o q l a t is the lateral flow contribution to stream discharge mm h2o q r f is the groundwater contribution to stream discharge mm h2o and q t l is the water lost from the reach via the evaporation and the transmission through the bed mm h2o similar to the procedure that was used for the calculation of the dissolved salt concentration of the soil layers and shallow aquifer an initial dissolved salt concentration of the reach at the end of the time step without the consideration of the salt precipitation can be derived using equation 12 a final dissolved salt concentration of the reach can be derived with the consideration of the salt precipitation as in equation 13 12 c c h t v c h s t g t c c h t 1 v c h s t g t 1 c u p s t v u p s t c s r f s u b t v s r f s u b t c l a t s u b t v l a t s u b t c r f s u b t v r f s u b t m c h d i s t m c h d e p t 10 3 c c h t c c h t 1 2 v d n s t v t l t where c c h t is the initial dissolved salt concentration of stored water in the reach at the end of the time step mg salt l v c h s t g t is the volume of stored water in the reach at the end of the time step m3 c c h t 1 is the final dissolved salt concentration of stored water in the reach at the beginning of the time step mg salt l v c h s t g t 1 is the volume of stored water in the reach at the beginning of the time step c u p s t is the weighted mean of the dissolved salt concentration of the upstream reaches based on the volume of input waters to the reach from the upstream reaches during the time step mg salt l v u p s t is the volume of input waters to the reach from the upstream reaches during the time step m3 c s r f s u b t is the weighted mean of the dissolved salt concentration of the input surface runoff to the reach from the whole subbasin during the time step mg salt l v s r f s u b t is the total volume of the input surface runoff to the reach from the whole subbasin during the time step m3 c l a t s u b t is the weighted mean of the dissolved salt concentration of the lateral inflow to the reach from the whole subbasin during the time step mg salt l v l a t s u b t is the total volume of the lateral inflow to the reach from the whole subbasin during the time step m3 c r f s u b t is the weighted mean of the dissolved salt concentration of the input groundwater flow to the reach from the whole subbasin during the time step mg salt l v r f s u b t is the total volume of the input groundwater flow to the reach from the whole subbasin during the time step m3 m c h d i s t is the amount of the input salt mass to the reach from the bed saline formation for input waters to the reach from the upstream reaches during the time step kg salt m c h d e p t is the amount of the precipitated salt in the reach during the time step kg salt v d n s t is the output water from the reach to the downstream reach during the time step m3 and v t l t is the volume of transmission losses through the side and bottom of the channel during the time step m3 with the consideration of the salt precipitation the final dissolved salt concentration of the reach at the end of the time step can be derived by comparing the initial dissolved salt concentration of the reach at the end of the time step with the maximum dissolved salt concentration of the reach as follow 13 c c h t c c h m a x i f c c h t c c h m a x c c h t c c h t i f c c h t c c h m a x where c c h t is the final dissolved salt concentration of stored water in the reach at the end of the time step mg salt l and c c h m a x is the maximum dissolved salt concentration of the reach mg salt l the precipitated salt mass in the reach at the end of the time step can be derived as follow 14 m c h d e p t c c h t c c h t v c h s t g t 0 5 v d n s t v t l t 10 3 2 2 addressing saline formations in the model three types of saline formations were proposed for each subbasin shallow saline formation and deep saline formation types exist in each hru besides the saline formation type exists in the reach bed the shallow saline formation type locates in the soil profile the deep saline formation type is located in the vadose zone below the soil profile in the proposed method as some parts of the hru may only be composed of the shallow saline formation the shallow saline formation is not considered for salinity modeling inside the soil profile otherwise the precipitated salt related to the saline parts of the hru can strongly affect the dissolved salt concentration of the non saline parts of the hru therefore the effects of the shallow saline formation on the salinity of water are not considered inside the soil profile effects of shallow saline formations are considered for the salinity of output waters from the soil profile surface runoff lateral flow and deep percolation based on the ratio of the shallow saline formation existence in each hru the dissolved salt concentration of the output surface runoff from the hru ϕ s r f α s h c h r u m a x 1 α s h c t o p s w t 1 c t o p s w t 2 is derived by combining the modified maximum dissolved salt concentration for the shallow saline formation area ϕ s r f c h r u m a x with the dissolved salt concentration of the output surface runoff from the hru for the non saline formation area c t o p s w t 1 c t o p s w t 2 via weighing based on the area ratio of the shallow saline formation in each hru α s h ϕ s r f is the correction coefficient for the salt dissolution of the surface runoff the subscript t o p indicates top 10 mm of the soil profile furthermore the dissolved concentration of the output lateral flow from each soil layer in the root zone of the hru α s h c h r u m a x 1 α s h c l y s w t 1 c l y s w t 2 is derived by combining the maximum dissolved salt concentration for the shallow saline formation area c h r u m a x with the dissolved salt concentration of the output lateral flow from each soil layer in the root zone of the hru for the non saline formation area c l y s w t 1 c l y s w t 2 via weighing based on the area ratio of the shallow saline formation in each hru α s h besides the actual dissolved salt concentration of the stored water in each soil layer in the root zone of the hru can be derived in the same way in the proposed method similar to the soil profile salinity modeling of the shallow aquifer is done without consideration of the deep saline formation in the shallow aquifer for each hru to better account for the effects of the deep saline formation on the salinity of groundwater the deep saline formation was located in the vadose zone below the soil profile and above the shallow aquifer for each hru effects of the deep saline formation on water salinity of the deep percolates which ultimately recharges the shallow aquifer were derived by using the maximum area ratio related to the existence of saline formations for each hru α r g this ratio which may range from 0 to 1 is derived by comparing the area ratios of the shallow saline formation α s h and deep saline formation α d p for each hru m a x α s h α d p the dissolved salt concentration of the recharge entering the shallow aquifer α r g c h r u m a x 1 α r g c l o w s w t 1 c l o w s w t 2 is derived by combining the maximum dissolved salt concentration for the saline formation area c h r u m a x with the dissolved salt concentration of the deep percolation for the non saline formation area c l o w s w t 1 c l o w s w t 2 via weighing based on the maximum area ratio of the saline formations in each hru α r g the subscript l o w means the lowest layer of the soil profile in the proposed method the part of the reach bed which contains the saline formation increases the salinity of the surface runoff lateral flow groundwater flow and upstream inflow when those enter the main channel the dissolved salt concentrations of the surface runoff ϕ c h α c h c c h m a x 1 α c h c s r f s u b t lateral flow α c h c c h m a x 1 α c h c l a t s u b t and groundwater flow α c h c c h m a x 1 α c h c r f s u b t into the main channel are derived by combining the modified maximum dissolved salt concentrations of the saline formation area of the reach bed ϕ c h c c h m a x or c c h m a x with the dissolved salt concentration of the surface runoff c s r f s u b t lateral flow c l a t s u b t and groundwater flow c r f s u b t into the main channel for the non saline formation area of the reach bed this is done via weighing based on the area ratio of the saline formation of the reach bed α c h because the upstream inflow flows entirely over the saline formation of the reach bed the salt dissolution for the upstream inflow ϕ c h c c h m a x c c h u p s t when ϕ c h c c h m a x c c h u p s t otherwise 0 when ϕ c h c c h m a x c c h u p s t is derived based on the modified maximum dissolved salt concentration of the saline formation area of the reach bed ϕ c h c c h m a x ϕ c h is the correction coefficient for the salt dissolution of input waters into the reach via the upstream reaches and the surface runoff this proposed method for consideration of the saline formations is helpful when the location and area of saline formations are not accurately delineated for a large area moreover high quality maps of the saline formations usually do not exist the location and area of saline formations can be adjusted during the calibration process using the proposed method also since each separate salt e g nacl and caso4 has specific saline formation maps the explained method can be used for simultaneously modeling of different salts furthermore the purity of each salt in the saline formation can also be defined using the proposed method 2 3 model sensitivity analysis and calibration global sensitivity analysis was performed to identify and rank the most responsive parameters that have a significant impact on the particular basin in the global sensitivity analysis sensitivities of parameters are estimates of the average changes in the objective function resulting from changes in each parameter while changing all the rest parameters the t test was used to identify the relative significance of the parameters t statistic and p values were obtained for the sensitivity analysis the t statistic shows the measure of sensitivity and p value provides the significance of the parameters if t statistic for a particular parameter is high and p value is low then that parameter is considered to have a more significant effect on outputs abbaspour 2015 the calibration was done by applying the shuffled complex evolution optimization algorithm method sce ua naeini et al 2019 in the uncsim package version 1 1c reichert 2005 the uncsim is a program package for statistical inference and sensitivity identifiability and uncertainty analysis and can be downloaded freely the iswat interface yang 2006 was prepared based on the input and output files of the swat version 2000 the iswat interface does not consider salinity parameters therefore the iswat interface was modified and the ability to consider salinity parameters was added to the iswat interface the model performance was assessed using the coefficient of determination r2 nash sutcliffe efficiency nse mccuen et al 2006 percent bias pbias gupta et al 1999 and rmse observations standard deviation ratio rsr moriasi et al 2007 besides the goodness of model performance in terms of calibration and uncertainty level is evaluated using the p factor and the r factor indices abbaspour 2015 since salinity data had lower precision than discharge data salt concentration calibrations without using the parameters that affect the discharge were performed separately after discharge calibration therefore the parameters that affect the discharge were not used for salt concentration calibrations the best group of parameters was selected manually based on the nse of all simulation results after completion of the calibration process 2 4 description of the study area the shour river basin with an area of about 622 km2 is located in the south of iran between 29 40 n to 30 10 n and 51 10 e to 51 40 e the location of the shour river basin in the south of iran with the swat delineated subbasins dem map river network gauging station average precipitation average potential evapotranspiration the distribution map of shallow saline formations and distribution map of deep saline formations were illustrated in fig 2 there are many hills and mountains in the basin the average elevation of the basin is 1124 m with a minimum of 669 m and a maximum of 2034 m the average slope of the basin is 23 percent with a semi arid climate the basin receives an average annual precipitation of approximately 540 mm year the average potential evapotranspiration is about 1750 mm the average discharge of the river at the terminal gauge station of shekastian fig 2 is 2 m3 s 1 abgeer 2013 the salt concentration of the river has not been measured continuously in the shour river basin the average observed tds of the river is about 17 000 mg l 1 at the shekastian station maximum and minimum observed tds values of the river are about 68 000 mg l 1 and 1000 mg l 1 at the shekastian station respectively sodium chloride and calcium sulfate salts are the main soluble salts in the shour river basin that can be dissolved from the saline formations human activities have insignificant effects on the salinity of the soil and water in the shour river basin abgeer 2013 a surface soil layer which is in red violet color in the distribution map of shallow saline formations in fig 2 is composed of the gypsic and saline limes sandstones and silty rocks the salinity and alkalinity of this surface soil layer are high abgeer 2013 shallow saline formations do not cover an extensive area in the shour river basin nevertheless shallow saline formations are mainly located adjacent to some upstream reaches the salinity of these upstream reaches affects the salinity in downstream reaches the out cropped geological formation in the area is gachsaran evaporite rock miocene the deep salt rock of the basin mainly consists of the anhydride salt grey and red marl alternating with the anhydride argillaceous limestone and limestone the gachsaran evaporite rock which is in grey color in the distribution map of deep saline formations in fig 2 is a ductile rock unit with a total thickness of about 1 6 km abgeer 2013 deep saline formations cover an extensive area in the shour river basin therefore the dissolution of salts from deep saline formations is the leading cause of the salinity in the shour river basin these dissolved salts are transported vertically and horizontally by water flows in the basin available gis maps sparsely depict distributions of shallow deep and reach bed saline formations in the shour river basin however available gis maps present coarse information on the purity of different salts in saline formations and their distributions in the shour river basin to address this data insufficiency the newly proposed methodology is used to simulate information on salinity the input data is adjusted separately for each salt during the calibration process 2 5 model setup the basic swat version 2012 model includes input data on soil from the global map of the food and agriculture organization of the united nations fao 1995 with data on 5000 soil types comprising two layers 0 30 and 30 100 cm depth at a spatial resolution of 10 km the soil in the study area is a mixture of sand silt clay and organic material with mostly loamy texture the land use map was obtained from the usgs global land use land cover characterization glcc database with a spatial resolution of 1 km which distinguish 24 land use and land cover classes usgs 2018a shrubland is the dominant land use covering 93 of the watershed area the digital elevation model at a 30 m resolution was obtained from the united states geological survey usgs 2018b the river map of major rivers was obtained from the iran water resources management company climate data were obtained from the climate forecast system reanalysis cfsr ncep 2018 the weather input includes the daily precipitation maximum and minimum daily air temperature relative humidity solar radiation and wind speed the spatial variability within the watershed was represented by 19 sub watersheds which were further sub divided into hydrologic response units hrus based on land use soil and slope characteristic features this resulted in 73 hrus defined using thresholds of 5 10 and 10 for land use soil and slope classes respectively also five elevation bands were used in the model to adjust the temperature and rainfall based on the subbasin elevation variation the local water authorities provided discharge and salt concentration data wrm 2020 other maps and data were provided by the abgeer consulting engineers company abgeer 2013 monthly observed discharge nacl concentration and caso4 concentration data were used for the model calibration 1994 2001 and validation 1990 1993 initial dissolved salt concentrations and solid salt masses in the soil layers of the root zone and shallow aquifer were considered zero five years of simulations from 1985 to 1989 of the climate data were used as a warm up period to account for instability in the soil water and salt balance computations caused by the initial conditions di luzio et al 2002 the error of the discharge and salt concentration data were considered 10 and 20 respectively 3 results and discussion 3 1 choice and the prior distribution of parameters twenty seven parameters were used in the model calibration of discharge seven parameters caso4 concentration ten parameters and nacl concentration ten parameters parameters were selected based on sensitivity analysis and preliminary calibrations all selected parameters of water quantity and quality models had uniform prior distributions within reasonable ranges these ranges were defined based on the recommendations given in the swat user manual neitsch et al 2011 and our knowledge table 1 provides an overview of the parameters that were used for model calibrations with their marginal prior distributions 3 2 description of parameters that were used for model calibration the stream flow was highly sensitive to parameters related to soil characteristics these parameters had high t stat and low p value the value of the available water capacity sol awc of the soil layer was increased in comparison with the initial input data during the calibration process moreover the value of the moist bulk density sol bd of the soil layer was increased therefore the field capacity of the soil layer was increased the increase in water holding also increased the potential for more evapotranspiration by vegetation and evaporation from the soil layers to better match the baseflows the curve number for moisture condition ii cn2 was decreased therefore the infiltration rate of the soil was increased the increase in the infiltration rate and the saturated hydraulic conductivity sol k at the same time increased the baseflows also the soil evaporation compensation factor esco was decreased thus higher evaporative demand from lower levels occurred the groundwater revap coefficient gw revap was increased therefore the rate of the transfer from the shallow aquifer into the overlying root zone was increased furthermore due to the decrease in the threshold depth of water in the shallow aquifer for revap revapmn the movement of water from the shallow aquifer to the root zone occurred at shorter time steps the initial value of the maximum dissolved nacl concentration was considered at 357 318 mg l 1 moreover the initial value of the maximum dissolved caso4 concentration was considered 2043 mg l 1 the initial values were estimated based on the solubility of these salts shaw et al 2011 values of area ratios related to the existence of shallow deep and reach bed saline formations were decreased during the calibration process therefore saline formations do not entirely consist of the pure salts and impurities exist in the saline formations the purity of nacl is especially very low in the saline formations the summation of the area ratios related to the existence of nacl and caso4 in saline formations sometimes is greater than 1 hence nacl and caso4 do not uniformly distribute vertically in saline formations besides values of the maximum dissolved salts concentrations were varied during the calibration process compared to the initial values values of the maximum dissolved salts concentrations were reached to different values for the hrus and reaches this indicates that the maximum dissolved salts concentrations are not unique values and can be varied based on the chemical and physical properties of the soil and water environments e g cation exchange reactions and adsorption the sensitivity analysis results indicated that the dissolution of nacl predominantly occurred in the land while the dissolution of caso4 occurred in both land and river therefore the proposed model results could distinguish dominant salinity processes in the shour river basin the dissolution of caso4 and nacl from the shallow and deep saline formations and the dissolution of caso4 from the reach bed saline formations 3 3 evaluation of the model performance although streamflow calibration performances of recent swat model application studies in arid and semi arid areas are acceptable water quality calibration performances are not at desired levels because of limited data availability however water quality models can still yield robust information in arid and semi arid areas özcan et al 2017 it is not uncommon that swat and other fate and transport models are used under data scarce conditions in the ungauged basins swat model is mostly used for hydrological predictions özcan et al 2017 the actual results of models only are achieved by the correct consideration of different processes throughout basins the proper utilization of limited available data plays a crucial role in achieving the right processes in this study we propose an appropriate framework for the correct estimation of natural salinity processes at the watershed scale even when there is data scarcity the quantity of the baseflow is very low less than 0 1 m3 s 1 in the shour river basin for some months of the year salt concentrations of output waters from the shour river basin are the highest values during these months of the year therefore correct predictions of very low values of discharges are essential for accurate predictions of the highest values of salt concentrations this was done by adjusting the shallow aquifer parameters baseflow alpha factor alpha bf and the threshold depth of water in the shallow aquifer required for return flow to occur gwqmn before the primary calibration process based on the observations of discharge in months that have very low values of the discharge and baseflow simiulated loadings of caso4 and nacl via the surface runoff lateral flow and groundwater flow to the river network were calibrated using observed discharges caso4 concentrations and nacl concentrations on a monthly basis at the shekastian gauge station for the shour river basin the performance results of the model are seen in table 2 fig 3 shows a comparison between the simulated and observed discharges caso4 concentrations and nacl concentrations at the shekastian station the shaded region is a 95 prediction uncertainty band 95 ppu the blue line shows the best model simulation about two thirds of the data were used for the calibration and the remaining for the validation in this study in order to compensate the possible bias related to the limited observed data model performances were evaluated based on the 1 multiple statistical performance measures including r2 nse pbias and rsr 2 comparison of the observed data with 95 prediction uncertainty band 95ppu for the simulated data and 3 graphical performance measures as can be seen in table 3 statistical model performances were evaluated based on the general performance evaluation criteria conformed from moriasi et al 2007 and moriasi et al 2015 for the monthly discharge caso4 concentration and nacl concentration the evaluation of values of r2 statistics indicated that collinearity between simulated and observed data were at least satisfactory for water quantity and quality models thus error variances of models were at least satisfactory nevertheless these r2 statistics might be oversensitive to peak values of discharges and salt concentrations and insensitive to additive and proportional differences between model predictions and observed data thus r2 statistics alone could not prove the rationality of model performances so evaluations of other performance measures were required based on the nse values since all nse values were greater than 0 simulated values of water quantity and quality models were better predictors than the mean observed values also nse statistics were at least satisfactory for water quantity and quality models however negative values of pbias for discharge predictions indicated model overestimation bias nevertheless some peak flows were underestimated in addition low magnitude values of pbias for caso4 concentration predictions represented accurate model simulation moreover both overestimation and underestimation biases existed for nacl concentration predictions also abbaspour et al 2015 recommend a working value of 0 7 for p factor and 1 5 for r factor for uncertainty analysis as can be seen in table 2 all p factor values are greater than 0 7 while all r factor values are smaller than 1 5 appropriate amounts of r factor and p factor generally indicate that the discharge and salt concentration predictions correctly consider the uncertainty of models furthermore the location of peak flows and peak salt concentrations were well predicted for the shekastian station therefore a combination of multiple model performance evaluations totally indicated that swat and swat s models appear to be sufficient for the complex climate soil and other geo environmental conditions related to the hydrology and salinity of the shour river basin 3 4 water balance components the long term mean precipitation is 530 3 mm surface runoff is 1 77 mm at 0 3 of precipitation and evapotranspiration is 374 6 mm at 70 6 of precipitation in the shour river basin more than 50 of precipitation annually is lost to evapotranspiration the percolation loss follows the evapotranspiration the long term mean percolation through the root zone accounts for 32 2 of the total precipitation besides the long term mean water yield is 116 4 mm at 22 of precipitation the mean ratio of the baseflow to the total flow is 0 98 the very high contribution of the baseflow may be related to the high infiltration rates together with runoff run on processes and soil crusted areas across the catchment it is also probable that the groundwater reservoir is closely connected to the stream 3 5 statistical characteristics of the daily salt concentration the spatial and temporal distributions of salts concentrations in the watershed can be delineated using the results of the new proposed integrated methodology the box plots in fig 4 illustrate long term statistical characteristics of the daily caso4 nacl and tds concentrations of the surface runoff root zone shallow aquifer and reach for all subbasins this figure graphically depicts distributions of the minimum lower quartile median upper quartile and maximum caso4 nacl and tds concentrations for all subbasins fig 4 shows that the maximum caso4 concentrations of the reaches are almost a unique value but the maximum nacl concentrations of the reaches are very different values fig 5 shows the long term average caso4 nacl and tds concentrations of the surface runoff fig 5a root zone fig 5b groundwater fig 5c and reach fig 5d for each subbasin of the shour river basin in the northern parts of the basin some upstream reaches have higher average salt concentration than other upstream reaches these saline upstream reaches locate in the subbasins that have a higher average salt concentration of the surface runoff root zone and groundwater than other upstream subbasins the subbasin 6 in the northwestern of the basin is the most saline in the shour river basin saline formations cover a vast area in this subbasin the subbasin 19 located in the south of the basin is one of the most saline subbasins in the middle parts of the basin some subbasins have a lower percentage of the saline formation than other subbasins these subbasins have lower salinity than other subbasins the salt concentration of the root zone generally is higher than the salt concentration of groundwater besides the salt concentration of groundwater is higher than the salt concentration of the surface runoff average caso4 nacl and tds concentrations of the root zone are almost 4 000 26 000 and 30 000 mg l 1 respectively besides average caso4 nacl and tds concentrations of groundwater are nearly 3000 14 000 and 17 000 mg l 1 respectively therefore sources of the root zone and groundwater salinities are almost the same in the shour river basin shallow saline formations cover a few subbasins in the shour river basin but deep saline formations cover all subbasins in the shour river basin therefore the main natural causes of the root zone and groundwater salinities are the existence of deep saline formations in the shour river basin these subterranean sources of the salinity become unsuitable water qualities of the root zone and groundwater deep saline formations increase the salinity of the groundwater recharge this saline recharge affects dissolved salt concentrations of groundwater for all subbasins average caso4 nacl and tds concentrations of the surface runoff are almost 500 4 000 and 4500 mg l 1 respectively because of low interactions of the salinity of the root zone as well as shallow saline formations with the salinity of the surface runoff in the shour river basin the salt concentration of the surface runoff is lower than the salt concentration of the root zone average caso4 nacl and tds concentrations of all reaches are almost 2000 13 000 and 15 000 mg l 1 respectively besides average caso4 nacl andtds concentrations of the most downstream reach are about 3000 15 000 and 18 000 mg l 1 respectively in the shour river basin therefore average caso4 nacl and tds concentrations of the most downstream reach are about 50 15 and 20 respectively higher than average caso4 nacl and tds concentrations of the reach for the whole basin two types of saline formations the shallow saline formation and deep saline formation exist in the shour river basin the third type of the saline formation the reach bed saline formation which is a specific part of the shallow saline formation exists in the bed of the river of the shour river basin however these types of saline formations have comparatively different severities of the salinity e g high saline and low saline in the shour river basin as can be seen in fig 2 shallow saline formations cover a small area in the shour river basin only subbasins 5 6 11 14 and 19 however dissolved salts which enter the root zone through the infiltration and upflux occasionally store in the root zone by the evapotranspiration when rain occurs infiltrating water into the root zone washes stored salts of the root zone furthermore it can be said that a combination of the 1 existence of the shallow saline formation 2 existence of the deep saline formation 3 amount of solid stored salts in the root zone 4 occurrence of the infiltration 5 occurrence of the upflux and 6 occurrence of the evapotranspiration affects the salinity of the surface runoff and soil water of the root zone in the shour river basin for example the subbasin 6 which has a large coverage of shallow and deep saline formations has the highest amount of the salinity of the surface runoff and soil water of the root zone in the shour river basin however the subbasin 3 which has only a large coverage of the deep saline formation has a lower amount of the salinity of the surface runoff and soil water of the root zone than the subbasin 6 moreover the subbasin 11 which has shallow and deep saline formations has a higher amount of the salinity of the surface runoff than the subbasin 3 and has a lower amount of the salinity of the soil water of the root zone than the subbasin 3 besides the subbasin 16 which has not even a large coverage of the deep saline formation has a lower amount of the salinity of the surface runoff and soil water of the root zone than subbasins 3 6 and 11 therefore the salinity of the surface runoff is firstly due to the extent of the existence of the shallow saline formation and secondly due to the extent of the existence of the deep saline formation in the shour river basin nevertheless the salinity of the soil water of the root zone is mainly due to the extent of the existence of both the shallow and deep saline formations in the shour river basin furthermore as can be seen in fig 2 the deep saline formation vastly exists in the shour river basin therefore the salinity of subbasins 1 4 7 10 12 13 and 15 18 is dominantly due to the existence of the deep saline formation also it can be stated that a combination of the 1 existence of the shallow saline formation 2 existence of the deep saline formation 3 amount of solid stored salts in the shallow aquifer and 4 occurrence of the recharge of the shallow aquifer affects the salinity of the groundwater in the shour river basin for example the subbasin 3 and the subbasin 6 have the same amount of groundwater salinity similarly the subbasin 11 and the subbasin 16 have an identical amount of groundwater salinity therefore the salinity of the groundwater is dominantly due to the extent of the existence of the deep saline formation in the shour river basin furthermore it can be said that a combination of the 1 existence of the reach bed saline formation 2 occurrence of the surface runoff 3 occurrence of the lateral flow 4 occurrence of the return flow 5 amount of solid stored salts in the river and 6 occurrence of the evapotranspiration affects the salinity of the river in the shour river basin however reach bed saline formations only exist in subbasins 5 6 and 19 moreover it can be declared that dissolved caso4 salts from reach bed saline formations have considerable effects on caso4 concentrations of the outlet of the shour river basin but dissolved nacl salts from reach bed saline formations have not considerable effects on nacl concentrations of the outlet of the shour river basin therefore various combinations of processes which contribute to the salinity of the surface water root zone groundwater and river of the shour river basin mobilize salts throughout the shour river basin these effects were properly considered with the aid of the swat s model 3 6 loadings of caso4 nacl and tds the results of the swat s can be used for the description of the spatial and temporal distributions of the terrestrial salts transportations through the main hydrologic pathways e g surface runoff lateral flow groundwater flow the recharge of aquifers and the upflux into the unsaturated zone in the watershed fig 6 illustrates the total mass of caso4 nacl and tds transported by the surface runoff fig 6a lateral flow fig 6b and groundwater flow fig 6c into the streamflow for the 1994 2001 period about 68 5 of the total salt loading into the streamflow is from the lateral flow contribution 30 7 of the total salt loading is from the groundwater flow contribution and 0 8 of the total salt loading is from the surface runoff contribution therefore about 99 2 of the total salt loading into the streamflow is from the baseflow in the shour river basin furthermore about 83 of the total salt loading is due to the nacl loading and 17 of the total salt loading is due to the caso4 loading as can be seen in fig 6 northern subbasins generally transport higher amounts of the salt mass to the streamflow than other subbasins fig 7 shows the total mass of caso4 nacl and tds transported by the recharge into the shallow aquifer fig 7a by the upflux into the soil profile fig 7b and by the recharge into the deep aquifer fig 7c in each unit area for the 1994 2001 period nacl transportation is attributed to 83 of the total salt transportation and 17 of the total salt transportation is due to caso4 transportation as shown in fig 7 the transportation of the salt through these mentioned pathways mainly is lower for southern subbasins than northern subbasins this is related to the existence of the saline formations besides fig 2 shows that southern subbasins mainly receive a lower amount of precipitation than northern subbasins therefore precipitation has a significant effect on the transportation of the salt in the shour river basin some subbasins in the middle of the basin have lower vertical salt transportation than other subbasins subbasin 6 has higher salt transportation than many subbasins fig 8 shows the daily loadings ton of caso4 nacl and tds through the surface runoff fig 8a lateral flow fig 8b and groundwater flow fig 8c into the streamflow for the 1994 2001 period the groundwater flow enters the salt into the streamflow continuously but the surface runoff and lateral flow enter the salt into the streamflow discontinuously as can be seen in fig 8 although the total input salt from the surface runoff into the streamflow 45 009 ton is lower than the total input salt from the groundwater flows into the streamflow 1 705 678 ton the maximum daily input salt from the surface runoff into the streamflow 18041 ton day is higher than the maximum daily input salt from the groundwater flow into the streamflow 758 ton day the total input salt from the lateral flow into the streamflow 3 800 739 ton is higher than the total input salt from the groundwater flow into the streamflow moreover the maximum daily input salt from the lateral flow into the streamflow 107 590 ton day is very higher than the maximum daily input salt from the surface runoff into the streamflow 3 7 salt balance components the results of the new proposed integrated methodology can be utilized for the determination of salt mass balance components of the watershed fig 9 shows the relative salt mass balance components of the shour river basin which were adjusted based on the magnitudes of the surface runoff components these components have the lowest values than other components in the shour river basin the shallow aquifer recharge 406 units upflux 243 units streamflow out of the watershed 240 units and lateral flow 104 units are the main transportation components of caso4 in the shour river basin respectively similarly the shallow aquifer recharge 270 units upflux 215 units streamflow out of the watershed 127 units and lateral flow 81 units are the main transportation components of nacl in the shour river basin respectively therefore the shallow aquifer recharge is the most critical salt transportation component in the shour river basin about 75 of the input salts to the shallow aquifer from the soil profile returns to the soil profile through the upflux furthermore as can be seen in fig 10 the lateral flow is the most important component 68 that enters the salts to the streamflow from the land area salt mass balances of the main channel indicate that the dissolution of caso4 has a noticeable effect on caso4 concentration of the reach however the dissolution of nacl has no significant impact on the nacl concentration of the reach this is because of the very low purity of nacl in the reach bed saline formations the results of the proposed model could reveal the significant processes that transport the highest amounts of salts in the shour river basin upward and downward transportations in the soil environment the results of the proposed model could also exhibit the chief process that loads the salts into the river in the shour river basin the loading of salts via the lateral flow into the river 3 8 salt export import for subbasins the catchment salt balance or salt export import ratio is a crucial indicator of a catchment that is undergoing salinization it indicates whether the watershed is in a state of salt mobilization additionally it also provides information about the salinity trend in the catchment the salt balance is also an efficient measure through which to encapsulate all of the salinity processes occurring in a watershed furthermore the salt balance indicates the severity of the salinity problem in an area bugan et al 2015 researchers who calculated salt export import e i ratios have typically used the rainfall salt inputs and streamflow export e g poulsen et al 2006 components of the salt balance for the subbasins considered in this study are the rainfall and input streamflows from the upstream subbasins as the salt imports and the output streamflow from the subbasins as the salt export catchments mobilizing salts are likely to have e i ratios 1 and can be as high as 15 30 cresswell et al 2005 the salt e i ratio of a catchment is therefore an important indicator of catchment hydrology and landscape processes in its own right biggs et al 2013 the caso4 nacl and total salt e i ratios are 13 25 and 21 for the whole shour river basin respectively this indicates that the basin is highly mobilizing salts e i 1 calculated e i ratios related to the caso4 nacl and total salt was provided in table 4 for all subbasins of the shour river basin it can also be argued that e i ratios reflect regional scale processes more than local processes particularly as the necessary data is often only available at the regional scale essential to the correct interpretation of salt e i ratios is a knowledge of where salt is being mobilized from and the key factors influencing the mobilization processes biggs et al 2013 fig 11 illustrates the total salt export import e i ratios for all subbasins of the shour river basin subbasins which have been located in the south and middle of the shour river basin mainly have lower e i ratios compared to the other subbasins rainfall is the only input source of the salt for other headwater subbasins that have higher e i ratios hydrological and geo environmental conditions cause the subbasin 6 in the north of the basin has the highest amount of e i ratio 70 5 and the subbasin 16 in the middle of the basin has the lowest amount of e i ratio 4 5 in comparison with other headwater subbasins 3 9 comparison of the average monthly precipitation discharge salt export and tds of the model predictions a comparison between the average occurred precipitation in the shour river basin discharge salt export and tds of the model predictions at the shekastian station can be seen in fig 12 for each month of the year it can be comprehended from fig 12 that the highest amounts of precipitation occurred from december to march the streamflow is high during this period high flow months of the year compared to the other months the lowest amounts of precipitation almost zero occurred from june to september since tds of water that infiltrates into the soil is remarkably lower than the maximum tds of groundwater for both the root zone and shallow aquifer tds of groundwater has lower values from december to march infiltration has the highest values therefore lower tds values of the baseflow contribution component eventuate to lower tds values of the river than other months of the year precipitation is meager during the dry months from june to october thus the infiltration amount is also inadequate and the groundwater table descends during these months of the year hence the highest tds values of groundwater occur besides the streamflow is very low during these months of the year eventually the tds values of the river are affected by the highest tds values of groundwater therefore the highest tds values of the river occur during these months of the year the highest amounts of salt export occur from december to march because the amount of precipitation frequently decreases after the january month the salt export also decreases often after this month tds of the river has the highest values from jun to october but the discharge of the river has the lowest levels in this period the salt export of the river has the most moderate values in this period conversely salt export has the highest values in the months with the highest amounts of the discharge and the lowest amounts of tds therefore the proposed model results could explain the temporal patterns of salt exports and salt concentrations at the outlet of each subbasin and the outlet of the shour river basin the maximum salt exports and minimum salt concentrations mainly occurred in colder seasons and vice versa a strong correlation was observed between the discharge salt export and salt concentration 3 10 usability of the model the consideration of all natural salt sources and hydrologic pathways is preferable for the development of a watershed scale natural hydro salinity model few deficient models exist that can be used for natural hydro salinity modeling at the watershed scale the bc2c model gilfedder et al 2009 which has a simple structure and uses relatively robust simplifications provides a useful starting point for consideration of the possible impacts of the land use change on the streamflow and salt load the 2csalt model stenson et al 2011 which is a step up in complexity from models such as bc2c has been designed to be applied to unregulated upland catchments with local unconfined aquifers draining into gaining streams this may be an issue for broader applicability outside of the murray darling basin the proposed salinity model for the swat model which uses generally available broad data in comparison with 2csalt and bc2c models profits from the water quantity model of the swat model therefore the swat s model can be employed for the salinity modeling of many small scale to large scale basins throughout the world besides the prepared salinity model compared to 2csalt and bc2c models are operated at daily time step furthermore the swat s model considered more details than 2csalt and bc2c models for salinity modeling therefore this model can be used for more comprehensive land use change climate change and management researches bailey et al 2019 were prepared a salinity module for the swat model which simulates the fate and transport of eight major salt ions so4 ca mg na k cl co3 and hco3 in a watershed system the proposed swat s model assumes that the mass of the salt is conservative and that steady state conditions can model transient conditions approximately the swat s model does not model individual ions only salt as a lumped parameter and thus cation exchange reactions or adsorption are not considered swat is a long term model swat allows several different physical processes to be simulated in a watershed no matter what type of problem is studied with swat water balance is the driving force behind everything that happens in the watershed neitsch et al 2011 similar to the structure and objectives of the swat model the swat s model has been designed to be applied to large systems and for long periods the swat s model s variables and parameters are spatial and temporal averages the swat salinity module of bailey et al 2019 requires initial concentrations of salt ions in the soil water and groundwater using initial concentrations equal to 0 mg l 1 has a significant effect on the results of the salinity module of bailey et al 2019 besides initial salt mineral solid concentrations of bulk soil in the soil and aquifer sediment are some of the required data for running the swat salinity module of bailey et al 2019 initial dissolved concentrations of salts in the soil water and groundwater and initial solid masses of salts in the soil and aquifer have not significant effects on the results of the swat s model however consideration of an adequate warm up period e g several years can adjust the dissolved concentrations and solid masses of salts in the soil profile and shallow aquifer bailey et al 2019 declared that differences between model output and field data highlight the need for better field survey data of salt mineral content in soils the swat s model uses gis maps and utilizes a specific method for consideration of shallow deep and reach bed saline formations the percentage of the existence of saline formations can be adjusted during the calibration process therefore any accessible gis maps of saline formations can be used also the importation of the salt through the rainfall to the soil is included in the swat s model furthermore the model considers dissolution precipitation of salt for the surface runoff soil water groundwater and main channel network the swat salinity module of bailey et al 2019 does not consider dissolution precipitation of salt in the main channel network therefore the swat s model has high usability for salinity modeling at the watershed scale bailey et al 2019 apply the swat salinity module to a highly managed irrigated watershed the swat s model was applied for modeling of the natural salinity at the watershed scale besides the swat s model can be used under data scarce conditions finally it is suggested that the method used for consideration of saline formations in this study can be utilized in other lumped semi distributed and distributed models for the accounting of saline formations 3 11 limitations and notifications some simplifications were considered interactions between different ions were not included in the swat s model it is assumed that the steady state condition exists and the mass of the salt is conservative in addition chemical equilibrium reactions e g complexation and cation exchange are not addressed in this study moreover hrus are not spatially distributed and there is no connection between hrus therefore the transportation of the salt does not consider for continuous media the shallow aquifer is similar to a reservoir for the swat model thus groundwater table depth and groundwater head gradient are not simulated accurately therefore the swat s model does not accurately model the salinity of groundwater the modflow langevin et al 2017 is a well tested and widely used groundwater model the coupled swat modflow model can be utilized for better groundwater salinity modeling in many areas however there is a lack of data describing groundwater systems limiting the use of complex models such as the modflow moreover to better model the groundwater s quantity and quality using the swat and swat s models the depth of the soil profile can be increased to reach the impervious layer it is assumed that the spatial distribution of the saline formations does not vary during the simulation period similar to the land use change the spatial distribution of the saline formations may vary during very long term periods for these cases the spatial distribution of the saline formations can be modified separately after any desired years from the first year of the simulation also the spatial distribution of the saline formations and the maximum dissolved concentrations of different salts may be varied repeatedly during the simulation period when these variations have significant effects on the results of the model those can be considered easily by updating the input data repetitively during the simulation period 4 conclusion the release of salts from saline formations and the fate and transport of salts in the soil and river environments have been little simulated in arid and semi arid regions to estimate salt loads at the watershed scale however defections of salinity models and the lack of observed data are limiting factors the hydrological component of the swat model allows the explicit calculation of different water and salt balance components in this research the proposed swat s model was internally integrated into the swat model to consider natural salinity sources specifically including saline formations transportations of salts from natural salinity sources through landscapes and into rivers and in stream transportations of salts in a watershed also the proposed model takes into account interactions between solid and dissolved salts for soil and river the proposed model s capabilities have been demonstrated by simulating the dynamics of salts in a semi arid watershed where the monitoring network is insufficient the outcomes of the proposed model could show the value of considering saline formations in water and soil salinities of the watershed with adjustable gis maps also the proposed model results could indicate that the salinity of different media of the watershed is a combination of various complex time dependent factors at the subbasin scale the proposed swat s model can be a helpful modeling tool to simulate the fate and transport of other non point source pollution which have the same nature at a watershed scale however the proposed model s capabilities can be improved to include any specific salinity sources and processes such as saline lakes in desired watersheds furthermore it is recommended that other lumped semi distributed and distributed models similarly regard saline formations in their modeling framework declaration of competing interest the author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104906 
25915,salinity is a complex process in watersheds and there is a need for data and modeling salinity in arid and semi arid watersheds development and application of a salinity model to accurately predict the salinity processes are difficult tasks at the watershed scale available salinity models are limited in sufficiently modeling natural salinity processes in watersheds in this study a salinity module is proposed for the soil and water assessment tool swat utilizing new salinity equations to model natural salinity processes in watershed systems data related to the distribution maps of saline formations is used in the modeling process an advantage of this module is that it can use low quality maps for salinity modeling of different salts the proposed methodology can be used to assess the impacts of management practices climate change and land use changes on non point source pollution at a watershed scale graphical abstract image 1 keywords iswat salinity modeling sce ua swat watershed modeling 1 introduction salts are highly soluble in surface water and groundwater and can be mobilized in the landscape by water movement salinity a global challenge to water and soil quality management is a measure of salts content in soil or water and may be of primary or secondary origin kaushal et al 2018 primary salinity occurs naturally in soils and water bodies due to natural salinity processes such as weathering of rocks with salts dissolution from saline formations through hydrologic processes naderi et al 2016 also wind and rain can deposit salts over a lengthy period the dissolution of salts from saline formations is a complicated physical and chemical process the dissolution process is dependent on the dissolved salt concentration of water when the solute concentration increases to a certain degree the precipitation phenomenon stops the solute from dissolving as the solution gets saturated yang et al 2018 secondary salinity is the result of human activities such as deforestation urbanization irrigation and surface mining cañedo argüelles et al 2013 the topography and age of landscapes affect the nature of salinity processes furthermore hydrologic pathways evaporative concentration vegetation patterns nature and depth of regolith all interact to determine salts storage location within landscapes moore et al 2018 excessive salinity makes water unsuitable for domestic use al ansari et al 2019 negatively affects irrigated agriculture majeed and muhammad 2019 damages wildlife habitat lawlor and arellano 2020 limits the ability to recycle water and limits recharge of groundwater given the high cost of treatment saline water is often discharged into water bodies rather than reused the monitoring and modelling of the quantity and salinity of water are vital in water resource management analysis of the transfer of salt and the ratio of salt along the hydrologic pathways to the river is crucial for watershed scale water resource management while there is a lack of information and salinity observed data in many watersheds around the world the reasonable determination of causes and origins of salinity problems is increasingly complex with the influence of land use change water management biodiversity climate change and agricultural production cheng et al 2014 besides water composition particularly in a large watershed reflects multiple salinity sources and processes that mobilize salts and change salt concentrations gitau and chaubey 2010 since the study of all salinity processes of a watershed is an enormous task it is challenging to establish a general salinity model to provide accurate predictions therefore salinity studies historically have been focused on the most crucial salinity processes at the watershed scale salt generation processes salt delivery processes through landscapes and from a landscape into a river and in stream salt transport processes are generally essential processes that can be considered for modelling the fate and transport of salts through a watershed for example the salt mass balance of a watershed the salt export import e i ratio is an efficient mechanism for encapsulating all salinity processes biggs et al 2013 many salinity models have been developed to determine the source transformation and transportation of salts through different media of watersheds these salinity models vary depending on process representations process complexities data requirements scales of the application and tend to focus on particular salt mobilization processes at others expense they can predict the salinity of areas which have no observed data or to evaluate the effects of different management practices and climate regimes on states of salinity issues based on the scale of the application some salinity models such as hydrus šimůnek et al 2013 leachm hutson 2003 swap kroes et al 2017 mt3dms zheng et al 2012 and saltmod oosterbaan 2001 have been used for salinity studies at a local scale also there are several salinity models such as dsm2 anderson and mierzwa 2002 oteq runkel 2010 msm bigmod close et al 2004 iqqm podger 2004 and realm perera et al 2005 that specifically simulate in stream salinity semi distributed watershed scale salinity models such as 2csalt stenson et al 2011 and bc2c gilfedder et al 2009 divide watersheds properties of land use soil type and topography each spatial unit represents similar hydrological behavior the bc2c model which uses a simple water balance approach can be used for regional prioritization and examining land use impacts in watersheds the bc2c model is a rapid assessment tool for the first approximations from hydrogeological information beverly et al 2006 the 2csalt model is a lumped parameter model that quantifies salt export through surface and subsurface contributions into the river and predicts land use change impacts the 2csalt model was exclusively designed for application to the murray darling basin the application of the model is highly questionable in other basins fully distributed watershed scale salinity models such as class teng et al 2008 catsalt tuteja et al 2003 and wec c croton and barry 2001 divide watersheds into hydraulically connected elements such as grids or triangular elements fully distributed salinity models require considerable amounts of data at finer scales and typically have much larger computational requirements than semi distributed salinity models therefore they are not considered feasible for large watersheds currently no model simulates salt transport in all significant hydrologic pathways rain surface runoff soil percolation and leaching lateral flow groundwater flow streamflow at the watershed scale and incorporates major solution reaction chemistry bailey et al 2019 the geographic information system gis allows spatial and exploratory analyses of landscapes dehghani et al 2019 determining the most critical salt sources and salt transport mechanisms in a watershed can be assessed using gis maps a comprehensive salinity model is needed to simulate the fate and transport of salts along all significant hydrologic pathways and consider crucial solution reaction chemistry including the dissolution precipitation gis maps of saline formations can improve salinity modeling at the watershed scale for assessing spatial and temporal distributions of salts contents of saline formations however this has not been well done yet among catchment scale hydrological models soil and water assessment tool swat is a popular semi distributed physically based agro hydrological model used to analyze the impact of water and land management measures on water sediment and chemical fluxes at a daily time step neitsch et al 2011 while swat incorporates modules to deal with different pollutants and chemicals including nitrogen phosphorous pesticides sediments nutrients bacteria and heavy metal it does not include a specific salinity module for transport and transformation of salts throughout a watershed bailey et al 2019 present a watershed scale salt ion fate and transport model by developing a salinity module for the swat model the module accounts for salt loading for each major hydrologic pathway in a watershed setting stream groundwater lateral flow surface runoff tile drain flow for each major salt ion so4 ca mg na k cl co3 and hco3 the module also accounts for principal equilibrium chemistry reactions precipitation dissolution complexation cation exchange however natural salinity sources and processes were disregarded in the model also bailey et al 2019 study applied the model to an irrigated area and chemical reactions do not change in stream salt ion concentrations in the salinity module therefore there is a need to develop new internal salinity modules for the swat model using new salinity equations to include natural salinity sources and processes in soil and river environments at a watershed scale a new watershed scale natural salinity model can consider main salinity processes related to the fate of salts in a watershed that includes the generation of salts transportation of salts through the landscapes transportation of salts from the landscapes into rivers and in stream transport of salts in this study an internal computational module is developed for modeling primary salinity in swat using the fortran language furthermore saline formations have been categorized into shallow deep and reach bed saline formations and included using gis the salinity module accounts for the transport of salts based on the hydrological processes of the swat model also the dissolution and precipitation of salts have been considered for the surface runoff soil water shallow aquifer and river flow 2 methodology the natural sources of salts were incorporated into the proposed swat s model the following sections explain details of the proposed swat s model for watershed scale salinity routing 2 1 hydrological and salinity models a water salt balance model swat s module to simulate discharge and salinity at the catchment scale was developed by modifying the swat model the swat includes subbasin and channel flow routing and each subbasin is further divided into hydrologic response units hrus neitsch et al 2011 the swat s considers the dissolution precipitation and transportation of salts in the watershed the interactions between different cations and anions related to different salts have not been directly assessed however the swat s assumes that the mass of the salt is conservative and that steady state conditions can simulate long term transient conditions approximately since the maximum dissolved concentration of different salts is not unique it is required to model the dissolution precipitation and transportation processes of each salt separately in any desired location as the output of the reaches tds total dissolved solids can be calculated by the summing of different dissolved salt concentrations fig 1 briefly illustrates various components of the swat s blue arrows indicate the displacement of water and black arrows indicate the displacement of the salt and salt mobilization the salt in the solution phase is transported through the root zone with the surface runoff lateral flow and percolation shallow aquifer and the stream reach there is a need to propose appropriate methods and salinity equations to model natural salinity at the watershed scale the main equations used for hydrological modeling and derived for salinity modeling are discussed in the following subsections equations presented in this section are generic to represent each salt type 2 1 1 root zone the amount of precipitation that contributes to surface runoff can be calculated using the scs curve number equation for each hru neitsch et al 2011 1 q s r f r t i a 2 r t i a s where q s r f is the accumulated surface runoff for the hru in a time step daily mm h2o r t is the precipitation depth for the hru in a time step daily mm h2o i a is the initial abstractions which includes the surface storage interception and infiltration before the surface runoff for the hru mm h2o and s is the retention parameter for the hru mm h2o the amount of water that percolates to the next layer is neitsch et al 2011 2 w l y p r c s w l y e x c 1 e x p δ t t t p r c where w l y p r c is the amount of water percolating to the underlying soil layer for the hru in a time step daily mm h2o s w l y e x c is the drainable volume of water in the soil layer in a time step daily mm h2o δ t is the length of the time step hrs and t t p r c is the travel time for the percolation hrs when the soil water of the soil layer is greater than the soil water of the field capacity the lateral flow is calculated as bellow neitsch et al 2011 3 q l a t 0 024 2 s w l y e x c k s a t s l p φ d l h i l l where q l a t is the lateral flow discharged from the hillslope for the hru in a time step one daily mm h2o k s a t is the saturated hydraulic conductivity for the layer mm hrs s l p is the increase in the elevation per unit distance mm mm φ d is the drainable porosity of the soil layer mm mm and l h i l l is the hillslope length m the evaporation of water from a soil layer increases the dissolved salt concentration of the soil layer if the dissolved salt concentration of a soil layer reaches the maximum dissolved salt concentration extra evaporation will cause the salt precipitation therefore the calculated dissolved salt concentration of a soil layer without considering the salt precipitation maybe become higher than the maximum dissolved salt concentration of the soil layer hence an initial dissolved salt concentration of the soil layer at the end of the time step without the consideration of the salt precipitation can be derived using equation 4 afterward as can be seen in equation 5 a final dissolved salt concentration of the soil layer can be derived with the consideration of the salt precipitation 4 c l y s w t s w l y t c l y s w t 1 s w l y t 1 λ c p t r t 1 λ c l y 1 s w t w l y 1 p r c t c s h t c s h t 1 2 w l y u p f t m l y d e p t 1 c l y s w t c l y s w t 1 2 β λ q s r f t q l y l a t t w l y p r c t η τ c l y s w t 1 e l y s o l t where c l y s w t is the initial dissolved salt concentration of the soil layer for the hru at the end of the time step kg salt mm h2o s w l y t is the water content of the soil layer for the hru at the end of the time step mm h2o c l y s w t 1 is the final dissolved salt concentration of the soil layer for the hru at the beginning of the time step kg salt mm h2o s w l y t 1 is the water content of the soil layer for the hru at the beginning of the time step mm h2o λ is a dimensionless factor λ 1 for the first soil layer and λ 0 for other soil layers c p t is the dissolved salt concentration of precipitation for the hru during the time step kg salt mm h2o r t is the precipitation depth for the hru during the time step mm h2o c l y 1 s w t is the final dissolved salt concentration of the upper soil layer for the hru at the end of the time step kg salt mm h2o w l y 1 p r c is the amount of water percolating to the soil layer from the upper soil layer for the hru during the time step mm h2o c s h t is the final dissolved salt concentration of the shallow aquifer for the hru at the end of the time step kg salt mm h2o c s h t 1 is the final dissolved salt concentration of the shallow aquifer for the hru at the beginning of the time step kg salt mm h2o w l y u p f t is the amount of upward flux from the shallow aquifer to the soil layer for the hru during the time step mm h2o m l y d e p t 1 is the amount of the precipitated salt in the soil layer for the hru at the beginning of the time step kg salt ha β is the salt percolation coefficient β 0 for the first soil layer and β 1 for other soil layers η is a dimensionless factor η 1 for the first soil layer η 1 for the second soil layer and η 0 for other soil layers τ is the coefficient related to the upward movement of salt from the first the soil layer to the top 10 mm of the soil profile and e l y s o l t is the amount of water removed from the first soil layer due to the evaporation for the hru during the time step mm h2o with the consideration of the salt precipitation the final dissolved salt concentration of the soil layer at the end of the time step can be derived by comparing the initial dissolved salt concentration of the soil layer at the end of the time step with the maximum dissolved salt concentration for the hru as follow 5 c l y s w t c h r u m a x i f c l y s w t c h r u m a x c l y s w t c l y s w t i f c l y s w t c h r u m a x where c l y s w t is the final dissolved salt concentration of the soil layer for the hru at the end of the time step kg salt mm h2o and c h r u m a x is the maximum dissolved salt concentration for the hru kg salt mm h2o the precipitated salt mass in the soil layer at the end of the time step can be derived as follow 6 m l y d e p t c l y s w t c l y s w t s w l y t 0 5 β λ q s r f t q l y l a t t w l y p r c t where m l y d e p t is the amount of the precipitated salt in the soil layer for the hru at the end of the time step kg salt ha 2 1 2 shallow aquifer the water balance in the shallow aquifer is neitsch et al 2011 7 q s h t q s h t 1 w r g s h t w r g d p q r f t w r e v t where q s h t is the amount of water stored in the shallow aquifer for the hru at the end of the time step mm h2o q s h t 1 is the amount of water stored in the shallow aquifer for the hru at the beginning of the time step mm h2o w r g s h t is the amount of the recharge entering the shallow aquifer for the hru during the time step mm h2o w r g d p t is the amount of the recharge entering the deep aquifer for the hru during the time step mm h2o q r f t is the groundwater flow or baseflow into the main channel for the hru during the time step mm h2o and w r e v t is the amount of water moving from the shallow aquifer to the soil zone in response to water deficiencies for the hru during the time step mm h2o similar to the procedure that was used for the calculation of the dissolved salt concentration of the soil layers an initial dissolved salt concentration of the shallow aquifer at the end of the time step without the consideration of the salt precipitation can be derived using equation 8 a final dissolved salt concentration of the shallow aquifer can be derived with the consideration of the salt precipitation in equation 9 8 c s h t q s h t c s h t 1 q s h t 1 c p e r t 1 w r g s h t m s h d e p t 1 c s h t c s h t 1 2 q r f t w r e v t w r g d p t where c s h t is the initial dissolved salt concentration of the shallow aquifer for the hru during the time step kg salt mm h2o c s h t 1 is the final dissolved salt concentration of the shallow aquifer for the hru at the beginning of the time step kg salt mm h2o c p e r t 1 is the final dissolved salt concentration of the lowest soil layer for the hru at the beginning of the time step kg salt mm h2o and m s h d e p t 1 is the amount of precipitated salt in the shallow aquifer for the hru at the beginning of the time step kg salt ha with the consideration of the salt precipitation the final dissolved salt concentration of the shallow aquifer at the end of the time step can be derived by comparing the initial dissolved salt concentration of the shallow aquifer at the end of the time step with the maximum dissolved salt concentration for the hru as follow 9 c s h t c h r u m a x i f c s h t c h r u m a x c s h t c c h t i f c s h t c h r u m a x where c s h t is the final dissolved salt concentration of the shallow aquifer for the hru at the end of the time step kg salt mm h2o and c h r u m a x is the maximum dissolved salt concentration for the hru kg salt mm h2o the precipitated salt mass in the shallow aquifer at the end of the time step can be derived as follow 10 m s h d e p t c s h t c s h t q s h t 0 5 q r f t w r e v t w r g d p t where m s h d e p t is the amount of the precipitated salt in the shallow aquifer for the hru at the end of the time step kg salt ha 2 1 3 stream reach the swat model calculates the net water yield y l d w in mm to the stream channel for each subbasin by using the following equation neitsch et al 2011 11 y l d w q s r f q l a t q r f q t l where q s r f is the surface runoff mm h2o q l a t is the lateral flow contribution to stream discharge mm h2o q r f is the groundwater contribution to stream discharge mm h2o and q t l is the water lost from the reach via the evaporation and the transmission through the bed mm h2o similar to the procedure that was used for the calculation of the dissolved salt concentration of the soil layers and shallow aquifer an initial dissolved salt concentration of the reach at the end of the time step without the consideration of the salt precipitation can be derived using equation 12 a final dissolved salt concentration of the reach can be derived with the consideration of the salt precipitation as in equation 13 12 c c h t v c h s t g t c c h t 1 v c h s t g t 1 c u p s t v u p s t c s r f s u b t v s r f s u b t c l a t s u b t v l a t s u b t c r f s u b t v r f s u b t m c h d i s t m c h d e p t 10 3 c c h t c c h t 1 2 v d n s t v t l t where c c h t is the initial dissolved salt concentration of stored water in the reach at the end of the time step mg salt l v c h s t g t is the volume of stored water in the reach at the end of the time step m3 c c h t 1 is the final dissolved salt concentration of stored water in the reach at the beginning of the time step mg salt l v c h s t g t 1 is the volume of stored water in the reach at the beginning of the time step c u p s t is the weighted mean of the dissolved salt concentration of the upstream reaches based on the volume of input waters to the reach from the upstream reaches during the time step mg salt l v u p s t is the volume of input waters to the reach from the upstream reaches during the time step m3 c s r f s u b t is the weighted mean of the dissolved salt concentration of the input surface runoff to the reach from the whole subbasin during the time step mg salt l v s r f s u b t is the total volume of the input surface runoff to the reach from the whole subbasin during the time step m3 c l a t s u b t is the weighted mean of the dissolved salt concentration of the lateral inflow to the reach from the whole subbasin during the time step mg salt l v l a t s u b t is the total volume of the lateral inflow to the reach from the whole subbasin during the time step m3 c r f s u b t is the weighted mean of the dissolved salt concentration of the input groundwater flow to the reach from the whole subbasin during the time step mg salt l v r f s u b t is the total volume of the input groundwater flow to the reach from the whole subbasin during the time step m3 m c h d i s t is the amount of the input salt mass to the reach from the bed saline formation for input waters to the reach from the upstream reaches during the time step kg salt m c h d e p t is the amount of the precipitated salt in the reach during the time step kg salt v d n s t is the output water from the reach to the downstream reach during the time step m3 and v t l t is the volume of transmission losses through the side and bottom of the channel during the time step m3 with the consideration of the salt precipitation the final dissolved salt concentration of the reach at the end of the time step can be derived by comparing the initial dissolved salt concentration of the reach at the end of the time step with the maximum dissolved salt concentration of the reach as follow 13 c c h t c c h m a x i f c c h t c c h m a x c c h t c c h t i f c c h t c c h m a x where c c h t is the final dissolved salt concentration of stored water in the reach at the end of the time step mg salt l and c c h m a x is the maximum dissolved salt concentration of the reach mg salt l the precipitated salt mass in the reach at the end of the time step can be derived as follow 14 m c h d e p t c c h t c c h t v c h s t g t 0 5 v d n s t v t l t 10 3 2 2 addressing saline formations in the model three types of saline formations were proposed for each subbasin shallow saline formation and deep saline formation types exist in each hru besides the saline formation type exists in the reach bed the shallow saline formation type locates in the soil profile the deep saline formation type is located in the vadose zone below the soil profile in the proposed method as some parts of the hru may only be composed of the shallow saline formation the shallow saline formation is not considered for salinity modeling inside the soil profile otherwise the precipitated salt related to the saline parts of the hru can strongly affect the dissolved salt concentration of the non saline parts of the hru therefore the effects of the shallow saline formation on the salinity of water are not considered inside the soil profile effects of shallow saline formations are considered for the salinity of output waters from the soil profile surface runoff lateral flow and deep percolation based on the ratio of the shallow saline formation existence in each hru the dissolved salt concentration of the output surface runoff from the hru ϕ s r f α s h c h r u m a x 1 α s h c t o p s w t 1 c t o p s w t 2 is derived by combining the modified maximum dissolved salt concentration for the shallow saline formation area ϕ s r f c h r u m a x with the dissolved salt concentration of the output surface runoff from the hru for the non saline formation area c t o p s w t 1 c t o p s w t 2 via weighing based on the area ratio of the shallow saline formation in each hru α s h ϕ s r f is the correction coefficient for the salt dissolution of the surface runoff the subscript t o p indicates top 10 mm of the soil profile furthermore the dissolved concentration of the output lateral flow from each soil layer in the root zone of the hru α s h c h r u m a x 1 α s h c l y s w t 1 c l y s w t 2 is derived by combining the maximum dissolved salt concentration for the shallow saline formation area c h r u m a x with the dissolved salt concentration of the output lateral flow from each soil layer in the root zone of the hru for the non saline formation area c l y s w t 1 c l y s w t 2 via weighing based on the area ratio of the shallow saline formation in each hru α s h besides the actual dissolved salt concentration of the stored water in each soil layer in the root zone of the hru can be derived in the same way in the proposed method similar to the soil profile salinity modeling of the shallow aquifer is done without consideration of the deep saline formation in the shallow aquifer for each hru to better account for the effects of the deep saline formation on the salinity of groundwater the deep saline formation was located in the vadose zone below the soil profile and above the shallow aquifer for each hru effects of the deep saline formation on water salinity of the deep percolates which ultimately recharges the shallow aquifer were derived by using the maximum area ratio related to the existence of saline formations for each hru α r g this ratio which may range from 0 to 1 is derived by comparing the area ratios of the shallow saline formation α s h and deep saline formation α d p for each hru m a x α s h α d p the dissolved salt concentration of the recharge entering the shallow aquifer α r g c h r u m a x 1 α r g c l o w s w t 1 c l o w s w t 2 is derived by combining the maximum dissolved salt concentration for the saline formation area c h r u m a x with the dissolved salt concentration of the deep percolation for the non saline formation area c l o w s w t 1 c l o w s w t 2 via weighing based on the maximum area ratio of the saline formations in each hru α r g the subscript l o w means the lowest layer of the soil profile in the proposed method the part of the reach bed which contains the saline formation increases the salinity of the surface runoff lateral flow groundwater flow and upstream inflow when those enter the main channel the dissolved salt concentrations of the surface runoff ϕ c h α c h c c h m a x 1 α c h c s r f s u b t lateral flow α c h c c h m a x 1 α c h c l a t s u b t and groundwater flow α c h c c h m a x 1 α c h c r f s u b t into the main channel are derived by combining the modified maximum dissolved salt concentrations of the saline formation area of the reach bed ϕ c h c c h m a x or c c h m a x with the dissolved salt concentration of the surface runoff c s r f s u b t lateral flow c l a t s u b t and groundwater flow c r f s u b t into the main channel for the non saline formation area of the reach bed this is done via weighing based on the area ratio of the saline formation of the reach bed α c h because the upstream inflow flows entirely over the saline formation of the reach bed the salt dissolution for the upstream inflow ϕ c h c c h m a x c c h u p s t when ϕ c h c c h m a x c c h u p s t otherwise 0 when ϕ c h c c h m a x c c h u p s t is derived based on the modified maximum dissolved salt concentration of the saline formation area of the reach bed ϕ c h c c h m a x ϕ c h is the correction coefficient for the salt dissolution of input waters into the reach via the upstream reaches and the surface runoff this proposed method for consideration of the saline formations is helpful when the location and area of saline formations are not accurately delineated for a large area moreover high quality maps of the saline formations usually do not exist the location and area of saline formations can be adjusted during the calibration process using the proposed method also since each separate salt e g nacl and caso4 has specific saline formation maps the explained method can be used for simultaneously modeling of different salts furthermore the purity of each salt in the saline formation can also be defined using the proposed method 2 3 model sensitivity analysis and calibration global sensitivity analysis was performed to identify and rank the most responsive parameters that have a significant impact on the particular basin in the global sensitivity analysis sensitivities of parameters are estimates of the average changes in the objective function resulting from changes in each parameter while changing all the rest parameters the t test was used to identify the relative significance of the parameters t statistic and p values were obtained for the sensitivity analysis the t statistic shows the measure of sensitivity and p value provides the significance of the parameters if t statistic for a particular parameter is high and p value is low then that parameter is considered to have a more significant effect on outputs abbaspour 2015 the calibration was done by applying the shuffled complex evolution optimization algorithm method sce ua naeini et al 2019 in the uncsim package version 1 1c reichert 2005 the uncsim is a program package for statistical inference and sensitivity identifiability and uncertainty analysis and can be downloaded freely the iswat interface yang 2006 was prepared based on the input and output files of the swat version 2000 the iswat interface does not consider salinity parameters therefore the iswat interface was modified and the ability to consider salinity parameters was added to the iswat interface the model performance was assessed using the coefficient of determination r2 nash sutcliffe efficiency nse mccuen et al 2006 percent bias pbias gupta et al 1999 and rmse observations standard deviation ratio rsr moriasi et al 2007 besides the goodness of model performance in terms of calibration and uncertainty level is evaluated using the p factor and the r factor indices abbaspour 2015 since salinity data had lower precision than discharge data salt concentration calibrations without using the parameters that affect the discharge were performed separately after discharge calibration therefore the parameters that affect the discharge were not used for salt concentration calibrations the best group of parameters was selected manually based on the nse of all simulation results after completion of the calibration process 2 4 description of the study area the shour river basin with an area of about 622 km2 is located in the south of iran between 29 40 n to 30 10 n and 51 10 e to 51 40 e the location of the shour river basin in the south of iran with the swat delineated subbasins dem map river network gauging station average precipitation average potential evapotranspiration the distribution map of shallow saline formations and distribution map of deep saline formations were illustrated in fig 2 there are many hills and mountains in the basin the average elevation of the basin is 1124 m with a minimum of 669 m and a maximum of 2034 m the average slope of the basin is 23 percent with a semi arid climate the basin receives an average annual precipitation of approximately 540 mm year the average potential evapotranspiration is about 1750 mm the average discharge of the river at the terminal gauge station of shekastian fig 2 is 2 m3 s 1 abgeer 2013 the salt concentration of the river has not been measured continuously in the shour river basin the average observed tds of the river is about 17 000 mg l 1 at the shekastian station maximum and minimum observed tds values of the river are about 68 000 mg l 1 and 1000 mg l 1 at the shekastian station respectively sodium chloride and calcium sulfate salts are the main soluble salts in the shour river basin that can be dissolved from the saline formations human activities have insignificant effects on the salinity of the soil and water in the shour river basin abgeer 2013 a surface soil layer which is in red violet color in the distribution map of shallow saline formations in fig 2 is composed of the gypsic and saline limes sandstones and silty rocks the salinity and alkalinity of this surface soil layer are high abgeer 2013 shallow saline formations do not cover an extensive area in the shour river basin nevertheless shallow saline formations are mainly located adjacent to some upstream reaches the salinity of these upstream reaches affects the salinity in downstream reaches the out cropped geological formation in the area is gachsaran evaporite rock miocene the deep salt rock of the basin mainly consists of the anhydride salt grey and red marl alternating with the anhydride argillaceous limestone and limestone the gachsaran evaporite rock which is in grey color in the distribution map of deep saline formations in fig 2 is a ductile rock unit with a total thickness of about 1 6 km abgeer 2013 deep saline formations cover an extensive area in the shour river basin therefore the dissolution of salts from deep saline formations is the leading cause of the salinity in the shour river basin these dissolved salts are transported vertically and horizontally by water flows in the basin available gis maps sparsely depict distributions of shallow deep and reach bed saline formations in the shour river basin however available gis maps present coarse information on the purity of different salts in saline formations and their distributions in the shour river basin to address this data insufficiency the newly proposed methodology is used to simulate information on salinity the input data is adjusted separately for each salt during the calibration process 2 5 model setup the basic swat version 2012 model includes input data on soil from the global map of the food and agriculture organization of the united nations fao 1995 with data on 5000 soil types comprising two layers 0 30 and 30 100 cm depth at a spatial resolution of 10 km the soil in the study area is a mixture of sand silt clay and organic material with mostly loamy texture the land use map was obtained from the usgs global land use land cover characterization glcc database with a spatial resolution of 1 km which distinguish 24 land use and land cover classes usgs 2018a shrubland is the dominant land use covering 93 of the watershed area the digital elevation model at a 30 m resolution was obtained from the united states geological survey usgs 2018b the river map of major rivers was obtained from the iran water resources management company climate data were obtained from the climate forecast system reanalysis cfsr ncep 2018 the weather input includes the daily precipitation maximum and minimum daily air temperature relative humidity solar radiation and wind speed the spatial variability within the watershed was represented by 19 sub watersheds which were further sub divided into hydrologic response units hrus based on land use soil and slope characteristic features this resulted in 73 hrus defined using thresholds of 5 10 and 10 for land use soil and slope classes respectively also five elevation bands were used in the model to adjust the temperature and rainfall based on the subbasin elevation variation the local water authorities provided discharge and salt concentration data wrm 2020 other maps and data were provided by the abgeer consulting engineers company abgeer 2013 monthly observed discharge nacl concentration and caso4 concentration data were used for the model calibration 1994 2001 and validation 1990 1993 initial dissolved salt concentrations and solid salt masses in the soil layers of the root zone and shallow aquifer were considered zero five years of simulations from 1985 to 1989 of the climate data were used as a warm up period to account for instability in the soil water and salt balance computations caused by the initial conditions di luzio et al 2002 the error of the discharge and salt concentration data were considered 10 and 20 respectively 3 results and discussion 3 1 choice and the prior distribution of parameters twenty seven parameters were used in the model calibration of discharge seven parameters caso4 concentration ten parameters and nacl concentration ten parameters parameters were selected based on sensitivity analysis and preliminary calibrations all selected parameters of water quantity and quality models had uniform prior distributions within reasonable ranges these ranges were defined based on the recommendations given in the swat user manual neitsch et al 2011 and our knowledge table 1 provides an overview of the parameters that were used for model calibrations with their marginal prior distributions 3 2 description of parameters that were used for model calibration the stream flow was highly sensitive to parameters related to soil characteristics these parameters had high t stat and low p value the value of the available water capacity sol awc of the soil layer was increased in comparison with the initial input data during the calibration process moreover the value of the moist bulk density sol bd of the soil layer was increased therefore the field capacity of the soil layer was increased the increase in water holding also increased the potential for more evapotranspiration by vegetation and evaporation from the soil layers to better match the baseflows the curve number for moisture condition ii cn2 was decreased therefore the infiltration rate of the soil was increased the increase in the infiltration rate and the saturated hydraulic conductivity sol k at the same time increased the baseflows also the soil evaporation compensation factor esco was decreased thus higher evaporative demand from lower levels occurred the groundwater revap coefficient gw revap was increased therefore the rate of the transfer from the shallow aquifer into the overlying root zone was increased furthermore due to the decrease in the threshold depth of water in the shallow aquifer for revap revapmn the movement of water from the shallow aquifer to the root zone occurred at shorter time steps the initial value of the maximum dissolved nacl concentration was considered at 357 318 mg l 1 moreover the initial value of the maximum dissolved caso4 concentration was considered 2043 mg l 1 the initial values were estimated based on the solubility of these salts shaw et al 2011 values of area ratios related to the existence of shallow deep and reach bed saline formations were decreased during the calibration process therefore saline formations do not entirely consist of the pure salts and impurities exist in the saline formations the purity of nacl is especially very low in the saline formations the summation of the area ratios related to the existence of nacl and caso4 in saline formations sometimes is greater than 1 hence nacl and caso4 do not uniformly distribute vertically in saline formations besides values of the maximum dissolved salts concentrations were varied during the calibration process compared to the initial values values of the maximum dissolved salts concentrations were reached to different values for the hrus and reaches this indicates that the maximum dissolved salts concentrations are not unique values and can be varied based on the chemical and physical properties of the soil and water environments e g cation exchange reactions and adsorption the sensitivity analysis results indicated that the dissolution of nacl predominantly occurred in the land while the dissolution of caso4 occurred in both land and river therefore the proposed model results could distinguish dominant salinity processes in the shour river basin the dissolution of caso4 and nacl from the shallow and deep saline formations and the dissolution of caso4 from the reach bed saline formations 3 3 evaluation of the model performance although streamflow calibration performances of recent swat model application studies in arid and semi arid areas are acceptable water quality calibration performances are not at desired levels because of limited data availability however water quality models can still yield robust information in arid and semi arid areas özcan et al 2017 it is not uncommon that swat and other fate and transport models are used under data scarce conditions in the ungauged basins swat model is mostly used for hydrological predictions özcan et al 2017 the actual results of models only are achieved by the correct consideration of different processes throughout basins the proper utilization of limited available data plays a crucial role in achieving the right processes in this study we propose an appropriate framework for the correct estimation of natural salinity processes at the watershed scale even when there is data scarcity the quantity of the baseflow is very low less than 0 1 m3 s 1 in the shour river basin for some months of the year salt concentrations of output waters from the shour river basin are the highest values during these months of the year therefore correct predictions of very low values of discharges are essential for accurate predictions of the highest values of salt concentrations this was done by adjusting the shallow aquifer parameters baseflow alpha factor alpha bf and the threshold depth of water in the shallow aquifer required for return flow to occur gwqmn before the primary calibration process based on the observations of discharge in months that have very low values of the discharge and baseflow simiulated loadings of caso4 and nacl via the surface runoff lateral flow and groundwater flow to the river network were calibrated using observed discharges caso4 concentrations and nacl concentrations on a monthly basis at the shekastian gauge station for the shour river basin the performance results of the model are seen in table 2 fig 3 shows a comparison between the simulated and observed discharges caso4 concentrations and nacl concentrations at the shekastian station the shaded region is a 95 prediction uncertainty band 95 ppu the blue line shows the best model simulation about two thirds of the data were used for the calibration and the remaining for the validation in this study in order to compensate the possible bias related to the limited observed data model performances were evaluated based on the 1 multiple statistical performance measures including r2 nse pbias and rsr 2 comparison of the observed data with 95 prediction uncertainty band 95ppu for the simulated data and 3 graphical performance measures as can be seen in table 3 statistical model performances were evaluated based on the general performance evaluation criteria conformed from moriasi et al 2007 and moriasi et al 2015 for the monthly discharge caso4 concentration and nacl concentration the evaluation of values of r2 statistics indicated that collinearity between simulated and observed data were at least satisfactory for water quantity and quality models thus error variances of models were at least satisfactory nevertheless these r2 statistics might be oversensitive to peak values of discharges and salt concentrations and insensitive to additive and proportional differences between model predictions and observed data thus r2 statistics alone could not prove the rationality of model performances so evaluations of other performance measures were required based on the nse values since all nse values were greater than 0 simulated values of water quantity and quality models were better predictors than the mean observed values also nse statistics were at least satisfactory for water quantity and quality models however negative values of pbias for discharge predictions indicated model overestimation bias nevertheless some peak flows were underestimated in addition low magnitude values of pbias for caso4 concentration predictions represented accurate model simulation moreover both overestimation and underestimation biases existed for nacl concentration predictions also abbaspour et al 2015 recommend a working value of 0 7 for p factor and 1 5 for r factor for uncertainty analysis as can be seen in table 2 all p factor values are greater than 0 7 while all r factor values are smaller than 1 5 appropriate amounts of r factor and p factor generally indicate that the discharge and salt concentration predictions correctly consider the uncertainty of models furthermore the location of peak flows and peak salt concentrations were well predicted for the shekastian station therefore a combination of multiple model performance evaluations totally indicated that swat and swat s models appear to be sufficient for the complex climate soil and other geo environmental conditions related to the hydrology and salinity of the shour river basin 3 4 water balance components the long term mean precipitation is 530 3 mm surface runoff is 1 77 mm at 0 3 of precipitation and evapotranspiration is 374 6 mm at 70 6 of precipitation in the shour river basin more than 50 of precipitation annually is lost to evapotranspiration the percolation loss follows the evapotranspiration the long term mean percolation through the root zone accounts for 32 2 of the total precipitation besides the long term mean water yield is 116 4 mm at 22 of precipitation the mean ratio of the baseflow to the total flow is 0 98 the very high contribution of the baseflow may be related to the high infiltration rates together with runoff run on processes and soil crusted areas across the catchment it is also probable that the groundwater reservoir is closely connected to the stream 3 5 statistical characteristics of the daily salt concentration the spatial and temporal distributions of salts concentrations in the watershed can be delineated using the results of the new proposed integrated methodology the box plots in fig 4 illustrate long term statistical characteristics of the daily caso4 nacl and tds concentrations of the surface runoff root zone shallow aquifer and reach for all subbasins this figure graphically depicts distributions of the minimum lower quartile median upper quartile and maximum caso4 nacl and tds concentrations for all subbasins fig 4 shows that the maximum caso4 concentrations of the reaches are almost a unique value but the maximum nacl concentrations of the reaches are very different values fig 5 shows the long term average caso4 nacl and tds concentrations of the surface runoff fig 5a root zone fig 5b groundwater fig 5c and reach fig 5d for each subbasin of the shour river basin in the northern parts of the basin some upstream reaches have higher average salt concentration than other upstream reaches these saline upstream reaches locate in the subbasins that have a higher average salt concentration of the surface runoff root zone and groundwater than other upstream subbasins the subbasin 6 in the northwestern of the basin is the most saline in the shour river basin saline formations cover a vast area in this subbasin the subbasin 19 located in the south of the basin is one of the most saline subbasins in the middle parts of the basin some subbasins have a lower percentage of the saline formation than other subbasins these subbasins have lower salinity than other subbasins the salt concentration of the root zone generally is higher than the salt concentration of groundwater besides the salt concentration of groundwater is higher than the salt concentration of the surface runoff average caso4 nacl and tds concentrations of the root zone are almost 4 000 26 000 and 30 000 mg l 1 respectively besides average caso4 nacl and tds concentrations of groundwater are nearly 3000 14 000 and 17 000 mg l 1 respectively therefore sources of the root zone and groundwater salinities are almost the same in the shour river basin shallow saline formations cover a few subbasins in the shour river basin but deep saline formations cover all subbasins in the shour river basin therefore the main natural causes of the root zone and groundwater salinities are the existence of deep saline formations in the shour river basin these subterranean sources of the salinity become unsuitable water qualities of the root zone and groundwater deep saline formations increase the salinity of the groundwater recharge this saline recharge affects dissolved salt concentrations of groundwater for all subbasins average caso4 nacl and tds concentrations of the surface runoff are almost 500 4 000 and 4500 mg l 1 respectively because of low interactions of the salinity of the root zone as well as shallow saline formations with the salinity of the surface runoff in the shour river basin the salt concentration of the surface runoff is lower than the salt concentration of the root zone average caso4 nacl and tds concentrations of all reaches are almost 2000 13 000 and 15 000 mg l 1 respectively besides average caso4 nacl andtds concentrations of the most downstream reach are about 3000 15 000 and 18 000 mg l 1 respectively in the shour river basin therefore average caso4 nacl and tds concentrations of the most downstream reach are about 50 15 and 20 respectively higher than average caso4 nacl and tds concentrations of the reach for the whole basin two types of saline formations the shallow saline formation and deep saline formation exist in the shour river basin the third type of the saline formation the reach bed saline formation which is a specific part of the shallow saline formation exists in the bed of the river of the shour river basin however these types of saline formations have comparatively different severities of the salinity e g high saline and low saline in the shour river basin as can be seen in fig 2 shallow saline formations cover a small area in the shour river basin only subbasins 5 6 11 14 and 19 however dissolved salts which enter the root zone through the infiltration and upflux occasionally store in the root zone by the evapotranspiration when rain occurs infiltrating water into the root zone washes stored salts of the root zone furthermore it can be said that a combination of the 1 existence of the shallow saline formation 2 existence of the deep saline formation 3 amount of solid stored salts in the root zone 4 occurrence of the infiltration 5 occurrence of the upflux and 6 occurrence of the evapotranspiration affects the salinity of the surface runoff and soil water of the root zone in the shour river basin for example the subbasin 6 which has a large coverage of shallow and deep saline formations has the highest amount of the salinity of the surface runoff and soil water of the root zone in the shour river basin however the subbasin 3 which has only a large coverage of the deep saline formation has a lower amount of the salinity of the surface runoff and soil water of the root zone than the subbasin 6 moreover the subbasin 11 which has shallow and deep saline formations has a higher amount of the salinity of the surface runoff than the subbasin 3 and has a lower amount of the salinity of the soil water of the root zone than the subbasin 3 besides the subbasin 16 which has not even a large coverage of the deep saline formation has a lower amount of the salinity of the surface runoff and soil water of the root zone than subbasins 3 6 and 11 therefore the salinity of the surface runoff is firstly due to the extent of the existence of the shallow saline formation and secondly due to the extent of the existence of the deep saline formation in the shour river basin nevertheless the salinity of the soil water of the root zone is mainly due to the extent of the existence of both the shallow and deep saline formations in the shour river basin furthermore as can be seen in fig 2 the deep saline formation vastly exists in the shour river basin therefore the salinity of subbasins 1 4 7 10 12 13 and 15 18 is dominantly due to the existence of the deep saline formation also it can be stated that a combination of the 1 existence of the shallow saline formation 2 existence of the deep saline formation 3 amount of solid stored salts in the shallow aquifer and 4 occurrence of the recharge of the shallow aquifer affects the salinity of the groundwater in the shour river basin for example the subbasin 3 and the subbasin 6 have the same amount of groundwater salinity similarly the subbasin 11 and the subbasin 16 have an identical amount of groundwater salinity therefore the salinity of the groundwater is dominantly due to the extent of the existence of the deep saline formation in the shour river basin furthermore it can be said that a combination of the 1 existence of the reach bed saline formation 2 occurrence of the surface runoff 3 occurrence of the lateral flow 4 occurrence of the return flow 5 amount of solid stored salts in the river and 6 occurrence of the evapotranspiration affects the salinity of the river in the shour river basin however reach bed saline formations only exist in subbasins 5 6 and 19 moreover it can be declared that dissolved caso4 salts from reach bed saline formations have considerable effects on caso4 concentrations of the outlet of the shour river basin but dissolved nacl salts from reach bed saline formations have not considerable effects on nacl concentrations of the outlet of the shour river basin therefore various combinations of processes which contribute to the salinity of the surface water root zone groundwater and river of the shour river basin mobilize salts throughout the shour river basin these effects were properly considered with the aid of the swat s model 3 6 loadings of caso4 nacl and tds the results of the swat s can be used for the description of the spatial and temporal distributions of the terrestrial salts transportations through the main hydrologic pathways e g surface runoff lateral flow groundwater flow the recharge of aquifers and the upflux into the unsaturated zone in the watershed fig 6 illustrates the total mass of caso4 nacl and tds transported by the surface runoff fig 6a lateral flow fig 6b and groundwater flow fig 6c into the streamflow for the 1994 2001 period about 68 5 of the total salt loading into the streamflow is from the lateral flow contribution 30 7 of the total salt loading is from the groundwater flow contribution and 0 8 of the total salt loading is from the surface runoff contribution therefore about 99 2 of the total salt loading into the streamflow is from the baseflow in the shour river basin furthermore about 83 of the total salt loading is due to the nacl loading and 17 of the total salt loading is due to the caso4 loading as can be seen in fig 6 northern subbasins generally transport higher amounts of the salt mass to the streamflow than other subbasins fig 7 shows the total mass of caso4 nacl and tds transported by the recharge into the shallow aquifer fig 7a by the upflux into the soil profile fig 7b and by the recharge into the deep aquifer fig 7c in each unit area for the 1994 2001 period nacl transportation is attributed to 83 of the total salt transportation and 17 of the total salt transportation is due to caso4 transportation as shown in fig 7 the transportation of the salt through these mentioned pathways mainly is lower for southern subbasins than northern subbasins this is related to the existence of the saline formations besides fig 2 shows that southern subbasins mainly receive a lower amount of precipitation than northern subbasins therefore precipitation has a significant effect on the transportation of the salt in the shour river basin some subbasins in the middle of the basin have lower vertical salt transportation than other subbasins subbasin 6 has higher salt transportation than many subbasins fig 8 shows the daily loadings ton of caso4 nacl and tds through the surface runoff fig 8a lateral flow fig 8b and groundwater flow fig 8c into the streamflow for the 1994 2001 period the groundwater flow enters the salt into the streamflow continuously but the surface runoff and lateral flow enter the salt into the streamflow discontinuously as can be seen in fig 8 although the total input salt from the surface runoff into the streamflow 45 009 ton is lower than the total input salt from the groundwater flows into the streamflow 1 705 678 ton the maximum daily input salt from the surface runoff into the streamflow 18041 ton day is higher than the maximum daily input salt from the groundwater flow into the streamflow 758 ton day the total input salt from the lateral flow into the streamflow 3 800 739 ton is higher than the total input salt from the groundwater flow into the streamflow moreover the maximum daily input salt from the lateral flow into the streamflow 107 590 ton day is very higher than the maximum daily input salt from the surface runoff into the streamflow 3 7 salt balance components the results of the new proposed integrated methodology can be utilized for the determination of salt mass balance components of the watershed fig 9 shows the relative salt mass balance components of the shour river basin which were adjusted based on the magnitudes of the surface runoff components these components have the lowest values than other components in the shour river basin the shallow aquifer recharge 406 units upflux 243 units streamflow out of the watershed 240 units and lateral flow 104 units are the main transportation components of caso4 in the shour river basin respectively similarly the shallow aquifer recharge 270 units upflux 215 units streamflow out of the watershed 127 units and lateral flow 81 units are the main transportation components of nacl in the shour river basin respectively therefore the shallow aquifer recharge is the most critical salt transportation component in the shour river basin about 75 of the input salts to the shallow aquifer from the soil profile returns to the soil profile through the upflux furthermore as can be seen in fig 10 the lateral flow is the most important component 68 that enters the salts to the streamflow from the land area salt mass balances of the main channel indicate that the dissolution of caso4 has a noticeable effect on caso4 concentration of the reach however the dissolution of nacl has no significant impact on the nacl concentration of the reach this is because of the very low purity of nacl in the reach bed saline formations the results of the proposed model could reveal the significant processes that transport the highest amounts of salts in the shour river basin upward and downward transportations in the soil environment the results of the proposed model could also exhibit the chief process that loads the salts into the river in the shour river basin the loading of salts via the lateral flow into the river 3 8 salt export import for subbasins the catchment salt balance or salt export import ratio is a crucial indicator of a catchment that is undergoing salinization it indicates whether the watershed is in a state of salt mobilization additionally it also provides information about the salinity trend in the catchment the salt balance is also an efficient measure through which to encapsulate all of the salinity processes occurring in a watershed furthermore the salt balance indicates the severity of the salinity problem in an area bugan et al 2015 researchers who calculated salt export import e i ratios have typically used the rainfall salt inputs and streamflow export e g poulsen et al 2006 components of the salt balance for the subbasins considered in this study are the rainfall and input streamflows from the upstream subbasins as the salt imports and the output streamflow from the subbasins as the salt export catchments mobilizing salts are likely to have e i ratios 1 and can be as high as 15 30 cresswell et al 2005 the salt e i ratio of a catchment is therefore an important indicator of catchment hydrology and landscape processes in its own right biggs et al 2013 the caso4 nacl and total salt e i ratios are 13 25 and 21 for the whole shour river basin respectively this indicates that the basin is highly mobilizing salts e i 1 calculated e i ratios related to the caso4 nacl and total salt was provided in table 4 for all subbasins of the shour river basin it can also be argued that e i ratios reflect regional scale processes more than local processes particularly as the necessary data is often only available at the regional scale essential to the correct interpretation of salt e i ratios is a knowledge of where salt is being mobilized from and the key factors influencing the mobilization processes biggs et al 2013 fig 11 illustrates the total salt export import e i ratios for all subbasins of the shour river basin subbasins which have been located in the south and middle of the shour river basin mainly have lower e i ratios compared to the other subbasins rainfall is the only input source of the salt for other headwater subbasins that have higher e i ratios hydrological and geo environmental conditions cause the subbasin 6 in the north of the basin has the highest amount of e i ratio 70 5 and the subbasin 16 in the middle of the basin has the lowest amount of e i ratio 4 5 in comparison with other headwater subbasins 3 9 comparison of the average monthly precipitation discharge salt export and tds of the model predictions a comparison between the average occurred precipitation in the shour river basin discharge salt export and tds of the model predictions at the shekastian station can be seen in fig 12 for each month of the year it can be comprehended from fig 12 that the highest amounts of precipitation occurred from december to march the streamflow is high during this period high flow months of the year compared to the other months the lowest amounts of precipitation almost zero occurred from june to september since tds of water that infiltrates into the soil is remarkably lower than the maximum tds of groundwater for both the root zone and shallow aquifer tds of groundwater has lower values from december to march infiltration has the highest values therefore lower tds values of the baseflow contribution component eventuate to lower tds values of the river than other months of the year precipitation is meager during the dry months from june to october thus the infiltration amount is also inadequate and the groundwater table descends during these months of the year hence the highest tds values of groundwater occur besides the streamflow is very low during these months of the year eventually the tds values of the river are affected by the highest tds values of groundwater therefore the highest tds values of the river occur during these months of the year the highest amounts of salt export occur from december to march because the amount of precipitation frequently decreases after the january month the salt export also decreases often after this month tds of the river has the highest values from jun to october but the discharge of the river has the lowest levels in this period the salt export of the river has the most moderate values in this period conversely salt export has the highest values in the months with the highest amounts of the discharge and the lowest amounts of tds therefore the proposed model results could explain the temporal patterns of salt exports and salt concentrations at the outlet of each subbasin and the outlet of the shour river basin the maximum salt exports and minimum salt concentrations mainly occurred in colder seasons and vice versa a strong correlation was observed between the discharge salt export and salt concentration 3 10 usability of the model the consideration of all natural salt sources and hydrologic pathways is preferable for the development of a watershed scale natural hydro salinity model few deficient models exist that can be used for natural hydro salinity modeling at the watershed scale the bc2c model gilfedder et al 2009 which has a simple structure and uses relatively robust simplifications provides a useful starting point for consideration of the possible impacts of the land use change on the streamflow and salt load the 2csalt model stenson et al 2011 which is a step up in complexity from models such as bc2c has been designed to be applied to unregulated upland catchments with local unconfined aquifers draining into gaining streams this may be an issue for broader applicability outside of the murray darling basin the proposed salinity model for the swat model which uses generally available broad data in comparison with 2csalt and bc2c models profits from the water quantity model of the swat model therefore the swat s model can be employed for the salinity modeling of many small scale to large scale basins throughout the world besides the prepared salinity model compared to 2csalt and bc2c models are operated at daily time step furthermore the swat s model considered more details than 2csalt and bc2c models for salinity modeling therefore this model can be used for more comprehensive land use change climate change and management researches bailey et al 2019 were prepared a salinity module for the swat model which simulates the fate and transport of eight major salt ions so4 ca mg na k cl co3 and hco3 in a watershed system the proposed swat s model assumes that the mass of the salt is conservative and that steady state conditions can model transient conditions approximately the swat s model does not model individual ions only salt as a lumped parameter and thus cation exchange reactions or adsorption are not considered swat is a long term model swat allows several different physical processes to be simulated in a watershed no matter what type of problem is studied with swat water balance is the driving force behind everything that happens in the watershed neitsch et al 2011 similar to the structure and objectives of the swat model the swat s model has been designed to be applied to large systems and for long periods the swat s model s variables and parameters are spatial and temporal averages the swat salinity module of bailey et al 2019 requires initial concentrations of salt ions in the soil water and groundwater using initial concentrations equal to 0 mg l 1 has a significant effect on the results of the salinity module of bailey et al 2019 besides initial salt mineral solid concentrations of bulk soil in the soil and aquifer sediment are some of the required data for running the swat salinity module of bailey et al 2019 initial dissolved concentrations of salts in the soil water and groundwater and initial solid masses of salts in the soil and aquifer have not significant effects on the results of the swat s model however consideration of an adequate warm up period e g several years can adjust the dissolved concentrations and solid masses of salts in the soil profile and shallow aquifer bailey et al 2019 declared that differences between model output and field data highlight the need for better field survey data of salt mineral content in soils the swat s model uses gis maps and utilizes a specific method for consideration of shallow deep and reach bed saline formations the percentage of the existence of saline formations can be adjusted during the calibration process therefore any accessible gis maps of saline formations can be used also the importation of the salt through the rainfall to the soil is included in the swat s model furthermore the model considers dissolution precipitation of salt for the surface runoff soil water groundwater and main channel network the swat salinity module of bailey et al 2019 does not consider dissolution precipitation of salt in the main channel network therefore the swat s model has high usability for salinity modeling at the watershed scale bailey et al 2019 apply the swat salinity module to a highly managed irrigated watershed the swat s model was applied for modeling of the natural salinity at the watershed scale besides the swat s model can be used under data scarce conditions finally it is suggested that the method used for consideration of saline formations in this study can be utilized in other lumped semi distributed and distributed models for the accounting of saline formations 3 11 limitations and notifications some simplifications were considered interactions between different ions were not included in the swat s model it is assumed that the steady state condition exists and the mass of the salt is conservative in addition chemical equilibrium reactions e g complexation and cation exchange are not addressed in this study moreover hrus are not spatially distributed and there is no connection between hrus therefore the transportation of the salt does not consider for continuous media the shallow aquifer is similar to a reservoir for the swat model thus groundwater table depth and groundwater head gradient are not simulated accurately therefore the swat s model does not accurately model the salinity of groundwater the modflow langevin et al 2017 is a well tested and widely used groundwater model the coupled swat modflow model can be utilized for better groundwater salinity modeling in many areas however there is a lack of data describing groundwater systems limiting the use of complex models such as the modflow moreover to better model the groundwater s quantity and quality using the swat and swat s models the depth of the soil profile can be increased to reach the impervious layer it is assumed that the spatial distribution of the saline formations does not vary during the simulation period similar to the land use change the spatial distribution of the saline formations may vary during very long term periods for these cases the spatial distribution of the saline formations can be modified separately after any desired years from the first year of the simulation also the spatial distribution of the saline formations and the maximum dissolved concentrations of different salts may be varied repeatedly during the simulation period when these variations have significant effects on the results of the model those can be considered easily by updating the input data repetitively during the simulation period 4 conclusion the release of salts from saline formations and the fate and transport of salts in the soil and river environments have been little simulated in arid and semi arid regions to estimate salt loads at the watershed scale however defections of salinity models and the lack of observed data are limiting factors the hydrological component of the swat model allows the explicit calculation of different water and salt balance components in this research the proposed swat s model was internally integrated into the swat model to consider natural salinity sources specifically including saline formations transportations of salts from natural salinity sources through landscapes and into rivers and in stream transportations of salts in a watershed also the proposed model takes into account interactions between solid and dissolved salts for soil and river the proposed model s capabilities have been demonstrated by simulating the dynamics of salts in a semi arid watershed where the monitoring network is insufficient the outcomes of the proposed model could show the value of considering saline formations in water and soil salinities of the watershed with adjustable gis maps also the proposed model results could indicate that the salinity of different media of the watershed is a combination of various complex time dependent factors at the subbasin scale the proposed swat s model can be a helpful modeling tool to simulate the fate and transport of other non point source pollution which have the same nature at a watershed scale however the proposed model s capabilities can be improved to include any specific salinity sources and processes such as saline lakes in desired watersheds furthermore it is recommended that other lumped semi distributed and distributed models similarly regard saline formations in their modeling framework declaration of competing interest the author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104906 
25916,water networks are spatially dispersed easily accessible and vulnerable to contaminating intrusions if contamination is detected too late then damage to the population may be irreversible for the hard task of optimal sensor placement this work presents a multi objective approach that is combined with post processing methods for a pareto front analysis the contamination is represented by the chemical reactions of the pesticide parathion in water quality simulations a multi objective approach is used that incorporates four contamination probability functions the pareto front is analyzed with a clustering approach and a coverage matrix is used to evaluate the centers of each cluster an automatic selection solution method based on distances to the most suitable solution is also explored keywords water supply sensor placement genetic algorithms k means algorithm 1 introduction a water distribution system wds provides drinking water through an infrastructure connecting consumers access points in the water networks e g hydrants consumer connections tanks reservoir and leak points are possible places for intentional and accidental contamination intrusion he et al 2018 oliker et al 2016 hart and murray 2010 an advanced monitoring system is one of the water security metrics currently proposed zolghadr asli et al 2017 continuous monitoring of water quality parameters plays a key role for implementing a wds early warning system quality sensors make monitoring possible and generate data for implementing anomaly detection systems eliades et al 2014 hart and murray 2010 under quickly identified abnormal scenarios security protocols can be implemented that avoid contamination and save lives an online monitoring system with real time or near real time data transmission and processing maximizes the benefits of early warning systems since the first hours after contamination are crucial for mitigating the impacts zulkifli et al 2018 optimal sensor placement is the first step for an efficient early warning system the accuracy of the anomaly detection in water quality is closely linked to the number and position of sensors more sensors can lead to fast detection and precise localization of the contamination ohar et al 2015 schwartz et al 2014 despite improving network security the high cost of sensors and the limited budget of water utilities make the design of the sensor network a strategic decision several criteria should be considered for designing a sensor network such as the capability to detect and identify the intrusion point and the time required to detect an intrusion palleti et al 2016 the challenge of optimal sensor placement has been explored in the literature since 1991 lee et al 1991 the problem became even more significant following the september 11 2001 attack in the united states allmann t p and carlson 2005 and other attacks around the world a scientific challenge launched in 2006 the battle of water sensors networks bwsn reflects the importance of the sensor placement problem the paper published by the bwsn ostfeld2008 compares the performance of 15 approaches regarding optimal sensor placement for intrusion detection participants used different techniques and the solutions are evaluated based on four objectives 1 minimize the expected detection time 2 minimize the expected affected population before detection 3 minimize the expected consumption of contaminated water before detection and 4 maximize the probability of detection critical reviews presented below offer different methodologies for sensor placement in water networks citing over 100 scientific articles hart and murray 2010 rathi et al 2015 most studies use the multi objective approach for sensor placement wéber and hês 2020 quiñones grueiro et al 2019 rathi et al 2012 sensors placement is generally addressed by multi objective formulation usually optimizing objective functions such as detection likelihood detection time and hit population giudicianni et al 2020 genetic algorithms ga are one of the preferred options for solving wds sensor placement multi objective approaches rathi and gupta 2014 tanyimboh and czajkowska 2018 li et al 2019 ehsani and afshar 2012 apply a non dominated sorted genetic algorithm nsga to minimize the detection time and maximize the detection probability naserizade nikoo and montaseri 2018 use the elitist multi objective sorting genetic algorithm ii nsga ii approach minimizing affected population and detection time and including the probability of undetected events and cost rathi and gupta 2017 formulate the optimization problem to maximize the demand coverage and detection probability with time constrained for the early detection of a greater number of contamination events the nsga ii approach deb et al 2002 applied to the sensor placement problem finds a compromise solution among the objective functions even if the pareto front solutions are all optimal for many engineering problems only one of them can be implemented the choice of one solution is a hard task for the decision maker since the solution set can be large cheikh et al 2010 post processing methods are explored in the literature to help decision makers select a single solution from the pareto front the experience of any decision maker involved may be relevant in selecting the pareto solution multi criteria decision making methods may be applied to get a consensus about the various evaluations of the same problem andersson 2002 carpitella et al 2018 another important point is that optimal sensor placement for contamination detection usually considers the probability of contamination to be equal for all nodes of the network in a recent study he et al 2018 suggested treating the sampling design in a realistic manner the authors considered the different properties of each wds zone such as population density varying the importance of users schools hospitals and government departments the authors analyze four contamination probability functions based on spatial distribution of nodal demands length of the pipe immediately connected to the nodes pipe flows and user properties in addition the authors verify the influence of the probabilities in the set of pareto solutions with the classical approach in which the probabilistic distribution is the same for all nodes the probability of chemical contamination in a wds can be different for each node and this work presents four probability functions pf on the placement process in a real water distribution network wdn the probabilities are based on nodal demands p f 1 distances between common nodes and special nodes p f 2 importance of the pipes in the water network p f 3 and any special user position p f 4 the nsga ii algorithm is applied considering three objective functions for sensor placement minimizing the detection time maximizing the probability of detection and minimizing the number of sensors pareto fronts are post processed by clustering analysis thus reducing the set of solutions for the decision maker 2 methodology the proposed methodology is divided into two levels the optimization process for sensor placement and the post processing of the pareto front initially several contamination scenarios are generated by water quality simulation with the intrusion of the parathion pa contaminant pesticides such as the pa are organophosphates with the potential to be used as contaminants since they are highly toxic and easily accessible in addition pa reacts with free chlorine present in distributed water generating an even more toxic by product ohar et al 2015 epanet msx epanet multi species extension shang et al 2008 and epanet 2 0 rossman et al 2000 software are jointly applied to simulate the injection scenarios of pa in a wdn to solve the optimization problem nsga ii is proposed the final pareto front is post processed using the k means clustering algorithm arthur and vassilvitskii 2006 2 1 optimal sensor placement 2 1 1 contamination scenario simulation epanet msx is a water quality model that simulates several chemical species interacting and reacting with each other and the mass transport within water network pipes for this the reaction stoichiometry on the chemical species to be simulated and kinetics must be included in the software epanet msx input file shang et al 2008 this work generates contamination scenarios based on ohar et al 2015 and the schwartz et al 2014 multi species system the authors simulate the pa reacting with the water components especially free chlorine table 1 describes the stoichiometry equations of pa the contaminant used in the simulation in this study the brazilian characteristics of drinking water are considered such as temperatures of 298 k total dissolved solids tds of 1000 m g l ph of 7 3 and alkalinity of 260 m g l brasil 2017 2 1 2 contamination probability the detection of all contamination events is a hard if not impossible task the budgets of water utilities are limited and this means that few sensors are placed in water networks a strategy for placing quality sensors in water distribution networks is to maximize coverage of the water network system and guarantee a quick detection of anomalies in the case of intentional contaminant intrusion in water systems the probability variability of nodal contamination can help build an efficient monitoring system based on he et al 2018 where the authors assume different probabilities of various contaminations this work studies four approaches to evaluate the effects of probability contamination in water systems a p f 1 spatial variation of nodal demands nodal demands are usually modelled on occupation density this means that the higher the nodal demand the greater the population around this node and the greater is the importance of the node in terms of water consumption therefore he et al 2018 shows the nodal contamination probability as a fraction of the nodal demand regarding the total wdn demands equation 1 is a simple and direct relation between nodal demand and the total demand of the water network 1 p 1 i t δ t a δ t q i t i 1 n t δ t a δ t q i t where p 1 i is the nodal contamination probability i 1 2 n n is the total number of nodes within the wdn q i t is the water demand of node i at time t δ t 2 δ t a δ t a δ t is the total duration simulation time period b p f 2 distances from common nodes to those of special users regions with especially vulnerable users such as schools hospitals and government departments are more susceptible to attacks due to the impact of intentional attacks on these points special users can be located in any region of the water network and are usually modelled by creating a node with a specific demand and time pattern in general special nodes are places with good vigilance and restricted access thus the smaller the distance from a common node to the special nodes the greater the contamination probability the euclidean distance d i j is used to calculate the distance between special nodes j and common nodes i after calculating the distances a normalization process is conducted leading to a probabilistic distribution in which the smaller distances have higher contamination probability these values are stored in matrix d n z equation 2 2 d d 1 1 d 1 j d i 1 d i j where z is the number of special nodes and n is the number of common nodes in the wdn the shortest distance between special nodes for all of the common nodes of the network is used to build the probability vector z 1 c p f 3 pipe relevance in the water network water systems have pipes with differing water transportation capacities graph theory tools can help identify the contribution of each pipe in the water network the topology of the wds can be read as a graph where the vertices represent the nodes and the edges are the links campbell et al 2015 in this work the shortest path sp concept is implemented through breadth first search bfs moore 1959 the sp between nodes in a given network is a widely explored concept within graph domain and social network theory meirelles et al 2018 campbell et al 2015 by applying the concept of digraph a graph with defined orientation it is possible to define the frequency of all paths in a graph campbell et al 2015 links information such as flow direction can be incorporated into the edges of the graph and define the orientation when performed in the maximum demand scenarios the flow direction in each link represents the most critical scenario for hydraulic parameters such as hydraulic head loss meirelles et al 2018 and also for contamination scenarios where the chemical intrusion will be quickly carried by the high velocities within the pipes given a start node the bfs explores all the vertices of the graph to find the subset of achievable nodes from the origin node meirelles et al 2018 therefore the distance in terms of number of vertices from the source to all sets of nodes is computed subsequently the shortest path value spv for each pair of nodes is stored in a square array in which the columns represent the starting nodes s n and the rows and the end nodes e n the matrix is built according to the set of rules campbell et al 2015 if s n e n 0 if s n cannot reach e n for any other case number of node in the sp where s n represents the starting node and e n represents the ending node with the s p matrix it is possible to classify the nodes according to importance within the digraph for a specific node the higher the number of nodes it can reach then the more important is the node the sum of each row of the s p matrix is called the accumulated path value aspv a normalization of aspv is made to transform the aspv into probability d p f 4 special users position in the case of intentional contamination common nodes linked to special nodes are more likely to be attacked the safety of the water in these nodes needs special attention the p f 4 seeks to ensure that special nodes have a high probability of contamination independently of their real demands and other properties p f 4 differs from p f 2 by ensuring that special users have a higher likelihood of contamination when compared to other nodes that is the probability of contamination in the special nodes is independent of demand therefore based on the contamination probability function for special users created by he et al 2018 the p 4 is defined by combining the distances between common nodes and special users and the respective demands represented in equation 3 for this a probability τ is assigned to the group of special nodes in addition the likelihood of contamination of common nodes is attributed by multiplying nodal demand by the minimum distance between them and the special nodes p f 4 is written as 3 p 4 t τ i ε ψ t δ t a δ t q i t n i 1 t δ t a δ t q i t 1 d where ψ is the set of special users a fixed probability is attributed to ψ ensuring high probability independent of nodal demand the other nodes of the wdn receive probability according to their demands equation 1 multiplied by the minimum distance between the special nodes equation 2 2 1 3 mathematical description of the optimal sensor placement for optimal quality sensor placement this work considers two objective functions from bwsn minimizing the detection time and maximizing the probability of detection a third objective function is included minimizing the number allocated sensors i detection time f 1 the detection time for a node i contaminated at time t γ i t is defined as the time elapsed since starting the contamination at node i until the first identification of the contaminant concentration by any of the sensors in the network he et al 2018 ostfeld et al 2008 therefore the expected time f 1 can be expressed as 4 f 1 m i n i 1 n t δ t a δ t p i γ i t i 1 n t δ t a δ t α 5 α 1 γ i t 0 0 γ i t is not available where p i is the contamination probability of node i no tempo t δ t 2 δ t a δ t a δ t is total simulation duration the indicator α eq 5 is equal to one if the contamination intrusion at the node i at time t is detected by at least one of the network sensors if α 0 the solution discarded ii probability of detection f 2 f 2 quantifies the contamination scenarios detected by any sensor in the wdn equation 6 6 f 2 m a x i 1 n t δ t a δ t α 1 a p i where p i is the contamination probability of node i iii number of sensors f 3 to find the optimal sensors number function f 3 is minimized this is an important objective function because it is directly linked to the cost of the sensor s network a good optimization process will result in the best position of n s sensors the minimization function can be represented by the equation 7 f 3 m i n n s where n s is the number of sensors placed in the network 2 1 4 optimization algorithm and pareto front post processing nsga ii deb et al 2002 is meta heuristic algorithm applied to multi objective problems the algorithm enables generating a set of solutions with the best trade off among the objective functions the pareto front chiandussi et al 2012 since the pareto front can have hundreds or even thousands of solutions the selection of a single solution to be implemented is not a simple task for this task a post processing methodology is proposed among the methods to evaluate the pareto front clustering methods offer decision makers only the representative points of solutions k means algorithms group the pareto front in k clusters according to similarities initially the centers of the groups are randomly placed thus the data is clustered according to the euclidean distance between a point and the center a datum i is classified as belonging to a group j if the smallest distance between the datum and the clusters centers is d i j an iterative process then starts and recalculates the position of the centers as the average position of the points belonging to the cluster the process repeats until there are no changes for center positions cheikh et al 2010 the silhouette index is used to determine the ideal number of clusters the silhouette index for each datum is a measure of the similarity of that datum between the data in a cluster when compared to data in other clusters kaufman and rousseeuw 2009 the silhouette index s k for the i t h datum is defined as 8 s k b i a i m a x a i b i where a i is the average distance from the i t h datum to the other data in the same cluster as i and b i is the minimum average distance from the i t h datum to data in a different cluster minimized over the clusters demand coverage d c is used to assess the performance of each pf d c presented by lee and deininger 1992 considers the amount of demand covered by the monitoring network with this it is possible to determine the relevance of a node in terms of percentage of coverage the water d c is calculated with the equation presented by liu et al 2012 9 d c m a x φ a b t 1 a δ t i 1 n q i t y i t a b 1 n where y i t 0 1 and denotes whether the demand at node i is covered q i t is demand at node i at time step k n is the number of demand nodes in the network and k is number of demand patterns φ is a decision variable which is the set of locations to place monitoring stations t 1 a δ t i 1 n q i t y i t is the accumulation of its demand coverage for any node 3 case study the methodology is applied to the real network jiayou jyn he et al 2018 fig 1 which is composed of 300 nodes 451 links and 2 reservoirs chlorine injection is modelled in both reservoirs with a concentration of 1 5 m g l the injection of pa is made after 24 h of normal simulation with a duration of 12 h at a concentration of 12 4 m g l schwartz et al 2014 this concentration of pa corresponds to a saturation concentration that produces an invisible detection of pa in water by considering all possible nodes for contamination the network is submitted to 300 contamination events chlorine behavior during normal simulation without contaminant intrusion oscillates following demand behavior when reacting with the pa contaminant a sudden decrease in chlorine occurs fig 2 because of this behavior chlorine is a contamination indicator parameter as already considered by ohar et al 2015 the results of the optimization are presented in terms of the pareto front by comparing the probabilistic approach presented in this work with the classical approach where the nodal intrusion probability is equal for all nodes p f 0 3 1 spatial distribution of probability functions fig 3 presents the spatial distribution of nodes considering the pfs applied to the jyn network the size of nodes are proportional to the pf value for the node as expected differing distributions of probability are observed according to the pfs in p f 3 and p f 4 this is even more remarkable because the functions give much more importance to some nodes than others p f 3 gives higher priority to nodes connected to pipes with greater transportation capacity and p f 4 gives priority to special nodes 3 2 pareto solutions reduction based on k means method the final solutions offered for wds managers are based on the following steps 1 obtain pareto solutions set by the nsga ii deb et al 2002 2 apply the k means algorithm to form clusters in the solutions contained in the pareto set 3 determine the optimal number of clusters k based on the silhouette index and 4 select a representative solution for each cluster in this work the nsga ii is performed with the population size 1500 and the crossover point is equal to 2 for stopping criteria the algorithm uses the number of generations without improvement of the objective function equal to 100 and the maximum number of generations equal to 200 multiplied by the number of variables the k means algorithm applied to selected points from the pareto front is based on internal objective functions and external demand coverage values by clustering the pareto front it is possible to offer the decision maker a smaller number of representative solutions and so make the engineering implementation process easier the largest value of the silhouette index s k is five groups for all pfs figs 4 8 show the pareto front clustered into five groups and the solution closest to the center for each group black dots the figures also show the dc for each number of sensors placed in the network dc increases as more sensors are placed p f 3 stands out with the lowest coverage rate per maximum number of sensors while other pfs reach about 90 of coverage p f 3 reaches only 63 table 2 presents the five representative solutions for each pf the dc values shown in table 2 are relatively low even with a large number of sensors placed dc is related to the number of nodes that receive water from the sensor node that is the number of nodes downstream from the sensors f 2 refers to contamination scenarios detected by any sensor meaning the percentage of nodes upstream of a sensor another point to be highlighted in table 2 is the detection time f 1 it is expected that the larger number of sensors the shorter the detection time however the difference is between options with a large number of sensors or with few sensors is not significant it is observed that cluster 1 for p f 0 has a detection time equal to 1616s for 78 sensors however in cluster 2 only 4 sensors are placed and this produces a detection time of 1693s we analyze the objective functions f 1 f 2 and dc and make a comparison with the number of sensors f 3 it is worth highlighting that f 1 has the lowest value for cluster 3 in p f 0 the highest value is in cluster 1 of p f 3 however the difference in terms of number of sensors is large 140 sensors and 6 sensors respectively when analyzing f 2 the lowest value is found in cluster 2 of p f 1 and the highest value is found in cluster 3 of p f 4 in this case the number of sensors is also a determining factor for decision makers even so a high number of placed sensors is not directly linked to better f 2 rates this behavior can be observed mainly in clusters 2 and 4 of p f 1 cluster 4 has fewer sensors placed but f 2 is larger than cluster 2 cluster 3 of p f 4 also appears with the highest rate of dc 84 70 and cluster 4 of p f 1 appears with the lowest value of about 2 11 4 conclusions this work proposes a methodology for optimal placement of sensors incorporating different contamination probabilities for this several contamination scenarios are generated with the injection of the parathion contaminant a new and automatic method for pareto front post processing is presented the k means method clustered the pareto front into five groups according to the parameters f 1 f 2 f 3 and dc where an ideal representative solution for each group is presented to the decision makers the post pareto analysis approach has proven to be efficient and collaborative for decision making but the water supply decision maker must decide which of the ideal solutions is best suited to his or her preferences in addition all pfs affected the search for optimal sensor allocation this makes it possible to determine which nodes should be prioritized according to the preferences of the wds manager future works aim to improve wds safety by optimally allocating quality sensors by incorporating different contamination probabilities in the methodology and different optimization and clustering methods data availability statement the data and input file model are available contacting directly the authors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge the coordenação de aperfeiçoamento de pessoal de nível superior brasil capes finance code 001 and instituto de pesquisas tecnológicas do estado de são paulo ipt to support this research appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104896 
25916,water networks are spatially dispersed easily accessible and vulnerable to contaminating intrusions if contamination is detected too late then damage to the population may be irreversible for the hard task of optimal sensor placement this work presents a multi objective approach that is combined with post processing methods for a pareto front analysis the contamination is represented by the chemical reactions of the pesticide parathion in water quality simulations a multi objective approach is used that incorporates four contamination probability functions the pareto front is analyzed with a clustering approach and a coverage matrix is used to evaluate the centers of each cluster an automatic selection solution method based on distances to the most suitable solution is also explored keywords water supply sensor placement genetic algorithms k means algorithm 1 introduction a water distribution system wds provides drinking water through an infrastructure connecting consumers access points in the water networks e g hydrants consumer connections tanks reservoir and leak points are possible places for intentional and accidental contamination intrusion he et al 2018 oliker et al 2016 hart and murray 2010 an advanced monitoring system is one of the water security metrics currently proposed zolghadr asli et al 2017 continuous monitoring of water quality parameters plays a key role for implementing a wds early warning system quality sensors make monitoring possible and generate data for implementing anomaly detection systems eliades et al 2014 hart and murray 2010 under quickly identified abnormal scenarios security protocols can be implemented that avoid contamination and save lives an online monitoring system with real time or near real time data transmission and processing maximizes the benefits of early warning systems since the first hours after contamination are crucial for mitigating the impacts zulkifli et al 2018 optimal sensor placement is the first step for an efficient early warning system the accuracy of the anomaly detection in water quality is closely linked to the number and position of sensors more sensors can lead to fast detection and precise localization of the contamination ohar et al 2015 schwartz et al 2014 despite improving network security the high cost of sensors and the limited budget of water utilities make the design of the sensor network a strategic decision several criteria should be considered for designing a sensor network such as the capability to detect and identify the intrusion point and the time required to detect an intrusion palleti et al 2016 the challenge of optimal sensor placement has been explored in the literature since 1991 lee et al 1991 the problem became even more significant following the september 11 2001 attack in the united states allmann t p and carlson 2005 and other attacks around the world a scientific challenge launched in 2006 the battle of water sensors networks bwsn reflects the importance of the sensor placement problem the paper published by the bwsn ostfeld2008 compares the performance of 15 approaches regarding optimal sensor placement for intrusion detection participants used different techniques and the solutions are evaluated based on four objectives 1 minimize the expected detection time 2 minimize the expected affected population before detection 3 minimize the expected consumption of contaminated water before detection and 4 maximize the probability of detection critical reviews presented below offer different methodologies for sensor placement in water networks citing over 100 scientific articles hart and murray 2010 rathi et al 2015 most studies use the multi objective approach for sensor placement wéber and hês 2020 quiñones grueiro et al 2019 rathi et al 2012 sensors placement is generally addressed by multi objective formulation usually optimizing objective functions such as detection likelihood detection time and hit population giudicianni et al 2020 genetic algorithms ga are one of the preferred options for solving wds sensor placement multi objective approaches rathi and gupta 2014 tanyimboh and czajkowska 2018 li et al 2019 ehsani and afshar 2012 apply a non dominated sorted genetic algorithm nsga to minimize the detection time and maximize the detection probability naserizade nikoo and montaseri 2018 use the elitist multi objective sorting genetic algorithm ii nsga ii approach minimizing affected population and detection time and including the probability of undetected events and cost rathi and gupta 2017 formulate the optimization problem to maximize the demand coverage and detection probability with time constrained for the early detection of a greater number of contamination events the nsga ii approach deb et al 2002 applied to the sensor placement problem finds a compromise solution among the objective functions even if the pareto front solutions are all optimal for many engineering problems only one of them can be implemented the choice of one solution is a hard task for the decision maker since the solution set can be large cheikh et al 2010 post processing methods are explored in the literature to help decision makers select a single solution from the pareto front the experience of any decision maker involved may be relevant in selecting the pareto solution multi criteria decision making methods may be applied to get a consensus about the various evaluations of the same problem andersson 2002 carpitella et al 2018 another important point is that optimal sensor placement for contamination detection usually considers the probability of contamination to be equal for all nodes of the network in a recent study he et al 2018 suggested treating the sampling design in a realistic manner the authors considered the different properties of each wds zone such as population density varying the importance of users schools hospitals and government departments the authors analyze four contamination probability functions based on spatial distribution of nodal demands length of the pipe immediately connected to the nodes pipe flows and user properties in addition the authors verify the influence of the probabilities in the set of pareto solutions with the classical approach in which the probabilistic distribution is the same for all nodes the probability of chemical contamination in a wds can be different for each node and this work presents four probability functions pf on the placement process in a real water distribution network wdn the probabilities are based on nodal demands p f 1 distances between common nodes and special nodes p f 2 importance of the pipes in the water network p f 3 and any special user position p f 4 the nsga ii algorithm is applied considering three objective functions for sensor placement minimizing the detection time maximizing the probability of detection and minimizing the number of sensors pareto fronts are post processed by clustering analysis thus reducing the set of solutions for the decision maker 2 methodology the proposed methodology is divided into two levels the optimization process for sensor placement and the post processing of the pareto front initially several contamination scenarios are generated by water quality simulation with the intrusion of the parathion pa contaminant pesticides such as the pa are organophosphates with the potential to be used as contaminants since they are highly toxic and easily accessible in addition pa reacts with free chlorine present in distributed water generating an even more toxic by product ohar et al 2015 epanet msx epanet multi species extension shang et al 2008 and epanet 2 0 rossman et al 2000 software are jointly applied to simulate the injection scenarios of pa in a wdn to solve the optimization problem nsga ii is proposed the final pareto front is post processed using the k means clustering algorithm arthur and vassilvitskii 2006 2 1 optimal sensor placement 2 1 1 contamination scenario simulation epanet msx is a water quality model that simulates several chemical species interacting and reacting with each other and the mass transport within water network pipes for this the reaction stoichiometry on the chemical species to be simulated and kinetics must be included in the software epanet msx input file shang et al 2008 this work generates contamination scenarios based on ohar et al 2015 and the schwartz et al 2014 multi species system the authors simulate the pa reacting with the water components especially free chlorine table 1 describes the stoichiometry equations of pa the contaminant used in the simulation in this study the brazilian characteristics of drinking water are considered such as temperatures of 298 k total dissolved solids tds of 1000 m g l ph of 7 3 and alkalinity of 260 m g l brasil 2017 2 1 2 contamination probability the detection of all contamination events is a hard if not impossible task the budgets of water utilities are limited and this means that few sensors are placed in water networks a strategy for placing quality sensors in water distribution networks is to maximize coverage of the water network system and guarantee a quick detection of anomalies in the case of intentional contaminant intrusion in water systems the probability variability of nodal contamination can help build an efficient monitoring system based on he et al 2018 where the authors assume different probabilities of various contaminations this work studies four approaches to evaluate the effects of probability contamination in water systems a p f 1 spatial variation of nodal demands nodal demands are usually modelled on occupation density this means that the higher the nodal demand the greater the population around this node and the greater is the importance of the node in terms of water consumption therefore he et al 2018 shows the nodal contamination probability as a fraction of the nodal demand regarding the total wdn demands equation 1 is a simple and direct relation between nodal demand and the total demand of the water network 1 p 1 i t δ t a δ t q i t i 1 n t δ t a δ t q i t where p 1 i is the nodal contamination probability i 1 2 n n is the total number of nodes within the wdn q i t is the water demand of node i at time t δ t 2 δ t a δ t a δ t is the total duration simulation time period b p f 2 distances from common nodes to those of special users regions with especially vulnerable users such as schools hospitals and government departments are more susceptible to attacks due to the impact of intentional attacks on these points special users can be located in any region of the water network and are usually modelled by creating a node with a specific demand and time pattern in general special nodes are places with good vigilance and restricted access thus the smaller the distance from a common node to the special nodes the greater the contamination probability the euclidean distance d i j is used to calculate the distance between special nodes j and common nodes i after calculating the distances a normalization process is conducted leading to a probabilistic distribution in which the smaller distances have higher contamination probability these values are stored in matrix d n z equation 2 2 d d 1 1 d 1 j d i 1 d i j where z is the number of special nodes and n is the number of common nodes in the wdn the shortest distance between special nodes for all of the common nodes of the network is used to build the probability vector z 1 c p f 3 pipe relevance in the water network water systems have pipes with differing water transportation capacities graph theory tools can help identify the contribution of each pipe in the water network the topology of the wds can be read as a graph where the vertices represent the nodes and the edges are the links campbell et al 2015 in this work the shortest path sp concept is implemented through breadth first search bfs moore 1959 the sp between nodes in a given network is a widely explored concept within graph domain and social network theory meirelles et al 2018 campbell et al 2015 by applying the concept of digraph a graph with defined orientation it is possible to define the frequency of all paths in a graph campbell et al 2015 links information such as flow direction can be incorporated into the edges of the graph and define the orientation when performed in the maximum demand scenarios the flow direction in each link represents the most critical scenario for hydraulic parameters such as hydraulic head loss meirelles et al 2018 and also for contamination scenarios where the chemical intrusion will be quickly carried by the high velocities within the pipes given a start node the bfs explores all the vertices of the graph to find the subset of achievable nodes from the origin node meirelles et al 2018 therefore the distance in terms of number of vertices from the source to all sets of nodes is computed subsequently the shortest path value spv for each pair of nodes is stored in a square array in which the columns represent the starting nodes s n and the rows and the end nodes e n the matrix is built according to the set of rules campbell et al 2015 if s n e n 0 if s n cannot reach e n for any other case number of node in the sp where s n represents the starting node and e n represents the ending node with the s p matrix it is possible to classify the nodes according to importance within the digraph for a specific node the higher the number of nodes it can reach then the more important is the node the sum of each row of the s p matrix is called the accumulated path value aspv a normalization of aspv is made to transform the aspv into probability d p f 4 special users position in the case of intentional contamination common nodes linked to special nodes are more likely to be attacked the safety of the water in these nodes needs special attention the p f 4 seeks to ensure that special nodes have a high probability of contamination independently of their real demands and other properties p f 4 differs from p f 2 by ensuring that special users have a higher likelihood of contamination when compared to other nodes that is the probability of contamination in the special nodes is independent of demand therefore based on the contamination probability function for special users created by he et al 2018 the p 4 is defined by combining the distances between common nodes and special users and the respective demands represented in equation 3 for this a probability τ is assigned to the group of special nodes in addition the likelihood of contamination of common nodes is attributed by multiplying nodal demand by the minimum distance between them and the special nodes p f 4 is written as 3 p 4 t τ i ε ψ t δ t a δ t q i t n i 1 t δ t a δ t q i t 1 d where ψ is the set of special users a fixed probability is attributed to ψ ensuring high probability independent of nodal demand the other nodes of the wdn receive probability according to their demands equation 1 multiplied by the minimum distance between the special nodes equation 2 2 1 3 mathematical description of the optimal sensor placement for optimal quality sensor placement this work considers two objective functions from bwsn minimizing the detection time and maximizing the probability of detection a third objective function is included minimizing the number allocated sensors i detection time f 1 the detection time for a node i contaminated at time t γ i t is defined as the time elapsed since starting the contamination at node i until the first identification of the contaminant concentration by any of the sensors in the network he et al 2018 ostfeld et al 2008 therefore the expected time f 1 can be expressed as 4 f 1 m i n i 1 n t δ t a δ t p i γ i t i 1 n t δ t a δ t α 5 α 1 γ i t 0 0 γ i t is not available where p i is the contamination probability of node i no tempo t δ t 2 δ t a δ t a δ t is total simulation duration the indicator α eq 5 is equal to one if the contamination intrusion at the node i at time t is detected by at least one of the network sensors if α 0 the solution discarded ii probability of detection f 2 f 2 quantifies the contamination scenarios detected by any sensor in the wdn equation 6 6 f 2 m a x i 1 n t δ t a δ t α 1 a p i where p i is the contamination probability of node i iii number of sensors f 3 to find the optimal sensors number function f 3 is minimized this is an important objective function because it is directly linked to the cost of the sensor s network a good optimization process will result in the best position of n s sensors the minimization function can be represented by the equation 7 f 3 m i n n s where n s is the number of sensors placed in the network 2 1 4 optimization algorithm and pareto front post processing nsga ii deb et al 2002 is meta heuristic algorithm applied to multi objective problems the algorithm enables generating a set of solutions with the best trade off among the objective functions the pareto front chiandussi et al 2012 since the pareto front can have hundreds or even thousands of solutions the selection of a single solution to be implemented is not a simple task for this task a post processing methodology is proposed among the methods to evaluate the pareto front clustering methods offer decision makers only the representative points of solutions k means algorithms group the pareto front in k clusters according to similarities initially the centers of the groups are randomly placed thus the data is clustered according to the euclidean distance between a point and the center a datum i is classified as belonging to a group j if the smallest distance between the datum and the clusters centers is d i j an iterative process then starts and recalculates the position of the centers as the average position of the points belonging to the cluster the process repeats until there are no changes for center positions cheikh et al 2010 the silhouette index is used to determine the ideal number of clusters the silhouette index for each datum is a measure of the similarity of that datum between the data in a cluster when compared to data in other clusters kaufman and rousseeuw 2009 the silhouette index s k for the i t h datum is defined as 8 s k b i a i m a x a i b i where a i is the average distance from the i t h datum to the other data in the same cluster as i and b i is the minimum average distance from the i t h datum to data in a different cluster minimized over the clusters demand coverage d c is used to assess the performance of each pf d c presented by lee and deininger 1992 considers the amount of demand covered by the monitoring network with this it is possible to determine the relevance of a node in terms of percentage of coverage the water d c is calculated with the equation presented by liu et al 2012 9 d c m a x φ a b t 1 a δ t i 1 n q i t y i t a b 1 n where y i t 0 1 and denotes whether the demand at node i is covered q i t is demand at node i at time step k n is the number of demand nodes in the network and k is number of demand patterns φ is a decision variable which is the set of locations to place monitoring stations t 1 a δ t i 1 n q i t y i t is the accumulation of its demand coverage for any node 3 case study the methodology is applied to the real network jiayou jyn he et al 2018 fig 1 which is composed of 300 nodes 451 links and 2 reservoirs chlorine injection is modelled in both reservoirs with a concentration of 1 5 m g l the injection of pa is made after 24 h of normal simulation with a duration of 12 h at a concentration of 12 4 m g l schwartz et al 2014 this concentration of pa corresponds to a saturation concentration that produces an invisible detection of pa in water by considering all possible nodes for contamination the network is submitted to 300 contamination events chlorine behavior during normal simulation without contaminant intrusion oscillates following demand behavior when reacting with the pa contaminant a sudden decrease in chlorine occurs fig 2 because of this behavior chlorine is a contamination indicator parameter as already considered by ohar et al 2015 the results of the optimization are presented in terms of the pareto front by comparing the probabilistic approach presented in this work with the classical approach where the nodal intrusion probability is equal for all nodes p f 0 3 1 spatial distribution of probability functions fig 3 presents the spatial distribution of nodes considering the pfs applied to the jyn network the size of nodes are proportional to the pf value for the node as expected differing distributions of probability are observed according to the pfs in p f 3 and p f 4 this is even more remarkable because the functions give much more importance to some nodes than others p f 3 gives higher priority to nodes connected to pipes with greater transportation capacity and p f 4 gives priority to special nodes 3 2 pareto solutions reduction based on k means method the final solutions offered for wds managers are based on the following steps 1 obtain pareto solutions set by the nsga ii deb et al 2002 2 apply the k means algorithm to form clusters in the solutions contained in the pareto set 3 determine the optimal number of clusters k based on the silhouette index and 4 select a representative solution for each cluster in this work the nsga ii is performed with the population size 1500 and the crossover point is equal to 2 for stopping criteria the algorithm uses the number of generations without improvement of the objective function equal to 100 and the maximum number of generations equal to 200 multiplied by the number of variables the k means algorithm applied to selected points from the pareto front is based on internal objective functions and external demand coverage values by clustering the pareto front it is possible to offer the decision maker a smaller number of representative solutions and so make the engineering implementation process easier the largest value of the silhouette index s k is five groups for all pfs figs 4 8 show the pareto front clustered into five groups and the solution closest to the center for each group black dots the figures also show the dc for each number of sensors placed in the network dc increases as more sensors are placed p f 3 stands out with the lowest coverage rate per maximum number of sensors while other pfs reach about 90 of coverage p f 3 reaches only 63 table 2 presents the five representative solutions for each pf the dc values shown in table 2 are relatively low even with a large number of sensors placed dc is related to the number of nodes that receive water from the sensor node that is the number of nodes downstream from the sensors f 2 refers to contamination scenarios detected by any sensor meaning the percentage of nodes upstream of a sensor another point to be highlighted in table 2 is the detection time f 1 it is expected that the larger number of sensors the shorter the detection time however the difference is between options with a large number of sensors or with few sensors is not significant it is observed that cluster 1 for p f 0 has a detection time equal to 1616s for 78 sensors however in cluster 2 only 4 sensors are placed and this produces a detection time of 1693s we analyze the objective functions f 1 f 2 and dc and make a comparison with the number of sensors f 3 it is worth highlighting that f 1 has the lowest value for cluster 3 in p f 0 the highest value is in cluster 1 of p f 3 however the difference in terms of number of sensors is large 140 sensors and 6 sensors respectively when analyzing f 2 the lowest value is found in cluster 2 of p f 1 and the highest value is found in cluster 3 of p f 4 in this case the number of sensors is also a determining factor for decision makers even so a high number of placed sensors is not directly linked to better f 2 rates this behavior can be observed mainly in clusters 2 and 4 of p f 1 cluster 4 has fewer sensors placed but f 2 is larger than cluster 2 cluster 3 of p f 4 also appears with the highest rate of dc 84 70 and cluster 4 of p f 1 appears with the lowest value of about 2 11 4 conclusions this work proposes a methodology for optimal placement of sensors incorporating different contamination probabilities for this several contamination scenarios are generated with the injection of the parathion contaminant a new and automatic method for pareto front post processing is presented the k means method clustered the pareto front into five groups according to the parameters f 1 f 2 f 3 and dc where an ideal representative solution for each group is presented to the decision makers the post pareto analysis approach has proven to be efficient and collaborative for decision making but the water supply decision maker must decide which of the ideal solutions is best suited to his or her preferences in addition all pfs affected the search for optimal sensor allocation this makes it possible to determine which nodes should be prioritized according to the preferences of the wds manager future works aim to improve wds safety by optimally allocating quality sensors by incorporating different contamination probabilities in the methodology and different optimization and clustering methods data availability statement the data and input file model are available contacting directly the authors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge the coordenação de aperfeiçoamento de pessoal de nível superior brasil capes finance code 001 and instituto de pesquisas tecnológicas do estado de são paulo ipt to support this research appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104896 
25917,environmental decisions are complex as they are multi dimensional highly interdisciplinary and not only involve multiple stakeholders with conflicting objectives but also many possible alternatives with uncertain consequences the difficulty lies in making trade offs between tough value trade offs on the one hand while appreciating uncertain impacts of alternatives on the other to support decisions tackling such problems a combination of multi criteria decision analysis mcda and environmental models is promising yet limited by the available mcda software here we present decisi o rama an open source python mcda library for single and sets portfolios of alternatives in the context of multi attribute value utility theory maut mavt its development was driven by four aspirations that are crucial for usability in the context of environmental decision making 1 interoperability 2 uncertainty awareness 3 computational efficiency and 4 integration with portfolio decisions the results indicate that these aspirations are met thus facilitating the adoption of mcda methods by environmental researchers and practitioners keywords maut mavt portfolio decision analysis multi criteria decision analysis python 1 introduction environmental decisions are often complex as there are usually many courses of action often conflicting and uncertain the reason for this complexity lies in how the utilisation of natural resources has sociopolitical environmental economic and even ethical implications at the time these decisions are often supported by uncertain and often incomparable data that need to be synthesized to support decision makers multi criteria decision analysis provides a framework to systematically evaluate decisions providing transparency traceability and reproducibility in the process as a result there has been an increasing interest in mcda methods for environmental decision making kiker et al 2005 huang et al 2011 linkov and moberg 2017 esmail and geneletti 2018 ortiz et al 2018 encompassing decision support langhans et al 2014 lahtinen et al 2017 haag et al 2019 kuemmerlen et al 2019 marttunen et al 2019 constructing ecological indicators convertino et al 2013 cinelli et al 2014 langhans et al 2014 zheng et al 2016 blattert et al 2017 and the support of planning activities joubert et al 2003 gómez delgado and tarantola 2006 hajkowicz 2007 calizaya et al 2010 huang et al 2011 sa nguanduan and nititvattananon 2011 lienert et al 2014 scholten et al 2015 scholten et al 2017 decision analysis aims to formalise common sense for decision problems which are too complex for informal use of common sense 30 p 806 decisions can become complex when there are multiple decision makers with different views of the issues that need addressing conflicting goals and manifold possible courses of action uncertainty about outcomes of different actions unfamiliarity especially in case of one off strategic decisions high interdisciplinarity and also different subjective valuation of outcomes and risks decision making is further complicated by the overall goals that are considered to compare alternative courses of action often being incommensurate and high dimensional in this context recurrent decisions with lower stakes and well defined objectives are generally easier than one off decisions with high stakes in complex decision contexts where the objectives need to be identified jointly with the decision makers in the first place keeney 1982 bureš et al 2019 environmental decisions are often of the latter type to account for these complex and inter related processes in a formal context decision analysis methods that combine problem structuring and multi criteria decision analysis mcda models are often used marttunen et al 2017 these methods support the decision making process by structuring the decision problem systematising the objectives and assessing their fulfilment and measureable attributes given a set of alternatives individual actions or portfolios of actions reichert et al 2013 this allows to compare among alternatives thus supporting the decision makers in exploring the decision space and model results voinov and bousquet 2010 voinov et al 2016 as a result decisions become more transparent traceable and reproducible mcda methods are generally classified with respect to how goal attainment is defined 1 overall value score or rank 2 goal reference or aspiration level and 3 outranking belton and stewart 2002 mendoza and martins 2006 summarising value methods indicate the preference of an alternative or portfolios of alternatives relative to others based on a numerical score these methods include the multi attribute utility and value approaches maut and mavt respectively eisenführ et al 2010 aspiration level methods assess the solutions based on the level of attainment of a set of goals as met or not an example of this is goal programming jones and tamiz 2016 outranking methods use a pair wise comparison of alternatives to identify a ranking of preferences examples are electre govindan and jepsen 2016 and promethee behzadian et al 2010 family methods for a more detailed overview of mcda and related methods see figueira et al 2006 mcda models come into play once the decision problem is structured these imply that 1 there is clarity about who takes part in the decision 2 what objectives and criteria are to be considered and 3 which alternatives are feasible mendoza and martins 2006 assuming a summarising value approach it is then necessary to understand and quantify the impacts of the alternatives on the attributes indicators to measure the outcomes of alternatives regarding the objectives the impact of different alternatives on the attributes are obtained from the assumed cause effect relationship between alternatives and anticipated consequences on the attributes this can be obtained based on conceptual or mathematical models that make these assumed relationships explicit or from estimates obtained directly from data or expert knowledge here we use the term assessment model to describe the mathematical models that are used to map the portfolios of alternatives to the expected outcomes on the attributes next mcda models require a preference model to appraise the portfolios of alternatives based on the perceived valuation of the anticipated outcomes and their trade offs this preference model should consider the different perspectives of the stakeholders trade offs among different competing objectives risk attitudes and ambiguity attitudes of the decision makers keeney 1982 reichert et al 2013 scholten et al 2015 in addressing a complex decision problem we agree with belton and stewart 2002 in that the purpose of an mcda model is to provide a focus for discussion not to prescribe the solution therefore mcda models are useful to learn about trade offs among alternatives and to formulate construct decision maker preferences unlike optimisation driven approaches the purpose of mcda models is not to determine a normatively best strategy to choose given a set of conflicting objectives as decision making involves judgment and valuation subjectivity cannot be avoided and the responsibility for the decision and its consequences remain with the decision maker s generally mcda methods are used to evaluate among different alternatives however in many cases the actions to be taken go beyond a single alternative e g which project to conduct seeking to identify a set of potential alternatives portfolio to implement e g a combination of several projects salo et al 2006 the number and combination of alternatives in a portfolio is subject to constraints such as the available budget or personnel gustafsson and salo 2005 liesiö et al 2008 also contingencies and interactions matter regarding which actions can be combined and whether the outcomes are independent of each other i e add up or not e g economies of scale upstream downstream interactions portfolio decision analysis pda integrates portfolio theory and multi attribute value models kleinmuntz 2007 salo et al 2011 to explore optimal deterministic portfolios or robust portfolios when considering the uncertainty in the decision maker preferences and alternative performance liesiö et al 2007 2008 applications of pda in mcda are reported in e g salo et al 2006 kleinmuntz 2007 mild and sahlo 2009 convertino et al 2013 lahtinen et al 2017 to navigate these decision spaces it is necessary to have tools that help in exploring the consequences of different individual or portfolio alternatives along with their uncertainties therefore it is necessary that modern decision analysis tools are able to accommodate uncertainty and sensitivity analysis french 2003 gómez delgado and tarantola 2006 durbach and stewart 2011 scholten et al 2015 esmail and geneletti 2018 conventionally uncertainty and sensitivity analysis require considerable computational resources for simulation and model evaluation over larger samples of model parameters and inputs as analytical solutions are either too simplified unavailable or too restrictive regarding the characteristics of each particular model for example one of us conducted extensive uncertainty and sensitivity analysis using an earlier version of the utility r package reichert et al 2013 for which it took days to compute the results for a real world decision problem on a high performance computing cluster scholten et al 2015 2017 that makes on the fly exploratory decision modelling with stakeholders unviable ideally decision tools should be computationally efficient to produce results in a short time and generic enough to be integrated together with other model exploration tools such as optimisation routines and scenario modelling to summarise the identified mcda modelling needs we need software or software libraries ideally open source that are 1 extensible and interoperable with complementary modelling and visualisation tools that 2 support uncertainty aware mcda including uncertainty in outcomes and preferences while also being 3 computationally efficient to be used on the fly with decision makers and 4 for portfolio decisions first extensibility and interoperability allows to accommodate problem specific process models integration into larger decision support frameworks and use complementary model analysis tools such as optimisation and visualisation second uncertainty aware decision models provide the decision makers with information about the magnitude and sources of the quantifiable uncertainty in the model third a computationally efficient model allows modelling and interaction with decision makers on the fly providing direct feedback on the consequences of the alternatives or portfolios and preference model supporting discussion fourth by including the possibility to add portfolio decisions it is possible to support realistic mcda to our surprise we were not able to find any suitable software among the plethora of tools available to support mcda that was able to do so let alone for our target method maut mavt for portfolio decisions see reviews in weistroffer and li 2016 mustajoki and marttunen 2017 international society on mcdm 2020 cinelli et al 2020 we found that most of these tools are either embedded into closed source decision support platforms built over particular data environments not without a reason or as commercial software making it difficult to extend and adapt these to specific needs neither in practice nor research there are a few notable exceptions including mcda bigaret et al 2017 r java and above mentioned utility reichert et al 2013 r the latter do not however provide an alternative valuation framework and lack the built in capability to conduct portfolio decision analyses lastly given the ubiquity of use of python for mathematical modelling and scientific computing it is surprising that no native mcda libraries are available for mavt maut and pda consequently we see value in developing a decision analysis tool that fulfils the identified needs and thus 1 is extensible and interoperable 2 uncertainty aware 3 computationally efficient and 4 able to integrate portfolio decisions in this paper we present the development and testing of decisi o rama an open source python library for mcda on single alternatives and portfolios in the context of mavt maut the name decisi o rama uses the suffix orama as in panorama derived from ancient greek hórama meaning view or spectacle the spelling o rama is popularly used to stress the latter meaning thus decision show or decision spect acle as we aim to provide a platform to expose the story behind a decision by making it transparent logic understandable and for everyone to see decisi o rama provides an interoperable efficient and flexible framework to support mcda using the summarising value type preference modelling for uncertain decision analysis problems this paper is organised as follows first a literature review of mavt maut and the current preference modelling tools is presented second the main features of decisi o rama are presented exposing the concepts behind its functionality third decisi o rama is used to solve two large synthetic problems to test its performance fourth a portfolio decision analysis problem as presented in lahtinen et al 2017 is solved where additional features of decisi o rama are showcased lastly conclusions recommendations and future directions are presented 2 multi criteria valuation models multi attribute decision analysis models including mavt and maut and referred to only as maut from this point on as common in the literature while noting that only the former warrants measurable value functions dyer and sarin 1979 next to many integrated assessment models aim to estimate the utility or attainment of an objective given a set of hierarchically structured attributes or criteria nardo et al 2008 singh et al 2012 scholten et al 2017 maut implies that a score can be constructed by aggregating different attributes which require the construction elicitation of a preference model keeney and howard 2002 cinelli et al 2014 among its more interesting features are its conceptual simplicity and its suitability to include risky choices dyer 2005 eisenführ et al 2010 preference models following multi attribute value theory are based on three main elements including an objectives hierarchy the assessment of the marginal utilities or values and trade offs among its different objectives the objectives hierarchy makes a top down assessment where the overall objective of the decision is broken down into intermediate objectives which can be further disaggregated either into other intermediate objectives or into lowest level objectives for each of which measurable or estimable attributes are defined it is important that only fundamental objectives are included in the hierarchy further reading regarding the formulation of objectives and structuring of objective hierarchies can be found in eisenführ et al 2010 once the objective hierarchy is defined the marginal valuation functions over the attributes and the trade offs among the attributes and objectives are to be elicited the valuation functions either include the risk preferences of the decision makers utility functions or not value functions regarding the attributes and objectives the trade offs are elicited by understanding what are the desirable trade offs often expressed as importance weights among the attributes and how these should be aggregated to represent these trade offs value or utility aggregation function this elicitation process yields uncertain parameters of the preference model as a consequence of the conceptual simplicity of the preference models preference instability e g lienert et al 2016 and limited interaction with the decision makers among other reasons scholten et al 2015 to summarise we can represent the components of portfolio decisions in the mavt maut as presented in fig 1 these components also represent the type of problems that decisi o rama addresses starting from the top it is possible to identify the preference model orange box the assessment model blue box and the alternative portfolios white box in the preference model g represents the overall objective o1 and o2 are intermediate objectives with utilities values of o1 constructed by aggregating the attributes at1 and at2 and o2 being quantified by at3 the overall objective is defined by aggregating o1 and o2 continuing the assessment model permits mapping the consequences of each portfolio p of alternatives to attributes outcomes and respective valuation in light of the given preference model finally the alternatives a are defined as either single actions or a collection of actions portfolios that are selected 1 or not 0 as candidates in the mcda model 2 1 constructing mavt and maut models once the objectives and their hierarchy are defined a maut preference model needs to be specified to valuate the alternatives based on the assessment model outputs with regard to the objectives 2 1 1 valuating objectives marginal valuation functions valuating an objective in the objectives hierarchy requires defining a value or a utility function the value of the objective represents the subjective desirability of its level of attainment under certainty the utility function combines the risk attitude of the decision maker and the subjective desirability of attaining some level on a particular attribute or objective with quantifiable probability risk dyer 2005 for mcda with maut utility preferences are modeled through expected utility theory eut von neumann and morgenstern 2007 in eut the risk preferences are reflected in von neumann and morgenstern type utility functions obtained from elicitation of preferences over lotteries according to which the decision makers are either risk averse risk seeking or risk neutral this utility is characterised by relating the value of attributes and utilities to monotonic mappings in which the utility is absolute assuming a rational decision von neumann and morgenstern 2007 to model the utility of an objective several approximations have been used in the context of eut common models of utility are the exponential and power model which correspond to a constant relative and absolute risk aversion respectively both approximations use a single parameter r or p to indicates the risk attitude in fig 2 the exponential and power utilities are presented for different risk attitudes where convexity r 0 and p 1 represents risk aversion and concavity r 0 and p 1 represents risk seeking 2 1 2 aggregating objectives overall valuation function once the utilities for attributes and objectives are assessed the utilities corresponding to the attribute levels are aggregated in the intermediate objectives and later aggregated towards the overall objective this aggregation requires the definition of the relative importance weights of each attribute or objective and the aggregation function the relative importance of each of the attributes or objectives is represented by its weight the aggregation function determines how to compute an overall score across the individual attributes representing the trade offs among attributes and objectives to explore the consequences of different aggregation functions and their impact we consider aggregation models that represent different preferences regarding compensation between objectives on the one hand compensatory models are defined by the existence of a trade off between the different attributes to be integrated in other words a loss in value due to changes in one attribute can be compensated by the gains in another on the other hand non compensatory aggregation models are used to indicate that is not possible to compensate the losses in a given attribute by improving another an example of compensatory attributes is revenue in contrast to non compensatory aggregation such as bio diversity indicators langhans et al 2014 the mathematical formulation of some of the most used aggregation functions can be found in reichert et al 2013 in fig 3 we present the impact of different aggregation functions and weights on the trade off between two attributes as in langhans et al 2014 these figures show the compensation rate for two attributes represented in the horizontal and vertical axes while the contour lines indicate iso aggregated values if the lines are vertical or horizontal then there is no compensation between attributes if there is a gradient it describes the degree of compensation between them here it is possible to see how aggregation functions like the additive linear model are fully compensatory the trade off is independent of the value of the attributes while either the maximum and minimum aggregation functions are non compensatory other functions such as the geometric and harmonic mean indicate a varying non linear compensation where the degree of compensation varies with the value on the respective attributes depending on the characteristics of the trade offs other conventional e g multiplicative utility keeney and howard 2002 mixed scholten et al 2015 or unconventional aggregation functions could be used reichert et al 2019 2 1 3 alternatives and portfolios alternatives and portfolios are the building blocks of any decision analysis system as the problems are framed to evaluate their preferability alternatives are potential actions that will be decided upon in the decision analysis system in this respect a portfolio can be seen as a collection of actions that are simultaneously carried out salo et al 2011 in many decision analysis problems the alternatives correspond to a set of potential actions that can be carried out to affect the system the characteristics of these actions are specific to each problem and are typically proposed by teams of experts once all the potential alternatives are set portfolios of alternatives can be constructed by combining individual alternatives and their corresponding actions where a portfolio of potential alternatives is to be selected it is necessary to note that the preferability of a given portfolio cannot be interpreted as the sum or weighted sum of its components the reason for this relates to the non linearity of the preference and assessment models this non linearity can be seen in the form of interactions among alternatives in the resulting attributes as estimated by the assessment model utilities in the intermediate objectives of the preference model and potential constraints e g competition for resources 3 decisi o rama features and reference decisi o rama is an open source python library that supports the development of mcda models for mavt and maut to compare alternatives or portfolios decisi o rama encompasses the development of the preference models as well as pre and post processing tools to support the mcda process relying on common python objects these features make the integration with attribute performance assessment models straight forward permitting transparent workflows in the implementation of a diverse range of mavt maut models in addition the tool is focused on simplicity and extensibility allowing the users to implement user defined components in the framework without necessarily modifying the source code one of the main features of decisi o rama is the recursive execution of the objectives hierarchy this allows to 1 execute large models with minimum impact on memory usage and 2 flexibly evaluating only parts of the objective hierarchy without model redefinition as a consequence it is suitable for the implementation of large decision models in conventional hardware configurations making on the fly model exploration in group settings feasible the construction of the objectives hierarchy is separated in two instances 1 the creation of the individual objectives nodes and 2 the creation of the hierarchy of objectives in decisi o rama nodes in the objectives hierarchy including the attributes intermediate objectives and overall objective are conceptualised as objectives thus simplifying model construction the creation of the nodes requires defining attributes weights value and aggregation functions in case the node corresponds to an attribute the aggregation function will not be considered in the same way if the node is an objective intermediate or overall its value will be calculated from the aggregation of the lower level objectives the attributes are the outcome of an alternative and can be defined either as a fixed value deterministic vector representing samples from a probability distribution function pdf in a monte carlo experiment or generators routines that yield pdf samples instead of the samples themselves by storing only the generation rule and its parameters see yee and van rossum 2001 these outcomes are the result of the mapping of the anticipated outcomes of the alternatives on the attributes which is often carried out using an assessment model which is problem dependent and therefore out of the scope of this document the objective hierarchy is constructed by mapping the attributes to the lower level objectives which again are mapped aggregated to the higher level objectives consequently the execution model will determine the value of each objective as either the value of its attributes or the aggregated value of its sub objectives where the latter overrides the former the aggregation functions are defined for each objective indicating how the lower level objectives will be aggregated weights are also defined for each objective indicating their relative importance regarding the other objectives at the same level this is to always ensure a one to many relationship between higher level and lower level objectives as there may only be one single weight for each objective in the hierarchy a schematic overview of the objective class is presented in fig 4 it shows that for each objective in the hierarchy it is required to obtain the 1 value utility v u we require to define the 2 weight and utility function if any for each objective the value of the objective can be calculated either by 3 a re scaling the attributes to the 0 1 interval of the 4 a attributes in their natural scale alternatively the objective s value can be calculated from the 3 b aggregation of the 4 b value of its sub objectives with their own defined weights 3 1 attribute valuation functions currently exponential and power utility functions keeney and howard 2002 are implemented in decisi o rama as marginal valuation functions over the individual attributes the exponential utility function corresponds to a constant absolute risk aversion while the power utility function to a constant relative risk aversion eisenführ et al 2010 setting the respective risk attitude parameter these models simplify to respective marginal value functions being r 0 for the exponential and p 1 for the power utility function both formulations assume that utilities can be measured based on the absolute magnitude of the outcomes and that probabilities are weighted linearly i e a reduction in risk of 5 is valued equally no matter if this concerns risk reduction from 50 to 45 or from 5 to 0 in addition decisi o rama supports user defined utility functions these functions map a normalized attribute value to a utility value on the range 0 1 the function signature receives two parameters 1 the scalar value in the natural range of the variable by default 0 1 and 2 a data structure containing the additional function parameters these characteristics allows the implementation to be scaleable and generalisable 3 2 multi attribute aggregation models decisi o rama includes some fundamental aggregation models used to aggregate across multiple attributes these include additive weighted sum geometric mean also known as cobb douglas harmonic mean power split power maximum and minimum aggregation langhans et al 2014 the definition of these basic functions are equivalent to those in the r package utility reichert et al 2013 also it is possible to include user defined aggregation functions in addition a meta function is defined to mix different aggregation functions as suggested by langhans et al 2013 and scholten et al 2015 this flexibility supports the development of custom aggregation models which may help in the generalisation and representation of the desirability of objectives reichert et al 2019 as an example it is possible to define a utility function as the minimum between the additive model and the geometric mean or make an additive aggregation of the power split power and geometric means 3 3 uncertainty aware preference and outcome modelling next to quantifying the anticipated outcomes of the alternatives on the attributes one of the main challenges is the elicitation of a preference model consequently this is represented in uncertain estimates of model parameters and aggregation functions these uncertain parameters propagate through the model yielding uncertain estimates of the value of objectives at a higher place in the hierarchy leading to uncertain estimates of the overall valuation of each alternative one of the approaches to quantify model uncertainty is monte carlo simulation the implementation of this approach is generally straight forward yet comes at the cost of considerable processing resources therefore to avoid the overload of a monte carlo we developed an execution model considering three main principles 1 using generators to sample the pdf s of the parameters of the utility aggregation functions and weights and attribute value where applicable for each node 2 a vectorised execution model of marginal utility and multi attribute aggregation functions and 3 on demand lazy model execution first using random number generators yee and van rossum 2001 has a significant impact on memory efficiency as only a representation of the pdf is kept instead of a sample of random values this feature allows to dispose intermediate results keeping only its representation and making the required pdf samples for the monte carlo analysis only available at node execution time alternatively to provide flexibility in the parameterisation of the models at the cost of memory performance it is possible to directly pass the random samples of the uncertain inputs or parameters in the analysis second using a vectorised execution model van der walt et al 2011 virtanen et al 2020 for the node operations attribute valuation and value utility aggregation allows considerable speed up with respect to looped implementations by making use of modern hardware architecture this vectorisation is possible as the node operations are relatively simple which can be integrated into linear algebra frameworks as a consequence it is possible to exploit the single instruction multiple data processing framework present in modern processors allowing to simultaneously compute several instructions within the same cpu cycle third an lazy execution watt 2006 of the objectives hierarchy permits executing only the required operations to modify only those parts of the problem at the time that are of interest providing flexibility in the re evaluation of specific branches this flexibility permits that nodes are evaluated separately such as for in depth analysis of particular parts of the hierarchy without re defining the problem in practical applications it means that only values or utilities for lower level objectives are calculated without executing any of the higher level objectives leading to more efficient evaluation of the objectives hierarchy for model exploration with these features it is possible to efficiently execute probabilistic preference models with conventional computing power this feature also allows to use decisi o rama to explore the impact of different preferences on valuation and outcome computation on the fly in addition the library does not require the use of specific data models making the integration with other applications such as models for determining the outcome of alternatives or the implementation of visualisation tools straight forward 3 4 evaluating results the architecture of decisi o rama separates the execution and the post processing of results this allows for different analyses to be carried out without re executing the model thus ensuring consistency in the analyses to support these analyses we introduce the evaluator class the evaluator permits to create a container for both aggregated valuation scores for each alternative and the performance metrics that summarise the results the valuation scores corresponds to the results of the value and utility of the hierarchical objectives while the performance metrics are the different functions that are used to summarise these results into point values i e mean median standard deviation coefficient of variation containing the results into a specific class ensures consistency when applying different performance metrics and provides a common ground to obtain result analysis tools such as pareto front identification and alternative rankings similarly to the other components of decisi o rama the evaluator class also supports user defined performance metrics 3 5 visualising objective hierarchies commonly hierarchical objectives are visualised through trees this representation provides a logical connection between the hierarchies of nodes and have been frequently used in different decision analysis tools to visualise the results and states of a given preference model reichert et al 2013 these trees serve as a representation of the objective hierarchy but do not enable to simultaneously highlight model parameters and results in addition using trees to represent a hierarchical model uses a vast amounts of empty screen or paper as the geometric structure its pyramidal leaving at least half of the available space unused to overcome these limitations we rely on a variant of a sunburst diagram 5 here we can visualise several features of the objective hierarchy including 1 its structure 2 weights and 3 values and or utilities the structure of the objective hierarchy is represented in concentric circular wedges where the internal wedge represents the upper level objective while an external wedge represents a lower level objective in this direction the circle in the centre corresponds to the highest level objective the weights w are represented with the proportional relative area of the wedge higher weights are represented with wider wedges the values and utilities are represented using a concentric bar within the wedge that is limited between the 10 p10 and 90 p90 percentile oriented in the anti clockwise direction and scaled to the relative width of each wedge for the particular case of the overall objective the value of zero is located over the right side of the horizontal axis in 0 position the mean value utility of each objective is represented by the color of the wedge with lighter colors indicating lower mean values utilities while darker colors indicate higher values utilities the visualisation in fig 5 represents one of the portfolios of the sample problem presented in fig 8 to further understand the proposed visualisation we will use the example in fig 5 here we can observe that the average weights for each of the sub objectives is about 0 25 being represented by wedges of the same size it can also be seen that c l i m a t e is the sub objective with the highest variability in value while n 2 and p have the lowest indicated by smaller orange wedge within the wedge of the respective sub objective regarding mean values it is possible to observe that p and n 2 have the highest values thus darker color while c l i m a t e has the lowest among the sub objectives additionally we see that the overall objective the circle in the centre has a mean value approximately of 0 5 180 counter clockwise from the horizon on the right with a p10 on 0 46 and p90 on 0 55 3 6 decisi o rama for solving portfolio problems one of the main features of decisi o rama is the possibility to explore portfolios of alternatives that are linked to a preference model to this end it is possible to use decisi o rama in three different ways first it is possible to assess the attributes of the lowest level objectives directly by computing the consequences of the outcomes of individual alternatives second if additivity of the alternatives can be assumed the portfolio valuation model can be fed with the consequences of each action in line with how portfolio decision problems are often framed third it is possible to pass the results of an assessment model directly into the attributes of the preference model in this context it is possible to determine not only the portfolio solutions in terms of binary values present or not but also as a fraction of a given action or more complex interactions between alternatives provided by the assessment model 3 7 implementation and software availability decisi o rama has been implemented as an open source project as such we have decided that decisi o rama is published under the mit license making it available as it is without restriction to access modification or commercialisation the intention of this licensing is to develop a community of users and collaborators to further advance its development making mcda accessible to practice and research decisi o rama is available in python 3 x making use of only few commonly used libraries ensuring its sustainability in time including numpy oliphant 2006 numba lam et al 2015 and scipy virtanen et al 2020 numpy is used for mathematical and linear algebra operations numba is used to interface to the llvm compiler lattner and adve 2004 thus supporting performance and scaleability while scipy is limited to its use in calculating percentile statistics in the post processing evaluator class 4 demonstration in this section we will explore the performance and usability of decisi o rama to test its computational performance we will use it to solve two large synthetic problems with a varying number of levels and nodes in homogeneous objective hierarchies with the same number of sub objectives these types of problems are easily scaleable and provide information about the capabilities of the tool to solve large problems following the usability of the tool is showcased in modelling the portfolio problem presented in lahtinen et al 2017 the benchmarks and simulations are carried out in conventional settings the tests in this implementation were carried out using python 3 7 7 the benchmark is performed in an intel core i5 3320m cpu 2 60 ghz with 8 gb of memory using ubuntu linux 18 04 64 bits tests in a windows environment yielded similar results the scripts to obtain these results are included in the documentation of decisi o rama 4 1 computational performance to test the computational performance we are solving a hierarchical aggregation problem using perfect binary 2 sub objectives and quinary 5 sub objectives trees for which each additional layer depth level in the hierarchy will yield an exponentially growing number of nodes the analysis is carried out for 2 to 12 depth levels in a binary tree thus ranging from 3 to 4095 nodes and for 2 to 6 depth levels in a quinary tree 6 3906 nodes although in practice the use of such large objective hierarchies is unrealistic we present these here as limiting cases for the sake of performance testing the evaluation is set to use uniform random values for the attributes in the lowest level objectives for the test we use a varying number of random samples from 100 10 2 to 100 000 10 5 in addition we estimate the marginal utility at each node using an exponential utility function with fixed parameters for the lowest level objectives while using an additive aggregation function to estimate the joint utility at the intermediate and overall objectives we used common indicators of computational performance namely wall time fig 6 and memory usage fig 7 obtained on an intel core i5 at 2 6 ghz the results in fig 6 indicate that the computational run time for the experiment is low even for large problems 100 000 random runs in more than 4000 nodes with evaluations being computed in the fraction of a second to help summarising the information about the performance two different experiments were carried out for the run time analysis fig 6 we calculated the time that it took to run each tree at different depth levels from 2 to 12 for the binary and from 2 to 6 for the quinary tree for different number of random samples from 100 to 100 000 for the memory usage fig 7 we use a sequential run indicating that over the same number of random samples the trees at different depths are run in the same experiment yielding a single line over time it has to be noted that the run times in fig 7 do not perfectly match those on fig 6 as it includes the definition of the problem and not only the execution time the results show a slightly better performance for the quinary over the binary tree figs 6 and 7 these results are explained by the fact that the binary tree in this experiment is deeper than the quinary tree thus requiring the allocation of a larger number of stacks before the calculations are executed due to the architecture of python stacks tend to be rather big in comparison to other lower level programming languages thus accounting for a more noticeable impact on memory use and run times than the amount of samples used in the experiment however run time and memory used in computing both of the experiments show an almost negligible impact below 7 mb for current day hardware configurations these results show that decisi o rama allows to handle large problems using little resources the size of the models depends largely on the number of objectives and not on the number of random samples that are drawn in the monte carlo experiment fig 7 this leads to large models being stored with almost negligible memory footprint making it suitable for analysis on nowadays standard hardware configurations 4 2 using decisi o rama in a portfolio decision analysis problem in this section we use decisi o rama to the portfolio decision analysis problem as presented in lahtinen et al 2017 a detailed description is given in the original reference here we will only describe those aspects which are relevant for demonstrating decisi o rama for this decision problem the decision problem is to support the selection of a portfolio of alternatives to reduce the water demand in the city of bass the overall objective consists not only in the saving of water volume but also considers long term financial effects million australian dollars maud climate change related impacts score and consequences for the local water system reduction in phosphorous and nitrogen ton yr the problem is represented based on six attributes table 1 in this example the cost and water demand are used as constraints to create portfolios as a water saving target of 50 should be achieved given a budget of 45 maud therefore the value model is constructed over the remaining attributes in the model to identify the best portfolio given these 6 objectives 9 potential alternatives to reduce water demand in the area are considered table 2 the consequences of these alternatives are given in table 3 assuming a uniform distribution of the attributes within the given ranges in the brackets in addition a series of constraints define which alternatives can be combined into a potential solution portfolio table 4 as a result the problem can be conceptualised as hierarchical estimation of the overall value based on aggregation of four attributes see fig 8 the alternatives are binary indicating that they are executed value of 1 or not value of 0 to create the vector representing a portfolio in fig 8 we present only two of the possible portfolios for illustration where in the first portfolio p1 only alternative 1 is considered and in the other p2 alternative 1 and 2 are selected in combination in the original decision problem the weights are assumed equal here we add uncertainty to the weights assuming that these come from the same normal distribution where w i n 0 25 0 05 additionally the attributes of the alternatives are modeled using a normal distribution within the upper and lower bounds as described in table 3 let us assume that the preferred portfolio is then the one with the highest expected value and smallest uncertainty and ignore more complex preferences with regard to uncertainty in the outcomes the optimal set of solutions in a multi objective optimisation problem are found in the non dominated pareto set where each of the solutions is better on at least one objective and at least equal over the other dominated solutions the expected value is calculated using the mean of the distribution while its uncertainty is expressed by the interquartile range difference between the percentile 25 and 75 iqr fig 9 to complement the analysis we add another metric of variation in the form of coefficient of variation cov fig 9 in this particular case several non dominated solutions meaning that are optimal in the multi objective optimisation sense can be found in the set in addition we include the proposed visualisation of the solutions in the non dominated set fig 10 here we can observe the characteristics of the model structure the results for the main and intermediate objectives with an indication of its uncertainty for example it is possible to see that about half of the non dominated solutions tend to produce low scores in the climate scenario as indicated by the orange segments representing its value being close to the start line in portfolios p2 p3 p5 p6 and p7 remember that the value increases in counter clockwise direction within each segment of the non dominated portfolios p8 clearly outperforms by mean the others as indicated by the darker color of the circle at the centre and position of the orange wedge within the circle over 270 from origin we acknowledge the limitations in visualising many features simultaneously therefore this visualisation proposes a way for the decision analyst of having an overall picture of the problem in terms of structure weights and hierarchy results values or utilities and its uncertainty we acknowledge that by no means this visualisation is aimed to obtain exact values as wedges are scaled and therefore we see the benefits of presenting this figure alongside tables that describe the exact values of interest in the objectives or by using interactive visualisations such as mouseover displays or dynamic legends 4 3 limitations and potential for future extension the current version of decisi o rama includes only basic mavt and maut models while these are generally sufficient for exploratory research and applications future users including ourselves can implement user defined preference structures that current preference models cannot handle langhans et al 2014 langhans and lienert 2016 reichert et al 2019 more specifically mavt and maut assume rational decision makers linear probability weighting and evaluation of alternatives on an absolute scale these contradict observed decision making behaviour which suggests for example presence of reference points non linear weighting of probabilities or ambiguous outcomes as well as different valuation of outcomes depending on whether these are framed as gains or losses as reflected in cumulative prospect theory stott 2006 beyond the modelling of preferences about outcomes and attached risks and uncertainties incorporation of time dependent preferences andreoni et al 2015 hermann and musshoff 2016 or social preferences chuang and schechter 2015 galizzi and navarro martinez 2019 would also be necessary to advance the mcda field both in terms of theory and practice beyond preference modelling for individual actions further developments will be needed to encompass more complicated portfolio preferences such as multi linear value functions for robust portfolio modelling liesiö 2014 and portfolio level measures of risk and regret liesiö et al 2007 vilkkumaa et al 2014 also the example we presented did assume additivity i e independence between the value of actions in a portfolio further more involved assessment models on attribute level will be needed to reflect dependencies in portfolio performance and contingencies in portfolio creation gustafsson and salo 2005 liesiö et al 2008 decisi o rama has two main limitations that we can foresee in the scope of its implementation that constraint its applicability in the view of more generic processes or approches to the mavt maut problem first decisi o rama does not support correlated or conditioned random variables in its generators as instances of the generated random variables are created on the fly however this can be circumvented by passing complete vectors to represent the samples of the random variable instead of generators second decisi o rama only handles uncertainty in the form of probability distribution functions and not in other encoding such as fuzzy sets or possibilistic distributions in the close future of decisi o rama we expect to add other features including sensitivity analysis and optimisation routines in sensitivity analysis we see utility in developing local one at a time and global sensitivity analysis for single portfolio and model assessment respectively in the optimisation routines we expect to integrate heuristic optimisation methods that support exploring the potential portfolios thus implying binary optimisation as well as integer continuous and mixed integer optimisation using linear and non linear approaches the decisi o rama library has been built keeping these possible developments in mind ensuring flexibility to all types of extension and efficiency in computation we explicitly invite the community to build it further and to explore its usability in real world applications 5 conclusions we have introduced decisi o rama an open source python library for mcda on single alternative and portfolio approaches in the context of multi attribute utility value theory we have demonstrated the capabilities of the tool to efficiently manage large multi criteria decision analysis problems that include uncertain attributes and preference parameters using conventional hardware therefore it can be used for exploratory modelling of decision alternatives with decision makers on the fly also due to its high computational efficiency it can be used to support robust portfolio modelling applications alternative optimisation algorithms and sensitivity analysis that may enrich the decision analysis process in addition decisi o rama offers a flexible implementation that supports user defined preference models in this respect it is possible to define non customary marginal value or utility functions as well as aggregation functions also it is possible that the users specify uncertain parameters of the preference model through their user defined distributions these features ensure straightforward extensibility for different types of models and users keeping in mind that accessibility and extensibility are paramount for future usability of decisi o rama it is offered to the community as open source this same principle was maintained through the development of the tool reducing the number of third party libraries for its execution and supporting the readability and maintainability of the code furthermore the tool operates with regular python objects making its integration with other applications such as different attribute assessment process models and visualisation tools with these decisions we hope that this tool will be used in different settings and for different user groups facilitating the adoption of mcda methods by researchers and practitioners alike declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was conducted as part of the cooperation programme tisca technology innovation for sewer condition assessment with project number 15343 which is partly financed by nwo domain ttw the domain applied and engineering sciences of the netherlands organisation for scientific research the rioned foundation stowa foundation for applied water research and the knowledge program urban drainage kpud furthermore the authors would like to thank julia hartmann and job van der werf for their collaboration in reviewing this document and the three anonymous reviewers whose constructive suggestions helped us in considerably improving the final version of this document 
25917,environmental decisions are complex as they are multi dimensional highly interdisciplinary and not only involve multiple stakeholders with conflicting objectives but also many possible alternatives with uncertain consequences the difficulty lies in making trade offs between tough value trade offs on the one hand while appreciating uncertain impacts of alternatives on the other to support decisions tackling such problems a combination of multi criteria decision analysis mcda and environmental models is promising yet limited by the available mcda software here we present decisi o rama an open source python mcda library for single and sets portfolios of alternatives in the context of multi attribute value utility theory maut mavt its development was driven by four aspirations that are crucial for usability in the context of environmental decision making 1 interoperability 2 uncertainty awareness 3 computational efficiency and 4 integration with portfolio decisions the results indicate that these aspirations are met thus facilitating the adoption of mcda methods by environmental researchers and practitioners keywords maut mavt portfolio decision analysis multi criteria decision analysis python 1 introduction environmental decisions are often complex as there are usually many courses of action often conflicting and uncertain the reason for this complexity lies in how the utilisation of natural resources has sociopolitical environmental economic and even ethical implications at the time these decisions are often supported by uncertain and often incomparable data that need to be synthesized to support decision makers multi criteria decision analysis provides a framework to systematically evaluate decisions providing transparency traceability and reproducibility in the process as a result there has been an increasing interest in mcda methods for environmental decision making kiker et al 2005 huang et al 2011 linkov and moberg 2017 esmail and geneletti 2018 ortiz et al 2018 encompassing decision support langhans et al 2014 lahtinen et al 2017 haag et al 2019 kuemmerlen et al 2019 marttunen et al 2019 constructing ecological indicators convertino et al 2013 cinelli et al 2014 langhans et al 2014 zheng et al 2016 blattert et al 2017 and the support of planning activities joubert et al 2003 gómez delgado and tarantola 2006 hajkowicz 2007 calizaya et al 2010 huang et al 2011 sa nguanduan and nititvattananon 2011 lienert et al 2014 scholten et al 2015 scholten et al 2017 decision analysis aims to formalise common sense for decision problems which are too complex for informal use of common sense 30 p 806 decisions can become complex when there are multiple decision makers with different views of the issues that need addressing conflicting goals and manifold possible courses of action uncertainty about outcomes of different actions unfamiliarity especially in case of one off strategic decisions high interdisciplinarity and also different subjective valuation of outcomes and risks decision making is further complicated by the overall goals that are considered to compare alternative courses of action often being incommensurate and high dimensional in this context recurrent decisions with lower stakes and well defined objectives are generally easier than one off decisions with high stakes in complex decision contexts where the objectives need to be identified jointly with the decision makers in the first place keeney 1982 bureš et al 2019 environmental decisions are often of the latter type to account for these complex and inter related processes in a formal context decision analysis methods that combine problem structuring and multi criteria decision analysis mcda models are often used marttunen et al 2017 these methods support the decision making process by structuring the decision problem systematising the objectives and assessing their fulfilment and measureable attributes given a set of alternatives individual actions or portfolios of actions reichert et al 2013 this allows to compare among alternatives thus supporting the decision makers in exploring the decision space and model results voinov and bousquet 2010 voinov et al 2016 as a result decisions become more transparent traceable and reproducible mcda methods are generally classified with respect to how goal attainment is defined 1 overall value score or rank 2 goal reference or aspiration level and 3 outranking belton and stewart 2002 mendoza and martins 2006 summarising value methods indicate the preference of an alternative or portfolios of alternatives relative to others based on a numerical score these methods include the multi attribute utility and value approaches maut and mavt respectively eisenführ et al 2010 aspiration level methods assess the solutions based on the level of attainment of a set of goals as met or not an example of this is goal programming jones and tamiz 2016 outranking methods use a pair wise comparison of alternatives to identify a ranking of preferences examples are electre govindan and jepsen 2016 and promethee behzadian et al 2010 family methods for a more detailed overview of mcda and related methods see figueira et al 2006 mcda models come into play once the decision problem is structured these imply that 1 there is clarity about who takes part in the decision 2 what objectives and criteria are to be considered and 3 which alternatives are feasible mendoza and martins 2006 assuming a summarising value approach it is then necessary to understand and quantify the impacts of the alternatives on the attributes indicators to measure the outcomes of alternatives regarding the objectives the impact of different alternatives on the attributes are obtained from the assumed cause effect relationship between alternatives and anticipated consequences on the attributes this can be obtained based on conceptual or mathematical models that make these assumed relationships explicit or from estimates obtained directly from data or expert knowledge here we use the term assessment model to describe the mathematical models that are used to map the portfolios of alternatives to the expected outcomes on the attributes next mcda models require a preference model to appraise the portfolios of alternatives based on the perceived valuation of the anticipated outcomes and their trade offs this preference model should consider the different perspectives of the stakeholders trade offs among different competing objectives risk attitudes and ambiguity attitudes of the decision makers keeney 1982 reichert et al 2013 scholten et al 2015 in addressing a complex decision problem we agree with belton and stewart 2002 in that the purpose of an mcda model is to provide a focus for discussion not to prescribe the solution therefore mcda models are useful to learn about trade offs among alternatives and to formulate construct decision maker preferences unlike optimisation driven approaches the purpose of mcda models is not to determine a normatively best strategy to choose given a set of conflicting objectives as decision making involves judgment and valuation subjectivity cannot be avoided and the responsibility for the decision and its consequences remain with the decision maker s generally mcda methods are used to evaluate among different alternatives however in many cases the actions to be taken go beyond a single alternative e g which project to conduct seeking to identify a set of potential alternatives portfolio to implement e g a combination of several projects salo et al 2006 the number and combination of alternatives in a portfolio is subject to constraints such as the available budget or personnel gustafsson and salo 2005 liesiö et al 2008 also contingencies and interactions matter regarding which actions can be combined and whether the outcomes are independent of each other i e add up or not e g economies of scale upstream downstream interactions portfolio decision analysis pda integrates portfolio theory and multi attribute value models kleinmuntz 2007 salo et al 2011 to explore optimal deterministic portfolios or robust portfolios when considering the uncertainty in the decision maker preferences and alternative performance liesiö et al 2007 2008 applications of pda in mcda are reported in e g salo et al 2006 kleinmuntz 2007 mild and sahlo 2009 convertino et al 2013 lahtinen et al 2017 to navigate these decision spaces it is necessary to have tools that help in exploring the consequences of different individual or portfolio alternatives along with their uncertainties therefore it is necessary that modern decision analysis tools are able to accommodate uncertainty and sensitivity analysis french 2003 gómez delgado and tarantola 2006 durbach and stewart 2011 scholten et al 2015 esmail and geneletti 2018 conventionally uncertainty and sensitivity analysis require considerable computational resources for simulation and model evaluation over larger samples of model parameters and inputs as analytical solutions are either too simplified unavailable or too restrictive regarding the characteristics of each particular model for example one of us conducted extensive uncertainty and sensitivity analysis using an earlier version of the utility r package reichert et al 2013 for which it took days to compute the results for a real world decision problem on a high performance computing cluster scholten et al 2015 2017 that makes on the fly exploratory decision modelling with stakeholders unviable ideally decision tools should be computationally efficient to produce results in a short time and generic enough to be integrated together with other model exploration tools such as optimisation routines and scenario modelling to summarise the identified mcda modelling needs we need software or software libraries ideally open source that are 1 extensible and interoperable with complementary modelling and visualisation tools that 2 support uncertainty aware mcda including uncertainty in outcomes and preferences while also being 3 computationally efficient to be used on the fly with decision makers and 4 for portfolio decisions first extensibility and interoperability allows to accommodate problem specific process models integration into larger decision support frameworks and use complementary model analysis tools such as optimisation and visualisation second uncertainty aware decision models provide the decision makers with information about the magnitude and sources of the quantifiable uncertainty in the model third a computationally efficient model allows modelling and interaction with decision makers on the fly providing direct feedback on the consequences of the alternatives or portfolios and preference model supporting discussion fourth by including the possibility to add portfolio decisions it is possible to support realistic mcda to our surprise we were not able to find any suitable software among the plethora of tools available to support mcda that was able to do so let alone for our target method maut mavt for portfolio decisions see reviews in weistroffer and li 2016 mustajoki and marttunen 2017 international society on mcdm 2020 cinelli et al 2020 we found that most of these tools are either embedded into closed source decision support platforms built over particular data environments not without a reason or as commercial software making it difficult to extend and adapt these to specific needs neither in practice nor research there are a few notable exceptions including mcda bigaret et al 2017 r java and above mentioned utility reichert et al 2013 r the latter do not however provide an alternative valuation framework and lack the built in capability to conduct portfolio decision analyses lastly given the ubiquity of use of python for mathematical modelling and scientific computing it is surprising that no native mcda libraries are available for mavt maut and pda consequently we see value in developing a decision analysis tool that fulfils the identified needs and thus 1 is extensible and interoperable 2 uncertainty aware 3 computationally efficient and 4 able to integrate portfolio decisions in this paper we present the development and testing of decisi o rama an open source python library for mcda on single alternatives and portfolios in the context of mavt maut the name decisi o rama uses the suffix orama as in panorama derived from ancient greek hórama meaning view or spectacle the spelling o rama is popularly used to stress the latter meaning thus decision show or decision spect acle as we aim to provide a platform to expose the story behind a decision by making it transparent logic understandable and for everyone to see decisi o rama provides an interoperable efficient and flexible framework to support mcda using the summarising value type preference modelling for uncertain decision analysis problems this paper is organised as follows first a literature review of mavt maut and the current preference modelling tools is presented second the main features of decisi o rama are presented exposing the concepts behind its functionality third decisi o rama is used to solve two large synthetic problems to test its performance fourth a portfolio decision analysis problem as presented in lahtinen et al 2017 is solved where additional features of decisi o rama are showcased lastly conclusions recommendations and future directions are presented 2 multi criteria valuation models multi attribute decision analysis models including mavt and maut and referred to only as maut from this point on as common in the literature while noting that only the former warrants measurable value functions dyer and sarin 1979 next to many integrated assessment models aim to estimate the utility or attainment of an objective given a set of hierarchically structured attributes or criteria nardo et al 2008 singh et al 2012 scholten et al 2017 maut implies that a score can be constructed by aggregating different attributes which require the construction elicitation of a preference model keeney and howard 2002 cinelli et al 2014 among its more interesting features are its conceptual simplicity and its suitability to include risky choices dyer 2005 eisenführ et al 2010 preference models following multi attribute value theory are based on three main elements including an objectives hierarchy the assessment of the marginal utilities or values and trade offs among its different objectives the objectives hierarchy makes a top down assessment where the overall objective of the decision is broken down into intermediate objectives which can be further disaggregated either into other intermediate objectives or into lowest level objectives for each of which measurable or estimable attributes are defined it is important that only fundamental objectives are included in the hierarchy further reading regarding the formulation of objectives and structuring of objective hierarchies can be found in eisenführ et al 2010 once the objective hierarchy is defined the marginal valuation functions over the attributes and the trade offs among the attributes and objectives are to be elicited the valuation functions either include the risk preferences of the decision makers utility functions or not value functions regarding the attributes and objectives the trade offs are elicited by understanding what are the desirable trade offs often expressed as importance weights among the attributes and how these should be aggregated to represent these trade offs value or utility aggregation function this elicitation process yields uncertain parameters of the preference model as a consequence of the conceptual simplicity of the preference models preference instability e g lienert et al 2016 and limited interaction with the decision makers among other reasons scholten et al 2015 to summarise we can represent the components of portfolio decisions in the mavt maut as presented in fig 1 these components also represent the type of problems that decisi o rama addresses starting from the top it is possible to identify the preference model orange box the assessment model blue box and the alternative portfolios white box in the preference model g represents the overall objective o1 and o2 are intermediate objectives with utilities values of o1 constructed by aggregating the attributes at1 and at2 and o2 being quantified by at3 the overall objective is defined by aggregating o1 and o2 continuing the assessment model permits mapping the consequences of each portfolio p of alternatives to attributes outcomes and respective valuation in light of the given preference model finally the alternatives a are defined as either single actions or a collection of actions portfolios that are selected 1 or not 0 as candidates in the mcda model 2 1 constructing mavt and maut models once the objectives and their hierarchy are defined a maut preference model needs to be specified to valuate the alternatives based on the assessment model outputs with regard to the objectives 2 1 1 valuating objectives marginal valuation functions valuating an objective in the objectives hierarchy requires defining a value or a utility function the value of the objective represents the subjective desirability of its level of attainment under certainty the utility function combines the risk attitude of the decision maker and the subjective desirability of attaining some level on a particular attribute or objective with quantifiable probability risk dyer 2005 for mcda with maut utility preferences are modeled through expected utility theory eut von neumann and morgenstern 2007 in eut the risk preferences are reflected in von neumann and morgenstern type utility functions obtained from elicitation of preferences over lotteries according to which the decision makers are either risk averse risk seeking or risk neutral this utility is characterised by relating the value of attributes and utilities to monotonic mappings in which the utility is absolute assuming a rational decision von neumann and morgenstern 2007 to model the utility of an objective several approximations have been used in the context of eut common models of utility are the exponential and power model which correspond to a constant relative and absolute risk aversion respectively both approximations use a single parameter r or p to indicates the risk attitude in fig 2 the exponential and power utilities are presented for different risk attitudes where convexity r 0 and p 1 represents risk aversion and concavity r 0 and p 1 represents risk seeking 2 1 2 aggregating objectives overall valuation function once the utilities for attributes and objectives are assessed the utilities corresponding to the attribute levels are aggregated in the intermediate objectives and later aggregated towards the overall objective this aggregation requires the definition of the relative importance weights of each attribute or objective and the aggregation function the relative importance of each of the attributes or objectives is represented by its weight the aggregation function determines how to compute an overall score across the individual attributes representing the trade offs among attributes and objectives to explore the consequences of different aggregation functions and their impact we consider aggregation models that represent different preferences regarding compensation between objectives on the one hand compensatory models are defined by the existence of a trade off between the different attributes to be integrated in other words a loss in value due to changes in one attribute can be compensated by the gains in another on the other hand non compensatory aggregation models are used to indicate that is not possible to compensate the losses in a given attribute by improving another an example of compensatory attributes is revenue in contrast to non compensatory aggregation such as bio diversity indicators langhans et al 2014 the mathematical formulation of some of the most used aggregation functions can be found in reichert et al 2013 in fig 3 we present the impact of different aggregation functions and weights on the trade off between two attributes as in langhans et al 2014 these figures show the compensation rate for two attributes represented in the horizontal and vertical axes while the contour lines indicate iso aggregated values if the lines are vertical or horizontal then there is no compensation between attributes if there is a gradient it describes the degree of compensation between them here it is possible to see how aggregation functions like the additive linear model are fully compensatory the trade off is independent of the value of the attributes while either the maximum and minimum aggregation functions are non compensatory other functions such as the geometric and harmonic mean indicate a varying non linear compensation where the degree of compensation varies with the value on the respective attributes depending on the characteristics of the trade offs other conventional e g multiplicative utility keeney and howard 2002 mixed scholten et al 2015 or unconventional aggregation functions could be used reichert et al 2019 2 1 3 alternatives and portfolios alternatives and portfolios are the building blocks of any decision analysis system as the problems are framed to evaluate their preferability alternatives are potential actions that will be decided upon in the decision analysis system in this respect a portfolio can be seen as a collection of actions that are simultaneously carried out salo et al 2011 in many decision analysis problems the alternatives correspond to a set of potential actions that can be carried out to affect the system the characteristics of these actions are specific to each problem and are typically proposed by teams of experts once all the potential alternatives are set portfolios of alternatives can be constructed by combining individual alternatives and their corresponding actions where a portfolio of potential alternatives is to be selected it is necessary to note that the preferability of a given portfolio cannot be interpreted as the sum or weighted sum of its components the reason for this relates to the non linearity of the preference and assessment models this non linearity can be seen in the form of interactions among alternatives in the resulting attributes as estimated by the assessment model utilities in the intermediate objectives of the preference model and potential constraints e g competition for resources 3 decisi o rama features and reference decisi o rama is an open source python library that supports the development of mcda models for mavt and maut to compare alternatives or portfolios decisi o rama encompasses the development of the preference models as well as pre and post processing tools to support the mcda process relying on common python objects these features make the integration with attribute performance assessment models straight forward permitting transparent workflows in the implementation of a diverse range of mavt maut models in addition the tool is focused on simplicity and extensibility allowing the users to implement user defined components in the framework without necessarily modifying the source code one of the main features of decisi o rama is the recursive execution of the objectives hierarchy this allows to 1 execute large models with minimum impact on memory usage and 2 flexibly evaluating only parts of the objective hierarchy without model redefinition as a consequence it is suitable for the implementation of large decision models in conventional hardware configurations making on the fly model exploration in group settings feasible the construction of the objectives hierarchy is separated in two instances 1 the creation of the individual objectives nodes and 2 the creation of the hierarchy of objectives in decisi o rama nodes in the objectives hierarchy including the attributes intermediate objectives and overall objective are conceptualised as objectives thus simplifying model construction the creation of the nodes requires defining attributes weights value and aggregation functions in case the node corresponds to an attribute the aggregation function will not be considered in the same way if the node is an objective intermediate or overall its value will be calculated from the aggregation of the lower level objectives the attributes are the outcome of an alternative and can be defined either as a fixed value deterministic vector representing samples from a probability distribution function pdf in a monte carlo experiment or generators routines that yield pdf samples instead of the samples themselves by storing only the generation rule and its parameters see yee and van rossum 2001 these outcomes are the result of the mapping of the anticipated outcomes of the alternatives on the attributes which is often carried out using an assessment model which is problem dependent and therefore out of the scope of this document the objective hierarchy is constructed by mapping the attributes to the lower level objectives which again are mapped aggregated to the higher level objectives consequently the execution model will determine the value of each objective as either the value of its attributes or the aggregated value of its sub objectives where the latter overrides the former the aggregation functions are defined for each objective indicating how the lower level objectives will be aggregated weights are also defined for each objective indicating their relative importance regarding the other objectives at the same level this is to always ensure a one to many relationship between higher level and lower level objectives as there may only be one single weight for each objective in the hierarchy a schematic overview of the objective class is presented in fig 4 it shows that for each objective in the hierarchy it is required to obtain the 1 value utility v u we require to define the 2 weight and utility function if any for each objective the value of the objective can be calculated either by 3 a re scaling the attributes to the 0 1 interval of the 4 a attributes in their natural scale alternatively the objective s value can be calculated from the 3 b aggregation of the 4 b value of its sub objectives with their own defined weights 3 1 attribute valuation functions currently exponential and power utility functions keeney and howard 2002 are implemented in decisi o rama as marginal valuation functions over the individual attributes the exponential utility function corresponds to a constant absolute risk aversion while the power utility function to a constant relative risk aversion eisenführ et al 2010 setting the respective risk attitude parameter these models simplify to respective marginal value functions being r 0 for the exponential and p 1 for the power utility function both formulations assume that utilities can be measured based on the absolute magnitude of the outcomes and that probabilities are weighted linearly i e a reduction in risk of 5 is valued equally no matter if this concerns risk reduction from 50 to 45 or from 5 to 0 in addition decisi o rama supports user defined utility functions these functions map a normalized attribute value to a utility value on the range 0 1 the function signature receives two parameters 1 the scalar value in the natural range of the variable by default 0 1 and 2 a data structure containing the additional function parameters these characteristics allows the implementation to be scaleable and generalisable 3 2 multi attribute aggregation models decisi o rama includes some fundamental aggregation models used to aggregate across multiple attributes these include additive weighted sum geometric mean also known as cobb douglas harmonic mean power split power maximum and minimum aggregation langhans et al 2014 the definition of these basic functions are equivalent to those in the r package utility reichert et al 2013 also it is possible to include user defined aggregation functions in addition a meta function is defined to mix different aggregation functions as suggested by langhans et al 2013 and scholten et al 2015 this flexibility supports the development of custom aggregation models which may help in the generalisation and representation of the desirability of objectives reichert et al 2019 as an example it is possible to define a utility function as the minimum between the additive model and the geometric mean or make an additive aggregation of the power split power and geometric means 3 3 uncertainty aware preference and outcome modelling next to quantifying the anticipated outcomes of the alternatives on the attributes one of the main challenges is the elicitation of a preference model consequently this is represented in uncertain estimates of model parameters and aggregation functions these uncertain parameters propagate through the model yielding uncertain estimates of the value of objectives at a higher place in the hierarchy leading to uncertain estimates of the overall valuation of each alternative one of the approaches to quantify model uncertainty is monte carlo simulation the implementation of this approach is generally straight forward yet comes at the cost of considerable processing resources therefore to avoid the overload of a monte carlo we developed an execution model considering three main principles 1 using generators to sample the pdf s of the parameters of the utility aggregation functions and weights and attribute value where applicable for each node 2 a vectorised execution model of marginal utility and multi attribute aggregation functions and 3 on demand lazy model execution first using random number generators yee and van rossum 2001 has a significant impact on memory efficiency as only a representation of the pdf is kept instead of a sample of random values this feature allows to dispose intermediate results keeping only its representation and making the required pdf samples for the monte carlo analysis only available at node execution time alternatively to provide flexibility in the parameterisation of the models at the cost of memory performance it is possible to directly pass the random samples of the uncertain inputs or parameters in the analysis second using a vectorised execution model van der walt et al 2011 virtanen et al 2020 for the node operations attribute valuation and value utility aggregation allows considerable speed up with respect to looped implementations by making use of modern hardware architecture this vectorisation is possible as the node operations are relatively simple which can be integrated into linear algebra frameworks as a consequence it is possible to exploit the single instruction multiple data processing framework present in modern processors allowing to simultaneously compute several instructions within the same cpu cycle third an lazy execution watt 2006 of the objectives hierarchy permits executing only the required operations to modify only those parts of the problem at the time that are of interest providing flexibility in the re evaluation of specific branches this flexibility permits that nodes are evaluated separately such as for in depth analysis of particular parts of the hierarchy without re defining the problem in practical applications it means that only values or utilities for lower level objectives are calculated without executing any of the higher level objectives leading to more efficient evaluation of the objectives hierarchy for model exploration with these features it is possible to efficiently execute probabilistic preference models with conventional computing power this feature also allows to use decisi o rama to explore the impact of different preferences on valuation and outcome computation on the fly in addition the library does not require the use of specific data models making the integration with other applications such as models for determining the outcome of alternatives or the implementation of visualisation tools straight forward 3 4 evaluating results the architecture of decisi o rama separates the execution and the post processing of results this allows for different analyses to be carried out without re executing the model thus ensuring consistency in the analyses to support these analyses we introduce the evaluator class the evaluator permits to create a container for both aggregated valuation scores for each alternative and the performance metrics that summarise the results the valuation scores corresponds to the results of the value and utility of the hierarchical objectives while the performance metrics are the different functions that are used to summarise these results into point values i e mean median standard deviation coefficient of variation containing the results into a specific class ensures consistency when applying different performance metrics and provides a common ground to obtain result analysis tools such as pareto front identification and alternative rankings similarly to the other components of decisi o rama the evaluator class also supports user defined performance metrics 3 5 visualising objective hierarchies commonly hierarchical objectives are visualised through trees this representation provides a logical connection between the hierarchies of nodes and have been frequently used in different decision analysis tools to visualise the results and states of a given preference model reichert et al 2013 these trees serve as a representation of the objective hierarchy but do not enable to simultaneously highlight model parameters and results in addition using trees to represent a hierarchical model uses a vast amounts of empty screen or paper as the geometric structure its pyramidal leaving at least half of the available space unused to overcome these limitations we rely on a variant of a sunburst diagram 5 here we can visualise several features of the objective hierarchy including 1 its structure 2 weights and 3 values and or utilities the structure of the objective hierarchy is represented in concentric circular wedges where the internal wedge represents the upper level objective while an external wedge represents a lower level objective in this direction the circle in the centre corresponds to the highest level objective the weights w are represented with the proportional relative area of the wedge higher weights are represented with wider wedges the values and utilities are represented using a concentric bar within the wedge that is limited between the 10 p10 and 90 p90 percentile oriented in the anti clockwise direction and scaled to the relative width of each wedge for the particular case of the overall objective the value of zero is located over the right side of the horizontal axis in 0 position the mean value utility of each objective is represented by the color of the wedge with lighter colors indicating lower mean values utilities while darker colors indicate higher values utilities the visualisation in fig 5 represents one of the portfolios of the sample problem presented in fig 8 to further understand the proposed visualisation we will use the example in fig 5 here we can observe that the average weights for each of the sub objectives is about 0 25 being represented by wedges of the same size it can also be seen that c l i m a t e is the sub objective with the highest variability in value while n 2 and p have the lowest indicated by smaller orange wedge within the wedge of the respective sub objective regarding mean values it is possible to observe that p and n 2 have the highest values thus darker color while c l i m a t e has the lowest among the sub objectives additionally we see that the overall objective the circle in the centre has a mean value approximately of 0 5 180 counter clockwise from the horizon on the right with a p10 on 0 46 and p90 on 0 55 3 6 decisi o rama for solving portfolio problems one of the main features of decisi o rama is the possibility to explore portfolios of alternatives that are linked to a preference model to this end it is possible to use decisi o rama in three different ways first it is possible to assess the attributes of the lowest level objectives directly by computing the consequences of the outcomes of individual alternatives second if additivity of the alternatives can be assumed the portfolio valuation model can be fed with the consequences of each action in line with how portfolio decision problems are often framed third it is possible to pass the results of an assessment model directly into the attributes of the preference model in this context it is possible to determine not only the portfolio solutions in terms of binary values present or not but also as a fraction of a given action or more complex interactions between alternatives provided by the assessment model 3 7 implementation and software availability decisi o rama has been implemented as an open source project as such we have decided that decisi o rama is published under the mit license making it available as it is without restriction to access modification or commercialisation the intention of this licensing is to develop a community of users and collaborators to further advance its development making mcda accessible to practice and research decisi o rama is available in python 3 x making use of only few commonly used libraries ensuring its sustainability in time including numpy oliphant 2006 numba lam et al 2015 and scipy virtanen et al 2020 numpy is used for mathematical and linear algebra operations numba is used to interface to the llvm compiler lattner and adve 2004 thus supporting performance and scaleability while scipy is limited to its use in calculating percentile statistics in the post processing evaluator class 4 demonstration in this section we will explore the performance and usability of decisi o rama to test its computational performance we will use it to solve two large synthetic problems with a varying number of levels and nodes in homogeneous objective hierarchies with the same number of sub objectives these types of problems are easily scaleable and provide information about the capabilities of the tool to solve large problems following the usability of the tool is showcased in modelling the portfolio problem presented in lahtinen et al 2017 the benchmarks and simulations are carried out in conventional settings the tests in this implementation were carried out using python 3 7 7 the benchmark is performed in an intel core i5 3320m cpu 2 60 ghz with 8 gb of memory using ubuntu linux 18 04 64 bits tests in a windows environment yielded similar results the scripts to obtain these results are included in the documentation of decisi o rama 4 1 computational performance to test the computational performance we are solving a hierarchical aggregation problem using perfect binary 2 sub objectives and quinary 5 sub objectives trees for which each additional layer depth level in the hierarchy will yield an exponentially growing number of nodes the analysis is carried out for 2 to 12 depth levels in a binary tree thus ranging from 3 to 4095 nodes and for 2 to 6 depth levels in a quinary tree 6 3906 nodes although in practice the use of such large objective hierarchies is unrealistic we present these here as limiting cases for the sake of performance testing the evaluation is set to use uniform random values for the attributes in the lowest level objectives for the test we use a varying number of random samples from 100 10 2 to 100 000 10 5 in addition we estimate the marginal utility at each node using an exponential utility function with fixed parameters for the lowest level objectives while using an additive aggregation function to estimate the joint utility at the intermediate and overall objectives we used common indicators of computational performance namely wall time fig 6 and memory usage fig 7 obtained on an intel core i5 at 2 6 ghz the results in fig 6 indicate that the computational run time for the experiment is low even for large problems 100 000 random runs in more than 4000 nodes with evaluations being computed in the fraction of a second to help summarising the information about the performance two different experiments were carried out for the run time analysis fig 6 we calculated the time that it took to run each tree at different depth levels from 2 to 12 for the binary and from 2 to 6 for the quinary tree for different number of random samples from 100 to 100 000 for the memory usage fig 7 we use a sequential run indicating that over the same number of random samples the trees at different depths are run in the same experiment yielding a single line over time it has to be noted that the run times in fig 7 do not perfectly match those on fig 6 as it includes the definition of the problem and not only the execution time the results show a slightly better performance for the quinary over the binary tree figs 6 and 7 these results are explained by the fact that the binary tree in this experiment is deeper than the quinary tree thus requiring the allocation of a larger number of stacks before the calculations are executed due to the architecture of python stacks tend to be rather big in comparison to other lower level programming languages thus accounting for a more noticeable impact on memory use and run times than the amount of samples used in the experiment however run time and memory used in computing both of the experiments show an almost negligible impact below 7 mb for current day hardware configurations these results show that decisi o rama allows to handle large problems using little resources the size of the models depends largely on the number of objectives and not on the number of random samples that are drawn in the monte carlo experiment fig 7 this leads to large models being stored with almost negligible memory footprint making it suitable for analysis on nowadays standard hardware configurations 4 2 using decisi o rama in a portfolio decision analysis problem in this section we use decisi o rama to the portfolio decision analysis problem as presented in lahtinen et al 2017 a detailed description is given in the original reference here we will only describe those aspects which are relevant for demonstrating decisi o rama for this decision problem the decision problem is to support the selection of a portfolio of alternatives to reduce the water demand in the city of bass the overall objective consists not only in the saving of water volume but also considers long term financial effects million australian dollars maud climate change related impacts score and consequences for the local water system reduction in phosphorous and nitrogen ton yr the problem is represented based on six attributes table 1 in this example the cost and water demand are used as constraints to create portfolios as a water saving target of 50 should be achieved given a budget of 45 maud therefore the value model is constructed over the remaining attributes in the model to identify the best portfolio given these 6 objectives 9 potential alternatives to reduce water demand in the area are considered table 2 the consequences of these alternatives are given in table 3 assuming a uniform distribution of the attributes within the given ranges in the brackets in addition a series of constraints define which alternatives can be combined into a potential solution portfolio table 4 as a result the problem can be conceptualised as hierarchical estimation of the overall value based on aggregation of four attributes see fig 8 the alternatives are binary indicating that they are executed value of 1 or not value of 0 to create the vector representing a portfolio in fig 8 we present only two of the possible portfolios for illustration where in the first portfolio p1 only alternative 1 is considered and in the other p2 alternative 1 and 2 are selected in combination in the original decision problem the weights are assumed equal here we add uncertainty to the weights assuming that these come from the same normal distribution where w i n 0 25 0 05 additionally the attributes of the alternatives are modeled using a normal distribution within the upper and lower bounds as described in table 3 let us assume that the preferred portfolio is then the one with the highest expected value and smallest uncertainty and ignore more complex preferences with regard to uncertainty in the outcomes the optimal set of solutions in a multi objective optimisation problem are found in the non dominated pareto set where each of the solutions is better on at least one objective and at least equal over the other dominated solutions the expected value is calculated using the mean of the distribution while its uncertainty is expressed by the interquartile range difference between the percentile 25 and 75 iqr fig 9 to complement the analysis we add another metric of variation in the form of coefficient of variation cov fig 9 in this particular case several non dominated solutions meaning that are optimal in the multi objective optimisation sense can be found in the set in addition we include the proposed visualisation of the solutions in the non dominated set fig 10 here we can observe the characteristics of the model structure the results for the main and intermediate objectives with an indication of its uncertainty for example it is possible to see that about half of the non dominated solutions tend to produce low scores in the climate scenario as indicated by the orange segments representing its value being close to the start line in portfolios p2 p3 p5 p6 and p7 remember that the value increases in counter clockwise direction within each segment of the non dominated portfolios p8 clearly outperforms by mean the others as indicated by the darker color of the circle at the centre and position of the orange wedge within the circle over 270 from origin we acknowledge the limitations in visualising many features simultaneously therefore this visualisation proposes a way for the decision analyst of having an overall picture of the problem in terms of structure weights and hierarchy results values or utilities and its uncertainty we acknowledge that by no means this visualisation is aimed to obtain exact values as wedges are scaled and therefore we see the benefits of presenting this figure alongside tables that describe the exact values of interest in the objectives or by using interactive visualisations such as mouseover displays or dynamic legends 4 3 limitations and potential for future extension the current version of decisi o rama includes only basic mavt and maut models while these are generally sufficient for exploratory research and applications future users including ourselves can implement user defined preference structures that current preference models cannot handle langhans et al 2014 langhans and lienert 2016 reichert et al 2019 more specifically mavt and maut assume rational decision makers linear probability weighting and evaluation of alternatives on an absolute scale these contradict observed decision making behaviour which suggests for example presence of reference points non linear weighting of probabilities or ambiguous outcomes as well as different valuation of outcomes depending on whether these are framed as gains or losses as reflected in cumulative prospect theory stott 2006 beyond the modelling of preferences about outcomes and attached risks and uncertainties incorporation of time dependent preferences andreoni et al 2015 hermann and musshoff 2016 or social preferences chuang and schechter 2015 galizzi and navarro martinez 2019 would also be necessary to advance the mcda field both in terms of theory and practice beyond preference modelling for individual actions further developments will be needed to encompass more complicated portfolio preferences such as multi linear value functions for robust portfolio modelling liesiö 2014 and portfolio level measures of risk and regret liesiö et al 2007 vilkkumaa et al 2014 also the example we presented did assume additivity i e independence between the value of actions in a portfolio further more involved assessment models on attribute level will be needed to reflect dependencies in portfolio performance and contingencies in portfolio creation gustafsson and salo 2005 liesiö et al 2008 decisi o rama has two main limitations that we can foresee in the scope of its implementation that constraint its applicability in the view of more generic processes or approches to the mavt maut problem first decisi o rama does not support correlated or conditioned random variables in its generators as instances of the generated random variables are created on the fly however this can be circumvented by passing complete vectors to represent the samples of the random variable instead of generators second decisi o rama only handles uncertainty in the form of probability distribution functions and not in other encoding such as fuzzy sets or possibilistic distributions in the close future of decisi o rama we expect to add other features including sensitivity analysis and optimisation routines in sensitivity analysis we see utility in developing local one at a time and global sensitivity analysis for single portfolio and model assessment respectively in the optimisation routines we expect to integrate heuristic optimisation methods that support exploring the potential portfolios thus implying binary optimisation as well as integer continuous and mixed integer optimisation using linear and non linear approaches the decisi o rama library has been built keeping these possible developments in mind ensuring flexibility to all types of extension and efficiency in computation we explicitly invite the community to build it further and to explore its usability in real world applications 5 conclusions we have introduced decisi o rama an open source python library for mcda on single alternative and portfolio approaches in the context of multi attribute utility value theory we have demonstrated the capabilities of the tool to efficiently manage large multi criteria decision analysis problems that include uncertain attributes and preference parameters using conventional hardware therefore it can be used for exploratory modelling of decision alternatives with decision makers on the fly also due to its high computational efficiency it can be used to support robust portfolio modelling applications alternative optimisation algorithms and sensitivity analysis that may enrich the decision analysis process in addition decisi o rama offers a flexible implementation that supports user defined preference models in this respect it is possible to define non customary marginal value or utility functions as well as aggregation functions also it is possible that the users specify uncertain parameters of the preference model through their user defined distributions these features ensure straightforward extensibility for different types of models and users keeping in mind that accessibility and extensibility are paramount for future usability of decisi o rama it is offered to the community as open source this same principle was maintained through the development of the tool reducing the number of third party libraries for its execution and supporting the readability and maintainability of the code furthermore the tool operates with regular python objects making its integration with other applications such as different attribute assessment process models and visualisation tools with these decisions we hope that this tool will be used in different settings and for different user groups facilitating the adoption of mcda methods by researchers and practitioners alike declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was conducted as part of the cooperation programme tisca technology innovation for sewer condition assessment with project number 15343 which is partly financed by nwo domain ttw the domain applied and engineering sciences of the netherlands organisation for scientific research the rioned foundation stowa foundation for applied water research and the knowledge program urban drainage kpud furthermore the authors would like to thank julia hartmann and job van der werf for their collaboration in reviewing this document and the three anonymous reviewers whose constructive suggestions helped us in considerably improving the final version of this document 
25918,parameter calibration for computationally expensive environmental models e g hydrodynamic models is challenging because of limits on computing budget and on human time for analysis and because the optimization problem can have multiple local minima and no available derivatives we present a new general purpose parallel surrogate global optimization method parallel optimization with dynamic coordinate search using surrogates pods that reduces the number of model simulations as well as the human time needed for proper calibration of these multimodal problems without derivatives pods outperforms state of art parallel surrogate algorithms and a heuristic method parallel differential evolution p de on all eight well known test problems we further apply pods to the parameter calibration of two expensive 5 h per simulation three dimensional hydrodynamic models with the assistant of high performance computing hpc results indicate that pods outperforms the popularly used p de algorithm in speed about twice faster and accuracy with 24 parallel processors keywords lake hydrodynamics global optimization parallel optimization dynamic coordinate search surrogate model parameter calibration high performance computing software availability name of software pods description a python optimization package including 1 a standalone pods algorithm code 2 examples using pods with test function 3 examples using pods with deflt3d flow models problems developer wei xia contact address department of civil and environmental engineering national university of singapore block e1 08 23 no 1 engineering derive 2 singapore 117576 year first available 2020 hardware required pc or hpc software required python2 pysot 0 1 36 eriksson et al 2019 delft3d flow only need for delft3d flow model problems program language python program size 24 0 mb availability and cost free and open source for non commercial use availability in github https github com louisxw pods program documentation available at https louisxw github io pods 1 introduction 1 1 motivation it is our goal to introduce a new parallel global optimization algorithm that reduces the number of necessary expensive objective function evaluations for parameter calibration by using a surrogate approximation to help guide the analysis and to demonstrate its use on expensive hydrodynamic lake models calibration of model parameters in environmental simulation models is difficult especially if the simulation model is computationally expensive which means only a limited number of model simulations are possible hydrodynamic models dissanayake et al 2019 gaeta et al 2020 soulignac et al 2017 teng et al 2017 viti et al 2019 groundwater models mugunthan and shoemaker 2006 mugunthan et al 2005 song et al 2018 zhao et al 2016 distributed watershed models dembélé et al 2020 koo et al 2020 and global climate change models müller et al 2015 are some prominent examples of computationally expensive environmental simulation models that require calibration within the context of lake hydrodynamic models calibration is essential to ensure that a model can closely reproduce the actual thermal and flow behavior spatially distributed in a lake water body accurate spatiotemporal representation of hydrodynamics is in turn critical for the simulation and forecasting of water quality behavior in aquatic ecosystems chanudet et al 2012 in hydrodynamic models hydrodynamic behavior is calculated by solving a couple of partial differential equations with multiple model parameters which can vary in value in different lakes and reservoirs and hence need to be calibrated the most common method for the parameter calibration of hydrodynamic models is manual calibration whereby the modelers select sequentially multiple combinations of parameters that are evaluated via simulation examples can be found in galelli et al 2015 kaçıkoç and beyhan 2014 and råman vinnå et al 2017 this method is also referred to as the trial and error method manual calibration has numerous drawbacks as highlighted by previous studies dung et al 2011 kamali et al 2013 zou and lung 2004 zou et al 2007 for instance 1 manual calibration is inefficient in both computational time and human expert time and 2 manual calibration is unlikely to find a globally optimum calibration solution given an appropriate calibration objective moreover manual calibration becomes extremely challenging when there are many parameters to calibrate because a large number of simulation trials with different combinations of parameter values are usually needed to find suitable calibration solutions automatic calibration that incorporates optimization algorithms can significantly improve the efficiency and accuracy of parameter estimation of computationally expensive hydrodynamic models however automatic calibration methods have not been adequately explored in hydrodynamic model calibration this study tackles the challenges of calibrating expensive hydrodynamic models with a new parallel global optimization algorithm using surrogate and parallel computing techniques 1 2 literature review optimization algorithms have been applied for the calibration of hydrodynamic models in past studies for instance afshar et al 2011 used the automatic optimization approach for temperature and water budget calibration of a two dimensional hydrodynamic model ce qual w2 of the karkheh reservoir in iran fabio et al 2010 used the gauss levenberg marquardt method levenberg 1944 marquardt 1963 in the optimizer pest doherty 2004 to calibrate a two dimensional flood propagation model 4 h for a single simulation in multiple calibration stages and calibrated a maximum of 5 roughness coefficients simultaneously however gauss levenberg marquardt method in pest is a local optimization method that cannot guarantee a global minimum solution dung et al 2011 applied the non dominated sorting genetic algorithm ii nsga ii in parallel to calibrate 5 manning coefficients of a one dimensional mike11 flood model for mekong delta river which takes 10 h for a single simulation parallelization was essential in their study since the optimization method used 1560 expensive simulations evaluations takes 12 5 days using 13 processors to adequately calibrate the model the genetic algorithm had also been used in an earlier study for calibration of a pipe flow model babović and wu 1994 which can be computed relatively inexpensively and the method was not applied to open water there are also a few studies that tested the automated parameter methods in the calibration of hydrodynamic models developed in the popular delft3d flow deltares 2014 software some delft3d flow deltares 2014 hydrodynamic models that have used optimization for automatic calibration include coastal tide models garcia et al 2015 kurniawan et al 2011 and coastal morphological models briere et al 2011 for calibration of these hydrodynamic models a serial semi automated parameter estimation method from the openda library verlaan et al 2010 called does not use derivatives dud ralston and jennrich 1978 was used briere et al 2011 explained in their study that dud is a local minimization method and the probability is high that dud identifies a local minimum rather than a global minimum when multiple nonlinear terms are interacting with each other as is the case for lake hydrodynamic models it is highly likely that a calibration optimization problem has multiple local minima gorelick and zheng 2015 xia and shoemaker 2020 solomatine et al 1999 also highlighted that the error function of a numerical model is normally multimodal hence a global optimization method should be used as described above the delft3d flow model applications that have been calibrated in the past using the optimization method dud are relatively computationally cheap with a computational time that does not exceed 45 min for one simulation specifically the runtime of the delft3d model in optimization is about 30 min in garcia et al 2015 around 45 min in kurniawan et al 2011 and 20 min in briere et al 2011 however most hydrodynamic models especially three dimensional lake models in a real application can easily take much longer many hours or even days to run thus an efficient global optimization method should be applied for calibration of such models for instance if a single lake hydrodynamic model run takes 4 h per evaluation and optimization takes 2000 serial evaluations to find an acceptable answer the calibration process takes almost nine months which is not feasible efficient optimization techniques that require only a few simulation evaluations are thus desired many lakes around the world are now being modeled using computationally expensive 3d hydrodynamic models castelletti et al 2010b chanudet et al 2012 elhakeem et al 2015 galelli et al 2015 hui et al 2018 kaçıkoç and beyhan 2014 soulignac et al 2017 wahl and peeters 2014 however there is no published research to the best of our knowledge that uses efficient global optimization methods for expensive three dimensional lake hydrodynamic model calibration iterative surrogate response surface algorithms can significantly reduce the computational burden associated with automatic calibration of lake hydrodynamic models and have been applied to expensive optimization problems in the past castelletti et al 2010a 2010b razavi et al 2012b regis and shoemaker 2007b zou et al 2009 in the study of castelletti et al 2010b they proposed an iterative learning and planning procedure based on surrogate approximation and successfully applied on a water quality planning problem of a costly 3d lake models 7 days per simulation in their problem there are three decision variables and three iterations are conducted to obtain a satisfying solution which would require about 5 5 years of computation with a modern compute if by exhaustive scenario analysis 286 feasible decisions the basic premise in iterative surrogate algorithms is that the expensive objective function which in calibration problems requires an expensive simulation is replaced by an inexpensive statistical approximation function surrogate based on prior function evaluations and that surrogate assists in selecting which value of the parameter vector will be used to do the next expensive function evaluation the algorithms in regis and shoemaker 2007b and regis and shoemaker 2013 use a weighted average of estimated function value from surrogate and distance to previously evaluated points of randomly sampled candidate points to select the next evaluation point which is not necessarily at the minimum of the surrogate some surrogate methods use a heuristic e g a genetic algorithm or complete enumeration to find the minimum or pareto front of the surrogate and do a function evaluation there at the estimated minimum point of the surrogate castelletti et al 2010b fen et al 2008 zou et al 2009 müller and shoemaker 2014 compare these two methods minimizing with a heuristic optimization algorithm versus the selection of the best weighted average of candidate points and find that neither method dominates the other when tested across a range of surrogate types and surrogate optimization strategies widely used approximation functions include polynomial regression johnson and rogers 2000 schultz et al 2006 yin and tsai 2018 kriging bau and mayer 2006 di pierro et al 2009 hemker et al 2008 razavi et al 2012a zhao et al 2016 radial basis function rbf castelletti et al 2010b christelis et al 2018 razavi et al 2012a regis and shoemaker 2004 2007b 2009 tsoukalas et al 2016 support vector machine svm fan et al 2020 zhang et al 2009 and artificial neural network ann broad et al 2005 saadatpour 2020 song et al 2018 zou et al 2007 2009 detailed information of these approximations could be found in bhosekar and ierapetritou 2018 forrester et al 2008 and solomatine and ostfeld 2008 müller and shoemaker 2014 compared the effect of different types of surrogates including rbf kriging polynomial and multivariate adaptive regression spline mars as single or as ensemble surrogates and find that the most efficient ensemble surrogate is the combination of rbf with the polynomial dual surrogate and the most efficient single surrogate is rbf the advantages of the combination of rbf with polynomial were so small over the single rbf surrogate in müller and shoemaker 2014 that in the pods algorithm we use a single rbf surrogate in iterative surrogate algorithms the surrogate function is embedded in an iterative framework i e new trial points which are values of the decision vector are evaluated on the cheap surrogate function to determine which specific value will be evaluated by the expensive objective evaluations after the expensive evaluation is computed then surrogate functions are simultaneously improved castelletti et al 2010b razavi et al 2012b regis and shoemaker 2005 2007b multiple such algorithms have been proposed and applied to expensive water resources optimization problems in prior literature razavi et al 2012b moreover most surrogate optimization methods applied in prior literature are global since many water resources optimization problems are global application areas of these global surrogate methods include 1 groundwater bioremediation where rbf or kriging is used as surrogates bau and mayer 2006 mugunthan et al 2005 razavi et al 2012a regis and shoemaker 2009 2 water distribution systems where ann or kriging are utilized behzadian et al 2009 di pierro et al 2009 and 3 watershed problems where rbf are employed razavi et al 2012a shoemaker et al 2007 however the application of surrogate response surface algorithms on the calibration of open water hydrodynamic models is under explored in addition to using surrogates the efficiency of hydrodynamic calibration can be significantly improved via parallelization techniques with the assistance of high performance computing hpc parallel optimization methods with deployment on hpc systems have been successfully applied in the past to reduce wall clock time of automatic calibration of expensive water resources models regis and shoemaker 2007a 2009 xia and shoemaker 2020 tang et al 2007 vrugt et al 2006 zhang et al 2013 2016 most of these parallel application studies focus on watershed models however pang 2017 applied parallel global optimization to groundwater flow and transport models such parallel optimization methods can be even more effective in auto calibration of the extremely computational expensive hydrodynamic models models that take hours even days to run for a single simulation numerous efforts have also been made in prior literature to parallelize iterative global surrogate algorithms a comprehensive review of such methods is given in haftka et al 2016 moreover these methods predominantly include algorithms that use kriging models as surrogates sóbester et al 2004 wang et al 2016 or rbf as surrogates krityakierne et al 2016 regis and shoemaker 2007a 2009 kriging based parallel surrogate methods are prevalent in computationally expensive global optimization applications and especially machine learning applications however some recent studies show that rbf based methods are also very effective for computationally expensive parallel optimization applications and even more so for problems with many decision variables ilievski et al 2017 1 3 contributions this study develops a new parallel surrogate global optimization algorithm parallel optimization with dynamic coordinate search using surrogate pods pods is a parallel version of the serial algorithm dycors proposed by regis and shoemaker 2013 the key to parallelize a serial algorithm is how to generate p effective evaluation points based on the only information available so that these p processors are valuable which however is non trivial pods differs from previous algorithms because of one or more of the following things 1 use dynamical coordinate search to choose which decision variables to perturb 2 use a truncated normal distribution to generate candidate point perturbation and 3 use a surrogate distance metric to select multiple candidate points as evaluation points we investigate the efficiency of pods on eight well known test problems by comparing it with previous methods the effectiveness of the surrogate distance metric for selecting multiple points is examined by comparing pods with pods l that is a particular instance of pods without using the surrogate distance metric pods is further tested on two lake hydrodynamic model calibration problems note that this is the first time that surrogate global optimization algorithms have been applied to three dimensional lake hydrodynamic model calibration the first calibration problem is on a hydrodynamic model for a tropical reservoir in singapore based on actual observation data at multiple depths of one station referred to as calibration scenario 1 below another calibration problem referred to as calibration scenario 2 below is on the same real world model in calibration scenario 1 but with synthetic observation data at multiple stations generated from the model simulation with a known parameter solution hence for the synthetic calibration problem in calibration scenario 2 the true solution of the optimization problem is known with the synthetic calibration problem we could adequately assess the performance of optimization algorithms by comparing the solution from optimization to the known set of true solution the synthetic case also enables us to assess the algorithm performance when observations at multiple stations instead of one station as in calibration scenario 1 are used for calibration in addition we can compare the solution quality from optimization at these locations where there is no observation used for model calibration for spatial validation we think such an investigation would be interesting and essential because one important function of modeling is to provide spatial and temporal descriptions of the system with the observations at limited locations and time hence we want solutions from optimization not only fitting good at these locations and time steps with observations but also good at other sites note that such an analysis is not possible in calibration scenario 1 2 pods 2 1 general framework pods is a parallel implementation of the serial dycors algorithm regis and shoemaker 2013 that was designed for global with multiple local minima black box optimizations problems of the form 1 min x θ f x where θ l b x u b ℝ d l b u b ℝ d are the lower and upper bound of variable vector x d is the number of variables in x f x is a computationally expensive black box function that is not differentiable pods algorithm as shown in fig 1 keeps the master worker framework firstly proposed by regis and shoemaker 2007a and later utilized by regis and shoemaker 2009 for the parallelization implementation of the lmsrbf algorithm regis and shoemaker 2007b assume that p processors are available pods selects p distinct points for simultaneous objective function evaluation in parallel in each iteration pods starts the optimization process with a set of n 0 initial points generated via space filling experimental design e g latin hypercube experiment in the iterative loop the surrogate model s n x of the original expensive function f x is built or updated with the set of previously evaluated points denoted as a n x 1 x n where n is the number of points evaluated on black box function f x in pods the cubic rbf augmented by a linear polynomial is used for the surrogate model s n x as serial dycors does then n c a n d a user defined hyperparameter candidate points denoted as ω n ω n y n 1 y n n c a n d are generated around the best solution found so far x b e s t with the dynamical coordinate search strategy the dynamical coordinate search dynamically changes the proportion of coordinates i e decision variables of x b e s t that are allowed to be perturbed which was proven to be effective in practice regis and shoemaker 2013 tolson and shoemaker 2007 the details of the dynamical coordinate search will be discussed in detail in section 2 2 given p processors p out of n c a n d candidate points in ω n will be selected as next iteration evaluation points denoted as ℱ n x n 1 x n p which will be sent to the p workers for the evaluation of f x in eq 1 pods uses the surrogate distance metric used in parallel srbf regis and shoemaker 2009 for the selection of these p evaluation points the surrogate distance metric considers 1 the estimated objective function value from the surrogate model s n x as well as 2 the uncertainty of the surrogate model which is measured by the distance based metric the minimum distance from previously evaluated and selected evaluation points the distance based metric is effective for selecting p multiple evaluation points for parallel evaluation to demonstrate the effectiveness of distance based metric we developed a special version of pods which only consider the surrogate value of the candidate points as criteria for selection of evaluation points we expect that not using the distance based metric would make the algorithm search more locally because the uncertainty of the surrogate model is not considered hence we call this special version of pods as pods l we discuss surrogate distance metric further in detail in section 2 3 the iterative loop repeats until the termination condition is met which in this case is the maximum number of evaluation n max the pseudocode of the main framework of pods is in supplementary material s1 2 2 dynamic coordinate search the generation of candidate points with dynamic coordinate search is where pods is mainly different from parallel srbf regis and shoemaker 2009 similar to parallel srbf pods generates the candidate points based on the best solution found so far x b e s t however instead of allowing all the coordinates of x b e s t to be perturbed in all iterations as parallel srbf does pods allows only a subset of coordinates to be perturbed the number of coordinates that are allowed to change is controlled by a probabilistic variable p s e l e c t whose value is decreasing with the value of n increases note n is the number of function evaluations on f x and n 0 n n max 1 the value of p s e l e c t is calculated by a strictly decreasing function ϕ n ϕ 0 1 ln n n 0 1 ln n max n 0 where ϕ 0 min 20 d 1 the ϕ n function is the same as the function used in serial dycors for cases that no coordinate being selected for perturbation we randomly assign one coordinate out of the d variables to be perturbed the dynamical reduction of the number of coordinates to be perturbed is a strategy to adjust from global search to local search as the number of evaluations approaches the computational budget the dynamic coordinate search idea was motivated from the manual calibration of model parameters where in early calibration stage the solutions are relatively poor hence perturbing all or majority of coordinates is helpful for search but as the solutions improve it is necessary to perturb only a few coordinates so that the improvement gained is not lost tolson and shoemaker 2007 while if all coordinates of the best solution found in later search stages are allowed to change as in parallel srbf the value of the best solution s well calibrated coordinates parameters has a high chance of being disturbed practices also proved that dynamic coordinate search strategy is very efficient for calibration problems especially for these problems that have many decision variables regis and shoemaker 2013 tolson and shoemaker 2007 hence the dynamic coordinate search is expected to be helpful for pods on the calibration of hydrodynamic and other environmental models in pods for these coordinates selected to be perturbed a random perturbation is generated from a truncated normal distribution with mean 0 standard deviation σ n which is different from the serial dycors paper regis and shoemaker 2013 where a successive reflection method is used to ensure the candidate points are within the solution domain the pseudocode of dynamic coordinate search is in supplementary s2 note that the value of the search radius σ n affects the shape of the truncated normal distribution a larger value of σ n means that the candidate points have a higher possibility of being far from the best solution found so far x b e s t vice versa the update of σ n s value is based on the learning of the consecutive failed or successful iterations the implementation of the adaptive learning for σ n adjustment is the same as parallel srbf and is described in supplementary material s3 we allow restart function in pods when the value of σ n becoming too small or the algorithm does not improve the best solution for many iterations details are also included in supplementary material s3 note that for problems that only a relatively limited maximum number of function evaluations are allowed as problems in this study a restart might not occur however for other problems where there is more evaluation budget restart function could be helpful 2 3 surrogate distance metric for multiple evaluation points selection the key to the parallelization of a serial algorithm e g dycors is how to generate multiple evaluation points for expensive objective function evaluation in one iteration we adopt the surrogate distances metric idea from parallel srbf that the p evaluation points given p processors are selected sequentially from a larger number of candidate points ω n based on two criteria 1 the estimated function value from the surrogate model s n x and 2 minimum distance from previously evaluated points and previously selected points within that iteration a candidate point y n k where k 1 n c a n d in ω n is considered a good candidate point when it has a low value of s n x y n k for exploitation and far away from these points evaluated for exploration a weighted score is used for candidate point selection to balance these two criteria let w n r be the weight on surrogate criteria when selecting the n 1 evaluation point where 0 w n r 1 the weight on distance criterion w n d is then equal to 1 w n r the value of w n r can be used to balance global search when the value of w n r is small and local search when the value of w n r is large we adopt a weight cycling strategy that was also used in serial dyocrs and parallel srbf where the value of w n r cyclically changes from small to large values through a user defined parameter weight pattern ϒ v 1 v κ where 0 v 1 v κ 1 hence the weight on the surrogate criterion for each evaluation point is eventually selected sequentially from the list v 1 v κ v 1 v κ v 1 v κ and global search and local search are conducted in turn in pods in such a way the exploration and exploitation are balanced during the whole search process which can effectively avoid pods getting sucked in local minima and also prevent pods spending too many evaluation budgets on exploration more details of the evaluation point selection are in supplementary material s4 2 4 pods l to investigate the effectiveness of the surrogate distance metric on the performance of pods we developed another parallel version of dycors which blocks the use of surrogate distance metric in pods for the selection of p evaluation points we use only the approximated objective function value from the surrogate model s n x as the criterion for selecting candidate points in ω n as evaluation points hence the best p candidate points with the smallest surrogate value s n x are selected as evaluation points for the next iteration we expect only using the surrogate value information for evaluation point selection would make the algorithm search more locally because the uncertainty of the surrogate model is not considered hence we call this version of parallel dycors as pods l note that pods l is a particular case of pods when ϒ is set to be 1 0 3 hydrodynamic lake model temperature calibration pods is applied to two hydrodynamic lake model calibration problems i e calibration scenario 1 and 2 the calibration problem in calibration scenario 1 is calibrating the model parameter vector x of a three dimensional hydrodynamic model based on the real observed temperature data at one station with multiple depths of a tropical reservoir the real value of x is unknown the goal is to find a set of x with which the temperature simulation output from the hydrodynamic model is as close as possible to the real observed temperature data government agency employees and consultants did a previous calibration of the model in calibration scenario 1 and we call their method the manual calibration since no optimization methods were used calibration scenario 1 only uses observation data at one station of the reservoir in order to investigate the algorithm performance in cases where there are observation data at multiple stations we created the calibration scenario 2 the calibration problem in calibration scenario 2 is the parameter estimation on the same hydrodynamic model in calibration scenario 1 but with a synthetic set of observation data which is generated from the model simulation in calibration scenario 1 given a known parameter set denoted as x r we use the solution from manual calibration as x r since manual calibration is a very high quality solution that was achieved by the expert in months who are familiar with the studied system we save the simulation output at all locations all grids and during the whole simulation period the time series temperature data a at multiple stations and in multiple depths are used as observation data for model calibration the goal of the calibration is to find x r with which the temperature simulation output from the hydrodynamic model is a 3 1 site description and data the horizontal boundary of the studied reservoir in both cases is given in fig 2 this reservoir has over 250 ha of water surface and uneven depth for calibration scenario 1 one online water quality profiler station stn a1 was installed in the middle of the reservoir the water temperature data at the station are available at various depths the temperature data is for one year the year 2013 with a temporal resolution of around 20 min with outliers removed there are a few temperature observations that are unreasonable above 45 c due to the temporary failure of the sensor we replace these outliers with the average value of the nearest regular reading before and after the outliers since the model output is hourly for error calculation the model output for temperature for each hour is compared to the measured temperature of the same hourly time point if there is no measurement data at the hourly time point the temperature used for error calculation is a linear interpolation of the two temperature measurements just before and after the hour in calibration scenario 2 four additional sampling stations across the study region are added stn b1 4 in fig 2 the temperature time series data at five stations and in multiple depths are recorded and saved from the model simulation output with a given parameter vector x r the saved temperature time series data are used as synthetic observation data for model calibration in calibration scenario 2 3 2 hydrodynamic model and calibration parameters delft3d flow deltares 2014 was used to build the hydrodynamic model for the reservoir delft3d flow is a three dimensional hydrodynamic simulation solver that calculates non steady flow and transport phenomena based on the solution of the navier stokes equations the system of equations consists of the continuity equation equations of motion and the transport equation of heat these equations are partial differential equations and are solved by a finite difference method detailed information about the open source delft3d flow is provided in the user manual deltares 2014 the delft3d flow hydrodynamic model used in calibration scenario 1 and calibration scenario 2 was set up by the water utilities employees and consultants including the domain construction input data preparation such as meteorological data inflow and outflow data precipitation and evaporation data and model configuration the grid coordinate system uses cartesian coordinates z grid the number of grid points in the x direction is 65 the number of grid points in the y direction is 67 and the number of layers in vertical is 19 the length of the simulation in time is one year and the computational time step applied is 1 5 min resulting in 350 400 time steps a single 1 year simulation for the reservoir hydrodynamic model takes about 5 h to run in serial on a windows desktop with cpu intel core i7 4790 in this study nine parameters shown in table 1 with their calibration range that affect directly or indirectly the thermal activity in the water body are included in the calibration process for both calibration scenario 1 and 2 the calibration ranges for these parameters given in table 1 are suggested by singapore water utilities employees and consultants based on previous studies the detailed description of these physical processes in the model and the parameters are described in supplementary material s6 3 3 optimization problem formulation the objective of the optimization in both calibration scenario 1 and calibration scenario 2 is to minimize the error between the simulated water temperature and the measured water temperature nash sutcliffe efficiency nse introduced by nash and sutcliffe 1970 is used to quantify the goodness of fit between the observed and simulated water temperatures at different locations the objective function is defined in eqs 2 and 3 below 2 f x i 1 m j 1 n i n s e i j x 3 n s e i j x 1 t 1 k s i m t i j t e m x o b s t i j t e m 2 t 1 k s i m t i j t e m x μ o b s i j t e m 2 where s i m t i j t e m x and o b s t i j t e m denote the simulated water temperature given a parameter vector x and observed water temperature respectively at the time step t and depth h j of station i μ o b s i j is the mean of o b s t i j t e m over all k time steps k is the number of time steps corresponding to the observed water temperature m is the total number of monitor stations in calibration scenario 1 m 1 one station stn a1 in calibration scenario 2 m 5 5 stations stn a1 and stn b1 4 n i is the number of different depths where the water temperature data is observed at station i i 1 m in calibration scenario 1 n 1 3 3 different depths at stn a1 in calibration scenario 2 n i 3 for i 1 and 2 3 different depths at stn a1 and stn b1 and n i 2 for i 3 4 5 2 different depths at stn b2 4 because they are shallower than stn a1 and stn b1 4 computational experiments 4 1 alternative algorithms to assess the performance of pods we first compared pods with earlier parallel surrogate based optimization methods including sop krityakierne et al 2016 which is a parallel rbf based method and moe qei wang et al 2016 which is a parallel kriging based gaussian process based method sop algorithm was compared with other previous rbf based algorithms including parallel srbf regis and shoemaker 2009 and an evolutionary algorithm that uses rbf regis and shoemaker 2004 their result showed that sop is more efficient than these algorithms hence we did not compare pods with them besides the surrogate algorithms we also compared pods with a popular population based stochastic algorithm parallel differential evolution p de storn and price 1997 tasoulis et al 2004 which had been used for water resource optimization problems tashkova et al 2012 zheng et al 2015 the description of these alternative algorithms is provided in supplementary material s7 4 2 test functions before we tested the pods algorithm on the computationally expensive hydrodynamic model calibration problems we compared the performance of pods with previous surrogate algorithms first on inexpensive test problems since they are inexpensive we can test the algorithm performance on a range of problems we used eight 10 dimensional black box test functions summarized in table s2 in supplementary material s8 these test problems are the well known rastrigin michalewicz ackley levy schewefel and styblinskitang whitley and weierstrass functions which have a large number of local minima detail information of these functions is given by surjanovic and bingham 2013 and jamil and yang 2013 4 3 experimental setup all computational experiments in this study were implemented on a single node on the national supercomputer center nscc of singapore which is a linux based platform with dual intel xeon e5 2690 v3 processors the code coupling pods with the delft3d flow model suite is available in the software pods https github com louisxw pods other users of this code can follow the instructions available in the pods software to apply pods to their models in this study five algorithms i e pods pods l section 2 4 and three previous methods were compared on 8 test problems since they are not computationally expensive to evaluate however for the hydrodynamic calibration problems we did not compare all the five algorithms on the hydrodynamic calibration problems since they are too computationally expensive to compute i e a single model simulation takes around 5 h we then selected the surrogate algorithm with the best performance on the test problems and compared it with the popular heuristic algorithm p de that does not use surrogates for the comparison on the test functions the number of processors used was set to 24 and the maximum number of evaluations was set to 504 including 24 evaluations in initial experiment design so there are 21 iterations each of which evaluates the objective function for 24 different parameter vectors each experiment one algorithm solves a problem once was repeated with 30 trials for all algorithms on all test problems for hydrodynamic calibration problems the number of the processor was set to be the same as that on test problems with p 24 the maximum number of evaluations was set to be 192 hence there are eight parallel iterations with 24 hydrodynamic simulations per iteration given that one evaluation takes about 5 h to compute eight iterations take less than two days 40 h of wall clock time for the hydrodynamic calibration problems analyzed in this study total computation time per trial is 40 24 960 core hours at a cost around s 25 8 given price at s 0 0269 core hour on nscc this is a reasonable time for automatic calibration since it is much less than the time typically taken in the manual calibration of the real lake which may be in the order of weeks or even a month of human time for experts engineers very experienced with this lake and model who previously manually calibrated the hydrodynamic model for the current data set since the hydrodynamic models being calibrated in this study are extremely expensive we performed five trials for each optimization experiment furthermore in order to remove any initial sampling bias all algorithms including p de were initialized with the same set of evaluation points in the initial experimental design for the respective trial for all the numerical experiments in this study moreover there are few hyperparameters in pods and we used the same algorithm hyperparameter values for all test problems these values are given in table s1 of supplementary material s5 for all the numerical experiments in this study we used the values from previous studies regis and shoemaker 2009 2013 and the robust performance of these values was verified in previous studies by many theoretical test problems and real world applications in general we do not encourage using lots of expensive function evaluation to set hyperparmeter valued since it is not clear that the extra function evaluations done to improve hyperparameters for specific problems would generally be more effective than just using the standard hyperparmeter values in table s1 and using the extra function evaluations to do more optimization iterations 5 results and discussion 5 1 algorithm performance on test problems 5 1 1 pods and pods l vs alternative algorithms data profiles fig 3 shows the data profiles moré and wild 2009 of all the five algorithms on the eight optimization test problems each with 30 trials data profile plots provide a concise summary of comparative algorithm performance of stochastic optimization algorithms on multiple test problems we adopt the method from moré and wild 2009 to generate these data profile plots that consider the results of all trials on all problems to compare the overall performance of each algorithm the detailed explanation of the calculations in data profiles is given in supplementary material s9 an intuitive interpretation of a data profile is that for a given algorithm s i e one of the five algorithms a data profile plots the percentage of problems solved within n function evaluations in the data profiles of fig 3 a problem is deemed solved if the algorithm s finds a solution that is smaller than f l τ f x 0 f l where f x 0 is the function value of starting point for each problem i e the best solution in the initial experimental design of a trial and f l is the best solution found among five algorithms for each problem in the end the whole optimization process consequently in data profiles high values of the percentage of problems solved indicate the good algorithms it is clear from fig 3 that the new algorithm pods and pods l outperform all three other algorithms pods and pod l solved within the tolerance level more than 80 and 60 problems respectively compared to less than 40 for other methods after the equivalent of about 30 simplex gradient estimates it is surprising that moe qei is even worse than p de which does not use the surrogate to further illustrate the results in fig 3 we look at the performance of the different algorithms on two specific test functions i e rastrigin and michalewicz rastrigin is a complex multimodal problem with a large number of local optima which can be used to test if an algorithm can avoid locally optimum solutions for the michalewicz problem the global minima are located in a very small sub region of domain space which is difficult to locate hence it is a difficult optimization problem many calibration problems depict a similar behavior in the search domain fig 4 shows in detail the average calibration progress plots of all the five algorithms in solving the rastrigin and michalewicz problems the error bars in fig 4 represent 95 t confidence intervals for the mean of the best solution found so far in objective function values since both test problems are minimization problems the lowest optimization progress curves for pods and pods l denote faster convergence and find better solutions than all other algorithms at the end of 504 evaluations moe qei is the worst of all the five algorithms on both rastrigin and michalewicz problems and is even worse than the non surrogate based p de on the rastrigin problem a plausible reason for moe qei s performance is that it uses a gaussian process based optimization approach that is not designed for problems with many decision variables it is common in machine learning papers to mention that gaussian process optimization does not do well in higher dimensions e g ten and above ilievski et al 2017 show that an rbf algorithm does significantly outperform the gaussian process model on problems with dimensions 15 and 19 as mentioned in section 4 1 sop and moe qei were included in this analysis as benchmark parallel surrogate algorithms pods outperforms e g converges faster and finds a better solution at completion than these methods as depicted by data profiles in fig 3 pods is an extension of a serial code dycors regis and shoemaker 2013 that is designed to be effective on high dimensional problems by reducing the number of dimensions that are perturbed from the previous best solution for any random candidate point dycors has been shown to work well on problems of up to 200 dimensions regis and shoemaker 2013 pods better performance relative to sop is essentially a consequence of the difference in both algorithms mechanisms for generating candidate points generation of candidates in sop is based on a trade off between exploration and exploitation krityakierne et al 2016 that permits points with relatively poor surrogate values to be selected because they are far away from previously evaluated points by contrast pods generates candidates only around the best solution found so far thus the above results indicate that the design of parallel surrogate algorithms may benefit more from not evaluating too many points with bad function value just because they are in unexplored areas we also provide the performance profile dolan and moré 2002 for the five algorithms in the supplementary material s9 performance profile plots defined in s9 also show that pods and pods l outperform other algorithms on a limited evaluation budget 5 1 2 pods vs pods l pods and pods l are the fastest two algorithms as shown in fig 3 the data profiles of fig 3 show that pods l solves a similar proportion of problems as pods before the equivalent of about 200 function evaluations however pods outperforms pods l after about 200 function evaluations for instance after 450 evaluations pods is successful in solving 30 more problems than pods l calibration progress plots see fig 4 also show that pods is more efficient than pods l especially after about 250 evaluations the difference between pods and pods l becomes significantly large note that the main difference between pods and pods l is that pods uses the surrogate distance metric for evaluation point selection while pods l does not use the surrogate distance metric consequently pods l does not explore the search domain adequately which weakens the global search ability and may force it to converge to a locally optimum solution this probably explains why pods l shows similar performance to pods in the initial optimization iterations i e before 200 evaluations but is outperformed by pods which has stronger global search ability in the latter optimization iterations hence surrogate distance metric helps to improve the performance of pods 5 2 performance of pods on hydrodynamic model calibration calibration scenario 1 pods which is the best surrogate global algorithm on test problems as shown in fig 3 was compared with the non surrogate algorithm p de on the hydrodynamic model calibration in calibration scenario 1 and 2 this section discusses the comparison of two algorithms on the problem in calibration scenario 1 in terms of i calibration efficiency i e the speed with which the objective function improves and ii accuracy of the final solution obtained within a fixed computing budget fig 5 shows the calibration progress plot of both algorithms for calibration scenario 1 the calibration progress plot is the plot of the average among five trials of the best solution found so far in terms of the objective function value f x in eq 2 as a function of the number of expensive function evaluations since both algorithms use the same initial experiment design sets for corresponding trials the calibration progress plot only shows the result obtained after the initial experiment design after 24 evaluations the objective function value of the manual solution obtained via a manual tweaking of parameters by expert modelers is also plotted black dash line as a reference pods is twice as fast as p de in terms of speed of convergence on a limited budget of 192 evaluations this is noted in fig 5 with the dashed red line that indicates it takes only around 96 evaluations for pods to achieve the average best solution obtained by p de i e 1 7992 after 192 evaluations which is over a two fold increase in speed hence for this complex problem pods is twice as fast as p de from fig 5 we can find that pods takes only around 120 evaluations on average averaged over five iterations to achieve the manual solution 1 8449 in this case one iteration takes 5 h to run so it takes around 25 h for pods to get the manual solution which as discussed earlier takes experienced experts weeks to achieve note that the pods method did not include the known manual solution as an initial value so the use of pods is a labor saving approach in comparison to manual calibration also note that p de does not do well in this case since it does not even find the manual solution according to average error over five trials within 192 evaluations so this indicated that even finding a solution as good as the manual solution is challenging the final calibration solutions obtained by optimization with both pods and p de are compared in terms of graphical time series visualization in this sub section since multiple optimization trials were performed for each algorithm we choose the final calibration i e the best calibration as per objective function after 192 evaluations of the median trial of each algorithm for graphical time series comparison as discussed above the solution found by pods is better than the manual solution in terms of the objective function value moreover both the solutions obtained by pods and by manual calibration are better than the final median trial solution obtained by p de 1 7944 this supports the quality of the solution obtained by pods fig 6 gives the water temperature simulation output of the final calibrations from both algorithms since the temperature simulation outputs corresponding to solutions from both algorithms i e pods and p de show significant difference at the surface layer both in terms of error metrics or graphic visualization simulated temperatures at the middle and bottom layers are very similar only the time series of temperature error δ t i e the difference between the measured temperature and simulated temperature at the surface layer are shown in fig 6 the upper panel a is the time series temperature error plot over one year and the lower panel b is the scaled up time series temperature error plot over a selected period the second half of aug the time series plot of the water temperature from simulation output and real observation is provided in fig s6 in supplementary material s10 however since the temperature error plot gives a more intuitive comparison between the solution quality between pods and p de we focus the discussion on the temperature error plot fig 6 it is evident from fig 6 that the solution obtained by pods is very close to the manual solution and is much better than the solution from p de at the surface layer pods solution as well as the manual solution overall have significantly lower temperature error time series in comparison to p de s solution this outcome is consistent with the progress plot comparison of fig 5 which is based on the calibration objective function as a performance metric the nse values at the surface layer of median solutions of pods and p de are 0 6175 and 0 4656 respectively so pods solution is about 32 6 better than the p de s solution higher nse value is a better solution in terms of rmse value pods solution is better than p de s solution at the surface layer 0 3570 vs 0 4220 c pods uses parallel computing which saves wall clock time than manual calibration however parallel computing is not the only thing that makes pods efficient to make full use of p processors the high quality of the p evaluation points is critical pods compared with p de has used a fast to run surrogate as a replacement of the original expensive model to guide the search pods also has a very effective method to generate p high quality evaluation points firstly pods uses the dynamic coordinate search idea in section 2 2 that the number of coordinates allowed to be perturbed is dynamically reducing as the calibration process progresses this strategy was used in serial dycors regis and shoemaker 2013 where it was shown to be very effective with higher dimensional problems in comparison to other algorithms secondly pods uses an effective way to select p evaluation points by considering the prediction value of the surrogate for exploitation as well as the distance of evaluation points from evaluated points for exploration this way of selecting multiple evaluation points is proven to be effective in previous studies for many real world optimization problems e g calibration of groundwater bioremediation remediation models regis and shoemaker 2009 5 3 performance of pods on hydrodynamic model calibration calibration scenario 2 in this section we use a new test function to compare pods and p de on the hydrodynamic model calibration problem in calibration scenario 2 calibration scenario 2 used the same model same model configuration and input data as used in calibration scenario 1 however the calibration problem in calibration scenario 2 is different from the problem in calibration scenario 1 by 1 the true solution x r of the calibration problem in calibration scenario 2 is known and 2 there are five monitoring stations spread out in the study area as shown in fig 2 instead of one station as in calibration scenario 1 the known solution x r in calibration scenario 2 helps to assess the performance of the algorithm by comparing the true solution and also enables us to assess the algorithm performance when more observation data in spatial for calibration is available in addition calibration scenario 2 enables the performance evaluation in table 2 below at all the locations where the monitoring data is not used in model calibration for spatial validation of calibration the calibration progress plot of both algorithms on the problem in calibration scenario 2 is shown in fig 7 similar to the result in calibration scenario 1 pods converges faster and find a better solution than p de at the end of calibration the final solution obtained by p de with 192 evaluations takes pods around 92 evaluations 48 less we also compared the solution quality obtained by pods and p de the median solution over five trials of the error over five sampling stations at all sampling times obtained by each algorithm is shown in table 2 the median solution obtained by pods 11 9122 is better than that by p de 11 7899 in terms of objective function values top row table 2 we also compared both solutions by calculating the temperature error between optimization solution and true solution at all locations grids and all time steps as shown in table 2 row 2 note these quantities were not used for calibration so they indicate how significant are the differences in the calibration solutions obtained by the two algorithms the sum of absolute temperature error at all locations and time steps of the solution obtained by pods is nearly 50 less than that of the solution found by p de indicating much better spatial and temporal accuracy of temperature associated with the pods calibration result for the magnitude of maximum and minimum temperature error at all locations and time steps pods solution is also much smaller than p de s solution note that we calculated the temperature error at all locations grids for the water body of the reservoir not only at the five stations where synthetic observation data is used for calibration in the objective function f x in eq 2 this shows that solution found by pods is not only better than p de s solution at locations considered in objective function but also at locations where no observation data was used for calibration this analysis is an attempt at spatial validation of the optimization solution obtained by pods the time depth contour plots for the temperature error between the solution obtained by pods and p de and the true solution at two stations stn a1 and stn b1 are shown in fig 8 and fig 9 respectively note figs 8 and 9 not only show the temperature error at the location where the data were used for model parameter calibration but also show the temperature error at the places where the observation is not included in the objective function f x in eq 2 from figs 8 and 9 we can evaluate if the solution from optimization fits well at locations that observation is not used for model calibration while in calibration scenario 1 no such analysis could be done this is one of the advantages of assuming a true solution in calibration scenario 2 it is clear from figs 8 and 9 that the solution obtained by pods is better than the solution obtained by p de the temperature error of the solution by pods is relatively smaller than that by p de at the majority of depths and time steps this conclusion is consistent for both stations stn a1 and stn b1 especially in stn b1 for most of the depths and time steps the magnitude of the temperature error of pods solution is smaller than 0 05 c in contrast for p de s solution there are lots of depths and time steps where the magnitude of error is larger than 0 20 c figs 8 and 9 show that pods solution is not only better than p de s solution at locations where the observed temperature data were used for calibration included objective functions but also better at these locations where the temperature data was not included in the calibration this demonstrates that the automatic calibration methodology with the use of pods is a reliable method that is validated spatially in figs 8 and 9 for model parameter calibration in real world cases where the observation data are usually available at only a few locations compared with the large study region 5 4 extension to other applications pods effectiveness is demonstrated based on a variety of optimization problems both theoretical test problems and real word applications in different calibration scenarios in this paper we applied this method to hydrodynamic calibration problems however it is important to note that pods is a general purpose method and can be used for many other environmental and water resource optimization problems especially problems involving computationally expensive models the derivative of the objective function is usually not available for these simulation model based problems and there are often multiple local minimums one application area of pods is to solve inverse problems for example a model parameter calibration problems as in this study b pollutant source identification problems where simulation models are used as tools to simulate the response model output of different inputs i e pollutant source locations or pollutant amount and optimization methods are used to search the inputs that give best model output compared with observations another application area of pods is to address reservoir management problems where expensive physical models are used to do scenario analysis given different management policies pods can also be applied to environmental design problems e g groundwater pump well design pods is a competitive method that can find a suitable solution within a limited number of evaluations which is very important for these problems that involve expensive objective functions computed by model simulations 6 conclusions we presented a very efficient general purpose parallel surrogate optimization algorithm pods available in open source software for calibration of models which are typically very computationally expensive moreover we demonstrated its performance on lake hydrodynamic model calibration problems as well as on test problems pods is an expansion of serial dycors and incorporated the surrogate distance metric for selecting of multiple evaluation points in each iteration which is required to implement an algorithm in parallel we investigated the efficiency of pods on eight well known test problems before testing it on two computationally expensive hydrodynamic problems pods was compared with three previous methods two are parallel surrogate algorithms sop rbf based and moe qei kriging based the third one is a heuristic method that does not use the surrogate p de we also created pods l which is like pods except without the use of surrogate distance metric to test how important the distance metric was in the pods algorithm the numerical results on 8 test problems i e the performance and data profile as well as the calibration progress plots indicate that pods and pods l outperform the previous methods pods with the assistance of surrogate distance metric performs better than pods l without the use of surrogate distance metric especially in the latter search stage hence even without the surrogate distance metic the algorithm worked well what is surprising is that moe qei performs the worst among all the methods and is even worse than p de which does not use a surrogate the best surrogate algorithm pods is compared with the popularly used heuristic method p de on two hydrodynamic model parameter calibration problems calibration scenario 1 and 2 the problem in calibration scenario 1 is based on real data with temperature observation at one station with different depths the calibration problem in calibration scenario 2 is a synthetic calibration problem in calibration scenario 2 the observation data at multiple stations across the lake surface and with different depths are synthetically generated from the model simulation with a known parameter set calibration scenario 2 enables us to compare optimization ability when there are measurements available at multiple stations in addition the synthetic case helps to evaluate the fit of the solution from optimization at locations where no observation data is used in calibration for spatial validation we conclude here that the pods algorithm outperforms on both the real lake calibration problem in calibration scenario 1 and the synthetic lake calibration problem in calibration scenario 2 parallel differential evolution which is a popular alternative algorithm in engineering problems the pods calibration also requires less time than the manual calibration on the real tropical lake problem because hydrodynamic models are so expensive often hours per single evaluation it is essential to have algorithms that can find a set of parameters that have a good fit with a limited number of hydrodynamic simulations pods should be considered for use by others for challenging hydrodynamic optimization problems there are many possible future areas for the application of this method our group is currently working on the use of a similar method for calibration of the water quality portion of the delft3d model we also will be working on the hydrodynamics application to other different tropical reservoirs declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is supported by the national research foundation prime minister s office singapore under its campus for research excellence and technological enterprise create programme and prof shoemaker s startup grant from the national university of singapore nus data and numerical models were provided by pub singapore s national water agency we would also like to acknowledge the modeling expertise provided by dr zhang jingjie in identifying some of the parameters to calibrate the computational work for this article was performed on resources of the national supercomputing centre singapore https www nscc sg the first author acknowledges the department of civil and environmental engineering in national university of singapore for awarding him the moe research scholarship appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104910 
25918,parameter calibration for computationally expensive environmental models e g hydrodynamic models is challenging because of limits on computing budget and on human time for analysis and because the optimization problem can have multiple local minima and no available derivatives we present a new general purpose parallel surrogate global optimization method parallel optimization with dynamic coordinate search using surrogates pods that reduces the number of model simulations as well as the human time needed for proper calibration of these multimodal problems without derivatives pods outperforms state of art parallel surrogate algorithms and a heuristic method parallel differential evolution p de on all eight well known test problems we further apply pods to the parameter calibration of two expensive 5 h per simulation three dimensional hydrodynamic models with the assistant of high performance computing hpc results indicate that pods outperforms the popularly used p de algorithm in speed about twice faster and accuracy with 24 parallel processors keywords lake hydrodynamics global optimization parallel optimization dynamic coordinate search surrogate model parameter calibration high performance computing software availability name of software pods description a python optimization package including 1 a standalone pods algorithm code 2 examples using pods with test function 3 examples using pods with deflt3d flow models problems developer wei xia contact address department of civil and environmental engineering national university of singapore block e1 08 23 no 1 engineering derive 2 singapore 117576 year first available 2020 hardware required pc or hpc software required python2 pysot 0 1 36 eriksson et al 2019 delft3d flow only need for delft3d flow model problems program language python program size 24 0 mb availability and cost free and open source for non commercial use availability in github https github com louisxw pods program documentation available at https louisxw github io pods 1 introduction 1 1 motivation it is our goal to introduce a new parallel global optimization algorithm that reduces the number of necessary expensive objective function evaluations for parameter calibration by using a surrogate approximation to help guide the analysis and to demonstrate its use on expensive hydrodynamic lake models calibration of model parameters in environmental simulation models is difficult especially if the simulation model is computationally expensive which means only a limited number of model simulations are possible hydrodynamic models dissanayake et al 2019 gaeta et al 2020 soulignac et al 2017 teng et al 2017 viti et al 2019 groundwater models mugunthan and shoemaker 2006 mugunthan et al 2005 song et al 2018 zhao et al 2016 distributed watershed models dembélé et al 2020 koo et al 2020 and global climate change models müller et al 2015 are some prominent examples of computationally expensive environmental simulation models that require calibration within the context of lake hydrodynamic models calibration is essential to ensure that a model can closely reproduce the actual thermal and flow behavior spatially distributed in a lake water body accurate spatiotemporal representation of hydrodynamics is in turn critical for the simulation and forecasting of water quality behavior in aquatic ecosystems chanudet et al 2012 in hydrodynamic models hydrodynamic behavior is calculated by solving a couple of partial differential equations with multiple model parameters which can vary in value in different lakes and reservoirs and hence need to be calibrated the most common method for the parameter calibration of hydrodynamic models is manual calibration whereby the modelers select sequentially multiple combinations of parameters that are evaluated via simulation examples can be found in galelli et al 2015 kaçıkoç and beyhan 2014 and råman vinnå et al 2017 this method is also referred to as the trial and error method manual calibration has numerous drawbacks as highlighted by previous studies dung et al 2011 kamali et al 2013 zou and lung 2004 zou et al 2007 for instance 1 manual calibration is inefficient in both computational time and human expert time and 2 manual calibration is unlikely to find a globally optimum calibration solution given an appropriate calibration objective moreover manual calibration becomes extremely challenging when there are many parameters to calibrate because a large number of simulation trials with different combinations of parameter values are usually needed to find suitable calibration solutions automatic calibration that incorporates optimization algorithms can significantly improve the efficiency and accuracy of parameter estimation of computationally expensive hydrodynamic models however automatic calibration methods have not been adequately explored in hydrodynamic model calibration this study tackles the challenges of calibrating expensive hydrodynamic models with a new parallel global optimization algorithm using surrogate and parallel computing techniques 1 2 literature review optimization algorithms have been applied for the calibration of hydrodynamic models in past studies for instance afshar et al 2011 used the automatic optimization approach for temperature and water budget calibration of a two dimensional hydrodynamic model ce qual w2 of the karkheh reservoir in iran fabio et al 2010 used the gauss levenberg marquardt method levenberg 1944 marquardt 1963 in the optimizer pest doherty 2004 to calibrate a two dimensional flood propagation model 4 h for a single simulation in multiple calibration stages and calibrated a maximum of 5 roughness coefficients simultaneously however gauss levenberg marquardt method in pest is a local optimization method that cannot guarantee a global minimum solution dung et al 2011 applied the non dominated sorting genetic algorithm ii nsga ii in parallel to calibrate 5 manning coefficients of a one dimensional mike11 flood model for mekong delta river which takes 10 h for a single simulation parallelization was essential in their study since the optimization method used 1560 expensive simulations evaluations takes 12 5 days using 13 processors to adequately calibrate the model the genetic algorithm had also been used in an earlier study for calibration of a pipe flow model babović and wu 1994 which can be computed relatively inexpensively and the method was not applied to open water there are also a few studies that tested the automated parameter methods in the calibration of hydrodynamic models developed in the popular delft3d flow deltares 2014 software some delft3d flow deltares 2014 hydrodynamic models that have used optimization for automatic calibration include coastal tide models garcia et al 2015 kurniawan et al 2011 and coastal morphological models briere et al 2011 for calibration of these hydrodynamic models a serial semi automated parameter estimation method from the openda library verlaan et al 2010 called does not use derivatives dud ralston and jennrich 1978 was used briere et al 2011 explained in their study that dud is a local minimization method and the probability is high that dud identifies a local minimum rather than a global minimum when multiple nonlinear terms are interacting with each other as is the case for lake hydrodynamic models it is highly likely that a calibration optimization problem has multiple local minima gorelick and zheng 2015 xia and shoemaker 2020 solomatine et al 1999 also highlighted that the error function of a numerical model is normally multimodal hence a global optimization method should be used as described above the delft3d flow model applications that have been calibrated in the past using the optimization method dud are relatively computationally cheap with a computational time that does not exceed 45 min for one simulation specifically the runtime of the delft3d model in optimization is about 30 min in garcia et al 2015 around 45 min in kurniawan et al 2011 and 20 min in briere et al 2011 however most hydrodynamic models especially three dimensional lake models in a real application can easily take much longer many hours or even days to run thus an efficient global optimization method should be applied for calibration of such models for instance if a single lake hydrodynamic model run takes 4 h per evaluation and optimization takes 2000 serial evaluations to find an acceptable answer the calibration process takes almost nine months which is not feasible efficient optimization techniques that require only a few simulation evaluations are thus desired many lakes around the world are now being modeled using computationally expensive 3d hydrodynamic models castelletti et al 2010b chanudet et al 2012 elhakeem et al 2015 galelli et al 2015 hui et al 2018 kaçıkoç and beyhan 2014 soulignac et al 2017 wahl and peeters 2014 however there is no published research to the best of our knowledge that uses efficient global optimization methods for expensive three dimensional lake hydrodynamic model calibration iterative surrogate response surface algorithms can significantly reduce the computational burden associated with automatic calibration of lake hydrodynamic models and have been applied to expensive optimization problems in the past castelletti et al 2010a 2010b razavi et al 2012b regis and shoemaker 2007b zou et al 2009 in the study of castelletti et al 2010b they proposed an iterative learning and planning procedure based on surrogate approximation and successfully applied on a water quality planning problem of a costly 3d lake models 7 days per simulation in their problem there are three decision variables and three iterations are conducted to obtain a satisfying solution which would require about 5 5 years of computation with a modern compute if by exhaustive scenario analysis 286 feasible decisions the basic premise in iterative surrogate algorithms is that the expensive objective function which in calibration problems requires an expensive simulation is replaced by an inexpensive statistical approximation function surrogate based on prior function evaluations and that surrogate assists in selecting which value of the parameter vector will be used to do the next expensive function evaluation the algorithms in regis and shoemaker 2007b and regis and shoemaker 2013 use a weighted average of estimated function value from surrogate and distance to previously evaluated points of randomly sampled candidate points to select the next evaluation point which is not necessarily at the minimum of the surrogate some surrogate methods use a heuristic e g a genetic algorithm or complete enumeration to find the minimum or pareto front of the surrogate and do a function evaluation there at the estimated minimum point of the surrogate castelletti et al 2010b fen et al 2008 zou et al 2009 müller and shoemaker 2014 compare these two methods minimizing with a heuristic optimization algorithm versus the selection of the best weighted average of candidate points and find that neither method dominates the other when tested across a range of surrogate types and surrogate optimization strategies widely used approximation functions include polynomial regression johnson and rogers 2000 schultz et al 2006 yin and tsai 2018 kriging bau and mayer 2006 di pierro et al 2009 hemker et al 2008 razavi et al 2012a zhao et al 2016 radial basis function rbf castelletti et al 2010b christelis et al 2018 razavi et al 2012a regis and shoemaker 2004 2007b 2009 tsoukalas et al 2016 support vector machine svm fan et al 2020 zhang et al 2009 and artificial neural network ann broad et al 2005 saadatpour 2020 song et al 2018 zou et al 2007 2009 detailed information of these approximations could be found in bhosekar and ierapetritou 2018 forrester et al 2008 and solomatine and ostfeld 2008 müller and shoemaker 2014 compared the effect of different types of surrogates including rbf kriging polynomial and multivariate adaptive regression spline mars as single or as ensemble surrogates and find that the most efficient ensemble surrogate is the combination of rbf with the polynomial dual surrogate and the most efficient single surrogate is rbf the advantages of the combination of rbf with polynomial were so small over the single rbf surrogate in müller and shoemaker 2014 that in the pods algorithm we use a single rbf surrogate in iterative surrogate algorithms the surrogate function is embedded in an iterative framework i e new trial points which are values of the decision vector are evaluated on the cheap surrogate function to determine which specific value will be evaluated by the expensive objective evaluations after the expensive evaluation is computed then surrogate functions are simultaneously improved castelletti et al 2010b razavi et al 2012b regis and shoemaker 2005 2007b multiple such algorithms have been proposed and applied to expensive water resources optimization problems in prior literature razavi et al 2012b moreover most surrogate optimization methods applied in prior literature are global since many water resources optimization problems are global application areas of these global surrogate methods include 1 groundwater bioremediation where rbf or kriging is used as surrogates bau and mayer 2006 mugunthan et al 2005 razavi et al 2012a regis and shoemaker 2009 2 water distribution systems where ann or kriging are utilized behzadian et al 2009 di pierro et al 2009 and 3 watershed problems where rbf are employed razavi et al 2012a shoemaker et al 2007 however the application of surrogate response surface algorithms on the calibration of open water hydrodynamic models is under explored in addition to using surrogates the efficiency of hydrodynamic calibration can be significantly improved via parallelization techniques with the assistance of high performance computing hpc parallel optimization methods with deployment on hpc systems have been successfully applied in the past to reduce wall clock time of automatic calibration of expensive water resources models regis and shoemaker 2007a 2009 xia and shoemaker 2020 tang et al 2007 vrugt et al 2006 zhang et al 2013 2016 most of these parallel application studies focus on watershed models however pang 2017 applied parallel global optimization to groundwater flow and transport models such parallel optimization methods can be even more effective in auto calibration of the extremely computational expensive hydrodynamic models models that take hours even days to run for a single simulation numerous efforts have also been made in prior literature to parallelize iterative global surrogate algorithms a comprehensive review of such methods is given in haftka et al 2016 moreover these methods predominantly include algorithms that use kriging models as surrogates sóbester et al 2004 wang et al 2016 or rbf as surrogates krityakierne et al 2016 regis and shoemaker 2007a 2009 kriging based parallel surrogate methods are prevalent in computationally expensive global optimization applications and especially machine learning applications however some recent studies show that rbf based methods are also very effective for computationally expensive parallel optimization applications and even more so for problems with many decision variables ilievski et al 2017 1 3 contributions this study develops a new parallel surrogate global optimization algorithm parallel optimization with dynamic coordinate search using surrogate pods pods is a parallel version of the serial algorithm dycors proposed by regis and shoemaker 2013 the key to parallelize a serial algorithm is how to generate p effective evaluation points based on the only information available so that these p processors are valuable which however is non trivial pods differs from previous algorithms because of one or more of the following things 1 use dynamical coordinate search to choose which decision variables to perturb 2 use a truncated normal distribution to generate candidate point perturbation and 3 use a surrogate distance metric to select multiple candidate points as evaluation points we investigate the efficiency of pods on eight well known test problems by comparing it with previous methods the effectiveness of the surrogate distance metric for selecting multiple points is examined by comparing pods with pods l that is a particular instance of pods without using the surrogate distance metric pods is further tested on two lake hydrodynamic model calibration problems note that this is the first time that surrogate global optimization algorithms have been applied to three dimensional lake hydrodynamic model calibration the first calibration problem is on a hydrodynamic model for a tropical reservoir in singapore based on actual observation data at multiple depths of one station referred to as calibration scenario 1 below another calibration problem referred to as calibration scenario 2 below is on the same real world model in calibration scenario 1 but with synthetic observation data at multiple stations generated from the model simulation with a known parameter solution hence for the synthetic calibration problem in calibration scenario 2 the true solution of the optimization problem is known with the synthetic calibration problem we could adequately assess the performance of optimization algorithms by comparing the solution from optimization to the known set of true solution the synthetic case also enables us to assess the algorithm performance when observations at multiple stations instead of one station as in calibration scenario 1 are used for calibration in addition we can compare the solution quality from optimization at these locations where there is no observation used for model calibration for spatial validation we think such an investigation would be interesting and essential because one important function of modeling is to provide spatial and temporal descriptions of the system with the observations at limited locations and time hence we want solutions from optimization not only fitting good at these locations and time steps with observations but also good at other sites note that such an analysis is not possible in calibration scenario 1 2 pods 2 1 general framework pods is a parallel implementation of the serial dycors algorithm regis and shoemaker 2013 that was designed for global with multiple local minima black box optimizations problems of the form 1 min x θ f x where θ l b x u b ℝ d l b u b ℝ d are the lower and upper bound of variable vector x d is the number of variables in x f x is a computationally expensive black box function that is not differentiable pods algorithm as shown in fig 1 keeps the master worker framework firstly proposed by regis and shoemaker 2007a and later utilized by regis and shoemaker 2009 for the parallelization implementation of the lmsrbf algorithm regis and shoemaker 2007b assume that p processors are available pods selects p distinct points for simultaneous objective function evaluation in parallel in each iteration pods starts the optimization process with a set of n 0 initial points generated via space filling experimental design e g latin hypercube experiment in the iterative loop the surrogate model s n x of the original expensive function f x is built or updated with the set of previously evaluated points denoted as a n x 1 x n where n is the number of points evaluated on black box function f x in pods the cubic rbf augmented by a linear polynomial is used for the surrogate model s n x as serial dycors does then n c a n d a user defined hyperparameter candidate points denoted as ω n ω n y n 1 y n n c a n d are generated around the best solution found so far x b e s t with the dynamical coordinate search strategy the dynamical coordinate search dynamically changes the proportion of coordinates i e decision variables of x b e s t that are allowed to be perturbed which was proven to be effective in practice regis and shoemaker 2013 tolson and shoemaker 2007 the details of the dynamical coordinate search will be discussed in detail in section 2 2 given p processors p out of n c a n d candidate points in ω n will be selected as next iteration evaluation points denoted as ℱ n x n 1 x n p which will be sent to the p workers for the evaluation of f x in eq 1 pods uses the surrogate distance metric used in parallel srbf regis and shoemaker 2009 for the selection of these p evaluation points the surrogate distance metric considers 1 the estimated objective function value from the surrogate model s n x as well as 2 the uncertainty of the surrogate model which is measured by the distance based metric the minimum distance from previously evaluated and selected evaluation points the distance based metric is effective for selecting p multiple evaluation points for parallel evaluation to demonstrate the effectiveness of distance based metric we developed a special version of pods which only consider the surrogate value of the candidate points as criteria for selection of evaluation points we expect that not using the distance based metric would make the algorithm search more locally because the uncertainty of the surrogate model is not considered hence we call this special version of pods as pods l we discuss surrogate distance metric further in detail in section 2 3 the iterative loop repeats until the termination condition is met which in this case is the maximum number of evaluation n max the pseudocode of the main framework of pods is in supplementary material s1 2 2 dynamic coordinate search the generation of candidate points with dynamic coordinate search is where pods is mainly different from parallel srbf regis and shoemaker 2009 similar to parallel srbf pods generates the candidate points based on the best solution found so far x b e s t however instead of allowing all the coordinates of x b e s t to be perturbed in all iterations as parallel srbf does pods allows only a subset of coordinates to be perturbed the number of coordinates that are allowed to change is controlled by a probabilistic variable p s e l e c t whose value is decreasing with the value of n increases note n is the number of function evaluations on f x and n 0 n n max 1 the value of p s e l e c t is calculated by a strictly decreasing function ϕ n ϕ 0 1 ln n n 0 1 ln n max n 0 where ϕ 0 min 20 d 1 the ϕ n function is the same as the function used in serial dycors for cases that no coordinate being selected for perturbation we randomly assign one coordinate out of the d variables to be perturbed the dynamical reduction of the number of coordinates to be perturbed is a strategy to adjust from global search to local search as the number of evaluations approaches the computational budget the dynamic coordinate search idea was motivated from the manual calibration of model parameters where in early calibration stage the solutions are relatively poor hence perturbing all or majority of coordinates is helpful for search but as the solutions improve it is necessary to perturb only a few coordinates so that the improvement gained is not lost tolson and shoemaker 2007 while if all coordinates of the best solution found in later search stages are allowed to change as in parallel srbf the value of the best solution s well calibrated coordinates parameters has a high chance of being disturbed practices also proved that dynamic coordinate search strategy is very efficient for calibration problems especially for these problems that have many decision variables regis and shoemaker 2013 tolson and shoemaker 2007 hence the dynamic coordinate search is expected to be helpful for pods on the calibration of hydrodynamic and other environmental models in pods for these coordinates selected to be perturbed a random perturbation is generated from a truncated normal distribution with mean 0 standard deviation σ n which is different from the serial dycors paper regis and shoemaker 2013 where a successive reflection method is used to ensure the candidate points are within the solution domain the pseudocode of dynamic coordinate search is in supplementary s2 note that the value of the search radius σ n affects the shape of the truncated normal distribution a larger value of σ n means that the candidate points have a higher possibility of being far from the best solution found so far x b e s t vice versa the update of σ n s value is based on the learning of the consecutive failed or successful iterations the implementation of the adaptive learning for σ n adjustment is the same as parallel srbf and is described in supplementary material s3 we allow restart function in pods when the value of σ n becoming too small or the algorithm does not improve the best solution for many iterations details are also included in supplementary material s3 note that for problems that only a relatively limited maximum number of function evaluations are allowed as problems in this study a restart might not occur however for other problems where there is more evaluation budget restart function could be helpful 2 3 surrogate distance metric for multiple evaluation points selection the key to the parallelization of a serial algorithm e g dycors is how to generate multiple evaluation points for expensive objective function evaluation in one iteration we adopt the surrogate distances metric idea from parallel srbf that the p evaluation points given p processors are selected sequentially from a larger number of candidate points ω n based on two criteria 1 the estimated function value from the surrogate model s n x and 2 minimum distance from previously evaluated points and previously selected points within that iteration a candidate point y n k where k 1 n c a n d in ω n is considered a good candidate point when it has a low value of s n x y n k for exploitation and far away from these points evaluated for exploration a weighted score is used for candidate point selection to balance these two criteria let w n r be the weight on surrogate criteria when selecting the n 1 evaluation point where 0 w n r 1 the weight on distance criterion w n d is then equal to 1 w n r the value of w n r can be used to balance global search when the value of w n r is small and local search when the value of w n r is large we adopt a weight cycling strategy that was also used in serial dyocrs and parallel srbf where the value of w n r cyclically changes from small to large values through a user defined parameter weight pattern ϒ v 1 v κ where 0 v 1 v κ 1 hence the weight on the surrogate criterion for each evaluation point is eventually selected sequentially from the list v 1 v κ v 1 v κ v 1 v κ and global search and local search are conducted in turn in pods in such a way the exploration and exploitation are balanced during the whole search process which can effectively avoid pods getting sucked in local minima and also prevent pods spending too many evaluation budgets on exploration more details of the evaluation point selection are in supplementary material s4 2 4 pods l to investigate the effectiveness of the surrogate distance metric on the performance of pods we developed another parallel version of dycors which blocks the use of surrogate distance metric in pods for the selection of p evaluation points we use only the approximated objective function value from the surrogate model s n x as the criterion for selecting candidate points in ω n as evaluation points hence the best p candidate points with the smallest surrogate value s n x are selected as evaluation points for the next iteration we expect only using the surrogate value information for evaluation point selection would make the algorithm search more locally because the uncertainty of the surrogate model is not considered hence we call this version of parallel dycors as pods l note that pods l is a particular case of pods when ϒ is set to be 1 0 3 hydrodynamic lake model temperature calibration pods is applied to two hydrodynamic lake model calibration problems i e calibration scenario 1 and 2 the calibration problem in calibration scenario 1 is calibrating the model parameter vector x of a three dimensional hydrodynamic model based on the real observed temperature data at one station with multiple depths of a tropical reservoir the real value of x is unknown the goal is to find a set of x with which the temperature simulation output from the hydrodynamic model is as close as possible to the real observed temperature data government agency employees and consultants did a previous calibration of the model in calibration scenario 1 and we call their method the manual calibration since no optimization methods were used calibration scenario 1 only uses observation data at one station of the reservoir in order to investigate the algorithm performance in cases where there are observation data at multiple stations we created the calibration scenario 2 the calibration problem in calibration scenario 2 is the parameter estimation on the same hydrodynamic model in calibration scenario 1 but with a synthetic set of observation data which is generated from the model simulation in calibration scenario 1 given a known parameter set denoted as x r we use the solution from manual calibration as x r since manual calibration is a very high quality solution that was achieved by the expert in months who are familiar with the studied system we save the simulation output at all locations all grids and during the whole simulation period the time series temperature data a at multiple stations and in multiple depths are used as observation data for model calibration the goal of the calibration is to find x r with which the temperature simulation output from the hydrodynamic model is a 3 1 site description and data the horizontal boundary of the studied reservoir in both cases is given in fig 2 this reservoir has over 250 ha of water surface and uneven depth for calibration scenario 1 one online water quality profiler station stn a1 was installed in the middle of the reservoir the water temperature data at the station are available at various depths the temperature data is for one year the year 2013 with a temporal resolution of around 20 min with outliers removed there are a few temperature observations that are unreasonable above 45 c due to the temporary failure of the sensor we replace these outliers with the average value of the nearest regular reading before and after the outliers since the model output is hourly for error calculation the model output for temperature for each hour is compared to the measured temperature of the same hourly time point if there is no measurement data at the hourly time point the temperature used for error calculation is a linear interpolation of the two temperature measurements just before and after the hour in calibration scenario 2 four additional sampling stations across the study region are added stn b1 4 in fig 2 the temperature time series data at five stations and in multiple depths are recorded and saved from the model simulation output with a given parameter vector x r the saved temperature time series data are used as synthetic observation data for model calibration in calibration scenario 2 3 2 hydrodynamic model and calibration parameters delft3d flow deltares 2014 was used to build the hydrodynamic model for the reservoir delft3d flow is a three dimensional hydrodynamic simulation solver that calculates non steady flow and transport phenomena based on the solution of the navier stokes equations the system of equations consists of the continuity equation equations of motion and the transport equation of heat these equations are partial differential equations and are solved by a finite difference method detailed information about the open source delft3d flow is provided in the user manual deltares 2014 the delft3d flow hydrodynamic model used in calibration scenario 1 and calibration scenario 2 was set up by the water utilities employees and consultants including the domain construction input data preparation such as meteorological data inflow and outflow data precipitation and evaporation data and model configuration the grid coordinate system uses cartesian coordinates z grid the number of grid points in the x direction is 65 the number of grid points in the y direction is 67 and the number of layers in vertical is 19 the length of the simulation in time is one year and the computational time step applied is 1 5 min resulting in 350 400 time steps a single 1 year simulation for the reservoir hydrodynamic model takes about 5 h to run in serial on a windows desktop with cpu intel core i7 4790 in this study nine parameters shown in table 1 with their calibration range that affect directly or indirectly the thermal activity in the water body are included in the calibration process for both calibration scenario 1 and 2 the calibration ranges for these parameters given in table 1 are suggested by singapore water utilities employees and consultants based on previous studies the detailed description of these physical processes in the model and the parameters are described in supplementary material s6 3 3 optimization problem formulation the objective of the optimization in both calibration scenario 1 and calibration scenario 2 is to minimize the error between the simulated water temperature and the measured water temperature nash sutcliffe efficiency nse introduced by nash and sutcliffe 1970 is used to quantify the goodness of fit between the observed and simulated water temperatures at different locations the objective function is defined in eqs 2 and 3 below 2 f x i 1 m j 1 n i n s e i j x 3 n s e i j x 1 t 1 k s i m t i j t e m x o b s t i j t e m 2 t 1 k s i m t i j t e m x μ o b s i j t e m 2 where s i m t i j t e m x and o b s t i j t e m denote the simulated water temperature given a parameter vector x and observed water temperature respectively at the time step t and depth h j of station i μ o b s i j is the mean of o b s t i j t e m over all k time steps k is the number of time steps corresponding to the observed water temperature m is the total number of monitor stations in calibration scenario 1 m 1 one station stn a1 in calibration scenario 2 m 5 5 stations stn a1 and stn b1 4 n i is the number of different depths where the water temperature data is observed at station i i 1 m in calibration scenario 1 n 1 3 3 different depths at stn a1 in calibration scenario 2 n i 3 for i 1 and 2 3 different depths at stn a1 and stn b1 and n i 2 for i 3 4 5 2 different depths at stn b2 4 because they are shallower than stn a1 and stn b1 4 computational experiments 4 1 alternative algorithms to assess the performance of pods we first compared pods with earlier parallel surrogate based optimization methods including sop krityakierne et al 2016 which is a parallel rbf based method and moe qei wang et al 2016 which is a parallel kriging based gaussian process based method sop algorithm was compared with other previous rbf based algorithms including parallel srbf regis and shoemaker 2009 and an evolutionary algorithm that uses rbf regis and shoemaker 2004 their result showed that sop is more efficient than these algorithms hence we did not compare pods with them besides the surrogate algorithms we also compared pods with a popular population based stochastic algorithm parallel differential evolution p de storn and price 1997 tasoulis et al 2004 which had been used for water resource optimization problems tashkova et al 2012 zheng et al 2015 the description of these alternative algorithms is provided in supplementary material s7 4 2 test functions before we tested the pods algorithm on the computationally expensive hydrodynamic model calibration problems we compared the performance of pods with previous surrogate algorithms first on inexpensive test problems since they are inexpensive we can test the algorithm performance on a range of problems we used eight 10 dimensional black box test functions summarized in table s2 in supplementary material s8 these test problems are the well known rastrigin michalewicz ackley levy schewefel and styblinskitang whitley and weierstrass functions which have a large number of local minima detail information of these functions is given by surjanovic and bingham 2013 and jamil and yang 2013 4 3 experimental setup all computational experiments in this study were implemented on a single node on the national supercomputer center nscc of singapore which is a linux based platform with dual intel xeon e5 2690 v3 processors the code coupling pods with the delft3d flow model suite is available in the software pods https github com louisxw pods other users of this code can follow the instructions available in the pods software to apply pods to their models in this study five algorithms i e pods pods l section 2 4 and three previous methods were compared on 8 test problems since they are not computationally expensive to evaluate however for the hydrodynamic calibration problems we did not compare all the five algorithms on the hydrodynamic calibration problems since they are too computationally expensive to compute i e a single model simulation takes around 5 h we then selected the surrogate algorithm with the best performance on the test problems and compared it with the popular heuristic algorithm p de that does not use surrogates for the comparison on the test functions the number of processors used was set to 24 and the maximum number of evaluations was set to 504 including 24 evaluations in initial experiment design so there are 21 iterations each of which evaluates the objective function for 24 different parameter vectors each experiment one algorithm solves a problem once was repeated with 30 trials for all algorithms on all test problems for hydrodynamic calibration problems the number of the processor was set to be the same as that on test problems with p 24 the maximum number of evaluations was set to be 192 hence there are eight parallel iterations with 24 hydrodynamic simulations per iteration given that one evaluation takes about 5 h to compute eight iterations take less than two days 40 h of wall clock time for the hydrodynamic calibration problems analyzed in this study total computation time per trial is 40 24 960 core hours at a cost around s 25 8 given price at s 0 0269 core hour on nscc this is a reasonable time for automatic calibration since it is much less than the time typically taken in the manual calibration of the real lake which may be in the order of weeks or even a month of human time for experts engineers very experienced with this lake and model who previously manually calibrated the hydrodynamic model for the current data set since the hydrodynamic models being calibrated in this study are extremely expensive we performed five trials for each optimization experiment furthermore in order to remove any initial sampling bias all algorithms including p de were initialized with the same set of evaluation points in the initial experimental design for the respective trial for all the numerical experiments in this study moreover there are few hyperparameters in pods and we used the same algorithm hyperparameter values for all test problems these values are given in table s1 of supplementary material s5 for all the numerical experiments in this study we used the values from previous studies regis and shoemaker 2009 2013 and the robust performance of these values was verified in previous studies by many theoretical test problems and real world applications in general we do not encourage using lots of expensive function evaluation to set hyperparmeter valued since it is not clear that the extra function evaluations done to improve hyperparameters for specific problems would generally be more effective than just using the standard hyperparmeter values in table s1 and using the extra function evaluations to do more optimization iterations 5 results and discussion 5 1 algorithm performance on test problems 5 1 1 pods and pods l vs alternative algorithms data profiles fig 3 shows the data profiles moré and wild 2009 of all the five algorithms on the eight optimization test problems each with 30 trials data profile plots provide a concise summary of comparative algorithm performance of stochastic optimization algorithms on multiple test problems we adopt the method from moré and wild 2009 to generate these data profile plots that consider the results of all trials on all problems to compare the overall performance of each algorithm the detailed explanation of the calculations in data profiles is given in supplementary material s9 an intuitive interpretation of a data profile is that for a given algorithm s i e one of the five algorithms a data profile plots the percentage of problems solved within n function evaluations in the data profiles of fig 3 a problem is deemed solved if the algorithm s finds a solution that is smaller than f l τ f x 0 f l where f x 0 is the function value of starting point for each problem i e the best solution in the initial experimental design of a trial and f l is the best solution found among five algorithms for each problem in the end the whole optimization process consequently in data profiles high values of the percentage of problems solved indicate the good algorithms it is clear from fig 3 that the new algorithm pods and pods l outperform all three other algorithms pods and pod l solved within the tolerance level more than 80 and 60 problems respectively compared to less than 40 for other methods after the equivalent of about 30 simplex gradient estimates it is surprising that moe qei is even worse than p de which does not use the surrogate to further illustrate the results in fig 3 we look at the performance of the different algorithms on two specific test functions i e rastrigin and michalewicz rastrigin is a complex multimodal problem with a large number of local optima which can be used to test if an algorithm can avoid locally optimum solutions for the michalewicz problem the global minima are located in a very small sub region of domain space which is difficult to locate hence it is a difficult optimization problem many calibration problems depict a similar behavior in the search domain fig 4 shows in detail the average calibration progress plots of all the five algorithms in solving the rastrigin and michalewicz problems the error bars in fig 4 represent 95 t confidence intervals for the mean of the best solution found so far in objective function values since both test problems are minimization problems the lowest optimization progress curves for pods and pods l denote faster convergence and find better solutions than all other algorithms at the end of 504 evaluations moe qei is the worst of all the five algorithms on both rastrigin and michalewicz problems and is even worse than the non surrogate based p de on the rastrigin problem a plausible reason for moe qei s performance is that it uses a gaussian process based optimization approach that is not designed for problems with many decision variables it is common in machine learning papers to mention that gaussian process optimization does not do well in higher dimensions e g ten and above ilievski et al 2017 show that an rbf algorithm does significantly outperform the gaussian process model on problems with dimensions 15 and 19 as mentioned in section 4 1 sop and moe qei were included in this analysis as benchmark parallel surrogate algorithms pods outperforms e g converges faster and finds a better solution at completion than these methods as depicted by data profiles in fig 3 pods is an extension of a serial code dycors regis and shoemaker 2013 that is designed to be effective on high dimensional problems by reducing the number of dimensions that are perturbed from the previous best solution for any random candidate point dycors has been shown to work well on problems of up to 200 dimensions regis and shoemaker 2013 pods better performance relative to sop is essentially a consequence of the difference in both algorithms mechanisms for generating candidate points generation of candidates in sop is based on a trade off between exploration and exploitation krityakierne et al 2016 that permits points with relatively poor surrogate values to be selected because they are far away from previously evaluated points by contrast pods generates candidates only around the best solution found so far thus the above results indicate that the design of parallel surrogate algorithms may benefit more from not evaluating too many points with bad function value just because they are in unexplored areas we also provide the performance profile dolan and moré 2002 for the five algorithms in the supplementary material s9 performance profile plots defined in s9 also show that pods and pods l outperform other algorithms on a limited evaluation budget 5 1 2 pods vs pods l pods and pods l are the fastest two algorithms as shown in fig 3 the data profiles of fig 3 show that pods l solves a similar proportion of problems as pods before the equivalent of about 200 function evaluations however pods outperforms pods l after about 200 function evaluations for instance after 450 evaluations pods is successful in solving 30 more problems than pods l calibration progress plots see fig 4 also show that pods is more efficient than pods l especially after about 250 evaluations the difference between pods and pods l becomes significantly large note that the main difference between pods and pods l is that pods uses the surrogate distance metric for evaluation point selection while pods l does not use the surrogate distance metric consequently pods l does not explore the search domain adequately which weakens the global search ability and may force it to converge to a locally optimum solution this probably explains why pods l shows similar performance to pods in the initial optimization iterations i e before 200 evaluations but is outperformed by pods which has stronger global search ability in the latter optimization iterations hence surrogate distance metric helps to improve the performance of pods 5 2 performance of pods on hydrodynamic model calibration calibration scenario 1 pods which is the best surrogate global algorithm on test problems as shown in fig 3 was compared with the non surrogate algorithm p de on the hydrodynamic model calibration in calibration scenario 1 and 2 this section discusses the comparison of two algorithms on the problem in calibration scenario 1 in terms of i calibration efficiency i e the speed with which the objective function improves and ii accuracy of the final solution obtained within a fixed computing budget fig 5 shows the calibration progress plot of both algorithms for calibration scenario 1 the calibration progress plot is the plot of the average among five trials of the best solution found so far in terms of the objective function value f x in eq 2 as a function of the number of expensive function evaluations since both algorithms use the same initial experiment design sets for corresponding trials the calibration progress plot only shows the result obtained after the initial experiment design after 24 evaluations the objective function value of the manual solution obtained via a manual tweaking of parameters by expert modelers is also plotted black dash line as a reference pods is twice as fast as p de in terms of speed of convergence on a limited budget of 192 evaluations this is noted in fig 5 with the dashed red line that indicates it takes only around 96 evaluations for pods to achieve the average best solution obtained by p de i e 1 7992 after 192 evaluations which is over a two fold increase in speed hence for this complex problem pods is twice as fast as p de from fig 5 we can find that pods takes only around 120 evaluations on average averaged over five iterations to achieve the manual solution 1 8449 in this case one iteration takes 5 h to run so it takes around 25 h for pods to get the manual solution which as discussed earlier takes experienced experts weeks to achieve note that the pods method did not include the known manual solution as an initial value so the use of pods is a labor saving approach in comparison to manual calibration also note that p de does not do well in this case since it does not even find the manual solution according to average error over five trials within 192 evaluations so this indicated that even finding a solution as good as the manual solution is challenging the final calibration solutions obtained by optimization with both pods and p de are compared in terms of graphical time series visualization in this sub section since multiple optimization trials were performed for each algorithm we choose the final calibration i e the best calibration as per objective function after 192 evaluations of the median trial of each algorithm for graphical time series comparison as discussed above the solution found by pods is better than the manual solution in terms of the objective function value moreover both the solutions obtained by pods and by manual calibration are better than the final median trial solution obtained by p de 1 7944 this supports the quality of the solution obtained by pods fig 6 gives the water temperature simulation output of the final calibrations from both algorithms since the temperature simulation outputs corresponding to solutions from both algorithms i e pods and p de show significant difference at the surface layer both in terms of error metrics or graphic visualization simulated temperatures at the middle and bottom layers are very similar only the time series of temperature error δ t i e the difference between the measured temperature and simulated temperature at the surface layer are shown in fig 6 the upper panel a is the time series temperature error plot over one year and the lower panel b is the scaled up time series temperature error plot over a selected period the second half of aug the time series plot of the water temperature from simulation output and real observation is provided in fig s6 in supplementary material s10 however since the temperature error plot gives a more intuitive comparison between the solution quality between pods and p de we focus the discussion on the temperature error plot fig 6 it is evident from fig 6 that the solution obtained by pods is very close to the manual solution and is much better than the solution from p de at the surface layer pods solution as well as the manual solution overall have significantly lower temperature error time series in comparison to p de s solution this outcome is consistent with the progress plot comparison of fig 5 which is based on the calibration objective function as a performance metric the nse values at the surface layer of median solutions of pods and p de are 0 6175 and 0 4656 respectively so pods solution is about 32 6 better than the p de s solution higher nse value is a better solution in terms of rmse value pods solution is better than p de s solution at the surface layer 0 3570 vs 0 4220 c pods uses parallel computing which saves wall clock time than manual calibration however parallel computing is not the only thing that makes pods efficient to make full use of p processors the high quality of the p evaluation points is critical pods compared with p de has used a fast to run surrogate as a replacement of the original expensive model to guide the search pods also has a very effective method to generate p high quality evaluation points firstly pods uses the dynamic coordinate search idea in section 2 2 that the number of coordinates allowed to be perturbed is dynamically reducing as the calibration process progresses this strategy was used in serial dycors regis and shoemaker 2013 where it was shown to be very effective with higher dimensional problems in comparison to other algorithms secondly pods uses an effective way to select p evaluation points by considering the prediction value of the surrogate for exploitation as well as the distance of evaluation points from evaluated points for exploration this way of selecting multiple evaluation points is proven to be effective in previous studies for many real world optimization problems e g calibration of groundwater bioremediation remediation models regis and shoemaker 2009 5 3 performance of pods on hydrodynamic model calibration calibration scenario 2 in this section we use a new test function to compare pods and p de on the hydrodynamic model calibration problem in calibration scenario 2 calibration scenario 2 used the same model same model configuration and input data as used in calibration scenario 1 however the calibration problem in calibration scenario 2 is different from the problem in calibration scenario 1 by 1 the true solution x r of the calibration problem in calibration scenario 2 is known and 2 there are five monitoring stations spread out in the study area as shown in fig 2 instead of one station as in calibration scenario 1 the known solution x r in calibration scenario 2 helps to assess the performance of the algorithm by comparing the true solution and also enables us to assess the algorithm performance when more observation data in spatial for calibration is available in addition calibration scenario 2 enables the performance evaluation in table 2 below at all the locations where the monitoring data is not used in model calibration for spatial validation of calibration the calibration progress plot of both algorithms on the problem in calibration scenario 2 is shown in fig 7 similar to the result in calibration scenario 1 pods converges faster and find a better solution than p de at the end of calibration the final solution obtained by p de with 192 evaluations takes pods around 92 evaluations 48 less we also compared the solution quality obtained by pods and p de the median solution over five trials of the error over five sampling stations at all sampling times obtained by each algorithm is shown in table 2 the median solution obtained by pods 11 9122 is better than that by p de 11 7899 in terms of objective function values top row table 2 we also compared both solutions by calculating the temperature error between optimization solution and true solution at all locations grids and all time steps as shown in table 2 row 2 note these quantities were not used for calibration so they indicate how significant are the differences in the calibration solutions obtained by the two algorithms the sum of absolute temperature error at all locations and time steps of the solution obtained by pods is nearly 50 less than that of the solution found by p de indicating much better spatial and temporal accuracy of temperature associated with the pods calibration result for the magnitude of maximum and minimum temperature error at all locations and time steps pods solution is also much smaller than p de s solution note that we calculated the temperature error at all locations grids for the water body of the reservoir not only at the five stations where synthetic observation data is used for calibration in the objective function f x in eq 2 this shows that solution found by pods is not only better than p de s solution at locations considered in objective function but also at locations where no observation data was used for calibration this analysis is an attempt at spatial validation of the optimization solution obtained by pods the time depth contour plots for the temperature error between the solution obtained by pods and p de and the true solution at two stations stn a1 and stn b1 are shown in fig 8 and fig 9 respectively note figs 8 and 9 not only show the temperature error at the location where the data were used for model parameter calibration but also show the temperature error at the places where the observation is not included in the objective function f x in eq 2 from figs 8 and 9 we can evaluate if the solution from optimization fits well at locations that observation is not used for model calibration while in calibration scenario 1 no such analysis could be done this is one of the advantages of assuming a true solution in calibration scenario 2 it is clear from figs 8 and 9 that the solution obtained by pods is better than the solution obtained by p de the temperature error of the solution by pods is relatively smaller than that by p de at the majority of depths and time steps this conclusion is consistent for both stations stn a1 and stn b1 especially in stn b1 for most of the depths and time steps the magnitude of the temperature error of pods solution is smaller than 0 05 c in contrast for p de s solution there are lots of depths and time steps where the magnitude of error is larger than 0 20 c figs 8 and 9 show that pods solution is not only better than p de s solution at locations where the observed temperature data were used for calibration included objective functions but also better at these locations where the temperature data was not included in the calibration this demonstrates that the automatic calibration methodology with the use of pods is a reliable method that is validated spatially in figs 8 and 9 for model parameter calibration in real world cases where the observation data are usually available at only a few locations compared with the large study region 5 4 extension to other applications pods effectiveness is demonstrated based on a variety of optimization problems both theoretical test problems and real word applications in different calibration scenarios in this paper we applied this method to hydrodynamic calibration problems however it is important to note that pods is a general purpose method and can be used for many other environmental and water resource optimization problems especially problems involving computationally expensive models the derivative of the objective function is usually not available for these simulation model based problems and there are often multiple local minimums one application area of pods is to solve inverse problems for example a model parameter calibration problems as in this study b pollutant source identification problems where simulation models are used as tools to simulate the response model output of different inputs i e pollutant source locations or pollutant amount and optimization methods are used to search the inputs that give best model output compared with observations another application area of pods is to address reservoir management problems where expensive physical models are used to do scenario analysis given different management policies pods can also be applied to environmental design problems e g groundwater pump well design pods is a competitive method that can find a suitable solution within a limited number of evaluations which is very important for these problems that involve expensive objective functions computed by model simulations 6 conclusions we presented a very efficient general purpose parallel surrogate optimization algorithm pods available in open source software for calibration of models which are typically very computationally expensive moreover we demonstrated its performance on lake hydrodynamic model calibration problems as well as on test problems pods is an expansion of serial dycors and incorporated the surrogate distance metric for selecting of multiple evaluation points in each iteration which is required to implement an algorithm in parallel we investigated the efficiency of pods on eight well known test problems before testing it on two computationally expensive hydrodynamic problems pods was compared with three previous methods two are parallel surrogate algorithms sop rbf based and moe qei kriging based the third one is a heuristic method that does not use the surrogate p de we also created pods l which is like pods except without the use of surrogate distance metric to test how important the distance metric was in the pods algorithm the numerical results on 8 test problems i e the performance and data profile as well as the calibration progress plots indicate that pods and pods l outperform the previous methods pods with the assistance of surrogate distance metric performs better than pods l without the use of surrogate distance metric especially in the latter search stage hence even without the surrogate distance metic the algorithm worked well what is surprising is that moe qei performs the worst among all the methods and is even worse than p de which does not use a surrogate the best surrogate algorithm pods is compared with the popularly used heuristic method p de on two hydrodynamic model parameter calibration problems calibration scenario 1 and 2 the problem in calibration scenario 1 is based on real data with temperature observation at one station with different depths the calibration problem in calibration scenario 2 is a synthetic calibration problem in calibration scenario 2 the observation data at multiple stations across the lake surface and with different depths are synthetically generated from the model simulation with a known parameter set calibration scenario 2 enables us to compare optimization ability when there are measurements available at multiple stations in addition the synthetic case helps to evaluate the fit of the solution from optimization at locations where no observation data is used in calibration for spatial validation we conclude here that the pods algorithm outperforms on both the real lake calibration problem in calibration scenario 1 and the synthetic lake calibration problem in calibration scenario 2 parallel differential evolution which is a popular alternative algorithm in engineering problems the pods calibration also requires less time than the manual calibration on the real tropical lake problem because hydrodynamic models are so expensive often hours per single evaluation it is essential to have algorithms that can find a set of parameters that have a good fit with a limited number of hydrodynamic simulations pods should be considered for use by others for challenging hydrodynamic optimization problems there are many possible future areas for the application of this method our group is currently working on the use of a similar method for calibration of the water quality portion of the delft3d model we also will be working on the hydrodynamics application to other different tropical reservoirs declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research is supported by the national research foundation prime minister s office singapore under its campus for research excellence and technological enterprise create programme and prof shoemaker s startup grant from the national university of singapore nus data and numerical models were provided by pub singapore s national water agency we would also like to acknowledge the modeling expertise provided by dr zhang jingjie in identifying some of the parameters to calibrate the computational work for this article was performed on resources of the national supercomputing centre singapore https www nscc sg the first author acknowledges the department of civil and environmental engineering in national university of singapore for awarding him the moe research scholarship appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104910 
25919,in colorado s lower arkansas river basin larb inefficient irrigation and canal seepage contribute to salinization and waterlogging of irrigated lands and to stream aquifer pollution the geographic information system gis based river basin management model river geodss is applied to further explore best management practices bmps earlier determined to remedy these agro environmental impacts unfortunately bmp benefits are offset by altered irrigation return flows which change historical downstream river flows threatening compliance with water rights and the arkansas river compact compensation is possible through optimal sizing and operation of a dedicated reservoir storage account multi agent optimization combines a metaheuristic mutation linear particle swarm optimization mlpso with a fuzzy rule based system to produce generalized operational policies along with optimal storage sizing to enable bmps while satisfying legal constraints a storage account making up less than 5 of available reservoir capacity can be operated with rules that enable implementation of even the most aggressive bmps keywords irrigation river basin management water law compliance artificial neural networks metaheuristic optimization fuzzy logic 1 introduction 1 1 best management practices for improving water quality in the larb colorado when changes in water management are needed to improve agricultural and environmental conditions in the western united states water law can stand as an impediment this is the case in the lower arkansas river basin larb of southeastern colorado fig 1 the larb contains a central alluvial valley with approximately 14 000 fields covering around 110 000 ha of irrigated farmland the valley is supplied by 25 canals that divert water from the arkansas river along with pumping from about 2400 alluvial groundwater wells all of which directly impact streamflow in the arkansas river and its tributaries most fields are irrigated using surface irrigation methods with only about 20 employing more water efficient sprinkler or drip irrigation methods osborn et al 2017 the larb has long been known for its valuable agricultural production with the introduction of extensive irrigation dating back to the 19th century over the course of time however inefficient irrigation practices and leakage from unlined canals have led to shallow groundwater tables salinization of water and agricultural lands and exacerbation of growing nutrient and trace element pollution of groundwater and surface water resources gates et al 2016 any measures under consideration for agro environmental improvement in the larb are constrained by a complex legal system namely colorado water rights and a transboundary compact the complexity of the problem along with the availability of extensive data makes the larb a unique and exemplary case study this paper presents the latest in a series of inter related investigations to understand and address the need for reducing levels of environmental pollution while ensuring sustainability and productivity of irrigated lands in the larb in a manner compliant with water law broad and long term primary data collection as well as secondary data compiled from agencies such as the united states geological survey usgs the colorado division of water resources and the national oceanic and atmospheric agency noaa gates et al 2016 have led to calibrated and tested computational models morway et al 2013 bailey et al 2014 shultz et al 2018a tavakoli kivi et al 2019 that were applied to two representative larb regions the upstream study region usr and downstream study region dsr located along stretches of the central alluvial valley upstream and downstream of john martin reservoir jmr respectively fig 1 these coupled groundwater streamflow and reactive solute transport models are based on the modflow sfr2 niswonger and prudic 2010 uzf rt3d clement et al 1998 clement and johnson 2002 bailey et al 2013 and otis models runkel 1998 succeeding studies employed these models to investigate various proposed best management practices bmps with the potential for improving water quality while also conserving water in the basin morway et al 2013 bailey et al 2015a 2015b shultz et al 2018b the bmps evaluated in these studies include combinations of incremental levels of reduced irrigation water application placement of water from a canal or pumped source to the surface of cropped fields decreased canal seepage lease fallowing of irrigated fields reduced fertilizer application and improved riparian buffers impacts of bmp implementations simulated in these previous studies have indicated good prospects for lowering of shallow saline water tables for reductions in selenium se nitrate no3 and salt concentrations and for conserving water the implications of bmp implementations to river flows at the basin scale have been studied through the machine learning based coupling of surface water groundwater models triana et al 2010a rohmat et al 2019 although found to be environmentally advantageous basin wide adoption of bmps nevertheless also were found to considerably modify arkansas river flow patterns which could potentially violate legal constraints in the basin rohmat et al 2019 1 2 implementation of bmps water rights irrigation water accounting and interstate compact compliance water bmps considered in pre cursor studies include reduced irrigation ri which upgrades inefficient surface irrigation practices and lessens surface runoff and deep percolation losses and canal sealing cs which decreases water losses from canal seepage if implemented in the larb these bmps would lower the required irrigation canal diversions water taken from the river typically through a headgate into an irrigation canal and would alter patterns of return flow water in excess of consumptive use that returns over the surface or through the subsurface back to the stream system although the bmp implementations have been indicated advantageous for enhancement of the larb their adoption is hindered by the complex legal constraints imposed in the basin based on colorado s water rights system and the arkansas river compact agreement between colorado and kansas concerning apportionment of river flows between the two states colorado revised statutes 1949 these laws prohibit changes to the river system that would alter irrigation diversion and return flow in a manner that results in significant changes to in stream flow conditions thereby threatening to violate both colorado water rights and the interstate compact colorado water law is based upon a strict version of the prior appropriation doctrine that is common in the western united states water rights are decreed by the state for designated beneficial uses at specific locations like irrigation of agricultural land and are administered in order of seniority under a decreed water right irrigated acreage cannot be expanded nor can crop type be significantly altered so as to increase historical consumptive use water evaporated or transpired by the plants thereby lowering the amount of water in the stream system colorado foundation for water education 2004 colorado agricultural water alliance 2008 specifically in the arkansas river basin the irrigation improvement rules promulgated by the colorado state engineer director of the colorado division of water resources and enacted in 2011 prohibit implementation of agricultural efficiency improvements that during some periods reduce return flows and diminish useable flows at the colorado kansas stateline as stipulated in the arkansas river compact colorado division of water resources 2020 thus colorado proves to be an exception to the assumption of perry and steduto 2017 and re asserted by grafton et al 2018 that the typical impact of more efficient irrigation practices in many parts of the world is to increase current consumption and to increase demand for water making scarcity both worse and more difficult to manage on the contrary in colorado any water saved by increased irrigation efficiency must be allowed to remain in or to return to the stream aquifer source for extraction by other water rights holders in order of priority given that irrigation water rights holders cannot increase their consumptive use above historical levels one might wonder why these irrigators would nevertheless consider implementing water bmps in their canals and fields the motivation is twofold first these bmps result in a lowering of shallow water tables thereby reducing waterlogging and salinity which damage crop yields and second these bmps enhance the stream environment and thereby lessen the likelihood of the future imposition of nonpoint source pollution control measures by the state as in the case of colorado regulation 85 as described by colorado ag water quality 2020 rohmat et al 2019 showed that water bmps in the larb generally would result in more net flow in the arkansas river during the irrigation season 15 march 15 november and less water in the non irrigation off season this is because more water efficient irrigation management diverts less water from the stream during the irrigation season while also returning less water through groundwater or surface water in both the irrigation and off seasons fig 2 illustrates water allocation and return flows in an irrigated alluvial valley where canals divert and convey water from the main stream lose some of the diverted water through seepage and apply the remaining amount to the fields while the main river and the tributaries carry un diverted water and receive surface and subsurface return flows the figure shows how the implementation of water bmps would alter flow diversions to canals seepage from canals irrigation applications return flows and in stream flows in comparison to historical baseline conditions for the case where increases in crop consumptive use are prohibited by law storage and release of flows from a reservoir such as jmr in the larb could be used to offset the impact of bmp implementations on in stream flow patterns which vary between irrigation season and off season and which in turn would affect flows available for diversion and flows downstream at the stateline in the case of the larb releases from pueblo reservoir form the upstream boundary condition on the system and are assumed under bmp implementations to remain unchanged from historical conditions river flows at the stateline form the downstream boundary condition and would be altered by changes in diversion and return flow patterns along the valley brought about by bmp implementation and amended by flows released from jmr to further understand the implications of the side effects of proposed bmps at the basin wide scale a decision support system dss modeling tool called river geodss originally developed by triana et al 2010a was significantly upgraded and refined by rohmat et al 2019 river geodss is a dss in a geographical information system gis environment that was partially motivated and developed within the context of describing legally constrained flow allocation in the larb it employs the geomodsim river basin network flow model for streamflow routing and water rights analysis capabilities designed to accommodate complicated exchanges between water right holders transfer of water rights and plans for augmentation similar to those in the larb rohmat et al 2019 provided a major advancement within river geodss by incorporating the scikit learn deep neural networks dnn package pedregosa et al 2011 for accurate compute efficient emulation of modflow sfr2 for simulation of complex stream aquifer interactions and wrapped within a custom georeferenced arcmap environmental systems research institute 2011 extension fig 3 this gis based application was adopted to provide georeferenced data management user interface analysis and presentation that can be combined with any other geospatial dataset results of application of the dnn upgraded river geodss by rohmat et al 2019 show that the 74 alternative bmp scenarios simulated in the study although environmentally beneficial would indeed result in considerable modification of arkansas river flow patterns due in part to influences on stream aquifer system exchanges that detrimentally impact adjudicated water rights and the colorado kansas stateline flows all bmps under consideration would to varying degrees cause harm to senior water rights as well as result in periods of stateline flow deficits that may violate the kansas colorado interstate compact agreement the present study shows however that availability of a new storage account in jmr along with optimal operational strategies for releasing water from the storage account could be the answer to achieving environmental improvements in the basin while still satisfying all legal and administrative statutes triana et al 2010b using an adhoc trial and error approach show that availability of a dedicated storage account in jmr along with alteration of the reservoir operational policies could offer a potential solution to maintaining compliance with the compact with implementation of bmps however determining optimal generalizable operational policies for the storage account was beyond the scope of the study by triana et al 2010a requiring more formalized optimization methodologies in the present paper attention is turned toward finding those policies 1 3 new reservoir storage account to facilitate bmp implementation a storage account to a reservoir is like a bank account to a bank a reservoir can have multiple accounts each with its owners and purposes flow releases are debited from the account and inflows are credited or stored to the account with an associated cap or maximum account size an agent based methodology allowing integration of multi agent optimization with the geomodsim river basin management simulation model is presented herein for determining optimal operational policies for a dedicated storage account in jmr to allow implementation of environmentally beneficial bmps without violating water law such storage accounts are currently being authorized by the colorado kansas arkansas river compact administration as recently reported in the la junta colorado tribune democrat 2019 the colorado kansas arkansas river compact administration passed a resolution 2019 01 on feb 14 jay winner general manager of the lower arkansas valley water conservancy district believes the passing of resolution 2019 01 will open the door for future agreements to allow those in agriculture and other industries to store their water in jmr this is particularly good news according to winner because the reservoir has been underutilized in the past the highly dynamic nature of the optimization of reservoir operations along with the complexity of accurately modeling the spatially distributed stream aquifer system of the larb results in a temporally high dimensional optimization problem since solutions must be obtained over a large number of time steps as the degree of temporal dimensionality increases the likelihood of encountering saddle points that are neither local minima nor maxima in the decision space also increases dauphin et al 2014 point out that optimization within high dimensional decision spaces is prone to stagnation due to saddle points surrounded by high error plateaus that can dramatically slow down the learning process with the probability of occurrence of saddle points growing exponentially with increasing decision space dimension although methods such as dynamic programming dp are effective for solving optimization problems with high temporal dimensionality labadie 2004 it would be difficult to imbed geomodsim along with the dnn as a compute efficient surrogate for the modflow modeling of the complex stream aquifer system of the larb into a recursive dp structure to overcome the high temporal dimensionality of the multi agent optimization a novel hybridization of a variant of a metaheuristic algorithm with fuzzy logic is applied herein for generating robust and generalized reservoir operating policies wan et al 2006 labadie and wan 2010 ostadrahimi et al 2012 and labadie et al 2012 successfully applied other types of hybridization approaches to optimal operation of reservoir systems results of the present study show that appropriated water rights in the basin can be maintained along with adherence to the colorado kansas interstate compact agreement while minimizing requisite storage account capacity this outcome points a way forward in making it possible to adopt bmps for improved water and environmental management without violating state and interstate law 2 methodology 2 1 multi agent optimization with metaheuristic algorithm wooldridge 2009 defines a multi agent optimization as a modeling framework with a collection of agents acting in a distributed optimization process where an agent is defined as computer systems with autonomous actions that are capable of interacting with each other to satisfy their system objectives a metaheuristic algorithm e g particle swarm optimization genetic algorithm families is fit for this multi agent problem since traditional optimization methods developed in the management sciences and operations research require the objective function and constraints of the optimization model to be formulated in an explicit analytical structure whereas for the problem addressed herein the objective function and constraints are implicitly embodied in geomodsim as the river basin management simulation engine within river geodss in addition traditional optimization methods perform sequences of point by point searches in the decision space to iteratively reduce the objective function starting from some selected initial solution which for complex nonconvex problems will most likely converge to less desirable local optima the mutation linear particle swarm optimization mlpso metaheuristic algorithm a variant of particle swarm optimization pso developed by bonyadi and michalewicz 2015 is applied herein in a multi agent optimization framework fig 4 for determining optimal fuzzy operating rules for diversion to or release from a proposed storage account in jmr that meets water law demands and minimizes the size of the storage account the optimization agent mlpso iteratively conveys the means of fuzzy consequences of the decision variables to the fuzzy rule based system which in turn generates fuzzy reservoir operating rules these fuzzy rules are then input into river geodss for accurate simulation and evaluation of the impacts of those operating rules on 1 irrigation diversions based on decreed water rights 2 flow exchanges between the aquifer and streams 3 evaluation of available river flows based on simulated impacts of the various bmp implementations 4 evaluation of compliance with the colorado kansas interstate compact agreement and 5 weekly simulation of the operation of the storage account in jmr including evaluation of the maximum storage in the account over the simulation period this maximum storage represents the required capacity of the account for a particular bmp implementation the geomodsim river basin management model imbedded in river geodss employs a highly efficient river basin network flow optimization model guaranteeing that decreed water right priorities are adhered to based on available flows in the arkansas river the available flows are directly impacted by the simulated stream aquifer interactions influenced by the bmp implementations in addition river flows in the dsr of the larb also are influenced by storage account operations in jmr information returned to the mlpso agent by river geodss includes the current evaluation of the objective function value calculated based on total stateline flow deficits for each bmp implementation plus an arbitrary penalty term as an indirect means of minimizing the account capacity required for each bmp implementation the intelligent agent is essentially engaging in a learning process with the goal of converging to the best decisions that minimize the objective function the multi agent optimization algorithm is applied to all 75 considered scenarios one baseline scenario and 74 alternative bmps as simulated in river geodss these bmps are the same water bmps assessed by morway et al 2013 shultz et al 2018b and rohmat et al 2019 the merits of which are discussed in these papers shultz et al 2018b modeled combinations of these water bmps along with land management bmps i e enhancing the riparian buffer adjacent to the river and tributaries and reducing fertilizer applications land management bmps have no impact on river diversions and return flows and hence are not considered here 2 2 bmp flow alteration problem and john martin storage account operations it is assumed in this study that portions of the un diverted flows resulting from the adoption of water bmps in the irrigation season upstream of jmr could be diverted into the reservoir storage account for augmenting downstream flows in order to assure full compliance with the compact irrigators downstream of the reservoir also would benefit from releases from the storage account by ensured satisfaction of water rights with implementation of the bmps releases from the storage account are essentially bookkeeping releases where the account is debited by the portion of the jmr physical release that is attributed to drawdown of the storage account this study does not aim toward the construction of a new on stream or off stream reservoir a lengthy bureaucratically cumbersome and costly endeavor instead the storage account and its operational policies proposed in this study will reside within the existing jmr and will augment existing operational rules of the reservoir 2 3 optimization model definition the objective function for the optimization model is defined as 1 min f x t 1 t d e f i c i t t w s t o r a g e a c c o u n t s i z e where the decision vector x represents the decision variables d e f i c i t t b t q t if b t q t 0 otherwise where b t is the average historical baseline flow in week t in compliance with the compact agreement m³ s q t is the geomodsim simulated average flow rate m³ s at the stateline under specified bmp implementations m³ s and s t o r a g e a c c o u n t s i z e max t 1 t s t is the maximum value of storage s t in the reservoir account calculated during execution of geomodsim over t weekly time steps based on assumed initial storage levels in the account although the primary objective is to minimize total deficits consideration is given to determining the minimum storage account size allowing implementation of the bmps hence a term is added for penalizing high storage account capacities using an assigned penalty weight w instead of laying the problem in a multi objective setup instead of using the actuations of the modeled system i e diversion to or release from storage as the decision variables in reservoir simulation this study adopts an approach that aligns with other policy based reservoir operation approaches e g parameterization simulation optimization koutsoyiannis and economou 2003 and direct policy search giuliani et al 2016 koutsoyiannis and economou 2003 used an optimized combination of parameterized inflow system operations and randomness to determine releases from multiple reservoirs in a multi objective setup while giuliani et al 2016 searched through the parameter space of the parameterized operating policy to optimize the expected long term costs the similar aim here is to replace many reservoir control variables with relatively lower dimensional parameterized rules while producing solutions that are not inferior to the benchmarked high dimensional perfect foresight method in this study a fuzzy rule based system is used to generate the weekly actuations of the storage accounts the optimizer engine optimizes the means b n of the fuzzy consequences of n 1 n overlapping fuzzy rules to find the optimal fuzzy reservoir rules q t i t s t t 1 t conditioned on reservoir inflows i t during week t along with s t at the beginning of week t the means of the fuzzy rule consequences define the fuzzy reservoir operating rules which are evaluated using geomodsim simulated volumes in the jmr storage account occurring over the t weekly time steps with the maximum volume representing the storage account capacity millions of m³ required for implementing selected bmps for this study a sensitivity analysis was performed for the penalty weight w in eq 1 which indicated a desirable range between 0 01 and 1 5 with values outside the range resulting in failure of the algorithm to converge and with w 0 2 appearing to provide the correct balance between minimizing total deficits and minimizing the required storage account size the optimization problem can be concisely defined as 2 f i n d x s r d s u c h t h a t x s f x f x where s is the d dimensional search space as a subset of r d euclidean space x and x are d dimensional vectors where the starred term denotes the optimal vector and f is the objective function to be minimized bonyadi and michalewicz 2017 again rather than an explicit closed form function f is implicitly defined using the geomodsim river basin network flow model within river geodss where the agent i e the mlpso metaheuristic algorithm sends proposed fuzzy operating rules x described subsequently to geomodsim which then simulates the impacts of these operating rules on stream aquifer system exchanges downstream river flows and flow deficits at the stateline as depicted in fig 4 it should be noted that although geomodsim guarantees that water is delivered according to the decreed water right priorities certain aggressive bmps may result in junior water right holders failing to fully receive the diversion amounts that were historically delivered thereby violating the water rights of those irrigators the current value of the objective function representing total stateline flow deficits over all time steps are returned to the mlpso agent which then determines a new improved set of operating rules and the iterative process continues until convergence when all flow deficits are minimized with geomodsim maintaining compliance with colorado water rights for a range of bmps details on the original pso standard particle swam optimization spso and mlpso are provided in supplementary material 3 optimal reservoir operating rules based on fuzzy logic fuzzy logic was first introduced by zadeh 1965 as a contribution to information and control theory in contrast with probability measure theory fuzzy logic addresses uncertainty in the form of vagueness or subjectivity where the classes of objects under consideration do not always have precisely defined criteria of membership klir and folger 1988 for example the class of cars made in february 1999 has a fairly crisp membership function for cars made between 1 february 1999 and 28 february 1999 excluding any other dates this is usually termed a classical or crisp set on the other hand there is a loose definition of old cars or new cars where both the bounds and the degree of membership of the age range are vague fuzzy sets are designed to accommodate membership uncertainty by using a certain measurable property of an object to assign a degree of membership of that object in a set with the degree of membership usually varying continuously between 0 and 1 another characteristic of fuzzy sets is that an object can belong to multiple classes as opposed to membership in a conventional set where the exclusivity rule is applied tayfur 2014 an example of a fuzzy set is illustrated in fig 5 where a reservoir inflow value could belong to more than one set with varying degrees of membership values as well as with 0 membership values in sets that it does not belong to notice that fuzzy sets are also useful for quantitative description of verbal statements or linguistically described concepts e g the x likelihood or degree of truth that an inflow value belongs to the low flow fuzzy set and with y likelihood of it belonging to the moderately low flow fuzzy set in fuzzy rule based systems with operational policies defined by fuzzy logic rules outputs or actuations of the system are inferred from a series of fuzzy logic processed inputs there are several variations in fuzzy logic system processing such as that developed by mamdani 1977 sequentially perform fuzzification inference and defuzzification steps the general structure of a fuzzy rule n is 3 i f a 1 i s a n 1 a 2 i s a n 2 a k i s a n k t h e n b n where the operator refers to the and or or xor exclusive or operator with the arguments in the if rule premises assumed to belong to fuzzy sets and with the then consequence also belonging to a fuzzy set bogardi et al 2003 the if premises represent the fuzzification step a i i s a n i with the operator with the then consequence comprising the inference step which is then followed by defuzzification of the resulting b n fuzzy set of consequences to a crisp value note that a major characteristic of a fuzzy rule based systems is that for a given set of inputs multiple rules can be activated but at varying degrees of fulfillment there are variations in the selection and interpretation of the operator where the most commonly used operator is the and operator interpreted as fuzzy product rule which is employed in this study such as 4 i f i t i s a l 1 a n d s t i s a m 2 t h e n q t i s b n where i t and s t are the reservoir inflows during week t and reservoir storage at the beginning of week t respectively with the given initial storage specified as s 0 a l 1 is the fuzzy set of inflow premises for rules l 1 n l a m 2 is the fuzzy set of the storage premises for rules m 1 n m and b n is the fuzzy set of release consequences q t for rules n 1 n n where n n n l n m as illustrated in fig 5 note that the number of discrete fuzzy rules n l and n m are predetermined with n n equaling d or the dimension in the mlpso search space although there are several defuzzification methods e g centroid and center of sets defuzzification mendel 2017 for this study the defuzzified actuation of q t is calculated by the normed weighted sum combination method 5 q t i t s t n 1 n n μ a l 1 i t μ a m 2 s t b n n 1 n n μ a l 1 i t μ a m 2 s t where b n is the mean of the fuzzy consequence rule b n which in relation to the mlpso is the optimized position vector x b 1 b n n this method was selected for its widespread use and computational simplicity bardossy et al 1995 mendel 2017 4 application of multi agent optimization algorithm to the larb as stipulated by the u s supreme court littleworth 2008 the hydrologic institutional model h i model is used by the colorado division of water resources colorado department of natural resources 2020 to determine whether flows at the colorado kansas stateline during a certain period are in compliance with the arkansas river compact since it would be difficult to directly incorporate the h i model into river geodss and to predict future larb hydrologic conditions with confidence a conservative approach is adopted herein for ascertaining if implementation of various proposed bmps along with optimal sizing and operating rules for the jmr storage account would result in compact compliance this approach is based on utilizing actual measured flows at the colorado kansas stateline over the historical period 1999 to 2009 which has been documented as a period when colorado was in full compliance with the compact fortunately this 11 year period included some of the wettest and driest years in the long term hydrologic records for the larb since the h i model utilizes the calculation of 1 year lagged running 10 year annual averages of flows at the stateline to determine compliance a conservative approximation of this procedure as adopted herein is to execute geomodsim using the optimal fuzzy operating rules for each modeled bmp scenario compliance is then guaranteed if the net of each annual average surplus flow computed over all weeks where surplus flow occurred and the annual average deficit flow relative to the compact compliant historical annual average flows is greater than or equal to zero over that period in modeling the bmp scenarios geomodsim is first run in historical or baseline mode where all hydrological variables including releases from jmr and diversions to irrigation canals are taken from historical records based on an accurate well calibrated stream aquifer model and weekly temporal resolution morway et al 2013 shultz et al 2018a 2018b the baseline model was run with temporal range of 575 weekly time steps from the first week of 1999 through the last week of 2009 11 years where it is found that the basin is in compliance with the compact during this period the 74 bmp scenarios were then run using the same period of simulation rohmat et al 2019 the bmps are composed of varying levels of three categories of water bmps cs ri and lease fallowing lf of irrigated lands the multi agent optimization procedure fig 6 is applied to determining optimal operating rules governing diversions from upstream river inflows into a new storage account in jmr and optimal release rules from the storage account back to the river downstream but in such a way as to not interfere with u s army corps of engineers management of jmr during normal and emergency flood conditions based on the objective function for the multi agent optimization of minimizing total deficits the optimal release decisions are designed to ensure that the net of annual average surplus flow and annual average deficit flow in comparison with the historical compact compliant flows at the stateline are positive or zero while also maintaining satisfaction of colorado water rights downstream of the reservoir again under a bmp implementation required diversions from the river to canals to satisfy water rights are reduced due to decreased water losses derived from efficiency improvements in this study the fuzzy mlpso optimizer iterates over the 75 bmp scenarios from river geodss which is comprised of a well calibrated geomodsim surface water model coupled with the emulation of stream aquifer interactions previously simulated with a calibrated modflow sfr2 model using dnns fig 6 the entire process of dnn training and testing including separation of training and testing timeseries as well as measures to avoid overfitting has been described in detail by rohmat et al 2019 the optimizer loads one bmp scenario at a time defines mlpso swarm properties optimizes the fuzzy operating rules governing diversions into and releases flow from the new storage account in jmr in this study an mlpso swarm size of 48 is used for the runs were conducted in parallel on a 16 core machine a swarm size of 32 proved unable to converge better than a size of 48 while a swarm size of 64 and beyond yielded the same performance as 48 albeit with longer computation time the optimization termination criterion is the number of maximum iterations in this study the swarms typically converge to their final positions before iteration number 100 the pso coefficients used follow the values recommended by pedersen 2010 a comparison of performances between spso and mlpso for all modeled bmps is presented in the results and discussion section for interacting with the environment two layers of fuzzy rules are implemented as conditioned on inflows into the reservoir and the initial reservoir storage i e the inflow and storage rules for two parts of the year having distinct flow patterns the irrigation season 15 march to 15 november and the non irrigation season optimal operating rules are determined for each hydrologic state i e wet normal or dry the hydrologic state was determined by ranking the average annual arkansas river inflow to jmr from highest to lowest over the period of record a year having average annual inflow within the top third is classified as wet within the next third as normal and within the bottom third as dry the weekly geomodsim simulated flows at the stateline are calculated based on the optimal operating rules and corresponding optimal sizing of the storage account for each bmp as determined by the multi agent optimization it should be noted that the maximum available weekly diversions to the storage account are limited to the total upstream flows not diverted to irrigation canals i e flows left in the river due to reductions in irrigation requirements associated with bmp implementation the 1999 2009 modeling period is split into optimization 1999 november 2006 and testing portions november 2006 2009 the optimal operating rules were determined for the optimization portion and the optimal rules then were applied to simulate system performance for both the optimization and testing portions the basis for the definition of these two portions is hydrologic year variability where the portion selected for optimization covers years within the three hydrologic classes dry normal and wet rohmat et al 2019 mentioned that the training to population ratio for this dataset should fall between 40 and 80 with a higher ratio giving better testing performance but being more prone to overfitting the ratio of the optimization training portion to the total period is 72 411 weeks out of 575 a higher ratio is not selected since the seven year optimization portion already covers the dry normal wet hydrologic year range the criteria for determining whether an optimized rule set generalizes well are 1 the optimized rule set stores water into the storage account 2 the optimized storage volume in the testing period remains under the maximum storage determined in the optimization period and 3 the average deficit flow at the stateline is reduced 5 results and discussion illustrating the merits of mlpso application fig 7 displays the optimization objective function value improvement between mlpso and spso for all bmps it is generally shown that mlpso acquires a better solution i e lower objective function value as compared to spso with a slower time of convergence by 13 iterations on average out of a maximum of 100 iterations with these solutions undiverted flows are left in the river thereby augmenting arkansas river flows as a direct result of bmp implementations that effectively decrease irrigation requirements due to reduced seepage losses from sealed canals improved irrigation efficiency and removal of irrigation from lease fallowed land as an example for the cs60 bmp i e canal sealing to reduce seepage by 60 fig 8 displays the fuzzy mlpso optimized rules for determining what portion of the additional reservoir inflows as generated by upstream bmp implementation should be diverted to the new storage account for weekly time steps or if releases should be made from the storage account during the current week as conditioned on 1 the total available weekly reservoir inflows as accumulations of upstream un diverted flows due to bmp implementation and 2 the initial storage in the account at the beginning of a weekly time step cs60 is selected for illustration since it is one of the few water bmp implementations along with cs20 and cs40 shown to have a positive impact on both se and no₃ reduction in the larb when combined with reduced fertilizer rf land management bmps shultz et al 2018b the six rules correspond to the specification of optimal operational decisions under wet normal and dry hydrologic conditions during either the irrigation or non irrigation season in order to mitigate the return flow alteration side effects of the cs60 bmp assuming implementation of cs60 the plots in fig 9 a and b show the pattern of simulated weekly average surplus and deficit flows relative to the historic baseline at the stateline for the cases without and with the optimized storage account rules respectively fig 9 c provides a time series plot of storage levels in the new storage account under the optimal fuzzy reservoir operating rules as shown bmp implementation without availability of the new storage account has a significant impact on altering flows at the stateline with deficits particularly present during the off season without the storage account the most effective bmps i e generally those that apply any level of cs alone or in combination with other bmps threaten to result in compact violation fig 10 a and b show respectively the average surplus and deficit patterns at the stateline as aggregated monthly over the 11 year historical period and annually for each year within this period for the cs60 bmp it is seen that implementation of the new optimized storage account substantially reduces stateline depletions with significantly lower deficits present during the off season along with lower surpluses during the irrigation season this behavior reveals the tendency of the optimized storage account operation to divert flow to storage in the irrigation season and to release water during the off season for each year the net of average surplus flow and average deficit flow i e net average surplus flow average deficit flow with the new storage account is always positive indicating compliance with the compact as shown implementation of the cs60 bmp without a new storage account results in a net monthly averaged stateline flow that tends to be negative during the off season with implementation of the new storage account all of the monthly averaged net flows become positive the same behavior is shown for the annual averaged net flows fig 10 b shows that prior to implementation of the new storage account the average net stateline flow in the year 2002 is negative which is of concern however with implementation of the storage account the annual averaged net flow is positive for all years for this bmp indicating full compliance with the compact this same behavior was confirmed for implementation of the fuzzy mlpso optimized rules for all 74 modeled bmps with availability of the storage account as exemplified in the case of cs60 an overall net surplus of flow available at the stateline is estimated for most of the bmps this net surplus is attributed in part to a decrease in non beneficial consumptive use of water from naturally vegetated and fallow lands resulting from bmp implementation due to lowered water table elevations and reduced upflux niemann et al 2011 morway et al 2013 in other words it is expected that bmp implementation not only would result in improved water quality but also in net water conservation within the basin fig 11 summarizes the optimal jmr storage account sizes needed to ensure fulfillment of water right demands and satisfaction of the arkansas river compact for the 74 bmps fig 11 reveals that the more aggressive bmps require larger storage account volumes to offset the detrimental side effects in satisfying water rights and complying with the compact agreement bmp aggressiveness is a qualitative description indicative of the level of effort and investment required to alter management practices for a given bmp in relation to current baseline practices here a higher degree of implementation within a given class of bmps is considered more aggressive than a lower degree e g cs80 is more aggressive than cs60 also a combination of three bmps is considered more aggressive than a combination of two bmps and consequently more aggressive than a single bmp implementation however since the csx lfx and rix bmps are distinctly different the sorting rule between the bmp types is alphabetical the coloring of the bars in fig 11 indicates the relative bmp aggressiveness with a darker color bar used for a more aggressive bmp and a lighter color bar used for a less aggressive bmp nevertheless in all cases the required size of the new storage account to facilitate bmp implementation is less than 5 of the total capacity of jmr table 1 provides summary results for four examples that combine water bmps cs and or ri with rf bmps and which were found by shultz et al 2018b to be among the most effective bmps for reducing both se and no3 n concentrations in the arkansas river and in groundwater presented are the percent reductions in pollutant concentrations expected to result from bmp implementation shultz et al 2018b along with the capacity of the new storage account in jmr required for ensuring arkansas river compact compliance and satisfying water rights the average annual required diversion to and release from the new storage account and the average annual surplus and deficit period flow rates at the stateline the average annual surplus or deficit period flow rate at the stateline with the new account is calculated by summing the volumes of surplus or deficit flow during the respective weeks in which there is a surplus or deficit compared to the baseline scenario and then dividing by the time of surplus or deficit flows the required new storage account capacity is less than 3 of the total available jmr storage capacity for each of these example bmps concerning water rights in the system although implementation of the new storage account eliminates all water right shortages downstream of jmr there remain some small shortages to senior water rights 0 5 and to junior water rights 2 upstream of jmr since these irrigation canals are unable to directly benefit from downstream storage account releases from jmr one possible solution is to explore the development of exchange agreements that can conceptually move water upstream along with possible alteration of upstream pueblo reservoir operations to address this issue but this is left for future work considering the complex chain of data and modeling tools within the river geodss suite fig 6 uncertainty is a significant concern uncertainty in the river geodss simulation is conditioned on the original groundwater stream interaction model the dnn emulation and the water allocation and stream routing model as well as the available data the reservoir storage account sizes and operational policies and the associated uncertainties are in turn conditioned on the pso swarm properties and the river geodss simulation a direct characterization of the sources and nature of these compounded model uncertainties as well as locating and implementing potential measures to reduce them are not addressed here but are left for future work instead an effort has been made herein and in the prequel work upon which this study is founded to ensure reasonable compliance of model results with overall system mass balance and with available field observations 6 summary and conclusions judicious implementation of various strategic water and land management bmps can improve surface and groundwater quality and enhance the sustainability and productivity of irrigated agriculture in the larb however there are legitimate concerns that bmp implementation also would significantly alter diversions from the river and historical stream aquifer flow interchange mechanisms in the alluvial valley resulting in potential violation of senior water rights and a failure to comply with provisions of the colorado kansas interstate compact agreement for the arkansas river this study demonstrates that optimal sizing and operation of a new storage account in john martin reservoir can make it possible to implement beneficial bmps in this challenging regulatory setting storage account operations are modeled using a fuzzy mlpso algorithm applied within the river geodss river basin management model where a fuzzy rule based system is combined with an innovative variant of particle swarm optimization pso that overcomes the drawbacks of the original pso algorithm fuzzy mlpso uses four layers of fuzzy rules conditioned on 1 reservoir inflows available for diversion to the storage account based on additional inflows to john martin reservoir made available by bmp implementation over weekly time steps and 2 initial reservoir storage levels at the beginning of each week along with seasonal changes in flow conditions and annual hydrologic states governing divert to storage release from storage decisions for the new storage account model implementation results in generalized operational policies for managing the new storage account i e weekly divert to storage release from storage feedback decision rules as conditioned on the current storage and available reservoir inflows the optimal reservoir operation policies developed in this study ensure compliance with the arkansas river compact and mostly eliminate shortages in meeting water rights demands because of bmp implementation further study is required to find feasible ways to rid the system of the small water rights shortages that remain along the river upstream of john martin reservoir as expected the more aggressive bmps which achieve greater impacts on the reduction of pollutants lowering of the shallow saline water table and water conservation require larger storage account capacities however even the largest required accounts make up less than 5 of the available reservoir capacity moreover the methods presented here hold out promise for other river basins which are governed by strict prior appropriation laws and interstate compacts but where optimal reservoir operations could facilitate the application of beneficial bmps the inclusion of pueblo reservoir can be further explored to address some of the limitations of the study presented herein since the reservoir is located at the very upstream end of the currently studied system augmenting its operational rules in addition to those of john martin reservoir could address both the remaining small water right deficits and stateline flow delivery issues however modifications of pueblo reservoir operational rules will interact with the rest of the system further altering the equilibrium condition that has been proposed in this paper as well as introducing dimensionality and computational time problems the curse of dimensionality there are methods available to address this problem e g parallel discrete differential dynamic programming pdddp which has been applied in a multi reservoir setup cheng et al 2014 deep multi agent reinforcement learning foerster et al 2016 which presents multiple artificial intelligence agents sensing and acting in an environment to maximize their shared goal as well as multiple ant colony optimization maco gambardella et al 1999 ting and chen 2013 which shares many similarities with pso albeit in a multiple colonies swarm setup to specifically address the computational time problem an adaptive timestep optimization approach could also be proposed i e by solving the optimization problem on a larger timestep and or with suboptimal results being obtained first then progressively moving towards a finer timestep and or improved results cheng et al 2012 fu et al 2020 other future directions could include ways to quantify and better understand the nature of system uncertainties one of which would rely on running multiple realizations of the system with varying model inputs and settings declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors appreciate the financial support for this study provided by the national integrated water quality program of the usda national institute of food and agriculture 2014 51130 22491 and the financial support and cooperation provided by the indonesian endowment fund for education appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2020 104909 
25919,in colorado s lower arkansas river basin larb inefficient irrigation and canal seepage contribute to salinization and waterlogging of irrigated lands and to stream aquifer pollution the geographic information system gis based river basin management model river geodss is applied to further explore best management practices bmps earlier determined to remedy these agro environmental impacts unfortunately bmp benefits are offset by altered irrigation return flows which change historical downstream river flows threatening compliance with water rights and the arkansas river compact compensation is possible through optimal sizing and operation of a dedicated reservoir storage account multi agent optimization combines a metaheuristic mutation linear particle swarm optimization mlpso with a fuzzy rule based system to produce generalized operational policies along with optimal storage sizing to enable bmps while satisfying legal constraints a storage account making up less than 5 of available reservoir capacity can be operated with rules that enable implementation of even the most aggressive bmps keywords irrigation river basin management water law compliance artificial neural networks metaheuristic optimization fuzzy logic 1 introduction 1 1 best management practices for improving water quality in the larb colorado when changes in water management are needed to improve agricultural and environmental conditions in the western united states water law can stand as an impediment this is the case in the lower arkansas river basin larb of southeastern colorado fig 1 the larb contains a central alluvial valley with approximately 14 000 fields covering around 110 000 ha of irrigated farmland the valley is supplied by 25 canals that divert water from the arkansas river along with pumping from about 2400 alluvial groundwater wells all of which directly impact streamflow in the arkansas river and its tributaries most fields are irrigated using surface irrigation methods with only about 20 employing more water efficient sprinkler or drip irrigation methods osborn et al 2017 the larb has long been known for its valuable agricultural production with the introduction of extensive irrigation dating back to the 19th century over the course of time however inefficient irrigation practices and leakage from unlined canals have led to shallow groundwater tables salinization of water and agricultural lands and exacerbation of growing nutrient and trace element pollution of groundwater and surface water resources gates et al 2016 any measures under consideration for agro environmental improvement in the larb are constrained by a complex legal system namely colorado water rights and a transboundary compact the complexity of the problem along with the availability of extensive data makes the larb a unique and exemplary case study this paper presents the latest in a series of inter related investigations to understand and address the need for reducing levels of environmental pollution while ensuring sustainability and productivity of irrigated lands in the larb in a manner compliant with water law broad and long term primary data collection as well as secondary data compiled from agencies such as the united states geological survey usgs the colorado division of water resources and the national oceanic and atmospheric agency noaa gates et al 2016 have led to calibrated and tested computational models morway et al 2013 bailey et al 2014 shultz et al 2018a tavakoli kivi et al 2019 that were applied to two representative larb regions the upstream study region usr and downstream study region dsr located along stretches of the central alluvial valley upstream and downstream of john martin reservoir jmr respectively fig 1 these coupled groundwater streamflow and reactive solute transport models are based on the modflow sfr2 niswonger and prudic 2010 uzf rt3d clement et al 1998 clement and johnson 2002 bailey et al 2013 and otis models runkel 1998 succeeding studies employed these models to investigate various proposed best management practices bmps with the potential for improving water quality while also conserving water in the basin morway et al 2013 bailey et al 2015a 2015b shultz et al 2018b the bmps evaluated in these studies include combinations of incremental levels of reduced irrigation water application placement of water from a canal or pumped source to the surface of cropped fields decreased canal seepage lease fallowing of irrigated fields reduced fertilizer application and improved riparian buffers impacts of bmp implementations simulated in these previous studies have indicated good prospects for lowering of shallow saline water tables for reductions in selenium se nitrate no3 and salt concentrations and for conserving water the implications of bmp implementations to river flows at the basin scale have been studied through the machine learning based coupling of surface water groundwater models triana et al 2010a rohmat et al 2019 although found to be environmentally advantageous basin wide adoption of bmps nevertheless also were found to considerably modify arkansas river flow patterns which could potentially violate legal constraints in the basin rohmat et al 2019 1 2 implementation of bmps water rights irrigation water accounting and interstate compact compliance water bmps considered in pre cursor studies include reduced irrigation ri which upgrades inefficient surface irrigation practices and lessens surface runoff and deep percolation losses and canal sealing cs which decreases water losses from canal seepage if implemented in the larb these bmps would lower the required irrigation canal diversions water taken from the river typically through a headgate into an irrigation canal and would alter patterns of return flow water in excess of consumptive use that returns over the surface or through the subsurface back to the stream system although the bmp implementations have been indicated advantageous for enhancement of the larb their adoption is hindered by the complex legal constraints imposed in the basin based on colorado s water rights system and the arkansas river compact agreement between colorado and kansas concerning apportionment of river flows between the two states colorado revised statutes 1949 these laws prohibit changes to the river system that would alter irrigation diversion and return flow in a manner that results in significant changes to in stream flow conditions thereby threatening to violate both colorado water rights and the interstate compact colorado water law is based upon a strict version of the prior appropriation doctrine that is common in the western united states water rights are decreed by the state for designated beneficial uses at specific locations like irrigation of agricultural land and are administered in order of seniority under a decreed water right irrigated acreage cannot be expanded nor can crop type be significantly altered so as to increase historical consumptive use water evaporated or transpired by the plants thereby lowering the amount of water in the stream system colorado foundation for water education 2004 colorado agricultural water alliance 2008 specifically in the arkansas river basin the irrigation improvement rules promulgated by the colorado state engineer director of the colorado division of water resources and enacted in 2011 prohibit implementation of agricultural efficiency improvements that during some periods reduce return flows and diminish useable flows at the colorado kansas stateline as stipulated in the arkansas river compact colorado division of water resources 2020 thus colorado proves to be an exception to the assumption of perry and steduto 2017 and re asserted by grafton et al 2018 that the typical impact of more efficient irrigation practices in many parts of the world is to increase current consumption and to increase demand for water making scarcity both worse and more difficult to manage on the contrary in colorado any water saved by increased irrigation efficiency must be allowed to remain in or to return to the stream aquifer source for extraction by other water rights holders in order of priority given that irrigation water rights holders cannot increase their consumptive use above historical levels one might wonder why these irrigators would nevertheless consider implementing water bmps in their canals and fields the motivation is twofold first these bmps result in a lowering of shallow water tables thereby reducing waterlogging and salinity which damage crop yields and second these bmps enhance the stream environment and thereby lessen the likelihood of the future imposition of nonpoint source pollution control measures by the state as in the case of colorado regulation 85 as described by colorado ag water quality 2020 rohmat et al 2019 showed that water bmps in the larb generally would result in more net flow in the arkansas river during the irrigation season 15 march 15 november and less water in the non irrigation off season this is because more water efficient irrigation management diverts less water from the stream during the irrigation season while also returning less water through groundwater or surface water in both the irrigation and off seasons fig 2 illustrates water allocation and return flows in an irrigated alluvial valley where canals divert and convey water from the main stream lose some of the diverted water through seepage and apply the remaining amount to the fields while the main river and the tributaries carry un diverted water and receive surface and subsurface return flows the figure shows how the implementation of water bmps would alter flow diversions to canals seepage from canals irrigation applications return flows and in stream flows in comparison to historical baseline conditions for the case where increases in crop consumptive use are prohibited by law storage and release of flows from a reservoir such as jmr in the larb could be used to offset the impact of bmp implementations on in stream flow patterns which vary between irrigation season and off season and which in turn would affect flows available for diversion and flows downstream at the stateline in the case of the larb releases from pueblo reservoir form the upstream boundary condition on the system and are assumed under bmp implementations to remain unchanged from historical conditions river flows at the stateline form the downstream boundary condition and would be altered by changes in diversion and return flow patterns along the valley brought about by bmp implementation and amended by flows released from jmr to further understand the implications of the side effects of proposed bmps at the basin wide scale a decision support system dss modeling tool called river geodss originally developed by triana et al 2010a was significantly upgraded and refined by rohmat et al 2019 river geodss is a dss in a geographical information system gis environment that was partially motivated and developed within the context of describing legally constrained flow allocation in the larb it employs the geomodsim river basin network flow model for streamflow routing and water rights analysis capabilities designed to accommodate complicated exchanges between water right holders transfer of water rights and plans for augmentation similar to those in the larb rohmat et al 2019 provided a major advancement within river geodss by incorporating the scikit learn deep neural networks dnn package pedregosa et al 2011 for accurate compute efficient emulation of modflow sfr2 for simulation of complex stream aquifer interactions and wrapped within a custom georeferenced arcmap environmental systems research institute 2011 extension fig 3 this gis based application was adopted to provide georeferenced data management user interface analysis and presentation that can be combined with any other geospatial dataset results of application of the dnn upgraded river geodss by rohmat et al 2019 show that the 74 alternative bmp scenarios simulated in the study although environmentally beneficial would indeed result in considerable modification of arkansas river flow patterns due in part to influences on stream aquifer system exchanges that detrimentally impact adjudicated water rights and the colorado kansas stateline flows all bmps under consideration would to varying degrees cause harm to senior water rights as well as result in periods of stateline flow deficits that may violate the kansas colorado interstate compact agreement the present study shows however that availability of a new storage account in jmr along with optimal operational strategies for releasing water from the storage account could be the answer to achieving environmental improvements in the basin while still satisfying all legal and administrative statutes triana et al 2010b using an adhoc trial and error approach show that availability of a dedicated storage account in jmr along with alteration of the reservoir operational policies could offer a potential solution to maintaining compliance with the compact with implementation of bmps however determining optimal generalizable operational policies for the storage account was beyond the scope of the study by triana et al 2010a requiring more formalized optimization methodologies in the present paper attention is turned toward finding those policies 1 3 new reservoir storage account to facilitate bmp implementation a storage account to a reservoir is like a bank account to a bank a reservoir can have multiple accounts each with its owners and purposes flow releases are debited from the account and inflows are credited or stored to the account with an associated cap or maximum account size an agent based methodology allowing integration of multi agent optimization with the geomodsim river basin management simulation model is presented herein for determining optimal operational policies for a dedicated storage account in jmr to allow implementation of environmentally beneficial bmps without violating water law such storage accounts are currently being authorized by the colorado kansas arkansas river compact administration as recently reported in the la junta colorado tribune democrat 2019 the colorado kansas arkansas river compact administration passed a resolution 2019 01 on feb 14 jay winner general manager of the lower arkansas valley water conservancy district believes the passing of resolution 2019 01 will open the door for future agreements to allow those in agriculture and other industries to store their water in jmr this is particularly good news according to winner because the reservoir has been underutilized in the past the highly dynamic nature of the optimization of reservoir operations along with the complexity of accurately modeling the spatially distributed stream aquifer system of the larb results in a temporally high dimensional optimization problem since solutions must be obtained over a large number of time steps as the degree of temporal dimensionality increases the likelihood of encountering saddle points that are neither local minima nor maxima in the decision space also increases dauphin et al 2014 point out that optimization within high dimensional decision spaces is prone to stagnation due to saddle points surrounded by high error plateaus that can dramatically slow down the learning process with the probability of occurrence of saddle points growing exponentially with increasing decision space dimension although methods such as dynamic programming dp are effective for solving optimization problems with high temporal dimensionality labadie 2004 it would be difficult to imbed geomodsim along with the dnn as a compute efficient surrogate for the modflow modeling of the complex stream aquifer system of the larb into a recursive dp structure to overcome the high temporal dimensionality of the multi agent optimization a novel hybridization of a variant of a metaheuristic algorithm with fuzzy logic is applied herein for generating robust and generalized reservoir operating policies wan et al 2006 labadie and wan 2010 ostadrahimi et al 2012 and labadie et al 2012 successfully applied other types of hybridization approaches to optimal operation of reservoir systems results of the present study show that appropriated water rights in the basin can be maintained along with adherence to the colorado kansas interstate compact agreement while minimizing requisite storage account capacity this outcome points a way forward in making it possible to adopt bmps for improved water and environmental management without violating state and interstate law 2 methodology 2 1 multi agent optimization with metaheuristic algorithm wooldridge 2009 defines a multi agent optimization as a modeling framework with a collection of agents acting in a distributed optimization process where an agent is defined as computer systems with autonomous actions that are capable of interacting with each other to satisfy their system objectives a metaheuristic algorithm e g particle swarm optimization genetic algorithm families is fit for this multi agent problem since traditional optimization methods developed in the management sciences and operations research require the objective function and constraints of the optimization model to be formulated in an explicit analytical structure whereas for the problem addressed herein the objective function and constraints are implicitly embodied in geomodsim as the river basin management simulation engine within river geodss in addition traditional optimization methods perform sequences of point by point searches in the decision space to iteratively reduce the objective function starting from some selected initial solution which for complex nonconvex problems will most likely converge to less desirable local optima the mutation linear particle swarm optimization mlpso metaheuristic algorithm a variant of particle swarm optimization pso developed by bonyadi and michalewicz 2015 is applied herein in a multi agent optimization framework fig 4 for determining optimal fuzzy operating rules for diversion to or release from a proposed storage account in jmr that meets water law demands and minimizes the size of the storage account the optimization agent mlpso iteratively conveys the means of fuzzy consequences of the decision variables to the fuzzy rule based system which in turn generates fuzzy reservoir operating rules these fuzzy rules are then input into river geodss for accurate simulation and evaluation of the impacts of those operating rules on 1 irrigation diversions based on decreed water rights 2 flow exchanges between the aquifer and streams 3 evaluation of available river flows based on simulated impacts of the various bmp implementations 4 evaluation of compliance with the colorado kansas interstate compact agreement and 5 weekly simulation of the operation of the storage account in jmr including evaluation of the maximum storage in the account over the simulation period this maximum storage represents the required capacity of the account for a particular bmp implementation the geomodsim river basin management model imbedded in river geodss employs a highly efficient river basin network flow optimization model guaranteeing that decreed water right priorities are adhered to based on available flows in the arkansas river the available flows are directly impacted by the simulated stream aquifer interactions influenced by the bmp implementations in addition river flows in the dsr of the larb also are influenced by storage account operations in jmr information returned to the mlpso agent by river geodss includes the current evaluation of the objective function value calculated based on total stateline flow deficits for each bmp implementation plus an arbitrary penalty term as an indirect means of minimizing the account capacity required for each bmp implementation the intelligent agent is essentially engaging in a learning process with the goal of converging to the best decisions that minimize the objective function the multi agent optimization algorithm is applied to all 75 considered scenarios one baseline scenario and 74 alternative bmps as simulated in river geodss these bmps are the same water bmps assessed by morway et al 2013 shultz et al 2018b and rohmat et al 2019 the merits of which are discussed in these papers shultz et al 2018b modeled combinations of these water bmps along with land management bmps i e enhancing the riparian buffer adjacent to the river and tributaries and reducing fertilizer applications land management bmps have no impact on river diversions and return flows and hence are not considered here 2 2 bmp flow alteration problem and john martin storage account operations it is assumed in this study that portions of the un diverted flows resulting from the adoption of water bmps in the irrigation season upstream of jmr could be diverted into the reservoir storage account for augmenting downstream flows in order to assure full compliance with the compact irrigators downstream of the reservoir also would benefit from releases from the storage account by ensured satisfaction of water rights with implementation of the bmps releases from the storage account are essentially bookkeeping releases where the account is debited by the portion of the jmr physical release that is attributed to drawdown of the storage account this study does not aim toward the construction of a new on stream or off stream reservoir a lengthy bureaucratically cumbersome and costly endeavor instead the storage account and its operational policies proposed in this study will reside within the existing jmr and will augment existing operational rules of the reservoir 2 3 optimization model definition the objective function for the optimization model is defined as 1 min f x t 1 t d e f i c i t t w s t o r a g e a c c o u n t s i z e where the decision vector x represents the decision variables d e f i c i t t b t q t if b t q t 0 otherwise where b t is the average historical baseline flow in week t in compliance with the compact agreement m³ s q t is the geomodsim simulated average flow rate m³ s at the stateline under specified bmp implementations m³ s and s t o r a g e a c c o u n t s i z e max t 1 t s t is the maximum value of storage s t in the reservoir account calculated during execution of geomodsim over t weekly time steps based on assumed initial storage levels in the account although the primary objective is to minimize total deficits consideration is given to determining the minimum storage account size allowing implementation of the bmps hence a term is added for penalizing high storage account capacities using an assigned penalty weight w instead of laying the problem in a multi objective setup instead of using the actuations of the modeled system i e diversion to or release from storage as the decision variables in reservoir simulation this study adopts an approach that aligns with other policy based reservoir operation approaches e g parameterization simulation optimization koutsoyiannis and economou 2003 and direct policy search giuliani et al 2016 koutsoyiannis and economou 2003 used an optimized combination of parameterized inflow system operations and randomness to determine releases from multiple reservoirs in a multi objective setup while giuliani et al 2016 searched through the parameter space of the parameterized operating policy to optimize the expected long term costs the similar aim here is to replace many reservoir control variables with relatively lower dimensional parameterized rules while producing solutions that are not inferior to the benchmarked high dimensional perfect foresight method in this study a fuzzy rule based system is used to generate the weekly actuations of the storage accounts the optimizer engine optimizes the means b n of the fuzzy consequences of n 1 n overlapping fuzzy rules to find the optimal fuzzy reservoir rules q t i t s t t 1 t conditioned on reservoir inflows i t during week t along with s t at the beginning of week t the means of the fuzzy rule consequences define the fuzzy reservoir operating rules which are evaluated using geomodsim simulated volumes in the jmr storage account occurring over the t weekly time steps with the maximum volume representing the storage account capacity millions of m³ required for implementing selected bmps for this study a sensitivity analysis was performed for the penalty weight w in eq 1 which indicated a desirable range between 0 01 and 1 5 with values outside the range resulting in failure of the algorithm to converge and with w 0 2 appearing to provide the correct balance between minimizing total deficits and minimizing the required storage account size the optimization problem can be concisely defined as 2 f i n d x s r d s u c h t h a t x s f x f x where s is the d dimensional search space as a subset of r d euclidean space x and x are d dimensional vectors where the starred term denotes the optimal vector and f is the objective function to be minimized bonyadi and michalewicz 2017 again rather than an explicit closed form function f is implicitly defined using the geomodsim river basin network flow model within river geodss where the agent i e the mlpso metaheuristic algorithm sends proposed fuzzy operating rules x described subsequently to geomodsim which then simulates the impacts of these operating rules on stream aquifer system exchanges downstream river flows and flow deficits at the stateline as depicted in fig 4 it should be noted that although geomodsim guarantees that water is delivered according to the decreed water right priorities certain aggressive bmps may result in junior water right holders failing to fully receive the diversion amounts that were historically delivered thereby violating the water rights of those irrigators the current value of the objective function representing total stateline flow deficits over all time steps are returned to the mlpso agent which then determines a new improved set of operating rules and the iterative process continues until convergence when all flow deficits are minimized with geomodsim maintaining compliance with colorado water rights for a range of bmps details on the original pso standard particle swam optimization spso and mlpso are provided in supplementary material 3 optimal reservoir operating rules based on fuzzy logic fuzzy logic was first introduced by zadeh 1965 as a contribution to information and control theory in contrast with probability measure theory fuzzy logic addresses uncertainty in the form of vagueness or subjectivity where the classes of objects under consideration do not always have precisely defined criteria of membership klir and folger 1988 for example the class of cars made in february 1999 has a fairly crisp membership function for cars made between 1 february 1999 and 28 february 1999 excluding any other dates this is usually termed a classical or crisp set on the other hand there is a loose definition of old cars or new cars where both the bounds and the degree of membership of the age range are vague fuzzy sets are designed to accommodate membership uncertainty by using a certain measurable property of an object to assign a degree of membership of that object in a set with the degree of membership usually varying continuously between 0 and 1 another characteristic of fuzzy sets is that an object can belong to multiple classes as opposed to membership in a conventional set where the exclusivity rule is applied tayfur 2014 an example of a fuzzy set is illustrated in fig 5 where a reservoir inflow value could belong to more than one set with varying degrees of membership values as well as with 0 membership values in sets that it does not belong to notice that fuzzy sets are also useful for quantitative description of verbal statements or linguistically described concepts e g the x likelihood or degree of truth that an inflow value belongs to the low flow fuzzy set and with y likelihood of it belonging to the moderately low flow fuzzy set in fuzzy rule based systems with operational policies defined by fuzzy logic rules outputs or actuations of the system are inferred from a series of fuzzy logic processed inputs there are several variations in fuzzy logic system processing such as that developed by mamdani 1977 sequentially perform fuzzification inference and defuzzification steps the general structure of a fuzzy rule n is 3 i f a 1 i s a n 1 a 2 i s a n 2 a k i s a n k t h e n b n where the operator refers to the and or or xor exclusive or operator with the arguments in the if rule premises assumed to belong to fuzzy sets and with the then consequence also belonging to a fuzzy set bogardi et al 2003 the if premises represent the fuzzification step a i i s a n i with the operator with the then consequence comprising the inference step which is then followed by defuzzification of the resulting b n fuzzy set of consequences to a crisp value note that a major characteristic of a fuzzy rule based systems is that for a given set of inputs multiple rules can be activated but at varying degrees of fulfillment there are variations in the selection and interpretation of the operator where the most commonly used operator is the and operator interpreted as fuzzy product rule which is employed in this study such as 4 i f i t i s a l 1 a n d s t i s a m 2 t h e n q t i s b n where i t and s t are the reservoir inflows during week t and reservoir storage at the beginning of week t respectively with the given initial storage specified as s 0 a l 1 is the fuzzy set of inflow premises for rules l 1 n l a m 2 is the fuzzy set of the storage premises for rules m 1 n m and b n is the fuzzy set of release consequences q t for rules n 1 n n where n n n l n m as illustrated in fig 5 note that the number of discrete fuzzy rules n l and n m are predetermined with n n equaling d or the dimension in the mlpso search space although there are several defuzzification methods e g centroid and center of sets defuzzification mendel 2017 for this study the defuzzified actuation of q t is calculated by the normed weighted sum combination method 5 q t i t s t n 1 n n μ a l 1 i t μ a m 2 s t b n n 1 n n μ a l 1 i t μ a m 2 s t where b n is the mean of the fuzzy consequence rule b n which in relation to the mlpso is the optimized position vector x b 1 b n n this method was selected for its widespread use and computational simplicity bardossy et al 1995 mendel 2017 4 application of multi agent optimization algorithm to the larb as stipulated by the u s supreme court littleworth 2008 the hydrologic institutional model h i model is used by the colorado division of water resources colorado department of natural resources 2020 to determine whether flows at the colorado kansas stateline during a certain period are in compliance with the arkansas river compact since it would be difficult to directly incorporate the h i model into river geodss and to predict future larb hydrologic conditions with confidence a conservative approach is adopted herein for ascertaining if implementation of various proposed bmps along with optimal sizing and operating rules for the jmr storage account would result in compact compliance this approach is based on utilizing actual measured flows at the colorado kansas stateline over the historical period 1999 to 2009 which has been documented as a period when colorado was in full compliance with the compact fortunately this 11 year period included some of the wettest and driest years in the long term hydrologic records for the larb since the h i model utilizes the calculation of 1 year lagged running 10 year annual averages of flows at the stateline to determine compliance a conservative approximation of this procedure as adopted herein is to execute geomodsim using the optimal fuzzy operating rules for each modeled bmp scenario compliance is then guaranteed if the net of each annual average surplus flow computed over all weeks where surplus flow occurred and the annual average deficit flow relative to the compact compliant historical annual average flows is greater than or equal to zero over that period in modeling the bmp scenarios geomodsim is first run in historical or baseline mode where all hydrological variables including releases from jmr and diversions to irrigation canals are taken from historical records based on an accurate well calibrated stream aquifer model and weekly temporal resolution morway et al 2013 shultz et al 2018a 2018b the baseline model was run with temporal range of 575 weekly time steps from the first week of 1999 through the last week of 2009 11 years where it is found that the basin is in compliance with the compact during this period the 74 bmp scenarios were then run using the same period of simulation rohmat et al 2019 the bmps are composed of varying levels of three categories of water bmps cs ri and lease fallowing lf of irrigated lands the multi agent optimization procedure fig 6 is applied to determining optimal operating rules governing diversions from upstream river inflows into a new storage account in jmr and optimal release rules from the storage account back to the river downstream but in such a way as to not interfere with u s army corps of engineers management of jmr during normal and emergency flood conditions based on the objective function for the multi agent optimization of minimizing total deficits the optimal release decisions are designed to ensure that the net of annual average surplus flow and annual average deficit flow in comparison with the historical compact compliant flows at the stateline are positive or zero while also maintaining satisfaction of colorado water rights downstream of the reservoir again under a bmp implementation required diversions from the river to canals to satisfy water rights are reduced due to decreased water losses derived from efficiency improvements in this study the fuzzy mlpso optimizer iterates over the 75 bmp scenarios from river geodss which is comprised of a well calibrated geomodsim surface water model coupled with the emulation of stream aquifer interactions previously simulated with a calibrated modflow sfr2 model using dnns fig 6 the entire process of dnn training and testing including separation of training and testing timeseries as well as measures to avoid overfitting has been described in detail by rohmat et al 2019 the optimizer loads one bmp scenario at a time defines mlpso swarm properties optimizes the fuzzy operating rules governing diversions into and releases flow from the new storage account in jmr in this study an mlpso swarm size of 48 is used for the runs were conducted in parallel on a 16 core machine a swarm size of 32 proved unable to converge better than a size of 48 while a swarm size of 64 and beyond yielded the same performance as 48 albeit with longer computation time the optimization termination criterion is the number of maximum iterations in this study the swarms typically converge to their final positions before iteration number 100 the pso coefficients used follow the values recommended by pedersen 2010 a comparison of performances between spso and mlpso for all modeled bmps is presented in the results and discussion section for interacting with the environment two layers of fuzzy rules are implemented as conditioned on inflows into the reservoir and the initial reservoir storage i e the inflow and storage rules for two parts of the year having distinct flow patterns the irrigation season 15 march to 15 november and the non irrigation season optimal operating rules are determined for each hydrologic state i e wet normal or dry the hydrologic state was determined by ranking the average annual arkansas river inflow to jmr from highest to lowest over the period of record a year having average annual inflow within the top third is classified as wet within the next third as normal and within the bottom third as dry the weekly geomodsim simulated flows at the stateline are calculated based on the optimal operating rules and corresponding optimal sizing of the storage account for each bmp as determined by the multi agent optimization it should be noted that the maximum available weekly diversions to the storage account are limited to the total upstream flows not diverted to irrigation canals i e flows left in the river due to reductions in irrigation requirements associated with bmp implementation the 1999 2009 modeling period is split into optimization 1999 november 2006 and testing portions november 2006 2009 the optimal operating rules were determined for the optimization portion and the optimal rules then were applied to simulate system performance for both the optimization and testing portions the basis for the definition of these two portions is hydrologic year variability where the portion selected for optimization covers years within the three hydrologic classes dry normal and wet rohmat et al 2019 mentioned that the training to population ratio for this dataset should fall between 40 and 80 with a higher ratio giving better testing performance but being more prone to overfitting the ratio of the optimization training portion to the total period is 72 411 weeks out of 575 a higher ratio is not selected since the seven year optimization portion already covers the dry normal wet hydrologic year range the criteria for determining whether an optimized rule set generalizes well are 1 the optimized rule set stores water into the storage account 2 the optimized storage volume in the testing period remains under the maximum storage determined in the optimization period and 3 the average deficit flow at the stateline is reduced 5 results and discussion illustrating the merits of mlpso application fig 7 displays the optimization objective function value improvement between mlpso and spso for all bmps it is generally shown that mlpso acquires a better solution i e lower objective function value as compared to spso with a slower time of convergence by 13 iterations on average out of a maximum of 100 iterations with these solutions undiverted flows are left in the river thereby augmenting arkansas river flows as a direct result of bmp implementations that effectively decrease irrigation requirements due to reduced seepage losses from sealed canals improved irrigation efficiency and removal of irrigation from lease fallowed land as an example for the cs60 bmp i e canal sealing to reduce seepage by 60 fig 8 displays the fuzzy mlpso optimized rules for determining what portion of the additional reservoir inflows as generated by upstream bmp implementation should be diverted to the new storage account for weekly time steps or if releases should be made from the storage account during the current week as conditioned on 1 the total available weekly reservoir inflows as accumulations of upstream un diverted flows due to bmp implementation and 2 the initial storage in the account at the beginning of a weekly time step cs60 is selected for illustration since it is one of the few water bmp implementations along with cs20 and cs40 shown to have a positive impact on both se and no₃ reduction in the larb when combined with reduced fertilizer rf land management bmps shultz et al 2018b the six rules correspond to the specification of optimal operational decisions under wet normal and dry hydrologic conditions during either the irrigation or non irrigation season in order to mitigate the return flow alteration side effects of the cs60 bmp assuming implementation of cs60 the plots in fig 9 a and b show the pattern of simulated weekly average surplus and deficit flows relative to the historic baseline at the stateline for the cases without and with the optimized storage account rules respectively fig 9 c provides a time series plot of storage levels in the new storage account under the optimal fuzzy reservoir operating rules as shown bmp implementation without availability of the new storage account has a significant impact on altering flows at the stateline with deficits particularly present during the off season without the storage account the most effective bmps i e generally those that apply any level of cs alone or in combination with other bmps threaten to result in compact violation fig 10 a and b show respectively the average surplus and deficit patterns at the stateline as aggregated monthly over the 11 year historical period and annually for each year within this period for the cs60 bmp it is seen that implementation of the new optimized storage account substantially reduces stateline depletions with significantly lower deficits present during the off season along with lower surpluses during the irrigation season this behavior reveals the tendency of the optimized storage account operation to divert flow to storage in the irrigation season and to release water during the off season for each year the net of average surplus flow and average deficit flow i e net average surplus flow average deficit flow with the new storage account is always positive indicating compliance with the compact as shown implementation of the cs60 bmp without a new storage account results in a net monthly averaged stateline flow that tends to be negative during the off season with implementation of the new storage account all of the monthly averaged net flows become positive the same behavior is shown for the annual averaged net flows fig 10 b shows that prior to implementation of the new storage account the average net stateline flow in the year 2002 is negative which is of concern however with implementation of the storage account the annual averaged net flow is positive for all years for this bmp indicating full compliance with the compact this same behavior was confirmed for implementation of the fuzzy mlpso optimized rules for all 74 modeled bmps with availability of the storage account as exemplified in the case of cs60 an overall net surplus of flow available at the stateline is estimated for most of the bmps this net surplus is attributed in part to a decrease in non beneficial consumptive use of water from naturally vegetated and fallow lands resulting from bmp implementation due to lowered water table elevations and reduced upflux niemann et al 2011 morway et al 2013 in other words it is expected that bmp implementation not only would result in improved water quality but also in net water conservation within the basin fig 11 summarizes the optimal jmr storage account sizes needed to ensure fulfillment of water right demands and satisfaction of the arkansas river compact for the 74 bmps fig 11 reveals that the more aggressive bmps require larger storage account volumes to offset the detrimental side effects in satisfying water rights and complying with the compact agreement bmp aggressiveness is a qualitative description indicative of the level of effort and investment required to alter management practices for a given bmp in relation to current baseline practices here a higher degree of implementation within a given class of bmps is considered more aggressive than a lower degree e g cs80 is more aggressive than cs60 also a combination of three bmps is considered more aggressive than a combination of two bmps and consequently more aggressive than a single bmp implementation however since the csx lfx and rix bmps are distinctly different the sorting rule between the bmp types is alphabetical the coloring of the bars in fig 11 indicates the relative bmp aggressiveness with a darker color bar used for a more aggressive bmp and a lighter color bar used for a less aggressive bmp nevertheless in all cases the required size of the new storage account to facilitate bmp implementation is less than 5 of the total capacity of jmr table 1 provides summary results for four examples that combine water bmps cs and or ri with rf bmps and which were found by shultz et al 2018b to be among the most effective bmps for reducing both se and no3 n concentrations in the arkansas river and in groundwater presented are the percent reductions in pollutant concentrations expected to result from bmp implementation shultz et al 2018b along with the capacity of the new storage account in jmr required for ensuring arkansas river compact compliance and satisfying water rights the average annual required diversion to and release from the new storage account and the average annual surplus and deficit period flow rates at the stateline the average annual surplus or deficit period flow rate at the stateline with the new account is calculated by summing the volumes of surplus or deficit flow during the respective weeks in which there is a surplus or deficit compared to the baseline scenario and then dividing by the time of surplus or deficit flows the required new storage account capacity is less than 3 of the total available jmr storage capacity for each of these example bmps concerning water rights in the system although implementation of the new storage account eliminates all water right shortages downstream of jmr there remain some small shortages to senior water rights 0 5 and to junior water rights 2 upstream of jmr since these irrigation canals are unable to directly benefit from downstream storage account releases from jmr one possible solution is to explore the development of exchange agreements that can conceptually move water upstream along with possible alteration of upstream pueblo reservoir operations to address this issue but this is left for future work considering the complex chain of data and modeling tools within the river geodss suite fig 6 uncertainty is a significant concern uncertainty in the river geodss simulation is conditioned on the original groundwater stream interaction model the dnn emulation and the water allocation and stream routing model as well as the available data the reservoir storage account sizes and operational policies and the associated uncertainties are in turn conditioned on the pso swarm properties and the river geodss simulation a direct characterization of the sources and nature of these compounded model uncertainties as well as locating and implementing potential measures to reduce them are not addressed here but are left for future work instead an effort has been made herein and in the prequel work upon which this study is founded to ensure reasonable compliance of model results with overall system mass balance and with available field observations 6 summary and conclusions judicious implementation of various strategic water and land management bmps can improve surface and groundwater quality and enhance the sustainability and productivity of irrigated agriculture in the larb however there are legitimate concerns that bmp implementation also would significantly alter diversions from the river and historical stream aquifer flow interchange mechanisms in the alluvial valley resulting in potential violation of senior water rights and a failure to comply with provisions of the colorado kansas interstate compact agreement for the arkansas river this study demonstrates that optimal sizing and operation of a new storage account in john martin reservoir can make it possible to implement beneficial bmps in this challenging regulatory setting storage account operations are modeled using a fuzzy mlpso algorithm applied within the river geodss river basin management model where a fuzzy rule based system is combined with an innovative variant of particle swarm optimization pso that overcomes the drawbacks of the original pso algorithm fuzzy mlpso uses four layers of fuzzy rules conditioned on 1 reservoir inflows available for diversion to the storage account based on additional inflows to john martin reservoir made available by bmp implementation over weekly time steps and 2 initial reservoir storage levels at the beginning of each week along with seasonal changes in flow conditions and annual hydrologic states governing divert to storage release from storage decisions for the new storage account model implementation results in generalized operational policies for managing the new storage account i e weekly divert to storage release from storage feedback decision rules as conditioned on the current storage and available reservoir inflows the optimal reservoir operation policies developed in this study ensure compliance with the arkansas river compact and mostly eliminate shortages in meeting water rights demands because of bmp implementation further study is required to find feasible ways to rid the system of the small water rights shortages that remain along the river upstream of john martin reservoir as expected the more aggressive bmps which achieve greater impacts on the reduction of pollutants lowering of the shallow saline water table and water conservation require larger storage account capacities however even the largest required accounts make up less than 5 of the available reservoir capacity moreover the methods presented here hold out promise for other river basins which are governed by strict prior appropriation laws and interstate compacts but where optimal reservoir operations could facilitate the application of beneficial bmps the inclusion of pueblo reservoir can be further explored to address some of the limitations of the study presented herein since the reservoir is located at the very upstream end of the currently studied system augmenting its operational rules in addition to those of john martin reservoir could address both the remaining small water right deficits and stateline flow delivery issues however modifications of pueblo reservoir operational rules will interact with the rest of the system further altering the equilibrium condition that has been proposed in this paper as well as introducing dimensionality and computational time problems the curse of dimensionality there are methods available to address this problem e g parallel discrete differential dynamic programming pdddp which has been applied in a multi reservoir setup cheng et al 2014 deep multi agent reinforcement learning foerster et al 2016 which presents multiple artificial intelligence agents sensing and acting in an environment to maximize their shared goal as well as multiple ant colony optimization maco gambardella et al 1999 ting and chen 2013 which shares many similarities with pso albeit in a multiple colonies swarm setup to specifically address the computational time problem an adaptive timestep optimization approach could also be proposed i e by solving the optimization problem on a larger timestep and or with suboptimal results being obtained first then progressively moving towards a finer timestep and or improved results cheng et al 2012 fu et al 2020 other future directions could include ways to quantify and better understand the nature of system uncertainties one of which would rely on running multiple realizations of the system with varying model inputs and settings declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors appreciate the financial support for this study provided by the national integrated water quality program of the usda national institute of food and agriculture 2014 51130 22491 and the financial support and cooperation provided by the indonesian endowment fund for education appendix a supplementary data the following is the supplementary data related to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2020 104909 
