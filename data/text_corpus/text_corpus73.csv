index,text
365,climate change is expected to significantly alter river hydrological regimes throughout the world affecting water resources and the frequency of floods and droughts the objectives of this study were to determine the impacts of climate change and sea level rise on streamflow and floodplain inundation in the subtropical logan albert catchment australia an ensemble of 11 high resolution climate models forced under high representative concentration pathway 8 5 rcp8 5 and intermediate emission rcp4 5 scenarios was applied there was considerable variation from the model ensemble result in projections of major flooding events at 5 and 100 year average recurrence intervals aris the largest events 100 year ari tended towards an increase whereas the smallest 5 year ari tended towards a decrease floodplain inundation from a 100 year ari event increased in all simulations and inclusion of sea level rise resulted in increased floodplain inundation area nearly doubling by the end of the century which has substantial implications for flood risk our study highlights the non linear nature of climate change impacts on streamflow and floodplain inundation demonstrating the need for a comprehensive assessment at the floodplain scale when informing preparedness for future flooding events keywords river high flows flooding sea level rise discharge flood risk 1 introduction climate change is expected to alter hydrological regimes throughout the world with major implications for water resources and hydrological extremes e g floods and droughts blöschl et al 2019 flooding is one of the most costly and widespread climate related natural disasters jonkman 2005 it has major implications for landholders and urban infrastructure and requires costly mitigation measures magnification of the variability of the hydrological cycle due to climate change is expected to result in greater frequency of extreme precipitation throughout most parts of the world groisman et al 2005 trends for the latter half of the 20th century indicate an increase in extreme flood events milly et al 2002 while analysis of smaller floods has suggested a decreasing trend particularly in larger catchments do et al 2017 a number of studies on a global scale have shown that the trend of increasing flood magnitude for the most extreme events will continue over the 21st century for many regions of the world berghuijs et al 2017 gosling et al 2017 hirabayashi et al 2013 wiel et al 2019 in recent years there has been a marked increase in the number of studies assessing the impacts of climate change on flooding and high flow events at the regional and catchment scales gu et al 2018 jin et al 2018 li et al 2016 liu et al 2013 phi hoang et al 2016 tian et al 2016 these studies typically make use of a modelling framework outlined by xu et al 2005 consisting of global circulation model gcm outputs downscaling bias correction techniques and hydrological model applications each step inevitably introduces some degree of error which is compounded sequentially and different combinations of models and techniques can lead to some disparity in results praskievicz and chang 2009 gcms are widely held as the largest source of uncertainty de niel et al 2019 kay et al 2009 and it is recommended that an ensemble of climate models be used for hydrological projections prudhomme and davies 2009 teutschbein and seibert 2010 due to spatiotemporal differences in climate change the hydrological response varies substantially between regions of the world hirabayashi et al 2013 it is therefore important that the impacts of climate change are evaluated across different regions while there has been a marked increase in the number of studies on this subject there remains a paucity of literature for many tropical and subtropical regions including subtropical australia eccles et al 2019 gu et al 2020 reported uncertainty in response to flood magnitudes across subtropical and tropical australia however most increases were concentrated in the tropical north and decreases in the temperate south chen and yu 2015 found little change in large floods magnitudes for two subtropical southeast queensland seq creeks by the 2030s using two climate models however studies based on a limited number of climate projections and predicting less than two decades into the future may not adequately inform preparedness for future flooding events the impacts of climate change on flooding may also be exacerbated by the nonlinear backflow effects of sea level rise in many coastal catchments which has been found to be a major contributor to increased inundation budiyono et al 2016 hamman et al 2016 västilä et al 2010 particularly under storm surge scenarios bilskie et al 2016 bilskie et al 2014 relatively few studies however have considered the combined effects of sea level rise and atmospheric climate change on riverine flooding despite coastal zones being some of the most densely populated areas in the world neumann et al 2015 though studies are available that address other topics such as sediment transport huang et al 2016 such studies require an additional hydrodynamic model as part of the modelling framework to simulate freshwater marine interactions along estuaries and coastal floodplains budiyono et al 2016 västilä et al 2010 sea level rises will also have implications for coastal land cover vegetation and shoreline morphology bilskie et al 2016 passeri et al 2015 saintilan and rogers 2009 which may work to exacerbate flooding in coastal catchments subtropical seq is one of the most flood prone regions in australia abbs et al 2007 and has been identified as one of the hotspots for climate change by the international panel for climate change ipcc hennessy et al 2007 urban development in the region is located along low lying coastal floodplains sea level rise and altered streamflow conditions may therefore act synergistically to intensify flooding the logan albert catchment is the second largest catchment in the region and is anticipated to be subject to significant population growth queensland treasury 2018 potentially increasing population exposure to flood events while land use changes may lead to more severe events this study implemented a modelling framework that integrated downscaled climate projections hydrological modelling and coupled 1 d and 2 d hydrodynamic models to capture coastal inundation from floods the aims were threefold i to assess the hydrological response of a coastal subtropical catchment to future climate change ii to examine how the projected changes in hydrology may impact floodplain inundation and iii to evaluate the synergic effects of atmospheric climate change and sea level rise on floodplain inundation 2 methods 2 1 study area the logan albert catchment has an area of approximately 3862 km2 of which the logan covers around 3080 km2 and the albert 782 km2 fig 1 the two major rivers of each catchment arise from the mcpherson range and converge 11 km from the coast the climate is subtropical with most precipitation in the summer december to february wet season and a dry season in winter june to august major flooding events are usually associated with low pressure systems or ex tropical cyclones and commonly affect the region during the wet season precipitation over the catchment is also spatially variable with 2000 mm yr 1 in the south eastern headwaters and 900 mm yr 1 in the west land use in the upper and middle catchment is dominated by native forest cover and cattle grazing extensive sugarcane plantations are located along the southern floodplain near the river mouth fig s1 logan city located in the lower catchment south of brisbane has a population of 319 000 which is projected to grow by about 50 to 490 000 by 2036 queensland treasury 2018 there are two major dams in the upper logan catchment which attenuate flood events arising from the upper catchment fig 1 additionally two major wastewater treatment plants wwtps are located along the estuarine reaches of the logan and albert rivers respectively and are reliant in part on some baseline levels of flushing to reduce their ecological impact 2 2 data bathymetry data 5 m horizontal resolution of the lower logan and albert rivers was retrieved from logan city council and the queensland maritime service the data was incorporated into a digital elevation model dem of the catchment obtained from geoscience australia https elevation fsdf org au also consisting of a 5 m horizontal resolution land use and soil data for the region were obtained from the queensland government http qldspatial information qld gov au catalogue custom index page daily synthetic pan evaporation data at 5 km spatial resolution was retrieved from the scientific information for land owners silo queensland government 2019 and converted to potential evapotranspiration pet using a pan coefficient value of 0 7 allen et al 1998 daily observed rainfall data was obtained for 56 gauges across the catchment fig 1 from the bureau of meteorology bom http www bom gov au climate data thiessen polygons for all the rainfall gauges were developed and missing values at each gauge were filled using an inverse distance weighting technique de silva et al 2007 daily streamflow data for four stream gauges id 145014a 145102b 145031a and 145018a was obtained from the queensland department of natural resources mines and energy dnrme 2019 daily dam discharge data was retrieved from the queensland government bulk water supply authority and downstream water level data for six gauges was obtained from the bom fig 2 hourly tidal data from the russell island tidal gauge near the river mouth fig 1 was available after 2017 from the queensland maritime service the mike 21 tidal analysis and prediction module dhi 2017c was used to calculate 69 tidal constituents based on the ios method foreman 1977 including 45 constituents of astronomical origin and 24 shallow water constituents tidal constituents were then used to estimate tidal levels over the full calibration and validation periods effects of storm surge and other atmospheric driven components of tidal height were not included in this analysis daily high resolution climate projections from the fifth phase of the coupled model intercomparison project cmip5 multi model database were obtained from the queensland department of environment and science syktus et al 2020 table s2 the climate models were dynamically downscaled by syktus et al 2020 to a 10 km spatial resolution over queensland under the rcp8 5 and rcp4 5 scenarios using the regional conformal cubic atmospheric model these downscaled projections have been used to understand future climate risk and inform regional climate adaptation policy in queensland trancoso et al 2020 the rcp8 5 scenario represents a high emissions scenario with potentially the most significant changes to rainfall representing a worst case scenario it is important to consider high emission pathways when informing natural hazards and disaster preparedness such as extreme floods to avoid neglecting the unknown future risk precipitation and pan evaporation data were retrieved from the 11 climate models for the baseline 1980 2009 and three future periods representing the 2020s 2010 2039 2050s 2040 2069 and the 2080s 2070 2099 precipitation outputs were bias corrected by syktus et al 2020 using a quantile mapping technique piani et al 2010 we adopted the same technique to bias correct pan evaporation outputs using historical synthetic pan evaporation data after which the pan coefficient 0 7 was applied to convert values to pet sea level rise projections for the region under rcp8 5 and rcp4 5 were obtained from the commonwealth scientific and industrial research organisation csiro church et al 2016 3 modelling this study employed a modelling framework consisting of both hydrological and hydrodynamic models first the hydrological model was used to simulate daily streamflow from the logan and albert catchments respectively we then applied coupled 1 d and 2 d hydrodynamic models to estimate inundation along the coastal floodplain from an extreme event 3 1 hydrological model the deterministic lumped and conceptual nedbør afstrømnings model nam dhi 2019 was used to simulate river streamflow nam is a rainfall runoff model which requires inputs of precipitation and pet the model accounts for moisture content in three separate storages namely the surface root zone and groundwater catchment runoff is split into three conceptual components overland flow interflow and baseflow dhi 2019 the nam model has been applied to assess the effects of climate change on flooding for catchments in kenya and ethiopia taye et al 2011 thailand kure and tebakari 2012 supharatid et al 2016 and belgium alam et al 2014 the logan albert catchment was split into four sub catchments during model setup one for each of the two major dams maroon and wyaralong and one for both the logan and albert rivers at the yarrahappini and bromfleet stream gauges respectively fig 1 a mean area weighting technique was adopted to calculate areal precipitation and pet for each sub catchment an initial one year warmup period 2003 was designated in order to get the model operational we calibrated nam during 2004 2014 against daily observed streamflow at the gauges shown in fig 1 and validated nam between 2015 and 2017 model applications to dam sub catchments were calibrated using the nearest available upstream gauge fig 1 as wyaralong dam was impounded mid way through the calibration period december 2010 two model runs were set up using the same parameter values for model calibration one model did not consider the dam and ran until the date of impoundment and the other model which did consider the dam ran thereafter outputs of the first model run initialised the second model run and avoided an additional warmup period in the calibration period prior to impoundment simulated streamflow from the wyaralong dam sub catchment was used in the calibration of the downstream yarrahappini gauge after impoundment measured dam discharge values were applied outputs from the two models were combined to assess the performance of the hydrological model during calibration model performance was evaluated at the bromfleet and yarrahappini gauges using the nash sutcliffe efficiency nse ratio of the root mean square error to the standard deviation of the observed data rsr and percentage bias pbias 3 2 coupled 1 d and 2 d hydrodynamic model mike hydro is a one dimensional numerical river model which applies simplified equations of continuity and momentum saint venant equations to simulate unsteady streamflow dhi 2019 as the model is one dimensional it is assumed that velocity and depth only vary in the longitudinal direction it is also assumed that there is negligible variation in water density that the bottom slope of the channel is small and that the wave lengths are large compared to the water depth so that the streamflow direction is parallel to the bottom dhi 2017a mike hydro was set up using measured streamflow from the yarrahappini and bromfleet gauges as upper boundary conditions and estimated tidal levels near the river mouth as lower boundary conditions fig 1 outputs from the hydrological model were not applied as upstream boundary conditions cross sections were developed manually from the combined topography and bathymetry dem and separated by distances ranging from 97 m to 1825 m depending on stream morphology additional inflows along the principal channels from the ungauged creeks and streams were simulated with nam using one way coupling to mike hydro santiago collazo et al 2019 with parameter values taken from the calibrated upstream basins the hydrodynamic model domain encompassed the lower sections of both the logan and albert rivers where the effects of flooding are most consequential this section of river is the most urbanised bound by extensive floodplains near the river mouth and is expected to undergo significant future population growth queensland treasury 2018 additionally the impacts of sea level rise will likely be greatest along the lower reaches of the river the lower logan albert is bounded by extensive floodplains that inundate regularly during high flow events this type of overland flow is not well suited to 1 d river modelling so a 2 d overland flow model was also set up using mike 21 the mike 21 model is based on the navier stokes equations for velocity pressure temperature and density of river water including the effect of viscosity it has been widely applied for simulations of lakes seas rivers estuaries and floodplains dhi 2017b we coupled together the two models in the form of a lateral link using mike flood fig 2 in this way the 2 d overland model was only employed at times when the cross sectional heights of the 1 d model were exceeded saving on computational power this type of model coupling exercise has rarely been adopted in predicting climate change impacts on hydrology supharatid et al 2016 wu et al 2017 the 2 d model consisted of a flexible mesh allowing for improved resolution in regions of greatest interest and lower resolution in regions unlikely to be flooded or where topography varied little although the minimum element area used in the model was relatively coarse at approximately 4000 m2 land use data for the region was used to estimate spatially variable overland manning s n bed resistance values while soil data was used to estimate spatially variable infiltration rates across the model domain hill et al 2015 te chow 1959 a mean areal pet was applied across the overland model based on historical data which was altered for model runs in the future to reflect climate change rainfall over the overland model was not considered as it had already been considered using the hydrological model input to the 1 d hydrodynamic model a two stage procedure was used to calibrate the hydrodynamic model the 1 d river model was first calibrated and validated between 2004 and 2017 using observed water levels at six downstream gauges shown in fig 2 the calibration principally involved adjusting the manning s n along the length of both rivers we then coupled the calibrated 1 d river model with the 2 d overland model and calibrated and validated the combined model against 2017 and 2013 flood events respectively only three of the six gauges carbrook logan village and waterford could be used for validation due to a lack of data of adequate quality in this period whereas all gauges were used in calibration the simulation period for these events extended over two months to include both normal and high flow conditions model performance was evaluated at the six downstream gauges shown in fig 2 using the nse rsr and pbias of the peak water level in addition to the performance indicators the maximum inundation extent from the 2017 flood event simulation was compared to the measured maximum flood extent 3 3 analyses we considered the effects of climate change on flooding high flows and mean flows at the yarrahappini and bromfleet stream gauges fig 1 flood magnitudes were estimated using the generalised pareto distribution and the peak over threshold pot series which consist of all statistically independent peaks that exceed a chosen threshold keast and ellison 2013 the pot series was considered advantageous over the more commonly adopted annual maxima as it allows for more data to be assessed excludes data from years with no flooding and includes data from years with several floods keast and ellison 2013 we adopted a threshold that was exceeded on average 1 65 times annually stedinger and foufoula georgiou 1993 and followed a technique outlined by lang et al 1999 to ensure consecutive peaks met an assumption of independence 1 θ 5 d a y s l o g a 2 59 o r x m i n 3 4 m i n q 1 q 2 where θ is the minimum time difference between successive flood peaks days a is the catchment area km2 xmin is the minimum streamflow between two peaks m3 s and q1 and q2 are the maximum daily streamflow of the two peaks m3 s relative changes to 5 10 25 50 and 100 year average recurrence interval ari flood events high flows consisting of the top 10 q10 5 q5 and 1 q1 of streamflow and mean flows were calculated at each site for each climate model and for each of the future time periods 2020s 2050s and 2080s by comparing simulated future values to simulated baseline 1980 2009 values we also considered changes to the seasonality of high and mean flows by determining monthly changes for each future time period relative to the values in the baseline considering the effects of the dams during the calibration was important however when simulating the catchment response to the climate models we did not consider the impacts of the dams similar to methods used in other studies gu et al 2018 mohammed et al 2018 phi hoang et al 2016 as we only considered relative changes between baseline and future periods from each of the climate models the effects of the two dams within the model domain were assumed to be minor this assumption would not hold if there were significant spatial variations in rainfall change or if differences in streamflow were assessed rather than relative changes downstream changes to the maximum flood extent and to the inundated area under various water depths resulting from both changing streamflow and sea level rise were evaluated using the coupled hydrodynamic model for this purpose measured streamflow from a 2017 flood which included the attenuating impacts of the dams 100 year ari event ex tropical cyclone debbie was designated as a baseline event dnrme 2019 the measured baseline event was perturbed by the multi model median predicted change to a 100 year ari flood event for each of the future time periods simulated inflows from the downstream creeks and streams used in the baseline model run were likewise perturbed using this multi model median change these altered events represented estimates of the magnitude of a 100 year ari flood and were used as upstream boundaries to drive the coupled hydrodynamic model allowing for a comparison of the inundated area in each of the future periods relative to the inundated area from the baseline event in addition to scenarios considering changes to streamflow three additional scenarios were developed which considered the effects of sea level rise table 1 by varying the downstream boundary conditions based on projections for the region under rcp8 5 and rcp4 5 we applied a normalised nonlinearity index proposed by bilskie et al 2014 as a means to quantify the nonlinear changes to flooding from sea level rise adopting predicted tidal levels as the coastal boundary neglected non astronomic components of the tides including storm surge although storm surges in the region are small 1 m fig s3 compared to those reported in compound flood studies in the usa moftakhari et al 2019 moftakhari et al 2017 an analysis of the dependence between non tidal residuals assumed to be indicative of storm surge and streamflow events in the catchment section s3 of the supplementary materials showed significant dependence between streamflow and non tidal residuals fig s2 additional scenarios were therefore analysed for the 2080s to combine non tidal residuals from ex tropical cyclone debbie of 2017 with predicted tidal levels with and without the impacts of sea level rise 4 results 4 1 model calibration and validation 4 1 1 hydrological model simulated and observed streamflow at the bromfleet and yarrahappini stream gauges during calibration and validation are shown in fig 3 the nse values for the calibration and validation periods at the bromfleet gauge were 0 872 and 0 944 respectively and 0 860 and 0 916 at the yarrahappini gauge respectively indicating very good overall model performance moriasi et al 2007 the performance of the model to simulate high flows was also assessed at these stations using the top 1 of streamflow with nse values of 0 823 and 0 936 obtained at the bromfleet gauge and 0 812 and 0 897 at the yarrahappini gauge during calibration and validation respectively table 2 4 1 2 coupled 1 d and 2 d hydrodynamic model the performance of the coupled hydrodynamic model was evaluated against major flooding events in 2017 and 2013 during calibration and validation respectively at the six downstream water level gauges nse values ranged between 0 938 and 0 973 during calibration during validation only three of the gauges carbrook logan village and waterford were operational and the nse ranged between 0 879 and 0 950 table 3 there was a reasonable match between the simulated and observed flood peaks during both calibration fig 4 and validation as the pbias of the peak water level never exceeded 10 at any gauge during calibration or validation table 3 the pbias of the peak water levels generally indicated a minor overestimation of the flood peak during calibration and an underestimation during validation such differences may relate to uncertainty in the measurement of large streamflow events or to changes in stream morphology that occur following large floods the simulated flood extent can be seen in fig 5 to be a good match to the observed flood extent except for some parts of the lower floodplain where flood extent was underestimated which may relate to inadequate resolution of the mesh 4000 m2 for representing some of the changes in topography 4 2 changes to precipitation and potential evapotranspiration changes in average precipitation pet and the deficit precipitation pet across the catchment under the rcp8 5 model ensemble are presented in fig 6 for the three future time periods relative to the baseline 1980 2010 precipitation in winter and spring was predicted to decrease in the future and this decrease became most apparent by the end of the century 2080s summer precipitation generally showed an opposing pattern of increase which was also greatest by the 2080s this pattern of change is more apparent when looking at the relative changes to precipitation fig s8 these changes represent an amplification of the seasonality of precipitation a decrease during the winter dry season and an increase during the summer wet season for the early century 2020s there was little agreement on the directionality of change in precipitation for any season with most climate models not indicating a clear increase or decrease however by the 2050s and 2080s there was widespread agreement in the model ensemble about the directionality of change in winter spring and summer but not for autumn similar changes are noted under the rcp4 5 fig s9 and fig s10 though the magnitude of the changes was smaller than under the rcp8 5 interestingly despite significant shifts in the projected distribution of precipitation for the different seasons the multi model median change to annual precipitation was predicted to vary only slightly 4 compared to the baseline for the three future time periods under both emission scenarios therefore the precipitation increases in summer are projected to be large enough to offset decreases in winter and spring the largest increases to pet occurred during summer and spring while the smallest increases coincided with autumn and winter fig 6 relative changes to the seasonal distribution of pet however were consistent across all seasons fig s8 by the 2080s under the rcp8 5 the largest increases to the multi model median were predicted in spring 32 4 and the smallest in autumn 27 7 annual pet assessed as the multi model median increased by 4 9 for the 2020s 17 1 for the 2050s and 29 5 for the 2080s compared with more moderate increases under the rcp4 5 of 5 8 10 9 and 14 6 for the three future periods respectively 4 3 hydrological impacts of climate change projections of precipitation and pet from the climate model ensemble were used as inputs to run nam changes to high and mean flows in each of the three future periods were evaluated relative to the baseline at the yarrahappini logan river and bromfleet albert river gauges respectively fig 7 high and mean flows were predicted to decrease significantly in the future under rcp8 5 and these decreases became largest by the end of the century by the 2050s and 2080s most of the climate models indicated decreased streamflow whereas for the 2020s there was no clear indication on the sign of change under rcp4 5 only by the 2080s did most of the climate models indicate decreased streamflow fig s11 multi model median changes to high flows ranged between 0 7 and 6 1 for the 2020s 14 6 to 36 for the 2050s and 15 1 to 51 7 for the 2080s under rcp8 5 depending on the site and streamflow quantile assessed changes were less extreme under rcp4 5 between 0 7 to 9 2 for the 2020s 7 8 to 12 2 for the 2050s and 8 2 to 24 8 by the 2080s different streamflow quantiles gave different magnitudes of change with the largest decreases generally reported for q10 flows and the smallest decreases for q1 flows multi model median decreases to q1 flows ranged between 15 1 to 22 5 and between 46 2 and 49 1 for q10 flows by the 2080s under rcp8 5 indicating that the relative change in the frequency of smaller flow events would decrease more than that of larger events seasonal changes to high and mean flows under the rcp8 5 climate forcing largely reflected the changes for precipitation with the largest decreases occurring during winter and spring months and by the end of the century fig 8 decreases in streamflow however were substantially greater during these months than decreases in precipitation multi model median changes in high and mean flows were predicted to decrease by between 26 3 and 65 in winter 26 1 and 78 7 in spring and 22 7 and 30 7 in autumn by the 2050s depending on the site and streamflow quantile assessed while in summer the comparable changes were between 1 9 and 13 8 multi model median high and mean flows decreased further in winter and spring by the 2080s varying between 52 6 and 83 7 in winter and between 59 and 93 4 in spring for autumn these changes varied between 11 and 40 6 and in summer between 3 and 22 it is important to note that historically streamflow in winter and spring has been considerably lower than in summer and therefore relative changes in these seasons can appear overly large compared with summer summer high and mean flows showed a lower probability of change despite most climate models predicting an increase in summer precipitation by the 2080s fig 6 projected changes under the rcp4 5 fig s12 followed the same seasonal pattern as for the rcp8 5 case but with smaller decreases predicted during the winter and spring months on account of smaller decreases in precipitation and smaller increases in pet changes to the magnitude of different ari flood events are shown in fig 9 for rcp8 5 and in fig s13 for rcp4 5 generally there was no clear agreement amongst the model ensemble on the sign or of change for smaller ari events 5 and 10 year in any of the future periods for larger ari events 50 and 100 year however most of the models predicted increased flood magnitude in all future periods meaning that larger flood events were likely to increase more than smaller events multi model median changes to the 100 year ari flood event under the rcp8 5 were predicted to increase by 38 1 and 24 7 at the albert and logan rivers respectively by the 2080s in contrast 5 year ari flood events were predicted to change by 0 5 and 14 6 respectively in the same time period a similar tendency was noted for high flows where larger flows q1 generated smaller decreases than smaller flows q10 fig 7 there was greater variability in the projections of larger ari events than smaller events evident by the size of the interquartile range somewhat surprisingly there was generally greater variability in projections of future flood quantiles in the 2020s than in the 2050s and 2080s 4 4 inundation changes of extreme events multi model median changes predicted for major 100 year ari flood events in each of the three future periods were used to perturb measured hydrographs from the baseline flood event at the yarrahappini and bromfleet gauges fig 10 dnrme 2019 using the median change flood magnitudes under the rcp8 5 were predicted to increase considerably for the logan river by the 2020s 56 5 and 2050s 57 3 with lesser increases by the 2080s 38 1 increases were smaller for the albert river 41 5 by the 2020s 40 by the 2050s and 24 7 by the 2080s under the rcp4 5 increases were of a similar magnitude for the 2020s but were smaller for the 2050s and 2080s fig s14 changes to the maximum flood extent compared to the baseline flood extent were determined for the three future periods rcp8 5 with and without the effects of sea level rise fig 11 in the 2080s the additional impacts of a storm surge event from 2017 were considered newly inundated areas are shown in red and green areas are flooded in the baseline but no longer flooded in the climate change scenario the maximum flood extent increases substantially in all future period with 55 increase by the 2020s with and without sea level rise table 4 the effects of sea level rise were amplified by the 2050s and 2080s increasing the predicted change in inundated area from 54 to 64 by the 2050s and from 33 4 to 60 2 by the 2080s by the end of the century sea level rise nearly doubles floodplain inundation increases that were predicted solely from atmospheric climate change primarily affecting the downstream section of the floodplain fig s16 considering the additional effects of storm surge resulted in only minor increases in flooding from 60 2 to 63 8 inundation by the 2080s with sea level rise in all scenarios the area of land inundated with more than 2 m of water also increased considerably 52 6 this increase was primarily along the upper estuarine reaches as a response to increased inflows rather than changes to sea level the majority of the newly inundated land was situated along the southern floodplains near the river mouth particularly under sea level rise scenarios and along the upper reaches of the logan river fig 11 5 discussion 5 1 changes in precipitation pet and streamflow predicting changes in floods high flows and mean flows under climate change is crucial if informed adaptation strategies are to be implemented the climate scenarios used in this study indicate that changes in precipitation are likely to vary seasonally with increases predicted in summer and decreases in winter and spring increases in pet which are strongly influenced by changes in temperature and humidity are considerable by the 2050s and 2080s with the largest increases predicted for summer and the smallest increases coinciding with winter however in terms of relative changes these increases show little seasonal variation fig s8 increases in the deficit between precipitation and pet were greatest in spring leading to some of the largest relative decreases in streamflow in this season fig 8 teng et al 2012 showed that a 1 increase in pet led to a 1 2 reduction in streamflow across australia while a 1 increase in precipitation increased streamflow by 2 3 we found seasonal changes to high and mean flows largely reflected the changes in precipitation with the largest decreases occurring in winter and spring months fig 8 decreases in streamflow however were substantially greater during these months than decreases in precipitation while increased precipitation in summer did not necessarily lead to increased streamflow this reflects the influence of elevated pet and by extension lower soil moisture in the root and surface zones of the hydrological model which amplified decreases in streamflow during the subtropical winter dry season and diminished increases in the summer wet season as evident by the increase in the deficit fig 6 unlike high flows however most models did not predict a decrease in major floods indicating that increases in pet did not mitigate the size of different ari flood events in the same way as was noted for high flows the directionality of change in the different flood quantiles under climate change varied with model predictions indicating both increases and decreases in the future though larger events tended to increase this outcome was in contrast to the widespread convergence among the models of decreased high and mean flows by the 2050s and 2080s predicted changes to flooding in the 2080s had smaller variability amongst the individual models than for the 2020s evident as smaller differences in the interquartile range fig 9 this result is different from that of liu et al 2013 and shen et al 2018 who reported increased uncertainty for high and flooding flow predictions using gcm drivers extending further into the future our study also showed greater variability from the model ensemble of flood events of larger magnitude than smaller magnitude fig 9 which is consistent with the results of liu et al 2013 who noted greater uncertainty for more extreme flood events considering changes to the multi model median there was predicted to be a slight decrease in the magnitude of smaller flood events 5 year ari by the 2080s and an increase of larger events 50 and 100 year ari this change indicates that different flood quantiles may have different rates of change highlighting the importance of considering a range of flood magnitudes yin et al 2018 and essou and brissette 2013 likewise reported different rates of change for different flood quantiles in subtropical china and tropical west africa respectively predictions for the logan albert catchment showed changes predicted for high flows differed substantially from flooding a number of studies report changes in high flows to be indicative of changes to flooding aich et al 2014 asadieh and krakauer 2017 li et al 2016 but our results suggest that high flows and flooding should be differentiated and high flows cannot always be relied upon to indicate the likely changes to flooding this has important implications for cases in which infrastructure damage or design criteria are being considered as flooding even at lower frequency may have a disproportionate influence over high flows assessing changes to high flows can be advantageous over flood frequency analysis however as predictions for high flows are more accurate than those for floods aich et al 2016 we adopted a traditional flood frequency analysis technique using the generalised pareto distribution and the pot series in using traditional flood frequency analysis techniques it was assumed that flow regimes were stationary around each of the time periods considered non stationary techniques e g yin et al 2018 would allow consideration of continually changing climate and hydraulic conditions e g from changes to river infrastructure land use or urbanisation notably in our case from the wyaralong dam in the logan catchment however in a study on the wainganga river in india das and umamahesh 2017 found only minor differences between stationary and non stationary techniques when estimating the magnitude of larger ari events 5 2 changes in inundation from extreme events the predicted inundation area from a 100 year ari flood event increased significantly in all future periods under the rcp8 5 the effects of sea level rise were important especially by the end of the century when the change in inundation area nearly doubled comparatively few studies have considered the impacts of sea level rise when assessing changes in riverine flooding due to atmospheric climate change budiyono et al 2016 hamman et al 2016 västilä et al 2010 despite coastal regions being some of the most densely populated in the world our results suggest the effects of sea level rise on flooding should not be ignored in coastal catchments and can be just as important as atmospheric changes similar findings have been reported for jarkarta budiyono et al 2016 the skagit river hamman et al 2016 and the mekong river västilä et al 2010 the maximum inundation extent for the 2080s was less than that seen for the 2050s or 2020s when sea level rise was not considered potentially relating to substantial increases in pet predicted by the end of the century however it is important to note that there is considerable uncertainty in these predictions particularly for large magnitude flood events and as such the inundation results should not be interpreted as deterministic multi model median changes for the 2020s and 2050s were within the inter quartile range of predictions for the 2080s fig 9 which indicates that 100 year ari streamflow events in these periods may not necessarily be larger than those in the 2080s the additional impacts of storm surge on inundation for the 2080s was relatively minor particularly compared to that of sea level rise this likely relates to the timing of the storm surge event used which peaked two days prior to the peak of the streamflow event the issue of flooding could also be made worse in the catchment by increasing impervious area from urbanisation as has been highlighted in previous studies budiyono et al 2016 zhao et al 2016 as the logan albert catchment is projected to undergo significant urbanisation and population growth in the future 50 increase in population between 2017 and 2036 queensland treasury 2018 further research could evaluate potential for land use change to affect flooding and risks to human lives and infrastructure climate change may also be associated with changes to in stream and shoreline morphology bilskie et al 2014 passeri et al 2015 which could exacerbate flooding these changes are challenging to address without detailed stream and estuary geomorphological predictions the unequivocal direction of inundation predictions from this study provides a solid basis however to inform the planning process about the vulnerability of areas subject to rapid urbanisation in the catchment floodplain 5 3 environmental and water supply issues the projected changes in high and mean flows will also have important consequences for water quality in the logan albert catchment and other coastal catchments in the region nutrient and sediment loads have increased substantially since european arrival olley et al 2015 and the estuarine reaches of the logan albert river experience elevated concentrations of nutrients and high turbidity levels healthy land and water 2019 flushing times in subtropical australian estuaries already tend to be very long particularly during the winter dry season eyre 1998 and can lead to a build up of in stream pollutants and sediments as noted during the recent 2002 09 millennium drought eccles et al 2020 van dijk et al 2013 there is projected to be an intensification in the seasonality of streamflow in the logan albert river system with large decreases in the dry season as well as increased extension of this season fig 8 low flushing rates for large portions of the year are likely to lead to a greater build up of nutrients and sediments along the estuarine reaches particularly in the region of wwtp outfalls due to urbanisation two additional wwtps are planned for the lower catchment that will necessitate offset schemes to ensure nutrient loads do not exceed guidelines however a majority of sediment and nutrient loads are delivered from diffuse sources during the wet season abal et al 2005 and it is not known how projections for an intensification in the seasonality of precipitation and streamflow will affect diffuse loads and these offset schemes decreased flows and greater point source inputs are likely to become a growing issue during the dry season when catchment offset schemes would have less impact lower flows in the future may necessitate additional discharges from upstream dams to flush nutrients from the system additionally nutrient loads from wwtps are likely to consist of greater relative quantities of dissolved nutrients than particulates when compared with catchment sources which are more bioavailable and may present additional management issues the influence of these changes on both point and diffuse loads in the catchment is an area that requires further examination lower mean flows and higher pet may also reduce water supplies throughout the region additionally increases in the largest flooding events may require dams in the region to operate at lower maximum storage capacity to accommodate the large floods diminishing long term water reserves a notable example is the 2011 flood event that occurred in the adjacent brisbane river catchment which led to significant dam releases 18 000 properties inundated and a class action lawsuit against the dam operators van den honert and mcaneney 2011 the 2011 flood event has led to a more conservative approach to dam operating levels in order to avoid downstream damage but with a potential trade off to water supply capacity 5 4 model framework a key feature of this study was the use of the coupled 1 d 2 d hydrodynamic model which was easily adaptable to represent regions of greater interest e g urban areas varying topography in more detail than regions of little interest e g un inundated areas when applying this coupled model changes to the multi model median were used to perturb a historical flood event rather than considering the full ensemble of climate models which would have been difficult to do due to long computational run times it is important to note that there is large uncertainty in the boundary conditions applied to run this model and the results should therefore be interpreted with this in mind additionally only changes to major 100 year ari events in the inundation model were considered as smaller ari events do not represent nearly as big of a risk to property or life the coastal boundary conditions applied in this study included predicted tidal levels with no additional impacts of storm surge for most modelling scenarios except for use of a historical storm surge event for consideration of inundation in the 2080s the magnitude of future storm surges could increase as more frequent and intense tropical cyclones are predicted to impact the region department of climate change 2009 nguyen and walsh 2001 likewise the nonlinear interactions between the different flood causing mechanisms will change in the future as sea levels rise this change may also alter the timing and magnitude of surge events future studies would be useful to examine details of interactions among these drivers this study was limited to two emissions scenarios rcp8 5 and rcp4 5 and a single downscaling technique and hydrological model it is widely accepted that the choice of gcm is the largest source of uncertainty in climate change impact studies de niel et al 2019 kay et al 2009 an ensemble of 11 high resolution dynamically downscaled climate models was therefore used in this study the models were dynamically downscaled using the conformal cubic atmospheric model and bias corrected using a quantile mapping approach which has been shown to perform better in predicting extremes than other techniques chen et al 2013 dobler et al 2012 the use of high resolution 10 km spatial resolution dynamically downscaled climate data was considered advantageous as the study catchment is relatively small 3862 km2 with significant heterogeneity in topography land use and precipitation use of coarser climate model outputs which may be suitable for larger catchments may not adequately represent these local climatic drivers and features for informing disaster preparedness rcp8 5 is generally considered to be the most appropriate emissions scenario but for comparative purposes results from rcp4 5 are presented in section s4 of the supplementary materials future studies could consider a wider array of hydrological models as they are important additional sources of uncertainty in climate change impact studies tian et al 2016 wilby and harris 2006 6 conclusion this study provides one of the first assessments of the impacts of climate change on high flows flooding and floodplain inundation of subtropical catchments hydrological modelling using an ensemble of climate models showed climate change was likely to cause greater seasonality of high and mean flows with significant decreases in flow in winter and spring and highly variable changes in summer the magnitude of large flooding events was predicted to vary significantly among climate models although the multi model median tended to show an increase in the magnitude of the largest events 100 year ari and a slight decrease or no change for smaller events 5 year ari in all future periods these results highlight the importance of considering a range of flood quantiles in impact studies and show that changes in high flows should not necessarily be relied upon to inform the likely changes to flooding the inundation area from a 100 year ari flood was predicted to increase considerably in all future periods and increases in streamflow and sea level rise acted synergistically to increase floodplain inundation substantially by the 2050s and 2080s for instance when sea level rise was included in the modelling the increase in floodplain inundation area was almost two fold by the 2080s which has important ramifications for flood risk our study highlights the non linear hydrological changes that result from climate change the potential impacts on nutrient and sediments and demonstrates the need for comprehensive assessments of floodplain inundation at the local scale to better inform preparedness for future flooding credit authorship contribution statement rohan eccles writing original draft conceptualization methodology formal analysis hong zhang supervision writing review editing conceptualization david hamilton supervision writing review editing conceptualization ralph trancoso writing review editing data curation jozef syktus writing review editing data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author received a griffith university postgraduate research scholarship the authors thank logan city council and seqwater for providing data and dhi for providing a license to the mike software suite supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103825 appendix supplementary materials image application 1 image application 2 
365,climate change is expected to significantly alter river hydrological regimes throughout the world affecting water resources and the frequency of floods and droughts the objectives of this study were to determine the impacts of climate change and sea level rise on streamflow and floodplain inundation in the subtropical logan albert catchment australia an ensemble of 11 high resolution climate models forced under high representative concentration pathway 8 5 rcp8 5 and intermediate emission rcp4 5 scenarios was applied there was considerable variation from the model ensemble result in projections of major flooding events at 5 and 100 year average recurrence intervals aris the largest events 100 year ari tended towards an increase whereas the smallest 5 year ari tended towards a decrease floodplain inundation from a 100 year ari event increased in all simulations and inclusion of sea level rise resulted in increased floodplain inundation area nearly doubling by the end of the century which has substantial implications for flood risk our study highlights the non linear nature of climate change impacts on streamflow and floodplain inundation demonstrating the need for a comprehensive assessment at the floodplain scale when informing preparedness for future flooding events keywords river high flows flooding sea level rise discharge flood risk 1 introduction climate change is expected to alter hydrological regimes throughout the world with major implications for water resources and hydrological extremes e g floods and droughts blöschl et al 2019 flooding is one of the most costly and widespread climate related natural disasters jonkman 2005 it has major implications for landholders and urban infrastructure and requires costly mitigation measures magnification of the variability of the hydrological cycle due to climate change is expected to result in greater frequency of extreme precipitation throughout most parts of the world groisman et al 2005 trends for the latter half of the 20th century indicate an increase in extreme flood events milly et al 2002 while analysis of smaller floods has suggested a decreasing trend particularly in larger catchments do et al 2017 a number of studies on a global scale have shown that the trend of increasing flood magnitude for the most extreme events will continue over the 21st century for many regions of the world berghuijs et al 2017 gosling et al 2017 hirabayashi et al 2013 wiel et al 2019 in recent years there has been a marked increase in the number of studies assessing the impacts of climate change on flooding and high flow events at the regional and catchment scales gu et al 2018 jin et al 2018 li et al 2016 liu et al 2013 phi hoang et al 2016 tian et al 2016 these studies typically make use of a modelling framework outlined by xu et al 2005 consisting of global circulation model gcm outputs downscaling bias correction techniques and hydrological model applications each step inevitably introduces some degree of error which is compounded sequentially and different combinations of models and techniques can lead to some disparity in results praskievicz and chang 2009 gcms are widely held as the largest source of uncertainty de niel et al 2019 kay et al 2009 and it is recommended that an ensemble of climate models be used for hydrological projections prudhomme and davies 2009 teutschbein and seibert 2010 due to spatiotemporal differences in climate change the hydrological response varies substantially between regions of the world hirabayashi et al 2013 it is therefore important that the impacts of climate change are evaluated across different regions while there has been a marked increase in the number of studies on this subject there remains a paucity of literature for many tropical and subtropical regions including subtropical australia eccles et al 2019 gu et al 2020 reported uncertainty in response to flood magnitudes across subtropical and tropical australia however most increases were concentrated in the tropical north and decreases in the temperate south chen and yu 2015 found little change in large floods magnitudes for two subtropical southeast queensland seq creeks by the 2030s using two climate models however studies based on a limited number of climate projections and predicting less than two decades into the future may not adequately inform preparedness for future flooding events the impacts of climate change on flooding may also be exacerbated by the nonlinear backflow effects of sea level rise in many coastal catchments which has been found to be a major contributor to increased inundation budiyono et al 2016 hamman et al 2016 västilä et al 2010 particularly under storm surge scenarios bilskie et al 2016 bilskie et al 2014 relatively few studies however have considered the combined effects of sea level rise and atmospheric climate change on riverine flooding despite coastal zones being some of the most densely populated areas in the world neumann et al 2015 though studies are available that address other topics such as sediment transport huang et al 2016 such studies require an additional hydrodynamic model as part of the modelling framework to simulate freshwater marine interactions along estuaries and coastal floodplains budiyono et al 2016 västilä et al 2010 sea level rises will also have implications for coastal land cover vegetation and shoreline morphology bilskie et al 2016 passeri et al 2015 saintilan and rogers 2009 which may work to exacerbate flooding in coastal catchments subtropical seq is one of the most flood prone regions in australia abbs et al 2007 and has been identified as one of the hotspots for climate change by the international panel for climate change ipcc hennessy et al 2007 urban development in the region is located along low lying coastal floodplains sea level rise and altered streamflow conditions may therefore act synergistically to intensify flooding the logan albert catchment is the second largest catchment in the region and is anticipated to be subject to significant population growth queensland treasury 2018 potentially increasing population exposure to flood events while land use changes may lead to more severe events this study implemented a modelling framework that integrated downscaled climate projections hydrological modelling and coupled 1 d and 2 d hydrodynamic models to capture coastal inundation from floods the aims were threefold i to assess the hydrological response of a coastal subtropical catchment to future climate change ii to examine how the projected changes in hydrology may impact floodplain inundation and iii to evaluate the synergic effects of atmospheric climate change and sea level rise on floodplain inundation 2 methods 2 1 study area the logan albert catchment has an area of approximately 3862 km2 of which the logan covers around 3080 km2 and the albert 782 km2 fig 1 the two major rivers of each catchment arise from the mcpherson range and converge 11 km from the coast the climate is subtropical with most precipitation in the summer december to february wet season and a dry season in winter june to august major flooding events are usually associated with low pressure systems or ex tropical cyclones and commonly affect the region during the wet season precipitation over the catchment is also spatially variable with 2000 mm yr 1 in the south eastern headwaters and 900 mm yr 1 in the west land use in the upper and middle catchment is dominated by native forest cover and cattle grazing extensive sugarcane plantations are located along the southern floodplain near the river mouth fig s1 logan city located in the lower catchment south of brisbane has a population of 319 000 which is projected to grow by about 50 to 490 000 by 2036 queensland treasury 2018 there are two major dams in the upper logan catchment which attenuate flood events arising from the upper catchment fig 1 additionally two major wastewater treatment plants wwtps are located along the estuarine reaches of the logan and albert rivers respectively and are reliant in part on some baseline levels of flushing to reduce their ecological impact 2 2 data bathymetry data 5 m horizontal resolution of the lower logan and albert rivers was retrieved from logan city council and the queensland maritime service the data was incorporated into a digital elevation model dem of the catchment obtained from geoscience australia https elevation fsdf org au also consisting of a 5 m horizontal resolution land use and soil data for the region were obtained from the queensland government http qldspatial information qld gov au catalogue custom index page daily synthetic pan evaporation data at 5 km spatial resolution was retrieved from the scientific information for land owners silo queensland government 2019 and converted to potential evapotranspiration pet using a pan coefficient value of 0 7 allen et al 1998 daily observed rainfall data was obtained for 56 gauges across the catchment fig 1 from the bureau of meteorology bom http www bom gov au climate data thiessen polygons for all the rainfall gauges were developed and missing values at each gauge were filled using an inverse distance weighting technique de silva et al 2007 daily streamflow data for four stream gauges id 145014a 145102b 145031a and 145018a was obtained from the queensland department of natural resources mines and energy dnrme 2019 daily dam discharge data was retrieved from the queensland government bulk water supply authority and downstream water level data for six gauges was obtained from the bom fig 2 hourly tidal data from the russell island tidal gauge near the river mouth fig 1 was available after 2017 from the queensland maritime service the mike 21 tidal analysis and prediction module dhi 2017c was used to calculate 69 tidal constituents based on the ios method foreman 1977 including 45 constituents of astronomical origin and 24 shallow water constituents tidal constituents were then used to estimate tidal levels over the full calibration and validation periods effects of storm surge and other atmospheric driven components of tidal height were not included in this analysis daily high resolution climate projections from the fifth phase of the coupled model intercomparison project cmip5 multi model database were obtained from the queensland department of environment and science syktus et al 2020 table s2 the climate models were dynamically downscaled by syktus et al 2020 to a 10 km spatial resolution over queensland under the rcp8 5 and rcp4 5 scenarios using the regional conformal cubic atmospheric model these downscaled projections have been used to understand future climate risk and inform regional climate adaptation policy in queensland trancoso et al 2020 the rcp8 5 scenario represents a high emissions scenario with potentially the most significant changes to rainfall representing a worst case scenario it is important to consider high emission pathways when informing natural hazards and disaster preparedness such as extreme floods to avoid neglecting the unknown future risk precipitation and pan evaporation data were retrieved from the 11 climate models for the baseline 1980 2009 and three future periods representing the 2020s 2010 2039 2050s 2040 2069 and the 2080s 2070 2099 precipitation outputs were bias corrected by syktus et al 2020 using a quantile mapping technique piani et al 2010 we adopted the same technique to bias correct pan evaporation outputs using historical synthetic pan evaporation data after which the pan coefficient 0 7 was applied to convert values to pet sea level rise projections for the region under rcp8 5 and rcp4 5 were obtained from the commonwealth scientific and industrial research organisation csiro church et al 2016 3 modelling this study employed a modelling framework consisting of both hydrological and hydrodynamic models first the hydrological model was used to simulate daily streamflow from the logan and albert catchments respectively we then applied coupled 1 d and 2 d hydrodynamic models to estimate inundation along the coastal floodplain from an extreme event 3 1 hydrological model the deterministic lumped and conceptual nedbør afstrømnings model nam dhi 2019 was used to simulate river streamflow nam is a rainfall runoff model which requires inputs of precipitation and pet the model accounts for moisture content in three separate storages namely the surface root zone and groundwater catchment runoff is split into three conceptual components overland flow interflow and baseflow dhi 2019 the nam model has been applied to assess the effects of climate change on flooding for catchments in kenya and ethiopia taye et al 2011 thailand kure and tebakari 2012 supharatid et al 2016 and belgium alam et al 2014 the logan albert catchment was split into four sub catchments during model setup one for each of the two major dams maroon and wyaralong and one for both the logan and albert rivers at the yarrahappini and bromfleet stream gauges respectively fig 1 a mean area weighting technique was adopted to calculate areal precipitation and pet for each sub catchment an initial one year warmup period 2003 was designated in order to get the model operational we calibrated nam during 2004 2014 against daily observed streamflow at the gauges shown in fig 1 and validated nam between 2015 and 2017 model applications to dam sub catchments were calibrated using the nearest available upstream gauge fig 1 as wyaralong dam was impounded mid way through the calibration period december 2010 two model runs were set up using the same parameter values for model calibration one model did not consider the dam and ran until the date of impoundment and the other model which did consider the dam ran thereafter outputs of the first model run initialised the second model run and avoided an additional warmup period in the calibration period prior to impoundment simulated streamflow from the wyaralong dam sub catchment was used in the calibration of the downstream yarrahappini gauge after impoundment measured dam discharge values were applied outputs from the two models were combined to assess the performance of the hydrological model during calibration model performance was evaluated at the bromfleet and yarrahappini gauges using the nash sutcliffe efficiency nse ratio of the root mean square error to the standard deviation of the observed data rsr and percentage bias pbias 3 2 coupled 1 d and 2 d hydrodynamic model mike hydro is a one dimensional numerical river model which applies simplified equations of continuity and momentum saint venant equations to simulate unsteady streamflow dhi 2019 as the model is one dimensional it is assumed that velocity and depth only vary in the longitudinal direction it is also assumed that there is negligible variation in water density that the bottom slope of the channel is small and that the wave lengths are large compared to the water depth so that the streamflow direction is parallel to the bottom dhi 2017a mike hydro was set up using measured streamflow from the yarrahappini and bromfleet gauges as upper boundary conditions and estimated tidal levels near the river mouth as lower boundary conditions fig 1 outputs from the hydrological model were not applied as upstream boundary conditions cross sections were developed manually from the combined topography and bathymetry dem and separated by distances ranging from 97 m to 1825 m depending on stream morphology additional inflows along the principal channels from the ungauged creeks and streams were simulated with nam using one way coupling to mike hydro santiago collazo et al 2019 with parameter values taken from the calibrated upstream basins the hydrodynamic model domain encompassed the lower sections of both the logan and albert rivers where the effects of flooding are most consequential this section of river is the most urbanised bound by extensive floodplains near the river mouth and is expected to undergo significant future population growth queensland treasury 2018 additionally the impacts of sea level rise will likely be greatest along the lower reaches of the river the lower logan albert is bounded by extensive floodplains that inundate regularly during high flow events this type of overland flow is not well suited to 1 d river modelling so a 2 d overland flow model was also set up using mike 21 the mike 21 model is based on the navier stokes equations for velocity pressure temperature and density of river water including the effect of viscosity it has been widely applied for simulations of lakes seas rivers estuaries and floodplains dhi 2017b we coupled together the two models in the form of a lateral link using mike flood fig 2 in this way the 2 d overland model was only employed at times when the cross sectional heights of the 1 d model were exceeded saving on computational power this type of model coupling exercise has rarely been adopted in predicting climate change impacts on hydrology supharatid et al 2016 wu et al 2017 the 2 d model consisted of a flexible mesh allowing for improved resolution in regions of greatest interest and lower resolution in regions unlikely to be flooded or where topography varied little although the minimum element area used in the model was relatively coarse at approximately 4000 m2 land use data for the region was used to estimate spatially variable overland manning s n bed resistance values while soil data was used to estimate spatially variable infiltration rates across the model domain hill et al 2015 te chow 1959 a mean areal pet was applied across the overland model based on historical data which was altered for model runs in the future to reflect climate change rainfall over the overland model was not considered as it had already been considered using the hydrological model input to the 1 d hydrodynamic model a two stage procedure was used to calibrate the hydrodynamic model the 1 d river model was first calibrated and validated between 2004 and 2017 using observed water levels at six downstream gauges shown in fig 2 the calibration principally involved adjusting the manning s n along the length of both rivers we then coupled the calibrated 1 d river model with the 2 d overland model and calibrated and validated the combined model against 2017 and 2013 flood events respectively only three of the six gauges carbrook logan village and waterford could be used for validation due to a lack of data of adequate quality in this period whereas all gauges were used in calibration the simulation period for these events extended over two months to include both normal and high flow conditions model performance was evaluated at the six downstream gauges shown in fig 2 using the nse rsr and pbias of the peak water level in addition to the performance indicators the maximum inundation extent from the 2017 flood event simulation was compared to the measured maximum flood extent 3 3 analyses we considered the effects of climate change on flooding high flows and mean flows at the yarrahappini and bromfleet stream gauges fig 1 flood magnitudes were estimated using the generalised pareto distribution and the peak over threshold pot series which consist of all statistically independent peaks that exceed a chosen threshold keast and ellison 2013 the pot series was considered advantageous over the more commonly adopted annual maxima as it allows for more data to be assessed excludes data from years with no flooding and includes data from years with several floods keast and ellison 2013 we adopted a threshold that was exceeded on average 1 65 times annually stedinger and foufoula georgiou 1993 and followed a technique outlined by lang et al 1999 to ensure consecutive peaks met an assumption of independence 1 θ 5 d a y s l o g a 2 59 o r x m i n 3 4 m i n q 1 q 2 where θ is the minimum time difference between successive flood peaks days a is the catchment area km2 xmin is the minimum streamflow between two peaks m3 s and q1 and q2 are the maximum daily streamflow of the two peaks m3 s relative changes to 5 10 25 50 and 100 year average recurrence interval ari flood events high flows consisting of the top 10 q10 5 q5 and 1 q1 of streamflow and mean flows were calculated at each site for each climate model and for each of the future time periods 2020s 2050s and 2080s by comparing simulated future values to simulated baseline 1980 2009 values we also considered changes to the seasonality of high and mean flows by determining monthly changes for each future time period relative to the values in the baseline considering the effects of the dams during the calibration was important however when simulating the catchment response to the climate models we did not consider the impacts of the dams similar to methods used in other studies gu et al 2018 mohammed et al 2018 phi hoang et al 2016 as we only considered relative changes between baseline and future periods from each of the climate models the effects of the two dams within the model domain were assumed to be minor this assumption would not hold if there were significant spatial variations in rainfall change or if differences in streamflow were assessed rather than relative changes downstream changes to the maximum flood extent and to the inundated area under various water depths resulting from both changing streamflow and sea level rise were evaluated using the coupled hydrodynamic model for this purpose measured streamflow from a 2017 flood which included the attenuating impacts of the dams 100 year ari event ex tropical cyclone debbie was designated as a baseline event dnrme 2019 the measured baseline event was perturbed by the multi model median predicted change to a 100 year ari flood event for each of the future time periods simulated inflows from the downstream creeks and streams used in the baseline model run were likewise perturbed using this multi model median change these altered events represented estimates of the magnitude of a 100 year ari flood and were used as upstream boundaries to drive the coupled hydrodynamic model allowing for a comparison of the inundated area in each of the future periods relative to the inundated area from the baseline event in addition to scenarios considering changes to streamflow three additional scenarios were developed which considered the effects of sea level rise table 1 by varying the downstream boundary conditions based on projections for the region under rcp8 5 and rcp4 5 we applied a normalised nonlinearity index proposed by bilskie et al 2014 as a means to quantify the nonlinear changes to flooding from sea level rise adopting predicted tidal levels as the coastal boundary neglected non astronomic components of the tides including storm surge although storm surges in the region are small 1 m fig s3 compared to those reported in compound flood studies in the usa moftakhari et al 2019 moftakhari et al 2017 an analysis of the dependence between non tidal residuals assumed to be indicative of storm surge and streamflow events in the catchment section s3 of the supplementary materials showed significant dependence between streamflow and non tidal residuals fig s2 additional scenarios were therefore analysed for the 2080s to combine non tidal residuals from ex tropical cyclone debbie of 2017 with predicted tidal levels with and without the impacts of sea level rise 4 results 4 1 model calibration and validation 4 1 1 hydrological model simulated and observed streamflow at the bromfleet and yarrahappini stream gauges during calibration and validation are shown in fig 3 the nse values for the calibration and validation periods at the bromfleet gauge were 0 872 and 0 944 respectively and 0 860 and 0 916 at the yarrahappini gauge respectively indicating very good overall model performance moriasi et al 2007 the performance of the model to simulate high flows was also assessed at these stations using the top 1 of streamflow with nse values of 0 823 and 0 936 obtained at the bromfleet gauge and 0 812 and 0 897 at the yarrahappini gauge during calibration and validation respectively table 2 4 1 2 coupled 1 d and 2 d hydrodynamic model the performance of the coupled hydrodynamic model was evaluated against major flooding events in 2017 and 2013 during calibration and validation respectively at the six downstream water level gauges nse values ranged between 0 938 and 0 973 during calibration during validation only three of the gauges carbrook logan village and waterford were operational and the nse ranged between 0 879 and 0 950 table 3 there was a reasonable match between the simulated and observed flood peaks during both calibration fig 4 and validation as the pbias of the peak water level never exceeded 10 at any gauge during calibration or validation table 3 the pbias of the peak water levels generally indicated a minor overestimation of the flood peak during calibration and an underestimation during validation such differences may relate to uncertainty in the measurement of large streamflow events or to changes in stream morphology that occur following large floods the simulated flood extent can be seen in fig 5 to be a good match to the observed flood extent except for some parts of the lower floodplain where flood extent was underestimated which may relate to inadequate resolution of the mesh 4000 m2 for representing some of the changes in topography 4 2 changes to precipitation and potential evapotranspiration changes in average precipitation pet and the deficit precipitation pet across the catchment under the rcp8 5 model ensemble are presented in fig 6 for the three future time periods relative to the baseline 1980 2010 precipitation in winter and spring was predicted to decrease in the future and this decrease became most apparent by the end of the century 2080s summer precipitation generally showed an opposing pattern of increase which was also greatest by the 2080s this pattern of change is more apparent when looking at the relative changes to precipitation fig s8 these changes represent an amplification of the seasonality of precipitation a decrease during the winter dry season and an increase during the summer wet season for the early century 2020s there was little agreement on the directionality of change in precipitation for any season with most climate models not indicating a clear increase or decrease however by the 2050s and 2080s there was widespread agreement in the model ensemble about the directionality of change in winter spring and summer but not for autumn similar changes are noted under the rcp4 5 fig s9 and fig s10 though the magnitude of the changes was smaller than under the rcp8 5 interestingly despite significant shifts in the projected distribution of precipitation for the different seasons the multi model median change to annual precipitation was predicted to vary only slightly 4 compared to the baseline for the three future time periods under both emission scenarios therefore the precipitation increases in summer are projected to be large enough to offset decreases in winter and spring the largest increases to pet occurred during summer and spring while the smallest increases coincided with autumn and winter fig 6 relative changes to the seasonal distribution of pet however were consistent across all seasons fig s8 by the 2080s under the rcp8 5 the largest increases to the multi model median were predicted in spring 32 4 and the smallest in autumn 27 7 annual pet assessed as the multi model median increased by 4 9 for the 2020s 17 1 for the 2050s and 29 5 for the 2080s compared with more moderate increases under the rcp4 5 of 5 8 10 9 and 14 6 for the three future periods respectively 4 3 hydrological impacts of climate change projections of precipitation and pet from the climate model ensemble were used as inputs to run nam changes to high and mean flows in each of the three future periods were evaluated relative to the baseline at the yarrahappini logan river and bromfleet albert river gauges respectively fig 7 high and mean flows were predicted to decrease significantly in the future under rcp8 5 and these decreases became largest by the end of the century by the 2050s and 2080s most of the climate models indicated decreased streamflow whereas for the 2020s there was no clear indication on the sign of change under rcp4 5 only by the 2080s did most of the climate models indicate decreased streamflow fig s11 multi model median changes to high flows ranged between 0 7 and 6 1 for the 2020s 14 6 to 36 for the 2050s and 15 1 to 51 7 for the 2080s under rcp8 5 depending on the site and streamflow quantile assessed changes were less extreme under rcp4 5 between 0 7 to 9 2 for the 2020s 7 8 to 12 2 for the 2050s and 8 2 to 24 8 by the 2080s different streamflow quantiles gave different magnitudes of change with the largest decreases generally reported for q10 flows and the smallest decreases for q1 flows multi model median decreases to q1 flows ranged between 15 1 to 22 5 and between 46 2 and 49 1 for q10 flows by the 2080s under rcp8 5 indicating that the relative change in the frequency of smaller flow events would decrease more than that of larger events seasonal changes to high and mean flows under the rcp8 5 climate forcing largely reflected the changes for precipitation with the largest decreases occurring during winter and spring months and by the end of the century fig 8 decreases in streamflow however were substantially greater during these months than decreases in precipitation multi model median changes in high and mean flows were predicted to decrease by between 26 3 and 65 in winter 26 1 and 78 7 in spring and 22 7 and 30 7 in autumn by the 2050s depending on the site and streamflow quantile assessed while in summer the comparable changes were between 1 9 and 13 8 multi model median high and mean flows decreased further in winter and spring by the 2080s varying between 52 6 and 83 7 in winter and between 59 and 93 4 in spring for autumn these changes varied between 11 and 40 6 and in summer between 3 and 22 it is important to note that historically streamflow in winter and spring has been considerably lower than in summer and therefore relative changes in these seasons can appear overly large compared with summer summer high and mean flows showed a lower probability of change despite most climate models predicting an increase in summer precipitation by the 2080s fig 6 projected changes under the rcp4 5 fig s12 followed the same seasonal pattern as for the rcp8 5 case but with smaller decreases predicted during the winter and spring months on account of smaller decreases in precipitation and smaller increases in pet changes to the magnitude of different ari flood events are shown in fig 9 for rcp8 5 and in fig s13 for rcp4 5 generally there was no clear agreement amongst the model ensemble on the sign or of change for smaller ari events 5 and 10 year in any of the future periods for larger ari events 50 and 100 year however most of the models predicted increased flood magnitude in all future periods meaning that larger flood events were likely to increase more than smaller events multi model median changes to the 100 year ari flood event under the rcp8 5 were predicted to increase by 38 1 and 24 7 at the albert and logan rivers respectively by the 2080s in contrast 5 year ari flood events were predicted to change by 0 5 and 14 6 respectively in the same time period a similar tendency was noted for high flows where larger flows q1 generated smaller decreases than smaller flows q10 fig 7 there was greater variability in the projections of larger ari events than smaller events evident by the size of the interquartile range somewhat surprisingly there was generally greater variability in projections of future flood quantiles in the 2020s than in the 2050s and 2080s 4 4 inundation changes of extreme events multi model median changes predicted for major 100 year ari flood events in each of the three future periods were used to perturb measured hydrographs from the baseline flood event at the yarrahappini and bromfleet gauges fig 10 dnrme 2019 using the median change flood magnitudes under the rcp8 5 were predicted to increase considerably for the logan river by the 2020s 56 5 and 2050s 57 3 with lesser increases by the 2080s 38 1 increases were smaller for the albert river 41 5 by the 2020s 40 by the 2050s and 24 7 by the 2080s under the rcp4 5 increases were of a similar magnitude for the 2020s but were smaller for the 2050s and 2080s fig s14 changes to the maximum flood extent compared to the baseline flood extent were determined for the three future periods rcp8 5 with and without the effects of sea level rise fig 11 in the 2080s the additional impacts of a storm surge event from 2017 were considered newly inundated areas are shown in red and green areas are flooded in the baseline but no longer flooded in the climate change scenario the maximum flood extent increases substantially in all future period with 55 increase by the 2020s with and without sea level rise table 4 the effects of sea level rise were amplified by the 2050s and 2080s increasing the predicted change in inundated area from 54 to 64 by the 2050s and from 33 4 to 60 2 by the 2080s by the end of the century sea level rise nearly doubles floodplain inundation increases that were predicted solely from atmospheric climate change primarily affecting the downstream section of the floodplain fig s16 considering the additional effects of storm surge resulted in only minor increases in flooding from 60 2 to 63 8 inundation by the 2080s with sea level rise in all scenarios the area of land inundated with more than 2 m of water also increased considerably 52 6 this increase was primarily along the upper estuarine reaches as a response to increased inflows rather than changes to sea level the majority of the newly inundated land was situated along the southern floodplains near the river mouth particularly under sea level rise scenarios and along the upper reaches of the logan river fig 11 5 discussion 5 1 changes in precipitation pet and streamflow predicting changes in floods high flows and mean flows under climate change is crucial if informed adaptation strategies are to be implemented the climate scenarios used in this study indicate that changes in precipitation are likely to vary seasonally with increases predicted in summer and decreases in winter and spring increases in pet which are strongly influenced by changes in temperature and humidity are considerable by the 2050s and 2080s with the largest increases predicted for summer and the smallest increases coinciding with winter however in terms of relative changes these increases show little seasonal variation fig s8 increases in the deficit between precipitation and pet were greatest in spring leading to some of the largest relative decreases in streamflow in this season fig 8 teng et al 2012 showed that a 1 increase in pet led to a 1 2 reduction in streamflow across australia while a 1 increase in precipitation increased streamflow by 2 3 we found seasonal changes to high and mean flows largely reflected the changes in precipitation with the largest decreases occurring in winter and spring months fig 8 decreases in streamflow however were substantially greater during these months than decreases in precipitation while increased precipitation in summer did not necessarily lead to increased streamflow this reflects the influence of elevated pet and by extension lower soil moisture in the root and surface zones of the hydrological model which amplified decreases in streamflow during the subtropical winter dry season and diminished increases in the summer wet season as evident by the increase in the deficit fig 6 unlike high flows however most models did not predict a decrease in major floods indicating that increases in pet did not mitigate the size of different ari flood events in the same way as was noted for high flows the directionality of change in the different flood quantiles under climate change varied with model predictions indicating both increases and decreases in the future though larger events tended to increase this outcome was in contrast to the widespread convergence among the models of decreased high and mean flows by the 2050s and 2080s predicted changes to flooding in the 2080s had smaller variability amongst the individual models than for the 2020s evident as smaller differences in the interquartile range fig 9 this result is different from that of liu et al 2013 and shen et al 2018 who reported increased uncertainty for high and flooding flow predictions using gcm drivers extending further into the future our study also showed greater variability from the model ensemble of flood events of larger magnitude than smaller magnitude fig 9 which is consistent with the results of liu et al 2013 who noted greater uncertainty for more extreme flood events considering changes to the multi model median there was predicted to be a slight decrease in the magnitude of smaller flood events 5 year ari by the 2080s and an increase of larger events 50 and 100 year ari this change indicates that different flood quantiles may have different rates of change highlighting the importance of considering a range of flood magnitudes yin et al 2018 and essou and brissette 2013 likewise reported different rates of change for different flood quantiles in subtropical china and tropical west africa respectively predictions for the logan albert catchment showed changes predicted for high flows differed substantially from flooding a number of studies report changes in high flows to be indicative of changes to flooding aich et al 2014 asadieh and krakauer 2017 li et al 2016 but our results suggest that high flows and flooding should be differentiated and high flows cannot always be relied upon to indicate the likely changes to flooding this has important implications for cases in which infrastructure damage or design criteria are being considered as flooding even at lower frequency may have a disproportionate influence over high flows assessing changes to high flows can be advantageous over flood frequency analysis however as predictions for high flows are more accurate than those for floods aich et al 2016 we adopted a traditional flood frequency analysis technique using the generalised pareto distribution and the pot series in using traditional flood frequency analysis techniques it was assumed that flow regimes were stationary around each of the time periods considered non stationary techniques e g yin et al 2018 would allow consideration of continually changing climate and hydraulic conditions e g from changes to river infrastructure land use or urbanisation notably in our case from the wyaralong dam in the logan catchment however in a study on the wainganga river in india das and umamahesh 2017 found only minor differences between stationary and non stationary techniques when estimating the magnitude of larger ari events 5 2 changes in inundation from extreme events the predicted inundation area from a 100 year ari flood event increased significantly in all future periods under the rcp8 5 the effects of sea level rise were important especially by the end of the century when the change in inundation area nearly doubled comparatively few studies have considered the impacts of sea level rise when assessing changes in riverine flooding due to atmospheric climate change budiyono et al 2016 hamman et al 2016 västilä et al 2010 despite coastal regions being some of the most densely populated in the world our results suggest the effects of sea level rise on flooding should not be ignored in coastal catchments and can be just as important as atmospheric changes similar findings have been reported for jarkarta budiyono et al 2016 the skagit river hamman et al 2016 and the mekong river västilä et al 2010 the maximum inundation extent for the 2080s was less than that seen for the 2050s or 2020s when sea level rise was not considered potentially relating to substantial increases in pet predicted by the end of the century however it is important to note that there is considerable uncertainty in these predictions particularly for large magnitude flood events and as such the inundation results should not be interpreted as deterministic multi model median changes for the 2020s and 2050s were within the inter quartile range of predictions for the 2080s fig 9 which indicates that 100 year ari streamflow events in these periods may not necessarily be larger than those in the 2080s the additional impacts of storm surge on inundation for the 2080s was relatively minor particularly compared to that of sea level rise this likely relates to the timing of the storm surge event used which peaked two days prior to the peak of the streamflow event the issue of flooding could also be made worse in the catchment by increasing impervious area from urbanisation as has been highlighted in previous studies budiyono et al 2016 zhao et al 2016 as the logan albert catchment is projected to undergo significant urbanisation and population growth in the future 50 increase in population between 2017 and 2036 queensland treasury 2018 further research could evaluate potential for land use change to affect flooding and risks to human lives and infrastructure climate change may also be associated with changes to in stream and shoreline morphology bilskie et al 2014 passeri et al 2015 which could exacerbate flooding these changes are challenging to address without detailed stream and estuary geomorphological predictions the unequivocal direction of inundation predictions from this study provides a solid basis however to inform the planning process about the vulnerability of areas subject to rapid urbanisation in the catchment floodplain 5 3 environmental and water supply issues the projected changes in high and mean flows will also have important consequences for water quality in the logan albert catchment and other coastal catchments in the region nutrient and sediment loads have increased substantially since european arrival olley et al 2015 and the estuarine reaches of the logan albert river experience elevated concentrations of nutrients and high turbidity levels healthy land and water 2019 flushing times in subtropical australian estuaries already tend to be very long particularly during the winter dry season eyre 1998 and can lead to a build up of in stream pollutants and sediments as noted during the recent 2002 09 millennium drought eccles et al 2020 van dijk et al 2013 there is projected to be an intensification in the seasonality of streamflow in the logan albert river system with large decreases in the dry season as well as increased extension of this season fig 8 low flushing rates for large portions of the year are likely to lead to a greater build up of nutrients and sediments along the estuarine reaches particularly in the region of wwtp outfalls due to urbanisation two additional wwtps are planned for the lower catchment that will necessitate offset schemes to ensure nutrient loads do not exceed guidelines however a majority of sediment and nutrient loads are delivered from diffuse sources during the wet season abal et al 2005 and it is not known how projections for an intensification in the seasonality of precipitation and streamflow will affect diffuse loads and these offset schemes decreased flows and greater point source inputs are likely to become a growing issue during the dry season when catchment offset schemes would have less impact lower flows in the future may necessitate additional discharges from upstream dams to flush nutrients from the system additionally nutrient loads from wwtps are likely to consist of greater relative quantities of dissolved nutrients than particulates when compared with catchment sources which are more bioavailable and may present additional management issues the influence of these changes on both point and diffuse loads in the catchment is an area that requires further examination lower mean flows and higher pet may also reduce water supplies throughout the region additionally increases in the largest flooding events may require dams in the region to operate at lower maximum storage capacity to accommodate the large floods diminishing long term water reserves a notable example is the 2011 flood event that occurred in the adjacent brisbane river catchment which led to significant dam releases 18 000 properties inundated and a class action lawsuit against the dam operators van den honert and mcaneney 2011 the 2011 flood event has led to a more conservative approach to dam operating levels in order to avoid downstream damage but with a potential trade off to water supply capacity 5 4 model framework a key feature of this study was the use of the coupled 1 d 2 d hydrodynamic model which was easily adaptable to represent regions of greater interest e g urban areas varying topography in more detail than regions of little interest e g un inundated areas when applying this coupled model changes to the multi model median were used to perturb a historical flood event rather than considering the full ensemble of climate models which would have been difficult to do due to long computational run times it is important to note that there is large uncertainty in the boundary conditions applied to run this model and the results should therefore be interpreted with this in mind additionally only changes to major 100 year ari events in the inundation model were considered as smaller ari events do not represent nearly as big of a risk to property or life the coastal boundary conditions applied in this study included predicted tidal levels with no additional impacts of storm surge for most modelling scenarios except for use of a historical storm surge event for consideration of inundation in the 2080s the magnitude of future storm surges could increase as more frequent and intense tropical cyclones are predicted to impact the region department of climate change 2009 nguyen and walsh 2001 likewise the nonlinear interactions between the different flood causing mechanisms will change in the future as sea levels rise this change may also alter the timing and magnitude of surge events future studies would be useful to examine details of interactions among these drivers this study was limited to two emissions scenarios rcp8 5 and rcp4 5 and a single downscaling technique and hydrological model it is widely accepted that the choice of gcm is the largest source of uncertainty in climate change impact studies de niel et al 2019 kay et al 2009 an ensemble of 11 high resolution dynamically downscaled climate models was therefore used in this study the models were dynamically downscaled using the conformal cubic atmospheric model and bias corrected using a quantile mapping approach which has been shown to perform better in predicting extremes than other techniques chen et al 2013 dobler et al 2012 the use of high resolution 10 km spatial resolution dynamically downscaled climate data was considered advantageous as the study catchment is relatively small 3862 km2 with significant heterogeneity in topography land use and precipitation use of coarser climate model outputs which may be suitable for larger catchments may not adequately represent these local climatic drivers and features for informing disaster preparedness rcp8 5 is generally considered to be the most appropriate emissions scenario but for comparative purposes results from rcp4 5 are presented in section s4 of the supplementary materials future studies could consider a wider array of hydrological models as they are important additional sources of uncertainty in climate change impact studies tian et al 2016 wilby and harris 2006 6 conclusion this study provides one of the first assessments of the impacts of climate change on high flows flooding and floodplain inundation of subtropical catchments hydrological modelling using an ensemble of climate models showed climate change was likely to cause greater seasonality of high and mean flows with significant decreases in flow in winter and spring and highly variable changes in summer the magnitude of large flooding events was predicted to vary significantly among climate models although the multi model median tended to show an increase in the magnitude of the largest events 100 year ari and a slight decrease or no change for smaller events 5 year ari in all future periods these results highlight the importance of considering a range of flood quantiles in impact studies and show that changes in high flows should not necessarily be relied upon to inform the likely changes to flooding the inundation area from a 100 year ari flood was predicted to increase considerably in all future periods and increases in streamflow and sea level rise acted synergistically to increase floodplain inundation substantially by the 2050s and 2080s for instance when sea level rise was included in the modelling the increase in floodplain inundation area was almost two fold by the 2080s which has important ramifications for flood risk our study highlights the non linear hydrological changes that result from climate change the potential impacts on nutrient and sediments and demonstrates the need for comprehensive assessments of floodplain inundation at the local scale to better inform preparedness for future flooding credit authorship contribution statement rohan eccles writing original draft conceptualization methodology formal analysis hong zhang supervision writing review editing conceptualization david hamilton supervision writing review editing conceptualization ralph trancoso writing review editing data curation jozef syktus writing review editing data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author received a griffith university postgraduate research scholarship the authors thank logan city council and seqwater for providing data and dhi for providing a license to the mike software suite supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103825 appendix supplementary materials image application 1 image application 2 
366,riverine bathymetry is of crucial importance for shipping operations and flood management however obtaining direct measurements of depth is not always easy conversely with recent advances in sensor technology indirect measurements can be obtained and used to estimate high resolution river bed topography physics based inverse modeling techniques have been used to estimate bathymetry using indirect measurements like flow velocity at the surface however these methods are computationally expensive for large scale problems recently deep learning has opened a new door toward knowledge representation and complex pattern identification in many fields however these techniques have not been used for high dimensional riverine bathymetry problems since they require a large amount of data in the training phase to have a good estimation performance that can be generalized for new river profiles also unless one reduces the dimension of the problem these methods can have a computationally expensive similar to that of physics based techniques here we develop a new deep learning framework for riverine problems that can be trained using only a few river profiles and in a computationally efficient way that allows finding solutions on personal computers the proposed method exploits the spatially local connection between the observations and river bed profile and combines a fully connected deep neural network dnn with principal component analysis pca to image river bed topography using depth averaged flow velocity observations the new method is presented and applied to three riverine bathymetry identification problems results show that the proposed method achieves satisfactory performance in bathymetry estimation providing a powerful data driven technique for riverine bathymetry in terms of prediction quality robustness and computational cost that requires only a relatively small number of training samples keywords bathymetry estimation data assimilation inverse modeling deep learning low rank approximation 1 introduction efficient and accurate prediction of river channel geometry with high resolution is crucial for the successful operation of shipping and navigation the study of flood routing and risk assessment sediment transport aquatic habitat management and monitoring of geomorphological change casas et al 2006 westaway et al 2000 lane et al 1994 as sensor technology has improved the use of observations in riverine bathymetry has received increasing attention however direct high resolution bathymetric surveys by wading or watercraft mounted multibeam sonar equipment casas et al 2006 marcus 2002 is time consuming and costly for long study reaches in watershed scales therefore remote sensing techniques are valuable tools to monitor bathymetry changes in rivers garambois and monnier 2015 emery et al 2010 dietrich 2017 several remote sensing methods have been proposed to estimate riverine bathymetry such as airborne bathymetric lidar systems marcus 2002 hilldale and raff 2008 mckean et al 2009 or indirect riverine observations such as water surface elevation yoon et al 2012 garambois and monnier 2015 and surface velocity measured from gps drifters emery et al 2010 or estimated through particle image velocimetry with digital video camera muste et al 2008 and thermal imagery puleo et al 2012 in this work we estimate bathymetry using surface velocity measurements because they are sensitive to river depth and easy to acquire at a low cost in addition the shallow water equations landon et al 2014 wilson and ozkan haller 2012 are used as a model to generate flow velocity measurements corresponding to river bathymetry profiles several inverse modeling techniques have been used to incorporate surface observations for bathymetry estimation wilson and ozkan haller 2012 wilson et al 2010 for example wilson and ozkan haller 2012 used the ensemble kalman filter method to estimate the river depth from noisy synthetic flow velocity observations along the same line landon et al 2014 applied ensemble based methods on velocity measurements obtained by drifters to estimate riverine bathymetry lee et al 2018 applied the principal component geostatistical approach pcga lee and kitanidis 2014 kitanidis and lee 2014 to two high dimensional riverine bathymetry estimation problems by an efficient dimension reduction technique and provide accurate bathymetry images with their estimation uncertainty at a reasonable computation cost although these techniques provide good results they can be computationally expensive with many expensive forward model calls and also require some assumptions about the spatial structure of the solution recently data driven machine learning techniques have become popular in many fields such as image processing and natural language processing these techniques have two major steps the training step offline phase and the prediction step online phase the training step requires a large amount of data and can be expensive to obtain however once the network is trained the prediction step is fast and can provide accurate results making these methods suitable for online applications that require updating system state rapidly as they get a new set of measurements as a result there is tremendous interest in the inverse modeling community in leveraging the strength of these techniques for bathymetry estimation poggio et al 2017 however there are valid concerns regarding the applicability of these techniques for bathymetry problems since the performance of learning based techniques can be only as good as the training data wang et al 2018 karimpouli and tahmasebi 2019 yang et al 2019 agrafiotis et al 2019 therefore in order to achieve high accuracy these algorithms require a large amount of labeled training data i e co observed predictors and predictands which is generally hard to acquire for bathymetry estimation problems since direct high resolution bathymetry surveys by wading or watercraft mounted multibeam sonar equipment are infrequently performed over long river reaches due to logistical and budget constraints casas et al 2006 a second significant issue for applying deep models for high dimensional problems is their computational cost bengio et al 2015 poggio et al 2017 esmaeilzadeh et al 2019a 2019b for example a network with input and output size of order 104 requires training more than 108 parameters a task that quickly becomes intractable as the depth of the network increases despite these concerns a number of data driven machine learning techniques have been applied to bathymetry application problems recently for example ceyhun and yalçın 2010 used an artificial neural network ann for detecting the nonlinear relationship between reflectance from different spectral bands and water depths their proposed method isolates water attenuation and hence depth from other environmental factors as bottom material and vegetation by using different combinations of spectral bands a similar algorithm was applied by gholamalifard et al 2013 using ann method for landsat images moses et al 2013 used an ann algorithm for lake bathymetry using satellite images corucci et al 2011 negm et al 2017 used a neuro fuzzy approach for quickbird images while wang et al 2019 agrafiotis et al 2019 misra et al 2018 used support vector machine svm algorithms to estimate shallow water bathymetry from optical images mohamed et al 2016 introduced using ensemble learning fitting algorithm of least squares boosting for shallow lake bathymetry estimation from a combination of satellite images and water depth samples although these efforts provided reasonable results only limited test cases were considered and the critical issue of requiring large training datasets and generalizing these methods for new unseen problems was not investigated neyshabur et al 2017 also they considered low resolution bathymetry and did not discuss how to tackle the high computational cost of these methods for high dimensional bathymetry problems to avoid this curse of dimensionality and to reduce the need for a large training dataset we propose a novel deep neural network dnn framework for riverine bathymetry using limited labeled data in order to achieve these goals we combine a fully connected deep neural network with principal component analysis pca to reduce the size of the network pca can be done by using eigenvalue decomposition of data covariance matrix or a predefined covariance matrix using the pca technique here the network needs to estimate only a few number of coefficients that corresponds to the significant basis vectors lee et al 2018 kitanidis 2015 also in the proposed method the river profile is divided into smaller segments where the network tries to predict only a few adjacent cross sections instead of the whole river profile these modifications have two major benefits first the input size and the output size of the network is reduced dramatically resulting in reduction in the overall dimension of the network as the size of the network drops the number of training data required to train the network decreases because the total number of parameters to train is decreased since training the network requires sweeping through all training data and performing matrix vector products and computing derivatives of the loss function with respect to weighting parameters reducing the network size and the size of the training dataset lowers the computational cost of training the network second by dividing the entire domain into small segments each river profile can provide several hundred or even several thousands of training samples as opposed to only one training sample when the network is trained on the whole river therefore the deep network can be trained using only a few river profiles also since the network predicts small segments of river profiles it is less problem dependent and can be easily applied to new river profiles with different geometries in order to demonstrate the contribution of the proposed method better consider a river profile discretized into 20 000 40 500 grid points applying a fully connected dnn model to this river will result in having a number of parameters in order of 109 to 1010 training such a network is computationally very expensive and requires millions of training samples each river profile is considered as one sample here using the proposed method the number of network parameters will be in the order of 105 to 106 which requires several thousand or several hundred thousands training samples however the training samples here are small segments of a river and each river provides several hundred or even several thousands of training samples therefore we need only a few hundred river profiles to train the network and the network can be trained on a personal computer in a few hours in the following sections we first introduce the proposed dnn pca framework which combines the traditional fully connected deep learning method and the principal component analysis to reduce the network dimension then in order to evaluate the performance of the proposed method we apply it to three different riverine bathymetry problems in the first problem we compare the performance of the proposed method with that of pcga lee et al 2018 a fast geostatistical approach developed for high dimensional problems in terms of the accuracy and the computational cost on a high resolution bathymetry problem in the second problem we examine the performance of the proposed method for bathymetry problems with sparse measurements finally in the last problem we show the performance of the proposed method on riverine problems with different parameter distributions in all these applications we show that the new method is adept at learning the subtle spatial patterns in bathymetry profiles using a limited number of training data in a computationally efficient way thus offering a powerful tool for bathymetry estimation 2 methods 2 1 deep neural network a dnn is an artificial neural network or simply a neural network with multiple layers between the input and output layers schmidhuber 2015 bengio et al 2009 the layered structure of dnn methods allows them to learn at each layer representation of the raw input with increasing levels of abstraction as the number of layers increases the network can learn the fine scale features in high dimensional problems better vincent et al 2008 these techniques are powerful tools that can handle linear and non linear functions and provide a flexible approximation for many types of data and model complex distributions efficiently bengio et al 2007 larochelle et al 2009 before describing the proposed algorithm and the case studies within this paper we briefly review some concepts and the network architectures used in this work 2 2 deep neural network structure dnns consist of l layers of hidden units neurons called hidden layers that are connected in an acyclic graph where the output of each hidden unit in layer i can be used as the input signal for layer i 1 the first hidden layer is connected to the input layer and the last hidden layer is connected to the output layer as shown in fig 1 the most common layer type of neural network is the fully connected layer where the nodes in adjacent layers are fully pairwise connected but there is no connection between hidden units in the same layer the first layer input layer receives the raw input information then each successive layer hidden layers receives the output from the layer preceding it the last layer output layer produces the output of the system there may be one or multiple nodes in the output layer from which the answer it produces can be read the network that we consider here consists of several fully connected layers consider a deep neural network with l 2 layers l hidden layers the input vector x rn and the output vector y rm the resulting network will have the following structure h 0 x h 1 f 1 w 0 h 0 b 0 h 2 f 2 w 1 h 1 b 1 h l 1 f l 1 w l h l b l y h l 1 where w i r n i n i 1 is the weight matrix for layer i b r n i is the bias term ni is the number of hidden units in layer i and fi is the activation function for layer i the activation function is added to the network to introduce nonlinear properties to the network otherwise the output would be a linear function of the input there exist various choices for the activation function the sigmoid function hyperbolic tangent and rectified linear unit relu function are among the most popular activation functions that have been widely used in neural networks larochelle et al 2009 leshno et al 1993 maas et al 2013 in order to train the network we need to specify a loss function based on the predicted output and the target output therefore the network can find wi s and bi s to minimize the loss function for regression problems the loss function is usually the mean square error between the predicted output y and the target output y 2 3 pca dnn framework deep learning techniques work best when they have lots of quality labeled data i e co observed predictors and predictands available and this performance improves with more and better quality data goodfellow et al 2016 however when enough quality data is not fed into a deep learning system it can fail quite badly this issue becomes even more severe for high dimensional problems since they require larger networks i e deeper networks with a larger number of neurons at each layer as the size of the network increases the number of weighting parameters to be trained increases which requires even more training data this issue limits the application of deep learning techniques to bathymetry problems since they are usually high dimensional and it is impossible to have many survey data due to logistic restrictions and cost also finding optimal network structures requires considering many different values for hyperparameters such as the size number of layers and number of units per layer the learning rate optimization technique activation functions etc sweeping through the hyperparameter space to find the optimal hyperparameters may be costly in time and computational resources for high dimensional problems even using large processing capabilities of many core architectures such as gpus bengio et al 2015 in order to tackle the aforementioned issues for riverine bathymetry estimation using surface observations we propose a new approach that makes the application of deep learning techniques for high resolution bathymetry problems possible since it requires a much smaller number of river profiles for training the network also the proposed method reduces the computational cost for training dnn methods which makes it even possible to train the network on personal computers without access to multi core gpus the modification can be summarized in two main steps in the first step we divide each river profile and the corresponding flow velocity profiles into segments that consists of only a few adjacent cross sections of the river profile the number of adjacent cross sections n is a hyperparameter that can be optimized in the training phase of the network and is smaller than the number of grid points along the river using this idea we exploit spatial locality by enforcing a local connectivity pattern between the observations flow velocity measurements and the outputs river depth then each of these segments is considered as a training sample for the network fig 2 shows how we divide each river profile into segments in the second step we seek to find an approximate representation for the bathymetry profile in each segment as a linear combination of a small number of significant basis vectors u 1 u 2 u r lee et al 2018 kitanidis 2015 these vectors can be obtained by pca of the set of all segments in the training dataset or by assuming that the profiles have a gaussian distribution and computing the low rank approximation of the covariance matrix that represents the distribution lee et al 2018 wilson and ozkan haller 2012 here we use the latter approach and consider the first r significant eigenvectors of the covariance matrix as the basis vectors then the approximate solution can be written as 1 y i 1 r α i u i where αi s are the coefficients for representing the approximate solution in the reduced order model in order to use this approach the significant eigenvectors ui s are computed before training the dnn model and the bathymetry profiles of each segment are projected to these bases to compute the corresponding coefficients then the network will be trained to estimate these coefficients using the flow velocity measurements after finding the coefficients the bathymetry profile for each segment can be obtained using eq 1 as described in the algorithm 1 the pca dnn method is decoupled into two offline and online phases all the river profiles with their corresponding flow velocity profiles are given as inputs in the offline phase in a preprocessing step these data are divided into small segments to generate training data x x 1 x 2 x n and z z 1 z 2 z n where xi s are flow velocity profiles for small segments and zi s are their corresponding river bed profiles then the basis vectors ui s are computed using low rank approximation of a defined covariance matrix the river bed profiles zi s are then projected to these bases to obtain their corresponding vector of low rank coefficients y y 1 y 2 y n where y i α i 1 α i 2 α i r and z i j 1 r α i j u j finally in the last step of the offline stage x and y are used as the input and output of the deep learning method to train the network once the offline phase is done the network is trained and it is ready to be used for prediction purposes it should be noted that the offline phase of the proposed method is the expensive part where we need to prepare all training data and requires a large number of iterations forward and backward steps to train the network and find optimal hyperparameters once the network is trained it can be used for prediction purposes in the online phase the online phase only requires the flow velocity observations and the predefined basis functions similar to the offline phase first we need to divide the flow velocity profile into the small segments to make it ready for the dnn network then the dnn network predicts the corresponding vector of coefficients for each small segment yi s the outputs of the dnn network are used in a post processing step where the vectors of coefficients are projected back to construct the small segments in the final step these segments are put together to create the entire river as shown in fig 2 the adjacent segments have a large overlap area therefore the final bathymetry for each cross section is obtained by averaging the estimated bathymetry for segments containing that cross section it should be noted that the solution is computed in the online phase at a very low cost since it only involves a few small algebraic operations 2 4 applications in order to illustrate the applicability and effectiveness of the proposed method for riverine bathymetry problems we apply the model to three representative riverine problems with different conditions in the first test case we demonstrate the applicability of pca dnn for the riverine problem with high resolution flow velocity measurements in this problem we compare the performance of pca dnn with that of pcga which has been previously applied to riverine bathymetry problems and provided high accuracy results this case serves to highlight the performance benefits of using pca dnn in contrast to fast physics based inverse modeling approaches in the second test case we examine the performance of pca dnn for the riverine problem with sparse flow velocity measurements this case demonstrate the applicability of pca dnn for more general riverine problems with different measurement scenarios in the last test case we show the performance of pca dnn for rivers with different profile distributions to investigate whether the performance can be generalized this has the merit of demonstrating the performance of pca dnn on predicting new solutions given new parameters i e parameters unseen during training the pca dnn networks used in this paper were built and trained using the open source deep learning library keras gulli and pal 2017 in all the aforementioned cases the river dynamics are simulated using the 2 d depth averaged shallow water equations landon et al 2014 wilson and ozkan haller 2012 zaron et al 2011 the model used for this study is the 2 d shallow water module of the u s army corps of engineers adaptive hydraulics adh model savant et al 2010 adh provides a stabilized finite element fe approximation on unstructured simplicial meshes and is fully implicit in time 2 d meshes for each example were created using a mesh generator gridgen https github com sakov gridgen c adh supports spatial and heuristic temporal adaptivity although spatial adaption was disabled in the tests below additional information about adh can be accessed at https chl erdc dren mil chladh 3 results and discussion 3 1 bathymetry problem with high resolution measurement data the first case we consider is based on a roughly one mile reach of the savannah river near augusta ga the domain is located 7 64 km downstream of the savannah bluff lock and dam and includes a single 90 bend a high resolution bathymetry survey was conducted at the site by the u s army corps of engineers and is plotted in fig 3 the true bathymetry includes river training structures i e dikes just upstream of the bend and also includes a shift of the thalweg to the outside the river bend as expected due to the physics driving the channel formation the length of the domain along the centerline is approximately 1 2 km and the across channel distance ranged between 59 6 m and 155 3 m with an average of 98 4 m flow over 1 day is simulated to reach quasi steady state condition assuming a constant discharge of q b 187 4 m 3 s specified at the inflow upper left boundary and a constant free surface elevation of z b 29 3 m at the outflow right the model mesh includes 501 nodes in the along channel direction 41 nodes across channel direction and contains 40 000 triangles the approximate nominal spacings are 2 4 m in each direction in order to generate the training data 100 stochastic gaussian realization of the river profile are generated by using a gaussian kernel of the form lee et al 2018 wilson and ozkan haller 2012 2 cov x y β 2 exp d x 2 l x 2 d y 2 l y 2 where β 1 3 lx 20 50 and ly 80 140 in order to have a wide variety of river profiles in the dataset 100 stochastic gaussian realizations of river profiles with different parabolic cross sections are generated using similar gaussian kernel as described above three sample river profiles from these rivers are shown in fig 4 after generating these stochastic realizations the corresponding synthetic velocity measurements are obtained from adh simulation with these bathymetry profiles and then a gaussian error with a standard deviation of 0 1 m s is added to simulate modeling and measurement errors including the potential noise in the measurements to prepare the data for training the network we divided these river profiles and the corresponding flow velocity profiles into segments consisting of 11 cross sections n 11 therefore these 200 river profiles generated a dataset of size 98 200 this dataset is then shuffled and randomly split to train validation and test sets with ratios of 80 10 and 10 respectively also the original savannah river profile is not used in the training and validation sets so it can be used to compare the performance of pca dnn with the performance of pcga method the training set is used to train the network and find the weight parameters w and b the validation set is used to find the best hyperparameters for the network the final optimal network consists of eight fully connected layers with tanh activation function the training time for pca dnn method was around 21 min for 30 epochs and the prediction step for each river profile including post processing the results takes only 18 s the root mean square error rmse for the train validation and test sets was 0 51 m 0 54 m and 0 55 m respectively in order to compare the pca dnn performance with physics based online inverse modeling approaches we applied the trained pca dnn model and the pcga method to estimate the savannah river bed profile the pcga method has been previously applied to the savannah river and provided good results with a fewer number of measurements lee et al 2018 here we used 100 principal components for the pcga method fig 5 compares the prediction results of pca dnn with the pcga results comparing to fig 3 shows that both methods provide accurate results for the savannah river with rmse values of 0 54 m and 0 58 m respectively however the trained pca dnn network obtains these results in 18 s while pcga takes more than 20 h to generate these results on the same machine it should be noted that in order to have a fair comparison between these two methods all the simlulations and training for both methods were carried out on a linux workstation equipped with 24 intel core 3 47 ghz processors and 96 gb ram it should be noted that similar to pca dnn pcga was run in serial mode and we did not take advantage of multiprocessing for pcga runs the pcga method can get the results much faster on multi core workstations since adh model runs can be independently performed on different cpu processors however even using multi core workstations pcga cannot obtain these results as fast as the pca dnn this shows that the proposed learning algorithm is much faster than even fast physics based methods that are developed for high dimensional problems and still can provide more accurate results in order to compute the uncertainty incurred due to measurement errors we used a parametric bootstrapping approach kitanidis 1995 jeong et al 2018 where an ensemble of 100 sets of measurements are used to make 100 predictions then this ensemble set of prediction can be used to quantify the uncertainty in the measurements sun et al 2017 jeong et al 2018 fig 6 depicts the estimated uncertainty for the savannah river using pca dnn and pcga methods as shown here the estimated uncertainty for deeper locations is higher than the uncertainty at other locations 3 2 bathymetry problem with sparse measurements in the previous test case we used dense measurement regime where we had a flow velocity measurement at each grid point in this test case we are interested to evaluate the performance of the proposed method for cases with sparse measurements in order to achieve this goal we used the same bathymetry profile as the first test case but we only used 415 measurement locations as opposed to the first test case that we had 20 541 measurement locations using the segmentation idea similar as before we can generate several hundreds training sample from each river profile however in the case of sparse measurements the relative coordinates of the measurements with respect to the geometry of the segment varies from one sample to another sample therefore the relative coordinates of the measurements with respect to the segment can be informative for the network as a result knowing the relative location of measurements with respect to the center of the segment is essential for the network this information is used as additional input parameters for the network the train validation and test sets are the same as the first test case but only measurements at the new measurement locations are used here as the input the rmse values for train validation and test sets are 0 71 m 0 76 m and 0 79 m respectively fig 7 shows the estimated river bed profile for the savannah river using sparse measurements as expected the accuracy decreases compared to the results of the first test case since we had a fewer number of measurements however the pca dnn can still provide accurate results using sparse measurement profile 3 3 pca dnn performance on unseen river profiles in the last test problem we examine the performance of pca dnn for unseen river profiles with different profile distributions to evaluate whether we can generalize the performance of the method for new bathymetry problems with different parameter distributions in this test case we consider an approximately 10 km reach of the american river near its confluence with the sacramento river in california usa as with the savannah river example high resolution bathymetry taken from a u s army corps of engineer survey is used to define a true bottom elevation field as shown in fig 8 the length of the domain along the centerline is approximately 10 1 km and the across channel distance ranged between 45 2 m and 150 2 m with an average of 88 7 m a constant discharge of q b 5350 m 3 s is specified at the inflow lower right boundary and a constant free surface elevation of z b 510 m at the outflow left the computational mesh consists of 2001 nodes in the along channel direction 51 nodes in the transverse direction and contains 72 000 triangles the nominal spacing in the along channel direction is 8 4 and 4 12 m in the cross channel direction for this test case 100 stochastic realizations of the river profile and 100 stochastic gaussian realization of river profiles with different parabolic cross sections are generated by using gaussian kernel with the parameters β 1 5 lx 20 60 and ly 100 200 respectively after generating these profiles the corresponding flow velocity measurements at all grid points were obtained from adh simulations in order to examine the performance of the proposed method on unseen data only the training samples generated from the first half of the river profile in the along channel direction were used for training the network to account for the model and measurement uncertainty we added a gaussian error with a standard deviation of 0 05 m s similar to the first test case the river was divided into small segments with a width of 11 grid points along the river this resulted in a dataset of size 298 200 samples this dataset was then shuffled and randomly split into train validation and test sets with ratios of 80 10 and 10 percents respectively a similar network structure was used here but with a different number of hidden units in each layer the training time for pca dnn method was around 45 min for 20 epochs and the prediction step including post processing the results takes less than a minute on the same machine as the first test case the rmse values for train validation and test sets are 0 54 m 0 59 m and 0 59 m respectively fig 8 compares the prediction results of pca dnn with the reference river bed elevation the results show that the proposed method provides accurate results for both halves of the river with rmse values of 0 59 m and 0 63 m for the first and the second halves respectively this shows that the performance of the trained network can be generalized to unseen data and it still can provide accurate results fig 9 shows the estimated uncertainty and estimation error using the pca dnn method the results show that the method can provide more accurate results around the centerline while the accuracy close to the margins of the river decreases along channel estimates are presented in fig 10 showing that the estimated bathymetry successfully captures most large scale variability and the true bathymetry locates within the 95 uncertainty bayesian confidence intervals 4 conclusions in this work we present a data driven inversion modeling technique that can be applied for large dimensional riverine bathymetry problems with limited available data and computational resources the proposed model consists of three major components each of which performs a key task in the bathymetry estimation first the river is divided into small segments using the assumption that the river bed profile is locally dependent on flow velocity observations second the dimension of the output of the network is reduced by projecting the bathymetry profile of each segment onto predefined bases obtained in the offline step of the algorithm in the online step these bases are used to reconstruct the river profile by using the estimated coefficients which are the outputs of dnn the third component is a dnn which is trained in the training step using the preprocessed training data then the trained network is used in the online step for prediction purposes combined with the low rank pca basis for reconstructing the predicted river profile the first two components are used to reduce the size of dnn and also to provide more training samples given limited number of sample river profiles using the first two components the learning part of the algorithm can be trained even on personal computers in a few hours and then in the prediction step it can provide accurate results in a few seconds it should be noted that we used a fully connected neural network for this step however other types of neural network can be explored to improve the method we demonstrated the performance of our proposed method on three test problems the first evaluates the accuracy and prediction power of our method by comparing its performance with that of a powerful fast physics based inverse modeling technique for a high dimensional riverine bathymetry problem the second example shows that this method can be used for more general problems with sparse measurements the final example highlights the applicability and generalization of the proposed method for unseen regions beyond the original domain learned during the training step collectively these examples show that the proposed method is a powerful data driven technique for riverine bathymetry in terms of prediction quality robustness and computational cost and requires a small number of training samples credit authorship contribution statement hojat ghorbanidehno conceptualization methodology software writing review editing jonghyun lee data curation methodology software matthew farthing data curation writing review editing supervision tyler hesser writing review editing supervision eric f darve methodology supervision peter k kitanidis methodology supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported in part by an appointment to the postgraduate research participation program at the u s army engineer research and development center coastal and hydraulics laboratory erdc chl administered by the oak ridge institute for science and education through an interagency agreement between the u s department of energy and erdc the research is also supported in part by funding from the army high performance computing research center ahpcrc sponsored by the u s army research laboratory under contract no w911nf 07 2 0027 at stanford jonghyun lee was supported in part by hawai i experimental program to stimulate competitive research epscor provided by the national science foundation research infrastructure improvement rii track 1 ike wai securing hawai i s water future award oia 1557349 the computing resources for this project were provided under the aws cloud credits for research program the financial support is gratefully acknowledged the data for this paper is available upon request to the corresponding author hojat ghorbanidehno hojjatgh stanford edu 
366,riverine bathymetry is of crucial importance for shipping operations and flood management however obtaining direct measurements of depth is not always easy conversely with recent advances in sensor technology indirect measurements can be obtained and used to estimate high resolution river bed topography physics based inverse modeling techniques have been used to estimate bathymetry using indirect measurements like flow velocity at the surface however these methods are computationally expensive for large scale problems recently deep learning has opened a new door toward knowledge representation and complex pattern identification in many fields however these techniques have not been used for high dimensional riverine bathymetry problems since they require a large amount of data in the training phase to have a good estimation performance that can be generalized for new river profiles also unless one reduces the dimension of the problem these methods can have a computationally expensive similar to that of physics based techniques here we develop a new deep learning framework for riverine problems that can be trained using only a few river profiles and in a computationally efficient way that allows finding solutions on personal computers the proposed method exploits the spatially local connection between the observations and river bed profile and combines a fully connected deep neural network dnn with principal component analysis pca to image river bed topography using depth averaged flow velocity observations the new method is presented and applied to three riverine bathymetry identification problems results show that the proposed method achieves satisfactory performance in bathymetry estimation providing a powerful data driven technique for riverine bathymetry in terms of prediction quality robustness and computational cost that requires only a relatively small number of training samples keywords bathymetry estimation data assimilation inverse modeling deep learning low rank approximation 1 introduction efficient and accurate prediction of river channel geometry with high resolution is crucial for the successful operation of shipping and navigation the study of flood routing and risk assessment sediment transport aquatic habitat management and monitoring of geomorphological change casas et al 2006 westaway et al 2000 lane et al 1994 as sensor technology has improved the use of observations in riverine bathymetry has received increasing attention however direct high resolution bathymetric surveys by wading or watercraft mounted multibeam sonar equipment casas et al 2006 marcus 2002 is time consuming and costly for long study reaches in watershed scales therefore remote sensing techniques are valuable tools to monitor bathymetry changes in rivers garambois and monnier 2015 emery et al 2010 dietrich 2017 several remote sensing methods have been proposed to estimate riverine bathymetry such as airborne bathymetric lidar systems marcus 2002 hilldale and raff 2008 mckean et al 2009 or indirect riverine observations such as water surface elevation yoon et al 2012 garambois and monnier 2015 and surface velocity measured from gps drifters emery et al 2010 or estimated through particle image velocimetry with digital video camera muste et al 2008 and thermal imagery puleo et al 2012 in this work we estimate bathymetry using surface velocity measurements because they are sensitive to river depth and easy to acquire at a low cost in addition the shallow water equations landon et al 2014 wilson and ozkan haller 2012 are used as a model to generate flow velocity measurements corresponding to river bathymetry profiles several inverse modeling techniques have been used to incorporate surface observations for bathymetry estimation wilson and ozkan haller 2012 wilson et al 2010 for example wilson and ozkan haller 2012 used the ensemble kalman filter method to estimate the river depth from noisy synthetic flow velocity observations along the same line landon et al 2014 applied ensemble based methods on velocity measurements obtained by drifters to estimate riverine bathymetry lee et al 2018 applied the principal component geostatistical approach pcga lee and kitanidis 2014 kitanidis and lee 2014 to two high dimensional riverine bathymetry estimation problems by an efficient dimension reduction technique and provide accurate bathymetry images with their estimation uncertainty at a reasonable computation cost although these techniques provide good results they can be computationally expensive with many expensive forward model calls and also require some assumptions about the spatial structure of the solution recently data driven machine learning techniques have become popular in many fields such as image processing and natural language processing these techniques have two major steps the training step offline phase and the prediction step online phase the training step requires a large amount of data and can be expensive to obtain however once the network is trained the prediction step is fast and can provide accurate results making these methods suitable for online applications that require updating system state rapidly as they get a new set of measurements as a result there is tremendous interest in the inverse modeling community in leveraging the strength of these techniques for bathymetry estimation poggio et al 2017 however there are valid concerns regarding the applicability of these techniques for bathymetry problems since the performance of learning based techniques can be only as good as the training data wang et al 2018 karimpouli and tahmasebi 2019 yang et al 2019 agrafiotis et al 2019 therefore in order to achieve high accuracy these algorithms require a large amount of labeled training data i e co observed predictors and predictands which is generally hard to acquire for bathymetry estimation problems since direct high resolution bathymetry surveys by wading or watercraft mounted multibeam sonar equipment are infrequently performed over long river reaches due to logistical and budget constraints casas et al 2006 a second significant issue for applying deep models for high dimensional problems is their computational cost bengio et al 2015 poggio et al 2017 esmaeilzadeh et al 2019a 2019b for example a network with input and output size of order 104 requires training more than 108 parameters a task that quickly becomes intractable as the depth of the network increases despite these concerns a number of data driven machine learning techniques have been applied to bathymetry application problems recently for example ceyhun and yalçın 2010 used an artificial neural network ann for detecting the nonlinear relationship between reflectance from different spectral bands and water depths their proposed method isolates water attenuation and hence depth from other environmental factors as bottom material and vegetation by using different combinations of spectral bands a similar algorithm was applied by gholamalifard et al 2013 using ann method for landsat images moses et al 2013 used an ann algorithm for lake bathymetry using satellite images corucci et al 2011 negm et al 2017 used a neuro fuzzy approach for quickbird images while wang et al 2019 agrafiotis et al 2019 misra et al 2018 used support vector machine svm algorithms to estimate shallow water bathymetry from optical images mohamed et al 2016 introduced using ensemble learning fitting algorithm of least squares boosting for shallow lake bathymetry estimation from a combination of satellite images and water depth samples although these efforts provided reasonable results only limited test cases were considered and the critical issue of requiring large training datasets and generalizing these methods for new unseen problems was not investigated neyshabur et al 2017 also they considered low resolution bathymetry and did not discuss how to tackle the high computational cost of these methods for high dimensional bathymetry problems to avoid this curse of dimensionality and to reduce the need for a large training dataset we propose a novel deep neural network dnn framework for riverine bathymetry using limited labeled data in order to achieve these goals we combine a fully connected deep neural network with principal component analysis pca to reduce the size of the network pca can be done by using eigenvalue decomposition of data covariance matrix or a predefined covariance matrix using the pca technique here the network needs to estimate only a few number of coefficients that corresponds to the significant basis vectors lee et al 2018 kitanidis 2015 also in the proposed method the river profile is divided into smaller segments where the network tries to predict only a few adjacent cross sections instead of the whole river profile these modifications have two major benefits first the input size and the output size of the network is reduced dramatically resulting in reduction in the overall dimension of the network as the size of the network drops the number of training data required to train the network decreases because the total number of parameters to train is decreased since training the network requires sweeping through all training data and performing matrix vector products and computing derivatives of the loss function with respect to weighting parameters reducing the network size and the size of the training dataset lowers the computational cost of training the network second by dividing the entire domain into small segments each river profile can provide several hundred or even several thousands of training samples as opposed to only one training sample when the network is trained on the whole river therefore the deep network can be trained using only a few river profiles also since the network predicts small segments of river profiles it is less problem dependent and can be easily applied to new river profiles with different geometries in order to demonstrate the contribution of the proposed method better consider a river profile discretized into 20 000 40 500 grid points applying a fully connected dnn model to this river will result in having a number of parameters in order of 109 to 1010 training such a network is computationally very expensive and requires millions of training samples each river profile is considered as one sample here using the proposed method the number of network parameters will be in the order of 105 to 106 which requires several thousand or several hundred thousands training samples however the training samples here are small segments of a river and each river provides several hundred or even several thousands of training samples therefore we need only a few hundred river profiles to train the network and the network can be trained on a personal computer in a few hours in the following sections we first introduce the proposed dnn pca framework which combines the traditional fully connected deep learning method and the principal component analysis to reduce the network dimension then in order to evaluate the performance of the proposed method we apply it to three different riverine bathymetry problems in the first problem we compare the performance of the proposed method with that of pcga lee et al 2018 a fast geostatistical approach developed for high dimensional problems in terms of the accuracy and the computational cost on a high resolution bathymetry problem in the second problem we examine the performance of the proposed method for bathymetry problems with sparse measurements finally in the last problem we show the performance of the proposed method on riverine problems with different parameter distributions in all these applications we show that the new method is adept at learning the subtle spatial patterns in bathymetry profiles using a limited number of training data in a computationally efficient way thus offering a powerful tool for bathymetry estimation 2 methods 2 1 deep neural network a dnn is an artificial neural network or simply a neural network with multiple layers between the input and output layers schmidhuber 2015 bengio et al 2009 the layered structure of dnn methods allows them to learn at each layer representation of the raw input with increasing levels of abstraction as the number of layers increases the network can learn the fine scale features in high dimensional problems better vincent et al 2008 these techniques are powerful tools that can handle linear and non linear functions and provide a flexible approximation for many types of data and model complex distributions efficiently bengio et al 2007 larochelle et al 2009 before describing the proposed algorithm and the case studies within this paper we briefly review some concepts and the network architectures used in this work 2 2 deep neural network structure dnns consist of l layers of hidden units neurons called hidden layers that are connected in an acyclic graph where the output of each hidden unit in layer i can be used as the input signal for layer i 1 the first hidden layer is connected to the input layer and the last hidden layer is connected to the output layer as shown in fig 1 the most common layer type of neural network is the fully connected layer where the nodes in adjacent layers are fully pairwise connected but there is no connection between hidden units in the same layer the first layer input layer receives the raw input information then each successive layer hidden layers receives the output from the layer preceding it the last layer output layer produces the output of the system there may be one or multiple nodes in the output layer from which the answer it produces can be read the network that we consider here consists of several fully connected layers consider a deep neural network with l 2 layers l hidden layers the input vector x rn and the output vector y rm the resulting network will have the following structure h 0 x h 1 f 1 w 0 h 0 b 0 h 2 f 2 w 1 h 1 b 1 h l 1 f l 1 w l h l b l y h l 1 where w i r n i n i 1 is the weight matrix for layer i b r n i is the bias term ni is the number of hidden units in layer i and fi is the activation function for layer i the activation function is added to the network to introduce nonlinear properties to the network otherwise the output would be a linear function of the input there exist various choices for the activation function the sigmoid function hyperbolic tangent and rectified linear unit relu function are among the most popular activation functions that have been widely used in neural networks larochelle et al 2009 leshno et al 1993 maas et al 2013 in order to train the network we need to specify a loss function based on the predicted output and the target output therefore the network can find wi s and bi s to minimize the loss function for regression problems the loss function is usually the mean square error between the predicted output y and the target output y 2 3 pca dnn framework deep learning techniques work best when they have lots of quality labeled data i e co observed predictors and predictands available and this performance improves with more and better quality data goodfellow et al 2016 however when enough quality data is not fed into a deep learning system it can fail quite badly this issue becomes even more severe for high dimensional problems since they require larger networks i e deeper networks with a larger number of neurons at each layer as the size of the network increases the number of weighting parameters to be trained increases which requires even more training data this issue limits the application of deep learning techniques to bathymetry problems since they are usually high dimensional and it is impossible to have many survey data due to logistic restrictions and cost also finding optimal network structures requires considering many different values for hyperparameters such as the size number of layers and number of units per layer the learning rate optimization technique activation functions etc sweeping through the hyperparameter space to find the optimal hyperparameters may be costly in time and computational resources for high dimensional problems even using large processing capabilities of many core architectures such as gpus bengio et al 2015 in order to tackle the aforementioned issues for riverine bathymetry estimation using surface observations we propose a new approach that makes the application of deep learning techniques for high resolution bathymetry problems possible since it requires a much smaller number of river profiles for training the network also the proposed method reduces the computational cost for training dnn methods which makes it even possible to train the network on personal computers without access to multi core gpus the modification can be summarized in two main steps in the first step we divide each river profile and the corresponding flow velocity profiles into segments that consists of only a few adjacent cross sections of the river profile the number of adjacent cross sections n is a hyperparameter that can be optimized in the training phase of the network and is smaller than the number of grid points along the river using this idea we exploit spatial locality by enforcing a local connectivity pattern between the observations flow velocity measurements and the outputs river depth then each of these segments is considered as a training sample for the network fig 2 shows how we divide each river profile into segments in the second step we seek to find an approximate representation for the bathymetry profile in each segment as a linear combination of a small number of significant basis vectors u 1 u 2 u r lee et al 2018 kitanidis 2015 these vectors can be obtained by pca of the set of all segments in the training dataset or by assuming that the profiles have a gaussian distribution and computing the low rank approximation of the covariance matrix that represents the distribution lee et al 2018 wilson and ozkan haller 2012 here we use the latter approach and consider the first r significant eigenvectors of the covariance matrix as the basis vectors then the approximate solution can be written as 1 y i 1 r α i u i where αi s are the coefficients for representing the approximate solution in the reduced order model in order to use this approach the significant eigenvectors ui s are computed before training the dnn model and the bathymetry profiles of each segment are projected to these bases to compute the corresponding coefficients then the network will be trained to estimate these coefficients using the flow velocity measurements after finding the coefficients the bathymetry profile for each segment can be obtained using eq 1 as described in the algorithm 1 the pca dnn method is decoupled into two offline and online phases all the river profiles with their corresponding flow velocity profiles are given as inputs in the offline phase in a preprocessing step these data are divided into small segments to generate training data x x 1 x 2 x n and z z 1 z 2 z n where xi s are flow velocity profiles for small segments and zi s are their corresponding river bed profiles then the basis vectors ui s are computed using low rank approximation of a defined covariance matrix the river bed profiles zi s are then projected to these bases to obtain their corresponding vector of low rank coefficients y y 1 y 2 y n where y i α i 1 α i 2 α i r and z i j 1 r α i j u j finally in the last step of the offline stage x and y are used as the input and output of the deep learning method to train the network once the offline phase is done the network is trained and it is ready to be used for prediction purposes it should be noted that the offline phase of the proposed method is the expensive part where we need to prepare all training data and requires a large number of iterations forward and backward steps to train the network and find optimal hyperparameters once the network is trained it can be used for prediction purposes in the online phase the online phase only requires the flow velocity observations and the predefined basis functions similar to the offline phase first we need to divide the flow velocity profile into the small segments to make it ready for the dnn network then the dnn network predicts the corresponding vector of coefficients for each small segment yi s the outputs of the dnn network are used in a post processing step where the vectors of coefficients are projected back to construct the small segments in the final step these segments are put together to create the entire river as shown in fig 2 the adjacent segments have a large overlap area therefore the final bathymetry for each cross section is obtained by averaging the estimated bathymetry for segments containing that cross section it should be noted that the solution is computed in the online phase at a very low cost since it only involves a few small algebraic operations 2 4 applications in order to illustrate the applicability and effectiveness of the proposed method for riverine bathymetry problems we apply the model to three representative riverine problems with different conditions in the first test case we demonstrate the applicability of pca dnn for the riverine problem with high resolution flow velocity measurements in this problem we compare the performance of pca dnn with that of pcga which has been previously applied to riverine bathymetry problems and provided high accuracy results this case serves to highlight the performance benefits of using pca dnn in contrast to fast physics based inverse modeling approaches in the second test case we examine the performance of pca dnn for the riverine problem with sparse flow velocity measurements this case demonstrate the applicability of pca dnn for more general riverine problems with different measurement scenarios in the last test case we show the performance of pca dnn for rivers with different profile distributions to investigate whether the performance can be generalized this has the merit of demonstrating the performance of pca dnn on predicting new solutions given new parameters i e parameters unseen during training the pca dnn networks used in this paper were built and trained using the open source deep learning library keras gulli and pal 2017 in all the aforementioned cases the river dynamics are simulated using the 2 d depth averaged shallow water equations landon et al 2014 wilson and ozkan haller 2012 zaron et al 2011 the model used for this study is the 2 d shallow water module of the u s army corps of engineers adaptive hydraulics adh model savant et al 2010 adh provides a stabilized finite element fe approximation on unstructured simplicial meshes and is fully implicit in time 2 d meshes for each example were created using a mesh generator gridgen https github com sakov gridgen c adh supports spatial and heuristic temporal adaptivity although spatial adaption was disabled in the tests below additional information about adh can be accessed at https chl erdc dren mil chladh 3 results and discussion 3 1 bathymetry problem with high resolution measurement data the first case we consider is based on a roughly one mile reach of the savannah river near augusta ga the domain is located 7 64 km downstream of the savannah bluff lock and dam and includes a single 90 bend a high resolution bathymetry survey was conducted at the site by the u s army corps of engineers and is plotted in fig 3 the true bathymetry includes river training structures i e dikes just upstream of the bend and also includes a shift of the thalweg to the outside the river bend as expected due to the physics driving the channel formation the length of the domain along the centerline is approximately 1 2 km and the across channel distance ranged between 59 6 m and 155 3 m with an average of 98 4 m flow over 1 day is simulated to reach quasi steady state condition assuming a constant discharge of q b 187 4 m 3 s specified at the inflow upper left boundary and a constant free surface elevation of z b 29 3 m at the outflow right the model mesh includes 501 nodes in the along channel direction 41 nodes across channel direction and contains 40 000 triangles the approximate nominal spacings are 2 4 m in each direction in order to generate the training data 100 stochastic gaussian realization of the river profile are generated by using a gaussian kernel of the form lee et al 2018 wilson and ozkan haller 2012 2 cov x y β 2 exp d x 2 l x 2 d y 2 l y 2 where β 1 3 lx 20 50 and ly 80 140 in order to have a wide variety of river profiles in the dataset 100 stochastic gaussian realizations of river profiles with different parabolic cross sections are generated using similar gaussian kernel as described above three sample river profiles from these rivers are shown in fig 4 after generating these stochastic realizations the corresponding synthetic velocity measurements are obtained from adh simulation with these bathymetry profiles and then a gaussian error with a standard deviation of 0 1 m s is added to simulate modeling and measurement errors including the potential noise in the measurements to prepare the data for training the network we divided these river profiles and the corresponding flow velocity profiles into segments consisting of 11 cross sections n 11 therefore these 200 river profiles generated a dataset of size 98 200 this dataset is then shuffled and randomly split to train validation and test sets with ratios of 80 10 and 10 respectively also the original savannah river profile is not used in the training and validation sets so it can be used to compare the performance of pca dnn with the performance of pcga method the training set is used to train the network and find the weight parameters w and b the validation set is used to find the best hyperparameters for the network the final optimal network consists of eight fully connected layers with tanh activation function the training time for pca dnn method was around 21 min for 30 epochs and the prediction step for each river profile including post processing the results takes only 18 s the root mean square error rmse for the train validation and test sets was 0 51 m 0 54 m and 0 55 m respectively in order to compare the pca dnn performance with physics based online inverse modeling approaches we applied the trained pca dnn model and the pcga method to estimate the savannah river bed profile the pcga method has been previously applied to the savannah river and provided good results with a fewer number of measurements lee et al 2018 here we used 100 principal components for the pcga method fig 5 compares the prediction results of pca dnn with the pcga results comparing to fig 3 shows that both methods provide accurate results for the savannah river with rmse values of 0 54 m and 0 58 m respectively however the trained pca dnn network obtains these results in 18 s while pcga takes more than 20 h to generate these results on the same machine it should be noted that in order to have a fair comparison between these two methods all the simlulations and training for both methods were carried out on a linux workstation equipped with 24 intel core 3 47 ghz processors and 96 gb ram it should be noted that similar to pca dnn pcga was run in serial mode and we did not take advantage of multiprocessing for pcga runs the pcga method can get the results much faster on multi core workstations since adh model runs can be independently performed on different cpu processors however even using multi core workstations pcga cannot obtain these results as fast as the pca dnn this shows that the proposed learning algorithm is much faster than even fast physics based methods that are developed for high dimensional problems and still can provide more accurate results in order to compute the uncertainty incurred due to measurement errors we used a parametric bootstrapping approach kitanidis 1995 jeong et al 2018 where an ensemble of 100 sets of measurements are used to make 100 predictions then this ensemble set of prediction can be used to quantify the uncertainty in the measurements sun et al 2017 jeong et al 2018 fig 6 depicts the estimated uncertainty for the savannah river using pca dnn and pcga methods as shown here the estimated uncertainty for deeper locations is higher than the uncertainty at other locations 3 2 bathymetry problem with sparse measurements in the previous test case we used dense measurement regime where we had a flow velocity measurement at each grid point in this test case we are interested to evaluate the performance of the proposed method for cases with sparse measurements in order to achieve this goal we used the same bathymetry profile as the first test case but we only used 415 measurement locations as opposed to the first test case that we had 20 541 measurement locations using the segmentation idea similar as before we can generate several hundreds training sample from each river profile however in the case of sparse measurements the relative coordinates of the measurements with respect to the geometry of the segment varies from one sample to another sample therefore the relative coordinates of the measurements with respect to the segment can be informative for the network as a result knowing the relative location of measurements with respect to the center of the segment is essential for the network this information is used as additional input parameters for the network the train validation and test sets are the same as the first test case but only measurements at the new measurement locations are used here as the input the rmse values for train validation and test sets are 0 71 m 0 76 m and 0 79 m respectively fig 7 shows the estimated river bed profile for the savannah river using sparse measurements as expected the accuracy decreases compared to the results of the first test case since we had a fewer number of measurements however the pca dnn can still provide accurate results using sparse measurement profile 3 3 pca dnn performance on unseen river profiles in the last test problem we examine the performance of pca dnn for unseen river profiles with different profile distributions to evaluate whether we can generalize the performance of the method for new bathymetry problems with different parameter distributions in this test case we consider an approximately 10 km reach of the american river near its confluence with the sacramento river in california usa as with the savannah river example high resolution bathymetry taken from a u s army corps of engineer survey is used to define a true bottom elevation field as shown in fig 8 the length of the domain along the centerline is approximately 10 1 km and the across channel distance ranged between 45 2 m and 150 2 m with an average of 88 7 m a constant discharge of q b 5350 m 3 s is specified at the inflow lower right boundary and a constant free surface elevation of z b 510 m at the outflow left the computational mesh consists of 2001 nodes in the along channel direction 51 nodes in the transverse direction and contains 72 000 triangles the nominal spacing in the along channel direction is 8 4 and 4 12 m in the cross channel direction for this test case 100 stochastic realizations of the river profile and 100 stochastic gaussian realization of river profiles with different parabolic cross sections are generated by using gaussian kernel with the parameters β 1 5 lx 20 60 and ly 100 200 respectively after generating these profiles the corresponding flow velocity measurements at all grid points were obtained from adh simulations in order to examine the performance of the proposed method on unseen data only the training samples generated from the first half of the river profile in the along channel direction were used for training the network to account for the model and measurement uncertainty we added a gaussian error with a standard deviation of 0 05 m s similar to the first test case the river was divided into small segments with a width of 11 grid points along the river this resulted in a dataset of size 298 200 samples this dataset was then shuffled and randomly split into train validation and test sets with ratios of 80 10 and 10 percents respectively a similar network structure was used here but with a different number of hidden units in each layer the training time for pca dnn method was around 45 min for 20 epochs and the prediction step including post processing the results takes less than a minute on the same machine as the first test case the rmse values for train validation and test sets are 0 54 m 0 59 m and 0 59 m respectively fig 8 compares the prediction results of pca dnn with the reference river bed elevation the results show that the proposed method provides accurate results for both halves of the river with rmse values of 0 59 m and 0 63 m for the first and the second halves respectively this shows that the performance of the trained network can be generalized to unseen data and it still can provide accurate results fig 9 shows the estimated uncertainty and estimation error using the pca dnn method the results show that the method can provide more accurate results around the centerline while the accuracy close to the margins of the river decreases along channel estimates are presented in fig 10 showing that the estimated bathymetry successfully captures most large scale variability and the true bathymetry locates within the 95 uncertainty bayesian confidence intervals 4 conclusions in this work we present a data driven inversion modeling technique that can be applied for large dimensional riverine bathymetry problems with limited available data and computational resources the proposed model consists of three major components each of which performs a key task in the bathymetry estimation first the river is divided into small segments using the assumption that the river bed profile is locally dependent on flow velocity observations second the dimension of the output of the network is reduced by projecting the bathymetry profile of each segment onto predefined bases obtained in the offline step of the algorithm in the online step these bases are used to reconstruct the river profile by using the estimated coefficients which are the outputs of dnn the third component is a dnn which is trained in the training step using the preprocessed training data then the trained network is used in the online step for prediction purposes combined with the low rank pca basis for reconstructing the predicted river profile the first two components are used to reduce the size of dnn and also to provide more training samples given limited number of sample river profiles using the first two components the learning part of the algorithm can be trained even on personal computers in a few hours and then in the prediction step it can provide accurate results in a few seconds it should be noted that we used a fully connected neural network for this step however other types of neural network can be explored to improve the method we demonstrated the performance of our proposed method on three test problems the first evaluates the accuracy and prediction power of our method by comparing its performance with that of a powerful fast physics based inverse modeling technique for a high dimensional riverine bathymetry problem the second example shows that this method can be used for more general problems with sparse measurements the final example highlights the applicability and generalization of the proposed method for unseen regions beyond the original domain learned during the training step collectively these examples show that the proposed method is a powerful data driven technique for riverine bathymetry in terms of prediction quality robustness and computational cost and requires a small number of training samples credit authorship contribution statement hojat ghorbanidehno conceptualization methodology software writing review editing jonghyun lee data curation methodology software matthew farthing data curation writing review editing supervision tyler hesser writing review editing supervision eric f darve methodology supervision peter k kitanidis methodology supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported in part by an appointment to the postgraduate research participation program at the u s army engineer research and development center coastal and hydraulics laboratory erdc chl administered by the oak ridge institute for science and education through an interagency agreement between the u s department of energy and erdc the research is also supported in part by funding from the army high performance computing research center ahpcrc sponsored by the u s army research laboratory under contract no w911nf 07 2 0027 at stanford jonghyun lee was supported in part by hawai i experimental program to stimulate competitive research epscor provided by the national science foundation research infrastructure improvement rii track 1 ike wai securing hawai i s water future award oia 1557349 the computing resources for this project were provided under the aws cloud credits for research program the financial support is gratefully acknowledged the data for this paper is available upon request to the corresponding author hojat ghorbanidehno hojjatgh stanford edu 
367,we present two statistical models for downscaling flood hazard indicators derived from upscaled shallow water simulations these downscaling models are based on the decomposition of hazard indicators into linear combinations of spatial patterns obtained from a principal component analysis pca artificial neural networks anns are used to model the relationship between low resolution lr and high resolution hr information drawn from hazard indicators in both statistical models the pca features i e the linear weights of the spatial patterns of the lr hazard indicator are taken as inputs to the anns in the first model there is one ann per hr cell where the hazard indicator is to be estimated and the output of the ann is the hazard indicator value at that cell in the second model there is a single ann for the whole hr mesh whose outputs are the pca features of the hr hazard indicator an estimate of the hazard indicator is obtained by combining the ann s outputs with the hr spatial patterns the two statistical downscaling models are evaluated and compared at estimating the water depth and the norm of the unit discharge two common hazard indicators on simulations from five synthetic urban configurations and one field test case analyses are carried out in terms of relative absolute errors of the statistical downscaling model with respect to the lr hazard indicator they show that i both statistical downscaling models provide estimates that are more accurate than the lr hazard indicator in most cases and ii the second downscaling model yields consistently lower errors for both hazard indicators for all flow scenarios on all configurations considered the statistical models are three orders of magnitude faster than hr flow simulations used in conjunction with upscaled flood models such as porosity models they appear as a promising operational alternative to direct flood hazard assessment from hr flow simulations keywords shallow water models porosity models flow variables for hazard assessment multisite statistical downscaling artificial neural networks principal component analysis 1 introduction flood hazard assessment requires the high resolution mapping of numerous indicators of different natures luke et al 2018 sanders and schubert 2019 sanders et al 2020 two dimensional shallow water models are widely accepted as a reference approach for providing high resolution flow data from which these hazard indicators may be derived however such models remain too computationally demanding in the current state of computer technology to be applicable to entire conurbations within reasonable computational times for this reason upscaled shallow water models have been under development over the past two decades bates 2000 bates and de roo 2000 bruwier et al 2017 chen et al 2012 defina 2000 ferrari et al 2019 guinot 2012 guinot et al 2017 2018 guinot and soares frazão 2006 hervouët et al 2000 özgen et al 2016 sanders et al 2008 viero 2019 a salient advantage of upscaled shallow water models is their computational efficiency with cpu times two to three orders of magnitude smaller than those of classical shallow water models guinot 2012 guinot et al 2017 kim et al 2015 the price to pay for the computational efficiency of an upscaled model is the coarseness of the approach the simulation results are provided in the form of upscaled or averaged flow variables over computational cells the size of one to several houses typically 10 to 50 m for practical purposes such as flood hazard mapping however the knowledge of the flow fields is required with a much finer resolution since hazard indicators are often strongly non linear with respect to the flow variables wagenaar et al 2016 using only coarse scale averages cannot be expected to yield reliable assessments seifert et al 2010 therefore a form of downscaling of the upscaled model simulations is needed to perform relevant hazard assessment in the context of climate change studies statistical downscaling methods are developed to bridge the gap between the low spatial resolution of general circulation models gcms which is in the order of hundreds of km and the resolution needed for impact studies from tens of km down to station locations ayar et al 2016 conventional downscaling approaches are univariate i e they seek to estimate a climatic variable at a single site either a station or a grid cell given information deduced from a simulation generated by a gcm ayar et al 2016 artificial neural networks anns have long been applied in this context hewitson and crane 1996 sailor et al 2000 dibike and coulibaly 2006 cannon and whitfield 2002 anns are non parametric non linear regression algorithms that are considered as universal approximators i e they can approximate any continuous function when trained on informative enough data and provided that the number of hidden neurons is selected adequately bishop 2011 a choice has to be made concerning the subset of gcm grid boxes to use as input in the downscaling method a common approach consists in selecting all the grid boxes in a sufficiently large region and to reduce their dimension by a projection onto a low dimensional feature space with principal component analysis pca ayar et al 2016 hewitson and crane 1996 more recent downscaling approaches perform a multivariate estimation by accounting for dependence structures e g to estimate a climatic variable jointly in several sites in vrac and friederichs 2015 schaake shuffle is applied to restore the empirical dependence structure present in a calibration data set thereby assuming that the co occurrences of the ranks of the variables always remain the same this assumption might be too restrictive in the urban flood hazard context as the range of spatial patterns displayed by the flow field might vary according to values taken by the initial and boundary conditions in contrast cannon 2018 relies on univariate techniques applied iteratively to random projections of the climatic variables whether this approach can scale to very high dimensions is not clear indeed refined shallow water models compute the flow variables over meshes counting tens or hundreds of thousands of discrete cells in this work a downscaling framework is proposed that relies on statistical models to estimate high resolution hr hazard indicators from low resolution lr ones derived from upscaled flow simulations the aim is to obtain fast and accurate estimates of hr hazard indicators for a given flooding configuration building geometry for any flow scenarios initial and boundary conditions two statistical downscaling models inspired from the ones developed in climate change studies are put forward the first statistical model is a conventional univariate approach in which the hr hazard indicator is estimated at each cell of the hr mesh separately to this end as many anns as there are hr cells where the hazard indicator has to be estimated are used the input to the anns is the low dimensional feature representation obtained by applying pca to the lr hazard indicator values over the whole domain the second statistical model makes use of pca not only to represent the lr hazard indicators as low dimensional features but also the hr ones this way both lr and hr hazard indicators are assumed to be decomposed into a linear combination of spatial patterns that are given by the principal components a single ann is set up to model the relationship between the two sets of low dimensional features i e between the linear weights of the lr spatial patterns and the hr ones hr hazard indicator estimates are then obtained by reversing the pca projection i e by combining the ann s outputs with the hr spatial patterns the paper is organised as follows section 2 presents the reference flow model and reviews briefly the indicators of interest in flood hazard assessment the focus is on two hazard indicators the water depth and the norm of the unit discharge section 2 then poses the upscaling problem and ends by introducing the downscaling framework proposed for urban flood hazard assessment section 3 is devoted to the two statistical downscaling models developed in the present work section 4 describes the five synthetic configurations and the field scale test for which simulations of the refined and exact upscaled solutions obtained by averaging each reference solution over a coarse grid are carried out section 5 reports the evaluation and the comparison of the two statistical downscaling models for each configuration considered in section 6 a discussion of the proposed downscaling framework is presented followed by conclusions and research perspectives in section 7 2 flood hazard assessment problem position 2 1 flow model and hazard indicators in what follows the reference hr model sometimes referred to as the microscopic model in the literature is the two dimensional shallow water model written in conservation form as 1a t u f s 1b u h q r f q r q 2 h g 2 h 2 q r h q r h q 2 h g 2 h 2 s 0 g h s 0 x s f x g h s 0 y s f y 1c s f x s f y n 2 h 10 3 q q where g is the gravitational acceleration h is the water depth n is manning s friction coefficient q q r t is the unit discharge vector s 0 x s 0 y t and s f x s f y t are respectively the bottom and friction slope vectors the water depth h and the norm of the unit discharge q are widely recognized as meaningful indicators for building damage blanco vogt and schanze 2014 merz et al 2010 wagenaar et al 2016 2017 and pedestrian safety assessment abt et al 1989 bernardini et al 2017 chanson et al 2014 chanson and brown 2015 cox et al 2010 foster and cox 1973 jonkman and penning rowsell 2008 karvonen et al 2000 matsuo et al 2011 schubert et al 2017 takahashi et al 1992 xia et al 2014 the specific force per unit width q 2 h g h 2 2 is occasionally reported to influence pedestrian evacuation speed bernardini et al 2017 ishigaki et al 2009 for the sake of conciseness the analysis reported hereafter focuses on the water depth and the norm of the unit discharge vector that are the most widely acknowledged indicators for flood hazard and the easiest variables to measure or compute let ψ denote an indicator of interest for hazard assessment the following two definitions are used in the present work 2 ψ h or q q 2 r 2 2 2 upscaling as mentioned in the introduction upscaled models have been developed as cpu efficient alternatives to reference hr shallow water models over large urban areas upscaling is understood as a filtering problem as in farmer 2002 consider a hr and an lr model obeying repectively the following governing equations 3a l hr θ hr u hr 0 3b l lr θ lr u lr 0 where l is a vector differential operator forming the governing equations θ and u are respectively the parameter and variable vectors the two dimensional shallow water model 1a 1c is a particular case of the general form 3a upscaling is understood as the process of deriving l lr model upscaling θ lr parameter upscaling and or u lr solution upscaling from the known hr model 3a since u hr and u lr are defined using different space time resolutions upscaling involves a filtering process the most widely used filter denoted by hereafter in the field of upscaled urban flood models is the averaging operator over the computational cells of the lr model 4 u hr x 1 ω i ω i u hr d ω i x ω i where ω i is the subdomain occupied by the i th computational cell in the lr model and ω i is its area in this approach the subdomains ω i form a partition of the overall computational domain ω the filtered hr solution u hr is compared directly to the finite volume solution u lr of the lr model over the computational cells bruwier et al 2017 guinot et al 2017 2018 kim et al 2015 özgen et al 2016 such a comparison is particularly meaningful when u hr and u lr are both conserved variables perfect upscaling is achieved if the upscaled solution is equal to the filtered hr solution 5 u lr i u hr x i 1 ω i ω i u hr d ω 2 3 proposed downscaling framework downscaling is the reverse operation to upscaling see fig 1 as mentioned in section 2 1 the usual approach to flood hazard assessment represented by the downward arrow on the left hand side of fig 1 consists in deriving the hr hazard indicator ψ see 2 directly from the simulated hr flow variable u hr the proposed downscaling framework consists of two steps i the lr hazard indicator ψ is computed from the simulated lr flow u lr as represented by the downward arrow in the right hand side of fig 1 and ii the hr hazard indicator ψ is estimated from ψ as shown by the leftward arrow at the bottom of fig 1 as stated in the introduction statistical models are widely used to perform downscaling in the context of climate change studies although setting up the statistical models may require a certain amount of work they are capable of providing fast and accurate estimates ayar et al 2016 two such statistical models adapted to this proposed downscaling framework for flood hazard assessment are described in section 3 to set up the statistical downscaling models for each considered flooding configuration pairs of lr and hr flow simulations must be available from which lr and hr hazard indicators are derived within a given flooding configuration the numerical values of the initial boundary conditions are allowed to vary from one pair of simulations to the next resulting in as many so called flow scenarios to ensure good performance of the statistical downscaling models a number of flow scenarios must be available representing sufficiently consistent space time behaviours of the flow fields consistency is appreciated in terms of the broad behaviour of the flow e g a positive wave heading to the left a negative wave heading to the right a set of frictionless simulations etc it is expected that for a given configuration once the downscaling models have seen a representative number of flow scenarios they can be applied to flow scenarios that were not necessarily seen before 3 statistical downscaling models 3 1 cell by cell artificial neural network model this downscaling model called c c ann for short seeks to learn separately for each cell of the hr mesh a relationship between the hr hazard indicator values at that cell and information drawn from the lr hazard indicator over all subdomains there are two main steps in this approach represented by the arrows in fig 2 a the first step top arrow in fig 2a consists in summarizing the lr hazard indicator by features with fewer dimensions obtained by a principal component analysis pca jolliffe and cadima 2016 the second step bottom arrow in fig 2a consists in learning a relationship between these low dimensional features and the hr hazard indicator values at a given cell of the hr mesh with an artificial neural network ann bishop 2011 there are as many anns as cells where the hr hazard indicator needs to be estimated in the first step of c c ann the vector ψ k r d that concatenates the lr hazard indicator over all subdomains at a given time t k is summarized by φ k r d with d d a low dimensional feature representation computed with pca so as to minimize the l 2 norm the low dimensional features φ k are in fact a projection of ψ k onto a linear subspace spanned by the first d eigenvectors of the sample covariance matrix of ψ k let a r d r d be such that each column is one of the first d eigenvectors spanning the linear subspace which are called principal components then since a 1 a t thanks to the orthogonality of the eigenvectors pca yields a decomposition of the form 6 ψ k a φ k φ k a t ψ k in the second step of c c ann the relationship between φ k k the low dimensional features summarizing the lr hazard indicator and ψ j k k the values of the hr hazard indicator at mesh s cell j is learned with an ann there are as many anns as cells j where ψ j needs to be estimated each of these anns is implemented as shown in fig 3a with a standard feed forward architecture that includes one hidden layer plus a direct linear connection such that the case with no neuron in the hidden layer boils down to classical linear regression bishop 2011 in what follows the dependence on time i e the subscript k is dropped to unclutter the notation in fig 3a the input layer of the ann consists of a special neuron permanently set to 1 to account for constants in the calculations and of the vector φ φ 1 φ d of d low dimensional features extracted by pca from the lr hazard indicator the d 1 weight vector w n hid connects the input layer to the n th neuron of the hidden layer with 1 n n h through linear combinations that are transformed non linearly with a hyperbolic tangent as follows 7 a n φ w n hid tanh i 1 d w n i hid φ i w n 0 hid n 1 n h in fig 3a the output layer has a single neuron ψ j that yields estimates of the hr hazard indicator values at the hr cell j computed as 8 ψ j φ w g n 1 n h w n out a n φ w n hid non linear i 1 d w i lin φ i w 0 lin linear for a given j where w out is a weight vector of length n h connecting the hidden layer to the output neuron through linear combinations w lin is a weight vector of length d 1 that links directly the input layer to the output layer through linear combinations and g log 1 exp serves to enforce positivity the weight vector w of the ann includes the weight vectors w n hid with n 1 up to n h the weight vector w out and the weight vector w lin for a given d the dimension of the feature space computed by pca and a given n h the number of neurons in the hidden layer an ann is trained separately for each cell j the training procedure involves optimising the ann s weights w so as to minimise over a training set made of pairs of the form φ k ψ j k k the following sum of squared errors 9 e loc w j 1 2 k ψ j φ k w ψ j k 2 a gradient descent optimisation algorithm is used resorting to the back propagation algorithm to efficiently compute the gradient rumelhart et al 1988 to avoid local minimums the optimisation is performed 10 times with random initial parameter values and the optimised parameters yielding the lowest error computed on the training set are retained this optimisation strategy was assessed by monitoring closely several anns training as d and n h are hyper parameters for each cell j suitable values must be selected with a validation procedure bishop 2011 indeed the complexity level of c c ann is directly related to the overall number of weights in the ann which depends on d and n h that control the size of the input and the hidden layers respectively the validation procedure works as follows several potential pairs of values are considered for the hyper parameters for each such pair of hyper parameter values the ann s weights are optimised on the training set the performance of the ann associated to each particular choice of hyper parameter values is evaluated in terms of the sum of squared errors as in 9 but computed on a validation set a data set distinct from the training set the hyper parameter values yielding the lowest validation error are retained different hyper parameter values are likely to be selected for different cells when the complexity of the relationship learned by the anns differ 3 2 pca spatial pattern based artificial neural network model this second downscaling model termed spattern ann for short seeks to learn a single relationship as opposed to c c ann that seeks to learn as many relationships as there are hr cells between low dimensional features that summarize the hr hazard indicator and low dimensional features that summarize the lr hazard indicator spattern ann has four main steps as shown by the arrows in fig 2b the first step left top arrow in fig 2b is the same as c c ann the lr hazard indicator over all subdomains is summarized by low dimensional features obtained by pca in the second step right top downward arrow in fig 2b the hr hazard indicator over all hr cells is also projected onto a low dimensional feature space with pca the third step horizontal arrow in fig 2b consists in learning a relationship between the low dimensional features of the lr hazard indicator and those of the hr hazard indicator with an ann with a similar architecture as the one used in c c ann see fig 3b the fourth and last step right top upward arrow in fig 2b consists in reconstructing the hr hazard indicator over all hr cells from the low dimensional feature representation estimates provided by the ann the second and fourth steps of spattern ann also rely on the pca decomposition this time applied to the hr hazard indicator instead of the lr hazard indicator as for the lr hazard indicator at a given time t k let ψ k r p be the concatenation into a vector of the hr hazard indicator over all hr cells where p is the total number of cells of the hr mesh let b r p r p p p be the matrix whose columns are the first p eigenvectors of the sample covariance matrix of ψ k then the pca decomposition relates ψ k to low dimensional features ϕ k r p as follows 10 ψ k b ϕ k ϕ k b t ψ k the principal components i e the eigenvectors contained in the columns of b can be interpreted as spatial patterns in spattern ann the hr hazard indicator is thus assumed to be a linear combination of these spatial patterns with the coefficients in the linear combination provided by the low dimensional features ϕ k in the third step of spattern ann an ann whose architecture is depicted in fig 3b seeks to estimate ϕ k k the low dimensional features that serve to weight the spatial patterns in order to reconstruct the hr hazard indicator based on φ k k the low dimensional features of the lr hazard indicator the ann s calculations at the hidden layer are as in 7 while at the output layer the ann has p neurons see fig 3b which perform the following calculations 11 ϕ j φ w n 1 n h w j n out a n φ w hid non linear i 1 d w j i lin φ i w j 0 lin linear j 1 p where as previously the time index k is dropped and w out is now a n h p matrix instead of a vector of length n h the size of the output layer and the absence of positivity constraints on the output neurons are thus the only differences with the architecture of the anns used in c c ann in the fourth step of spattern ann for any time t k the estimated values of the hr hazard indicator over all hr cells are given by 12 ψ φ k w b b ϕ φ k w where ϕ φ k w ϕ 1 φ k w ϕ p φ k w are the ann outputs as provided in 11 and φ k a t ψ k see 6 for a given d size of the input layer a given n h size of the hidden layer and a given p size of the output layer the ann s weigths w are optimized by minimising over a training set made of pairs φ k ϕ k k the following sum of squared errors 13 e fea w 1 2 j 1 p k ϕ j φ k w ϕ j k 2 the same optimisation strategy as in c c ann is used best optimised parameters out of 10 runs of back propagated gradient descent algorithm with random initialisations in spattern ann three hyper parameters control the number of weights in the ann which is directly related to the complexity level of this approach n h and d as in c c ann together with p the dimension of the feature space of the hr hazard indicator see 10 these hyper parameters must also be selected with a validation procedure as described in the c c ann s subsection to take into account the impact of the choice of p the dimension of the feature space of the hr hazard indicator the sum of squared errors that measures the performance on the validation set is different than the one in 13 used for training 14 e tot w b 1 2 j 1 p k ψ j φ k w b ψ j k 2 4 low and high resolution simulated data sets as mentioned in the introduction section a flooding configuration is defined as a given building geometry for which several flow scenarios implemented with initial and or boundary conditions are considered the hr data sets are obtained by solving the two dimensional shallow water eqs 1a 1c the lr data sets are the perfect upscaled solution obtained by averaging the hr simulation over the subdomains as in eq 5 4 1 synthetic urban configurations five synthetic urban configurations are considered they rely on a common layout consisting of a periodic array of length l made of building blocks see fig 4 and table 1 the buildings are aligned along the x and y directions the spatial period and building spacing in the x direction x x y are denoted by l x and w x respectively the computational domain is discretised into a high resolution mesh with 62 5 cm 62 5 cm square cells for 46 080 cells in total the subdomains used to derive the perfectly upscaled solution u lr see 5 are delineated by connecting the centroids of the building blocks dashed line in fig 4 there are 20 such subdomains in total other options are available for the definition of the subdomains for instance they might be centred around the building blocks or shifted by any distance in the x and or y direction besides the subdomains may include more than one x and or y building period the present choice is motivated by two main reasons i the size l x l y is the smallest possible one that keeps the averaging domain periodic thus ensuring maximum spatial resolution for the upscaled solution ii defining the subdomains by connecting the centroids of the buildings is consistent with the meshing strategies required by a number of porosity based shallow water models such as the ip or dip models guinot et al 2017 2018 sanders et al 2008 the lr hazard indicator values over the 20 subdomains are used to constitute the vector ψ r 20 used as input in the two statistical downscaling models see fig 2 however owing to the high computational time required by c c ann the hr hazard indicator values are restricted to three subdomains located slightly after the beginning at the middle and slightly before the end of the computational domain the subdomains x limits are 250 m 300 m 500 m 550 m and 750 m 800 m these three subdomains are selected based on hydraulic considerations more specifically flow gradients that are representative of the whole domain each subdomain contains 2304 cells for a total of 6912 cells considering the three subdomains spattern ann described in section 3 2 is applied on the full set of 6912 cells covering the three subdomains i e ψ r 6912 in fig 2b as c c ann described in section 3 1 requires to learn a separate relationship for each hr cell the number of cells was reduced to 125 within each subdomain for a total of 375 cells over the three subdomains to keep the computation time within reasonable limits i e ψ j j 1 375 in fig 2a for each of the three subdomains the 125 cells are selected as follows see fig 5 the subdomain is divided into 5 rectangular zones one for the central crossroads and four for each of the branches departing from the intersection each of these five zones comprises 5 5 cells spread regularly so as to allow for a maximum coverage of the rectangular zone the first two synthetic configurations considered are 1d negative and positive waves without friction that are 1d boundary value problems bvps these configurations identified as n wave nf and p wave nf respectively for short are one of the simplest possible bvps for layouts of this type the frictionless propagation along the x direction of a wave into still water is simulated fig 6 the bottom is flat the water is initially at rest in the domain with an initial depth h 0 the water level is set instantaneously to a constant value h 1 at the western boundary of the domain in n wave nf h 1 h 0 which yields a negative wave rarefaction wave in p wave nf h 1 h 0 and a positive wave shock wave appears the lr solution u lr is self similar in the x t domain guinot 2017 guinot et al 2017 2018 the next two synthetic configurations use the same geometry as n wave nf and p wave nf fig 6 but with a non zero bottom friction coefficient these configurations identified as n wave wf and p wave wf respectively are cases study closer to real world situations as a consequence of the non zero friction coefficient the upscaled solution is no longer self similar in the x t space both the hr and lr simulations and their spatial gradients span a different range of hydraulic configurations from that of n wave nf and p wave nf the last synthetic configuration identified as dam break is a 2d oblique urban dam break problem without friction the dam break problem is a riemann initial value problem ivp where the water is initially at rest and the water depth is piecewise constant equal to h l and h r respectively on the left and right hand sides of a broken divide line with average orientation se nw fig 7 a this results in an average flow field and wave propagation pattern oriented in the sw ne direction since the flow is diagonal to the main street directions fully meshing the domain involves as many block periods in both directions of space this makes the mesh size and the subsequent computational effort prohibitive the difficulty can be overcome guinot 2017 by meshing only a single block period in the transverse direction fig 7b the topology of the mesh is modified by connecting the northern side of the i th lateral street boundary segment n i in the figure with the southern side of the i 1 th lateral street boundary segment s i 1 in fig 7b while the upscaled solution of an urban dam break problem parallel to the main street axis is known to be self similar in x t guinot 2012 guinot et al 2017 guinot 2017 self similarity disappears when the propagation is oblique with respect to the street axes guinot 2017 the dam break configuration includes both negative and positive wave phenomena 4 2 field scale test case the field scale test case considered was reported in guinot et al 2017 for the evaluation of a porosity based shallow water model the frictionless propagation of a dike break flood wave into a neighbourhood of the sacramento urban area is simulated the test which is referred to as sacramento for short is informative in many aspects i the geometry is real non periodic ii the upscaled hydraulic pattern is genuinely two dimensional and iii the hr flow field exhibits a strong polarisation along two preferential flow directions guinot et al 2017 the dike breach is located on the left hand side of the domain in fig 8 the sacramento neighbourhood is discretised using a hr mesh made of 77 963 cells average cell area 6 5 m 2 for spattern ann subsection 3 2 ψ r 77 963 in fig 2b c c ann is restricted to an area containing 575 hr cells i e ψ j j 1 575 in fig 2a the lr mesh used for upscaling is much coarser with 1682 subdomains average subdomain area 285 m 2 which means that ψ r 1682 the input in the two statistical downscaling models see fig 2 these hr and lr meshes are used for the refined and porosity based shallow water simulations reported in guinot et al 2017 fig 9 shows close up views of the hr and lr meshes of the area where the 575 cells on which c c ann is applied are located 4 3 training validation and test sets in machine learning the focus is on the evaluation of model performance on previously unseen data to assess the so called generalisation capability bishop 2011 the training set is used to optimise the parameters of each model the validation set distinct from the training set serves to select the best hyper parameter values while the test set distinct from the training and validation sets serves to compare the models with the selected hyper parameter values in the flood hazard framework it is expected that statistical downscaling models given a flooding configuration such as positive or negative waves should be able to perform well at estimating the hr hazard indicator based on the lr hazard indicator for any flow scenarios therefore the generalisation capability of a downscaling model may be measured in terms of performance for a given configuration at estimating flow scenarios that were not seen before thus for each configuration the training validation and test sets are defined as pairs of hr and lr hazard indicators simulated for a number of flow scenarios in table 2 and 3 the flow scenarios labelled by a letter used to form the training validation and test sets for the synthetic urban configurations and the field scale test respectively are listed fig 10 illustrates the principle of the training validation test sets design the flow scenarios forming the training set are designed so as to span the space of bc and or ic values for the synthetic urban configurations three flow scenarios forming a right angled triangle in the bc and or ic space are needed since the space is two dimensional for the test field case only two flow scenarios are necessary as there is a single initial condition the validation set is designed so as to contain flow scenarios such that only one of the bc and or ic values is different from the values used in the training set but within their range for the synthetic urban configuration the validation set contains two flow scenarios taken as the midpoints of the two legs of the right angled triangle for the test field case the validation set consists of a single flow scenario defined by the ic value at the midpoint between the values forming the training set the flow scenarios considered for the test set correspond to bc and or ic values that extrapolate in different ways from the values seen for training and validation for the synthetic urban configurations five such flow scenarios are considered where as for the field scale test only two possible flow scenarios are included in the test set the sizes of the training validation and test sets compiled in table 4 depend on the length of the simulation period t and the sampling time step which are set as follows the sampling time step is 10 s in configurations n wave nf n wave wf p wave nf and p wave wf and 5 s for dam break and sacramento for each synthetic urban configuration the length of the simulation period may change with the flow scenario so as to ensure that there is no wave reflection phenomena that would introduce completely different hydraulic behaviors and patterns the negative wave configurations n wave nf and n wave wf have a duration of 400 s 41 time steps for all flow scenarios except for the j one which is set to 260 s 27 time steps p wave nf has a duration of 300 s 31 time steps in call cases except for the flow scenario labelled g that has a time span of 200 s 21 time steps p wave wf has a duration of 300 s 31 time steps in all cases dam break has a 100 s duration 21 time steps for all flow scenarios except the one labelled j that has a duration of 80 s 17 time steps the field scale simulation has a duration of 240 s 49 time steps except for the flow scenario labelled d which spans 480 s 97 time steps to allow the water to reach most areas 5 evaluation and comparison of statistical downscaling models 5 1 principal spatial patterns obtained from the training sets spattern ann described in section 3 2 relies on the assumption that the hr hazard indicator can be decomposed into a linear combination of spatial patterns see 10 these spatial patterns correspond to the eigenvectors contained in the matrix b of the pca decomposition termed principal components the number of spatial patterns that may best reconstruct the hr hazard indicator see 12 is selected on the validation set see the following section 5 2 a preliminary analysis of the spatial patterns uncovered by pca is presented here in order to qualitatively assess their interpretability in terms of hydraulic behaviour to this end pca is applied on the training set of each configuration see tables 2 and 3 and the first six spatial patterns are illustrated for each of the two hazard indicators considered i e the water depth and the norm of the unit discharge see 2 for the sake of conciseness among the synthetic urban configurations only the spatial patterns for dam break which includes both negative and positive waves are illustrated over the subdomain located in the middle of the computational domain with x limits 500 m 550 m as the other two subdomains have similar patterns the oblique orientation of the spatial patterns is visible for both the water depth and the norm of the unit discharge see figs 11 12 13 and 14 the spatial patterns of the water depth and the norm of the unit discharge for sacramento are shown although the spatial patterns of the norm of the unit discharge are sharper than those of the water depth the general shape with the propagation of the water from the breach in the dike located at the top left see fig 8 is similar for both hazard indicators 5 2 hyper parameter selected on the validation sets hyper parameters are selected for each downscaling model following a data driven procedure that relies on the performance evaluated on validation sets so as to estimate the generalisation capability several values are considered for each hyper parameter so as to span the range of possibilities starting from the lowest admissible value up to a value large enough to ensure that the selected value is not involuntarily bounded the performance of each combination of values considered for each hyper parameter is evaluated i e the corresponding anns are trained and the best combination i e the one that yields the lowest error on the validation set is retained for the five synthetic urban configurations and the field scale test the hyper parameters selected for the two downscaling models described in section 3 and for the two hazard indicators considered water depth and norm of the unit discharge are provided in table 5 for c c ann there are as many anns as hr cells 375 for the synthetic configurations and 575 for the field test case see section 4 therefore as many hyper parameters as hr cells are selected these are summarized by the median and the first and third quartiles in parentheses in table 5 the values selected for d the feature space dimension of the lr hazard indicator may be very different for the two downscaling models in particular for sacramento low values median of 3 for the two hazard indicators are selected for c c ann while high values 57 and 78 for water depth and unit discharge norm respectively are chosen for spattern ann in contrast for the unit discharge norm in the negative and positive wave configurations with or without friction the values selected for d are similar for n h the number of hidden units of the ann c c ann may select relatively high values for the positive wave configurations with or without friction for both hazard indicators in contrast in spattern ann n h is almost always zero indicating that a linear relationship is sufficient as the complexity may be embedded in the feature space representations of the hr hazard indicator provided by pca indeed the values of p selected the number of spatial patterns or equivalently the number of pcs in spattern ann is much lower when the hazard indicator is in a configuration yielding smoother dynamics this is the case for the water depth in the negative or positive wave configurations with or without friction in contrast the water depth in the dam break and sacramento configurations and the norm of the unit discharge especially in the p wave nf wf and sacramento configurations require higher numbers of spatial patterns the only configuration in which the norm of the unit discharge requires less spatial patterns than the water depth is the dam break configuration 5 3 relative errors computed on the test sets the performance of c c ann and spattern ann is compared on test sets made of several flow scenarios different than the ones that constitute the training and validation sets see tables 2 and 3 once again performance evaluation on previously unseen data aims to estimate the generalisation capability of the models the anns of the c c ann one per cell and the ann of spattern ann with the hyper parameters selected on the validation sets see table 5 are trained anew on a larger data set that merges together the training and validation sets in tables 6 and 7 the performance is provided in terms of the following relative error computed on the test sets for all configurations and for both hazard indicators 15 r j k ψ j k ψ j k ψ j k ψ i j k for j an element of the hr mesh k a simulation time step and i j the subdomain to which cell j belongs only time steps k such that ψ j k ψ i j k is among the 5 largest values are kept in the following analyses this choice is made on one hand to avoid infinite or very large values of r j k as ψ i j k ψ j k may be equal to zero or take on very small values and on the other hand to focus on time steps that are more challenging owing to a greater variability within the subdomain the relative error r j k from 15 can be interpreted as the estimation error of the downscaling model relative to the error made when using lr hazard indicator values as surrogates for hr hazard indicator values in particular whenever r j k 1 the downscaling model provides more accurate estimates that the lr hazard indicator table 6 concerns water depth downscaling and provides for each configuration and each test set in the second column the minimum and maximum values taken by the simulated hr water depth field in the third column e lr gives the 95 quantile of ψ j k ψ i j k the error of the simulated lr water depth field in the last two columns the median followed by the first and third quartiles in parentheses of r j k over time steps k such that ψ j k ψ i j k e lr for each of the two downscaling models in table 6 it can be seen that c c ann outperforms spattern ann i e the inter quartile interval of the relative error r j k is strictly smaller for the two negative wave configurations n wave nf and n wave wf over three test sets labelled f g and h out of five however for the two remaining test sets labelled i and j the relative error of c c ann is greater than one indicating that its absolute error is bigger than the absolute error of the lr hazard indicator in contrast the relative error of spattern ann remains way below one for all test sets for the two positive wave configurations p wave nf and p wave wf and for the dam break the relative errors of both downscaling models is comparable and below one for the first three test sets labelled f g and h for the last two test sets labelled i and j the relative error of c c ann reaches very high values whereas spattern ann s relative error remains low although somewhat above one in some cases for the field scale test sacramento both downscaling models performed rather well although spattern ann yielded slightly lower relative errors especially for the first test set labelled d table 7 contains the same information as in table 6 but concerns unit discharge norm downscaling for this hazard indicator the two downscaling models yielded in most cases a comparable performance in particular their relative error is low and bounded away from one one notable exception is for the test set i of dam break for which c c ann yielded a very poor performance whereas spattern ann performed relatively well although with relative error values moderately above one for sacramento spattern ann provides a good performance on both test sets with relative errors below one whereas c c ann is more challenged especially for the first test set labelled d 6 discussion we proposed a downscaling framework section 2 3 and fig 1 based on statistical models to estimate hr hazard indicators from lr hazard indicators derived from upscaled flow simulations such as those obtained from porosity models two such statistical models are proposed in section 3 both rely on non parametric data driven approaches that are very flexible since no strong parametric assumptions are made nevertheless the training set must be sufficiently representative of the underlying structure that is being estimated in order to enable the non parametric models to generalise well i e to perform well on previously unseen data in the operational phase this downscaling framework offers a considerable speedup over running a hr simulation for each new flow scenario for instance for one flow scenario of n wave nf 150 cpu s are needed to run the hr model in comparison running a lr porosity model and downscaling the results using spattern ann requires 7 5 10 3 2 22 10 2 2 97 10 2 cpu s the speedup factor is thus slightly above 5000 the first statistical downscaling model named c c ann is viewed as a reference approach as it relies on the same building blocks as conventional approaches to statistical downscaling that were applied in climate change studies see hewitson and crane 1996 furthermore c c ann is a univariate approach that does not attempt to capture the dependence structure of the hr hazard indicators indeed downscaling is performed cell by cell with an ann dedicated to each cell of the hr mesh see section 3 1 and fig 2a a potential alternative to this reference approach would be to consider a single ann that would be able to downscale all the cells of the mesh one at a time by including in its input specific information from each cell for instance in chadwick et al 2011 a climate variable simulated on a 25 km resolution grid by a regional climate model rcm constrained by a gcm on a lower resolution grid of 1 89 is downscaled with a single ann one rcm cell at a time among the ann input there is information from the large scale variable at the four gcm grid cells surrounding the rcm grid cell of interest by using information that changes with each rcm cell the ann is able to learn a relationship that can vary from cell to cell such an approach was considered initially in the shallow water models context but was put aside indeed as the number of cells within each subdomain is very high the question of which spatial information geographical coordinates not being sufficient would be useful to help discriminate each cell has no straightforward answer the second statistical downscaling model named spattern ann is capable of estimating very high dimensional fields such as the ones simulated by refined shallow water models see section 3 2 and fig 2b a key step of this downscaling model consists in decomposing the high dimensional hr hazard indicator into a linear combination of spatial patterns pca is often used to obtain spatial patterns from fields of climatic variables in order to infer weather types i e recurring patterns that can be found for instance in large scale atmospheric circulation yiou and nogaj 2004 the weights of the linear combination of spatial patterns lie on a low linear subspace of the high dimensional hr hazard indicator called feature space in the weather type approach mentioned above clustering can be performed on the features i e the linear weights obtained from pca to classify each time steps e g days into weather types in contrast in the proposed spattern ann model these features are taken as the multivariate dependent variable in a regression model a feed forward neural network with a direct linear connection as in fig 3b the aim of spattern ann is to estimate the low dimensional features of the hr hazard indicators from the low dimensional features of the lr hazard indicators several low and high resolution hydraulic simulations are used for the evaluation and the comparison of the statistical downscaling models see section 4 in this paper the perfect upscaled solution is used see 5 so as to evaluate the feasibility of such a downscaling framework when the upscaled flow fields are unbiased this allows the error arising from scale change to be assessed independently from the bias stemming from an inevitably inaccurate porosity model five synthetic urban flooding configurations are considered these idealistic cases are interesting in that i they may appear as elementary components of more complex real cases ii their geometry is entirely under control iii they allow the respective influences of the propagation and friction process to be assessed one field scale test is also included in the flooding configurations these pairs of hr and lr simulations serve i to set up the statistical downscaling models i e to optimise their parameters and to select adequate hyper parameter values with a training validation procedure and ii to evaluate their performance on flow scenarios forming the test set that were not used to set up the models the underlying rationale is to assess the generalisation capability of the statistical downscaling models i e their performance when estimating hr hazard indicator values for previously unseen flow scenarios a preliminary visual inspection of the spatial patterns obtained by pca showed that these are interpretable according to each configuration and for the two hazard indicators considered water depth and norm of the unit discharge see figs 11 12 13 and 14 for the dam break and sacramento configuration respectively the selection of hyper parameters on validation sets see table 5 showed that as was anticipated spattern ann needs more spatial patterns i e a larger dimension of the feature space of pca to reconstruct the spatial field of more turbulent hazard indicators with no hidden units selected in most cases for the ann spattern ann is in fact a large dimensional multivariate linear regression the complexity of spattern ann is rather controlled by the aforementioned number of spatial patterns the estimation of the model is carried out by combining three steps 1 pca of the lr hazard indicator 2 pca of the hr hazard indicator and 3 regression between the feature space of the low and high resolution fields despite being a linear model the estimation could not be achieved with a single direct estimation step as is the case with conventional linear regression owing to the large dimension of the dependent variable on the other hand c c ann requires decreasing hyper parameter values that are associated to a decreasing number of parameters in anns for negative waves positive waves dam break and sacramento especially for water depth as the hazard indicator the relative error see 15 used to evaluate the performance on the flow scenarios forming the test sets allows to directly assess the added value in terms of absolute error of the downscaling framework with respect to crude estimates provided by the lr hazard indicator the following conclusions are drawn from tables 6 and 7 for all five synthetic configurations the flow scenarios labelled i and j proved to be more difficult most likely owing to the stronger extrapolation in terms of bc and or ic values with respect to the ones seen during training and validation see fig 10 and table 2 comparing across synthetic configurations regardless of the downscaling model used negative wave configurations yield significantly smaller errors than positive wave configurations compare n wave and p wave in tables 6 and 7 friction exerts a much milder influence on the performance of downscaling models than the wave pattern does compare nf and wf in the tables this was to be expected because negative wave configurations yield much smoother flow fields than positive waves in most positive wave simulations the moving shock spreads within one to two lr cells inducing strong gradients and considerable variability in the hr flow such variability is very difficult to capture from the lr averaged variables in contrast negative waves often spread over several lr grid cells and the hr flow patterns are much less variable in dam break that includes both a negative and a positive wave the error is clearly dominated by that of the moving shock besides the waves propagate along the diagonal of the urban layout this induces a larger hr flow variability than with the n wave and p wave configurations where the waves propagate along the main axis of the urban layout the question might then arise why the sacramento configuration that involves a positive wave yields significantly smaller relative errors than the dam break configuration compare for instance the performance of spattern ann for sacramento flow scenarios d e with dam break flow scenarios i j that are all extrapolations it must be noticed however that the wave in the sacramento configuration propagates over a dry bottom therefore the positive wave is essentially a rarefaction wave on the lr scale not a shock shocks appear locally on the hr scale but they are due to the local wave reflections against the building walls they are not concentrated in the immediate neighbourhood of a moving bore as in the p wave and dam break configurations as a conclusion the smoothness of the flow mainly driven by the wave propagation process appears to be the predominant factor to downscaling accuracy 7 conclusion the analyses carried out in this work showed that the proposed downscaling framework may yield fast and accurate estimates of the hr hazard indicators either the water depth or the norm of the unit discharge for a large number of flow scenarios for the five synthetic urban configurations and the field scale test in particular the spattern ann statistical downscaling model provided consistently good performance further work is needed to understand how to bring improvements to spattern ann an interesting avenue of research would be to investigate the representativeness of the spatial pattern basis does the training set contain informative enough data to uncover the spatial pattern basis in other words does the training set include all the spatial patterns that are present in the validation and test sets another avenue would be to consider techniques other than pca to deduce the spatial patterns such as frames christensen 2008 besides stochasticity could be introduced in the downscaling methods which would be helpful to account for uncertainties in the estimation for c c ann it suffices to see the outputs of the ann as the parameters of a given probability distribution cannon 2012 carreau and vrac 2011 for spattern ann the stochastic version of pca could be implemented bishop 2011 other perspectives for this work are as follows the hydraulic simulations reported involve a flat topography the performance of the downscaling models in the case of a variable topography should be explored in the case of a spatially variable bottom elevation the free surface elevation is often smoother than the water depth whether the surface elevation is easier to downscale than the water depth should be assessed in addition imperfect upscaling should be tested as nonlocal effects are to be expected however the performance of the statistical downscaling models will not necessarily decrease as statistical strategies termed bias correction in the climate change context ayar et al 2016 might allow to circumvent the biases present in porosity model simulations as mentioned in section 2 1 the water depth and the norm of the unit discharge are not the only possible variables for flood hazard assessment the possibility of downscaling additional variables such as the specific force per unit width and the hydraulic head should also be investigated this might induce an increased level of complexity compared to the downscaling of the water depth and the unit discharge because the specific force and the hydraulic head are not conserved variables in the hr governing equations last in some cases only the maximum of a given flow variable within a given area might be needed for hazard assessment in such cases downscaling techniques developed within the theory of extreme values could be useful bechler et al 2015 credit authorship contribution statement j carreau conceptualization methodology software validation formal analysis writing original draft visualization v guinot conceptualization methodology software validation writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was partially supported by the french national program lefe insu fraise european program prima altos and bi national program hubert curien france tunisie amande supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j advwatres 2020 103821 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
367,we present two statistical models for downscaling flood hazard indicators derived from upscaled shallow water simulations these downscaling models are based on the decomposition of hazard indicators into linear combinations of spatial patterns obtained from a principal component analysis pca artificial neural networks anns are used to model the relationship between low resolution lr and high resolution hr information drawn from hazard indicators in both statistical models the pca features i e the linear weights of the spatial patterns of the lr hazard indicator are taken as inputs to the anns in the first model there is one ann per hr cell where the hazard indicator is to be estimated and the output of the ann is the hazard indicator value at that cell in the second model there is a single ann for the whole hr mesh whose outputs are the pca features of the hr hazard indicator an estimate of the hazard indicator is obtained by combining the ann s outputs with the hr spatial patterns the two statistical downscaling models are evaluated and compared at estimating the water depth and the norm of the unit discharge two common hazard indicators on simulations from five synthetic urban configurations and one field test case analyses are carried out in terms of relative absolute errors of the statistical downscaling model with respect to the lr hazard indicator they show that i both statistical downscaling models provide estimates that are more accurate than the lr hazard indicator in most cases and ii the second downscaling model yields consistently lower errors for both hazard indicators for all flow scenarios on all configurations considered the statistical models are three orders of magnitude faster than hr flow simulations used in conjunction with upscaled flood models such as porosity models they appear as a promising operational alternative to direct flood hazard assessment from hr flow simulations keywords shallow water models porosity models flow variables for hazard assessment multisite statistical downscaling artificial neural networks principal component analysis 1 introduction flood hazard assessment requires the high resolution mapping of numerous indicators of different natures luke et al 2018 sanders and schubert 2019 sanders et al 2020 two dimensional shallow water models are widely accepted as a reference approach for providing high resolution flow data from which these hazard indicators may be derived however such models remain too computationally demanding in the current state of computer technology to be applicable to entire conurbations within reasonable computational times for this reason upscaled shallow water models have been under development over the past two decades bates 2000 bates and de roo 2000 bruwier et al 2017 chen et al 2012 defina 2000 ferrari et al 2019 guinot 2012 guinot et al 2017 2018 guinot and soares frazão 2006 hervouët et al 2000 özgen et al 2016 sanders et al 2008 viero 2019 a salient advantage of upscaled shallow water models is their computational efficiency with cpu times two to three orders of magnitude smaller than those of classical shallow water models guinot 2012 guinot et al 2017 kim et al 2015 the price to pay for the computational efficiency of an upscaled model is the coarseness of the approach the simulation results are provided in the form of upscaled or averaged flow variables over computational cells the size of one to several houses typically 10 to 50 m for practical purposes such as flood hazard mapping however the knowledge of the flow fields is required with a much finer resolution since hazard indicators are often strongly non linear with respect to the flow variables wagenaar et al 2016 using only coarse scale averages cannot be expected to yield reliable assessments seifert et al 2010 therefore a form of downscaling of the upscaled model simulations is needed to perform relevant hazard assessment in the context of climate change studies statistical downscaling methods are developed to bridge the gap between the low spatial resolution of general circulation models gcms which is in the order of hundreds of km and the resolution needed for impact studies from tens of km down to station locations ayar et al 2016 conventional downscaling approaches are univariate i e they seek to estimate a climatic variable at a single site either a station or a grid cell given information deduced from a simulation generated by a gcm ayar et al 2016 artificial neural networks anns have long been applied in this context hewitson and crane 1996 sailor et al 2000 dibike and coulibaly 2006 cannon and whitfield 2002 anns are non parametric non linear regression algorithms that are considered as universal approximators i e they can approximate any continuous function when trained on informative enough data and provided that the number of hidden neurons is selected adequately bishop 2011 a choice has to be made concerning the subset of gcm grid boxes to use as input in the downscaling method a common approach consists in selecting all the grid boxes in a sufficiently large region and to reduce their dimension by a projection onto a low dimensional feature space with principal component analysis pca ayar et al 2016 hewitson and crane 1996 more recent downscaling approaches perform a multivariate estimation by accounting for dependence structures e g to estimate a climatic variable jointly in several sites in vrac and friederichs 2015 schaake shuffle is applied to restore the empirical dependence structure present in a calibration data set thereby assuming that the co occurrences of the ranks of the variables always remain the same this assumption might be too restrictive in the urban flood hazard context as the range of spatial patterns displayed by the flow field might vary according to values taken by the initial and boundary conditions in contrast cannon 2018 relies on univariate techniques applied iteratively to random projections of the climatic variables whether this approach can scale to very high dimensions is not clear indeed refined shallow water models compute the flow variables over meshes counting tens or hundreds of thousands of discrete cells in this work a downscaling framework is proposed that relies on statistical models to estimate high resolution hr hazard indicators from low resolution lr ones derived from upscaled flow simulations the aim is to obtain fast and accurate estimates of hr hazard indicators for a given flooding configuration building geometry for any flow scenarios initial and boundary conditions two statistical downscaling models inspired from the ones developed in climate change studies are put forward the first statistical model is a conventional univariate approach in which the hr hazard indicator is estimated at each cell of the hr mesh separately to this end as many anns as there are hr cells where the hazard indicator has to be estimated are used the input to the anns is the low dimensional feature representation obtained by applying pca to the lr hazard indicator values over the whole domain the second statistical model makes use of pca not only to represent the lr hazard indicators as low dimensional features but also the hr ones this way both lr and hr hazard indicators are assumed to be decomposed into a linear combination of spatial patterns that are given by the principal components a single ann is set up to model the relationship between the two sets of low dimensional features i e between the linear weights of the lr spatial patterns and the hr ones hr hazard indicator estimates are then obtained by reversing the pca projection i e by combining the ann s outputs with the hr spatial patterns the paper is organised as follows section 2 presents the reference flow model and reviews briefly the indicators of interest in flood hazard assessment the focus is on two hazard indicators the water depth and the norm of the unit discharge section 2 then poses the upscaling problem and ends by introducing the downscaling framework proposed for urban flood hazard assessment section 3 is devoted to the two statistical downscaling models developed in the present work section 4 describes the five synthetic configurations and the field scale test for which simulations of the refined and exact upscaled solutions obtained by averaging each reference solution over a coarse grid are carried out section 5 reports the evaluation and the comparison of the two statistical downscaling models for each configuration considered in section 6 a discussion of the proposed downscaling framework is presented followed by conclusions and research perspectives in section 7 2 flood hazard assessment problem position 2 1 flow model and hazard indicators in what follows the reference hr model sometimes referred to as the microscopic model in the literature is the two dimensional shallow water model written in conservation form as 1a t u f s 1b u h q r f q r q 2 h g 2 h 2 q r h q r h q 2 h g 2 h 2 s 0 g h s 0 x s f x g h s 0 y s f y 1c s f x s f y n 2 h 10 3 q q where g is the gravitational acceleration h is the water depth n is manning s friction coefficient q q r t is the unit discharge vector s 0 x s 0 y t and s f x s f y t are respectively the bottom and friction slope vectors the water depth h and the norm of the unit discharge q are widely recognized as meaningful indicators for building damage blanco vogt and schanze 2014 merz et al 2010 wagenaar et al 2016 2017 and pedestrian safety assessment abt et al 1989 bernardini et al 2017 chanson et al 2014 chanson and brown 2015 cox et al 2010 foster and cox 1973 jonkman and penning rowsell 2008 karvonen et al 2000 matsuo et al 2011 schubert et al 2017 takahashi et al 1992 xia et al 2014 the specific force per unit width q 2 h g h 2 2 is occasionally reported to influence pedestrian evacuation speed bernardini et al 2017 ishigaki et al 2009 for the sake of conciseness the analysis reported hereafter focuses on the water depth and the norm of the unit discharge vector that are the most widely acknowledged indicators for flood hazard and the easiest variables to measure or compute let ψ denote an indicator of interest for hazard assessment the following two definitions are used in the present work 2 ψ h or q q 2 r 2 2 2 upscaling as mentioned in the introduction upscaled models have been developed as cpu efficient alternatives to reference hr shallow water models over large urban areas upscaling is understood as a filtering problem as in farmer 2002 consider a hr and an lr model obeying repectively the following governing equations 3a l hr θ hr u hr 0 3b l lr θ lr u lr 0 where l is a vector differential operator forming the governing equations θ and u are respectively the parameter and variable vectors the two dimensional shallow water model 1a 1c is a particular case of the general form 3a upscaling is understood as the process of deriving l lr model upscaling θ lr parameter upscaling and or u lr solution upscaling from the known hr model 3a since u hr and u lr are defined using different space time resolutions upscaling involves a filtering process the most widely used filter denoted by hereafter in the field of upscaled urban flood models is the averaging operator over the computational cells of the lr model 4 u hr x 1 ω i ω i u hr d ω i x ω i where ω i is the subdomain occupied by the i th computational cell in the lr model and ω i is its area in this approach the subdomains ω i form a partition of the overall computational domain ω the filtered hr solution u hr is compared directly to the finite volume solution u lr of the lr model over the computational cells bruwier et al 2017 guinot et al 2017 2018 kim et al 2015 özgen et al 2016 such a comparison is particularly meaningful when u hr and u lr are both conserved variables perfect upscaling is achieved if the upscaled solution is equal to the filtered hr solution 5 u lr i u hr x i 1 ω i ω i u hr d ω 2 3 proposed downscaling framework downscaling is the reverse operation to upscaling see fig 1 as mentioned in section 2 1 the usual approach to flood hazard assessment represented by the downward arrow on the left hand side of fig 1 consists in deriving the hr hazard indicator ψ see 2 directly from the simulated hr flow variable u hr the proposed downscaling framework consists of two steps i the lr hazard indicator ψ is computed from the simulated lr flow u lr as represented by the downward arrow in the right hand side of fig 1 and ii the hr hazard indicator ψ is estimated from ψ as shown by the leftward arrow at the bottom of fig 1 as stated in the introduction statistical models are widely used to perform downscaling in the context of climate change studies although setting up the statistical models may require a certain amount of work they are capable of providing fast and accurate estimates ayar et al 2016 two such statistical models adapted to this proposed downscaling framework for flood hazard assessment are described in section 3 to set up the statistical downscaling models for each considered flooding configuration pairs of lr and hr flow simulations must be available from which lr and hr hazard indicators are derived within a given flooding configuration the numerical values of the initial boundary conditions are allowed to vary from one pair of simulations to the next resulting in as many so called flow scenarios to ensure good performance of the statistical downscaling models a number of flow scenarios must be available representing sufficiently consistent space time behaviours of the flow fields consistency is appreciated in terms of the broad behaviour of the flow e g a positive wave heading to the left a negative wave heading to the right a set of frictionless simulations etc it is expected that for a given configuration once the downscaling models have seen a representative number of flow scenarios they can be applied to flow scenarios that were not necessarily seen before 3 statistical downscaling models 3 1 cell by cell artificial neural network model this downscaling model called c c ann for short seeks to learn separately for each cell of the hr mesh a relationship between the hr hazard indicator values at that cell and information drawn from the lr hazard indicator over all subdomains there are two main steps in this approach represented by the arrows in fig 2 a the first step top arrow in fig 2a consists in summarizing the lr hazard indicator by features with fewer dimensions obtained by a principal component analysis pca jolliffe and cadima 2016 the second step bottom arrow in fig 2a consists in learning a relationship between these low dimensional features and the hr hazard indicator values at a given cell of the hr mesh with an artificial neural network ann bishop 2011 there are as many anns as cells where the hr hazard indicator needs to be estimated in the first step of c c ann the vector ψ k r d that concatenates the lr hazard indicator over all subdomains at a given time t k is summarized by φ k r d with d d a low dimensional feature representation computed with pca so as to minimize the l 2 norm the low dimensional features φ k are in fact a projection of ψ k onto a linear subspace spanned by the first d eigenvectors of the sample covariance matrix of ψ k let a r d r d be such that each column is one of the first d eigenvectors spanning the linear subspace which are called principal components then since a 1 a t thanks to the orthogonality of the eigenvectors pca yields a decomposition of the form 6 ψ k a φ k φ k a t ψ k in the second step of c c ann the relationship between φ k k the low dimensional features summarizing the lr hazard indicator and ψ j k k the values of the hr hazard indicator at mesh s cell j is learned with an ann there are as many anns as cells j where ψ j needs to be estimated each of these anns is implemented as shown in fig 3a with a standard feed forward architecture that includes one hidden layer plus a direct linear connection such that the case with no neuron in the hidden layer boils down to classical linear regression bishop 2011 in what follows the dependence on time i e the subscript k is dropped to unclutter the notation in fig 3a the input layer of the ann consists of a special neuron permanently set to 1 to account for constants in the calculations and of the vector φ φ 1 φ d of d low dimensional features extracted by pca from the lr hazard indicator the d 1 weight vector w n hid connects the input layer to the n th neuron of the hidden layer with 1 n n h through linear combinations that are transformed non linearly with a hyperbolic tangent as follows 7 a n φ w n hid tanh i 1 d w n i hid φ i w n 0 hid n 1 n h in fig 3a the output layer has a single neuron ψ j that yields estimates of the hr hazard indicator values at the hr cell j computed as 8 ψ j φ w g n 1 n h w n out a n φ w n hid non linear i 1 d w i lin φ i w 0 lin linear for a given j where w out is a weight vector of length n h connecting the hidden layer to the output neuron through linear combinations w lin is a weight vector of length d 1 that links directly the input layer to the output layer through linear combinations and g log 1 exp serves to enforce positivity the weight vector w of the ann includes the weight vectors w n hid with n 1 up to n h the weight vector w out and the weight vector w lin for a given d the dimension of the feature space computed by pca and a given n h the number of neurons in the hidden layer an ann is trained separately for each cell j the training procedure involves optimising the ann s weights w so as to minimise over a training set made of pairs of the form φ k ψ j k k the following sum of squared errors 9 e loc w j 1 2 k ψ j φ k w ψ j k 2 a gradient descent optimisation algorithm is used resorting to the back propagation algorithm to efficiently compute the gradient rumelhart et al 1988 to avoid local minimums the optimisation is performed 10 times with random initial parameter values and the optimised parameters yielding the lowest error computed on the training set are retained this optimisation strategy was assessed by monitoring closely several anns training as d and n h are hyper parameters for each cell j suitable values must be selected with a validation procedure bishop 2011 indeed the complexity level of c c ann is directly related to the overall number of weights in the ann which depends on d and n h that control the size of the input and the hidden layers respectively the validation procedure works as follows several potential pairs of values are considered for the hyper parameters for each such pair of hyper parameter values the ann s weights are optimised on the training set the performance of the ann associated to each particular choice of hyper parameter values is evaluated in terms of the sum of squared errors as in 9 but computed on a validation set a data set distinct from the training set the hyper parameter values yielding the lowest validation error are retained different hyper parameter values are likely to be selected for different cells when the complexity of the relationship learned by the anns differ 3 2 pca spatial pattern based artificial neural network model this second downscaling model termed spattern ann for short seeks to learn a single relationship as opposed to c c ann that seeks to learn as many relationships as there are hr cells between low dimensional features that summarize the hr hazard indicator and low dimensional features that summarize the lr hazard indicator spattern ann has four main steps as shown by the arrows in fig 2b the first step left top arrow in fig 2b is the same as c c ann the lr hazard indicator over all subdomains is summarized by low dimensional features obtained by pca in the second step right top downward arrow in fig 2b the hr hazard indicator over all hr cells is also projected onto a low dimensional feature space with pca the third step horizontal arrow in fig 2b consists in learning a relationship between the low dimensional features of the lr hazard indicator and those of the hr hazard indicator with an ann with a similar architecture as the one used in c c ann see fig 3b the fourth and last step right top upward arrow in fig 2b consists in reconstructing the hr hazard indicator over all hr cells from the low dimensional feature representation estimates provided by the ann the second and fourth steps of spattern ann also rely on the pca decomposition this time applied to the hr hazard indicator instead of the lr hazard indicator as for the lr hazard indicator at a given time t k let ψ k r p be the concatenation into a vector of the hr hazard indicator over all hr cells where p is the total number of cells of the hr mesh let b r p r p p p be the matrix whose columns are the first p eigenvectors of the sample covariance matrix of ψ k then the pca decomposition relates ψ k to low dimensional features ϕ k r p as follows 10 ψ k b ϕ k ϕ k b t ψ k the principal components i e the eigenvectors contained in the columns of b can be interpreted as spatial patterns in spattern ann the hr hazard indicator is thus assumed to be a linear combination of these spatial patterns with the coefficients in the linear combination provided by the low dimensional features ϕ k in the third step of spattern ann an ann whose architecture is depicted in fig 3b seeks to estimate ϕ k k the low dimensional features that serve to weight the spatial patterns in order to reconstruct the hr hazard indicator based on φ k k the low dimensional features of the lr hazard indicator the ann s calculations at the hidden layer are as in 7 while at the output layer the ann has p neurons see fig 3b which perform the following calculations 11 ϕ j φ w n 1 n h w j n out a n φ w hid non linear i 1 d w j i lin φ i w j 0 lin linear j 1 p where as previously the time index k is dropped and w out is now a n h p matrix instead of a vector of length n h the size of the output layer and the absence of positivity constraints on the output neurons are thus the only differences with the architecture of the anns used in c c ann in the fourth step of spattern ann for any time t k the estimated values of the hr hazard indicator over all hr cells are given by 12 ψ φ k w b b ϕ φ k w where ϕ φ k w ϕ 1 φ k w ϕ p φ k w are the ann outputs as provided in 11 and φ k a t ψ k see 6 for a given d size of the input layer a given n h size of the hidden layer and a given p size of the output layer the ann s weigths w are optimized by minimising over a training set made of pairs φ k ϕ k k the following sum of squared errors 13 e fea w 1 2 j 1 p k ϕ j φ k w ϕ j k 2 the same optimisation strategy as in c c ann is used best optimised parameters out of 10 runs of back propagated gradient descent algorithm with random initialisations in spattern ann three hyper parameters control the number of weights in the ann which is directly related to the complexity level of this approach n h and d as in c c ann together with p the dimension of the feature space of the hr hazard indicator see 10 these hyper parameters must also be selected with a validation procedure as described in the c c ann s subsection to take into account the impact of the choice of p the dimension of the feature space of the hr hazard indicator the sum of squared errors that measures the performance on the validation set is different than the one in 13 used for training 14 e tot w b 1 2 j 1 p k ψ j φ k w b ψ j k 2 4 low and high resolution simulated data sets as mentioned in the introduction section a flooding configuration is defined as a given building geometry for which several flow scenarios implemented with initial and or boundary conditions are considered the hr data sets are obtained by solving the two dimensional shallow water eqs 1a 1c the lr data sets are the perfect upscaled solution obtained by averaging the hr simulation over the subdomains as in eq 5 4 1 synthetic urban configurations five synthetic urban configurations are considered they rely on a common layout consisting of a periodic array of length l made of building blocks see fig 4 and table 1 the buildings are aligned along the x and y directions the spatial period and building spacing in the x direction x x y are denoted by l x and w x respectively the computational domain is discretised into a high resolution mesh with 62 5 cm 62 5 cm square cells for 46 080 cells in total the subdomains used to derive the perfectly upscaled solution u lr see 5 are delineated by connecting the centroids of the building blocks dashed line in fig 4 there are 20 such subdomains in total other options are available for the definition of the subdomains for instance they might be centred around the building blocks or shifted by any distance in the x and or y direction besides the subdomains may include more than one x and or y building period the present choice is motivated by two main reasons i the size l x l y is the smallest possible one that keeps the averaging domain periodic thus ensuring maximum spatial resolution for the upscaled solution ii defining the subdomains by connecting the centroids of the buildings is consistent with the meshing strategies required by a number of porosity based shallow water models such as the ip or dip models guinot et al 2017 2018 sanders et al 2008 the lr hazard indicator values over the 20 subdomains are used to constitute the vector ψ r 20 used as input in the two statistical downscaling models see fig 2 however owing to the high computational time required by c c ann the hr hazard indicator values are restricted to three subdomains located slightly after the beginning at the middle and slightly before the end of the computational domain the subdomains x limits are 250 m 300 m 500 m 550 m and 750 m 800 m these three subdomains are selected based on hydraulic considerations more specifically flow gradients that are representative of the whole domain each subdomain contains 2304 cells for a total of 6912 cells considering the three subdomains spattern ann described in section 3 2 is applied on the full set of 6912 cells covering the three subdomains i e ψ r 6912 in fig 2b as c c ann described in section 3 1 requires to learn a separate relationship for each hr cell the number of cells was reduced to 125 within each subdomain for a total of 375 cells over the three subdomains to keep the computation time within reasonable limits i e ψ j j 1 375 in fig 2a for each of the three subdomains the 125 cells are selected as follows see fig 5 the subdomain is divided into 5 rectangular zones one for the central crossroads and four for each of the branches departing from the intersection each of these five zones comprises 5 5 cells spread regularly so as to allow for a maximum coverage of the rectangular zone the first two synthetic configurations considered are 1d negative and positive waves without friction that are 1d boundary value problems bvps these configurations identified as n wave nf and p wave nf respectively for short are one of the simplest possible bvps for layouts of this type the frictionless propagation along the x direction of a wave into still water is simulated fig 6 the bottom is flat the water is initially at rest in the domain with an initial depth h 0 the water level is set instantaneously to a constant value h 1 at the western boundary of the domain in n wave nf h 1 h 0 which yields a negative wave rarefaction wave in p wave nf h 1 h 0 and a positive wave shock wave appears the lr solution u lr is self similar in the x t domain guinot 2017 guinot et al 2017 2018 the next two synthetic configurations use the same geometry as n wave nf and p wave nf fig 6 but with a non zero bottom friction coefficient these configurations identified as n wave wf and p wave wf respectively are cases study closer to real world situations as a consequence of the non zero friction coefficient the upscaled solution is no longer self similar in the x t space both the hr and lr simulations and their spatial gradients span a different range of hydraulic configurations from that of n wave nf and p wave nf the last synthetic configuration identified as dam break is a 2d oblique urban dam break problem without friction the dam break problem is a riemann initial value problem ivp where the water is initially at rest and the water depth is piecewise constant equal to h l and h r respectively on the left and right hand sides of a broken divide line with average orientation se nw fig 7 a this results in an average flow field and wave propagation pattern oriented in the sw ne direction since the flow is diagonal to the main street directions fully meshing the domain involves as many block periods in both directions of space this makes the mesh size and the subsequent computational effort prohibitive the difficulty can be overcome guinot 2017 by meshing only a single block period in the transverse direction fig 7b the topology of the mesh is modified by connecting the northern side of the i th lateral street boundary segment n i in the figure with the southern side of the i 1 th lateral street boundary segment s i 1 in fig 7b while the upscaled solution of an urban dam break problem parallel to the main street axis is known to be self similar in x t guinot 2012 guinot et al 2017 guinot 2017 self similarity disappears when the propagation is oblique with respect to the street axes guinot 2017 the dam break configuration includes both negative and positive wave phenomena 4 2 field scale test case the field scale test case considered was reported in guinot et al 2017 for the evaluation of a porosity based shallow water model the frictionless propagation of a dike break flood wave into a neighbourhood of the sacramento urban area is simulated the test which is referred to as sacramento for short is informative in many aspects i the geometry is real non periodic ii the upscaled hydraulic pattern is genuinely two dimensional and iii the hr flow field exhibits a strong polarisation along two preferential flow directions guinot et al 2017 the dike breach is located on the left hand side of the domain in fig 8 the sacramento neighbourhood is discretised using a hr mesh made of 77 963 cells average cell area 6 5 m 2 for spattern ann subsection 3 2 ψ r 77 963 in fig 2b c c ann is restricted to an area containing 575 hr cells i e ψ j j 1 575 in fig 2a the lr mesh used for upscaling is much coarser with 1682 subdomains average subdomain area 285 m 2 which means that ψ r 1682 the input in the two statistical downscaling models see fig 2 these hr and lr meshes are used for the refined and porosity based shallow water simulations reported in guinot et al 2017 fig 9 shows close up views of the hr and lr meshes of the area where the 575 cells on which c c ann is applied are located 4 3 training validation and test sets in machine learning the focus is on the evaluation of model performance on previously unseen data to assess the so called generalisation capability bishop 2011 the training set is used to optimise the parameters of each model the validation set distinct from the training set serves to select the best hyper parameter values while the test set distinct from the training and validation sets serves to compare the models with the selected hyper parameter values in the flood hazard framework it is expected that statistical downscaling models given a flooding configuration such as positive or negative waves should be able to perform well at estimating the hr hazard indicator based on the lr hazard indicator for any flow scenarios therefore the generalisation capability of a downscaling model may be measured in terms of performance for a given configuration at estimating flow scenarios that were not seen before thus for each configuration the training validation and test sets are defined as pairs of hr and lr hazard indicators simulated for a number of flow scenarios in table 2 and 3 the flow scenarios labelled by a letter used to form the training validation and test sets for the synthetic urban configurations and the field scale test respectively are listed fig 10 illustrates the principle of the training validation test sets design the flow scenarios forming the training set are designed so as to span the space of bc and or ic values for the synthetic urban configurations three flow scenarios forming a right angled triangle in the bc and or ic space are needed since the space is two dimensional for the test field case only two flow scenarios are necessary as there is a single initial condition the validation set is designed so as to contain flow scenarios such that only one of the bc and or ic values is different from the values used in the training set but within their range for the synthetic urban configuration the validation set contains two flow scenarios taken as the midpoints of the two legs of the right angled triangle for the test field case the validation set consists of a single flow scenario defined by the ic value at the midpoint between the values forming the training set the flow scenarios considered for the test set correspond to bc and or ic values that extrapolate in different ways from the values seen for training and validation for the synthetic urban configurations five such flow scenarios are considered where as for the field scale test only two possible flow scenarios are included in the test set the sizes of the training validation and test sets compiled in table 4 depend on the length of the simulation period t and the sampling time step which are set as follows the sampling time step is 10 s in configurations n wave nf n wave wf p wave nf and p wave wf and 5 s for dam break and sacramento for each synthetic urban configuration the length of the simulation period may change with the flow scenario so as to ensure that there is no wave reflection phenomena that would introduce completely different hydraulic behaviors and patterns the negative wave configurations n wave nf and n wave wf have a duration of 400 s 41 time steps for all flow scenarios except for the j one which is set to 260 s 27 time steps p wave nf has a duration of 300 s 31 time steps in call cases except for the flow scenario labelled g that has a time span of 200 s 21 time steps p wave wf has a duration of 300 s 31 time steps in all cases dam break has a 100 s duration 21 time steps for all flow scenarios except the one labelled j that has a duration of 80 s 17 time steps the field scale simulation has a duration of 240 s 49 time steps except for the flow scenario labelled d which spans 480 s 97 time steps to allow the water to reach most areas 5 evaluation and comparison of statistical downscaling models 5 1 principal spatial patterns obtained from the training sets spattern ann described in section 3 2 relies on the assumption that the hr hazard indicator can be decomposed into a linear combination of spatial patterns see 10 these spatial patterns correspond to the eigenvectors contained in the matrix b of the pca decomposition termed principal components the number of spatial patterns that may best reconstruct the hr hazard indicator see 12 is selected on the validation set see the following section 5 2 a preliminary analysis of the spatial patterns uncovered by pca is presented here in order to qualitatively assess their interpretability in terms of hydraulic behaviour to this end pca is applied on the training set of each configuration see tables 2 and 3 and the first six spatial patterns are illustrated for each of the two hazard indicators considered i e the water depth and the norm of the unit discharge see 2 for the sake of conciseness among the synthetic urban configurations only the spatial patterns for dam break which includes both negative and positive waves are illustrated over the subdomain located in the middle of the computational domain with x limits 500 m 550 m as the other two subdomains have similar patterns the oblique orientation of the spatial patterns is visible for both the water depth and the norm of the unit discharge see figs 11 12 13 and 14 the spatial patterns of the water depth and the norm of the unit discharge for sacramento are shown although the spatial patterns of the norm of the unit discharge are sharper than those of the water depth the general shape with the propagation of the water from the breach in the dike located at the top left see fig 8 is similar for both hazard indicators 5 2 hyper parameter selected on the validation sets hyper parameters are selected for each downscaling model following a data driven procedure that relies on the performance evaluated on validation sets so as to estimate the generalisation capability several values are considered for each hyper parameter so as to span the range of possibilities starting from the lowest admissible value up to a value large enough to ensure that the selected value is not involuntarily bounded the performance of each combination of values considered for each hyper parameter is evaluated i e the corresponding anns are trained and the best combination i e the one that yields the lowest error on the validation set is retained for the five synthetic urban configurations and the field scale test the hyper parameters selected for the two downscaling models described in section 3 and for the two hazard indicators considered water depth and norm of the unit discharge are provided in table 5 for c c ann there are as many anns as hr cells 375 for the synthetic configurations and 575 for the field test case see section 4 therefore as many hyper parameters as hr cells are selected these are summarized by the median and the first and third quartiles in parentheses in table 5 the values selected for d the feature space dimension of the lr hazard indicator may be very different for the two downscaling models in particular for sacramento low values median of 3 for the two hazard indicators are selected for c c ann while high values 57 and 78 for water depth and unit discharge norm respectively are chosen for spattern ann in contrast for the unit discharge norm in the negative and positive wave configurations with or without friction the values selected for d are similar for n h the number of hidden units of the ann c c ann may select relatively high values for the positive wave configurations with or without friction for both hazard indicators in contrast in spattern ann n h is almost always zero indicating that a linear relationship is sufficient as the complexity may be embedded in the feature space representations of the hr hazard indicator provided by pca indeed the values of p selected the number of spatial patterns or equivalently the number of pcs in spattern ann is much lower when the hazard indicator is in a configuration yielding smoother dynamics this is the case for the water depth in the negative or positive wave configurations with or without friction in contrast the water depth in the dam break and sacramento configurations and the norm of the unit discharge especially in the p wave nf wf and sacramento configurations require higher numbers of spatial patterns the only configuration in which the norm of the unit discharge requires less spatial patterns than the water depth is the dam break configuration 5 3 relative errors computed on the test sets the performance of c c ann and spattern ann is compared on test sets made of several flow scenarios different than the ones that constitute the training and validation sets see tables 2 and 3 once again performance evaluation on previously unseen data aims to estimate the generalisation capability of the models the anns of the c c ann one per cell and the ann of spattern ann with the hyper parameters selected on the validation sets see table 5 are trained anew on a larger data set that merges together the training and validation sets in tables 6 and 7 the performance is provided in terms of the following relative error computed on the test sets for all configurations and for both hazard indicators 15 r j k ψ j k ψ j k ψ j k ψ i j k for j an element of the hr mesh k a simulation time step and i j the subdomain to which cell j belongs only time steps k such that ψ j k ψ i j k is among the 5 largest values are kept in the following analyses this choice is made on one hand to avoid infinite or very large values of r j k as ψ i j k ψ j k may be equal to zero or take on very small values and on the other hand to focus on time steps that are more challenging owing to a greater variability within the subdomain the relative error r j k from 15 can be interpreted as the estimation error of the downscaling model relative to the error made when using lr hazard indicator values as surrogates for hr hazard indicator values in particular whenever r j k 1 the downscaling model provides more accurate estimates that the lr hazard indicator table 6 concerns water depth downscaling and provides for each configuration and each test set in the second column the minimum and maximum values taken by the simulated hr water depth field in the third column e lr gives the 95 quantile of ψ j k ψ i j k the error of the simulated lr water depth field in the last two columns the median followed by the first and third quartiles in parentheses of r j k over time steps k such that ψ j k ψ i j k e lr for each of the two downscaling models in table 6 it can be seen that c c ann outperforms spattern ann i e the inter quartile interval of the relative error r j k is strictly smaller for the two negative wave configurations n wave nf and n wave wf over three test sets labelled f g and h out of five however for the two remaining test sets labelled i and j the relative error of c c ann is greater than one indicating that its absolute error is bigger than the absolute error of the lr hazard indicator in contrast the relative error of spattern ann remains way below one for all test sets for the two positive wave configurations p wave nf and p wave wf and for the dam break the relative errors of both downscaling models is comparable and below one for the first three test sets labelled f g and h for the last two test sets labelled i and j the relative error of c c ann reaches very high values whereas spattern ann s relative error remains low although somewhat above one in some cases for the field scale test sacramento both downscaling models performed rather well although spattern ann yielded slightly lower relative errors especially for the first test set labelled d table 7 contains the same information as in table 6 but concerns unit discharge norm downscaling for this hazard indicator the two downscaling models yielded in most cases a comparable performance in particular their relative error is low and bounded away from one one notable exception is for the test set i of dam break for which c c ann yielded a very poor performance whereas spattern ann performed relatively well although with relative error values moderately above one for sacramento spattern ann provides a good performance on both test sets with relative errors below one whereas c c ann is more challenged especially for the first test set labelled d 6 discussion we proposed a downscaling framework section 2 3 and fig 1 based on statistical models to estimate hr hazard indicators from lr hazard indicators derived from upscaled flow simulations such as those obtained from porosity models two such statistical models are proposed in section 3 both rely on non parametric data driven approaches that are very flexible since no strong parametric assumptions are made nevertheless the training set must be sufficiently representative of the underlying structure that is being estimated in order to enable the non parametric models to generalise well i e to perform well on previously unseen data in the operational phase this downscaling framework offers a considerable speedup over running a hr simulation for each new flow scenario for instance for one flow scenario of n wave nf 150 cpu s are needed to run the hr model in comparison running a lr porosity model and downscaling the results using spattern ann requires 7 5 10 3 2 22 10 2 2 97 10 2 cpu s the speedup factor is thus slightly above 5000 the first statistical downscaling model named c c ann is viewed as a reference approach as it relies on the same building blocks as conventional approaches to statistical downscaling that were applied in climate change studies see hewitson and crane 1996 furthermore c c ann is a univariate approach that does not attempt to capture the dependence structure of the hr hazard indicators indeed downscaling is performed cell by cell with an ann dedicated to each cell of the hr mesh see section 3 1 and fig 2a a potential alternative to this reference approach would be to consider a single ann that would be able to downscale all the cells of the mesh one at a time by including in its input specific information from each cell for instance in chadwick et al 2011 a climate variable simulated on a 25 km resolution grid by a regional climate model rcm constrained by a gcm on a lower resolution grid of 1 89 is downscaled with a single ann one rcm cell at a time among the ann input there is information from the large scale variable at the four gcm grid cells surrounding the rcm grid cell of interest by using information that changes with each rcm cell the ann is able to learn a relationship that can vary from cell to cell such an approach was considered initially in the shallow water models context but was put aside indeed as the number of cells within each subdomain is very high the question of which spatial information geographical coordinates not being sufficient would be useful to help discriminate each cell has no straightforward answer the second statistical downscaling model named spattern ann is capable of estimating very high dimensional fields such as the ones simulated by refined shallow water models see section 3 2 and fig 2b a key step of this downscaling model consists in decomposing the high dimensional hr hazard indicator into a linear combination of spatial patterns pca is often used to obtain spatial patterns from fields of climatic variables in order to infer weather types i e recurring patterns that can be found for instance in large scale atmospheric circulation yiou and nogaj 2004 the weights of the linear combination of spatial patterns lie on a low linear subspace of the high dimensional hr hazard indicator called feature space in the weather type approach mentioned above clustering can be performed on the features i e the linear weights obtained from pca to classify each time steps e g days into weather types in contrast in the proposed spattern ann model these features are taken as the multivariate dependent variable in a regression model a feed forward neural network with a direct linear connection as in fig 3b the aim of spattern ann is to estimate the low dimensional features of the hr hazard indicators from the low dimensional features of the lr hazard indicators several low and high resolution hydraulic simulations are used for the evaluation and the comparison of the statistical downscaling models see section 4 in this paper the perfect upscaled solution is used see 5 so as to evaluate the feasibility of such a downscaling framework when the upscaled flow fields are unbiased this allows the error arising from scale change to be assessed independently from the bias stemming from an inevitably inaccurate porosity model five synthetic urban flooding configurations are considered these idealistic cases are interesting in that i they may appear as elementary components of more complex real cases ii their geometry is entirely under control iii they allow the respective influences of the propagation and friction process to be assessed one field scale test is also included in the flooding configurations these pairs of hr and lr simulations serve i to set up the statistical downscaling models i e to optimise their parameters and to select adequate hyper parameter values with a training validation procedure and ii to evaluate their performance on flow scenarios forming the test set that were not used to set up the models the underlying rationale is to assess the generalisation capability of the statistical downscaling models i e their performance when estimating hr hazard indicator values for previously unseen flow scenarios a preliminary visual inspection of the spatial patterns obtained by pca showed that these are interpretable according to each configuration and for the two hazard indicators considered water depth and norm of the unit discharge see figs 11 12 13 and 14 for the dam break and sacramento configuration respectively the selection of hyper parameters on validation sets see table 5 showed that as was anticipated spattern ann needs more spatial patterns i e a larger dimension of the feature space of pca to reconstruct the spatial field of more turbulent hazard indicators with no hidden units selected in most cases for the ann spattern ann is in fact a large dimensional multivariate linear regression the complexity of spattern ann is rather controlled by the aforementioned number of spatial patterns the estimation of the model is carried out by combining three steps 1 pca of the lr hazard indicator 2 pca of the hr hazard indicator and 3 regression between the feature space of the low and high resolution fields despite being a linear model the estimation could not be achieved with a single direct estimation step as is the case with conventional linear regression owing to the large dimension of the dependent variable on the other hand c c ann requires decreasing hyper parameter values that are associated to a decreasing number of parameters in anns for negative waves positive waves dam break and sacramento especially for water depth as the hazard indicator the relative error see 15 used to evaluate the performance on the flow scenarios forming the test sets allows to directly assess the added value in terms of absolute error of the downscaling framework with respect to crude estimates provided by the lr hazard indicator the following conclusions are drawn from tables 6 and 7 for all five synthetic configurations the flow scenarios labelled i and j proved to be more difficult most likely owing to the stronger extrapolation in terms of bc and or ic values with respect to the ones seen during training and validation see fig 10 and table 2 comparing across synthetic configurations regardless of the downscaling model used negative wave configurations yield significantly smaller errors than positive wave configurations compare n wave and p wave in tables 6 and 7 friction exerts a much milder influence on the performance of downscaling models than the wave pattern does compare nf and wf in the tables this was to be expected because negative wave configurations yield much smoother flow fields than positive waves in most positive wave simulations the moving shock spreads within one to two lr cells inducing strong gradients and considerable variability in the hr flow such variability is very difficult to capture from the lr averaged variables in contrast negative waves often spread over several lr grid cells and the hr flow patterns are much less variable in dam break that includes both a negative and a positive wave the error is clearly dominated by that of the moving shock besides the waves propagate along the diagonal of the urban layout this induces a larger hr flow variability than with the n wave and p wave configurations where the waves propagate along the main axis of the urban layout the question might then arise why the sacramento configuration that involves a positive wave yields significantly smaller relative errors than the dam break configuration compare for instance the performance of spattern ann for sacramento flow scenarios d e with dam break flow scenarios i j that are all extrapolations it must be noticed however that the wave in the sacramento configuration propagates over a dry bottom therefore the positive wave is essentially a rarefaction wave on the lr scale not a shock shocks appear locally on the hr scale but they are due to the local wave reflections against the building walls they are not concentrated in the immediate neighbourhood of a moving bore as in the p wave and dam break configurations as a conclusion the smoothness of the flow mainly driven by the wave propagation process appears to be the predominant factor to downscaling accuracy 7 conclusion the analyses carried out in this work showed that the proposed downscaling framework may yield fast and accurate estimates of the hr hazard indicators either the water depth or the norm of the unit discharge for a large number of flow scenarios for the five synthetic urban configurations and the field scale test in particular the spattern ann statistical downscaling model provided consistently good performance further work is needed to understand how to bring improvements to spattern ann an interesting avenue of research would be to investigate the representativeness of the spatial pattern basis does the training set contain informative enough data to uncover the spatial pattern basis in other words does the training set include all the spatial patterns that are present in the validation and test sets another avenue would be to consider techniques other than pca to deduce the spatial patterns such as frames christensen 2008 besides stochasticity could be introduced in the downscaling methods which would be helpful to account for uncertainties in the estimation for c c ann it suffices to see the outputs of the ann as the parameters of a given probability distribution cannon 2012 carreau and vrac 2011 for spattern ann the stochastic version of pca could be implemented bishop 2011 other perspectives for this work are as follows the hydraulic simulations reported involve a flat topography the performance of the downscaling models in the case of a variable topography should be explored in the case of a spatially variable bottom elevation the free surface elevation is often smoother than the water depth whether the surface elevation is easier to downscale than the water depth should be assessed in addition imperfect upscaling should be tested as nonlocal effects are to be expected however the performance of the statistical downscaling models will not necessarily decrease as statistical strategies termed bias correction in the climate change context ayar et al 2016 might allow to circumvent the biases present in porosity model simulations as mentioned in section 2 1 the water depth and the norm of the unit discharge are not the only possible variables for flood hazard assessment the possibility of downscaling additional variables such as the specific force per unit width and the hydraulic head should also be investigated this might induce an increased level of complexity compared to the downscaling of the water depth and the unit discharge because the specific force and the hydraulic head are not conserved variables in the hr governing equations last in some cases only the maximum of a given flow variable within a given area might be needed for hazard assessment in such cases downscaling techniques developed within the theory of extreme values could be useful bechler et al 2015 credit authorship contribution statement j carreau conceptualization methodology software validation formal analysis writing original draft visualization v guinot conceptualization methodology software validation writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was partially supported by the french national program lefe insu fraise european program prima altos and bi national program hubert curien france tunisie amande supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j advwatres 2020 103821 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
368,the coefficient of the brutsaert nieber equation cannot provide information on streamflow dynamics independently because its value depends on the exponent one way to address this challenge is to compute the coefficient after fixing the exponent which may involve fitting errors a recent study has therefore adopted a method to decorrelate the coefficient from the exponent here i argue that the decorrelation method does not dissociate the brutsaert nieber parameters or make them independent of each other analysis of real and synthetic recession curves suggests that for a wide variety of scenarios the fixed exponent method is preferable to the decorrelation method keywords power law relationships brutsaert nieber equation individual recession curve analysis parameter decorrelation the fixed exponent method 1 introduction a large number of natural phenomena can be expressed by means of a power law equation e g rodríguez iturbe and rinaldo 2001 pinto et al 2012 in hydrological science recession flows in rivers are commonly described as brutsaert and nieber 1977 1 d q d t k q α where q is the discharge at the river cross section at time t accurate estimation of the parameters α and k of the brutsaert nieber bn equation eq 1 is essential for many hydrological applications as they provide crucial information on basin storage which otherwise cannot be observed e g biswal and marani 2010 brutsaert and nieber 1977 doulatyari et al 2015 dralle et al 2015 li and nieber 2017 reddyvaraprasad et al 2020 santos et al 2018 shaw and riha 2012 wang and cai 2009 because of observational errors and subjectivities involved with recession analysis a large number of methods exist to estimate α and k e g stoelzle et al 2012 chen and krajewski 2016 roques et al 2017 dralle et al 2017 jachens et al 2020 traditional methods assume that the relationship between d q d t and q is static or one to one for a basin which also means that d q d t q data points can be plotted altogether in a log log plane to estimate α and k however many studies have recently supported the notion that d q d t q relationship changes significantly across recession events implying we should analyze recession curves individually biswal and marani 2010 shaw and riha 2012 mutzner et al 2013 biswal and marani 2014 dralle et al 2017 individual recession curve analysis however has its own limitations particularly with respect to the estimation of k while α is dimensionless the unit scale of k is dependent on α thus the values of k obtained for a basin are quite meaningless as they cannot be compared with each other one solution in this regard is to fix the value of α at α median α m for each recession curve of the basin and estimate the bn coefficient k m biswal and marani 2014 biswal and nagesh kumar 2014 bart and hope 2014 which will be henceforth called as the fixed exponent method although the fixed exponent method has been successfully exploited for various purposes such as prediction of recession discharge reddyvaraprasad et al 2020 it is expected to introduce curve fitting error when α is different from α m dralle et al 2015 therefore followed a method to decorrelate α and k which according to them generates decorrelated bn coefficients k d c s that can be compared with each other for obtaining meaningful hydrological information the decorrelation method was originally proposed by bergner and zouhar 2000 who studied fatigue behavior of materials using observed discharge data from 16 us catchments dralle et al 2015 showed that the decorrelated bn coefficients could explain the seasonal variability of streamflow nevertheless they did not quantitatively analyze how much benefit the decorrelation method provides particularly with respect to the fixed exponent method simply put both the decorrelation and the fixed exponent methods intend to obtain bn coefficients that independently explain streamflow dynamics however so far no framework is available to assess the effectiveness of a bn coefficient estimation method in this study i argue that there is no rational solution to the bn parameter estimation problem that is to say it is impossible to obtain bn coefficients that independently provide full information on streamflow dynamics i first analytically show that the decorrelation method does not actually dissociate k from α i e it does not make k completely independent of α thereafter i propose a framework to compare the decorrelation method with the fixed exponent method using observed as well as synthetic recession flow data 2 methods 2 1 the brutsaert nieber parameter decorrelation method to estimate α and k following the least squares linear regression method we need to take the logarithm of both sides of the bn equation eq 1 ln d q d t ln k α ln q if we change the unit of discharge such that q q z which also means q q z and d q d t d q d t z the log transformed bn equation will be ln d q d t ln k α ln q where ln k ln k α 1 ln z or k k z α 1 because of the presence of both α and z in the last equation we can expect the correlation between k and α r 2 to change if z changes therefore for a set of α k values it is possible to find a z z d c such that r 2 between k and α vanishes i e ln k ln k α α ln k ln k 2 α α 2 0 for this condition to be satisfied both ln k ln k 2 and α α 2 need to be nonzero but ln k ln k α α zero the overline sign here stands for mean value now replacing ln k with ln k α 1 ln z d c one can obtain the condition for decorrelation bergner and zouhar 2000 2 z d c exp ln k ln k α α α α 2 eq 2 suggests that decorrelation of k from α is possible provided that the underlying assumptions are satisfied the real question here however is what bn parameter decorrelation actually means 2 2 decorrelation is not dissociation the bn parameters of a re scaled recession curve d q d t 1 q 1 d q d t 2 q 2 d q d t n q n can be obtained by minimizing the sum of the squared errors of the logarithmic quantities e ln d q d t α ln q ln k 2 for e to be minimum its partial derivative with respect to ln k has to be zero which yields the equation 3 ln k ln d q d t n α ln q n according to eq 3 k will be dissociated from α and vice versa only when α n ln q 0 or ln q ln q ln z ln q 1 q 2 q n ln z n 0 which gives the condition 4 z q 1 q 2 q n 1 n q where symbolizes geometric mean when z q eq 4 eq 3 turns into ln k 1 n ln d q d t 1 n ln d q d t 1 d q d t 2 d q d t n d q d t which implies no matter what value of α is chosen k will be always equal to d q d t note that if we minimize e by setting its partial derivative with respect to α at zero we will also find that α and k are dissociated only when z q eq 4 if all the recession curves of a basin don t have the same q no single value of z can dissociate the bn coefficient from the exponent for every recession curve on the other hand instead of focusing on parameter dissociation the fixed exponent method allows only k m to vary biswal and marani 2014 2 3 the purpose of brutsaert nieber coefficient estimation to understand how a recession coefficient provides information on discharge let s integrate eq 1 q 0 q q α d q k 0 t d t which gives us 5 q q 0 1 α 1 k q 0 α 1 t 1 1 α where q 0 is the discharge at the beginning of the recession event eq 5 shows k and α combined cannot provide full information on streamflow variability due to the presence of q 0 however the effect of q 0 on q decreases with t and when α 1 and t is sufficiently large such that α 1 k t q 0 α 1 1 q α 1 k t 1 1 α if t is held constant say at t for all the recession events of a basin biswal and marani 2014 6 q t α 1 k 1 1 α where q t is q at time t the above equation provides a quantitative description of how k and α compete with each other to control q t considering eq 6 as the theoretical basis this study proposes that any bn coefficient estimation method can be evaluated in terms of the correlation r 2 between q t and the estimated coefficient for a reasonably large value of t stated differently if the objective of a bn coefficient estimation method is to obtain coefficients that independently explain streamflow variability it should aim to maximize the r 2 between q t and the estimated bn coefficient it should be noted that the dependency of q t on α will also not vanish even if we apply the decorrelation method as stated in section 2 2 k d c cannot be free from α for all recession events if q varies across events the only scenario in which q t variation across recession events will be fully explained by k alone is when α does not vary eq 6 although the fixed exponent method does not allow α to vary it too does not eliminate the dependency of q t on α on the contrary it incorporates the value of α to build k m for z 1 eq 3 will be ln k 1 n ln d q d t α n ln q when α is fixed at α m the equation can be written as ln k m 1 n ln d q d t α m n ln q comparing these two equations we can obtain the relationship between k m and k as ln k m ln k α m α ln q or 7 k m k q α m α which shows the dependency of k m on α when α α m if for example k is constant for a set of recession curves k m will vary because of variation of α 3 analysis of observed and synthetic recession flow curves this study compares the decorrelation method with the fixed exponent method using available daily discharge times series data in mm day from 31 randomly selected usgs basins situated across a wide variety of geographical regions see figure s1 and table s1 the drainage areas of the selected basins cover a fairly large range 21 70 to 2147 11 sq km a recession curve by definition is a monotonically decreasing discharge time series q and d q d t for ith day are computed as q i q i q i 1 2 and d q d t i q i q i 1 δ t brutsaert and nieber 1977 since daily discharge data is used in this study δ t 1 day computation of recession parameters is performed here following the least squares linear regression method both k and α are computed together for each recession curve considering the ln q ln d q d t data points recognizing mm day as the original unit of discharge k is denoted as k o mm 1 α day 2 α k d c for each event is computed considering the ln q z d c ln d q z d c d t data points note that the value of α is independent of the unit of discharge once α values are determined for all the recession events of a basin median α α m is chosen for computing k m mm 1 α m day 2 α m for each recession curve to further strengthen the analysis i also considered synthetic recession curves with controlled characteristics table s2 and performed the computations described in the paragraph above the synthetic recession discharge in mm day unit curves were generated by allowing the recession curve parameters q 0 k o and α eq 5 to assume random values in their specified ranges see table s2 the synthetic basins may not share similarities with real basins in all aspects for example possible correlations between the recession parameters q 0 k o and α biswal and nagesh kumar 2013 basso et al 2015 dralle et al 2015 santos et al 2018 were not taken into consideration while generating the synthetic recession curves however the purpose of the synthetic basins is to provide supplementary data for the analysis here for each synthetic basin a thousand recession curves were generated in total 10 synthetic basins with distinctly different combinations of parameter ranges were considered table s2 r 2 of the power law relationship between k o and q t is considered as the baseline performance r o 2 for the basin the performance of the decorrelation method i e r 2 of the of the power law relationship between k d c and q t is denoted as r d c 2 and the corresponding value for the fixed exponent method r m 2 note that for any value of t to be considered in the analysis for a basin the minimum length of each recession curve needs to be t days in other words the number of eligible recession curves is expected to decrease with t for any value of t basins with less than 50 eligible recession curves are not considered for further analysis results considering two values of t 5 and 10 are reported here 4 results and discussion with discharge data from a sample basin usgs id 07160500 fig 1 a and b assert that the decorrelation method can indeed decorrelate the bn parameters section 2 1 however as section 2 2 clarifies decorrelation is not dissociation i e although eq 2 decorrelates the bn parameters of a basin it does not dissociates them or make them independent of each other it is possible to dissociate the bn parameters only when all the recession curves of the basin have the same q which seems unlikely for a river basin see fig 1c fig 2 a c shows the results for the sample basin for t 5 days the scatter plot between k o and q 5 is shown for the basin in fig 2a a large amount of scatter r o 2 0 16 in the plot suggests k o obtained from discharge values in mm day cannot provide reliable information on streamflow dynamics the decorrelation method provides more useful hydrologic information which is highlighted by the presence of a relatively low amount of scatter in the q 5 vs k d c plot for the basin r d c 2 0 60 however r m 2 for the basin 0 94 is significantly greater than r d c 2 fig 2d f show the results from the same analysis for the sample basin but t 10 days r o 2 improved to 0 28 which is still low while r d c 2 did not change much r m 2 improved to 0 97 likely because the assumption α 1 k q 0 α 1 t 1 is more appropriate for t 10 than for t 5 see section 2 3 results from the analysis involving all the 31 study basins for t 5 are summarized in fig 3 a and table s1 the 25th 50th and 75th percentiles of r o 2 r d c 2 and r m 2 respectively are 0 09 0 20 0 41 0 25 0 33 0 43 and 0 77 0 90 0 92 fig 3a also shows that the 25th 50th and 75th percentiles of r d c 2 r o 2 are 0 01 0 10 and 0 19 implying that the decorrelation method provides only a marginally better explanation of streamflow dynamics on the other hand the 25th 50th and 75th percentiles of r m 2 r d c 2 are 0 40 0 51 and 0 56 implying that the fixed exponent method provides substantially more information on streamflow dynamics than the decorrelation method fig 3b and table s1 display the corresponding results for t 10 days note that 23 out of the 31 basins were considered in the analysis as the remaining 8 basins did not have a sufficient number of recession curves longer than 10 days the 25th 50th and 75th percentiles of r o 2 r d c 2 r m 2 r d c 2 r o 2 and r m 2 r d c 2 respectively are 0 10 0 21 0 47 0 23 0 37 0 54 0 88 0 92 0 94 0 04 0 07 0 175 and 0 37 0 46 0 66 suggesting that r m 2 improves with t in general and does not seem to let r d c 2 catch up with it although only two values of t 5 and 10 have been considered here it appears unlikely that there exists a t for which a majority of basins will show r d c 2 r m 2 in passing contrary to the prediction in section 2 3 r m 2 marginally declined for 6 of the 23 basins from t 5 to 10 table s1 possibly because lower streamflow values are more likely to be associated with observational errors interestingly r o 2 is greater than r d c 2 for 7 out of the 31 study basins see table s1 although a purely mathematical explanation for this is beyond the scope of this study this analysis makes it clear that z d c is not the optimum value of z that can be chosen to obtain information on streamflow variability it is also remarkable that r d c 2 r m 2 for none of the study basins for either t 5 or t 10 see table s1 the decorrelation method makes sense only when the variation of q is insignificant section 2 2 which seems to be improbable for recession flow curves overall results here suggest that the fixed exponent method is more reliable than the decorrelation method if the objective is to obtain bn coefficients independently explaining streamflow dynamics this remark is further supported by the results from the analysis of the synthetic recession curves for none of the 10 synthetic basins r d c 2 r m 2 table s2 nevertheless r m 2 is quite low 0 50 for some of the basins see fig 3 and tables s1 and s2 meaning k m cannot always provide adequate information on streamflow dynamics this happens perhaps when the assumption α 1 k q 0 α 1 t 1 eq 6 does not make a lot of sense another possibility is that incorporation of α in k m see eq 7 is not very efficient for those cases if the criterion α 1 k q 0 α 1 t 1 is fully satisfied q t will be a function of k alone for a basin according to eq 6 provided that the exponent α does not vary across recession events on the contrary the decorrelation method is not applicable when all the recession curves of a basin exhibit the same α thus to create an appropriate scenario to compare the two methods a synthetic basin was chosen with α varying within a narrow range between 1 94 and 1 97 in comparison k o was allowed to vary between 0 56 and 62 73 see fig 4 a and b and table s2 as predicted by eq 6 q 5 has a near perfect relationship with k o fig 4c however the q 5 vs k d c plot fig 4d has a lot more scatter than the q 5 vs k o plot fig 4c which further strengthens the notion that the decorrelation method sometimes adds noise rather than information to the analysis not surprisingly the q 5 vs k m plot displays almost no scatter fig 4e highlighting that the fixed α method is appropriate when α does not vary much the decorrelation method is also not applicable when k does not vary section 2 1 the question that may invoke curiosity is what if k varies within a narrow range strictly speaking the fixed exponent method is not expected to perform well if α varies much not if k o varies little to address these concerns a synthetic basin was chosen with k o ranging between 13 12 and 18 19 and α between 1 59 and 7 28 fig 4a and b and table s2 even though r o 2 is 0 98 for the synthetic basin fig 4f the plot does not seem to properly describe the actual relationship between k o and q 5 since according to eq 6 q 5 should exhibit an inverse relationship with k o contrary to the direct relationship shown in the plot q 5 vs k d c plot on the other hand shows a combination of direct and inverse relationships between the two variables fig 4g suggesting that the decorrelation method may change the fundamental nature of the relationship between the bn coefficient and streamflow fig 4h shows the plot between k m and q 5 which correctly describes the inverse relationship between the two variables perhaps because of the way the fixed exponent method uses α to obtain k m see eq 7 it should be acknowledged that the analyses conducted here might have been associated with several uncertainties such as those related to the definition of recession curves and numerical errors associated with the estimation of d q d t stoelzle et al 2012 roques et al 2017 dralle et al 2017 jachens et al 2020 however there seems to be no reason to believe that the conclusions will radically alter if another study is conducted as the synthetic recession curves also led to the same conclusions one may also doubt if the comparative analysis performed in this study itself is flawless to my knowledge no previous study has proposed a framework to evaluate the decorrelation method objectively dralle et al 2015 who for the first time used the decorrelation method to estimate bn coefficients provided visual i e not quantitative evidence that k d c is better than k o at explaining seasonal streamflow dynamics this study on the other hand provides a framework to perform the same comparative analysis quantitatively although this framework cannot fully explain when and why a bn coefficient estimation method works or not it has evidently been helpful in gaining crucial insights about the decorrelation and the fixed exponent methods 5 concluding remarks the precise value of streamflow at any given time can be obtained if k α and q 0 are known however we need to focus on the scale the unit of q if the objective is to extract hydrologic information only from k because rescaling of q by a factor z 1 will result in a new coefficient k k z α 1 depending on the value of z the correlation between k and α may strengthen or weaken dralle et al 2015 argued that the rescaling exercise would be truly effective if we choose a z such that k has no correlation with α on the contrary this study analytically demonstrated that decorrelation is not dissociation i e zero correlation between k and α does not necessarily mean that they are independent of each other to be precise k will be dissociated from α for a recession curve only if z q since it is very unlikely that all the recession events of a basin will have the same q dissociation of k from α as the decorrelation method intends to do by a single value of z is not feasible on the other hand instead of attempting to decorrelate k from α the fixed exponent method effectively utilizes α to obtain a coefficient k m that provides better hydrological information to evaluate bn coefficient estimation methods this study proposed a quantitative framework that appreciates the fact that the role of q 0 diminishes with time t when α 1 the effectiveness of a bn coefficient estimation method can thus be characterized by the r 2 between the bn coefficient and discharge at a large time analysis of observed as well as synthetic recession curves could not come across a single case for which the decorrelation method is more useful than the fixed exponent method supporting the notion that decorrelation is not dissociation it is worth mentioning here that the decorrleation method assumes the relationship between the bn parameters to be linear which may not be true e g helsel and hirsch 2002 moreover r d c 2 is not always greater than r o 2 which raises additional doubts regarding the ability of the decorrelation method to explain streamflow dynamics using synthetic recession curves with special properties this study also threw some light on the workings of the two methods overall this study asserts that it is impossible to obtain bn coefficients for a basin independent of the exponents nevertheless if the objective is to obtain coefficients that independently carry meaningful hydrological information the fixed exponent method can do a reasonable job credit authorship contribution statement basudev biswal conceptualization methodology data curation writing original draft visualization investigation validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements streamflow data used for this study were obtained from usgs https waterwatch usgs gov i would also like to thank the three anonymous reviewers for their insightful comments and suggestions supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103822 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
368,the coefficient of the brutsaert nieber equation cannot provide information on streamflow dynamics independently because its value depends on the exponent one way to address this challenge is to compute the coefficient after fixing the exponent which may involve fitting errors a recent study has therefore adopted a method to decorrelate the coefficient from the exponent here i argue that the decorrelation method does not dissociate the brutsaert nieber parameters or make them independent of each other analysis of real and synthetic recession curves suggests that for a wide variety of scenarios the fixed exponent method is preferable to the decorrelation method keywords power law relationships brutsaert nieber equation individual recession curve analysis parameter decorrelation the fixed exponent method 1 introduction a large number of natural phenomena can be expressed by means of a power law equation e g rodríguez iturbe and rinaldo 2001 pinto et al 2012 in hydrological science recession flows in rivers are commonly described as brutsaert and nieber 1977 1 d q d t k q α where q is the discharge at the river cross section at time t accurate estimation of the parameters α and k of the brutsaert nieber bn equation eq 1 is essential for many hydrological applications as they provide crucial information on basin storage which otherwise cannot be observed e g biswal and marani 2010 brutsaert and nieber 1977 doulatyari et al 2015 dralle et al 2015 li and nieber 2017 reddyvaraprasad et al 2020 santos et al 2018 shaw and riha 2012 wang and cai 2009 because of observational errors and subjectivities involved with recession analysis a large number of methods exist to estimate α and k e g stoelzle et al 2012 chen and krajewski 2016 roques et al 2017 dralle et al 2017 jachens et al 2020 traditional methods assume that the relationship between d q d t and q is static or one to one for a basin which also means that d q d t q data points can be plotted altogether in a log log plane to estimate α and k however many studies have recently supported the notion that d q d t q relationship changes significantly across recession events implying we should analyze recession curves individually biswal and marani 2010 shaw and riha 2012 mutzner et al 2013 biswal and marani 2014 dralle et al 2017 individual recession curve analysis however has its own limitations particularly with respect to the estimation of k while α is dimensionless the unit scale of k is dependent on α thus the values of k obtained for a basin are quite meaningless as they cannot be compared with each other one solution in this regard is to fix the value of α at α median α m for each recession curve of the basin and estimate the bn coefficient k m biswal and marani 2014 biswal and nagesh kumar 2014 bart and hope 2014 which will be henceforth called as the fixed exponent method although the fixed exponent method has been successfully exploited for various purposes such as prediction of recession discharge reddyvaraprasad et al 2020 it is expected to introduce curve fitting error when α is different from α m dralle et al 2015 therefore followed a method to decorrelate α and k which according to them generates decorrelated bn coefficients k d c s that can be compared with each other for obtaining meaningful hydrological information the decorrelation method was originally proposed by bergner and zouhar 2000 who studied fatigue behavior of materials using observed discharge data from 16 us catchments dralle et al 2015 showed that the decorrelated bn coefficients could explain the seasonal variability of streamflow nevertheless they did not quantitatively analyze how much benefit the decorrelation method provides particularly with respect to the fixed exponent method simply put both the decorrelation and the fixed exponent methods intend to obtain bn coefficients that independently explain streamflow dynamics however so far no framework is available to assess the effectiveness of a bn coefficient estimation method in this study i argue that there is no rational solution to the bn parameter estimation problem that is to say it is impossible to obtain bn coefficients that independently provide full information on streamflow dynamics i first analytically show that the decorrelation method does not actually dissociate k from α i e it does not make k completely independent of α thereafter i propose a framework to compare the decorrelation method with the fixed exponent method using observed as well as synthetic recession flow data 2 methods 2 1 the brutsaert nieber parameter decorrelation method to estimate α and k following the least squares linear regression method we need to take the logarithm of both sides of the bn equation eq 1 ln d q d t ln k α ln q if we change the unit of discharge such that q q z which also means q q z and d q d t d q d t z the log transformed bn equation will be ln d q d t ln k α ln q where ln k ln k α 1 ln z or k k z α 1 because of the presence of both α and z in the last equation we can expect the correlation between k and α r 2 to change if z changes therefore for a set of α k values it is possible to find a z z d c such that r 2 between k and α vanishes i e ln k ln k α α ln k ln k 2 α α 2 0 for this condition to be satisfied both ln k ln k 2 and α α 2 need to be nonzero but ln k ln k α α zero the overline sign here stands for mean value now replacing ln k with ln k α 1 ln z d c one can obtain the condition for decorrelation bergner and zouhar 2000 2 z d c exp ln k ln k α α α α 2 eq 2 suggests that decorrelation of k from α is possible provided that the underlying assumptions are satisfied the real question here however is what bn parameter decorrelation actually means 2 2 decorrelation is not dissociation the bn parameters of a re scaled recession curve d q d t 1 q 1 d q d t 2 q 2 d q d t n q n can be obtained by minimizing the sum of the squared errors of the logarithmic quantities e ln d q d t α ln q ln k 2 for e to be minimum its partial derivative with respect to ln k has to be zero which yields the equation 3 ln k ln d q d t n α ln q n according to eq 3 k will be dissociated from α and vice versa only when α n ln q 0 or ln q ln q ln z ln q 1 q 2 q n ln z n 0 which gives the condition 4 z q 1 q 2 q n 1 n q where symbolizes geometric mean when z q eq 4 eq 3 turns into ln k 1 n ln d q d t 1 n ln d q d t 1 d q d t 2 d q d t n d q d t which implies no matter what value of α is chosen k will be always equal to d q d t note that if we minimize e by setting its partial derivative with respect to α at zero we will also find that α and k are dissociated only when z q eq 4 if all the recession curves of a basin don t have the same q no single value of z can dissociate the bn coefficient from the exponent for every recession curve on the other hand instead of focusing on parameter dissociation the fixed exponent method allows only k m to vary biswal and marani 2014 2 3 the purpose of brutsaert nieber coefficient estimation to understand how a recession coefficient provides information on discharge let s integrate eq 1 q 0 q q α d q k 0 t d t which gives us 5 q q 0 1 α 1 k q 0 α 1 t 1 1 α where q 0 is the discharge at the beginning of the recession event eq 5 shows k and α combined cannot provide full information on streamflow variability due to the presence of q 0 however the effect of q 0 on q decreases with t and when α 1 and t is sufficiently large such that α 1 k t q 0 α 1 1 q α 1 k t 1 1 α if t is held constant say at t for all the recession events of a basin biswal and marani 2014 6 q t α 1 k 1 1 α where q t is q at time t the above equation provides a quantitative description of how k and α compete with each other to control q t considering eq 6 as the theoretical basis this study proposes that any bn coefficient estimation method can be evaluated in terms of the correlation r 2 between q t and the estimated coefficient for a reasonably large value of t stated differently if the objective of a bn coefficient estimation method is to obtain coefficients that independently explain streamflow variability it should aim to maximize the r 2 between q t and the estimated bn coefficient it should be noted that the dependency of q t on α will also not vanish even if we apply the decorrelation method as stated in section 2 2 k d c cannot be free from α for all recession events if q varies across events the only scenario in which q t variation across recession events will be fully explained by k alone is when α does not vary eq 6 although the fixed exponent method does not allow α to vary it too does not eliminate the dependency of q t on α on the contrary it incorporates the value of α to build k m for z 1 eq 3 will be ln k 1 n ln d q d t α n ln q when α is fixed at α m the equation can be written as ln k m 1 n ln d q d t α m n ln q comparing these two equations we can obtain the relationship between k m and k as ln k m ln k α m α ln q or 7 k m k q α m α which shows the dependency of k m on α when α α m if for example k is constant for a set of recession curves k m will vary because of variation of α 3 analysis of observed and synthetic recession flow curves this study compares the decorrelation method with the fixed exponent method using available daily discharge times series data in mm day from 31 randomly selected usgs basins situated across a wide variety of geographical regions see figure s1 and table s1 the drainage areas of the selected basins cover a fairly large range 21 70 to 2147 11 sq km a recession curve by definition is a monotonically decreasing discharge time series q and d q d t for ith day are computed as q i q i q i 1 2 and d q d t i q i q i 1 δ t brutsaert and nieber 1977 since daily discharge data is used in this study δ t 1 day computation of recession parameters is performed here following the least squares linear regression method both k and α are computed together for each recession curve considering the ln q ln d q d t data points recognizing mm day as the original unit of discharge k is denoted as k o mm 1 α day 2 α k d c for each event is computed considering the ln q z d c ln d q z d c d t data points note that the value of α is independent of the unit of discharge once α values are determined for all the recession events of a basin median α α m is chosen for computing k m mm 1 α m day 2 α m for each recession curve to further strengthen the analysis i also considered synthetic recession curves with controlled characteristics table s2 and performed the computations described in the paragraph above the synthetic recession discharge in mm day unit curves were generated by allowing the recession curve parameters q 0 k o and α eq 5 to assume random values in their specified ranges see table s2 the synthetic basins may not share similarities with real basins in all aspects for example possible correlations between the recession parameters q 0 k o and α biswal and nagesh kumar 2013 basso et al 2015 dralle et al 2015 santos et al 2018 were not taken into consideration while generating the synthetic recession curves however the purpose of the synthetic basins is to provide supplementary data for the analysis here for each synthetic basin a thousand recession curves were generated in total 10 synthetic basins with distinctly different combinations of parameter ranges were considered table s2 r 2 of the power law relationship between k o and q t is considered as the baseline performance r o 2 for the basin the performance of the decorrelation method i e r 2 of the of the power law relationship between k d c and q t is denoted as r d c 2 and the corresponding value for the fixed exponent method r m 2 note that for any value of t to be considered in the analysis for a basin the minimum length of each recession curve needs to be t days in other words the number of eligible recession curves is expected to decrease with t for any value of t basins with less than 50 eligible recession curves are not considered for further analysis results considering two values of t 5 and 10 are reported here 4 results and discussion with discharge data from a sample basin usgs id 07160500 fig 1 a and b assert that the decorrelation method can indeed decorrelate the bn parameters section 2 1 however as section 2 2 clarifies decorrelation is not dissociation i e although eq 2 decorrelates the bn parameters of a basin it does not dissociates them or make them independent of each other it is possible to dissociate the bn parameters only when all the recession curves of the basin have the same q which seems unlikely for a river basin see fig 1c fig 2 a c shows the results for the sample basin for t 5 days the scatter plot between k o and q 5 is shown for the basin in fig 2a a large amount of scatter r o 2 0 16 in the plot suggests k o obtained from discharge values in mm day cannot provide reliable information on streamflow dynamics the decorrelation method provides more useful hydrologic information which is highlighted by the presence of a relatively low amount of scatter in the q 5 vs k d c plot for the basin r d c 2 0 60 however r m 2 for the basin 0 94 is significantly greater than r d c 2 fig 2d f show the results from the same analysis for the sample basin but t 10 days r o 2 improved to 0 28 which is still low while r d c 2 did not change much r m 2 improved to 0 97 likely because the assumption α 1 k q 0 α 1 t 1 is more appropriate for t 10 than for t 5 see section 2 3 results from the analysis involving all the 31 study basins for t 5 are summarized in fig 3 a and table s1 the 25th 50th and 75th percentiles of r o 2 r d c 2 and r m 2 respectively are 0 09 0 20 0 41 0 25 0 33 0 43 and 0 77 0 90 0 92 fig 3a also shows that the 25th 50th and 75th percentiles of r d c 2 r o 2 are 0 01 0 10 and 0 19 implying that the decorrelation method provides only a marginally better explanation of streamflow dynamics on the other hand the 25th 50th and 75th percentiles of r m 2 r d c 2 are 0 40 0 51 and 0 56 implying that the fixed exponent method provides substantially more information on streamflow dynamics than the decorrelation method fig 3b and table s1 display the corresponding results for t 10 days note that 23 out of the 31 basins were considered in the analysis as the remaining 8 basins did not have a sufficient number of recession curves longer than 10 days the 25th 50th and 75th percentiles of r o 2 r d c 2 r m 2 r d c 2 r o 2 and r m 2 r d c 2 respectively are 0 10 0 21 0 47 0 23 0 37 0 54 0 88 0 92 0 94 0 04 0 07 0 175 and 0 37 0 46 0 66 suggesting that r m 2 improves with t in general and does not seem to let r d c 2 catch up with it although only two values of t 5 and 10 have been considered here it appears unlikely that there exists a t for which a majority of basins will show r d c 2 r m 2 in passing contrary to the prediction in section 2 3 r m 2 marginally declined for 6 of the 23 basins from t 5 to 10 table s1 possibly because lower streamflow values are more likely to be associated with observational errors interestingly r o 2 is greater than r d c 2 for 7 out of the 31 study basins see table s1 although a purely mathematical explanation for this is beyond the scope of this study this analysis makes it clear that z d c is not the optimum value of z that can be chosen to obtain information on streamflow variability it is also remarkable that r d c 2 r m 2 for none of the study basins for either t 5 or t 10 see table s1 the decorrelation method makes sense only when the variation of q is insignificant section 2 2 which seems to be improbable for recession flow curves overall results here suggest that the fixed exponent method is more reliable than the decorrelation method if the objective is to obtain bn coefficients independently explaining streamflow dynamics this remark is further supported by the results from the analysis of the synthetic recession curves for none of the 10 synthetic basins r d c 2 r m 2 table s2 nevertheless r m 2 is quite low 0 50 for some of the basins see fig 3 and tables s1 and s2 meaning k m cannot always provide adequate information on streamflow dynamics this happens perhaps when the assumption α 1 k q 0 α 1 t 1 eq 6 does not make a lot of sense another possibility is that incorporation of α in k m see eq 7 is not very efficient for those cases if the criterion α 1 k q 0 α 1 t 1 is fully satisfied q t will be a function of k alone for a basin according to eq 6 provided that the exponent α does not vary across recession events on the contrary the decorrelation method is not applicable when all the recession curves of a basin exhibit the same α thus to create an appropriate scenario to compare the two methods a synthetic basin was chosen with α varying within a narrow range between 1 94 and 1 97 in comparison k o was allowed to vary between 0 56 and 62 73 see fig 4 a and b and table s2 as predicted by eq 6 q 5 has a near perfect relationship with k o fig 4c however the q 5 vs k d c plot fig 4d has a lot more scatter than the q 5 vs k o plot fig 4c which further strengthens the notion that the decorrelation method sometimes adds noise rather than information to the analysis not surprisingly the q 5 vs k m plot displays almost no scatter fig 4e highlighting that the fixed α method is appropriate when α does not vary much the decorrelation method is also not applicable when k does not vary section 2 1 the question that may invoke curiosity is what if k varies within a narrow range strictly speaking the fixed exponent method is not expected to perform well if α varies much not if k o varies little to address these concerns a synthetic basin was chosen with k o ranging between 13 12 and 18 19 and α between 1 59 and 7 28 fig 4a and b and table s2 even though r o 2 is 0 98 for the synthetic basin fig 4f the plot does not seem to properly describe the actual relationship between k o and q 5 since according to eq 6 q 5 should exhibit an inverse relationship with k o contrary to the direct relationship shown in the plot q 5 vs k d c plot on the other hand shows a combination of direct and inverse relationships between the two variables fig 4g suggesting that the decorrelation method may change the fundamental nature of the relationship between the bn coefficient and streamflow fig 4h shows the plot between k m and q 5 which correctly describes the inverse relationship between the two variables perhaps because of the way the fixed exponent method uses α to obtain k m see eq 7 it should be acknowledged that the analyses conducted here might have been associated with several uncertainties such as those related to the definition of recession curves and numerical errors associated with the estimation of d q d t stoelzle et al 2012 roques et al 2017 dralle et al 2017 jachens et al 2020 however there seems to be no reason to believe that the conclusions will radically alter if another study is conducted as the synthetic recession curves also led to the same conclusions one may also doubt if the comparative analysis performed in this study itself is flawless to my knowledge no previous study has proposed a framework to evaluate the decorrelation method objectively dralle et al 2015 who for the first time used the decorrelation method to estimate bn coefficients provided visual i e not quantitative evidence that k d c is better than k o at explaining seasonal streamflow dynamics this study on the other hand provides a framework to perform the same comparative analysis quantitatively although this framework cannot fully explain when and why a bn coefficient estimation method works or not it has evidently been helpful in gaining crucial insights about the decorrelation and the fixed exponent methods 5 concluding remarks the precise value of streamflow at any given time can be obtained if k α and q 0 are known however we need to focus on the scale the unit of q if the objective is to extract hydrologic information only from k because rescaling of q by a factor z 1 will result in a new coefficient k k z α 1 depending on the value of z the correlation between k and α may strengthen or weaken dralle et al 2015 argued that the rescaling exercise would be truly effective if we choose a z such that k has no correlation with α on the contrary this study analytically demonstrated that decorrelation is not dissociation i e zero correlation between k and α does not necessarily mean that they are independent of each other to be precise k will be dissociated from α for a recession curve only if z q since it is very unlikely that all the recession events of a basin will have the same q dissociation of k from α as the decorrelation method intends to do by a single value of z is not feasible on the other hand instead of attempting to decorrelate k from α the fixed exponent method effectively utilizes α to obtain a coefficient k m that provides better hydrological information to evaluate bn coefficient estimation methods this study proposed a quantitative framework that appreciates the fact that the role of q 0 diminishes with time t when α 1 the effectiveness of a bn coefficient estimation method can thus be characterized by the r 2 between the bn coefficient and discharge at a large time analysis of observed as well as synthetic recession curves could not come across a single case for which the decorrelation method is more useful than the fixed exponent method supporting the notion that decorrelation is not dissociation it is worth mentioning here that the decorrleation method assumes the relationship between the bn parameters to be linear which may not be true e g helsel and hirsch 2002 moreover r d c 2 is not always greater than r o 2 which raises additional doubts regarding the ability of the decorrelation method to explain streamflow dynamics using synthetic recession curves with special properties this study also threw some light on the workings of the two methods overall this study asserts that it is impossible to obtain bn coefficients for a basin independent of the exponents nevertheless if the objective is to obtain coefficients that independently carry meaningful hydrological information the fixed exponent method can do a reasonable job credit authorship contribution statement basudev biswal conceptualization methodology data curation writing original draft visualization investigation validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements streamflow data used for this study were obtained from usgs https waterwatch usgs gov i would also like to thank the three anonymous reviewers for their insightful comments and suggestions supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103822 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
369,pore network representations of permeable media provide the framework for explicit simulation of capillary driven immiscible displacement governed by invasion percolation theory the most demanding task of a pore network flow simulation is the identification of trapped defending phase clusters at every displacement step i e the phase connectivity problem instead of employing the conventional adjacency list we represent the connectivity of a phase cluster as a tree accompanied by a set of adjacent non tree edges in this graph representation a decrease in phase connectivity due to a pore displacement event corresponds to deletion of either a tree or a non tree edge deletion of a tree edge invokes a computationally intensive search for a possible reconnection of the resulting subtrees by an adjacent non tree edge the tree representation facilitates a highly efficient execution of the reconnection search invasion percolation simulations of secondary water floods under different wetting conditions in pore networks of different origin and size confirm the efficiency of the proposed phase connectivity algorithm moreover a systematic simulation study of runtime growth with increasing model size on regular lattice networks demonstrates a consistent orders of magnitude speed up compared to conventional simulations consequently the proposed algorithm proves to be a powerful tool for invasion percolation simulations on large multi scale networks and for extensive stochastic analysis of typical single scale pore networks keywords phase connectivity pore network modelling invasion percolation simulation runtime growth 1 introduction multi phase flow in natural porous media is a phenomenon occurring during hydrocarbon production subsurface co2 sequestration and groundwater remediation estimation of residual saturation and continuum scale functions of the phase saturation such as capillary pressure and relative permeabilities is essential to predict darcy scale multi phase flow behaviour these flow functions are traditionally obtained from expensive and time consuming laboratory experiments that require non trivial interpretation steps hou et al 2012 richardson et al 1952 rose and bruce 1949 complementary to these experiments the flow functions may be calculated from simulations of multi phase displacements in models of the pore space blunt et al 2013 maes and geiger 2018 raeini et al 2014 ryazanov et al 2009 valvatne and blunt 2004 state of the art computed tomography ct imaging tools allow acquisition of high resolution 3d pore space models referred to as digital rocks cnudde and boone 2013 knackstedt et al 2009 pore scale multi phase flow simulation requires solution of the navier stokes equations coupled with equations for evolution of the capillary interfaces separating the immiscible fluids blunt 2017 hirt and nichols 1981 meakin and tartakovsky 2009 patankar and spalding 1972 various approaches exist to solve these equations directly on digital rocks but these are computationally expensive and therefore limited to sample volumes that are usually insufficient for representative calculations of the multi phase flow functions maes and geiger 2018 pan et al 2004 raeini et al 2014 when capillary forces dominate pore scale multi phase flow is often simulated as invasion percolation ip in pore networks wilkinson and willemsen 1983 a pore network approximates the 3d pore space as a collection of geometrically idealised elements i e pore nodes connected by pore throats while preserving its topology jiang et al 2007 starting from the inlet of the network the invading phase displaces a defending phase from network elements in order of their entry conditions which are determined by capillary forces at the fluid fluid interfaces lago and araujo 2001 a pore displacement event is only permitted if the defending phase is not trapped valvatne and blunt 2004 wilkinson and willemsen 1983 based on these rules ip produces a series of quasi static pore network phase occupancies comprising clusters of both phases as illustrated in fig 1 a phase cluster is defined as a collection of connected pore elements containing the same phase note that pore elements may contain more than one phase most notably when wetting films are present in pore corners hence a single pore element may belong to multiple phase clusters ryazanov et al 2009 berg et al 2013 andrew et al 2015 in two phase flow a defending phase cluster is considered trapped if it does not include the network outlet singh et al 2017 additionally for each quasi static occupancy relative permeabilities may be calculated based on steady state flow through phase clusters that are connected to both inlet and outlet assuming poiseuille flow in every pore element øren et al 1998 following each pore displacement event the cluster configuration and the corresponding phase connectivity may change the phase connectivity is either incremental when a phase invades a pore element or decremental when a phase vacates a pore element holm et al 2001 growth of the invading phase cluster during a capillary driven flow process is an example of incremental connectivity which may result in merger of invading phase clusters the corresponding connectivity of the defending phase is decremental which may result in break up and trapping of defending phase clusters berg et al 2013 joekar niasar et al 2013 fully dynamic connectivity arises when a phase both invades and vacates different pore elements simultaneously thus displaying both incremental and decremental connectivity during a given flow process this occurs when isolated phase clusters are mobilised usually when capillary forces are less significant compared to viscous forces or buoyancy joekar niasar et al 2010 mogensen and stenby 1998 additionally recent fast micro ct imaging has revealed that during slow steady state two phase flow phase clusters periodically break up and merge reynolds et al 2017 note that an invading phase may only partly displace the defending phase from a pore element for example when non wetting phase invasion leaves behind films of the defending wetting phase in the pore corners valvatne and blunt 2004 van dijke and sorbie 2006 partial displacement does not change the defending phase connectivity this paper focuses on decremental phase connectivity as its evaluation the query whether defending phase clusters remain connected or break up tends to be the most computationally expensive part of the phase connectivity problem and indeed of the entire ip simulation as outlined below most existing ip implementations gostick et al 2016 ryazanov et al 2009 valvatne and blunt 2004 knackstedt et al 2000 appear to have adopted either the hoshen kopelman cluster labelling hoshen and kopelman 1976 al futaisi and patzek 2003 or a general graph traversal algorithm such as depth first or breadth first search cormen et al 2001 to evaluate phase connectivity following each pore displacement event these algorithms employ the adjacency list to define phase clusters where each node stores a list of the throats that connect it to adjacent nodes this conventional graph representation is very efficient for the connectivity modification such as merging two clusters or establishing new clusters following break up however it is very inefficient for the connectivity query and leads to a high computational cost per ip simulation indeed as we will demonstrate in section 3 in conventional approaches the phase connectivity algorithm is computationally the most expensive operation in an ip simulation most networks derived from single micro ct scans are reasonably small therefore any inefficiency of the connectivity algorithm does not pose a major obstacle however to be representative of the corresponding macro scale flow properties pore networks often need to include features from micro ct scans at several different resolutions for example for carbonate rocks with so called micro porosity vik et al 2013 in this case pore scale information from different resolutions must be numerically integrated resulting in large multi scale networks bultreys et al 2015 jiang et al 2013 prodanovic et al 2015 the computational cost of ip simulations on these networks with multi million pore elements become prohibitive in this paper we introduce a novel approach to the phase connectivity problem based on an alternative graph representation which significantly reduces the overall computational cost of the ip simulation in section 2 we explain the concept of the new algorithm in section 3 we demonstrate the speed up achieved with the algorithm by simulating water flood under different wetting conditions in pore networks of different size and origin in the simulation results we focus on the trapped and residual saturations as these are most directly linked to the phase connectivity 2 phase connectivity algorithm 2 1 phase connectivity graph for the purpose of the new graph representation of phase connectivity it is sufficient to consider connectivity between pore nodes connectivity between any two pore nodes is determined by the existence of a continuous path of nodes and throats diestel 2010 phase connectivity is defined by paths on which all elements contain the considered phase the implication is that a pore throat only contributes to the phase connectivity if it is doubly connected i e it is adjacent to two nodes containing the same phase on the contrary singly connected throats do not contribute to phase connectivity neither do isolated throats fig 2 displays the connectivity of the defending phase shown in fig 1 illustrating the concepts of doubly and singly connected throats it is clear that singly connected and isolated throats arise when the defending phase vacates an adjacent node as long as a singly connected pore throat is adjacent to a pore node connected to an outlet node the defending phase may be displaced from this throat but this does not affect the phase connectivity the collection of all pore nodes and the interconnecting doubly connected pore throats containing a given phase establish the corresponding phase connectivity graph moreover each component of the phase connectivity graph corresponds to a phase cluster containing at least one node where the latter additionally includes adjacent singly connected throats updating the phase connectivity graph is sufficient for the evaluation of the phase connectivity between any pore nodes at any stage of ip simulation additionally a defending phase component is considered trapped if it does not contain nodes associated with the outlet since pore elements of a trapped component can no longer be invaded any trapped component can be excluded from the phase connectivity graph 2 2 phase connectivity representation following henzinger and king 1999 holm et al 2001 we represent the phase connectivity graph as a forest of trees and sets of non tree edges accompanying each tree as illustrated in fig 3 the tree for each component of the phase connectivity graph comprises all its pore nodes identified as the tree vertices and a subset of its pore throats identified as the tree edges this non unique subset of pore throats connects all pore nodes belonging to the same component to form the tree i e a graph without loops all other doubly connected pore throats belonging to the component are identified as non tree edges which if added to the tree would create loops trivially a component comprising nv pore nodes and ne doubly connected pore throats yields a tree with nv vertices n t e n v 1 tree edges and n n t e n e n t e non tree edges the corresponding forest of trees may be obtained through depth first search in each component decremental phase connectivity occurs through deletion of one or more edges if a phase is displaced from a pore throat identified as a non tree edge the corresponding tree and therefore the phase connectivity graph are not affected as the corresponding component does not break up fig 4 a the non tree edge is simply deleted which is a trivially cheap operation if a phase is displaced from a pore throat identified as a tree edge the deletion of the tree edge splits the corresponding tree into two subtrees and the corresponding component may break up fig 4b however if a non tree edge exists that reconnects the two subtrees the corresponding component does not break up fig 4c the main computational challenge in updating the phase connectivity graph is the efficient search for such a reconnecting non tree edge as mentioned above any resulting trapped component is trivially removed from the phase connectivity graph when a phase is displaced from a pore node any adjacent pore throats containing the considered phase no longer contribute to the connectivity of the component therefore all edges corresponding to these pore throats must be deleted as they become singly connected or isolated as a matter of efficiency non tree edges are deleted before any tree edges the resulting isolated vertex is trivially deleted as well deletion of each tree edge triggers a search for a non tree edge reconnecting the resulting subtrees 2 3 the search for a reconnecting non tree edge following a tree split it is necessary to efficiently search for a reconnecting non tree edge first instead of querying the set of all non tree edges adjacent to the original tree it is sufficient to examine the non tree edges adjacent to one of the two subtrees henzinger and king 1999 holm et al 2001 by definition each subtree is adjacent to a set of reconnecting and a set of non reconnecting non tree edges the former set is shared while the latter sets are mutually exclusive as is evident from fig 4 therefore the search for a reconnecting non tree edge is fastest in the subtree that is adjacent to the smallest number of non tree edges note that this is not necessarily the smaller subtree i e having fewest vertices in this approach the most pessimistic scenario arises when both subtrees are adjacent to similar numbers of non tree edges however trees often split into subtrees with very unequal numbers of adjacent non tree edges which also underlines the importance of efficient determination of the number of adjacent non tree edges in addition to identifying the subtree with fewest incident non tree edges efficient iteration over the non tree edges is required which is also facilitated by the proposed graph representation petrovskyy 2019 provide further details of the implementation of the reconnection search in the phase connectivity algorithm 3 simulation results we analyse the efficiency of the new phase connectivity algorithm for ip simulations in pore network models of different size and origin the models include pore networks directly extracted from 3d micro ct images of bentheimer sandstone and estaillades carbonate jiang et al 2007 an explicitly integrated two scale network jiang et al 2013 based on 3d micro ct images at two different resolutions for a vuggy carbonate vik et al 2013 as well as artificially generated regular cubic lattices of varying size the networks display a range of average coordination numbers z 2 n e n v varying from 2 8 for the two scale network to approximately 6 for cubic lattices table 1 the cross section of each pore element is assumed to have the shape of an equilateral triangle which allows the presence of corner wetting films we simulate capillary driven displacement of oil by water secondary water flood under a range of different wetting conditions the initial state for each water flood is achieved through simulation of displacement of water by oil primary drainage under water wet conditions with zero contact angle followed by wettability alteration of all oil contacted pore surfaces kovscek et al 1993 these surfaces are assigned an advancing contact angle θadv with values representing water wet or oil wet conditions because the secondary water flood usually traps a significant amount of the defending oil phase it represents a good benchmark for the newly proposed algorithm moreover the evolution of the trapped oil saturation during the water flood strongly depends on the wetting conditions as we will demonstrate in section 3 3 displacement events are modelled as described by ryazanov et al 2009 and include piston like displacement and snap off as well as oil layers formation and collapse in pores of non uniform wettability under strongly oil wet conditions note that we simulate ip strictly as proposed by wilkinson and willemsen 1983 by invading each accessible pore element based on ordering of the capillary entry pressures and evaluation of the displaced phase connectivity after each displacement event 3 1 bentheimer sandstone for the bentheimer sandstone we present the simulated water flood capillary pressure curves for four different wetting conditions in fig 5 we confirm that the capillary pressure curves as well as the underlying displacement sequences of pore elements and emerging trapped phase distributions are exactly the same as those resulting from a pore network simulator with a conventional ip implementation using a breadth first graph traversal algorithm ryazanov et al 2009 for phase connectivity evaluation however the simulation runtimes presented in table 2 which were achieved on an intel xeon gold 5120 cpu show that the new phase connectivity algorithm delivers a speed up of around 3 orders of magnitude fig 6 provides an insight into why the new algorithm delivers such efficiency the number of examined non tree edges required to find a reconnection is as small as possible and is in fact most frequently equal to 1 a more detailed insight can be found in petrovskyy 2019 observe also that the frequency distributions and therefore the simulation runtimes are effectively independent of the wetting conditions this is despite very different displacement patterns as illustrated by the fluid occupancies for the strongly water wet and weakly oil wet cases presented in figs 7 and 8 respectively fig 7 displays a pattern of uniform growth of the invading phase throughout the domain dominated by snap off events in contrast fig 8 displays a pattern of capillary fingering dominated by piston like displacement events both cases result in very homogeneous residual phase distributions the small residual saturation for the strongly oil wet case θ a d v 160 is the result of oil layers in conventional ip simulations relative permeability calculations constitute the second most expensive operation after the phase connectivity evaluation the runtimes for simulations including relative permeability calculations which have been carried out for each saturation data point of the capillary pressure curves of fig 5 are presented in table 2 these calculations have a noticeable effect on the runtimes although application of high performance adaptive multi grid linear solvers henson and yang 2002 ensures that they do not cancel out the runtime improvement achieved by the new phase connectivity algorithm 3 2 estaillades carbonate and a two scale network the estaillades carbonate pore network represents a porous medium with spatially correlated pore sizes which has around 12 isolated porosity fraction of the total porosity disconnected from the inlet and outlet faces the number of pore elements and average coordination number are similar to that of the bentheimer sandstone network as shown in table 1 the capillary pressure curves for the secondary water flood under different wetting conditions presented in fig 9 show relatively large residual oil saturations the final phase occupancy for the weakly oil wet case fig 10 reveals a spatially correlated residual distribution of the defending oil phase the simulation runtimes of under 0 2s for all wetting conditions is similar to those for the sandstone case table 2 and confirm that the new algorithm performs equally efficient for qualitatively different porous formations the two scale network is representative of a porous medium containing both macro porosity that is disconnected on its own as well as micro porosity that ensures overall connectivity jiang et al 2013 consequently the network has a distinct bimodal pore size distribution with a very large number of pore elements table 1 that renders ip simulations with a conventional connectivity algorithm practically impossible additionally although the average coordination number is small some macro pores have very large coordination numbers corresponding to their connectivity to the micro pores the capillary pressure curves for the secondary water flood fig 11 are very different from the curves for the single scale bentheimer and estaillades networks with large residual oil saturations under water wet conditions and much smaller residual saturations under oil wet conditions the final occupancy for the weakly oil wet case fig 12 reveals that the invading phase occupies almost all macro pores hence the defending phase is trapped in the micro pores although very large in number the micro pores represent only a small fraction of the overall porosity resulting in the small residual saturation the ip simulation runtimes in the multi scale network range from 35s to 55s for the different wetting conditions in the next section we demonstrate that a conventional simulation for an equivalently sized network is expected to take several weeks which reflects a speed up using the new algorithm of around 6 orders of magnitude 3 3 regular lattices we analyse the impact of network size using synthetically generated lattice networks of varying size table 1 with the same geometrical characteristics such as pore size distribution and coordination number the evolution of the trapped or disconnected defending oil phase saturation s o d presented in fig 13 shows significant variations for linear dimensions smaller than 60 for linear dimension 60 and beyond the trapped saturation curves are smooth and do no longer vary with size this may indicate that the trapped phase clusters no longer grow with network size in other words the network with linear dimension 60 constitutes a representative elementary volume rev with respect to the trapped saturation curve these observations are consistent with the study of joekar niasar et al 2013 and extend its conclusions to varying wetting conditions therefore accurate prediction of multi phase flow functions often requires simulation on large networks in fig 14 we systematically compare the ip simulation runtimes using the conventional and the new phase connectivity algorithms for the secondary water flood under strongly water wet conditions the runtime of the conventional simulator is not simply larger by a constant factor but its growth displays a steeper log log slope in comparison to the runtime growth of the new simulator petrovskyy 2019 provide a theoretical derivation of the expressions for the runtime growth with number of pore elements n in addition to the runtimes for the lattice networks we have also included in fig 14 the runtimes for the previously investigated networks as a function of their number of pore elements we already established that the runtimes are very similar for the different wetting conditions for a given network fig 14 demonstrates that the runtimes are also practically independent of the network geometry i e pore sizes spatial correlation and coordination number this means that the runtime growth expressions derived for the lattice networks can be used to estimate the runtime growth for any type of network for example the runtime required to simulate the two scale network containing 6 5 106 pore elements is around 60 s one minute for the new approach and by extrapolation around 107 s just over 2 weeks for the conventional implementation 4 conclusions in this paper we have implemented and evaluated a novel solution to the phase connectivity problem this problem arises when identifying trapped defending phase clusters in pore network simulation of capillary driven flow governed by invasion percolation theory the new approach represents the connectivity of each phase cluster as a tree accompanied by a set of adjacent non tree edges deletion of a tree edge corresponding to a pore displacement event invokes a computationally intensive search for a possible reconnection of the resulting subtrees by an adjacent non tree edge the tree representation facilitates a highly efficient execution of the reconnection search the new phase connectivity algorithm has been applied in the simulations of the secondary water floods in pore networks of different origin and size comparison of simulated capillary pressure curves for a bentheimer sandstone under different wetting conditions with those resulting from a conventional ip simulator have confirmed the equivalence of the conventional and the new connectivity algorithm the simulation runtimes are effectively independent of the wetting conditions and more importantly the new phase connectivity algorithm delivers a speed up of around 3 orders of magnitude for the relatively small sandstone network additionally we have demonstrated that relative permeability calculations do not cancel out the runtime improvement simulations in a similarly sized network of an estaillades carbonate resulting in spatially correlated residual phase distribution require runtimes similar to the sandstone network under all wettability conditions more importantly in a much larger two scale network for which conventional simulations become unfeasible simulations using the new phase connectivity algorithm run in 10s of seconds these simulations have revealed residual phase distributions that are dominated by the bimodal pore size distribution a systematic simulation study on regular lattice networks of increasing size has revealed that the runtimes of the new simulator grows proportional to nlog2 n for networks with n pore elements compared to the much more unfavourable n 2 2 growth for the conventional simulator moreover we have demonstrated that the runtime scalings for the new algorithm apply to networks of any pore space geometry the lattice network simulation study has also proven useful in determining the representative elementary volume for multiphase flow properties such as the trapped saturation curve the new simulation approach is a very powerful tool for 1 efficient simulation of capillary driven displacement in massively large multi scale pore network models for calculation of representative multiphase flow properties 2 comprehensive stochastic analysis in single scale pore networks and 3 extensive sensitivity studies with respect to poorly characterised input parameters such as wettability declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank petrobras and shell for their sponsorship of the international centre for carbonate reservoirs iccr sebastian geiger further thanks the energi simulation foundation for supporting his chair programme supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103776 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
369,pore network representations of permeable media provide the framework for explicit simulation of capillary driven immiscible displacement governed by invasion percolation theory the most demanding task of a pore network flow simulation is the identification of trapped defending phase clusters at every displacement step i e the phase connectivity problem instead of employing the conventional adjacency list we represent the connectivity of a phase cluster as a tree accompanied by a set of adjacent non tree edges in this graph representation a decrease in phase connectivity due to a pore displacement event corresponds to deletion of either a tree or a non tree edge deletion of a tree edge invokes a computationally intensive search for a possible reconnection of the resulting subtrees by an adjacent non tree edge the tree representation facilitates a highly efficient execution of the reconnection search invasion percolation simulations of secondary water floods under different wetting conditions in pore networks of different origin and size confirm the efficiency of the proposed phase connectivity algorithm moreover a systematic simulation study of runtime growth with increasing model size on regular lattice networks demonstrates a consistent orders of magnitude speed up compared to conventional simulations consequently the proposed algorithm proves to be a powerful tool for invasion percolation simulations on large multi scale networks and for extensive stochastic analysis of typical single scale pore networks keywords phase connectivity pore network modelling invasion percolation simulation runtime growth 1 introduction multi phase flow in natural porous media is a phenomenon occurring during hydrocarbon production subsurface co2 sequestration and groundwater remediation estimation of residual saturation and continuum scale functions of the phase saturation such as capillary pressure and relative permeabilities is essential to predict darcy scale multi phase flow behaviour these flow functions are traditionally obtained from expensive and time consuming laboratory experiments that require non trivial interpretation steps hou et al 2012 richardson et al 1952 rose and bruce 1949 complementary to these experiments the flow functions may be calculated from simulations of multi phase displacements in models of the pore space blunt et al 2013 maes and geiger 2018 raeini et al 2014 ryazanov et al 2009 valvatne and blunt 2004 state of the art computed tomography ct imaging tools allow acquisition of high resolution 3d pore space models referred to as digital rocks cnudde and boone 2013 knackstedt et al 2009 pore scale multi phase flow simulation requires solution of the navier stokes equations coupled with equations for evolution of the capillary interfaces separating the immiscible fluids blunt 2017 hirt and nichols 1981 meakin and tartakovsky 2009 patankar and spalding 1972 various approaches exist to solve these equations directly on digital rocks but these are computationally expensive and therefore limited to sample volumes that are usually insufficient for representative calculations of the multi phase flow functions maes and geiger 2018 pan et al 2004 raeini et al 2014 when capillary forces dominate pore scale multi phase flow is often simulated as invasion percolation ip in pore networks wilkinson and willemsen 1983 a pore network approximates the 3d pore space as a collection of geometrically idealised elements i e pore nodes connected by pore throats while preserving its topology jiang et al 2007 starting from the inlet of the network the invading phase displaces a defending phase from network elements in order of their entry conditions which are determined by capillary forces at the fluid fluid interfaces lago and araujo 2001 a pore displacement event is only permitted if the defending phase is not trapped valvatne and blunt 2004 wilkinson and willemsen 1983 based on these rules ip produces a series of quasi static pore network phase occupancies comprising clusters of both phases as illustrated in fig 1 a phase cluster is defined as a collection of connected pore elements containing the same phase note that pore elements may contain more than one phase most notably when wetting films are present in pore corners hence a single pore element may belong to multiple phase clusters ryazanov et al 2009 berg et al 2013 andrew et al 2015 in two phase flow a defending phase cluster is considered trapped if it does not include the network outlet singh et al 2017 additionally for each quasi static occupancy relative permeabilities may be calculated based on steady state flow through phase clusters that are connected to both inlet and outlet assuming poiseuille flow in every pore element øren et al 1998 following each pore displacement event the cluster configuration and the corresponding phase connectivity may change the phase connectivity is either incremental when a phase invades a pore element or decremental when a phase vacates a pore element holm et al 2001 growth of the invading phase cluster during a capillary driven flow process is an example of incremental connectivity which may result in merger of invading phase clusters the corresponding connectivity of the defending phase is decremental which may result in break up and trapping of defending phase clusters berg et al 2013 joekar niasar et al 2013 fully dynamic connectivity arises when a phase both invades and vacates different pore elements simultaneously thus displaying both incremental and decremental connectivity during a given flow process this occurs when isolated phase clusters are mobilised usually when capillary forces are less significant compared to viscous forces or buoyancy joekar niasar et al 2010 mogensen and stenby 1998 additionally recent fast micro ct imaging has revealed that during slow steady state two phase flow phase clusters periodically break up and merge reynolds et al 2017 note that an invading phase may only partly displace the defending phase from a pore element for example when non wetting phase invasion leaves behind films of the defending wetting phase in the pore corners valvatne and blunt 2004 van dijke and sorbie 2006 partial displacement does not change the defending phase connectivity this paper focuses on decremental phase connectivity as its evaluation the query whether defending phase clusters remain connected or break up tends to be the most computationally expensive part of the phase connectivity problem and indeed of the entire ip simulation as outlined below most existing ip implementations gostick et al 2016 ryazanov et al 2009 valvatne and blunt 2004 knackstedt et al 2000 appear to have adopted either the hoshen kopelman cluster labelling hoshen and kopelman 1976 al futaisi and patzek 2003 or a general graph traversal algorithm such as depth first or breadth first search cormen et al 2001 to evaluate phase connectivity following each pore displacement event these algorithms employ the adjacency list to define phase clusters where each node stores a list of the throats that connect it to adjacent nodes this conventional graph representation is very efficient for the connectivity modification such as merging two clusters or establishing new clusters following break up however it is very inefficient for the connectivity query and leads to a high computational cost per ip simulation indeed as we will demonstrate in section 3 in conventional approaches the phase connectivity algorithm is computationally the most expensive operation in an ip simulation most networks derived from single micro ct scans are reasonably small therefore any inefficiency of the connectivity algorithm does not pose a major obstacle however to be representative of the corresponding macro scale flow properties pore networks often need to include features from micro ct scans at several different resolutions for example for carbonate rocks with so called micro porosity vik et al 2013 in this case pore scale information from different resolutions must be numerically integrated resulting in large multi scale networks bultreys et al 2015 jiang et al 2013 prodanovic et al 2015 the computational cost of ip simulations on these networks with multi million pore elements become prohibitive in this paper we introduce a novel approach to the phase connectivity problem based on an alternative graph representation which significantly reduces the overall computational cost of the ip simulation in section 2 we explain the concept of the new algorithm in section 3 we demonstrate the speed up achieved with the algorithm by simulating water flood under different wetting conditions in pore networks of different size and origin in the simulation results we focus on the trapped and residual saturations as these are most directly linked to the phase connectivity 2 phase connectivity algorithm 2 1 phase connectivity graph for the purpose of the new graph representation of phase connectivity it is sufficient to consider connectivity between pore nodes connectivity between any two pore nodes is determined by the existence of a continuous path of nodes and throats diestel 2010 phase connectivity is defined by paths on which all elements contain the considered phase the implication is that a pore throat only contributes to the phase connectivity if it is doubly connected i e it is adjacent to two nodes containing the same phase on the contrary singly connected throats do not contribute to phase connectivity neither do isolated throats fig 2 displays the connectivity of the defending phase shown in fig 1 illustrating the concepts of doubly and singly connected throats it is clear that singly connected and isolated throats arise when the defending phase vacates an adjacent node as long as a singly connected pore throat is adjacent to a pore node connected to an outlet node the defending phase may be displaced from this throat but this does not affect the phase connectivity the collection of all pore nodes and the interconnecting doubly connected pore throats containing a given phase establish the corresponding phase connectivity graph moreover each component of the phase connectivity graph corresponds to a phase cluster containing at least one node where the latter additionally includes adjacent singly connected throats updating the phase connectivity graph is sufficient for the evaluation of the phase connectivity between any pore nodes at any stage of ip simulation additionally a defending phase component is considered trapped if it does not contain nodes associated with the outlet since pore elements of a trapped component can no longer be invaded any trapped component can be excluded from the phase connectivity graph 2 2 phase connectivity representation following henzinger and king 1999 holm et al 2001 we represent the phase connectivity graph as a forest of trees and sets of non tree edges accompanying each tree as illustrated in fig 3 the tree for each component of the phase connectivity graph comprises all its pore nodes identified as the tree vertices and a subset of its pore throats identified as the tree edges this non unique subset of pore throats connects all pore nodes belonging to the same component to form the tree i e a graph without loops all other doubly connected pore throats belonging to the component are identified as non tree edges which if added to the tree would create loops trivially a component comprising nv pore nodes and ne doubly connected pore throats yields a tree with nv vertices n t e n v 1 tree edges and n n t e n e n t e non tree edges the corresponding forest of trees may be obtained through depth first search in each component decremental phase connectivity occurs through deletion of one or more edges if a phase is displaced from a pore throat identified as a non tree edge the corresponding tree and therefore the phase connectivity graph are not affected as the corresponding component does not break up fig 4 a the non tree edge is simply deleted which is a trivially cheap operation if a phase is displaced from a pore throat identified as a tree edge the deletion of the tree edge splits the corresponding tree into two subtrees and the corresponding component may break up fig 4b however if a non tree edge exists that reconnects the two subtrees the corresponding component does not break up fig 4c the main computational challenge in updating the phase connectivity graph is the efficient search for such a reconnecting non tree edge as mentioned above any resulting trapped component is trivially removed from the phase connectivity graph when a phase is displaced from a pore node any adjacent pore throats containing the considered phase no longer contribute to the connectivity of the component therefore all edges corresponding to these pore throats must be deleted as they become singly connected or isolated as a matter of efficiency non tree edges are deleted before any tree edges the resulting isolated vertex is trivially deleted as well deletion of each tree edge triggers a search for a non tree edge reconnecting the resulting subtrees 2 3 the search for a reconnecting non tree edge following a tree split it is necessary to efficiently search for a reconnecting non tree edge first instead of querying the set of all non tree edges adjacent to the original tree it is sufficient to examine the non tree edges adjacent to one of the two subtrees henzinger and king 1999 holm et al 2001 by definition each subtree is adjacent to a set of reconnecting and a set of non reconnecting non tree edges the former set is shared while the latter sets are mutually exclusive as is evident from fig 4 therefore the search for a reconnecting non tree edge is fastest in the subtree that is adjacent to the smallest number of non tree edges note that this is not necessarily the smaller subtree i e having fewest vertices in this approach the most pessimistic scenario arises when both subtrees are adjacent to similar numbers of non tree edges however trees often split into subtrees with very unequal numbers of adjacent non tree edges which also underlines the importance of efficient determination of the number of adjacent non tree edges in addition to identifying the subtree with fewest incident non tree edges efficient iteration over the non tree edges is required which is also facilitated by the proposed graph representation petrovskyy 2019 provide further details of the implementation of the reconnection search in the phase connectivity algorithm 3 simulation results we analyse the efficiency of the new phase connectivity algorithm for ip simulations in pore network models of different size and origin the models include pore networks directly extracted from 3d micro ct images of bentheimer sandstone and estaillades carbonate jiang et al 2007 an explicitly integrated two scale network jiang et al 2013 based on 3d micro ct images at two different resolutions for a vuggy carbonate vik et al 2013 as well as artificially generated regular cubic lattices of varying size the networks display a range of average coordination numbers z 2 n e n v varying from 2 8 for the two scale network to approximately 6 for cubic lattices table 1 the cross section of each pore element is assumed to have the shape of an equilateral triangle which allows the presence of corner wetting films we simulate capillary driven displacement of oil by water secondary water flood under a range of different wetting conditions the initial state for each water flood is achieved through simulation of displacement of water by oil primary drainage under water wet conditions with zero contact angle followed by wettability alteration of all oil contacted pore surfaces kovscek et al 1993 these surfaces are assigned an advancing contact angle θadv with values representing water wet or oil wet conditions because the secondary water flood usually traps a significant amount of the defending oil phase it represents a good benchmark for the newly proposed algorithm moreover the evolution of the trapped oil saturation during the water flood strongly depends on the wetting conditions as we will demonstrate in section 3 3 displacement events are modelled as described by ryazanov et al 2009 and include piston like displacement and snap off as well as oil layers formation and collapse in pores of non uniform wettability under strongly oil wet conditions note that we simulate ip strictly as proposed by wilkinson and willemsen 1983 by invading each accessible pore element based on ordering of the capillary entry pressures and evaluation of the displaced phase connectivity after each displacement event 3 1 bentheimer sandstone for the bentheimer sandstone we present the simulated water flood capillary pressure curves for four different wetting conditions in fig 5 we confirm that the capillary pressure curves as well as the underlying displacement sequences of pore elements and emerging trapped phase distributions are exactly the same as those resulting from a pore network simulator with a conventional ip implementation using a breadth first graph traversal algorithm ryazanov et al 2009 for phase connectivity evaluation however the simulation runtimes presented in table 2 which were achieved on an intel xeon gold 5120 cpu show that the new phase connectivity algorithm delivers a speed up of around 3 orders of magnitude fig 6 provides an insight into why the new algorithm delivers such efficiency the number of examined non tree edges required to find a reconnection is as small as possible and is in fact most frequently equal to 1 a more detailed insight can be found in petrovskyy 2019 observe also that the frequency distributions and therefore the simulation runtimes are effectively independent of the wetting conditions this is despite very different displacement patterns as illustrated by the fluid occupancies for the strongly water wet and weakly oil wet cases presented in figs 7 and 8 respectively fig 7 displays a pattern of uniform growth of the invading phase throughout the domain dominated by snap off events in contrast fig 8 displays a pattern of capillary fingering dominated by piston like displacement events both cases result in very homogeneous residual phase distributions the small residual saturation for the strongly oil wet case θ a d v 160 is the result of oil layers in conventional ip simulations relative permeability calculations constitute the second most expensive operation after the phase connectivity evaluation the runtimes for simulations including relative permeability calculations which have been carried out for each saturation data point of the capillary pressure curves of fig 5 are presented in table 2 these calculations have a noticeable effect on the runtimes although application of high performance adaptive multi grid linear solvers henson and yang 2002 ensures that they do not cancel out the runtime improvement achieved by the new phase connectivity algorithm 3 2 estaillades carbonate and a two scale network the estaillades carbonate pore network represents a porous medium with spatially correlated pore sizes which has around 12 isolated porosity fraction of the total porosity disconnected from the inlet and outlet faces the number of pore elements and average coordination number are similar to that of the bentheimer sandstone network as shown in table 1 the capillary pressure curves for the secondary water flood under different wetting conditions presented in fig 9 show relatively large residual oil saturations the final phase occupancy for the weakly oil wet case fig 10 reveals a spatially correlated residual distribution of the defending oil phase the simulation runtimes of under 0 2s for all wetting conditions is similar to those for the sandstone case table 2 and confirm that the new algorithm performs equally efficient for qualitatively different porous formations the two scale network is representative of a porous medium containing both macro porosity that is disconnected on its own as well as micro porosity that ensures overall connectivity jiang et al 2013 consequently the network has a distinct bimodal pore size distribution with a very large number of pore elements table 1 that renders ip simulations with a conventional connectivity algorithm practically impossible additionally although the average coordination number is small some macro pores have very large coordination numbers corresponding to their connectivity to the micro pores the capillary pressure curves for the secondary water flood fig 11 are very different from the curves for the single scale bentheimer and estaillades networks with large residual oil saturations under water wet conditions and much smaller residual saturations under oil wet conditions the final occupancy for the weakly oil wet case fig 12 reveals that the invading phase occupies almost all macro pores hence the defending phase is trapped in the micro pores although very large in number the micro pores represent only a small fraction of the overall porosity resulting in the small residual saturation the ip simulation runtimes in the multi scale network range from 35s to 55s for the different wetting conditions in the next section we demonstrate that a conventional simulation for an equivalently sized network is expected to take several weeks which reflects a speed up using the new algorithm of around 6 orders of magnitude 3 3 regular lattices we analyse the impact of network size using synthetically generated lattice networks of varying size table 1 with the same geometrical characteristics such as pore size distribution and coordination number the evolution of the trapped or disconnected defending oil phase saturation s o d presented in fig 13 shows significant variations for linear dimensions smaller than 60 for linear dimension 60 and beyond the trapped saturation curves are smooth and do no longer vary with size this may indicate that the trapped phase clusters no longer grow with network size in other words the network with linear dimension 60 constitutes a representative elementary volume rev with respect to the trapped saturation curve these observations are consistent with the study of joekar niasar et al 2013 and extend its conclusions to varying wetting conditions therefore accurate prediction of multi phase flow functions often requires simulation on large networks in fig 14 we systematically compare the ip simulation runtimes using the conventional and the new phase connectivity algorithms for the secondary water flood under strongly water wet conditions the runtime of the conventional simulator is not simply larger by a constant factor but its growth displays a steeper log log slope in comparison to the runtime growth of the new simulator petrovskyy 2019 provide a theoretical derivation of the expressions for the runtime growth with number of pore elements n in addition to the runtimes for the lattice networks we have also included in fig 14 the runtimes for the previously investigated networks as a function of their number of pore elements we already established that the runtimes are very similar for the different wetting conditions for a given network fig 14 demonstrates that the runtimes are also practically independent of the network geometry i e pore sizes spatial correlation and coordination number this means that the runtime growth expressions derived for the lattice networks can be used to estimate the runtime growth for any type of network for example the runtime required to simulate the two scale network containing 6 5 106 pore elements is around 60 s one minute for the new approach and by extrapolation around 107 s just over 2 weeks for the conventional implementation 4 conclusions in this paper we have implemented and evaluated a novel solution to the phase connectivity problem this problem arises when identifying trapped defending phase clusters in pore network simulation of capillary driven flow governed by invasion percolation theory the new approach represents the connectivity of each phase cluster as a tree accompanied by a set of adjacent non tree edges deletion of a tree edge corresponding to a pore displacement event invokes a computationally intensive search for a possible reconnection of the resulting subtrees by an adjacent non tree edge the tree representation facilitates a highly efficient execution of the reconnection search the new phase connectivity algorithm has been applied in the simulations of the secondary water floods in pore networks of different origin and size comparison of simulated capillary pressure curves for a bentheimer sandstone under different wetting conditions with those resulting from a conventional ip simulator have confirmed the equivalence of the conventional and the new connectivity algorithm the simulation runtimes are effectively independent of the wetting conditions and more importantly the new phase connectivity algorithm delivers a speed up of around 3 orders of magnitude for the relatively small sandstone network additionally we have demonstrated that relative permeability calculations do not cancel out the runtime improvement simulations in a similarly sized network of an estaillades carbonate resulting in spatially correlated residual phase distribution require runtimes similar to the sandstone network under all wettability conditions more importantly in a much larger two scale network for which conventional simulations become unfeasible simulations using the new phase connectivity algorithm run in 10s of seconds these simulations have revealed residual phase distributions that are dominated by the bimodal pore size distribution a systematic simulation study on regular lattice networks of increasing size has revealed that the runtimes of the new simulator grows proportional to nlog2 n for networks with n pore elements compared to the much more unfavourable n 2 2 growth for the conventional simulator moreover we have demonstrated that the runtime scalings for the new algorithm apply to networks of any pore space geometry the lattice network simulation study has also proven useful in determining the representative elementary volume for multiphase flow properties such as the trapped saturation curve the new simulation approach is a very powerful tool for 1 efficient simulation of capillary driven displacement in massively large multi scale pore network models for calculation of representative multiphase flow properties 2 comprehensive stochastic analysis in single scale pore networks and 3 extensive sensitivity studies with respect to poorly characterised input parameters such as wettability declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank petrobras and shell for their sponsorship of the international centre for carbonate reservoirs iccr sebastian geiger further thanks the energi simulation foundation for supporting his chair programme supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103776 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
