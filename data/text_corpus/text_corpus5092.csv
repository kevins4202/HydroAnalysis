index,text
25460,effective monitoring and forecasting of urban flooding are crucial for climate change adaptation and resilience around the world we proposed a novel and automatic system for urban flood detection and quantification our software takes image video data of flooding as inputs because the such data source is easy to obtain and widely available compared with conventional water level sensors or flood gauges first the kernel of our system is a robust water region segmentation module that detects flooded regions together with surrounding reference objects from the scene we combine image and video segmentation technologies to make the system reliable under varying weather and illumination conditions second our system uses the detected situated objects to determine the inundation depth field experiments demonstrate that our segmentation results are accurate and reliable and our system can detect flooding and estimate inundation depths from images and time lapse videos our code is available at https github com xmlyqing00 v floodnet keywords water level estimation water image video segmentation object dimension estimation flood monitoring deep learning data availability all our developed codes and data are published on github for public usage with comprehensive usage instructions see the end of the abstract 1 introduction improving our ability to monitor and forecast flooding in urban environments is important to developing an early warning system flooding constitutes the largest portion of insured losses among all disasters in the world accounting for 71 percent of the global natural hazard costs and affecting 3 billion people from 1995 to 2015 colgan 2017 aerts et al 2014 more accurate urban flood mapping during a disaster will provide essential data on the number of damaged homes and businesses which will assist urban centers in requesting and doling out emergency funding it will also facilitate the estimation of the number of days businesses remained closed i e flood duration and the amount of inventories lost which will determine the economic impact and losses from the flood event real time flood monitoring also helps with evacuation routing and first responders during disasters the ability to construct the rise and fall of water levels i e hydrograph in urban areas in real time during hurricanes fluvial and pluvial floods and other extreme weather events is challenging because of the low spatio temporal density of water level measurements and the complex interactions of built infrastructure and natural landscape with flowing water urban flood mapping commonly involves using incomplete information from sensors mounted on stream or tide gages airborne sensing satellite imagery and physics based flood models water level gages are the most ubiquitous but they are installed along water bodies far from urban areas to relay accurate and real time data for emergency operation centers to make informed decisions airborne and aerial sensing imagery captures the spatial extent of flooding but it lacks temporal resolution and estimates of flood depths physics based models solve the spatial and temporal inconsistencies but they require lengthy run time duration and the prediction accuracy depends on the quality of the model inputs thus an immediate need exists to find non traditional data to complement and improve our flood prediction capabilities in this study we use photos and videos that are widely available from various sources such as traffic cameras webcams social media and so on to detect flooding and estimate water inundation depths such kinds of images videos are prevalent in many places in urban areas as a result with a software system developed upon such flexibly widely distributed data sources building a dense flood inundation map can become feasible current approaches and limitations recent computer vision based systems for water depth estimation can be generally classified into two types 1 the first category of methods directly uses observed reference objects to estimate water levels without explicitly detecting or segmenting flood water regions in the scene alizadeh et al 2021 kharazi and behzadan 2021 meng et al 2019 park et al 2021 the assumption in these methods is that incomplete reference objects are submerged in water but if the reference objects are occluded by other objects or scenes due to camera view angles such an assumption is no longer valid and it leads to less accurate prediction 2 the second category of methods explicitly segments both object and flood regions so that their boundary interface is identified and used for estimation geetha et al 2017 pally and samadi 2022 chaudhary et al 2019 hu et al 2017 segmentation techniques used in these systems run in a frame by frame manner however water flood has highly versatile and dynamic geometry and appearance under different motions reflections and weather illumination conditions consequently segmenting them frame by frame without utilizing temporal and spatial coherence in the video often results in less stable and less accurate water segmentation and depth estimation our approach to achieve more accurate and robust water level estimation we first develop a new video based water segmentation framework to segment water and surrounding reference objects and then with this segmentation we design an inundation depth estimation algorithm to predict water depth the whole pipeline consists of the following two steps 1 the first step is water region segmentation which detects water flood regions together with surrounding reference objects from the scene and computes their boundaries and interface an image based segmentation model trained using water data we downloaded and labeled is used to detect and segment water from the first frame of the video then a video based segmentation model analyzes the change in the water region and tracks the water boundary in each of the subsequent frames 2 the second step is inundation depth estimation detected reference objects that are incomplete and share a common contour boundary are used to estimate the water depth such a reference object is matched with its complete template model to calculate the dimensions of its visible and invisible submerged parts where the vertical length of the invisible submerged part is reported as the water depth the main contributions of this study are summarized as follows we built a new system to estimate water depth from images and videos by integrating effective segmentation and dimension estimation techniques this pipeline is automatic and can run robustly under different weather conditions we designed a new deep learning segmentation pipeline that integrates both image based and video based segmentation models to detect and track flood and reference objects in long video sequences under varying weather conditions we designed a new inundation depth estimation model based on template matching and use it to automatically calculate water depth we collected and annotated a comprehensive dataset that includes water related images and videos segmentation labels and water depth information such a dataset is currently not publicly available our dataset will be useful for training water flood detection and segmentation systems we have released this dataset together with our open source programs to the public 2 methods we first review existing work in several topics closely related to this work section 2 1 and then introduce our proposed method sections 2 2 2 6 2 1 related work 2 1 1 water depth level analysis recently image video based computer vision techniques have been used to estimate the depth or water level of floods these methods can be generally classified into two categories one category avoids the challenging water segmentation task and directly estimates water depth using detected objects in the scene alizadeh et al 2021 kharazi and behzadan 2021 meng et al 2019 park et al 2021 these approaches assume that if an object is incomplete in images videos then the missing part is in the water and the missing portion ratio is directly used to calculate water depth level ning et al 2020 proposed a screening system to classify flood and non flood related images from social media this system can detect the existence of flood from the scene but cannot quantify the inundation depth recent papers detect common objects in the scene and estimate the water depth by computing the ratio of invisible parts to the whole structure commonly used objects include stop signs alizadeh et al 2021 kharazi and behzadan 2021 human skeleton meng et al 2019 and vehicles park et al 2021 these methods assume the invisible region of the reference object is submerged in water but in many videos involving multiple objects or relatively complex scenes this assumption does not hold and these models could yield unreliable estimations the second category first segments water regions in the scene and models their interface with submerged objects then performs water level estimation accordingly geetha et al 2017 pally and samadi 2022 chaudhary et al 2019 however accurate water segmentation from images videos is challenging water segmentation methods adopted in related estimation systems include non learning based approaches geetha et al 2017 pally and samadi 2022 and learning based ones chaudhary et al 2019 non learning based methods such as binary thresholding geetha et al 2017 and canny edge detection pally and samadi 2022 are designed for relatively simple environments where the water region is a simple largest connected component in the image they have two major limitations first thresholds need to be manually adjusted and making the pipeline automatic and these parameters adaptive to practical scenes is very difficult second water regions are often more complex than a simple and big connected component in practical images videos these systems could miss the detection of significant water regions recently deep learning based technologies achieved state of the art performance in image video segmentation chaudhary et al 2019 utilizes the mask r cnn he et al 2017 pipeline to segment water from the image erfani et al 2022 proposes aquanet to segment aquatic and non aquatic regions from images mask r cnn and aquanet are image based methods and they run the segmentation frame by frame without utilizing temporal coherency information in the video see fig 10 for comparisons our observation is that such temporal coherency information is critical especially when segmenting appearance volatile objects like water under dynamic weather and lighting conditions in addition bounding box based segmentation such as mask r cnn could fail to correctly bound water regions due to their highly dynamic and deforming nature 2 1 2 object segmentation in images and videos effective segmentation of flood from input images and videos is critical for accurate segmentation based flood estimation we briefly review recent image and video segmentation techniques in computer vision image segmentation recent use of deep learning based algorithms allows the complex network to robustly model both local and global characteristics of the interested objects regions in images a widely adopted deep learning based framework for semantic segmentation is the fully convolutional network fcn long et al 2015 which trains a series of convolutional layers to extract features and uses a deconvolutional operation to upsample the feature vector to infer pixel wise category chaurasia and culurciello 2017 proposed linknet which connects feature extraction layers to their corresponding decoder layers to recover spatial information lost during encoder downsampling operations recently three feature extraction architectures are popularly adopted resnet xception and efficientnet resnet stacks residual nets special blocks of layers with skip connections to allow the bypassing of one or more layers he et al 2016 this allows for the creation of extremely deep networks while avoiding degradation in training accuracy in contrast to the resnet s focus on deep layers inception szegedy et al 2015 and its improved model xception chollet 2017 seek to discover optimal network width by applying multiple convolutional filters through inception blocks xception outperformed resnet 152 and inception v3 on the commonly used testing benchmark imagenet efficientnet was developed to uniformly scale a network s width depth and resolution using a compound coefficient tan and le 2019 it reduces the necessity for manual tuning of individual network dimensions while using much fewer parameters compared to other convolutional networks of similar accuracy more recently efficientnet used neural architecture search to optimize its network architecture and achieved state of the art accuracy on the imagenet benchmark however applying image based segmentation in a frame by frame manner without considering temporal and spatial coherency between adjacent frames in the video could lead to inaccurate and less reliable segmentation video segmentation recent video object segmentation techniques are mostly deep learning based and they can be generally divided into two categories implicit learning and explicit learning methods the implicit learning approaches use masks computed in previous frames to infer masks in the current frame perazzi et al 2017 hu et al 2017 bao et al 2018 hu et al 2018b these methods often require online learning to adapt to new objects in the test video where online learning means that the trained model requires additional training during inference time to fit the evaluation environment the explicit learning methods use previous frames to construct an embedding space to memorize the object appearance then classify each pixel s label using their similarity oh et al 2018 voigtlaender et al 2019 wang et al 2019 lin et al 2019 hu et al 2018a liang et al 2020a oh et al 2019 however popular video object segmentation methods face two key challenges that hinder segmentation performance in processing flood videos 1 most monitoring video are not short clips but are relatively long existing expensive frame by frame detection schemes could lead to an out of memory issue or significantly decrease the model performance caused by temporal downsampling and 2 the frequently changed appearance of the water makes reliable feature learning and tracking difficult in this study we extend our recently developed afb urr software pipeline which organizes an adaptive feature bank afb to flexibly and dynamically manage the features of objects and water in videos with changing weather and lighting conditions liang et al 2020b afb urr achieved state of the art reliability and accuracy under such scenarios video segmentation methods require objects of interest in the first frame to be provided often through a manual annotation in this task however we need the entire system to run automatically hence we propose to combine image and video segmentation technologies using image segmentation to segment the first frame and then using video segmentation to propagate the estimated water masks to the subsequent frames 2 1 3 deep learning models for hydrologic data with the help of computer technologies many systems have been developed recently to model and analyze rivers from images for example semantic segmentation models have been developed to identify and extract river drainage and hydrologic streamlines from images mao et al 2021 xu et al 2021 hosseiny 2021 in these systems the u net architectures commonly used in image segmentation models are adopted to extract features from images and then decode them to identify water regions ford et al 2019 proposes a flooding assessment framework for climate change studies 2 2 proposed approach overview our proposed v floodnet pipeline is shown in fig 1 it takes a video as input and performs both water and reference object segmentation for water segmentation section 2 4 an image based segmentation module runs on the first frame or any given reference frame to detect and segment out the water region with the video based segmentation module this propagates to all the other frames we also detect and segment reference objects from the scene section 2 5 after the contents in the video are segmented into the water reference objects and background regions the interfaces between water regions and reference objects are detected then a water level estimation module section 2 6 performs a template matching to align the detected reference objects with templates this alignment normalizes i e rectifies the view of the reference objects it then superimposes the boundary interfaces onto the reference object to estimate water depth 2 3 water image data collection and annotation to train an effective deep video analysis system a large dataset of annotated image and video is needed the well known coco dataset lin et al 2014 is a large public semantic segmentation dataset and benchmark including many common scenes and objects this dataset contains images and their pixel wise annotations and it has been widely used in many object detection programs most of the existing water estimation systems rely on the coco dataset to train their object detection and segmentation models however coco does not include water images and cannot be used to train water detection and segmentation in fact annotated water flood images are very limited on the internet erfani et al 2022 propose an atlantis dataset that captures a wide range of water related objects but the number of images that have flood labels is limited our previously collected waterdataset liang et al 2020a includes general water images like pool lake and ocean from the ade20k dataset zhou et al 2017 and river images from the riverseg dataset lopez fuentes et al 2017 where cameras are on the top of the water regions to support model training in this study we extended our previously collected dataset by adding flood images with pixel wise annotations we searched the images with the keywords flood image on the pixabay website 1 1 https pixabay com and downloaded all related images 682 images we manually selected 149 images that are good quality based on two criteria 1 images contain water regions in a moderate size especially floods and 2 images are not over retouching three people collaborated and manually annotated the selected images using a public open source tool labelme wada 2021 we divided the images with a certain overlap and assigned them to different people for annotation after labeling the consistency of annotations on the overlapped images was checked we also re examined all the images in waterdataset and removed those with tiny or no water regions the final processed image dataset contains 1912 images from the ade20k dataset which includes general water related scenes like pool lake and ocean 300 river images from the riverseg dataset and 149 flooding images from the pixabay website as for the video data we collected and labeled multiple videos with groundtruth water depth one video recorded the rise and fall of the flooded levels in houston buffalo bayou during hurricane harvey in 2017 a few other videos are from boston harbor where a web camera is mounted at the boston harbor tea party ship museum 2 2 https www bostonteapartyship com boston webcam for these videos we obtained the water level measurements from the boston tide station id 8443970 ma of the national oceanic and atmospheric administration noaa 3 3 https tidesandcurrents noaa gov waterlevels html id 8443970 we also collected multiple flood videos from a creek on the lsu campus we deployed a raspberry pi camera near this creek it was controlled to capture and transmit the flood video one image per minute whenever there is a rain storm a water level gage was installed at this location which provides accurate reference depths finally the water dataset collected in this work includes 2361 images with pixel wise annotations and 6 videos with accurate water level depth groundtruths some example training images are shown in fig 3 we used this dataset for the training and evaluation of the proposed system the dataset will be released in github for comparative studies 2 4 water region segmentation 2 4 1 image segmentation our image based water segmentation framework relies on the popular encoder decoder based architecture the encoder is a neural network that encodes the input image into feature maps then the decoder takes the feature maps to predict the segmentation mask we conducted experiments on three widely used feature encoders resnet 50 he et al 2016 efficientnet b4 tan and le 2019 and xception chollet 2017 and the state of the art decoder network linknet chaurasia and culurciello 2017 all three feature encoders were pre trained on the imagenet dataset through comprehensive experiments the combination of efficientnet b4 and linknet achieves the best performance see section 3 1 hence we selected this combination as our image segmentation module 2 4 2 video segmentation after the camera is deployed it keeps capturing scene images and sending the videos for monitoring and analysis the aforementioned image segmentation model will run on the first frame or any given calibration frame of the video we can assume such a first reference frame is taken in good weather lighting conditions whose segmentation is relatively easy then in the next step a video segmentation model will run to propagate this frame s segmentation to subsequent frames in contrast to the direct frame by frame image segmentation approaches adopted in existing water analysis systems hu et al 2017 maninis et al 2019 with this video segmentation strategy we can utilize the temporal and spatial coherency information across different frames this is crucial to maintaining the segmentation robustness and accuracy under changing severe weather and lighting conditions e g nighttime rainy foggy snowy weather fig 4 a d depicts several examples in which the water appearance significantly varied owing to the changes in weather lighting conditions fig 2 shows the pipeline of the proposed water segmentation model the proposed video water segmentation module is built upon our recently developed architecture afb urr liang et al 2020b afb urr automatically adjusts the learned features to capture the transition of the object s appearance changes this allows the system to more robustly propagate segmentation masks of water or other appearance volatile objects to subsequent frames in long videos we recap the key designs of the video water segmentation here we formulate the segmentation problem as a pixel wise classification task that labels each pixel as objects of interest water or other reference objects or background the video segmentation module consists of two components adaptive feature bank and segmentation network adaptive feature bank we maintain two adaptive feature banks afb f o b j and f b a c k g r o u n d separately to store the water and non water features once we segment a new frame we compare its features with the feature banks then the similarity scores are decoded into the water mask to adapt the object appearance changes in the video we update the feature banks by three operations merging appending and deleting when a new feature is distinct from the existing features we append the new feature to the feature banks in contrast when a new feature f n e w is close to the existing ones in the feature bank we merge them by updating the closest feature descriptor f c l o s e s t from the feature bank by weight averaging 1 f c l o s e s t α f c l o s e s t 1 α f n e w where f c l o s e s t is the updated feature descriptor and we set α 10 in our experiments the feature bank discards obsolete features according to the least frequently used lfu index during the video segmentation process such feature banks allow our segmentation system to effectively memorize the temporally changing appearance of foreground background objects as the feature banks adaptively absorb adjust feature descriptors according to the changing scene segmentation network the segmentation network also follows the encoder decoder architecture the encoder is designed to encode the current frame i t at frame t to its feature map f t for segmentation 2 f t e n c o d e r i t where i t r h w 3 is an rgb image with height h and width w we build an attention module a t t e n oh et al 2019 to compute the similarity between the features of current frame f t and the feature banks f o b j and f b a c k g r o u n d the attention module is used to make the network learn and focus more on the important information rather than non useful features we use this attention module to compute the most relevant features g o b j and g b a c k g r o u n d from the feature banks 3 g o b j a t t e n f t f o b j g b a c k g r o u n d a t t e n f t f b a c k g r o u n d then a decoder network takes the feature maps of the current frame f t and combines it with similarity information calculated from the attention module to estimate the water segmentation mask s t for the current frame in the decoder module we gradually upscale the size of the g o b j and g b a c k g r o u n d by using the information from feature map f t we assign the class label for each pixel as the estimated segmentation mask s t by checking the largest score from object or background maps 4 s t d e c o d e r f t g o b j g b a c k g r o u n d where s t has the same resolution as the original input frame and it will be used to update the feature bank the widely adopted cross entropy loss function is used to train the video segmentation model in fig 4 b c we selected challenging frames where water appearances are affected by different weather and illumination fig 4 a is the first frame of the video we used the image segmentation module to segment it in fig 4 e the segmented water regions are marked in blue our video segmentation module built two feature banks f o b j and f b a c k g r o u n d to memorize the appearances of the object water and background then these two feature banks were used to segment the following frames meanwhile the estimated water masks continuously update the feature banks the feature banks adjust their learned features by analyzing the transitions of new features to adapt to the changes in appearance fig 4 e h show the segmentation results of the boston harbor video we can see that our video segmentation system has the ability to produce accurate and robust segmentation of flood water that has rapidly changing appearances 2 5 reference object segmentation to estimate the inundation depth from the scene we detect and segment reference objects that are submerged in water then we use estimated water region and reference objects to compute the submerging interface we adopted the mask r cnn he et al 2017 network for object segmentation we used the model pretrained on the coco dataset lin et al 2014 which can segment 80 classes of common objects including cars bikes people stop signs and so on we can also use the new video segmentation pipeline introduced in section 2 4 2 to segment reference objects in our experiments and release programs we simply used the mask r cnn pipeline because of two main reasons 1 we noticed that most common reference objects have the relatively stable appearance e g objects such as human beings stop signs and buildings that we incorporated in this work so it is often not necessary to maintain adaptive feature banks for reference objects and 2 unlike water the detection and segmentation models trained on the big coco dataset are robust and effective once the reference objects are segmented we extract a parameterized structural representation for each reference object using a standard template of this object for example in this work we model a stop sign template in 2d as an octagon plate of 8 vertices and a straight line pole of 2 ending vertices fig 11 c and model a person using a parameterized triangular mesh model fig 12 c for other general objects if we can define a standard structural representation as its template we can similarly model that object by fitting it with its template such a parametric template is in general modeled as a graph consisting of vertices with coordinates and edges connecting vertices and it is then used for dimension estimation reference objects are often incomplete with some parts invisible due to submergence or self occlusion to recover the missing part of an object we match the object structure with its template structure on the visible parts once we know the visible above water and invisible underwater parts of the object structure we can compute the water boundary and estimate the water depth in the scene in this work we tested our design on two types of reference objects stop signs and human beings which are often seen in flood scenes our template based estimation scheme can be readily generalized to other reference objects as long as their standard dimensions are known and their templates can be modeled in the following we elaborate on how we model and match the templates of stop signs and people and use them to perform water depth estimation 2 6 water depth level estimation built upon the water and reference object segmentation our water depth estimation algorithm is automatic and it has two steps template matching and submergence ratio calculation fig 6 shows the pipeline of the proposed water depth level estimation system template matching given an estimated reference object m o and a standard template of this type of object m t we match m o and m t by finding a spatial transformation that minimizes vertex displacements 5 t arg min t 1 n i 1 n t v t i v o i p where v t i m t and v o i m o and i iterates the n vertices t is the transformation function that aligns the vertices from the template structure to the object structure the feasible space of t can be defined based on prior knowledge or physical properties of the reference objects or based on appropriate regularization schemes for example stop signs are planar and rigid and are unlikely to undergo free form deformations hence we can restrict the feasible space of t to be rigid transformations when matching a stop sign with its template when using human beings as reference objects they are often modeled with more complex 3d geometric models which can deform more flexibly in a non rigid manner their feasible space should contain more general free form deformations more specifically here we adopt a 3d template human model lin et al 2021 that contains 431 vertices and allows t to be free form deformation and regress it using a neural network then from its feasible space we search for a transformation t that best aligns the reference object with its template with the inverse t we can transform and superimpose the template onto the reference object in the scene this allows us to obtain a standardized reference object m t t m t fig 12 c shows such an example on a reference person where a template mesh in peach color is transformed to superimpose the detected reference object in the image scene for rigid and planar objects such as stop signs the transformations in different images can be related with a homography matrix hartley and zisserman 2003 hence we solve a homography matrix with 8 degrees of freedom by matching the detected octagon stop sign plate and the template octagon similarly the transformed stop sign template can be computed see fig 11 submergence ratio calculation template matching allows the system to deform the template to superimpose the reference object in the scene the water segmentation mask separates this template its vertices nodes into two parts the emergent part v a and the submerged part v s based on the assumption that the water level is locally horizontal our system computes the submergence ratio on the template using a vertical length ratio of v s over v s v a we use h to represent the heights on the vertical axis and the submergence ratio r w a t e r is computed by 6 r w a t e r h v s h v s h v a to estimate actual water depth d w a t e r from the submergence ratio r w a t e r this ratio should multiply the physical size of the reference object l o b j then add the vertical distance from the bottom of the reference object to the ground b 7 d w a t e r r w a t e r l o b j b where l o b j is the vertical dimension of the reference object in practice we often set l o b j using the dimension of the template specifically the standard height of this type of reference object estimation without reference objects when no reference object is detected in the scene our system allows the user to pick an object as a reference and give an estimated dimension the reference object or region is supposed to be vertically above the water then our system will automatically track the reference object in all frames and compute the vertical distance from the reference object to the ground the estimated water mask separates the vertical line into two parts the line segment that is emergent visible denoted as v a and submerged by water invisible denoted as v s similarly we compute the submergence ratio r by eq 6 and convert it to the estimated water depth by eq 7 our system also supports choosing multiple objects as reference objects the estimated water level is the average of each reference object 3 results we evaluated each sub component of our water estimation system and tested the entire pipeline on videos captured in the field our code and dataset are available at https github com xmlyqing00 v floodnet 3 1 evaluations on image segmentation to select the best image segmentation model we conducted experiments on three recent encoder decoder based architectures resnet50 linknet he et al 2016 chaurasia and culurciello 2017 efficientnetb4 linknet yang et al 2018 and xception linknet chollet 2017 we trained these models using our waterdataset where images are randomly divided into 80 training 13 validation and 7 testing respectively we provided the training details in section 4 1 after each epoch the models are evaluated on the validation images the experiment results are summarized in table 1 the parameter size measures the number of learnable parameters in the network and the unit is million m sample segmentation results are shown in fig 5 because of its best miou score 0 7619 on water segmentation and the smallest model size parameter size 18m we picked the combination of efficientnet b4 and linknet as our image segmentation model comparison with existing water segmentation systems we compared our image segmentation module with the existing water segmentation papers geetha et al 2017 chaudhary et al 2019 pally and samadi 2022 we directly used our trained model to segment the images from their paper without any additional training or tuning figs 7 8 and 9 show the water segmentation result comparisons segmentation from these existing papers tends to be incomplete or produce mis classified foreground background in contrast our water segmentation model can more reliably and accurately identify water regions from images our model was never trained on these images which demonstrates the generalization of our segmentation model on flood images from new scenes 3 2 evaluations on video segmentation a key advantage of our proposed system is that it can leverage the temporal coherency in the video to further enhance the reliability of water segmentation in videos this is critical when it is needed to monitor and analyze scenes during both day and night time and or under both clear and inclement weather conditions when our system takes video as input it first uses the image segmentation model to segment water flood and reference objects in the first frame then our video segmentation model propagates the segmentation of water and other reference objects to subsequent frames utilizing not only texture appearance but also spatial temporal coherence in the video fig 4 shows several example frames from a video recorded at the boston harbor this video spanning several days covers different weather lighting conditions including a clear b snowy c dark and d sunny weather conditions on a relatively clear day our image segmentation can detect the precise water boundary e g segmentation for the first frame but as the lighting and weather conditions change the water and background appearances can change significantly in many scenarios e g nighttime inclement weather windy and rainy days etc the image based water segmentation accuracy drops significantly because not only it is difficult to include all the possible scenarios in training datasets but also it is hard to memorize all possible water appearances caused by ripples reflections etc by using spatial temporal consistency exploited from the video our video segmentation technique can greatly improve water segmentation robustness and accuracy fig 10 compares the segmentation results of auqanet erfani et al 2022 and our models in some scenarios column a shows two frames one taken on a snowy and foggy day top and one in the nighttime bottom the reflections and lighting make the water looks drastically different from the daytime when the image segmentation model is directly applied the results the second column are less desirable see columns b and c but when the whole video sequence is considered our video segmentation model gradually and adaptively adjusts the water features and eventually produce much better segmentation results column d in both scenarios 3 3 inundation depth estimation we conducted multiple field experiments to evaluate our flood water and inundation depth estimation pipeline in the first field experiment we deployed a standard stop sign in a shallow lake on the louisiana state university lsu campus in baton rouge louisiana usa next we further tested our system with multiple monitoring videos of actual flooding or water level changes recorded during storms or severe weather conditions these videos were from 1 the lsu campus creek 2 boston harbor and 3 houston buffalo bayou the two videos at the creek on the lsu campus were captured during two heavy storms in the summer of 2020 the three videos at the boston harbor were recorded several days during winter and summer with different weather lighting conditions the video at the houston buffalo bayou recorded the water level changes during hurricane harvey 2017 field experiment 1 lsu lake fig 11 a shows a frame in a video a student stands in the water holding a standard stop sign a ruler is attached to this stop sign to measure the actual water depth at this location the water depth is 0 343 m see fig 11 f the automatic water region segmentation from our system is shown in the blue mask in fig 11 b we show that our system can use either the stop sign or the person to estimate the water depth when using the stop sign as the reference the system matches a template model fig 11 c to the detected object in the image then the visible region fig 11 d superimposed with the water object interface infers the submergence ratio r w a t e r on the stop sign template fig 11 e following u s department of transportation 2003 s report a standard stop sign on a conventional road in the usa has a 0 79 0 79 m plate and a 2 159 m long pole where 0 75 m 30 in across opposite flats of the red octagon with a 0 02 m 3 4 in white border with eqs 6 and 7 and this standard dimension l o b j 2 159 m and b 0 our system estimated the average water depth at this location to be 0 359 m hence with the stop sign as a reference the prediction error is 0 016 m 4 7 when using the person as the reference the 3d template mesh is deformed to fit the detected person in the image fig 12 c similarly with the superimposition of template mesh and the water interface the above water compared to submerged parts of the human body are computed visualized in green and red points respectively in fig 12 d the average submergence ratio on the person is then calculated to be 0 1882 fig 12 e finally using the average height of the adult male 1 754 m following fryar et al 2018 the system estimated the average water depth at this location to be 0 1882 1 754 0 330 m by eq 7 where l o b j 1 754 and b 0 because the adult male stands on the ground hence using the person as a reference the prediction error is 0 013 m 3 8 in our current system we use the standard dimension of the reference objects as a default dimension if the exact dimension of the reference object is known different we can simply adjust the parameters in the calculation to obtain a more accurate estimation field experiment 2 lsu campus creek we deployed a raspberry pi camera near the lsu creek to continuously capture the water depth change of the creek we used one video that captured a rain storm on june 24 2020 as an example to demonstrate our experiment fig 13 a shows one frame of this video fig 13 b illustrates the segmented water region in the blue mask because there is no known reference object detected in this scene the systems asked the user to pick a landmark and the program used it as the reference to calibrate the scene in this experiment the top of the gauge was selected as the reference marked in the green box at the top of the gage in fig 13 b a simple vertical line from the landmark to the ground water interface is used to estimate water depth the system partitions this reference vertical line into the above water and under water parts separated by the water interface the submergence ratio r w a t e r is the ratio between the dimension of underwater part and that of the entire line we convert the submergence ratio to water depth by eq 7 when the camera was first deployed its position orientation and focal lengths were calibrated and the object size l o b j and b were calculated and fixed after this step the system can automatically estimate the water depth over time the results are plotted in fig 13 c the average absolute error of the estimated water depth in this experiment is 0 018 m and its standard deviation is 0 012 m see table 2 field experiment 3 boston harbor we tested our system on several long videos to demonstrate its robustness in varying weather and illumination conditions in our collected waterdataset there are three long videos recorded on the boston harbor the camera is mounted on the tea party museum ship and we downloaded the video one frame per ten minutes for multiple days fig 14 a shows one frame of the recorded video from 2019 01 18 to 2019 01 23 when no salient reference objects are detected in the video our system allows users to adopt one of two interactive mechanisms for calibration 1 user can select four points to represent the horizontal axis and vertical axes then the system can use them to estimate a perspective transformation to calibrate the scene and 2 user can pick an arbitrary region in the scene as the reference in this experiment adopting the second mechanism a region on the background building was selected and tracked then pixel scale in the videos can be converted into physical units by fitting the parameters l o b j and b in eq 7 tracking water levels in these videos is highly challenging due to two main reasons 1 different weather and different day night illumination make the water appearance change continuously and significantly and 2 the water regions are often occluded by other moving objects e g ships people existing image and video segmentation systems cannot effectively detect and segment water regions in such situations our results are reported in table 2 where both the segmentation and water level estimation are reliable and produce small errors more visualization results are attached in the supplementary video on our github page field experiment 4 houston buffalo bayou hurricane harvey 2017 made landfall along the texas coast on 25 august 2017 as a category 4 hurricane this major storm caused catastrophic flooding of the densely populated regions of houston and beaumont leading to flooding of major interstate highways such as i 10 and i 45 a high water mark on the northern bank of buffalo bayou revealed the maximum flood water elevation of 10 3 m 33 7 ft navd88 at the study site details about the flooding can be found in jafari et al 2021 at milam street the rising floodwater reached an elevation of 8 3 m 27 1 ft on august 27 2017 a water level of 11 3 m 33 85 ft above the bottom of the bayou harris country flood warning system 2017 however the stream gage at milam street only collected data until 02 44 on august 27 this gage failure demonstrates the need for multiple methods to construct and verify flood hydrographs during hurricane harvey a time lapse camera was placed on the second floor of the bayou place offices building on capitol street near milam street and overlooked buffalo bayou memorial drive overpass interstate 45 overpass and the houston aquarium the camera recorded a video of the rise and fall of flood levels in buffalo bayou from august 25 to 31 our system took the video as input to estimate the water level and then created the hydrograph our system used two bridge piers adjacent to buffalo bayou as reference objects to estimate the changes in the water level over time we used the same conversion function eq 7 to convert the estimated submergence ratio to the water levels the elevation of the bottom of the pier was found to be 0 83 m 2 72 ft navd88 and the top 18 11 m 59 41 ft navd88 after calibration we have l o b j 18 11 0 83 17 28 and b 0 83 the system can then estimate the water level following eq 7 with the water level estimated by our system we compared it with the milam street gauge the closest measure information available near buffalo bayou and the results computed from our previous model jafari et al 2021 the comparison is plotted in fig 15 c in the morning of august 26 the milam stream gauge shows an initial rise of the hydrograph at midnight of august 27 the milam street gauge failed therefore the blue curve stops our proposed model successfully captures the water rise on the morning of august 26 in contrast the previous model jafari et al 2021 could not catch this rise effectively after the milam street gauge failed on august 27 our proposed model can still provide information on the flood specifically according to u s geological survey 2017 usgs a high water mark 4 4 stn site no txhar23198 in usgs recorded the peak flood level of approximately 10 3 m occurred in the early afternoon of august 27 the hydrograph reconstructed by our v floodnet reaches the peak flood elevation at a similar time by contrast the previous model jafari et al 2021 estimated the maximum water level over 14 m which is significantly higher than the observed high water mark reported by usgs the main reason that the previous model failed to produce accurate estimation is that it uses an image based segmentation model to identify the water boundary under severe and changing weather and illumination conditions image based models cannot reliably stably track the water region our proposed model uses a video based model which leverages the spatial temporal coherence in the video to achieve a more robust estimation it is important to note that during hurricane harvey water level fluctuations mostly occur during the night time nevertheless fig 15 shows our model is able to reliably capture them to build the flood hydrograph table 2 summarizes our estimation errors in field experiments 2 4 by comparing our estimations and the groundtruth depths collected from the nearest gages the estimation errors mainly come from two factors 1 the accuracy of water and reference object segmentation and 2 the accuracy of reference template modeling and matching for example in the boston harbor scenes the reference object building and water region are very far away from the camera a single pixel error in both water segmentation and reference object localization could lead to a relatively bigger estimation error therefore in general our estimation performs better when the camera is close to the flood scene and reference objects severe weather or bad illumination conditions poses extra challenges for example during hurricane harvey water level fluctuations mostly occur in the nighttime the water was hardly visible and its segmentation is very challenging this leads to bigger absolute estimation errors in the houston buffalo bayou scene 4 discussion 4 1 implementation details for image segmentation we use efficientnet b4 and linknet yang et al 2018 chaurasia and culurciello 2017 as the segmentation architecture the model was then trained using images from our waterdataset all the training images were resized to the network s input resolution of 416 416 and fed through the network in a batch size of 4 the learning rate was initialized to 10 4 and decreased to 1e 5 halfway through training the widely used metric mean intersection over union miou liang et al 2020a was adopted to evaluate the segmentation quality all these three segmentation networks were trained for 150 epochs using dice loss sudre et al 2017 for video segmentation we used afb urr liang et al 2020b as the architecture it was pre trained on the youtube vos xu et al 2018 dataset which contains 5k videos of the general objects we then fine tuned it on our waterdataset the initial learning rate was 1 e 5 and it was halved every 25 epochs the total number of training epochs was 100 we chose the adamw loshchilov and hutter 2017 as the back propagation optimizer the input resolution of the video is 480 854 note that in both image and video based flood segmentation we resize the input to meet the requirement of the segmentation module then we resize the estimated water mask back to the original resolution since the resolution range of most video cameras is from 480p to 1080p our segmentation modules balance the speed and segmentation quality which are insensitive to the input resolution for the reference object segmentation we used the mask rcnn to segment the stopsign and people he et al 2017 we used meshtransformer to complete the 3d mesh of the detected person lin et al 2021 we used csrt tracker to track the user selected bounding box in the input video lukezic et al 2017 we directly used their model and pretrained weights in the experiments 4 2 limitations and future work a key challenge in image video segmentation is the processing of nighttime images videos when the scene becomes dark and dim specular highlights and reflections could significantly affect water region identification and segmentation although with the help of spatio temporal coherency and adaptive feature updating our video segmentation model can identify the main water regions their boundaries are less accurately tracked such less accurate contours could consequently affect water level estimation in future work we will incorporate video based dark image enhancing techniques to tackle this issue the segmentation of the first frame affects the accuracy of the entire video segmentation to make this first frame segmentation more reliable the training dataset should contain flood and water images in various environments with different weather illumination and reflection conditions our waterdataset currently contains several thousand water images but there are only about 150 flood images we will collect and label more flood images especially those whose flood region is difficult to detect or segment this will improve the accuracy and robustness of our image segmentation model 5 conclusions we proposed a comprehensive video processing and analysis system that can detect and segment water and surrounding reference objects and use them to estimate the water level or inundation depth unlike existing segmentation techniques in the computer vision literature that were developed to handle common daily objects our system is capable of adaptively capturing the appearance change of water in the scene under dynamic noisy and weather lighting conditions hence it can produce more accurate and precise water segmentation which is crucial to flood height or inundation depth estimation we also developed a depth estimation algorithm using reference objects and template matching allowing the system to effectively calculate the inundation depth this pipeline can run automatically or if necessary interactively to support convenient water depth estimation and monitoring in our experiments we demonstrated the effectiveness and robustness of the proposed system in extensive applications validated through multiple field experiments our system can provide effective water depth estimations in various video scenes besides the methodology we also released a set of water related image and video data with annotations these data can benefit the training of other deep learning based water analysis systems we also made our code and models open sourced they can conveniently be used as enabling tools or used for comparative studies code and data availability the source code is open source for academic research it is available at https github com xmlyqing00 v floodnet detailed installation and usage instructions are included in the readme file the proposed waterdataset data and pretrained model weights are also available for downloading credit authorship contribution statement yongqing liang methodology software writing visualization xin li methodology resources writing supervision funding acquisition brian tsai software data curation writing visualization qin chen conceptualization validation resources writing project administration funding acquisition navid jafari validation investigation resources writing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we would like to thank thomas rinaudo claire white amina meselhe and dominion ajayi who helped label the water images and videos we thank alex wu who developed parts of the people mesh registration model we thank eli barbin who conducted the experiments in the lsu lake this material is based upon work supported by the national science foundation usa grant 1760582 2139882 2139883 and 1946231 the authors would like to thank louisiana sea grant undergraduate research opportunities program urop usa t baker smith inc louisiana board of regents industrial ties research program leqsf 2018 21 rd b 03 and the northeastern university global resilience institute usa for supporting this research we appreciate permission by mr teddy vandenberg to use his time lapse video of buffalo bayou during hurricane harvey any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation t baker smith louisiana board of regents and louisiana sea grant appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105586 appendix a supplementary data the following is the supplementary material related to this article mmc s1 supplementary material for v floodnet 
25460,effective monitoring and forecasting of urban flooding are crucial for climate change adaptation and resilience around the world we proposed a novel and automatic system for urban flood detection and quantification our software takes image video data of flooding as inputs because the such data source is easy to obtain and widely available compared with conventional water level sensors or flood gauges first the kernel of our system is a robust water region segmentation module that detects flooded regions together with surrounding reference objects from the scene we combine image and video segmentation technologies to make the system reliable under varying weather and illumination conditions second our system uses the detected situated objects to determine the inundation depth field experiments demonstrate that our segmentation results are accurate and reliable and our system can detect flooding and estimate inundation depths from images and time lapse videos our code is available at https github com xmlyqing00 v floodnet keywords water level estimation water image video segmentation object dimension estimation flood monitoring deep learning data availability all our developed codes and data are published on github for public usage with comprehensive usage instructions see the end of the abstract 1 introduction improving our ability to monitor and forecast flooding in urban environments is important to developing an early warning system flooding constitutes the largest portion of insured losses among all disasters in the world accounting for 71 percent of the global natural hazard costs and affecting 3 billion people from 1995 to 2015 colgan 2017 aerts et al 2014 more accurate urban flood mapping during a disaster will provide essential data on the number of damaged homes and businesses which will assist urban centers in requesting and doling out emergency funding it will also facilitate the estimation of the number of days businesses remained closed i e flood duration and the amount of inventories lost which will determine the economic impact and losses from the flood event real time flood monitoring also helps with evacuation routing and first responders during disasters the ability to construct the rise and fall of water levels i e hydrograph in urban areas in real time during hurricanes fluvial and pluvial floods and other extreme weather events is challenging because of the low spatio temporal density of water level measurements and the complex interactions of built infrastructure and natural landscape with flowing water urban flood mapping commonly involves using incomplete information from sensors mounted on stream or tide gages airborne sensing satellite imagery and physics based flood models water level gages are the most ubiquitous but they are installed along water bodies far from urban areas to relay accurate and real time data for emergency operation centers to make informed decisions airborne and aerial sensing imagery captures the spatial extent of flooding but it lacks temporal resolution and estimates of flood depths physics based models solve the spatial and temporal inconsistencies but they require lengthy run time duration and the prediction accuracy depends on the quality of the model inputs thus an immediate need exists to find non traditional data to complement and improve our flood prediction capabilities in this study we use photos and videos that are widely available from various sources such as traffic cameras webcams social media and so on to detect flooding and estimate water inundation depths such kinds of images videos are prevalent in many places in urban areas as a result with a software system developed upon such flexibly widely distributed data sources building a dense flood inundation map can become feasible current approaches and limitations recent computer vision based systems for water depth estimation can be generally classified into two types 1 the first category of methods directly uses observed reference objects to estimate water levels without explicitly detecting or segmenting flood water regions in the scene alizadeh et al 2021 kharazi and behzadan 2021 meng et al 2019 park et al 2021 the assumption in these methods is that incomplete reference objects are submerged in water but if the reference objects are occluded by other objects or scenes due to camera view angles such an assumption is no longer valid and it leads to less accurate prediction 2 the second category of methods explicitly segments both object and flood regions so that their boundary interface is identified and used for estimation geetha et al 2017 pally and samadi 2022 chaudhary et al 2019 hu et al 2017 segmentation techniques used in these systems run in a frame by frame manner however water flood has highly versatile and dynamic geometry and appearance under different motions reflections and weather illumination conditions consequently segmenting them frame by frame without utilizing temporal and spatial coherence in the video often results in less stable and less accurate water segmentation and depth estimation our approach to achieve more accurate and robust water level estimation we first develop a new video based water segmentation framework to segment water and surrounding reference objects and then with this segmentation we design an inundation depth estimation algorithm to predict water depth the whole pipeline consists of the following two steps 1 the first step is water region segmentation which detects water flood regions together with surrounding reference objects from the scene and computes their boundaries and interface an image based segmentation model trained using water data we downloaded and labeled is used to detect and segment water from the first frame of the video then a video based segmentation model analyzes the change in the water region and tracks the water boundary in each of the subsequent frames 2 the second step is inundation depth estimation detected reference objects that are incomplete and share a common contour boundary are used to estimate the water depth such a reference object is matched with its complete template model to calculate the dimensions of its visible and invisible submerged parts where the vertical length of the invisible submerged part is reported as the water depth the main contributions of this study are summarized as follows we built a new system to estimate water depth from images and videos by integrating effective segmentation and dimension estimation techniques this pipeline is automatic and can run robustly under different weather conditions we designed a new deep learning segmentation pipeline that integrates both image based and video based segmentation models to detect and track flood and reference objects in long video sequences under varying weather conditions we designed a new inundation depth estimation model based on template matching and use it to automatically calculate water depth we collected and annotated a comprehensive dataset that includes water related images and videos segmentation labels and water depth information such a dataset is currently not publicly available our dataset will be useful for training water flood detection and segmentation systems we have released this dataset together with our open source programs to the public 2 methods we first review existing work in several topics closely related to this work section 2 1 and then introduce our proposed method sections 2 2 2 6 2 1 related work 2 1 1 water depth level analysis recently image video based computer vision techniques have been used to estimate the depth or water level of floods these methods can be generally classified into two categories one category avoids the challenging water segmentation task and directly estimates water depth using detected objects in the scene alizadeh et al 2021 kharazi and behzadan 2021 meng et al 2019 park et al 2021 these approaches assume that if an object is incomplete in images videos then the missing part is in the water and the missing portion ratio is directly used to calculate water depth level ning et al 2020 proposed a screening system to classify flood and non flood related images from social media this system can detect the existence of flood from the scene but cannot quantify the inundation depth recent papers detect common objects in the scene and estimate the water depth by computing the ratio of invisible parts to the whole structure commonly used objects include stop signs alizadeh et al 2021 kharazi and behzadan 2021 human skeleton meng et al 2019 and vehicles park et al 2021 these methods assume the invisible region of the reference object is submerged in water but in many videos involving multiple objects or relatively complex scenes this assumption does not hold and these models could yield unreliable estimations the second category first segments water regions in the scene and models their interface with submerged objects then performs water level estimation accordingly geetha et al 2017 pally and samadi 2022 chaudhary et al 2019 however accurate water segmentation from images videos is challenging water segmentation methods adopted in related estimation systems include non learning based approaches geetha et al 2017 pally and samadi 2022 and learning based ones chaudhary et al 2019 non learning based methods such as binary thresholding geetha et al 2017 and canny edge detection pally and samadi 2022 are designed for relatively simple environments where the water region is a simple largest connected component in the image they have two major limitations first thresholds need to be manually adjusted and making the pipeline automatic and these parameters adaptive to practical scenes is very difficult second water regions are often more complex than a simple and big connected component in practical images videos these systems could miss the detection of significant water regions recently deep learning based technologies achieved state of the art performance in image video segmentation chaudhary et al 2019 utilizes the mask r cnn he et al 2017 pipeline to segment water from the image erfani et al 2022 proposes aquanet to segment aquatic and non aquatic regions from images mask r cnn and aquanet are image based methods and they run the segmentation frame by frame without utilizing temporal coherency information in the video see fig 10 for comparisons our observation is that such temporal coherency information is critical especially when segmenting appearance volatile objects like water under dynamic weather and lighting conditions in addition bounding box based segmentation such as mask r cnn could fail to correctly bound water regions due to their highly dynamic and deforming nature 2 1 2 object segmentation in images and videos effective segmentation of flood from input images and videos is critical for accurate segmentation based flood estimation we briefly review recent image and video segmentation techniques in computer vision image segmentation recent use of deep learning based algorithms allows the complex network to robustly model both local and global characteristics of the interested objects regions in images a widely adopted deep learning based framework for semantic segmentation is the fully convolutional network fcn long et al 2015 which trains a series of convolutional layers to extract features and uses a deconvolutional operation to upsample the feature vector to infer pixel wise category chaurasia and culurciello 2017 proposed linknet which connects feature extraction layers to their corresponding decoder layers to recover spatial information lost during encoder downsampling operations recently three feature extraction architectures are popularly adopted resnet xception and efficientnet resnet stacks residual nets special blocks of layers with skip connections to allow the bypassing of one or more layers he et al 2016 this allows for the creation of extremely deep networks while avoiding degradation in training accuracy in contrast to the resnet s focus on deep layers inception szegedy et al 2015 and its improved model xception chollet 2017 seek to discover optimal network width by applying multiple convolutional filters through inception blocks xception outperformed resnet 152 and inception v3 on the commonly used testing benchmark imagenet efficientnet was developed to uniformly scale a network s width depth and resolution using a compound coefficient tan and le 2019 it reduces the necessity for manual tuning of individual network dimensions while using much fewer parameters compared to other convolutional networks of similar accuracy more recently efficientnet used neural architecture search to optimize its network architecture and achieved state of the art accuracy on the imagenet benchmark however applying image based segmentation in a frame by frame manner without considering temporal and spatial coherency between adjacent frames in the video could lead to inaccurate and less reliable segmentation video segmentation recent video object segmentation techniques are mostly deep learning based and they can be generally divided into two categories implicit learning and explicit learning methods the implicit learning approaches use masks computed in previous frames to infer masks in the current frame perazzi et al 2017 hu et al 2017 bao et al 2018 hu et al 2018b these methods often require online learning to adapt to new objects in the test video where online learning means that the trained model requires additional training during inference time to fit the evaluation environment the explicit learning methods use previous frames to construct an embedding space to memorize the object appearance then classify each pixel s label using their similarity oh et al 2018 voigtlaender et al 2019 wang et al 2019 lin et al 2019 hu et al 2018a liang et al 2020a oh et al 2019 however popular video object segmentation methods face two key challenges that hinder segmentation performance in processing flood videos 1 most monitoring video are not short clips but are relatively long existing expensive frame by frame detection schemes could lead to an out of memory issue or significantly decrease the model performance caused by temporal downsampling and 2 the frequently changed appearance of the water makes reliable feature learning and tracking difficult in this study we extend our recently developed afb urr software pipeline which organizes an adaptive feature bank afb to flexibly and dynamically manage the features of objects and water in videos with changing weather and lighting conditions liang et al 2020b afb urr achieved state of the art reliability and accuracy under such scenarios video segmentation methods require objects of interest in the first frame to be provided often through a manual annotation in this task however we need the entire system to run automatically hence we propose to combine image and video segmentation technologies using image segmentation to segment the first frame and then using video segmentation to propagate the estimated water masks to the subsequent frames 2 1 3 deep learning models for hydrologic data with the help of computer technologies many systems have been developed recently to model and analyze rivers from images for example semantic segmentation models have been developed to identify and extract river drainage and hydrologic streamlines from images mao et al 2021 xu et al 2021 hosseiny 2021 in these systems the u net architectures commonly used in image segmentation models are adopted to extract features from images and then decode them to identify water regions ford et al 2019 proposes a flooding assessment framework for climate change studies 2 2 proposed approach overview our proposed v floodnet pipeline is shown in fig 1 it takes a video as input and performs both water and reference object segmentation for water segmentation section 2 4 an image based segmentation module runs on the first frame or any given reference frame to detect and segment out the water region with the video based segmentation module this propagates to all the other frames we also detect and segment reference objects from the scene section 2 5 after the contents in the video are segmented into the water reference objects and background regions the interfaces between water regions and reference objects are detected then a water level estimation module section 2 6 performs a template matching to align the detected reference objects with templates this alignment normalizes i e rectifies the view of the reference objects it then superimposes the boundary interfaces onto the reference object to estimate water depth 2 3 water image data collection and annotation to train an effective deep video analysis system a large dataset of annotated image and video is needed the well known coco dataset lin et al 2014 is a large public semantic segmentation dataset and benchmark including many common scenes and objects this dataset contains images and their pixel wise annotations and it has been widely used in many object detection programs most of the existing water estimation systems rely on the coco dataset to train their object detection and segmentation models however coco does not include water images and cannot be used to train water detection and segmentation in fact annotated water flood images are very limited on the internet erfani et al 2022 propose an atlantis dataset that captures a wide range of water related objects but the number of images that have flood labels is limited our previously collected waterdataset liang et al 2020a includes general water images like pool lake and ocean from the ade20k dataset zhou et al 2017 and river images from the riverseg dataset lopez fuentes et al 2017 where cameras are on the top of the water regions to support model training in this study we extended our previously collected dataset by adding flood images with pixel wise annotations we searched the images with the keywords flood image on the pixabay website 1 1 https pixabay com and downloaded all related images 682 images we manually selected 149 images that are good quality based on two criteria 1 images contain water regions in a moderate size especially floods and 2 images are not over retouching three people collaborated and manually annotated the selected images using a public open source tool labelme wada 2021 we divided the images with a certain overlap and assigned them to different people for annotation after labeling the consistency of annotations on the overlapped images was checked we also re examined all the images in waterdataset and removed those with tiny or no water regions the final processed image dataset contains 1912 images from the ade20k dataset which includes general water related scenes like pool lake and ocean 300 river images from the riverseg dataset and 149 flooding images from the pixabay website as for the video data we collected and labeled multiple videos with groundtruth water depth one video recorded the rise and fall of the flooded levels in houston buffalo bayou during hurricane harvey in 2017 a few other videos are from boston harbor where a web camera is mounted at the boston harbor tea party ship museum 2 2 https www bostonteapartyship com boston webcam for these videos we obtained the water level measurements from the boston tide station id 8443970 ma of the national oceanic and atmospheric administration noaa 3 3 https tidesandcurrents noaa gov waterlevels html id 8443970 we also collected multiple flood videos from a creek on the lsu campus we deployed a raspberry pi camera near this creek it was controlled to capture and transmit the flood video one image per minute whenever there is a rain storm a water level gage was installed at this location which provides accurate reference depths finally the water dataset collected in this work includes 2361 images with pixel wise annotations and 6 videos with accurate water level depth groundtruths some example training images are shown in fig 3 we used this dataset for the training and evaluation of the proposed system the dataset will be released in github for comparative studies 2 4 water region segmentation 2 4 1 image segmentation our image based water segmentation framework relies on the popular encoder decoder based architecture the encoder is a neural network that encodes the input image into feature maps then the decoder takes the feature maps to predict the segmentation mask we conducted experiments on three widely used feature encoders resnet 50 he et al 2016 efficientnet b4 tan and le 2019 and xception chollet 2017 and the state of the art decoder network linknet chaurasia and culurciello 2017 all three feature encoders were pre trained on the imagenet dataset through comprehensive experiments the combination of efficientnet b4 and linknet achieves the best performance see section 3 1 hence we selected this combination as our image segmentation module 2 4 2 video segmentation after the camera is deployed it keeps capturing scene images and sending the videos for monitoring and analysis the aforementioned image segmentation model will run on the first frame or any given calibration frame of the video we can assume such a first reference frame is taken in good weather lighting conditions whose segmentation is relatively easy then in the next step a video segmentation model will run to propagate this frame s segmentation to subsequent frames in contrast to the direct frame by frame image segmentation approaches adopted in existing water analysis systems hu et al 2017 maninis et al 2019 with this video segmentation strategy we can utilize the temporal and spatial coherency information across different frames this is crucial to maintaining the segmentation robustness and accuracy under changing severe weather and lighting conditions e g nighttime rainy foggy snowy weather fig 4 a d depicts several examples in which the water appearance significantly varied owing to the changes in weather lighting conditions fig 2 shows the pipeline of the proposed water segmentation model the proposed video water segmentation module is built upon our recently developed architecture afb urr liang et al 2020b afb urr automatically adjusts the learned features to capture the transition of the object s appearance changes this allows the system to more robustly propagate segmentation masks of water or other appearance volatile objects to subsequent frames in long videos we recap the key designs of the video water segmentation here we formulate the segmentation problem as a pixel wise classification task that labels each pixel as objects of interest water or other reference objects or background the video segmentation module consists of two components adaptive feature bank and segmentation network adaptive feature bank we maintain two adaptive feature banks afb f o b j and f b a c k g r o u n d separately to store the water and non water features once we segment a new frame we compare its features with the feature banks then the similarity scores are decoded into the water mask to adapt the object appearance changes in the video we update the feature banks by three operations merging appending and deleting when a new feature is distinct from the existing features we append the new feature to the feature banks in contrast when a new feature f n e w is close to the existing ones in the feature bank we merge them by updating the closest feature descriptor f c l o s e s t from the feature bank by weight averaging 1 f c l o s e s t α f c l o s e s t 1 α f n e w where f c l o s e s t is the updated feature descriptor and we set α 10 in our experiments the feature bank discards obsolete features according to the least frequently used lfu index during the video segmentation process such feature banks allow our segmentation system to effectively memorize the temporally changing appearance of foreground background objects as the feature banks adaptively absorb adjust feature descriptors according to the changing scene segmentation network the segmentation network also follows the encoder decoder architecture the encoder is designed to encode the current frame i t at frame t to its feature map f t for segmentation 2 f t e n c o d e r i t where i t r h w 3 is an rgb image with height h and width w we build an attention module a t t e n oh et al 2019 to compute the similarity between the features of current frame f t and the feature banks f o b j and f b a c k g r o u n d the attention module is used to make the network learn and focus more on the important information rather than non useful features we use this attention module to compute the most relevant features g o b j and g b a c k g r o u n d from the feature banks 3 g o b j a t t e n f t f o b j g b a c k g r o u n d a t t e n f t f b a c k g r o u n d then a decoder network takes the feature maps of the current frame f t and combines it with similarity information calculated from the attention module to estimate the water segmentation mask s t for the current frame in the decoder module we gradually upscale the size of the g o b j and g b a c k g r o u n d by using the information from feature map f t we assign the class label for each pixel as the estimated segmentation mask s t by checking the largest score from object or background maps 4 s t d e c o d e r f t g o b j g b a c k g r o u n d where s t has the same resolution as the original input frame and it will be used to update the feature bank the widely adopted cross entropy loss function is used to train the video segmentation model in fig 4 b c we selected challenging frames where water appearances are affected by different weather and illumination fig 4 a is the first frame of the video we used the image segmentation module to segment it in fig 4 e the segmented water regions are marked in blue our video segmentation module built two feature banks f o b j and f b a c k g r o u n d to memorize the appearances of the object water and background then these two feature banks were used to segment the following frames meanwhile the estimated water masks continuously update the feature banks the feature banks adjust their learned features by analyzing the transitions of new features to adapt to the changes in appearance fig 4 e h show the segmentation results of the boston harbor video we can see that our video segmentation system has the ability to produce accurate and robust segmentation of flood water that has rapidly changing appearances 2 5 reference object segmentation to estimate the inundation depth from the scene we detect and segment reference objects that are submerged in water then we use estimated water region and reference objects to compute the submerging interface we adopted the mask r cnn he et al 2017 network for object segmentation we used the model pretrained on the coco dataset lin et al 2014 which can segment 80 classes of common objects including cars bikes people stop signs and so on we can also use the new video segmentation pipeline introduced in section 2 4 2 to segment reference objects in our experiments and release programs we simply used the mask r cnn pipeline because of two main reasons 1 we noticed that most common reference objects have the relatively stable appearance e g objects such as human beings stop signs and buildings that we incorporated in this work so it is often not necessary to maintain adaptive feature banks for reference objects and 2 unlike water the detection and segmentation models trained on the big coco dataset are robust and effective once the reference objects are segmented we extract a parameterized structural representation for each reference object using a standard template of this object for example in this work we model a stop sign template in 2d as an octagon plate of 8 vertices and a straight line pole of 2 ending vertices fig 11 c and model a person using a parameterized triangular mesh model fig 12 c for other general objects if we can define a standard structural representation as its template we can similarly model that object by fitting it with its template such a parametric template is in general modeled as a graph consisting of vertices with coordinates and edges connecting vertices and it is then used for dimension estimation reference objects are often incomplete with some parts invisible due to submergence or self occlusion to recover the missing part of an object we match the object structure with its template structure on the visible parts once we know the visible above water and invisible underwater parts of the object structure we can compute the water boundary and estimate the water depth in the scene in this work we tested our design on two types of reference objects stop signs and human beings which are often seen in flood scenes our template based estimation scheme can be readily generalized to other reference objects as long as their standard dimensions are known and their templates can be modeled in the following we elaborate on how we model and match the templates of stop signs and people and use them to perform water depth estimation 2 6 water depth level estimation built upon the water and reference object segmentation our water depth estimation algorithm is automatic and it has two steps template matching and submergence ratio calculation fig 6 shows the pipeline of the proposed water depth level estimation system template matching given an estimated reference object m o and a standard template of this type of object m t we match m o and m t by finding a spatial transformation that minimizes vertex displacements 5 t arg min t 1 n i 1 n t v t i v o i p where v t i m t and v o i m o and i iterates the n vertices t is the transformation function that aligns the vertices from the template structure to the object structure the feasible space of t can be defined based on prior knowledge or physical properties of the reference objects or based on appropriate regularization schemes for example stop signs are planar and rigid and are unlikely to undergo free form deformations hence we can restrict the feasible space of t to be rigid transformations when matching a stop sign with its template when using human beings as reference objects they are often modeled with more complex 3d geometric models which can deform more flexibly in a non rigid manner their feasible space should contain more general free form deformations more specifically here we adopt a 3d template human model lin et al 2021 that contains 431 vertices and allows t to be free form deformation and regress it using a neural network then from its feasible space we search for a transformation t that best aligns the reference object with its template with the inverse t we can transform and superimpose the template onto the reference object in the scene this allows us to obtain a standardized reference object m t t m t fig 12 c shows such an example on a reference person where a template mesh in peach color is transformed to superimpose the detected reference object in the image scene for rigid and planar objects such as stop signs the transformations in different images can be related with a homography matrix hartley and zisserman 2003 hence we solve a homography matrix with 8 degrees of freedom by matching the detected octagon stop sign plate and the template octagon similarly the transformed stop sign template can be computed see fig 11 submergence ratio calculation template matching allows the system to deform the template to superimpose the reference object in the scene the water segmentation mask separates this template its vertices nodes into two parts the emergent part v a and the submerged part v s based on the assumption that the water level is locally horizontal our system computes the submergence ratio on the template using a vertical length ratio of v s over v s v a we use h to represent the heights on the vertical axis and the submergence ratio r w a t e r is computed by 6 r w a t e r h v s h v s h v a to estimate actual water depth d w a t e r from the submergence ratio r w a t e r this ratio should multiply the physical size of the reference object l o b j then add the vertical distance from the bottom of the reference object to the ground b 7 d w a t e r r w a t e r l o b j b where l o b j is the vertical dimension of the reference object in practice we often set l o b j using the dimension of the template specifically the standard height of this type of reference object estimation without reference objects when no reference object is detected in the scene our system allows the user to pick an object as a reference and give an estimated dimension the reference object or region is supposed to be vertically above the water then our system will automatically track the reference object in all frames and compute the vertical distance from the reference object to the ground the estimated water mask separates the vertical line into two parts the line segment that is emergent visible denoted as v a and submerged by water invisible denoted as v s similarly we compute the submergence ratio r by eq 6 and convert it to the estimated water depth by eq 7 our system also supports choosing multiple objects as reference objects the estimated water level is the average of each reference object 3 results we evaluated each sub component of our water estimation system and tested the entire pipeline on videos captured in the field our code and dataset are available at https github com xmlyqing00 v floodnet 3 1 evaluations on image segmentation to select the best image segmentation model we conducted experiments on three recent encoder decoder based architectures resnet50 linknet he et al 2016 chaurasia and culurciello 2017 efficientnetb4 linknet yang et al 2018 and xception linknet chollet 2017 we trained these models using our waterdataset where images are randomly divided into 80 training 13 validation and 7 testing respectively we provided the training details in section 4 1 after each epoch the models are evaluated on the validation images the experiment results are summarized in table 1 the parameter size measures the number of learnable parameters in the network and the unit is million m sample segmentation results are shown in fig 5 because of its best miou score 0 7619 on water segmentation and the smallest model size parameter size 18m we picked the combination of efficientnet b4 and linknet as our image segmentation model comparison with existing water segmentation systems we compared our image segmentation module with the existing water segmentation papers geetha et al 2017 chaudhary et al 2019 pally and samadi 2022 we directly used our trained model to segment the images from their paper without any additional training or tuning figs 7 8 and 9 show the water segmentation result comparisons segmentation from these existing papers tends to be incomplete or produce mis classified foreground background in contrast our water segmentation model can more reliably and accurately identify water regions from images our model was never trained on these images which demonstrates the generalization of our segmentation model on flood images from new scenes 3 2 evaluations on video segmentation a key advantage of our proposed system is that it can leverage the temporal coherency in the video to further enhance the reliability of water segmentation in videos this is critical when it is needed to monitor and analyze scenes during both day and night time and or under both clear and inclement weather conditions when our system takes video as input it first uses the image segmentation model to segment water flood and reference objects in the first frame then our video segmentation model propagates the segmentation of water and other reference objects to subsequent frames utilizing not only texture appearance but also spatial temporal coherence in the video fig 4 shows several example frames from a video recorded at the boston harbor this video spanning several days covers different weather lighting conditions including a clear b snowy c dark and d sunny weather conditions on a relatively clear day our image segmentation can detect the precise water boundary e g segmentation for the first frame but as the lighting and weather conditions change the water and background appearances can change significantly in many scenarios e g nighttime inclement weather windy and rainy days etc the image based water segmentation accuracy drops significantly because not only it is difficult to include all the possible scenarios in training datasets but also it is hard to memorize all possible water appearances caused by ripples reflections etc by using spatial temporal consistency exploited from the video our video segmentation technique can greatly improve water segmentation robustness and accuracy fig 10 compares the segmentation results of auqanet erfani et al 2022 and our models in some scenarios column a shows two frames one taken on a snowy and foggy day top and one in the nighttime bottom the reflections and lighting make the water looks drastically different from the daytime when the image segmentation model is directly applied the results the second column are less desirable see columns b and c but when the whole video sequence is considered our video segmentation model gradually and adaptively adjusts the water features and eventually produce much better segmentation results column d in both scenarios 3 3 inundation depth estimation we conducted multiple field experiments to evaluate our flood water and inundation depth estimation pipeline in the first field experiment we deployed a standard stop sign in a shallow lake on the louisiana state university lsu campus in baton rouge louisiana usa next we further tested our system with multiple monitoring videos of actual flooding or water level changes recorded during storms or severe weather conditions these videos were from 1 the lsu campus creek 2 boston harbor and 3 houston buffalo bayou the two videos at the creek on the lsu campus were captured during two heavy storms in the summer of 2020 the three videos at the boston harbor were recorded several days during winter and summer with different weather lighting conditions the video at the houston buffalo bayou recorded the water level changes during hurricane harvey 2017 field experiment 1 lsu lake fig 11 a shows a frame in a video a student stands in the water holding a standard stop sign a ruler is attached to this stop sign to measure the actual water depth at this location the water depth is 0 343 m see fig 11 f the automatic water region segmentation from our system is shown in the blue mask in fig 11 b we show that our system can use either the stop sign or the person to estimate the water depth when using the stop sign as the reference the system matches a template model fig 11 c to the detected object in the image then the visible region fig 11 d superimposed with the water object interface infers the submergence ratio r w a t e r on the stop sign template fig 11 e following u s department of transportation 2003 s report a standard stop sign on a conventional road in the usa has a 0 79 0 79 m plate and a 2 159 m long pole where 0 75 m 30 in across opposite flats of the red octagon with a 0 02 m 3 4 in white border with eqs 6 and 7 and this standard dimension l o b j 2 159 m and b 0 our system estimated the average water depth at this location to be 0 359 m hence with the stop sign as a reference the prediction error is 0 016 m 4 7 when using the person as the reference the 3d template mesh is deformed to fit the detected person in the image fig 12 c similarly with the superimposition of template mesh and the water interface the above water compared to submerged parts of the human body are computed visualized in green and red points respectively in fig 12 d the average submergence ratio on the person is then calculated to be 0 1882 fig 12 e finally using the average height of the adult male 1 754 m following fryar et al 2018 the system estimated the average water depth at this location to be 0 1882 1 754 0 330 m by eq 7 where l o b j 1 754 and b 0 because the adult male stands on the ground hence using the person as a reference the prediction error is 0 013 m 3 8 in our current system we use the standard dimension of the reference objects as a default dimension if the exact dimension of the reference object is known different we can simply adjust the parameters in the calculation to obtain a more accurate estimation field experiment 2 lsu campus creek we deployed a raspberry pi camera near the lsu creek to continuously capture the water depth change of the creek we used one video that captured a rain storm on june 24 2020 as an example to demonstrate our experiment fig 13 a shows one frame of this video fig 13 b illustrates the segmented water region in the blue mask because there is no known reference object detected in this scene the systems asked the user to pick a landmark and the program used it as the reference to calibrate the scene in this experiment the top of the gauge was selected as the reference marked in the green box at the top of the gage in fig 13 b a simple vertical line from the landmark to the ground water interface is used to estimate water depth the system partitions this reference vertical line into the above water and under water parts separated by the water interface the submergence ratio r w a t e r is the ratio between the dimension of underwater part and that of the entire line we convert the submergence ratio to water depth by eq 7 when the camera was first deployed its position orientation and focal lengths were calibrated and the object size l o b j and b were calculated and fixed after this step the system can automatically estimate the water depth over time the results are plotted in fig 13 c the average absolute error of the estimated water depth in this experiment is 0 018 m and its standard deviation is 0 012 m see table 2 field experiment 3 boston harbor we tested our system on several long videos to demonstrate its robustness in varying weather and illumination conditions in our collected waterdataset there are three long videos recorded on the boston harbor the camera is mounted on the tea party museum ship and we downloaded the video one frame per ten minutes for multiple days fig 14 a shows one frame of the recorded video from 2019 01 18 to 2019 01 23 when no salient reference objects are detected in the video our system allows users to adopt one of two interactive mechanisms for calibration 1 user can select four points to represent the horizontal axis and vertical axes then the system can use them to estimate a perspective transformation to calibrate the scene and 2 user can pick an arbitrary region in the scene as the reference in this experiment adopting the second mechanism a region on the background building was selected and tracked then pixel scale in the videos can be converted into physical units by fitting the parameters l o b j and b in eq 7 tracking water levels in these videos is highly challenging due to two main reasons 1 different weather and different day night illumination make the water appearance change continuously and significantly and 2 the water regions are often occluded by other moving objects e g ships people existing image and video segmentation systems cannot effectively detect and segment water regions in such situations our results are reported in table 2 where both the segmentation and water level estimation are reliable and produce small errors more visualization results are attached in the supplementary video on our github page field experiment 4 houston buffalo bayou hurricane harvey 2017 made landfall along the texas coast on 25 august 2017 as a category 4 hurricane this major storm caused catastrophic flooding of the densely populated regions of houston and beaumont leading to flooding of major interstate highways such as i 10 and i 45 a high water mark on the northern bank of buffalo bayou revealed the maximum flood water elevation of 10 3 m 33 7 ft navd88 at the study site details about the flooding can be found in jafari et al 2021 at milam street the rising floodwater reached an elevation of 8 3 m 27 1 ft on august 27 2017 a water level of 11 3 m 33 85 ft above the bottom of the bayou harris country flood warning system 2017 however the stream gage at milam street only collected data until 02 44 on august 27 this gage failure demonstrates the need for multiple methods to construct and verify flood hydrographs during hurricane harvey a time lapse camera was placed on the second floor of the bayou place offices building on capitol street near milam street and overlooked buffalo bayou memorial drive overpass interstate 45 overpass and the houston aquarium the camera recorded a video of the rise and fall of flood levels in buffalo bayou from august 25 to 31 our system took the video as input to estimate the water level and then created the hydrograph our system used two bridge piers adjacent to buffalo bayou as reference objects to estimate the changes in the water level over time we used the same conversion function eq 7 to convert the estimated submergence ratio to the water levels the elevation of the bottom of the pier was found to be 0 83 m 2 72 ft navd88 and the top 18 11 m 59 41 ft navd88 after calibration we have l o b j 18 11 0 83 17 28 and b 0 83 the system can then estimate the water level following eq 7 with the water level estimated by our system we compared it with the milam street gauge the closest measure information available near buffalo bayou and the results computed from our previous model jafari et al 2021 the comparison is plotted in fig 15 c in the morning of august 26 the milam stream gauge shows an initial rise of the hydrograph at midnight of august 27 the milam street gauge failed therefore the blue curve stops our proposed model successfully captures the water rise on the morning of august 26 in contrast the previous model jafari et al 2021 could not catch this rise effectively after the milam street gauge failed on august 27 our proposed model can still provide information on the flood specifically according to u s geological survey 2017 usgs a high water mark 4 4 stn site no txhar23198 in usgs recorded the peak flood level of approximately 10 3 m occurred in the early afternoon of august 27 the hydrograph reconstructed by our v floodnet reaches the peak flood elevation at a similar time by contrast the previous model jafari et al 2021 estimated the maximum water level over 14 m which is significantly higher than the observed high water mark reported by usgs the main reason that the previous model failed to produce accurate estimation is that it uses an image based segmentation model to identify the water boundary under severe and changing weather and illumination conditions image based models cannot reliably stably track the water region our proposed model uses a video based model which leverages the spatial temporal coherence in the video to achieve a more robust estimation it is important to note that during hurricane harvey water level fluctuations mostly occur during the night time nevertheless fig 15 shows our model is able to reliably capture them to build the flood hydrograph table 2 summarizes our estimation errors in field experiments 2 4 by comparing our estimations and the groundtruth depths collected from the nearest gages the estimation errors mainly come from two factors 1 the accuracy of water and reference object segmentation and 2 the accuracy of reference template modeling and matching for example in the boston harbor scenes the reference object building and water region are very far away from the camera a single pixel error in both water segmentation and reference object localization could lead to a relatively bigger estimation error therefore in general our estimation performs better when the camera is close to the flood scene and reference objects severe weather or bad illumination conditions poses extra challenges for example during hurricane harvey water level fluctuations mostly occur in the nighttime the water was hardly visible and its segmentation is very challenging this leads to bigger absolute estimation errors in the houston buffalo bayou scene 4 discussion 4 1 implementation details for image segmentation we use efficientnet b4 and linknet yang et al 2018 chaurasia and culurciello 2017 as the segmentation architecture the model was then trained using images from our waterdataset all the training images were resized to the network s input resolution of 416 416 and fed through the network in a batch size of 4 the learning rate was initialized to 10 4 and decreased to 1e 5 halfway through training the widely used metric mean intersection over union miou liang et al 2020a was adopted to evaluate the segmentation quality all these three segmentation networks were trained for 150 epochs using dice loss sudre et al 2017 for video segmentation we used afb urr liang et al 2020b as the architecture it was pre trained on the youtube vos xu et al 2018 dataset which contains 5k videos of the general objects we then fine tuned it on our waterdataset the initial learning rate was 1 e 5 and it was halved every 25 epochs the total number of training epochs was 100 we chose the adamw loshchilov and hutter 2017 as the back propagation optimizer the input resolution of the video is 480 854 note that in both image and video based flood segmentation we resize the input to meet the requirement of the segmentation module then we resize the estimated water mask back to the original resolution since the resolution range of most video cameras is from 480p to 1080p our segmentation modules balance the speed and segmentation quality which are insensitive to the input resolution for the reference object segmentation we used the mask rcnn to segment the stopsign and people he et al 2017 we used meshtransformer to complete the 3d mesh of the detected person lin et al 2021 we used csrt tracker to track the user selected bounding box in the input video lukezic et al 2017 we directly used their model and pretrained weights in the experiments 4 2 limitations and future work a key challenge in image video segmentation is the processing of nighttime images videos when the scene becomes dark and dim specular highlights and reflections could significantly affect water region identification and segmentation although with the help of spatio temporal coherency and adaptive feature updating our video segmentation model can identify the main water regions their boundaries are less accurately tracked such less accurate contours could consequently affect water level estimation in future work we will incorporate video based dark image enhancing techniques to tackle this issue the segmentation of the first frame affects the accuracy of the entire video segmentation to make this first frame segmentation more reliable the training dataset should contain flood and water images in various environments with different weather illumination and reflection conditions our waterdataset currently contains several thousand water images but there are only about 150 flood images we will collect and label more flood images especially those whose flood region is difficult to detect or segment this will improve the accuracy and robustness of our image segmentation model 5 conclusions we proposed a comprehensive video processing and analysis system that can detect and segment water and surrounding reference objects and use them to estimate the water level or inundation depth unlike existing segmentation techniques in the computer vision literature that were developed to handle common daily objects our system is capable of adaptively capturing the appearance change of water in the scene under dynamic noisy and weather lighting conditions hence it can produce more accurate and precise water segmentation which is crucial to flood height or inundation depth estimation we also developed a depth estimation algorithm using reference objects and template matching allowing the system to effectively calculate the inundation depth this pipeline can run automatically or if necessary interactively to support convenient water depth estimation and monitoring in our experiments we demonstrated the effectiveness and robustness of the proposed system in extensive applications validated through multiple field experiments our system can provide effective water depth estimations in various video scenes besides the methodology we also released a set of water related image and video data with annotations these data can benefit the training of other deep learning based water analysis systems we also made our code and models open sourced they can conveniently be used as enabling tools or used for comparative studies code and data availability the source code is open source for academic research it is available at https github com xmlyqing00 v floodnet detailed installation and usage instructions are included in the readme file the proposed waterdataset data and pretrained model weights are also available for downloading credit authorship contribution statement yongqing liang methodology software writing visualization xin li methodology resources writing supervision funding acquisition brian tsai software data curation writing visualization qin chen conceptualization validation resources writing project administration funding acquisition navid jafari validation investigation resources writing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we would like to thank thomas rinaudo claire white amina meselhe and dominion ajayi who helped label the water images and videos we thank alex wu who developed parts of the people mesh registration model we thank eli barbin who conducted the experiments in the lsu lake this material is based upon work supported by the national science foundation usa grant 1760582 2139882 2139883 and 1946231 the authors would like to thank louisiana sea grant undergraduate research opportunities program urop usa t baker smith inc louisiana board of regents industrial ties research program leqsf 2018 21 rd b 03 and the northeastern university global resilience institute usa for supporting this research we appreciate permission by mr teddy vandenberg to use his time lapse video of buffalo bayou during hurricane harvey any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation t baker smith louisiana board of regents and louisiana sea grant appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2022 105586 appendix a supplementary data the following is the supplementary material related to this article mmc s1 supplementary material for v floodnet 
25461,this paper presents a new open source and physically based model for spatial prediction of rainfall induced shallow landslides sprin sl through the quantum gis qgis software sprin sl consists of a set of shell scripts developed using the python language that can be directly run from the qgis processing toolbox through a user friendly graphical interface the tool implements the infinite slope method by incorporating the topog and the green ampt models to consider groundwater flow and transient rainfall infiltration respectively furthermore dem pre processing procedures to extract reliable terrain morphometric features a new statistical method for modelling soil depth and a procedure for predictive accuracy evaluation were implemented by using a 1 m resolution dem the developed model was tested in a small coastal catchment of cinque terre liguria italy providing accurate outcomes and proving to be an easy to use tool for landslide susceptibility zoning which can have useful implications on the risk reduction keywords cinque terre dem resolution open source physically based model qgis shallow landslides data availability data will be made available on request 1 introduction shallow landslides sls are the most common mass movements occurring in hilly and mountainous environments worldwide they typically affect steep slopes mantled by a few meters thick soil debris covers and are more frequently triggered by rainstorms crosta and dal negro 2003 harp et al 2009 park et al 2013a generally rainfall induced sls initiate rapidly through the detachment and consequent sliding downslope of small volumes of materials hungr et al 2014 during extremely intense rainstorms countless sls can be triggered over large areas in very short time intervals fuchu et al 1999 guzzetti et al 2004 guthrie and evans 2004 crozier 2005 larsen 2008 often after their initiation sls can evolve into or concur to generate rapid and destructive flow like phenomena e g debris and mud flows that can travel long distances guadagno et al 2005 cascini et al 2008 wooten et al 2008 depending on the intensity associated with the first failure and post triggering mechanisms sls can give rise to severe risk scenarios hilker et al 2009 pradhan and lee 2010 papathoma köhle et al 2015 galve et al 2016 narasimhan et al 2016 this is the reason why the prediction of sls prone areas has deserved increasingly attention in recent decades this goal can be addressed through approaches that involve the definition of empirical relationships between past landslide occurrences and rainfall features guzzetti et al 2008 segoni et al 2018 galanti et al 2018 or based on the application of statistical van westen et al 2008 corominas et al 2014 bordoni et al 2021 and machine learning zêzere et al 2017 di napoli et al 2020 2021 guo et al 2021 models aimed at investigating the combination of causal factors most influencing sls initiation other widely used approaches defined as physically based focus on the simulation of the triggering processes which primarily depend on the reduction of shear strength due to soil saturation in response to rainfall bogaard and greco 2016 the earliest modelling frameworks relied on the combination of an infinite slope stability analysis with simplified hydrological models linking rainfall and subsurface groundwater flow under the assumption of steady state conditions montgomery and dietrich 1994 wu and sidle 1995 pack et al 1998 borga et al 2002 subsequently this kind of approaches has been further developed by implementing more complex theoretical models that also account for transient hydrological conditions iverson 2000 baum et al 2002 2010 godt et al 2008 an et al 2016 kim et al 2014 physically based models have been extensively implemented in grid based spatial frameworks by exploiting the capability of geographical information systems gis to efficiently manage and process spatial data and several practical applications demonstrated their reliability in forecasting sls over large areas salciarini et al 2006 montrasio and valentino 2008 arnone et al 2011 mergili et al 2014 montrasio et al 2014 in the recent years the potential of landslide prediction modelling has also moved forward thanks to the combined contribution of open source gis such as quantum gis qgis and grass software packages bragagnolo et al 2020 guo et al 2022 titti et al 2022 and freely accessible geospatial and environmental data sources an et al 2018 gunarathna et al 2018 wadhawan et al 2020 however sls modelling at the basin scale can be still time consuming since theoretical models incorporate a large amount of geometrical geotechnical hydrological and hydraulic variables not always easily available or measurable both in the field and in laboratory and often characterized by significant spatial variability haneberg 2004 gorsevski et al 2006 raia et al 2014 this necessarily produces uncertainty in input data estimation which can represent an obstacle to the definition of landslide early warning procedures marin et al 2021 nevertheless the scientific community is making considerable efforts in overcoming the limitations correlated to the reliable determination of soil properties over wide areas park et al 2013b lee and park 2016 arnone et al 2016a medina et al 2021 it is known from literature that the location of sl source areas can be strongly correlated with terrain morphology montgomery and dietrich 1994 montgomery et al 1998 which can directly control stability through the effect of slope gradient or indirectly by influencing some other important landslide predisposing factors such as hydrological processes operating at the slope scale dietrich et al 1986 and spatial distribution of soil thickness dietrich et al 1995 recently digital elevation models dems have become increasingly available and their quality and accuracy have steadily increased also because of advanced remote sensing techniques such as lidar roering et al 2013 owing to their ability to capture small scale details high resolution dems allow to extract accurate derivative morphological attributes e g slope gradient virtually this should entail important advantages in sls modelling especially in understanding the role of instability factors controlled by topography however this is not as straightforward as would expected since it was found that the performance of process based models does not improve automatically by reducing the grid size claessens et al 2005 fuchs et al 2014 penna et al 2014 arnone et al 2021 moreover in some cases the use of very fine resolutions cannot be helpful if other factors such as bedrock morphology or soil piping drive the triggering mechanisms of sls freer et al 2002 crosta et al 2003 tarolli and tarboton 2006 penna et al 2014 this paper presents a new deterministic physically based model for the spatial prediction of rainfall induced shallow landslides sprin sl at the basin scale sprin sl consists of a set of shell scripts developed using the python language that can be directly run from the qgis processing toolbox each implemented script is supported by a user friendly graphical interface gui by integrating the infinite slope method with two simple and well known hydrological models namely the topog model o loughlin 1986 and the green ampt s model 1911 sprin sl allows to directly predict unstable slope areas in response to intense rainstorms moreover some dem pre processing procedures both new and already proposed in the literature to extract reliable terrain morphometric features along with a new method for modelling soil depth and a module to evaluate the simulation accuracy were implemented the developed tool was tested in a small coastal catchment located in eastern liguria region north western italy to describe both its functioning and performance the test site is characterized by a complex terrain morphology which was strongly modified in the past centuries by humans through the construction of agricultural terraces brandolini 2017 and highly prone to be affected by rainfall induced sls cevasco et al 2012 the main goal of this research is to provide an open source and free tool that can be useful for stakeholders and practitioners involved in landslide risk prevention 2 methods 2 1 infinite slope stability model the model presented is based on the geometrical slope sketch depicted in fig 1 a a permeable homogeneous and isotropic soil layer of modest vertical thickness z overlies a rock substratum which is supposed rigid and virtually impermeable from the hydraulic point of view the failure surface is presumed to be i coincident with the soil bedrock interface ii parallel to the ground surface and inclined by an angle β with respect to the horizontal and iii characterized by a length far greater than the soil depth i e infinite slope length assumption according to the limit equilibrium theory and the infinite slope method skempton and delory 1957 under these key simplifying hypotheses the edge effects can be neglected and the overall slope stability can be computed by analysing the equilibrium conditions of an elementary slice of slope with vertical thickness z m width b m and weight w kn fig 1a two mechanisms causing the increase in moisture content within the soil element up to saturation are assumed fig 1a the rising of the groundwater table at a depth z s m above the impervious bedrock and the formation of an advancing wetting front of thickness z i m measured from the ground surface in the former case a steady lateral seepage parallel to the slope related to the effective seasonal infiltration of rainwater is assumed in the second case a transient vertical infiltration of rainwater from the slope surface that develops during a rainfall event is supposed the lateral flow is indicative of the long term response of water table to antecedent rainfall while the vertical flow accounts for the short term effect of infiltration during a specific rainstorm iverson 2000 godt et al 2008 the simple one dimensional model of the infinite slope allows to quantify slope stability through the evaluation of the safety factor f s defined as the ratio between resisting and driving stresses acting at the base of the soil element namely the sliding plane unstable conditions occur for f s lower than 1 value at which the shear stress required for equilibrium overcomes the soil shear strength the implemented equation of f s takes inspiration from the skempton and delory 1957 general formulation and is given by eq 1 f s c δ s γ s z sin β cos β γ s m t γ w γ s tan φ tan β where β is the slope angle γ s kn m3 is the soil saturated unit weight γ w kn m3 is the water unit weight c kpa is the effective soil cohesion φ is the soil friction angle δs kpa is the shear strength increment due to plant roots and m t is the total wetness index a dimensionless parameter indicating the thickness of the saturated part of the soil element along the vertical under the validity of the mohr coulomb failure criteria c and φ describe the soil shear strength and are expressed in terms of effective stresses whereas the term c δs can be defined as total cohesion c the mechanical effect of vegetation was incorporated into eq 1 through the theoretical model developed by wu et al 1979 often used in spatially distributed slope stability models wu and sidle 1995 borga et al 2002 the additional resistance provided by root reinforcement is calculated by assuming that roots are flexible elastic and directed perpendicular to the slip surface and is expressed as follows eq 2 δ s k k t r r r where t r kpa is the average root tensile strength per unit area r r is the root area ratio rar k and k are factors considering that roots are randomly orientated with respect to the slip surface wu et al 1979 and that do not fail simultaneously greenwood et al 2004 respectively root tensile strength is computed as αd λ where d mm is the root diameter whereas α and λ are empirical constants depending on vegetation species whereas rar quantifies the root density into the soil and it can be calculated as a r a where a r m2 is the total section occupied by roots and a m2 is the soil reference area bischetti et al 2005 tosi 2007 the total wetness index is a fractional quantity varying between 0 dry conditions and 1 full saturation in the proposed model m t was split into two contributions that can be computed as follows eq 3 m s z s z eq 4 m i z i z where m s and m i define the relative wetness connected to a steady groundwater lateral flow parallel to the slope fig 1b and to transient vertical rainfall infiltration fig 1c respectively for each spatial unit used to discretize the basin therefore m t is defined as follows eq 5 m t m s m i 2 2 steady state groundwater flow model the groundwater flow was simulated by implementing the topog model o loughlin 1986 this approach based on topographic and hydrological parameters allows to compute the saturation conditions at each spatial unit in response to constant rainfall intensity montgomery and dietrich 1994 under the steady state flow assumption incorporated in the model along with the condition of mass conservation the local groundwater discharge q m3 at each soil element is assumed as balanced by the steady state recharge given by the effective rainwater infiltration i e eq 6 q i e a where a is the upslope contributing area uca m2 fig 1b in this context the parameter i e represents the amount of meteoric precipitation diminished by runoff and evapotranspiration that seasonally infiltrates into the soil medina et al 2021 according to the groundwater flow process described by the darcy s law under a hydraulic gradient i connected to the hydraulic head difference across the soil element which is computed as senβ the groundwater discharge can be derived as follows eq 7 q z s z b t sin β where b m is the width of the soil element and t m2s 1 is the soil transmissivity for simplicity the saturated hydraulic conductivity k s ms 1 can be considered uniform throughout the soil thickness hence it can be written that t k s z cosβ therefore by combining eqs 6 and 7 m s can be computed as eq 8 m s i e a b k s z cos β sin β which defines the relative wetness resulting from the steady groundwater flow parallel to the slope fig 1b 2 3 transient rainwater infiltration model to simulate rainwater infiltration into each soil element the green ampt s 1911 model was adopted which is based on a simplified analytical solution of the darcy s law for one dimensional vertical flow this model hypothesizes that rainwater infiltration takes place through the formation of a sharp wetting front i e piston type shape fig 1c moreover during the downward advancement of the wetting front the soil element is assumed to show a uniform moisture content profile in both space and time namely the soil below the wetting front is in unsaturated conditions with steady matrix suction head whereas the portion of soil above the wetting front is fully saturated with constant k s fig 1c since this model is applicable assuming ponding conditions at the ground surface some modifications of the original version have been developed kale and sahoo 2011 in this study the two stages model for transient vertical infiltration under steady rainfall conditions proposed by mein and larson 1973 was implemented this solution allows to define separately the quantity of rainwater infiltrated up to and after ponding initiation depending on the relationship between rainfall intensity i and infiltration rate f two possible scenarios can be outlined i i f rainwater totally infiltrates into the soil during a time interval t thus no ponding occurs and the cumulative infiltration f t is equal to cumulative rainfall computed as the product it ii i f only a fraction of rainfall can infiltrate into the soil hence surface ponding and runoff take place f t can be estimated using the following relationship eq 9 f t f p k s t t p ψ δ θ ln f t ψ δ θ f p ψ δ θ where ψ mm is the matrix suction head at the wetting front δθ is the soil moisture deficit t p h is the ponding time while f p mm is the cumulative infiltration at the begin of surface ponding which is calculated as eq 10 f p k s ψ δ θ i k s on the basis of the green ampt s model the wetting front depth can be estimated as follows eq 11 z i f t δ θ hence by combining eqs 4 and 11 the relative wetness due to transient rainfall infiltration can be expressed as eq 12 m i f t δ θ z where δθ can be computed as the difference between the effective porosity φ e and the initial water content θ i i e δθ φ e θ i 2 4 dem pre processing and geo morphometric features derivation in the application of spatially distributed models for sls prediction the safety factor is calculated for each spatial unit used to discretize the computational area the raster structure of dems makes it possible to represent and process the whole range of input variables appearing in slope stability calculations in addition by means of specific gis tools of digital terrain analysis dems can be used to extract secondary topographic attributes wilson and gallant 2000 in the sprin sl package the following secondary geo morphometric attributes are derived from dem slope gradient β uca a and topographic position index tpi β appears in all terms of safety factor equation eq 1 while a allows for steady state groundwater flow modelling eq 8 the former is computed based on the method described by horn 1981 the latter represents the area that can potentially produce surficial drainage to each cell and is derived using the multiple flow direction algorithm developed by quinn et al 1991 however dems produced through automatic procedures applied on remotely sensed data often contain defects that can negatively affect the outcomes of both slope stability and hydrological models santini et al 2009 therefore some specific pre processing procedures aimed at improving the quality of input dems are generally required before running grid based slope stability models in this section the dem pre processing methods and the algorithms to derive secondary geo morphometric attributes incorporated in the sprin sl tool are described in detail according to the infinite slope length hypothesis each cell is implicitly considered sufficiently long with respect to the slip surface depth however some studies have shown that the validity of this assumption can be affected by dem resolution specifically the use of high resolution dems from few meters to lower than 1m can make more likely the influence of edge effects which are neglected through the application of the one dimensional infinite slope stability analysis milledge et al 2012 moreover algorithms used to derive slope gradient may produce steeper terrain conditions because of more influenced by variations in topography at the local scale in case of complex or rugged morphologies these aspects may lead to overestimation of unstable areas causing errors i e false positive that reduce the accuracy of landslide susceptibility zoning maps godt et al 2008 arnone et al 2016b fig 2 a and b due to the importance of slope angle in the cell by cell application of the infinite slope method a specific dem pre processing procedure which is especially devoted to high resolution grids has been introduced this procedure involves a focal function i e low pass filter consisting of a moving mean assigning to each output cell of the resultant grid the average value of the input cells included within a circular kernel or window of radius r m defined by the user fig 2c the resultant grid allows to smooth out the small scale morphological features while keeping an adequate information accuracy connected to the use of very fine resolutions fig 2c the simulation of sub surface water flow and the prediction of saturation condition at any locations require the use of a hydrologically correct dem therefore in order to remove artefacts such as spikes holes and steps that commonly affect automatically generated dems and to calculate water flow directions the fill sinks algorithm developed by wang and liu 2006 has been implemented moreover the use of high resolution dems such as those derived from lidar data can give rise to anomalous or ambiguous flows connected to small scale morphological effects gillin et al 2015 riihimäki et al 2021 to smooth these effects a pre filtering tool i e uca pre filter has been introduced to be applied to the hydrologically correct dem before running the flow routing algorithm this filter involves a focal function employing a moving mean through a circular kernel with a fixed radius of 5 m tpi is a terrain attribute obtained from the comparison between the elevation of the single cell and the mean elevation of a group of cells in the neighbourhood of that cell weiss 2001 a positive tpi value indicates that the cell is at higher elevation than those included within the kernel whereas a negative value represents a cell located at lower elevation than its surroundings since tpi is inherently dependent on scale weiss 2001 in the proposed model such an index is calculated cell by cell by implementing an algorithm that defines the neighbourhood around the cell through a circular kernel with radius defined by the user as explained in the following paragraph this index was introduced because the topographic position can be correlated with the thickness of slope deposits 2 5 soil thicknesses modelling soil thickness is a fundamental input data of physically based modelling of rainfall induced sls salciarini et al 2006 godt et al 2008 even though its spatial pattern can be extremely complex depending on the geomorphological context tesfa et al 2009 catani et al 2010 segoni et al 2012 tufano et al 2021 generally the spatial distribution of soil depth can be predicted by using empirical catani et al 2010 del soldato et al 2018 physically based casadei et al 2003 and statistical approaches tesfa et al 2009 to predict the spatial distribution of soil vertical thickness a polynomial regression method with response surface analysis has been selected edwards 2007 this statistical procedure was developed to examine the degree of correlation between several explanatory variables and one response variable by plotting the outcomes of polynomial regression analysis in a three dimensional space for example in the case of a quadratic or second order polynomial the model can be described through the following general equation eq 13 y α 0 i 1 n α i x i i 1 n α i i x i 2 i 1 j 1 n α i j x i x j ε where y is the outcome variable x i and x j are independent variables α 0 α i α ii and α ij are regression coefficients computed based on input data n is the number of independent variables while ε is the random error component therefore the outcome variable is predicted both from each of the predictor variables and from their interactions i e fourth term in eq 13 the accuracy of the model can be evaluated by computing the correlation coefficient r2 in this work vertical soil thicknesses spatial distribution is computed by considering two predictor morphological variables namely slope angle β and tpi along with soil depth information obtained from in situ measurements β is an important controlling factor of soil erosion accumulation processes dietrich et al 1995 salciarini et al 2006 whereas tpi provides information on the position of a terrain unit with respect to specific slope landforms such as escarpments valleys or open slopes in a grid based context high positive tpi values indicate that a cell is located near zones of topographic divergence where soil is more easily eroded while high negative values reveal that a cell is positioned in the proximity of zones marked by topographic convergence hence mainly characterized by soil accumulation weiss 2001 2 6 model predictive accuracy evaluation to test the model performance the contingency tables technique ebert et al 2007 was implemented according to this approach it is defined as hit h a terrain unit predicted as unstable that correctly matches with an occurred landslide conversely an output cell is classified as miss m when an actual unstable zone is not detected by the model eventually a target cell is classified as false alarm f when a stable terrain unit is not correctly captured by the model thus leading to slope instability overestimation starting from these quantities four accuracy indices table 1 can be defined and analysed in conjunction to verify how the model performs in zoning landslide susceptibility liao et al 2011 probability of detection pod reflects the agreement between the model outputs and mapped landslides ranging from 0 poor agreement to 1 perfect agreement false alarm ratio far defines the percentage of false alarms with respect to the total unstable areas predicted by the model far ranges between 0 and 1 and the closer it is to 0 the more correctly the observed landslides are discriminated by the model on the contrary true prediction ratio tpr expresses the fraction of hits with respect to the total amount of terrain units predicted as unstable a tpr value approaching 1 means higher model performance lastly the critical successful index csi considers simultaneously misses and false alarms csi approaching 1 denotes that the total count of false alarms and misses is minimized thus captured landslides are getting closer and closer to those observed the ability of the model to effectively discriminate sls prone areas can be further tested by computing accuracy metrics that account for incremental buffer zones around the inventoried landslide polygons so that the spatial distribution of pixels modelled as unstable is evaluated over a larger area surrounding them moreover this procedure allows to minimize the influence of graphical errors inaccuracies of the landslide inventory guzzetti et al 2012 that can affect landslide susceptibility modelling and mapping arnone et al 2016b 2 7 computational workflow of sprin sl the computational framework of the sprin sl tool can be summed up into three main steps fig 3 i morphometric variables generation ii hydro geotechnical spatial variables generation and iii slope stability analysis and validation the current version of sprin sl runs on qgis 3 22 5 ltr where all equations and algorithms described above were implemented through the python application programming interface api pyqgis version 3 9 by creating a set of shell scripts for automating the process the scripts consist of chained command lines that allow the user to automatically launch algorithms both already integrated into qgis and new ones once the sprin sl package is downloaded the folder sprin sl scripts should be copied into the installation directory of qgis scripts before it can be run then each processing script can be directly launched from the qgis s processing toolbox by selecting the scripts tool icon in which the subfolder called sprin sl appears moreover since some machine learning algorithms were implemented see 2 7 2 subsection for the routines to work the scikit learn library for the python code should be also installed further details on the whole installation instruction are contained in the step by step software documentation each script exploits a simple graphical user interface gui consisting of a tab where the user must insert input data set or modify computational options in specific fields name and save output files and run commands input and output data can be either continuous i e raster structure or categorical i e vector structure along with constant values such as some hydrological and geotechnical parameters 2 7 1 morphometric variables generation the first step of the computational framework consists of three scripts fig 3 the first one allows to create the hydrologically correct dem and the hydrological basins vector map of the computational area in the upper left corner of the tab the user is requested to upload the original dem and to specify the resolution of the output raster fig 4 a once this is done the script sequentially runs the warp reprojection utility provided by the geospatial data abstraction library gdal which reprojects the input dem into the coordinate reference system crs of the current qgis project along with mosaicking it and subsequently the saga gis tool fill sinks of the module terrain analysis pre processing the second script is aimed at clipping the hydrologically correct dem to the extent of the target basin the user is requested to i upload the two output layers generated through the previous script ii select the target basin as vector mask directly in the map canvas by activating the hydrological basins layer within the table of content toc and iii set the option use only selected features before running the command line fig 4b finally by launching the third script the thematic raster maps of slope angle uca and tpi fig 3 of the target basin are created to run the script it is necessary to select the hydrological correct dem and the shapefile of the target basin and to set the kernel radii for the low pass filter application and for the tpi calculation respectively fig 4c as regards the computation of uca the script runs the saga gis tool flow accumulation recursive of the module terrain analysis hydrology the user can optionally select to apply the uca pre filter which is suggested in case of dem resolution lower than 5 m 2 7 2 hydro geotechnical spatial variables generation the second step of the computational workflow comprises two scripts that lead to the creation of the soil depth and total wetness index thematic raster maps fig 3 by uploading the slope angle and the tpi grids along with the vector map of the soil thicknesses measurements as input data the fourth script automatically performs a polynomial regression by means of the response surface analysis method fig 5 a this statistical procedure has been implemented through the scikit learn package a python tool containing a wide group of statistical algorithms for machine learning and the numpy and scipy libraries specifically the module called polynomialfeatures was exploited inside the tab the user is also requested to set the degree of the fitting polynomial by choosing between 1 and 3 and to specify the column of the vector layer attribute that contains information on soil depth figs 3 and 5a through the fifth command script the user is allowed to produce the soil total wetness raster map by uploading grids of vertical soil thickness slope angle and uca fig 5b additionally the user is requested to set the following hydrological parameters as constant value saturated hydraulic conductivity intensity and duration of the simulated rainstorm effective infiltration soil moisture deficit and matrix suction head at the wetting front fig 5b however k s can be optionally uploaded as raster format data if available furthermore this command separately produces the raster map of both the wetness index m i and m s as additional outputs 2 7 3 slope stability analysis and validation during the last step of the computational framework the user is requested to select some of the previously generated secondary raster maps and subsequently run the command line that performs cell by cell infinite slope calculations to produce the output safety factor map of the target basin fig 6 a the other input data that need to be set are soil saturated unit weight total cohesion and soil friction angle fig 3 alternatively these parameters can be inserted in the tab as constant value if not available as spatially distributed data fig 6a after completing slope stability calculations by launching the seventh script the model performance can be quantitatively analysed to this purpose the user is required to upload the available landslide inventory and to select the number of buffers along with their width around each inventoried landslide within which the accuracy statistics are evaluated fig 6b the validation outcomes are directly plotted on diagrams available on the layer panel 3 model application 3 1 general features of testing site the small vernazza coastal catchment 5 7 km2 which is located within the cinque terre national park eastern liguria region north western italy was selected as testing area fig 7 a the geology of the basin shows turbiditic formations belonging to the northern apennine chain and mostly consisting of arenaceous and pelitic rocks for 73 and 27 respectively cevasco et al 2014 the basin has a mountainous morphology marked by steep slopes 81 of the total area shows slope angles in the range 25 45 that are affected by short and incised channels and mantled by thin veneers of eluvial colluvial deposits which were largely reworked by humans to build agricultural terraces cevasco et al 2014 pepe et al 2019 slope deposits are characterized by granulometric heterogeneity and they prevalently consist of gravel sand silt mixtures and of sand silt mixtures coarse grained particles i e gravel and sand are abundant with respect to fine ones i e silt and clay on average fine fractions are less than 30 40 and are characterized by low plasticity cevasco et al 2013a terraced slopes cover almost 50 of the basin even though a large percentage more than 40 are currently abandoned while natural areas i e woods shrubs meadows mostly occupy the remaining portion cevasco et al 2013b fig 7b the local climate is mediterranean with hot and dry summers and mild winters the annual rainfall is on average 1000 mm although rainfall levels during autumn and winter are the greatest accounting for approximately 70 during these seasons high intensity rainstorms are historically documented as triggering cause of multiple occurrence sls cevasco et al 2015 in this study the violent rainfall event that occurred the 25 october 2011 has been simulated to test the sprin sl package this event strongly hit the whole eastern liguria region with cumulative rainfall levels in the range of around 300 500 mm concentrated in just under 6 h and with extremely high from 90 to 150 mm hourly intensities within the selected basin high intensity rainfall produced widespread erosion processes several hundreds of sls and a destructive debris flood that severely damaged the vernazza hamlet cevasco et al 2015 3 2 data sources a 1 m resolution lidar derived dem was used as topographic base such dem was retrieved from liguria region geoportal and was generated from lidar data acquired 2 3 years before the simulated rainstorm by the italian ministry of the environment in the framework of the pst a project i e piano straordinario di telerilevamento ambientale the land use map of the vernazza basin prepared by cevasco et al 2013b was used as land cover dataset this vector based map includes 7 classes of land use fig 7b and was prepared by means of accurate high resolution aerial photos interpretation rainfall data concerning the 25 october 2011 rainstorm refer to the monterosso al mare rain gauge station which is located approximately 2 5 km west of vernazza and where peak rainfall intensities next to 92 mm 1h 197 mm 3h and 349 mm 6h were recorded during the event arpal cfmi pc 2012 according to rainfall data series cevasco et al 2015 october is also the rainiest month mean monthly rainfall next to 150 mm information on both soil thickness and hydro mechanical soil properties were extracted from previous investigations performed in the study area cevasco et al 2014 raso et al 2020 that were integrated with further laboratory and in situ geotechnical tests specifically additional investigations included 20 grain size analyses on disturbed soil samples 20 dynamic penetration tests 10 borehole shear tests and 20 in situ permeability tests a vector based map consisting of a total of 103 survey points was used to derive the soil depth spatial distribution fig 7c the whole set of measured soil geotechnical and hydraulic parameters are reported in table 2 as explained in the following section some hydro mechanical parameters were obtained from literature by considering soil materials geological and geomorphological settings comparable to those of the study area the inventory of sls triggered during the 25 october 2011 rainstorm was compiled by merging landslides mapped by cevasco et al 2013b with new landslides detected through supplementary field surveys and detailed post event orthophotos analysis fig 7d overall the obtained vector based map consists of more than 750 landslides that were mainly classified as debris avalanches debris slides and debris flows cruden and varnes 1996 hungr et al 2001 the total area affected by slope failures was 0 08 km2 corresponding to 1 5 of the basin area the smallest mapped sl was approximately 4 m2 wide the greatest one approximately 2 103 m2 while the average landslide size was around 64 m2 as described in persichillo et al 2017 the inventoried sls were characterized by an average length of approximately 70 m and by sliding surface depth that depends on the land use setting up to 2 5 and 1 5 m on terraced and non terraced slopes respectively based on these morphometric features i e length depth ratio the infinite slope assumption can be considered as adequately satisfied during the model application milledge et al 2012 3 3 model setup given the high detail of the input dem in order to mitigate the effects of fine scale morphological variations on the derivation of slope angle a 10 m kernel size was set to apply the low pass filter in the test area terrain features at very low wavelength can arise from the rugged morphology of natural slopes and chiefly from the presence of agricultural terraces which produce extensive stepped slope morphologies the adopted size of the circular moving window was calibrated based on both the knowledge of local landforms raso et al 2021 and the most common geometrical and building features of agricultural terraces in the cinque terre area namely terrace width ranging from 2 to 4 m and dry stone wall height of 1 5 2 5 m brandolini 2017 also the resolution of the output hydrologically correct dem was left as the same as the original dem the uca pre filter was used to compute the contributing area while to produce the tpi map a 20 m kernel radius was selected this value was calibrated by considering the actual morphometric features of low order valleys that strongly control eluvial colluvial deposit accumulation and transportation processes inside the vernazza catchment raso et al 2021 afterwards a second degree polynomial was used to define the best fitting response surface model to direct soil depth measurement points table 3 summarizes setup details concerning the main geotechnical and hydrological input parameters based on the outcomes of geotechnical in situ tests table 2 mean values of parameters φ c and k s were adopted whereas the maximum value of γ s was used thus assuming a conservative condition the evaluation of the increase in soil shear strength due to the presence of plant roots δs was guided by land use context within the basin roots data for oak quercus s l and pine tree pinus pinaster species were used to quantify δs in non terraced areas i e wood and scrub where these two species are the most dominant roots data were defined according to bibliographic information on common species in mediterranean environments specifically a root mean diameter d of 5 mm was selected danjon et al 2006 while α and λ coefficients were set as equal to 42 5 and 0 80 for oak moresi et al 2019 respectively and to 23 4 and 0 87 for pine genet et al 2005 respectively by considering an average soil thickness of 1 3 m on non terraced areas cevasco et al 2014 a mean rar value of 0 001 was assumed moresi et al 2019 whereas k and k were set as 1 1 wu et al 1979 and 0 34 preti 2006 respectively the average of δs values of the two species was then assumed as representative of root reinforcement for non terraced areas table 3 for terraced slopes δs was attributed based on the degree of vegetation cover i e dense or scarce and vegetation type information reported in literature van beek et al 2005 table 3 the minimum value of δs was assigned to recently abandoned terraces table 3 since in this context slope stability is significantly reduced cevasco et al 2014 eventually on the basis of the land use map fig 7b and information reported in table 3 a grid based map of total cohesion was produced and used as input data for safety factor calculation because of the lack of specific investigations green ampt s model parameters i e ψ and δθ were attributed according to literature by referring to sandy and low plasticity soils chow et al 1988 and based on sls modelling studies performed in the nearby monterosso al mare basin schilirò et al 2018 where slope deposits are very similar to those of the test site from the hydro geotechnical side since characterized by high content of coarse grained fractions and by minor percentages of silt and clay on the basis of rainfall data recorded at the reference rain gauge station arpal cfmi pc 2012 by assuming a cumulative rainfall of 300 mm over a 6 h period a rainfall intensity of 50 mm h was selected to simulate the 25 october 2011 rainstorm in addition based on rainfall data series cevasco et al 2015 an effective infiltration rate of 72 mm month was assumed as representative of the local hydrological regime during the autumn period all the above mentioned parameters were set as uniform over the entire computational basin the model outcomes were tested using six 5 m width incremental buffers around each inventoried landslide in this study along with the implemented model performance procedure see subsection 2 6 the receiver operating characteristic roc technique fawcett 2006 was also applied this method is commonly used in the field of landslide susceptibility and hazard zoning to assess the discriminant capacity of a predictive model beguería 2006 godt et al 2008 park et al 2013b wang et al 2020 two accuracy metrics were evaluated the true positive rate tp rate which is the ratio of the number of true positives to the total amount of positives i e cells modelled as unstable and the false positive rate fp rate representing the ratio of the number of false positives to the total number of negatives i e non landslide cells by calculating fp and tp rate pairs for various values of the safety factor and by plotting them the roc curves can be obtained the predictive performance of the model is better the closer the curve approaches the upper right corner of the roc graph the best predictive skill would be reached when fp rate is equal to 0 and tp rate equal to 1 indicating that all observed landslides were captured from the model with no false alarms to quantitatively express the global model performance accuracy the area under the curve auc was computed fawcett 2006 the higher the auc value the better the model predictive performance even through this validation procedure roc accuracy metrics and auc values were computed for six buffer zones around observed sls 3 4 model outcomes some of the main secondary outputs of the executed sprin sl simulation namely the grid based maps showing the spatial distribution of slope angle soil depth and total wetness index are shown in fig 8 the consistency of the soil depth map can be explored through the three dimensional plot showing the surface best fitting the available dataset of in situ soil thickness measurement points fig 9 a which returns a r2 value equal to 0 30 as a measure of polynomial regression model goodness in any case it is worth noting that the accuracy of the multiple regression model is strongly affected by the fact that the selected catchment is to a large extent human modified due to the presence of agricultural terraces which produce extremely variable spatial patterns in soil thickness in fact when considering the best fitting surface only for the subset of soil depth data measured in non terraced areas the performance of the polynomial regression improves considerably as r2 is greater than 0 85 fig 9b this evidence suggests the validity of the proposed approach correlating soil thickness spatial distribution to slope angle and tpi as regards soil moisture conditions the total wetness map shows that at the end of the simulated rainstorm the degree of saturation of slope deposits ranged from 21 to 100 fig 8c also showing a strong relationship between topography and total wetness the lowest mt values occur in areas of high relief and along ridges while the highest values occur along areas of morphological convergence the outcomes of the simulation expressed in terms of spatial distribution of grid cells with safety factor lower than 1 are depicted in fig 10 a c together with inventoried sls at a first glance most of the predicted unstable areas and mapped sls seem largely overlapping each other suggesting a satisfactory agreement between model outputs and sls occurred from a quantitative point of view the capability of the model to capture most of the observed sls is reflected by the statistical accuracy indices computed to validate slope stability analysis fig 11 a this was done using six incremental buffer zones around each mapped landslide starting from a 5 m size to a maximum one of 30 m fig 10d and e primarily it is noteworthy that the whole set of accuracy indices significantly improve as buffer size increases nevertheless the most marked increase in accuracy metrics is observed at a short distance from the observed sls namely within 5 m revealing a high reliability in detecting unstable areas overall the pod value increases from 0 61 at the mapped landslide location to 0 93 at a distance of 30 m meaning that predicted sls prone areas well compare to landslide inventory data fig 11a interestingly the far value is as low as 0 19 within 5 m buffer size further dropping at a very low value of 0 07 indicating a considerable decrease in mismatch between predicted unstable areas and stable zones fig 11a the trend of csi values is strictly related to the ones of pod and far it shows the lowest value of 0 32 at the landslide location to then reaches a value as high as 0 81 at a distance of 30 m fig 11a mainly as a direct consequence of a very low far value the model accuracy is further confirmed by the roc analysis results fig 11b when considering the validation with no buffer application the auc is 0 829 indicating that the proposed tool has satisfactory predictive skills in fact it is generally accepted that auc values higher than 0 7 are indicator of adequate predictive accuracy corsini and mulas 2017 in good agreement with the outcomes of the validation procedure implemented in the sprin sl package the roc curves obtained for incremental buffer zones reveal that the auc indices tend to increase at increasing distance from the mapped landslide location fig 11b similarly the most marked increase in the global predictive accuracy is observed within a 5 m buffer zone specifically the auc value increases from 0 932 for a buffer zone of 5 m to 0 996 for a buffer zone of 30 m 4 discussion the application of the sprin sl package to the test site provided outcomes that can have significant implications in terms of landslide susceptibility zoning purposes in fact the obtained results clearly showed that the developed tool is able to identify the most likely unstable areas during short and intense rainstorms with high performance and very high accuracy this is highlighted by the goodness of the validation procedure implemented within the proposed tool which showed that for example by considering a neighbourhood of just 10 m around each inventoried landslides the model detected more than 70 of slope failures triggered by the simulated rainstorm which corresponds to almost 90 of the whole extent of areas predicted as unstable by the model within the target catchment fig 11a on the other side the overprediction metric i e far index showed a sharp decrease with buffer size increasing the highest decrease is observed within 10 m from mapped sls likewise roc scores returned auc values higher than 0 90 within 10 m buffer zones fig 11b these outcomes mean that false positives are more likely concentrated in close proximity of observed slope failures from the forecasting point of view this type of model output is preferable than that showing unstable pixels randomly scattered over terrain units free from landslides in this regard as observed in literature the simple comparison of terrain units occupied by known landslides with model outcomes may be affected by inherent inaccuracies and errors of the landslide inventory guzzetti et al 2012 or by issues coming from algorithms for polygon rasterization arnone et al 2016b this evidence reveals that the sprin sl package adequately fulfils one of the main requirements of landslide zoning models namely minimizing the prediction of unstable zones where no landslides occurred i e false alarms godt et al 2008 corominas et al 2014 consequently the developed tool may represent a valid support to the design of reliable alert systems for civil protection goals furthermore the model application allows to point out interesting insights about both the factors governing rainfall induced sls and the use of high resolution grid based dems for their modelling concerning the first aspect the performed simulation revealed that grid cells modelled as unstable were often concentrated along concave morphologies of the basin fig 10a c such as unchanneled slopes and small v shaped valleys where thicker soil deposits commonly mantle slope flanks these results confirm the crucial role of morphological conditions in controlling sls initiation dietrich et al 1986 montgomery and dietrich 1994 borga et al 2002 however they also highlight that the implemented algorithms for terrain morphometric parameters derivation e g tpi are effective in promoting the detection of sls prone areas especially in computational areas showing complex geomorphological contexts with regard to the second aspect it can be noted that the high performance of the developed tool was accomplished despite the use of a 1 m resolution dem for cell by cell slope stability calculations such observation apparently seems in disagreement with other studies tarolli and tarboton 2006 fuchs et al 2014 arnone et al 2016a wang et al 2020 investigating the influence of dem resolutions on sls modelling adopting physically based approaches which found that pixel sizes lower than 10 m often do not improve the model performance the reason for such discrepancy may be attributed to the dem pre processing procedures implemented into the sprin sl package in light of this fig 12 summarizes the outcomes obtained by running the model without the application of the implemented low pass filters for the computation of both slope angle and contributing area it is interesting to note a significant increase in the far values for all incremental buffers this indicates the increase in the number of unstable pixels where no landslides were mapped fig 12b and c thus resulting in worse performance of the model this suggests that the introduced low pass filtering methods could yield a satisfactory balancing between the exploitation of the more accurate representation of topography and the theoretical assumptions incorporated within hydro geotechnical models milledge et al 2012 gillin et al 2015 nevertheless in this case study the use of a very high resolution may have benefited from both the fact that the average size of inventoried sls was small and that inventory itself was extremely accurate this may restrict the effectiveness of the implemented dem pre processing methods to cases with conditions like those of the test site in this regard as highlighted by arnone et al 2016b a combined analysis of landslide geometrical features and dem resolution should be always undertaken before the application of landslide susceptibility analysis moreover the completeness and accuracy of landslide inventories is an essential requirement to achieve reliable landslide susceptibility zoning maps steger et al 2016 bordoni et al 2020 considering the above mentioned aspects an important perspective for future studies is the investigation on model suitability and performance through applications using different dem resolutions and in other geological geomorphological settings the potential limitations of the developed tool may concern the hydro mechanical soil input parameters and the adequacy of implemented hydrological and geotechnical theoretical models in this case study most input parameters were set as uniform over the entire computational area which may be considered as a limiting factor as many previous studies pointed out park et al 2013b raia et al 2014 lee and park 2016 marin et al 2021 uncertainty in soil hydro mechanical properties can produce significant discrepancies between model predictions and field observations i e landslide occurrence to this purpose probabilistic analysis has been introduced in physically based and spatial distributed modelling of sls haneberg 2004 park et al 2013b raia et al 2014 arnone et al 2016a medina et al 2021 recently probabilistic approaches have been also effectively integrated in gis based tools guo et al 2022 however it should be remarked that the flexible framework of the sprin sl package enables the user to upload many hydro mechanical soil properties also as raster data structure therefore this may virtually allow to investigate the role of input data spatial variation in influencing slope stability regarding the second potential limitation it is noteworthy that the coupled hydro geotechnical models albeit simple are widely used and their reliability was demonstrated in technical literature montgomery et al 1998 borga et al 2002 kale and sahoo 2011 with reference to this aspect the implementation of empirical hydrological models in conjunction with infinite slope analysis gave also satisfactory outcomes medina et al 2021 nonetheless the open source essence of the proposed tool can promote its application in other geological settings and it can facilitate the implementation of more advanced skills both conceptual and mathematical oriented towards improved versions in any case the authors would like to highlight the fact that sprin sl represents an easy to use tool that can be very useful for practitioners authorities and decision makers involved in sls susceptibility hazard and risk zoning for land planning in hilly mountainous regions where these phenomena are one of main causes of economic and life losses papathoma köhle et al 2015 narasimhan et al 2016 the user friendliness of the developed code is further enhanced by the availability of a built in module to automatically validate the performed simulations although several tools have been developed for the automatic slope stability analysis at the basin scale these usually require the user to assess model performance separately 5 conclusions in this paper a new open source and gis integrated deterministic physically based model for rainfall induced sls prediction is presented an infinite slope stability model was integrated with two simple hydrological models to consider both steady groundwater process and transient rainfall infiltration allowing the simulation of soil saturation conditions that lead to shallow slope failures during rainfall due to the key role played by topography in influencing the distributed estimation of slope stability dedicated pre processing dem procedures were incorporated into the model to derive reliable secondary morphological attributes in particular new low pass filtering methods were introduced to smooth terrain conditions at small wavelength which can often be a source of instability overestimation whereas other existing algorithms for both digital terrain features and waterflow pathway analyses were integrated moreover a novel geostatistical approach to predict soil depth spatial distribution based on terrain morphological derivatives and field measurement data was implemented a dedicated module aimed at automatically evaluating the performance of the safety factor map generated by the model was also incorporated the proposed model was packaged into a qgis tool consisting of seven command scripts written in python language and with a user friendly gui which can be directly launched by the users from the processing toolbox by simulating the effects of an intense rainstorm which hit a small coastal basin located within the cinque terre national park in 2011 both the functioning and performance of the developed tool are presented the outcomes comprehensively indicate that the sprin sl package can reach high performance and very high accuracy in predicting sls triggered by intense rainstorms at the small hilly mountainous catchment scale in this regard the cross validation of roc analysis showed that the global accuracy in predicting sls reached 99 within a 20 m radius of the observed landslide locations specifically the results revealed that sprin sl performed well in detecting sls source areas controlled by slope morphology and in reducing as much as possible the consequences connected to overprediction namely false instability alarms as revealed by far values lower than 15 when a 20 m buffer zone around mapped sls is considered applications to other test sites in different geological geomorphological settings to further examine the sprin sl performance are in progress also considering the spatial variation of soil parameters moreover attempts to integrate additional functions such as the implementation of models available in the scientific literature for the spatial estimation of soil depth may represent the future perspective of the next sprin sl versions nevertheless the proposed model can be considered as a valid and flexible tool for sls risk evaluation prevention and mitigation in hilly mountainous regions funding this research was partially funded by the genova university research fund fra 2020 fondi di ricerca di ateneo quota premiale di supporto alla ricerca responsible g pepe author contributions l r g p and a c designed and performed the research l r developed and wrote the code l r and g p setup the model and analysed data g p wrote the paper l r and a c contributed to the discussion of the model outputs d c m f and a c supervised the research software data availability name of software sprin sl spatial prediction of rainfall induced shallow landslides developer luca raimondi contact address lucaraimondi61 gmail com tel 39 0187 633055 fax 39 178 2289901 year first available 2022 software required windows os windows 10 or later quantum gis 3 22 5 ltr program language python 3 9 program size 488 kb zip file download link https drive google com file d 1cijwa72dcuylnmhvmgr5v jss7ej39cw view usp sharing cost free of charge declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank cinque terre national park for providing support during field activities and data collection the authors are also grateful to the editor and the four anonymous reviewers for their helpful comments and suggestions that improved the quality of this paper 
25461,this paper presents a new open source and physically based model for spatial prediction of rainfall induced shallow landslides sprin sl through the quantum gis qgis software sprin sl consists of a set of shell scripts developed using the python language that can be directly run from the qgis processing toolbox through a user friendly graphical interface the tool implements the infinite slope method by incorporating the topog and the green ampt models to consider groundwater flow and transient rainfall infiltration respectively furthermore dem pre processing procedures to extract reliable terrain morphometric features a new statistical method for modelling soil depth and a procedure for predictive accuracy evaluation were implemented by using a 1 m resolution dem the developed model was tested in a small coastal catchment of cinque terre liguria italy providing accurate outcomes and proving to be an easy to use tool for landslide susceptibility zoning which can have useful implications on the risk reduction keywords cinque terre dem resolution open source physically based model qgis shallow landslides data availability data will be made available on request 1 introduction shallow landslides sls are the most common mass movements occurring in hilly and mountainous environments worldwide they typically affect steep slopes mantled by a few meters thick soil debris covers and are more frequently triggered by rainstorms crosta and dal negro 2003 harp et al 2009 park et al 2013a generally rainfall induced sls initiate rapidly through the detachment and consequent sliding downslope of small volumes of materials hungr et al 2014 during extremely intense rainstorms countless sls can be triggered over large areas in very short time intervals fuchu et al 1999 guzzetti et al 2004 guthrie and evans 2004 crozier 2005 larsen 2008 often after their initiation sls can evolve into or concur to generate rapid and destructive flow like phenomena e g debris and mud flows that can travel long distances guadagno et al 2005 cascini et al 2008 wooten et al 2008 depending on the intensity associated with the first failure and post triggering mechanisms sls can give rise to severe risk scenarios hilker et al 2009 pradhan and lee 2010 papathoma köhle et al 2015 galve et al 2016 narasimhan et al 2016 this is the reason why the prediction of sls prone areas has deserved increasingly attention in recent decades this goal can be addressed through approaches that involve the definition of empirical relationships between past landslide occurrences and rainfall features guzzetti et al 2008 segoni et al 2018 galanti et al 2018 or based on the application of statistical van westen et al 2008 corominas et al 2014 bordoni et al 2021 and machine learning zêzere et al 2017 di napoli et al 2020 2021 guo et al 2021 models aimed at investigating the combination of causal factors most influencing sls initiation other widely used approaches defined as physically based focus on the simulation of the triggering processes which primarily depend on the reduction of shear strength due to soil saturation in response to rainfall bogaard and greco 2016 the earliest modelling frameworks relied on the combination of an infinite slope stability analysis with simplified hydrological models linking rainfall and subsurface groundwater flow under the assumption of steady state conditions montgomery and dietrich 1994 wu and sidle 1995 pack et al 1998 borga et al 2002 subsequently this kind of approaches has been further developed by implementing more complex theoretical models that also account for transient hydrological conditions iverson 2000 baum et al 2002 2010 godt et al 2008 an et al 2016 kim et al 2014 physically based models have been extensively implemented in grid based spatial frameworks by exploiting the capability of geographical information systems gis to efficiently manage and process spatial data and several practical applications demonstrated their reliability in forecasting sls over large areas salciarini et al 2006 montrasio and valentino 2008 arnone et al 2011 mergili et al 2014 montrasio et al 2014 in the recent years the potential of landslide prediction modelling has also moved forward thanks to the combined contribution of open source gis such as quantum gis qgis and grass software packages bragagnolo et al 2020 guo et al 2022 titti et al 2022 and freely accessible geospatial and environmental data sources an et al 2018 gunarathna et al 2018 wadhawan et al 2020 however sls modelling at the basin scale can be still time consuming since theoretical models incorporate a large amount of geometrical geotechnical hydrological and hydraulic variables not always easily available or measurable both in the field and in laboratory and often characterized by significant spatial variability haneberg 2004 gorsevski et al 2006 raia et al 2014 this necessarily produces uncertainty in input data estimation which can represent an obstacle to the definition of landslide early warning procedures marin et al 2021 nevertheless the scientific community is making considerable efforts in overcoming the limitations correlated to the reliable determination of soil properties over wide areas park et al 2013b lee and park 2016 arnone et al 2016a medina et al 2021 it is known from literature that the location of sl source areas can be strongly correlated with terrain morphology montgomery and dietrich 1994 montgomery et al 1998 which can directly control stability through the effect of slope gradient or indirectly by influencing some other important landslide predisposing factors such as hydrological processes operating at the slope scale dietrich et al 1986 and spatial distribution of soil thickness dietrich et al 1995 recently digital elevation models dems have become increasingly available and their quality and accuracy have steadily increased also because of advanced remote sensing techniques such as lidar roering et al 2013 owing to their ability to capture small scale details high resolution dems allow to extract accurate derivative morphological attributes e g slope gradient virtually this should entail important advantages in sls modelling especially in understanding the role of instability factors controlled by topography however this is not as straightforward as would expected since it was found that the performance of process based models does not improve automatically by reducing the grid size claessens et al 2005 fuchs et al 2014 penna et al 2014 arnone et al 2021 moreover in some cases the use of very fine resolutions cannot be helpful if other factors such as bedrock morphology or soil piping drive the triggering mechanisms of sls freer et al 2002 crosta et al 2003 tarolli and tarboton 2006 penna et al 2014 this paper presents a new deterministic physically based model for the spatial prediction of rainfall induced shallow landslides sprin sl at the basin scale sprin sl consists of a set of shell scripts developed using the python language that can be directly run from the qgis processing toolbox each implemented script is supported by a user friendly graphical interface gui by integrating the infinite slope method with two simple and well known hydrological models namely the topog model o loughlin 1986 and the green ampt s model 1911 sprin sl allows to directly predict unstable slope areas in response to intense rainstorms moreover some dem pre processing procedures both new and already proposed in the literature to extract reliable terrain morphometric features along with a new method for modelling soil depth and a module to evaluate the simulation accuracy were implemented the developed tool was tested in a small coastal catchment located in eastern liguria region north western italy to describe both its functioning and performance the test site is characterized by a complex terrain morphology which was strongly modified in the past centuries by humans through the construction of agricultural terraces brandolini 2017 and highly prone to be affected by rainfall induced sls cevasco et al 2012 the main goal of this research is to provide an open source and free tool that can be useful for stakeholders and practitioners involved in landslide risk prevention 2 methods 2 1 infinite slope stability model the model presented is based on the geometrical slope sketch depicted in fig 1 a a permeable homogeneous and isotropic soil layer of modest vertical thickness z overlies a rock substratum which is supposed rigid and virtually impermeable from the hydraulic point of view the failure surface is presumed to be i coincident with the soil bedrock interface ii parallel to the ground surface and inclined by an angle β with respect to the horizontal and iii characterized by a length far greater than the soil depth i e infinite slope length assumption according to the limit equilibrium theory and the infinite slope method skempton and delory 1957 under these key simplifying hypotheses the edge effects can be neglected and the overall slope stability can be computed by analysing the equilibrium conditions of an elementary slice of slope with vertical thickness z m width b m and weight w kn fig 1a two mechanisms causing the increase in moisture content within the soil element up to saturation are assumed fig 1a the rising of the groundwater table at a depth z s m above the impervious bedrock and the formation of an advancing wetting front of thickness z i m measured from the ground surface in the former case a steady lateral seepage parallel to the slope related to the effective seasonal infiltration of rainwater is assumed in the second case a transient vertical infiltration of rainwater from the slope surface that develops during a rainfall event is supposed the lateral flow is indicative of the long term response of water table to antecedent rainfall while the vertical flow accounts for the short term effect of infiltration during a specific rainstorm iverson 2000 godt et al 2008 the simple one dimensional model of the infinite slope allows to quantify slope stability through the evaluation of the safety factor f s defined as the ratio between resisting and driving stresses acting at the base of the soil element namely the sliding plane unstable conditions occur for f s lower than 1 value at which the shear stress required for equilibrium overcomes the soil shear strength the implemented equation of f s takes inspiration from the skempton and delory 1957 general formulation and is given by eq 1 f s c δ s γ s z sin β cos β γ s m t γ w γ s tan φ tan β where β is the slope angle γ s kn m3 is the soil saturated unit weight γ w kn m3 is the water unit weight c kpa is the effective soil cohesion φ is the soil friction angle δs kpa is the shear strength increment due to plant roots and m t is the total wetness index a dimensionless parameter indicating the thickness of the saturated part of the soil element along the vertical under the validity of the mohr coulomb failure criteria c and φ describe the soil shear strength and are expressed in terms of effective stresses whereas the term c δs can be defined as total cohesion c the mechanical effect of vegetation was incorporated into eq 1 through the theoretical model developed by wu et al 1979 often used in spatially distributed slope stability models wu and sidle 1995 borga et al 2002 the additional resistance provided by root reinforcement is calculated by assuming that roots are flexible elastic and directed perpendicular to the slip surface and is expressed as follows eq 2 δ s k k t r r r where t r kpa is the average root tensile strength per unit area r r is the root area ratio rar k and k are factors considering that roots are randomly orientated with respect to the slip surface wu et al 1979 and that do not fail simultaneously greenwood et al 2004 respectively root tensile strength is computed as αd λ where d mm is the root diameter whereas α and λ are empirical constants depending on vegetation species whereas rar quantifies the root density into the soil and it can be calculated as a r a where a r m2 is the total section occupied by roots and a m2 is the soil reference area bischetti et al 2005 tosi 2007 the total wetness index is a fractional quantity varying between 0 dry conditions and 1 full saturation in the proposed model m t was split into two contributions that can be computed as follows eq 3 m s z s z eq 4 m i z i z where m s and m i define the relative wetness connected to a steady groundwater lateral flow parallel to the slope fig 1b and to transient vertical rainfall infiltration fig 1c respectively for each spatial unit used to discretize the basin therefore m t is defined as follows eq 5 m t m s m i 2 2 steady state groundwater flow model the groundwater flow was simulated by implementing the topog model o loughlin 1986 this approach based on topographic and hydrological parameters allows to compute the saturation conditions at each spatial unit in response to constant rainfall intensity montgomery and dietrich 1994 under the steady state flow assumption incorporated in the model along with the condition of mass conservation the local groundwater discharge q m3 at each soil element is assumed as balanced by the steady state recharge given by the effective rainwater infiltration i e eq 6 q i e a where a is the upslope contributing area uca m2 fig 1b in this context the parameter i e represents the amount of meteoric precipitation diminished by runoff and evapotranspiration that seasonally infiltrates into the soil medina et al 2021 according to the groundwater flow process described by the darcy s law under a hydraulic gradient i connected to the hydraulic head difference across the soil element which is computed as senβ the groundwater discharge can be derived as follows eq 7 q z s z b t sin β where b m is the width of the soil element and t m2s 1 is the soil transmissivity for simplicity the saturated hydraulic conductivity k s ms 1 can be considered uniform throughout the soil thickness hence it can be written that t k s z cosβ therefore by combining eqs 6 and 7 m s can be computed as eq 8 m s i e a b k s z cos β sin β which defines the relative wetness resulting from the steady groundwater flow parallel to the slope fig 1b 2 3 transient rainwater infiltration model to simulate rainwater infiltration into each soil element the green ampt s 1911 model was adopted which is based on a simplified analytical solution of the darcy s law for one dimensional vertical flow this model hypothesizes that rainwater infiltration takes place through the formation of a sharp wetting front i e piston type shape fig 1c moreover during the downward advancement of the wetting front the soil element is assumed to show a uniform moisture content profile in both space and time namely the soil below the wetting front is in unsaturated conditions with steady matrix suction head whereas the portion of soil above the wetting front is fully saturated with constant k s fig 1c since this model is applicable assuming ponding conditions at the ground surface some modifications of the original version have been developed kale and sahoo 2011 in this study the two stages model for transient vertical infiltration under steady rainfall conditions proposed by mein and larson 1973 was implemented this solution allows to define separately the quantity of rainwater infiltrated up to and after ponding initiation depending on the relationship between rainfall intensity i and infiltration rate f two possible scenarios can be outlined i i f rainwater totally infiltrates into the soil during a time interval t thus no ponding occurs and the cumulative infiltration f t is equal to cumulative rainfall computed as the product it ii i f only a fraction of rainfall can infiltrate into the soil hence surface ponding and runoff take place f t can be estimated using the following relationship eq 9 f t f p k s t t p ψ δ θ ln f t ψ δ θ f p ψ δ θ where ψ mm is the matrix suction head at the wetting front δθ is the soil moisture deficit t p h is the ponding time while f p mm is the cumulative infiltration at the begin of surface ponding which is calculated as eq 10 f p k s ψ δ θ i k s on the basis of the green ampt s model the wetting front depth can be estimated as follows eq 11 z i f t δ θ hence by combining eqs 4 and 11 the relative wetness due to transient rainfall infiltration can be expressed as eq 12 m i f t δ θ z where δθ can be computed as the difference between the effective porosity φ e and the initial water content θ i i e δθ φ e θ i 2 4 dem pre processing and geo morphometric features derivation in the application of spatially distributed models for sls prediction the safety factor is calculated for each spatial unit used to discretize the computational area the raster structure of dems makes it possible to represent and process the whole range of input variables appearing in slope stability calculations in addition by means of specific gis tools of digital terrain analysis dems can be used to extract secondary topographic attributes wilson and gallant 2000 in the sprin sl package the following secondary geo morphometric attributes are derived from dem slope gradient β uca a and topographic position index tpi β appears in all terms of safety factor equation eq 1 while a allows for steady state groundwater flow modelling eq 8 the former is computed based on the method described by horn 1981 the latter represents the area that can potentially produce surficial drainage to each cell and is derived using the multiple flow direction algorithm developed by quinn et al 1991 however dems produced through automatic procedures applied on remotely sensed data often contain defects that can negatively affect the outcomes of both slope stability and hydrological models santini et al 2009 therefore some specific pre processing procedures aimed at improving the quality of input dems are generally required before running grid based slope stability models in this section the dem pre processing methods and the algorithms to derive secondary geo morphometric attributes incorporated in the sprin sl tool are described in detail according to the infinite slope length hypothesis each cell is implicitly considered sufficiently long with respect to the slip surface depth however some studies have shown that the validity of this assumption can be affected by dem resolution specifically the use of high resolution dems from few meters to lower than 1m can make more likely the influence of edge effects which are neglected through the application of the one dimensional infinite slope stability analysis milledge et al 2012 moreover algorithms used to derive slope gradient may produce steeper terrain conditions because of more influenced by variations in topography at the local scale in case of complex or rugged morphologies these aspects may lead to overestimation of unstable areas causing errors i e false positive that reduce the accuracy of landslide susceptibility zoning maps godt et al 2008 arnone et al 2016b fig 2 a and b due to the importance of slope angle in the cell by cell application of the infinite slope method a specific dem pre processing procedure which is especially devoted to high resolution grids has been introduced this procedure involves a focal function i e low pass filter consisting of a moving mean assigning to each output cell of the resultant grid the average value of the input cells included within a circular kernel or window of radius r m defined by the user fig 2c the resultant grid allows to smooth out the small scale morphological features while keeping an adequate information accuracy connected to the use of very fine resolutions fig 2c the simulation of sub surface water flow and the prediction of saturation condition at any locations require the use of a hydrologically correct dem therefore in order to remove artefacts such as spikes holes and steps that commonly affect automatically generated dems and to calculate water flow directions the fill sinks algorithm developed by wang and liu 2006 has been implemented moreover the use of high resolution dems such as those derived from lidar data can give rise to anomalous or ambiguous flows connected to small scale morphological effects gillin et al 2015 riihimäki et al 2021 to smooth these effects a pre filtering tool i e uca pre filter has been introduced to be applied to the hydrologically correct dem before running the flow routing algorithm this filter involves a focal function employing a moving mean through a circular kernel with a fixed radius of 5 m tpi is a terrain attribute obtained from the comparison between the elevation of the single cell and the mean elevation of a group of cells in the neighbourhood of that cell weiss 2001 a positive tpi value indicates that the cell is at higher elevation than those included within the kernel whereas a negative value represents a cell located at lower elevation than its surroundings since tpi is inherently dependent on scale weiss 2001 in the proposed model such an index is calculated cell by cell by implementing an algorithm that defines the neighbourhood around the cell through a circular kernel with radius defined by the user as explained in the following paragraph this index was introduced because the topographic position can be correlated with the thickness of slope deposits 2 5 soil thicknesses modelling soil thickness is a fundamental input data of physically based modelling of rainfall induced sls salciarini et al 2006 godt et al 2008 even though its spatial pattern can be extremely complex depending on the geomorphological context tesfa et al 2009 catani et al 2010 segoni et al 2012 tufano et al 2021 generally the spatial distribution of soil depth can be predicted by using empirical catani et al 2010 del soldato et al 2018 physically based casadei et al 2003 and statistical approaches tesfa et al 2009 to predict the spatial distribution of soil vertical thickness a polynomial regression method with response surface analysis has been selected edwards 2007 this statistical procedure was developed to examine the degree of correlation between several explanatory variables and one response variable by plotting the outcomes of polynomial regression analysis in a three dimensional space for example in the case of a quadratic or second order polynomial the model can be described through the following general equation eq 13 y α 0 i 1 n α i x i i 1 n α i i x i 2 i 1 j 1 n α i j x i x j ε where y is the outcome variable x i and x j are independent variables α 0 α i α ii and α ij are regression coefficients computed based on input data n is the number of independent variables while ε is the random error component therefore the outcome variable is predicted both from each of the predictor variables and from their interactions i e fourth term in eq 13 the accuracy of the model can be evaluated by computing the correlation coefficient r2 in this work vertical soil thicknesses spatial distribution is computed by considering two predictor morphological variables namely slope angle β and tpi along with soil depth information obtained from in situ measurements β is an important controlling factor of soil erosion accumulation processes dietrich et al 1995 salciarini et al 2006 whereas tpi provides information on the position of a terrain unit with respect to specific slope landforms such as escarpments valleys or open slopes in a grid based context high positive tpi values indicate that a cell is located near zones of topographic divergence where soil is more easily eroded while high negative values reveal that a cell is positioned in the proximity of zones marked by topographic convergence hence mainly characterized by soil accumulation weiss 2001 2 6 model predictive accuracy evaluation to test the model performance the contingency tables technique ebert et al 2007 was implemented according to this approach it is defined as hit h a terrain unit predicted as unstable that correctly matches with an occurred landslide conversely an output cell is classified as miss m when an actual unstable zone is not detected by the model eventually a target cell is classified as false alarm f when a stable terrain unit is not correctly captured by the model thus leading to slope instability overestimation starting from these quantities four accuracy indices table 1 can be defined and analysed in conjunction to verify how the model performs in zoning landslide susceptibility liao et al 2011 probability of detection pod reflects the agreement between the model outputs and mapped landslides ranging from 0 poor agreement to 1 perfect agreement false alarm ratio far defines the percentage of false alarms with respect to the total unstable areas predicted by the model far ranges between 0 and 1 and the closer it is to 0 the more correctly the observed landslides are discriminated by the model on the contrary true prediction ratio tpr expresses the fraction of hits with respect to the total amount of terrain units predicted as unstable a tpr value approaching 1 means higher model performance lastly the critical successful index csi considers simultaneously misses and false alarms csi approaching 1 denotes that the total count of false alarms and misses is minimized thus captured landslides are getting closer and closer to those observed the ability of the model to effectively discriminate sls prone areas can be further tested by computing accuracy metrics that account for incremental buffer zones around the inventoried landslide polygons so that the spatial distribution of pixels modelled as unstable is evaluated over a larger area surrounding them moreover this procedure allows to minimize the influence of graphical errors inaccuracies of the landslide inventory guzzetti et al 2012 that can affect landslide susceptibility modelling and mapping arnone et al 2016b 2 7 computational workflow of sprin sl the computational framework of the sprin sl tool can be summed up into three main steps fig 3 i morphometric variables generation ii hydro geotechnical spatial variables generation and iii slope stability analysis and validation the current version of sprin sl runs on qgis 3 22 5 ltr where all equations and algorithms described above were implemented through the python application programming interface api pyqgis version 3 9 by creating a set of shell scripts for automating the process the scripts consist of chained command lines that allow the user to automatically launch algorithms both already integrated into qgis and new ones once the sprin sl package is downloaded the folder sprin sl scripts should be copied into the installation directory of qgis scripts before it can be run then each processing script can be directly launched from the qgis s processing toolbox by selecting the scripts tool icon in which the subfolder called sprin sl appears moreover since some machine learning algorithms were implemented see 2 7 2 subsection for the routines to work the scikit learn library for the python code should be also installed further details on the whole installation instruction are contained in the step by step software documentation each script exploits a simple graphical user interface gui consisting of a tab where the user must insert input data set or modify computational options in specific fields name and save output files and run commands input and output data can be either continuous i e raster structure or categorical i e vector structure along with constant values such as some hydrological and geotechnical parameters 2 7 1 morphometric variables generation the first step of the computational framework consists of three scripts fig 3 the first one allows to create the hydrologically correct dem and the hydrological basins vector map of the computational area in the upper left corner of the tab the user is requested to upload the original dem and to specify the resolution of the output raster fig 4 a once this is done the script sequentially runs the warp reprojection utility provided by the geospatial data abstraction library gdal which reprojects the input dem into the coordinate reference system crs of the current qgis project along with mosaicking it and subsequently the saga gis tool fill sinks of the module terrain analysis pre processing the second script is aimed at clipping the hydrologically correct dem to the extent of the target basin the user is requested to i upload the two output layers generated through the previous script ii select the target basin as vector mask directly in the map canvas by activating the hydrological basins layer within the table of content toc and iii set the option use only selected features before running the command line fig 4b finally by launching the third script the thematic raster maps of slope angle uca and tpi fig 3 of the target basin are created to run the script it is necessary to select the hydrological correct dem and the shapefile of the target basin and to set the kernel radii for the low pass filter application and for the tpi calculation respectively fig 4c as regards the computation of uca the script runs the saga gis tool flow accumulation recursive of the module terrain analysis hydrology the user can optionally select to apply the uca pre filter which is suggested in case of dem resolution lower than 5 m 2 7 2 hydro geotechnical spatial variables generation the second step of the computational workflow comprises two scripts that lead to the creation of the soil depth and total wetness index thematic raster maps fig 3 by uploading the slope angle and the tpi grids along with the vector map of the soil thicknesses measurements as input data the fourth script automatically performs a polynomial regression by means of the response surface analysis method fig 5 a this statistical procedure has been implemented through the scikit learn package a python tool containing a wide group of statistical algorithms for machine learning and the numpy and scipy libraries specifically the module called polynomialfeatures was exploited inside the tab the user is also requested to set the degree of the fitting polynomial by choosing between 1 and 3 and to specify the column of the vector layer attribute that contains information on soil depth figs 3 and 5a through the fifth command script the user is allowed to produce the soil total wetness raster map by uploading grids of vertical soil thickness slope angle and uca fig 5b additionally the user is requested to set the following hydrological parameters as constant value saturated hydraulic conductivity intensity and duration of the simulated rainstorm effective infiltration soil moisture deficit and matrix suction head at the wetting front fig 5b however k s can be optionally uploaded as raster format data if available furthermore this command separately produces the raster map of both the wetness index m i and m s as additional outputs 2 7 3 slope stability analysis and validation during the last step of the computational framework the user is requested to select some of the previously generated secondary raster maps and subsequently run the command line that performs cell by cell infinite slope calculations to produce the output safety factor map of the target basin fig 6 a the other input data that need to be set are soil saturated unit weight total cohesion and soil friction angle fig 3 alternatively these parameters can be inserted in the tab as constant value if not available as spatially distributed data fig 6a after completing slope stability calculations by launching the seventh script the model performance can be quantitatively analysed to this purpose the user is required to upload the available landslide inventory and to select the number of buffers along with their width around each inventoried landslide within which the accuracy statistics are evaluated fig 6b the validation outcomes are directly plotted on diagrams available on the layer panel 3 model application 3 1 general features of testing site the small vernazza coastal catchment 5 7 km2 which is located within the cinque terre national park eastern liguria region north western italy was selected as testing area fig 7 a the geology of the basin shows turbiditic formations belonging to the northern apennine chain and mostly consisting of arenaceous and pelitic rocks for 73 and 27 respectively cevasco et al 2014 the basin has a mountainous morphology marked by steep slopes 81 of the total area shows slope angles in the range 25 45 that are affected by short and incised channels and mantled by thin veneers of eluvial colluvial deposits which were largely reworked by humans to build agricultural terraces cevasco et al 2014 pepe et al 2019 slope deposits are characterized by granulometric heterogeneity and they prevalently consist of gravel sand silt mixtures and of sand silt mixtures coarse grained particles i e gravel and sand are abundant with respect to fine ones i e silt and clay on average fine fractions are less than 30 40 and are characterized by low plasticity cevasco et al 2013a terraced slopes cover almost 50 of the basin even though a large percentage more than 40 are currently abandoned while natural areas i e woods shrubs meadows mostly occupy the remaining portion cevasco et al 2013b fig 7b the local climate is mediterranean with hot and dry summers and mild winters the annual rainfall is on average 1000 mm although rainfall levels during autumn and winter are the greatest accounting for approximately 70 during these seasons high intensity rainstorms are historically documented as triggering cause of multiple occurrence sls cevasco et al 2015 in this study the violent rainfall event that occurred the 25 october 2011 has been simulated to test the sprin sl package this event strongly hit the whole eastern liguria region with cumulative rainfall levels in the range of around 300 500 mm concentrated in just under 6 h and with extremely high from 90 to 150 mm hourly intensities within the selected basin high intensity rainfall produced widespread erosion processes several hundreds of sls and a destructive debris flood that severely damaged the vernazza hamlet cevasco et al 2015 3 2 data sources a 1 m resolution lidar derived dem was used as topographic base such dem was retrieved from liguria region geoportal and was generated from lidar data acquired 2 3 years before the simulated rainstorm by the italian ministry of the environment in the framework of the pst a project i e piano straordinario di telerilevamento ambientale the land use map of the vernazza basin prepared by cevasco et al 2013b was used as land cover dataset this vector based map includes 7 classes of land use fig 7b and was prepared by means of accurate high resolution aerial photos interpretation rainfall data concerning the 25 october 2011 rainstorm refer to the monterosso al mare rain gauge station which is located approximately 2 5 km west of vernazza and where peak rainfall intensities next to 92 mm 1h 197 mm 3h and 349 mm 6h were recorded during the event arpal cfmi pc 2012 according to rainfall data series cevasco et al 2015 october is also the rainiest month mean monthly rainfall next to 150 mm information on both soil thickness and hydro mechanical soil properties were extracted from previous investigations performed in the study area cevasco et al 2014 raso et al 2020 that were integrated with further laboratory and in situ geotechnical tests specifically additional investigations included 20 grain size analyses on disturbed soil samples 20 dynamic penetration tests 10 borehole shear tests and 20 in situ permeability tests a vector based map consisting of a total of 103 survey points was used to derive the soil depth spatial distribution fig 7c the whole set of measured soil geotechnical and hydraulic parameters are reported in table 2 as explained in the following section some hydro mechanical parameters were obtained from literature by considering soil materials geological and geomorphological settings comparable to those of the study area the inventory of sls triggered during the 25 october 2011 rainstorm was compiled by merging landslides mapped by cevasco et al 2013b with new landslides detected through supplementary field surveys and detailed post event orthophotos analysis fig 7d overall the obtained vector based map consists of more than 750 landslides that were mainly classified as debris avalanches debris slides and debris flows cruden and varnes 1996 hungr et al 2001 the total area affected by slope failures was 0 08 km2 corresponding to 1 5 of the basin area the smallest mapped sl was approximately 4 m2 wide the greatest one approximately 2 103 m2 while the average landslide size was around 64 m2 as described in persichillo et al 2017 the inventoried sls were characterized by an average length of approximately 70 m and by sliding surface depth that depends on the land use setting up to 2 5 and 1 5 m on terraced and non terraced slopes respectively based on these morphometric features i e length depth ratio the infinite slope assumption can be considered as adequately satisfied during the model application milledge et al 2012 3 3 model setup given the high detail of the input dem in order to mitigate the effects of fine scale morphological variations on the derivation of slope angle a 10 m kernel size was set to apply the low pass filter in the test area terrain features at very low wavelength can arise from the rugged morphology of natural slopes and chiefly from the presence of agricultural terraces which produce extensive stepped slope morphologies the adopted size of the circular moving window was calibrated based on both the knowledge of local landforms raso et al 2021 and the most common geometrical and building features of agricultural terraces in the cinque terre area namely terrace width ranging from 2 to 4 m and dry stone wall height of 1 5 2 5 m brandolini 2017 also the resolution of the output hydrologically correct dem was left as the same as the original dem the uca pre filter was used to compute the contributing area while to produce the tpi map a 20 m kernel radius was selected this value was calibrated by considering the actual morphometric features of low order valleys that strongly control eluvial colluvial deposit accumulation and transportation processes inside the vernazza catchment raso et al 2021 afterwards a second degree polynomial was used to define the best fitting response surface model to direct soil depth measurement points table 3 summarizes setup details concerning the main geotechnical and hydrological input parameters based on the outcomes of geotechnical in situ tests table 2 mean values of parameters φ c and k s were adopted whereas the maximum value of γ s was used thus assuming a conservative condition the evaluation of the increase in soil shear strength due to the presence of plant roots δs was guided by land use context within the basin roots data for oak quercus s l and pine tree pinus pinaster species were used to quantify δs in non terraced areas i e wood and scrub where these two species are the most dominant roots data were defined according to bibliographic information on common species in mediterranean environments specifically a root mean diameter d of 5 mm was selected danjon et al 2006 while α and λ coefficients were set as equal to 42 5 and 0 80 for oak moresi et al 2019 respectively and to 23 4 and 0 87 for pine genet et al 2005 respectively by considering an average soil thickness of 1 3 m on non terraced areas cevasco et al 2014 a mean rar value of 0 001 was assumed moresi et al 2019 whereas k and k were set as 1 1 wu et al 1979 and 0 34 preti 2006 respectively the average of δs values of the two species was then assumed as representative of root reinforcement for non terraced areas table 3 for terraced slopes δs was attributed based on the degree of vegetation cover i e dense or scarce and vegetation type information reported in literature van beek et al 2005 table 3 the minimum value of δs was assigned to recently abandoned terraces table 3 since in this context slope stability is significantly reduced cevasco et al 2014 eventually on the basis of the land use map fig 7b and information reported in table 3 a grid based map of total cohesion was produced and used as input data for safety factor calculation because of the lack of specific investigations green ampt s model parameters i e ψ and δθ were attributed according to literature by referring to sandy and low plasticity soils chow et al 1988 and based on sls modelling studies performed in the nearby monterosso al mare basin schilirò et al 2018 where slope deposits are very similar to those of the test site from the hydro geotechnical side since characterized by high content of coarse grained fractions and by minor percentages of silt and clay on the basis of rainfall data recorded at the reference rain gauge station arpal cfmi pc 2012 by assuming a cumulative rainfall of 300 mm over a 6 h period a rainfall intensity of 50 mm h was selected to simulate the 25 october 2011 rainstorm in addition based on rainfall data series cevasco et al 2015 an effective infiltration rate of 72 mm month was assumed as representative of the local hydrological regime during the autumn period all the above mentioned parameters were set as uniform over the entire computational basin the model outcomes were tested using six 5 m width incremental buffers around each inventoried landslide in this study along with the implemented model performance procedure see subsection 2 6 the receiver operating characteristic roc technique fawcett 2006 was also applied this method is commonly used in the field of landslide susceptibility and hazard zoning to assess the discriminant capacity of a predictive model beguería 2006 godt et al 2008 park et al 2013b wang et al 2020 two accuracy metrics were evaluated the true positive rate tp rate which is the ratio of the number of true positives to the total amount of positives i e cells modelled as unstable and the false positive rate fp rate representing the ratio of the number of false positives to the total number of negatives i e non landslide cells by calculating fp and tp rate pairs for various values of the safety factor and by plotting them the roc curves can be obtained the predictive performance of the model is better the closer the curve approaches the upper right corner of the roc graph the best predictive skill would be reached when fp rate is equal to 0 and tp rate equal to 1 indicating that all observed landslides were captured from the model with no false alarms to quantitatively express the global model performance accuracy the area under the curve auc was computed fawcett 2006 the higher the auc value the better the model predictive performance even through this validation procedure roc accuracy metrics and auc values were computed for six buffer zones around observed sls 3 4 model outcomes some of the main secondary outputs of the executed sprin sl simulation namely the grid based maps showing the spatial distribution of slope angle soil depth and total wetness index are shown in fig 8 the consistency of the soil depth map can be explored through the three dimensional plot showing the surface best fitting the available dataset of in situ soil thickness measurement points fig 9 a which returns a r2 value equal to 0 30 as a measure of polynomial regression model goodness in any case it is worth noting that the accuracy of the multiple regression model is strongly affected by the fact that the selected catchment is to a large extent human modified due to the presence of agricultural terraces which produce extremely variable spatial patterns in soil thickness in fact when considering the best fitting surface only for the subset of soil depth data measured in non terraced areas the performance of the polynomial regression improves considerably as r2 is greater than 0 85 fig 9b this evidence suggests the validity of the proposed approach correlating soil thickness spatial distribution to slope angle and tpi as regards soil moisture conditions the total wetness map shows that at the end of the simulated rainstorm the degree of saturation of slope deposits ranged from 21 to 100 fig 8c also showing a strong relationship between topography and total wetness the lowest mt values occur in areas of high relief and along ridges while the highest values occur along areas of morphological convergence the outcomes of the simulation expressed in terms of spatial distribution of grid cells with safety factor lower than 1 are depicted in fig 10 a c together with inventoried sls at a first glance most of the predicted unstable areas and mapped sls seem largely overlapping each other suggesting a satisfactory agreement between model outputs and sls occurred from a quantitative point of view the capability of the model to capture most of the observed sls is reflected by the statistical accuracy indices computed to validate slope stability analysis fig 11 a this was done using six incremental buffer zones around each mapped landslide starting from a 5 m size to a maximum one of 30 m fig 10d and e primarily it is noteworthy that the whole set of accuracy indices significantly improve as buffer size increases nevertheless the most marked increase in accuracy metrics is observed at a short distance from the observed sls namely within 5 m revealing a high reliability in detecting unstable areas overall the pod value increases from 0 61 at the mapped landslide location to 0 93 at a distance of 30 m meaning that predicted sls prone areas well compare to landslide inventory data fig 11a interestingly the far value is as low as 0 19 within 5 m buffer size further dropping at a very low value of 0 07 indicating a considerable decrease in mismatch between predicted unstable areas and stable zones fig 11a the trend of csi values is strictly related to the ones of pod and far it shows the lowest value of 0 32 at the landslide location to then reaches a value as high as 0 81 at a distance of 30 m fig 11a mainly as a direct consequence of a very low far value the model accuracy is further confirmed by the roc analysis results fig 11b when considering the validation with no buffer application the auc is 0 829 indicating that the proposed tool has satisfactory predictive skills in fact it is generally accepted that auc values higher than 0 7 are indicator of adequate predictive accuracy corsini and mulas 2017 in good agreement with the outcomes of the validation procedure implemented in the sprin sl package the roc curves obtained for incremental buffer zones reveal that the auc indices tend to increase at increasing distance from the mapped landslide location fig 11b similarly the most marked increase in the global predictive accuracy is observed within a 5 m buffer zone specifically the auc value increases from 0 932 for a buffer zone of 5 m to 0 996 for a buffer zone of 30 m 4 discussion the application of the sprin sl package to the test site provided outcomes that can have significant implications in terms of landslide susceptibility zoning purposes in fact the obtained results clearly showed that the developed tool is able to identify the most likely unstable areas during short and intense rainstorms with high performance and very high accuracy this is highlighted by the goodness of the validation procedure implemented within the proposed tool which showed that for example by considering a neighbourhood of just 10 m around each inventoried landslides the model detected more than 70 of slope failures triggered by the simulated rainstorm which corresponds to almost 90 of the whole extent of areas predicted as unstable by the model within the target catchment fig 11a on the other side the overprediction metric i e far index showed a sharp decrease with buffer size increasing the highest decrease is observed within 10 m from mapped sls likewise roc scores returned auc values higher than 0 90 within 10 m buffer zones fig 11b these outcomes mean that false positives are more likely concentrated in close proximity of observed slope failures from the forecasting point of view this type of model output is preferable than that showing unstable pixels randomly scattered over terrain units free from landslides in this regard as observed in literature the simple comparison of terrain units occupied by known landslides with model outcomes may be affected by inherent inaccuracies and errors of the landslide inventory guzzetti et al 2012 or by issues coming from algorithms for polygon rasterization arnone et al 2016b this evidence reveals that the sprin sl package adequately fulfils one of the main requirements of landslide zoning models namely minimizing the prediction of unstable zones where no landslides occurred i e false alarms godt et al 2008 corominas et al 2014 consequently the developed tool may represent a valid support to the design of reliable alert systems for civil protection goals furthermore the model application allows to point out interesting insights about both the factors governing rainfall induced sls and the use of high resolution grid based dems for their modelling concerning the first aspect the performed simulation revealed that grid cells modelled as unstable were often concentrated along concave morphologies of the basin fig 10a c such as unchanneled slopes and small v shaped valleys where thicker soil deposits commonly mantle slope flanks these results confirm the crucial role of morphological conditions in controlling sls initiation dietrich et al 1986 montgomery and dietrich 1994 borga et al 2002 however they also highlight that the implemented algorithms for terrain morphometric parameters derivation e g tpi are effective in promoting the detection of sls prone areas especially in computational areas showing complex geomorphological contexts with regard to the second aspect it can be noted that the high performance of the developed tool was accomplished despite the use of a 1 m resolution dem for cell by cell slope stability calculations such observation apparently seems in disagreement with other studies tarolli and tarboton 2006 fuchs et al 2014 arnone et al 2016a wang et al 2020 investigating the influence of dem resolutions on sls modelling adopting physically based approaches which found that pixel sizes lower than 10 m often do not improve the model performance the reason for such discrepancy may be attributed to the dem pre processing procedures implemented into the sprin sl package in light of this fig 12 summarizes the outcomes obtained by running the model without the application of the implemented low pass filters for the computation of both slope angle and contributing area it is interesting to note a significant increase in the far values for all incremental buffers this indicates the increase in the number of unstable pixels where no landslides were mapped fig 12b and c thus resulting in worse performance of the model this suggests that the introduced low pass filtering methods could yield a satisfactory balancing between the exploitation of the more accurate representation of topography and the theoretical assumptions incorporated within hydro geotechnical models milledge et al 2012 gillin et al 2015 nevertheless in this case study the use of a very high resolution may have benefited from both the fact that the average size of inventoried sls was small and that inventory itself was extremely accurate this may restrict the effectiveness of the implemented dem pre processing methods to cases with conditions like those of the test site in this regard as highlighted by arnone et al 2016b a combined analysis of landslide geometrical features and dem resolution should be always undertaken before the application of landslide susceptibility analysis moreover the completeness and accuracy of landslide inventories is an essential requirement to achieve reliable landslide susceptibility zoning maps steger et al 2016 bordoni et al 2020 considering the above mentioned aspects an important perspective for future studies is the investigation on model suitability and performance through applications using different dem resolutions and in other geological geomorphological settings the potential limitations of the developed tool may concern the hydro mechanical soil input parameters and the adequacy of implemented hydrological and geotechnical theoretical models in this case study most input parameters were set as uniform over the entire computational area which may be considered as a limiting factor as many previous studies pointed out park et al 2013b raia et al 2014 lee and park 2016 marin et al 2021 uncertainty in soil hydro mechanical properties can produce significant discrepancies between model predictions and field observations i e landslide occurrence to this purpose probabilistic analysis has been introduced in physically based and spatial distributed modelling of sls haneberg 2004 park et al 2013b raia et al 2014 arnone et al 2016a medina et al 2021 recently probabilistic approaches have been also effectively integrated in gis based tools guo et al 2022 however it should be remarked that the flexible framework of the sprin sl package enables the user to upload many hydro mechanical soil properties also as raster data structure therefore this may virtually allow to investigate the role of input data spatial variation in influencing slope stability regarding the second potential limitation it is noteworthy that the coupled hydro geotechnical models albeit simple are widely used and their reliability was demonstrated in technical literature montgomery et al 1998 borga et al 2002 kale and sahoo 2011 with reference to this aspect the implementation of empirical hydrological models in conjunction with infinite slope analysis gave also satisfactory outcomes medina et al 2021 nonetheless the open source essence of the proposed tool can promote its application in other geological settings and it can facilitate the implementation of more advanced skills both conceptual and mathematical oriented towards improved versions in any case the authors would like to highlight the fact that sprin sl represents an easy to use tool that can be very useful for practitioners authorities and decision makers involved in sls susceptibility hazard and risk zoning for land planning in hilly mountainous regions where these phenomena are one of main causes of economic and life losses papathoma köhle et al 2015 narasimhan et al 2016 the user friendliness of the developed code is further enhanced by the availability of a built in module to automatically validate the performed simulations although several tools have been developed for the automatic slope stability analysis at the basin scale these usually require the user to assess model performance separately 5 conclusions in this paper a new open source and gis integrated deterministic physically based model for rainfall induced sls prediction is presented an infinite slope stability model was integrated with two simple hydrological models to consider both steady groundwater process and transient rainfall infiltration allowing the simulation of soil saturation conditions that lead to shallow slope failures during rainfall due to the key role played by topography in influencing the distributed estimation of slope stability dedicated pre processing dem procedures were incorporated into the model to derive reliable secondary morphological attributes in particular new low pass filtering methods were introduced to smooth terrain conditions at small wavelength which can often be a source of instability overestimation whereas other existing algorithms for both digital terrain features and waterflow pathway analyses were integrated moreover a novel geostatistical approach to predict soil depth spatial distribution based on terrain morphological derivatives and field measurement data was implemented a dedicated module aimed at automatically evaluating the performance of the safety factor map generated by the model was also incorporated the proposed model was packaged into a qgis tool consisting of seven command scripts written in python language and with a user friendly gui which can be directly launched by the users from the processing toolbox by simulating the effects of an intense rainstorm which hit a small coastal basin located within the cinque terre national park in 2011 both the functioning and performance of the developed tool are presented the outcomes comprehensively indicate that the sprin sl package can reach high performance and very high accuracy in predicting sls triggered by intense rainstorms at the small hilly mountainous catchment scale in this regard the cross validation of roc analysis showed that the global accuracy in predicting sls reached 99 within a 20 m radius of the observed landslide locations specifically the results revealed that sprin sl performed well in detecting sls source areas controlled by slope morphology and in reducing as much as possible the consequences connected to overprediction namely false instability alarms as revealed by far values lower than 15 when a 20 m buffer zone around mapped sls is considered applications to other test sites in different geological geomorphological settings to further examine the sprin sl performance are in progress also considering the spatial variation of soil parameters moreover attempts to integrate additional functions such as the implementation of models available in the scientific literature for the spatial estimation of soil depth may represent the future perspective of the next sprin sl versions nevertheless the proposed model can be considered as a valid and flexible tool for sls risk evaluation prevention and mitigation in hilly mountainous regions funding this research was partially funded by the genova university research fund fra 2020 fondi di ricerca di ateneo quota premiale di supporto alla ricerca responsible g pepe author contributions l r g p and a c designed and performed the research l r developed and wrote the code l r and g p setup the model and analysed data g p wrote the paper l r and a c contributed to the discussion of the model outputs d c m f and a c supervised the research software data availability name of software sprin sl spatial prediction of rainfall induced shallow landslides developer luca raimondi contact address lucaraimondi61 gmail com tel 39 0187 633055 fax 39 178 2289901 year first available 2022 software required windows os windows 10 or later quantum gis 3 22 5 ltr program language python 3 9 program size 488 kb zip file download link https drive google com file d 1cijwa72dcuylnmhvmgr5v jss7ej39cw view usp sharing cost free of charge declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank cinque terre national park for providing support during field activities and data collection the authors are also grateful to the editor and the four anonymous reviewers for their helpful comments and suggestions that improved the quality of this paper 
25462,rapidly identifying high risk areas for potential wildfires is crucial for preparedness disaster management and operational logistics decisions with the advancement of technologies such as cloud computing high risk areas can be determined ahead of time by simulating several possible fires based on forecast conditions however such systems may take longer and delay decision making we introduce a novel approach that harnesses the benefits of quadtree based search strategies and conditional probability to enable rapid identification of high fire risk areas and produces an increasingly detailed risk map within a given time frame we also present a comprehensive performance analysis of different search strategies to investigate the trade off between risk areas coverage and time efficiency showcasing how decision makers can modify parameters based on time requirements experimental results show that up to 80 of high fire risk areas in tasmania can be identified with the proposed approach in about 20 less time than conventional comprehensive sweep methods keywords fire management high fire risk areas quadtree fire simulation tool spark wildfire behavior prediction 1 introduction the 2019 2020 australian wildfire season was one of the worst on record with around 17 million hectares burnt 3094 houses destroyed 33 lives lost and over a billion mammals killed richards et al 2020 around 1600 firefighters and 6386 interstate personnel were reported to be involved in operations around the country such devastating wildfire seasons are challenging for any fire authorities to manage and any predictive information on wildfire risk that can be provided quickly can be crucial for operational management especially in urban settlement areas that are subject to increased fire risks due to climate change lucas et al 2007 recently wildfire risk models have been widely used to identify high fire risk locations by predicting the fire spread rate or estimating various risk metrics in an operational framework to closely quantify the risks associated with fires in a given region a large number of model runs fire simulations collectively referred to as an ensemble are run and statistical analyses on the simulation outputs are carried out the output of a single simulation could be for example the locations burnt by the fire the maximum intensity of the fire the height of the flames or the smoke generated by the fire due to the complexity and the high number of simulations involved such ensembles are computationally expensive ujjwal et al 2019 2020a 2021b the propagation and behavior of fire as well as the resulting areas affected are dependent on several factors including topography fuel and weather weather conditions are critical in determining whether a fire will spread from an ignition point and during propagation of the fire ujjwal et al 2020b ruffault et al 2017 russo et al 2017 wildfires frequently occur on bad fire days with hot and dry weather combined with strong winds trigo et al 2016 cardil et al 2014 the behavior of a fire can quickly change with weather conditions and the ability to accurately rapidly predict fire behavior for a set of given weather conditions is required for effective planning and management one of the most challenging tasks for fire authorities is to identify high risk locations and position the resources effectively ahead of time o connor et al 2017 as during operational fire management practitioners have only limited time for assessment and adaptation to any evolving conditions furthermore relying on historical records to identify high fire risk areas may yield inaccurate results as the contributing factors may have changed over time consequently extrapolating past data may not serve as an alternative to simulating wildfire events pinto et al 2018 the advancement of computing technologies such as cloud computing has significantly decreased the overall time required to derive predictive risk metrics from sets of complex fire simulations ujjwal et al 2019 2020a the cloud provides scalable computational resources allowing multiple simulations to be run simultaneously however for large geographical regions the time taken to compute ensemble predictions at the scale required for an accurate assessment of risk may still be larger than the time window required for planning and response in addition a naive sweep method of the entire region such as simulating fires at a regularly spaced grid of possible starting locations covering the region requires all simulations to be run before areas of risk can be identified the central idea behind the method presented in this paper is the observation that only a fraction of the possible fires over a geographical area will result in high risk fires for example for a geographical region such as the one used in this study with around 70 000 possible fire start locations the number of locations where the resulting risk is extremely high under certain weather conditions may only be around 1 of these a conventional comprehensive sweep method must run fire simulations at all possible start locations to identify these high fire risk areas which can delay the overall time to generate risk information and may also need a large number of cloud computing machines a more efficient search strategy could be used to quickly identify high fire risk areas in less time but to the best of our knowledge such an approach is not used within current operational fire management systems and tools despite the potential benefits a quadtree based search strategy is one of the prime candidates for the approach quadtree based search schemes are one of the most widely used search methods in various applications such as image processing samet 1982 sullivan and baker 1994 spatial search gahegan 1989 zhang et al 2016 and information retrieval giblin 2001 wu et al 2005 menberg et al 2016 such a scheme divides space into four equally sized sub spaces rectangular or square and determines if the desired object falls into any of these sub spaces each sub space with desired objects is further divided into four equal spaces and searched for the desired object until the process reaches the maximum depth of the quadtree or no further space division is possible quadtree based search approaches exponentially refine the search space resulting in logarithmic o log n time complexity for searching finkel and bentley 1974 this exponential time saving compared to a naive search can be transferred to the identification of high fire risk areas the identification of high fire risk areas using a quadtree based strategy is a recursive process a few high risk areas can be rapidly identified at a coarse resolution and a detailed risk map can be produced at a finer resolution over time alternatively the quadtree based search operation can start at random locations and give an increasingly detailed map at regular intervals a quadtree based search operation may cover a maximum number of high fire risk areas in less time by avoiding unfavorable search spaces low fire risk areas in this paper we detail a novel approach that employs a quadtree based search strategy to adaptively identify as many high fire risk areas as possible for any fire weather within a given time frame referred to as planning time hereafter for clarity the proposed adaptive approach can help harness the benefits of a quadtree based search strategy in any system single or multi machine and produce an increasingly detailed fire risk map over time additionally we incorporate the concept of conditional probability to estimate the likelihood of a fire turning high risk before running any simulation which saves more time as illustrated in one of our previous works ujjwal et al 2022 in the proposed approach we define three different search methods based on the chess moves bishop rook and queen to define how to move finer into quadtree based search operations all low fire risk areas are discarded while moving finer to prioritize high fire risk areas after a level in the quadtree based search strategy referred to as drop level moreover we present a comprehensive performance analysis of the approach with search methods and demonstrate how the proposed approach can alternate between the methods to balance the trade off between the total number of identified high fire risk areas defined as the coverage of high fire risk areas hereafter and the planning time furthermore we apply the approach to the entire tasmanian region to validate its efficacy 2 methods 2 1 problem description for any given geographical region r with n different possible fire start locations the problem of high fire risk areas identification can be stated as a problem of maximizing the total number of identified high fire risk areas f i within the operating constraints of time t response time and computational resources p the risk metric r f i for f i should be greater than a threshold t h which identifies the possible fire damages as high risk mathematically it can be expressed as 1 max c s t j 1 2 p t p j t c i f i c n f i 0 if r f i t h 1 if r f i t h where t p j is the time for which the computational resource p j runs system time there is a non trivial trade off between the time and the coverage maximum number of the identified high fire risk areas solving the defined problem would give a sub optimal solution with a balanced trade off between the time and the coverage for the operating constraints of time and computational resources 2 2 proposed adaptive approach we chose a quadtree based search strategy in our proposed approach to solve the described problem due to its ability to quickly identify the desired search results by eliminating unfavorable options in the proposed approach the search strategy starts with a bigger space and divides it into four equal smaller spaces at each level to keep exploring finer an example of the quadtree based search strategy in the tasmanian region is shown in fig 1 where the search operation focuses on an identified high fire risk start location represented by a yellow dot for the sake of clarity these high risk areas are identified at different levels mostly at coarser levels during a quadtree based search operation and more high risk areas at finer levels are searched around the high risk areas identified at coarse levels the proposed approach can be applied to any location or already identified high risk areas as starting points in the search operation to apply the quadtree based search strategy we represent the geographical area with grid points where each point resembles a possible fire start location in the shallowest level of the quadtree based strategy space is represented by four corner points consequently dividing the space into four equal smaller spaces in the following level is equivalent to finding the neighbors of the corner points where the distance between the points and neighbors keeps changing at each level at level 0 the search strategy finds the neighboring points around the yellow dot at the farthest distance as shown by the largest grid in the figure with the increase in the value of the level closer neighboring points are determined with the distance decreasing the closer neighboring points are determined based on three chess moves and the three methods are named accordingly bishop rook and queen shown in fig 2 the same approach is followed for all the identified high fire risk start locations before running a fire simulation at any point the likelihood of fire risk is estimated using the calculated conditional probability the simulations are run only if the likelihood is more than a certain value 30 in our study all the identified low fire risk areas are also discarded after the search operation reaches the drop level to prioritize the identified high fire risk areas as a result the search strategy finds the high fire risk areas from a coarse to a finer resolution based on the resources time and computation available at various time steps within the given response time a fire start location is labeled as a high fire risk location based on the threshold for the risk metric the identification of high fire risk areas in the proposed model is algorithmically represented in algorithm 1 in the algorithmic representation l e v e l is the drop level in the quadtree based search t is the response time m e t h o d is the method chosen to find the neighbors to any point f h p is the list of identified high fire risk areas d p is the list to check and ensure the simulations are not repeated on the same point p h x i f w k is the likelihood of a fire starting at location x i turning high risk and g w is the width of the grid 2 3 calculation of conditional probability we used naive bayes theorem berry 1996 to estimate the likelihood p h x i f w k of a fire starting at a location x i under any fire weather condition f w k to turn high risk with data collected during our experiments accordingly p h x i f w k can be expressed as follows 2 p h x i f w k p x i h p f w k h p x i f w k p h 2 4 test of significance for experimental results to verify the statistical correctness of the experimental findings we drew 30 random samples for the methods under comparison and conduct a wilcoxon test for a 95 confidence interval we formulated the null hypothesis as the proposed approach is as good as the random or sequential search operation and the alternative hypothesis as the proposed approach has better performance than the random or sequential search operation as such we compared the calculated p value p against the standard value of 0 025 for the one tailed test and accept the null hypothesis if p 0 025 or reject the null hypothesis and support the alternative hypothesis if p 0 025 2 5 test setup 2 5 1 study area we chose tasmania as a test region for several reasons firstly tasmania is one of the australian regions with frequent wildfires during summer 841 wildfires in the 2018 2019 wildfire season with 310 311 hectares of area burnt by wildfires tasmania fire service 2019 secondly tasmania has high quality land data sets available from the tasmania fire service tfs and state emergency service ses tasmanian department of primary industries parks water and environment tasmanian vegetation monitoring and mapping program 2013 lastly tasmania has a well studied and systematic grid configuration for possible fire start locations within its entirety where fire simulations can easily be run with existing configurations tasmania fire service 2019 all the fire simulations were run to simulate the fire behaviors for five hours after the fires started 2 5 2 wildfire simulations and data for this study we used the wildfire modeling framework spark miller et al 2015 as the fire simulation tool spark is a flexible platform for simulating wildfires that allows different types of fire behavior to be defined using scripts including rates of spread in different fuel types firebrand dynamics and risk metrics for fire impact and severity calculations in spark are parallelized using the opencl framework to enable the efficient execution of the simulations fire simulations in spark typically require several input data sets for the fire behavior models including maps of the land classification fuel type topography fuel information and meteorological data since the focus of our work was the fire weather defined by the combination of air temperature wind speed and relative humidity we considered only those inputs as variables for the fire simulations the cumulative area burned by fires at the end of the simulation period was the output metric all the fire simulations were run to simulate the fire behaviors for five hours after the fires started the input configuration file used along with the simulation outputs is available on a cloud repository kc et al 2021 2 5 3 experimental platform to harness the benefits of advanced computing technology with parallel operations we utilized a cloud based framework as detailed in our previous work ujjwal et al 2020b to run the fire simulations the framework was developed over the cloud infrastructure of nectar cloud the simulations were run on m3 large instances with 8 vcpus 16 gb ram and 30 gb memory ubuntu 16 04 lts xenial amd64 2 5 4 comparable systems we compared the performance of the proposed approach against a conventional comprehensive sweep system and two different search operations random and sequential in a conventional comprehensive sweep system fire simulations are run for all the possible fire start locations in such a system the order in which the locations are picked for running the fire simulation is not important as all the possible locations need to be covered for a random search operation fire simulations are run one after another at locations picked randomly within a given response time in a sequential search operation the first location to run the fire simulation is picked randomly and the locations thereafter are chosen based on the incremental value of the point id the identifier for locations as the labeling of the locations with point id values has been done by tfs based on a pattern it is more likely that the high fire risk areas are concentrated at a particular region and thus the sequential search can be considered as a strategy with some prior information 2 5 5 fire weather the input data sets included air temperature relative humidity and wind speed for calculating the fire rate of spread in the spark simulations ujjwal et al 2020b based on the setup and results of the same work we consider the discretized ranges high medium and low for the values for parameters the permissible range and the discrete labels assigned based on the values of the factors are summarized in table 1 the range and discretization carried out here can simply be altered and adapted to suit any similar analysis 3 results 3 1 application to the tasmanian region we used a grid of 256 256 to represent 65536 different possible fire start locations within tasmania the high fire risk areas as identified by the proposed approach for the fire weather hhh are shown in fig 5 along with the result of a conventional comprehensive sweep where a threshold of 1000 hectares was used to label a start location as a high fire risk area the fire weather condition hhh is the condition when the factors temperature relative humidity and wind speed have the highest influence on fire growth the proposed approach using the bishop method was able to identify 23727 out of 36346 high fire risk areas about 66 coverage in about 30 less system time than that of a conventional comprehensive sweep in a single system similarly the proposed approach with rook and queen methods was able to find 26621 high fire risk areas about 74 in about 20 less system time and 35166 high fire risk areas about 97 in about 4 less system time when compared to a comprehensive search strategy in a single machine for a multi machine cloud system our proposed approach maintained the same coverage in about 35 bishop and 26 rook less system time for the method queen the proposed approach took about 30 more time than the conventional system fig 4 shows the adaptive identification of high fire risk areas with the method bishop from a coarse to a finer resolution at different time instants until an hour mark specified planning time as seen from the figure the proposed approach was able to identify 134 high fire risk areas in the first ten minutes with a total of 200 453 568 932 and 1072 high fire risk areas in each 10 minute time step until the hour mark the findings for other methods are listed in table 2 the proposed approach was able to identify more high fire risk areas with the queen method 3 2 performance analysis of the proposed approach 3 2 1 fire weather fig 5 a shows the performance high fire risk area coverage of the proposed approach for 27 different fire weather combinations drop level 6 the fire weather lll as included in the plot indicates the fire weather with low influences of temperature relative humidity and wind speed on the fire growth as described in table 1 the fire weather hhh indicates the high influence of the parameters on the fire growth as seen from the figure the proposed approach performed better when the fire weather was highly favorable for the fire to grow with a high possibility of more high fire risk areas when the number of possible high fire risk areas was low the queen method performed the best while the methods bishop and rook performed satisfactorily with a minimum of about 40 coverage for fire weather favorable for fire growth the proposed approach performed satisfactorily with all the methods the queen was found to be the best performing method within the proposed approach for most of the fire weather conditions except for the lll and mll fire weather conditions in which bishop and rook respectively performed the best in our analysis the maximum coverage of the proposed approach with three methods stood at about 83 88 and 99 respectively 3 2 2 coverage fig 5 b shows the variation of the high fire risk area coverage with the change in the value of the drop level as seen from the figure the queen method was more efficient than the two other methods as the minimum coverage of high fire risk areas with the method was about 84 while the same for the bishop and rook methods stood at about 23 and 30 respectively at drop level of two for the queen method the proposed approach started covering more than 90 of the possible high fire risk areas after the drop level of three for a drop level of seven the proposed approach achieved coverage of about 83 and 88 with the methods bishop and rook respectively 3 2 3 time efficiency fig 5 c represents the proportion of the system time that can be saved with the proposed approach when compared to a comprehensive sweep of all the fire start locations at different drop levels the proposed approach was the most time efficient when the bishop was used in finding the nearest neighboring locations for any high fire risk area while moving finer in the search operations the time efficiency with the proposed approach decreased with the increase in the value of the drop level the proposed approach took as much as the time taken by a comprehensive swap at drop levels of 8 bishop and rook and 6 queen when all the possible high fire risk areas were identified 3 2 4 trade off between time efficiency and coverage fig 5 d shows the analysis of the trade off between the coverage and the time efficiency within the proposed approach which can be further synthesized to determine the best way to use the proposed approach the proposed approach can cover a larger fraction of all the possible high fire risk areas when there is a long planning time left for coordinating preparedness activities for fire management for a balanced trade off between the coverage and the system time efficiency the proposed approach should be used with either the bishop method or the rook method as the proposed approach can cover over 80 of the high fire risk areas with about 20 time efficiency the proposed approach has a balanced linear trade off between the coverage and the time efficiency with an average of x coverage of the high fire risk areas in about y less time than a conventional sweep provided x y 100 when three methods are used in a complementing manner 3 2 5 comparison against a random and a sequential search operation fig 6 shows the comparison of the coverage of the proposed approach against a random search operation carried out for a total duration of an hour against a random search operation the proposed approach started off the search operation without any prior information on possible high fire risk areas and still performed better for almost all the fire weather conditions except the ones characterized by l l l m l l h l l for the queen method the mean of the coverage was less than that of the random and sequential ones for the weather condition h h h for all the weather conditions where the proposed approach performed better the calculated values of p in the wilcoxon test were extremely smaller than 0 025 thereby statistically verifying the superior performance of the proposed approach over a random and a sequential search operation the coverage of the queen method was not statistically better than the random and sequential search for the fire weather mhh and so was the case with the bishop and the rook method for the fire weather hhh 3 2 6 high fire risk area identification with multi machine system fig 7 depicts the total number of high fire risk areas identified by the proposed model with the rook method for a multi machine system for the fire weather condition identified by f w t h r h w h as seen from the figure the proposed approach can identify up to 1469 out of 36346 high fire risk areas in the first hour in a system with 100 parallel machines when operated with a multi machine system with about 2000 machines the proposed approach can cover more than 72 of the total high fire risk areas such a system which can be realized with cloud infrastructure ensures the best use of the proposed approach as a large number of possible high fire risk areas can be rapidly identified for better wildfire management 4 discussion during the approach application the entire grid space represented the tasmanian region at level 0 for any fire weather f w i t i r i w i at level 0 the proposed approach runs the file simulations at locations represented by the corner points in the grid the proposed approach explores finer with a unit increment in level by finding the neighboring points the approach stops its search operation in one of the two conditions whichever is earlier the first case when the time constraint planning time given to the approach ceases and the second case when the maximum depth level the level by which the quadtree based search operation covers all possible points is reached level 8 here in our approach application once the search operation is over the approach maps the grid points to the physical geographical locations in the tasmanian region along with the actual possible fire burnt area for all the identified high fire risk areas better search coverage was obtained with the queen method due to more eight neighbors around identified high fire risk areas taking more surrounding points around an identified high fire risk area minimizes the miss of possible high fire risk start locations for a multi machine cloud system the proposed approach with the queen method could cover all the high fire risk areas but with 30 more time than the conventional system this finding can be attributed to the necessity of finding eight neighboring locations around a high fire risk area that results in several same neighboring locations for multiple high fire risk areas a large number of the same neighboring locations for multiple locations can influence how the simulations are distributed among the multiple machines and incur longer execution times such shortcomings with the queen method can be overcome by intelligently distributing the simulations at each level within the search operation the proposed approach was also able to produce a more detailed map of high fire risk areas evolving with time for a given planning time of an hour such predictive information at coarse level resolution in quick time can help fire authorities to stay alert for better preparedness against an unfolding fire disaster in our previous studies ujjwal et al 2020b 2021a the relative humidity and the wind speed have been shown to have a higher influence on fire growth while the temperature was found to have a lesser influence for weather conditions favorable for wildfire growth high temperature wind speed and low relative humidity the number of possible high fire risk areas is high and vice versa while analyzing the approach performance in various fire weather conditions the performance is the worst at the weather conditions when there are a few possible high fire risk areas this worst performance is so because the method keeps dropping the low fire risk areas before the search operation reaches the deepest level such a method within the approach could miss a few high fire risk start points located at a finer level of a quadtree based search operation the worst performance of the proposed approach at these conditions explains why the proposed approach was statistically less efficient than a random and sequential search operation as a result the proposed approach should not be used for the fire weather conditions where it has been found to be statistically nevertheless the coverage achieved by the proposed approach can be improved by increasing the value of the drop level and prioritizing the queen method for the fire weather conditions where fires could grow quickly and burn massive areas the performance is quite good with as high as about 99 coverage we also demonstrated how to use the proposed approach in a multi machine system local or cloud for rapidly identifying high fire risk areas for effective wildfire management fire behaviors change drastically with fire weather conditions and thus predicting the fire behaviors without running fire simulations can be a difficult task additionally some wildfires can grow to burn thousands of hectares in a few hours and simulating those fires can significantly take longer while running ensembles in a multi machine system several machines may stay idle while a few machines are still running the batches simulating larger fires consequently optimizing computing resource utilization while running ensembles of fire simulations in a multi machine system is still an open challenge the proposed approach can minimize the number of simulations in analysis but has to be coupled with methods to efficiently distribute the simulations among the machines in a multi machine system 5 conclusions and future works in this paper we proposed a quadtree based adaptive approach that practitioners can use in existing systems single or multiple machines to identify high fire risk areas quickly within the desired response time during emergencies for better decision making we validated the efficacy of the proposed approach by applying it to the tasmanian region with a proof of concept system to identify such high fire risk areas at different time steps additionally we also conducted a thorough performance analysis of the proposed approach and its search strategies the experimental results showed that the proposed approach handles the trade off between the coverage of high fire risk areas and system time better in any system moreover compared to conventional comprehensive sweep in ensembles the proposed approach was able to identify more than 80 of high fire risk areas in about 20 less time thus our investigative effort has demonstrated that search strategy in ensemble predictions can help quickly identify high risk areas with statistically better results than random and sequential methods additionally the proposed approach is flexible too with the value of the threshold to define a high fire risk area the system to realize the approach the number of machines in the system the drop level for low fire risk areas and even the disaster simulation framework flood simulation tool or others easily changeable for comparable approach performance the increasingly detailed risk map produced from the proposed approach could be highly useful in urban settings for better preparedness and response against fire emergencies as updated alerts can be relayed at regular intervals with increased precision we expect the proposed approach to be useful for fire authorities to quickly grab some crucial predictive information during emergencies and make better informed decisions to minimize the losses conceded to unwanted wildfire occurrences as a future work the proposed approach can be extended and applied to examine if the quadtree steps into urban regions and quickly identify the imminent risks around urban edges similarly the proposed approach can start search operations in urban regions and build risk maps around the regions software availability the fire propagation tool spark on which the fire simulations required for this study was run is developed by data61 csiro and can be downloaded from https research csiro au spark the fire simulation data set used for this study is accessible at https data csiro au collections collection cicsiro 49133v1 ditrue kc et al 2021 uncited references fig 3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25462,rapidly identifying high risk areas for potential wildfires is crucial for preparedness disaster management and operational logistics decisions with the advancement of technologies such as cloud computing high risk areas can be determined ahead of time by simulating several possible fires based on forecast conditions however such systems may take longer and delay decision making we introduce a novel approach that harnesses the benefits of quadtree based search strategies and conditional probability to enable rapid identification of high fire risk areas and produces an increasingly detailed risk map within a given time frame we also present a comprehensive performance analysis of different search strategies to investigate the trade off between risk areas coverage and time efficiency showcasing how decision makers can modify parameters based on time requirements experimental results show that up to 80 of high fire risk areas in tasmania can be identified with the proposed approach in about 20 less time than conventional comprehensive sweep methods keywords fire management high fire risk areas quadtree fire simulation tool spark wildfire behavior prediction 1 introduction the 2019 2020 australian wildfire season was one of the worst on record with around 17 million hectares burnt 3094 houses destroyed 33 lives lost and over a billion mammals killed richards et al 2020 around 1600 firefighters and 6386 interstate personnel were reported to be involved in operations around the country such devastating wildfire seasons are challenging for any fire authorities to manage and any predictive information on wildfire risk that can be provided quickly can be crucial for operational management especially in urban settlement areas that are subject to increased fire risks due to climate change lucas et al 2007 recently wildfire risk models have been widely used to identify high fire risk locations by predicting the fire spread rate or estimating various risk metrics in an operational framework to closely quantify the risks associated with fires in a given region a large number of model runs fire simulations collectively referred to as an ensemble are run and statistical analyses on the simulation outputs are carried out the output of a single simulation could be for example the locations burnt by the fire the maximum intensity of the fire the height of the flames or the smoke generated by the fire due to the complexity and the high number of simulations involved such ensembles are computationally expensive ujjwal et al 2019 2020a 2021b the propagation and behavior of fire as well as the resulting areas affected are dependent on several factors including topography fuel and weather weather conditions are critical in determining whether a fire will spread from an ignition point and during propagation of the fire ujjwal et al 2020b ruffault et al 2017 russo et al 2017 wildfires frequently occur on bad fire days with hot and dry weather combined with strong winds trigo et al 2016 cardil et al 2014 the behavior of a fire can quickly change with weather conditions and the ability to accurately rapidly predict fire behavior for a set of given weather conditions is required for effective planning and management one of the most challenging tasks for fire authorities is to identify high risk locations and position the resources effectively ahead of time o connor et al 2017 as during operational fire management practitioners have only limited time for assessment and adaptation to any evolving conditions furthermore relying on historical records to identify high fire risk areas may yield inaccurate results as the contributing factors may have changed over time consequently extrapolating past data may not serve as an alternative to simulating wildfire events pinto et al 2018 the advancement of computing technologies such as cloud computing has significantly decreased the overall time required to derive predictive risk metrics from sets of complex fire simulations ujjwal et al 2019 2020a the cloud provides scalable computational resources allowing multiple simulations to be run simultaneously however for large geographical regions the time taken to compute ensemble predictions at the scale required for an accurate assessment of risk may still be larger than the time window required for planning and response in addition a naive sweep method of the entire region such as simulating fires at a regularly spaced grid of possible starting locations covering the region requires all simulations to be run before areas of risk can be identified the central idea behind the method presented in this paper is the observation that only a fraction of the possible fires over a geographical area will result in high risk fires for example for a geographical region such as the one used in this study with around 70 000 possible fire start locations the number of locations where the resulting risk is extremely high under certain weather conditions may only be around 1 of these a conventional comprehensive sweep method must run fire simulations at all possible start locations to identify these high fire risk areas which can delay the overall time to generate risk information and may also need a large number of cloud computing machines a more efficient search strategy could be used to quickly identify high fire risk areas in less time but to the best of our knowledge such an approach is not used within current operational fire management systems and tools despite the potential benefits a quadtree based search strategy is one of the prime candidates for the approach quadtree based search schemes are one of the most widely used search methods in various applications such as image processing samet 1982 sullivan and baker 1994 spatial search gahegan 1989 zhang et al 2016 and information retrieval giblin 2001 wu et al 2005 menberg et al 2016 such a scheme divides space into four equally sized sub spaces rectangular or square and determines if the desired object falls into any of these sub spaces each sub space with desired objects is further divided into four equal spaces and searched for the desired object until the process reaches the maximum depth of the quadtree or no further space division is possible quadtree based search approaches exponentially refine the search space resulting in logarithmic o log n time complexity for searching finkel and bentley 1974 this exponential time saving compared to a naive search can be transferred to the identification of high fire risk areas the identification of high fire risk areas using a quadtree based strategy is a recursive process a few high risk areas can be rapidly identified at a coarse resolution and a detailed risk map can be produced at a finer resolution over time alternatively the quadtree based search operation can start at random locations and give an increasingly detailed map at regular intervals a quadtree based search operation may cover a maximum number of high fire risk areas in less time by avoiding unfavorable search spaces low fire risk areas in this paper we detail a novel approach that employs a quadtree based search strategy to adaptively identify as many high fire risk areas as possible for any fire weather within a given time frame referred to as planning time hereafter for clarity the proposed adaptive approach can help harness the benefits of a quadtree based search strategy in any system single or multi machine and produce an increasingly detailed fire risk map over time additionally we incorporate the concept of conditional probability to estimate the likelihood of a fire turning high risk before running any simulation which saves more time as illustrated in one of our previous works ujjwal et al 2022 in the proposed approach we define three different search methods based on the chess moves bishop rook and queen to define how to move finer into quadtree based search operations all low fire risk areas are discarded while moving finer to prioritize high fire risk areas after a level in the quadtree based search strategy referred to as drop level moreover we present a comprehensive performance analysis of the approach with search methods and demonstrate how the proposed approach can alternate between the methods to balance the trade off between the total number of identified high fire risk areas defined as the coverage of high fire risk areas hereafter and the planning time furthermore we apply the approach to the entire tasmanian region to validate its efficacy 2 methods 2 1 problem description for any given geographical region r with n different possible fire start locations the problem of high fire risk areas identification can be stated as a problem of maximizing the total number of identified high fire risk areas f i within the operating constraints of time t response time and computational resources p the risk metric r f i for f i should be greater than a threshold t h which identifies the possible fire damages as high risk mathematically it can be expressed as 1 max c s t j 1 2 p t p j t c i f i c n f i 0 if r f i t h 1 if r f i t h where t p j is the time for which the computational resource p j runs system time there is a non trivial trade off between the time and the coverage maximum number of the identified high fire risk areas solving the defined problem would give a sub optimal solution with a balanced trade off between the time and the coverage for the operating constraints of time and computational resources 2 2 proposed adaptive approach we chose a quadtree based search strategy in our proposed approach to solve the described problem due to its ability to quickly identify the desired search results by eliminating unfavorable options in the proposed approach the search strategy starts with a bigger space and divides it into four equal smaller spaces at each level to keep exploring finer an example of the quadtree based search strategy in the tasmanian region is shown in fig 1 where the search operation focuses on an identified high fire risk start location represented by a yellow dot for the sake of clarity these high risk areas are identified at different levels mostly at coarser levels during a quadtree based search operation and more high risk areas at finer levels are searched around the high risk areas identified at coarse levels the proposed approach can be applied to any location or already identified high risk areas as starting points in the search operation to apply the quadtree based search strategy we represent the geographical area with grid points where each point resembles a possible fire start location in the shallowest level of the quadtree based strategy space is represented by four corner points consequently dividing the space into four equal smaller spaces in the following level is equivalent to finding the neighbors of the corner points where the distance between the points and neighbors keeps changing at each level at level 0 the search strategy finds the neighboring points around the yellow dot at the farthest distance as shown by the largest grid in the figure with the increase in the value of the level closer neighboring points are determined with the distance decreasing the closer neighboring points are determined based on three chess moves and the three methods are named accordingly bishop rook and queen shown in fig 2 the same approach is followed for all the identified high fire risk start locations before running a fire simulation at any point the likelihood of fire risk is estimated using the calculated conditional probability the simulations are run only if the likelihood is more than a certain value 30 in our study all the identified low fire risk areas are also discarded after the search operation reaches the drop level to prioritize the identified high fire risk areas as a result the search strategy finds the high fire risk areas from a coarse to a finer resolution based on the resources time and computation available at various time steps within the given response time a fire start location is labeled as a high fire risk location based on the threshold for the risk metric the identification of high fire risk areas in the proposed model is algorithmically represented in algorithm 1 in the algorithmic representation l e v e l is the drop level in the quadtree based search t is the response time m e t h o d is the method chosen to find the neighbors to any point f h p is the list of identified high fire risk areas d p is the list to check and ensure the simulations are not repeated on the same point p h x i f w k is the likelihood of a fire starting at location x i turning high risk and g w is the width of the grid 2 3 calculation of conditional probability we used naive bayes theorem berry 1996 to estimate the likelihood p h x i f w k of a fire starting at a location x i under any fire weather condition f w k to turn high risk with data collected during our experiments accordingly p h x i f w k can be expressed as follows 2 p h x i f w k p x i h p f w k h p x i f w k p h 2 4 test of significance for experimental results to verify the statistical correctness of the experimental findings we drew 30 random samples for the methods under comparison and conduct a wilcoxon test for a 95 confidence interval we formulated the null hypothesis as the proposed approach is as good as the random or sequential search operation and the alternative hypothesis as the proposed approach has better performance than the random or sequential search operation as such we compared the calculated p value p against the standard value of 0 025 for the one tailed test and accept the null hypothesis if p 0 025 or reject the null hypothesis and support the alternative hypothesis if p 0 025 2 5 test setup 2 5 1 study area we chose tasmania as a test region for several reasons firstly tasmania is one of the australian regions with frequent wildfires during summer 841 wildfires in the 2018 2019 wildfire season with 310 311 hectares of area burnt by wildfires tasmania fire service 2019 secondly tasmania has high quality land data sets available from the tasmania fire service tfs and state emergency service ses tasmanian department of primary industries parks water and environment tasmanian vegetation monitoring and mapping program 2013 lastly tasmania has a well studied and systematic grid configuration for possible fire start locations within its entirety where fire simulations can easily be run with existing configurations tasmania fire service 2019 all the fire simulations were run to simulate the fire behaviors for five hours after the fires started 2 5 2 wildfire simulations and data for this study we used the wildfire modeling framework spark miller et al 2015 as the fire simulation tool spark is a flexible platform for simulating wildfires that allows different types of fire behavior to be defined using scripts including rates of spread in different fuel types firebrand dynamics and risk metrics for fire impact and severity calculations in spark are parallelized using the opencl framework to enable the efficient execution of the simulations fire simulations in spark typically require several input data sets for the fire behavior models including maps of the land classification fuel type topography fuel information and meteorological data since the focus of our work was the fire weather defined by the combination of air temperature wind speed and relative humidity we considered only those inputs as variables for the fire simulations the cumulative area burned by fires at the end of the simulation period was the output metric all the fire simulations were run to simulate the fire behaviors for five hours after the fires started the input configuration file used along with the simulation outputs is available on a cloud repository kc et al 2021 2 5 3 experimental platform to harness the benefits of advanced computing technology with parallel operations we utilized a cloud based framework as detailed in our previous work ujjwal et al 2020b to run the fire simulations the framework was developed over the cloud infrastructure of nectar cloud the simulations were run on m3 large instances with 8 vcpus 16 gb ram and 30 gb memory ubuntu 16 04 lts xenial amd64 2 5 4 comparable systems we compared the performance of the proposed approach against a conventional comprehensive sweep system and two different search operations random and sequential in a conventional comprehensive sweep system fire simulations are run for all the possible fire start locations in such a system the order in which the locations are picked for running the fire simulation is not important as all the possible locations need to be covered for a random search operation fire simulations are run one after another at locations picked randomly within a given response time in a sequential search operation the first location to run the fire simulation is picked randomly and the locations thereafter are chosen based on the incremental value of the point id the identifier for locations as the labeling of the locations with point id values has been done by tfs based on a pattern it is more likely that the high fire risk areas are concentrated at a particular region and thus the sequential search can be considered as a strategy with some prior information 2 5 5 fire weather the input data sets included air temperature relative humidity and wind speed for calculating the fire rate of spread in the spark simulations ujjwal et al 2020b based on the setup and results of the same work we consider the discretized ranges high medium and low for the values for parameters the permissible range and the discrete labels assigned based on the values of the factors are summarized in table 1 the range and discretization carried out here can simply be altered and adapted to suit any similar analysis 3 results 3 1 application to the tasmanian region we used a grid of 256 256 to represent 65536 different possible fire start locations within tasmania the high fire risk areas as identified by the proposed approach for the fire weather hhh are shown in fig 5 along with the result of a conventional comprehensive sweep where a threshold of 1000 hectares was used to label a start location as a high fire risk area the fire weather condition hhh is the condition when the factors temperature relative humidity and wind speed have the highest influence on fire growth the proposed approach using the bishop method was able to identify 23727 out of 36346 high fire risk areas about 66 coverage in about 30 less system time than that of a conventional comprehensive sweep in a single system similarly the proposed approach with rook and queen methods was able to find 26621 high fire risk areas about 74 in about 20 less system time and 35166 high fire risk areas about 97 in about 4 less system time when compared to a comprehensive search strategy in a single machine for a multi machine cloud system our proposed approach maintained the same coverage in about 35 bishop and 26 rook less system time for the method queen the proposed approach took about 30 more time than the conventional system fig 4 shows the adaptive identification of high fire risk areas with the method bishop from a coarse to a finer resolution at different time instants until an hour mark specified planning time as seen from the figure the proposed approach was able to identify 134 high fire risk areas in the first ten minutes with a total of 200 453 568 932 and 1072 high fire risk areas in each 10 minute time step until the hour mark the findings for other methods are listed in table 2 the proposed approach was able to identify more high fire risk areas with the queen method 3 2 performance analysis of the proposed approach 3 2 1 fire weather fig 5 a shows the performance high fire risk area coverage of the proposed approach for 27 different fire weather combinations drop level 6 the fire weather lll as included in the plot indicates the fire weather with low influences of temperature relative humidity and wind speed on the fire growth as described in table 1 the fire weather hhh indicates the high influence of the parameters on the fire growth as seen from the figure the proposed approach performed better when the fire weather was highly favorable for the fire to grow with a high possibility of more high fire risk areas when the number of possible high fire risk areas was low the queen method performed the best while the methods bishop and rook performed satisfactorily with a minimum of about 40 coverage for fire weather favorable for fire growth the proposed approach performed satisfactorily with all the methods the queen was found to be the best performing method within the proposed approach for most of the fire weather conditions except for the lll and mll fire weather conditions in which bishop and rook respectively performed the best in our analysis the maximum coverage of the proposed approach with three methods stood at about 83 88 and 99 respectively 3 2 2 coverage fig 5 b shows the variation of the high fire risk area coverage with the change in the value of the drop level as seen from the figure the queen method was more efficient than the two other methods as the minimum coverage of high fire risk areas with the method was about 84 while the same for the bishop and rook methods stood at about 23 and 30 respectively at drop level of two for the queen method the proposed approach started covering more than 90 of the possible high fire risk areas after the drop level of three for a drop level of seven the proposed approach achieved coverage of about 83 and 88 with the methods bishop and rook respectively 3 2 3 time efficiency fig 5 c represents the proportion of the system time that can be saved with the proposed approach when compared to a comprehensive sweep of all the fire start locations at different drop levels the proposed approach was the most time efficient when the bishop was used in finding the nearest neighboring locations for any high fire risk area while moving finer in the search operations the time efficiency with the proposed approach decreased with the increase in the value of the drop level the proposed approach took as much as the time taken by a comprehensive swap at drop levels of 8 bishop and rook and 6 queen when all the possible high fire risk areas were identified 3 2 4 trade off between time efficiency and coverage fig 5 d shows the analysis of the trade off between the coverage and the time efficiency within the proposed approach which can be further synthesized to determine the best way to use the proposed approach the proposed approach can cover a larger fraction of all the possible high fire risk areas when there is a long planning time left for coordinating preparedness activities for fire management for a balanced trade off between the coverage and the system time efficiency the proposed approach should be used with either the bishop method or the rook method as the proposed approach can cover over 80 of the high fire risk areas with about 20 time efficiency the proposed approach has a balanced linear trade off between the coverage and the time efficiency with an average of x coverage of the high fire risk areas in about y less time than a conventional sweep provided x y 100 when three methods are used in a complementing manner 3 2 5 comparison against a random and a sequential search operation fig 6 shows the comparison of the coverage of the proposed approach against a random search operation carried out for a total duration of an hour against a random search operation the proposed approach started off the search operation without any prior information on possible high fire risk areas and still performed better for almost all the fire weather conditions except the ones characterized by l l l m l l h l l for the queen method the mean of the coverage was less than that of the random and sequential ones for the weather condition h h h for all the weather conditions where the proposed approach performed better the calculated values of p in the wilcoxon test were extremely smaller than 0 025 thereby statistically verifying the superior performance of the proposed approach over a random and a sequential search operation the coverage of the queen method was not statistically better than the random and sequential search for the fire weather mhh and so was the case with the bishop and the rook method for the fire weather hhh 3 2 6 high fire risk area identification with multi machine system fig 7 depicts the total number of high fire risk areas identified by the proposed model with the rook method for a multi machine system for the fire weather condition identified by f w t h r h w h as seen from the figure the proposed approach can identify up to 1469 out of 36346 high fire risk areas in the first hour in a system with 100 parallel machines when operated with a multi machine system with about 2000 machines the proposed approach can cover more than 72 of the total high fire risk areas such a system which can be realized with cloud infrastructure ensures the best use of the proposed approach as a large number of possible high fire risk areas can be rapidly identified for better wildfire management 4 discussion during the approach application the entire grid space represented the tasmanian region at level 0 for any fire weather f w i t i r i w i at level 0 the proposed approach runs the file simulations at locations represented by the corner points in the grid the proposed approach explores finer with a unit increment in level by finding the neighboring points the approach stops its search operation in one of the two conditions whichever is earlier the first case when the time constraint planning time given to the approach ceases and the second case when the maximum depth level the level by which the quadtree based search operation covers all possible points is reached level 8 here in our approach application once the search operation is over the approach maps the grid points to the physical geographical locations in the tasmanian region along with the actual possible fire burnt area for all the identified high fire risk areas better search coverage was obtained with the queen method due to more eight neighbors around identified high fire risk areas taking more surrounding points around an identified high fire risk area minimizes the miss of possible high fire risk start locations for a multi machine cloud system the proposed approach with the queen method could cover all the high fire risk areas but with 30 more time than the conventional system this finding can be attributed to the necessity of finding eight neighboring locations around a high fire risk area that results in several same neighboring locations for multiple high fire risk areas a large number of the same neighboring locations for multiple locations can influence how the simulations are distributed among the multiple machines and incur longer execution times such shortcomings with the queen method can be overcome by intelligently distributing the simulations at each level within the search operation the proposed approach was also able to produce a more detailed map of high fire risk areas evolving with time for a given planning time of an hour such predictive information at coarse level resolution in quick time can help fire authorities to stay alert for better preparedness against an unfolding fire disaster in our previous studies ujjwal et al 2020b 2021a the relative humidity and the wind speed have been shown to have a higher influence on fire growth while the temperature was found to have a lesser influence for weather conditions favorable for wildfire growth high temperature wind speed and low relative humidity the number of possible high fire risk areas is high and vice versa while analyzing the approach performance in various fire weather conditions the performance is the worst at the weather conditions when there are a few possible high fire risk areas this worst performance is so because the method keeps dropping the low fire risk areas before the search operation reaches the deepest level such a method within the approach could miss a few high fire risk start points located at a finer level of a quadtree based search operation the worst performance of the proposed approach at these conditions explains why the proposed approach was statistically less efficient than a random and sequential search operation as a result the proposed approach should not be used for the fire weather conditions where it has been found to be statistically nevertheless the coverage achieved by the proposed approach can be improved by increasing the value of the drop level and prioritizing the queen method for the fire weather conditions where fires could grow quickly and burn massive areas the performance is quite good with as high as about 99 coverage we also demonstrated how to use the proposed approach in a multi machine system local or cloud for rapidly identifying high fire risk areas for effective wildfire management fire behaviors change drastically with fire weather conditions and thus predicting the fire behaviors without running fire simulations can be a difficult task additionally some wildfires can grow to burn thousands of hectares in a few hours and simulating those fires can significantly take longer while running ensembles in a multi machine system several machines may stay idle while a few machines are still running the batches simulating larger fires consequently optimizing computing resource utilization while running ensembles of fire simulations in a multi machine system is still an open challenge the proposed approach can minimize the number of simulations in analysis but has to be coupled with methods to efficiently distribute the simulations among the machines in a multi machine system 5 conclusions and future works in this paper we proposed a quadtree based adaptive approach that practitioners can use in existing systems single or multiple machines to identify high fire risk areas quickly within the desired response time during emergencies for better decision making we validated the efficacy of the proposed approach by applying it to the tasmanian region with a proof of concept system to identify such high fire risk areas at different time steps additionally we also conducted a thorough performance analysis of the proposed approach and its search strategies the experimental results showed that the proposed approach handles the trade off between the coverage of high fire risk areas and system time better in any system moreover compared to conventional comprehensive sweep in ensembles the proposed approach was able to identify more than 80 of high fire risk areas in about 20 less time thus our investigative effort has demonstrated that search strategy in ensemble predictions can help quickly identify high risk areas with statistically better results than random and sequential methods additionally the proposed approach is flexible too with the value of the threshold to define a high fire risk area the system to realize the approach the number of machines in the system the drop level for low fire risk areas and even the disaster simulation framework flood simulation tool or others easily changeable for comparable approach performance the increasingly detailed risk map produced from the proposed approach could be highly useful in urban settings for better preparedness and response against fire emergencies as updated alerts can be relayed at regular intervals with increased precision we expect the proposed approach to be useful for fire authorities to quickly grab some crucial predictive information during emergencies and make better informed decisions to minimize the losses conceded to unwanted wildfire occurrences as a future work the proposed approach can be extended and applied to examine if the quadtree steps into urban regions and quickly identify the imminent risks around urban edges similarly the proposed approach can start search operations in urban regions and build risk maps around the regions software availability the fire propagation tool spark on which the fire simulations required for this study was run is developed by data61 csiro and can be downloaded from https research csiro au spark the fire simulation data set used for this study is accessible at https data csiro au collections collection cicsiro 49133v1 ditrue kc et al 2021 uncited references fig 3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25463,this paper compared simulation results from different swmm model constructions after radiological contamination from a hypothetical dirty bomb the model constructions included a utility provided combined sewer model a high resolution mesh surface flow model and a hybrid of the two the models were run under three different rainfall configurations including a 3 h event a 24 h event and a 24 h event with 48 h of drainage to investigate the influence that model structure had on transport of 137cs model structure was found to impact the contaminant loading of the stormwater model from the air modeling it was also found to impact the amount of contaminant washoff high resolution modeling produced more washoff flow of contamination through outfalls more contamination through outfalls connected to underground pipes and 137cs levels per pipe the implications of these findings are that few stormwater models are considered ready to aid in response and recovery as originally designed keywords stormwater modeling radiological emergency response data availability the authors do not have permission to share data 1 introduction the potential for nuclear power plant accidents nuclear war and terrorist attacks using a radiological dispersal device necessitate the development of models to predict radionuclide transport in urban stormwater kaiser 2012 mikelonis et al 2021 ratliff et al 2020 model predictions are intended to aid in the sampling decontamination and waste management of urban infrastructure if contaminated by long lasting radionuclides kaminski et al 2016 mikelonis et al 2018 stormwater models can be constructed with varying degrees of spatial resolution and infrastructure data depending on available datasets computational resources and remediation regulatory objectives anni et al 2020 guo et al 2021 mignot and dewals 2022 shrestha et al 2022 a better understanding of the sensitivity of simulation results to these variables is necessary for decision makers to interpret model outputs and decide if time and resource investments are warranted to build new or modify existing stormwater models the goal of this research was to identify differences in impacted urban infrastructure locations due to the use of varied stormwater model configurations increased knowledge of these differences will make the use of these models more robust for their intended role in remediation activities such as sampling waste staging and installation of treatment technologies stormwater washoff of radionuclides from surfaces contaminated via aerosol deposition was reported following the chernobyl and fukushima nuclear power plant accidents garcia sanchez 2008 garcia sanchez and konoplev 2009 konoplev 2016 konoplev et al 2002 2016 washoff data were primarily synthesized into transfer functions that described rapid and slow washoff in a first order decay format or as entrainment coefficients in the format of solid liquid partition coefficients monte et al 2004 the main application of experimentally derived radionuclide washoff coefficients has been in compartment models predicting radionuclide loadings into waterbodies such as lakes and rivers monte et al 2004 in urban systems lumped land use type models have been developed that account for urban drainage systems and removal of radionuclides by decontamination activities but consist of coarse spatial resolution and temporally aggregated monthly rainfall timeseries as opposed to using event based rainfall molina et al 1999 urso et al used a fully distributed event based approach using the kanal stormwater modeling software to perform hydraulic and hydrological calculations and estimated radiological loads from a notional dirty bomb explosion by multiplying the generated stormwater runoff volumes by solid liquid partition coefficients and radioactive decay for each radionuclide urso et al 2013 this approach assumed a fixed proportion of washoff between the surface and the stormwater alternatively others have used the u s environmental protection agency s stormwater management model s swmm exponential washoff function to capture the first flush and plateau of radionuclides washed from the surface according to simulated runoff rate ng 2019 shireman et al 2022 these studies served as proof of concept for radionuclide transport simulation by an urban stormwater model and did not examine the impact of model structure on contaminant transport such as investigated here flood modeling research has critically examined the impacts of model structure on simulation results researchers have studied the impact on flood water depth extent and or duration using 1 dimesional 1d pipe networks 1d and 2d representations of overland flow and various combinations of 1d 2d and quasi 2d dual drainage models barnard et al 2007 leandro et al 2009 a 1d 1d underground surface modeling approach is typically limited to confined channels such as a narrow river but with intentional application may supply comparable data to some 1d 2d models leandro et al 2009 in situations such as flow overtopping curbs or change of flow direction at intersections a 2d representation of surface flow was critical to accurately represent the processes leandro et al 2009 depth and area extent of flood waters have been found to be larger for models that did not include stormwater infrastructure compared to models that included a pipe layer due to the extra storage capacity the pipe network provided anni et al 2020 shrestha et al 2022 for models with similar infrastructure lower model resolution of the 2d layer resulted in underestimating flood depth and overestimating flood extent and volume the estimation difference was tied to the ease at which overland flow water could re enter the drainage system in the higher resolution model shrestha et al 2022 while a high resolution 2d surface flow model coupled with a 1d pipe network may appear ideal a higher resolution model comes with the tradeoff of computational expense noh et al 2018 depending on the size of the study area and the rainfall simulation period it may take hours to weeks to complete one model run time that is unavailable during an emergency approaches such as variable grid sizing and parallelization may be used to counteract the computational burden but are not easy to implement in all softwares kesserwani and liang 2012 neal et al 2010 ultimately the project modeler must assess existing models available infrastructure data available hardware and software and the project timeline to construct the optimal tool stormwater models role in the recovery from a wide area radiological contamination incident may be diverse models hold the potential to help answer questions such as where is washoff incomplete and leaving hotspots of surface contamination where are contaminants entering into the drainage system streams or lakes after rainfall events does stormwater runoff spread contamination and should waste staging locations be in areas that produce less runoff stormwater models may identify locations that samples should be collected to verify field conditions including at water outfalls and on urban surfaces this paper compares the differences in simulation results after radiological contamination from a dirty bomb due to different model constructions including a utility provided combined sewer stormwater model a surface flow model lacking buried pipe infrastructure and a hybrid of the two these configurations were selected for the following reasons 1 an existing model is ready to use immediately in a disaster by the model owner and usually accounts for major engineered transport pathways e g buried pipes and pump stations 2 a surface flow model is faster to construct with tailorable resolution to aid in remediation tasks such as surface sampling it can be assembled more easily by outside parties because the data inputs are typically more accessible than underground infrastructure and 3 a hybrid model which includes both engineered infrastructure and tailorable resolution by highlighting differences in impacted infrastructure using different model configurations this work aims to assist stormwater practitioners in making more informed modeling decisions as they approach in their applications 2 methodology 2 1 scenario and model configurations this paper expands upon our work published in shireman et al 2022 where a hypothetical downtown detonation point for a radiological dispersal device was selected within the drainage area of the great lakes water authority s glwa sewerage network and a radionuclide deposition plume developed using unobstructed dispersion modeling from the interagency modeling and atmospheric assessment center imaac was supplied as input into the stormwater model the same data sources were used but the glwa calibrated stormwater model was the 2018 version as opposed to the 2013 version epa s stormwater management model swmm version 5 1 015 was implemented using a proprietary software package pcswmm 2d v7 computational hydraulics international chi guelph ontario canada swmm is a widely used urban stormwater software it has been validated with field observations in many studies niazi et al 2017 the glwa stormwater model subcatchments ranged in size from 0 002 to 0 9 km2 with an average of 0 09 km2 and a standard deviation of 0 08 km2 additional information about the subcatchments i e percent imperviousness slope elevation may be found in great detail in shireman et al 2022 pcswmm 2d was instrumental in model configurations that included finer scale subcatchments and the addition of virtual dual drainage modeling components such as conduits junctions and nodes to simulate surface flows and connect to the underground pipes the virtual components are not physically present on the landscape but rather used in the software to provide quasi 2d modeling where 1d solutions in many locations aid in a 2d visualization as stated in shireman et al 2022 the spatial model domain was very large so for finer scale modeling a variable sized mesh was constructed in pcswmm 2d a fixed rectangular mesh of 12 m was used for roadways where grided sampling is more apt to be performed and a variable nested hexagonal mesh of 12 m 48 8 m was used with finer resolution components situated closer to the epicenter of the blast subcatchment resolution on the scale of 0 1 m2 is required to approximate 2d modeling in pcswmm 2d and was not tested in this research due to the computational burden not easily implemented during an emergency the 137cs removal was modeled using swmm s exponential washoff curve eq 1 with coefficient c 1 of 2 45 0 56 and 0 13 and c 2 of 1 25 1 23 and 1 20 for buildings roads and other urban areas respectively coefficient selection is explained in greater detail in shireman et al 2022 but in short 137cs was assumed to sorb to solids and the coefficient values represent average values for total suspended solids washoff 1 w t p o e c 1 q t c 2 t where w t is the quantity of constituent remaining on the material surface po is the initial quantity on the surface c1 and c2 are empirically fitted coefficients q t is runoff rate and t is time the following model configurations were examined in this paper 1a glwa model 1b modified glwa model 2 hybrid model and a 3 high resolution mesh hrm model configurations 1a and 2b were 1d pipe models with traditionally sized subcatchment drainage areas fig 1 a the models differed only in that the subcatchments along the boundary of the radiological deposition plume were split to achieve better agreement between the estimated deposited radioactivity by the air model and the total modeled activity when applying the washoff loadings model 1b was used for comparison with models 2 and 3 as it had a similar starting amount of radiological contamination model 2 the hybrid model consisted of the original glwa pipe network the hrm model for the overland flow at the epicenter of the blast and the glwa subcatchments in the distal areas of the model the hrm was connected to the glwa infrastructure at the outlet nodes of the glwa subcatchments fig 1b model 3 was the hrm model alone and did not include an underground pipe network fig 1c or low radioactivity level contours from the ground deposition plume integrated into the stormwater model because they fell outside the model domain for the hrm model 20 outfalls were copied along with their connecting conduits from the glwa model along the river edge of the model domain and connected to the nearest node of the 2d mesh the models were run using three different rainfall configurations based on a real storm that occurred on september 25 2018 the 3 h event totaled 24 mm of precipitation and contained a 1 h peak rainfall intensity equivalent to a 1 h peak rainfall for the 2 year return frequency the 24 h event totaled 50 mm of precipitation and was representative of a 24 h 2 year return frequency storm event for detroit when modeling the 3 h and 24 h events no post rainfall periods were included in the simulation runtime the 72 h simulation was the 24 h rainfall event and 48 h of post rainfall dry weather to track any lagging movements of water and contaminants through the system horton was used as the infiltration method dynamic wave as the flow routing method and extran as the surcharge method these solver methods are explained in detail in rossman 2015 the model was run with a 0 25 s routing time step and a 15 min reporting timestep surface concentrations of 137cs were applied to the subcatchments from a ground deposition air modeling plume using the area weighting tool in pcswmm 2d fig 2 this tool computes a weighted average of an attribute of interest for a feature in a target layer from intersecting features of a source layer this led to more radioactivity being applied to the stormwater model than contained in the plume and differences in starting radioactivity levels between the models model 1a contained 61 more radioactivity than the ground deposition plume model 1b 19 model 2 20 and model 3 10 more due to these discrepancies model 1a was not used for further comparisons and contaminant values were normalized by starting concentration for comparisons between models for accurate conservation of radioactivity levels from the deposition plume to the stormwater model subcatchment resolution needs be modified so that the size of the subcatchments match the border of the plume even if contaminant loadings were manually adjusted along the border of the plume so that concentrations in the stormwater model and air dispersion models matched the runoff generated by the areas of the subcatchments would be scaled inconsistently the runoff water quality error was 0 001 for all three scenarios and all three rain events 3 results 3 1 subcatchments the total fraction of 137cs that washed off the subcatchments varied depending on the model configuration and the simulation event on average the lower accumulation shorter rainfall duration 3 h event had less total washoff than the higher accumulation longer 24 h event for all model configurations the longer system drainage time for the 72 h simulation did not make a discernible difference in surface washoff compared to the 24 h event without drainage time table 1 the modified glwa model 1b consistently washed off less than the hybrid and hrm models for the 3 h rain event model 1b had 14 lower washoff than the hybrid model and 17 lower than the hrm percentages were calculated as differences of subcatchment values in table 1 for the 24 h and 72 h events model 1b washed off 8 and 9 less than the hybrid and hrm respectively overall the hrm model washed off the most 137cs but was only 5 percent higher than the hybrid model regardless of the simulated event the spatial distribution of 137cs remaining after the 24 h event is visualized for each model configuration in fig 3 where the hrm and hybrid models overlap geographically the results were the same the hybrid model used unmodified glwa subcatchments in the distal part of the model domain and therefore does not align with the modified glwa subcatchments for direct comparison 3 2 outfalls the modified glwa model contained 150 outfalls the hrm model 20 outfalls and the hybrid model 161 outfalls 11 of the hrm outfalls remained in the hybrid model but transferred flow into the buried pipe network a lower total percentage of radioactivity was released through the outfalls for the hybrid model than the modified glwa model 14 38 and 36 lower for the 3 24 and 72 h events respectively this was most likely due to more of the 137cs held in the 2d conduits the modified glwa model and the hybrid model contained buried underground pipes that collect water from the entire model area and convey large amounts of water through two terminal outfall locations outside of the bounds of the hrm model this combined with the fact that like the hybrid model the hrm also held 137cs on the surface instead of routing through outfalls resulted in very little release of radioactivity through the outfalls for the 3 hour event and only 1 of the starting deposition for the 24 hr and 72 hr events the location of the highest radioactivity outfall was geographically the same for the hybrid and modified glwa model configurations and represented 43 73 of all radioactivity discharged table 1 when comparing only the 20 outfalls in all three of the models the location of maximum discharge did vary this means that the model configurations do not drastically alter the destination routing in a conveyance system model but can alter localized drainage locations 3 3 pipes each model configuration contained a different combination of virtual and physically present pipes this caused the location exhibiting the highest pipe flux of radioactivity to differ based on model construction compared to the modified glwa model the location exhibiting the highest flux differed by 41 and 75 less radioactivity in the hrm and hybrid models respectively this is related to the hrm and hybrid models having the capacity to hold water on the surface in the virtual pipe network at a finer scale whereas the glwa models must eventually transfer the loads to the underground conveyance system even if ponding is enabled in swmm the pipe exhibiting the highest flux of radioactivity in model 1b had 100 of the 137cs washoff routed through it during the 72 h event simulation for the same simulation the hybrid model pipe exhibiting the highest flux of radioactivity saw 63 of the total washoff flow through it whereas the hrm model only saw 40 of the total radioactivity in addition to holding water on the surface the virtual 2d pipes also allowed additional flow pathways which redistributed flow directions and allowed for the visualization of surface drainage pathways fig 4 b the hybrid model shared surface pipes with the hrm model and underground pipes with the glwa models by comparing the differences in contaminant flux during the simulation between the shared components it is possible to see the locations in the model where the interplay between the surface and the underground pipe flows causes differences in radioactivity fig 4 in certain areas the presence of the underground pipe network causes differences in the flux modeled on the surface and these locations are generally in the same locations when comparing overlapping underground locations with overlapping surface locations for example along the drainage pathway directly southeast of the detonation there is a difference in pipe flux of 13 322 gbq depending on if you use the hybrid vs modified glwa model and a 86 221 gbq surface flux difference along the same pathway when comparing the hybrid vs hrm model overall however the magnitude of these radioactivity differences are isolated to 7 distinct areas of surface drainage pipe interplay and there is a 322 gbq difference in pipe network fluxes excluding the furthest downstream line of the pipe network 4 discussion using ground deposition plumes from atmospheric pollutant models as input into watershed models has historically run into challenges such as differing grid cells and definition of dry deposition removal from the atmosphere vs amount depositing on the watershed available for washoff and is model specific on both the air and water side burian et al 2001 hong et al 2017 stormwater subcatchments are traditionally delineated based on topography and known drainage to infrastructure and will not necessarily overlap precisely with the shape of the deposition plume the research presented in this paper indicated that directly loading a hypothetical ground deposition plume from imaac into an existing swmm subcatchment array has the advantage of a rapid response but there is a potential for overestimating the contaminant load 61 more radioactivity than the air model deposition plume the magnitude of the overestimation is dependent on the size of the subcatchment and the path of the plume but for km2 sized subcatchments typical of many off the shelf stormwater models the error may be substantial over double the amount this mass overestimate can be managed by splitting the stormwater model subcatchments where significant portions of the subcatchment would be outside of the plume footprint and is most important from a mass balance standpoint in areas of highest contamination simply scaling the loading amount would not account for the extra runoff generated by the uncontaminated part of the subcatchment which can lead to inaccurate calculations of contaminant concentrations due to finer spatial detail developing a hrm has the advantage of better representing the contaminant mass loads without needing to split subcatchments along the path of the plume although one could split hrm subcatchments to exactly map the predicted footprint the air deposition results are also from a model so this level of precision is not necessarily physically accurate and potentially only worth the effort if the values are from field measurements e g a gamma spectroscopy aerial monitoring system for 137cs as observed in shireman et al 2022 within each model more total runoff from a subcatchment corresponded to greater washoff total runoff did not correlate with the size of a subcatchment but it did relate to how land use was classified higher impervious values leading to greater runoff greater washoff from a specific subcatchment was also related to land use classification because different runoff coefficients were used for roads buildings and impervious areas between models it was observed that finer spatial resolution resulted in more overall washoff of the contaminant plume washoff parameters have been shown to better hold their physical meaning at the small scale bonhomme and petrucci 2017 and micro delineation of subcatchments has been shown to reduce uncertainty in flow predictions sun et al 2012 peak flows have been shown to be influenced by subcatchment scale with different results for larger storms reduction in peak flow and smaller storms increase of peak flows caused by differences in infiltration overland flow and conduit routing at different scales cao et al 2020 ghosh and hellweger 2012 pollutant predictions are influenced by accuracy of flow predictions e g dilution and have also been shown to be sensitive to spatial scales and vary by pollutant dai et al 2018 with all three model configurations it is possible to visualize areas that will be more or less intensely affected by the release and subsequent washoff but on very different scales areas of finer resolution may be more suitable for remediation activities such as targeted sampling of contamination hotspots and deciding where to stage waste generated during remediation and recovery efforts such that it does not contaminate sensitive areas there is a distinct difference in how runoff is handled between the glwa and hybrid model hrm subcatchments in the glwa model and glwa subcatchments included in the hybrid model runoff is aggregated and enters the swmm stormwater conveyance system directly at the model subcatchment outlet nodes in the hrm mesh areas of the hybrid model and the hrm model runoff for each cell enters the 2d conduit mesh at a 2d node and is conveyed through the 2d system via the 2d conduits simulating overland flow this difference had important implications because the 2d conduit system represents a secondary flow system and behaved as temporary storage during a model run the 2d conduit system contained substantially more components and therefore allowed for more flow paths and lower radioactivity levels per pipe also since the hybrid and hrm model were able to hold contaminant in the surface flow it was observed that less contamination flowed through outfalls 0 1 of total contamination for the hrm and 0 4 63 for the hybrid depending on storm event than in the glwa model configurations 14 99 if the stormwater model is being used to develop an outfall monitoring plan at a set threshold this is a significant finding to be aware of since model structure greatly impacts the number of outfalls indicated by modeling as being contaminated 5 conclusions this research investigated the impact of model structure on transport of radiological deposition from a dirty bomb explosion in an urban area model structure impacted the contaminant loading into the stormwater model from the air deposition modeling and found that unless the original model subcatchments were of sufficiently fine scale adjustments to the stormwater model are necessary to split the original subcatchments along the path of the plume model structure was also found to impact the amount of washoff high resolution modeling produced more washoff flow of contamination through outfalls more contamination flowing through outfalls with the underground pipe network and flux of radioactivity per pipe lower levels in the 2d conduit system the implications of these findings are that few stormwater models are considered ready off the shelf monitoring plans must be field verified with lower resolution models potentially more conservative on the number of impacted outfalls and radioactivity levels per pipe and that hybrid hrm models are necessary to visualize transport at a scale applicable to surface sampling and waste staging while stormwater models in any configuration require field verification and have known shortcomings they can also provide very useful information in predicting impacts of severe weather providing risk assessors with water quality ranges for dose assessments and deciding where to sample that aid in nuclear emergency response disclaimer the research described in this article has been funded wholly or in part by the u s environmental protection agency contract no 68herc19d0009 to aptim government services this manuscript was subject to administrative review but does not necessarily reflect the view of the u s environmental protection agency no official endorsement should be inferred as the epa does not endorse the purchase or sale of any commercial products or services credit authorship contribution statement anne mikelonis conceptualization methodology writing original draft writing review editing supervision project administration funding acquisition jonathan shireman methodology software validation formal analysis data curation writing review editing katherine ratliff conceptualization methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful to numerous contributors including radha krishnan of aptim sherri gee john norton wendy barrott and walter davis of the great lakes water authority and brian kelly of the usepa region 5 thank you to lifeng yuan and michelle simon for internal technical reviews and ramona sherman for quality assurance review 
25463,this paper compared simulation results from different swmm model constructions after radiological contamination from a hypothetical dirty bomb the model constructions included a utility provided combined sewer model a high resolution mesh surface flow model and a hybrid of the two the models were run under three different rainfall configurations including a 3 h event a 24 h event and a 24 h event with 48 h of drainage to investigate the influence that model structure had on transport of 137cs model structure was found to impact the contaminant loading of the stormwater model from the air modeling it was also found to impact the amount of contaminant washoff high resolution modeling produced more washoff flow of contamination through outfalls more contamination through outfalls connected to underground pipes and 137cs levels per pipe the implications of these findings are that few stormwater models are considered ready to aid in response and recovery as originally designed keywords stormwater modeling radiological emergency response data availability the authors do not have permission to share data 1 introduction the potential for nuclear power plant accidents nuclear war and terrorist attacks using a radiological dispersal device necessitate the development of models to predict radionuclide transport in urban stormwater kaiser 2012 mikelonis et al 2021 ratliff et al 2020 model predictions are intended to aid in the sampling decontamination and waste management of urban infrastructure if contaminated by long lasting radionuclides kaminski et al 2016 mikelonis et al 2018 stormwater models can be constructed with varying degrees of spatial resolution and infrastructure data depending on available datasets computational resources and remediation regulatory objectives anni et al 2020 guo et al 2021 mignot and dewals 2022 shrestha et al 2022 a better understanding of the sensitivity of simulation results to these variables is necessary for decision makers to interpret model outputs and decide if time and resource investments are warranted to build new or modify existing stormwater models the goal of this research was to identify differences in impacted urban infrastructure locations due to the use of varied stormwater model configurations increased knowledge of these differences will make the use of these models more robust for their intended role in remediation activities such as sampling waste staging and installation of treatment technologies stormwater washoff of radionuclides from surfaces contaminated via aerosol deposition was reported following the chernobyl and fukushima nuclear power plant accidents garcia sanchez 2008 garcia sanchez and konoplev 2009 konoplev 2016 konoplev et al 2002 2016 washoff data were primarily synthesized into transfer functions that described rapid and slow washoff in a first order decay format or as entrainment coefficients in the format of solid liquid partition coefficients monte et al 2004 the main application of experimentally derived radionuclide washoff coefficients has been in compartment models predicting radionuclide loadings into waterbodies such as lakes and rivers monte et al 2004 in urban systems lumped land use type models have been developed that account for urban drainage systems and removal of radionuclides by decontamination activities but consist of coarse spatial resolution and temporally aggregated monthly rainfall timeseries as opposed to using event based rainfall molina et al 1999 urso et al used a fully distributed event based approach using the kanal stormwater modeling software to perform hydraulic and hydrological calculations and estimated radiological loads from a notional dirty bomb explosion by multiplying the generated stormwater runoff volumes by solid liquid partition coefficients and radioactive decay for each radionuclide urso et al 2013 this approach assumed a fixed proportion of washoff between the surface and the stormwater alternatively others have used the u s environmental protection agency s stormwater management model s swmm exponential washoff function to capture the first flush and plateau of radionuclides washed from the surface according to simulated runoff rate ng 2019 shireman et al 2022 these studies served as proof of concept for radionuclide transport simulation by an urban stormwater model and did not examine the impact of model structure on contaminant transport such as investigated here flood modeling research has critically examined the impacts of model structure on simulation results researchers have studied the impact on flood water depth extent and or duration using 1 dimesional 1d pipe networks 1d and 2d representations of overland flow and various combinations of 1d 2d and quasi 2d dual drainage models barnard et al 2007 leandro et al 2009 a 1d 1d underground surface modeling approach is typically limited to confined channels such as a narrow river but with intentional application may supply comparable data to some 1d 2d models leandro et al 2009 in situations such as flow overtopping curbs or change of flow direction at intersections a 2d representation of surface flow was critical to accurately represent the processes leandro et al 2009 depth and area extent of flood waters have been found to be larger for models that did not include stormwater infrastructure compared to models that included a pipe layer due to the extra storage capacity the pipe network provided anni et al 2020 shrestha et al 2022 for models with similar infrastructure lower model resolution of the 2d layer resulted in underestimating flood depth and overestimating flood extent and volume the estimation difference was tied to the ease at which overland flow water could re enter the drainage system in the higher resolution model shrestha et al 2022 while a high resolution 2d surface flow model coupled with a 1d pipe network may appear ideal a higher resolution model comes with the tradeoff of computational expense noh et al 2018 depending on the size of the study area and the rainfall simulation period it may take hours to weeks to complete one model run time that is unavailable during an emergency approaches such as variable grid sizing and parallelization may be used to counteract the computational burden but are not easy to implement in all softwares kesserwani and liang 2012 neal et al 2010 ultimately the project modeler must assess existing models available infrastructure data available hardware and software and the project timeline to construct the optimal tool stormwater models role in the recovery from a wide area radiological contamination incident may be diverse models hold the potential to help answer questions such as where is washoff incomplete and leaving hotspots of surface contamination where are contaminants entering into the drainage system streams or lakes after rainfall events does stormwater runoff spread contamination and should waste staging locations be in areas that produce less runoff stormwater models may identify locations that samples should be collected to verify field conditions including at water outfalls and on urban surfaces this paper compares the differences in simulation results after radiological contamination from a dirty bomb due to different model constructions including a utility provided combined sewer stormwater model a surface flow model lacking buried pipe infrastructure and a hybrid of the two these configurations were selected for the following reasons 1 an existing model is ready to use immediately in a disaster by the model owner and usually accounts for major engineered transport pathways e g buried pipes and pump stations 2 a surface flow model is faster to construct with tailorable resolution to aid in remediation tasks such as surface sampling it can be assembled more easily by outside parties because the data inputs are typically more accessible than underground infrastructure and 3 a hybrid model which includes both engineered infrastructure and tailorable resolution by highlighting differences in impacted infrastructure using different model configurations this work aims to assist stormwater practitioners in making more informed modeling decisions as they approach in their applications 2 methodology 2 1 scenario and model configurations this paper expands upon our work published in shireman et al 2022 where a hypothetical downtown detonation point for a radiological dispersal device was selected within the drainage area of the great lakes water authority s glwa sewerage network and a radionuclide deposition plume developed using unobstructed dispersion modeling from the interagency modeling and atmospheric assessment center imaac was supplied as input into the stormwater model the same data sources were used but the glwa calibrated stormwater model was the 2018 version as opposed to the 2013 version epa s stormwater management model swmm version 5 1 015 was implemented using a proprietary software package pcswmm 2d v7 computational hydraulics international chi guelph ontario canada swmm is a widely used urban stormwater software it has been validated with field observations in many studies niazi et al 2017 the glwa stormwater model subcatchments ranged in size from 0 002 to 0 9 km2 with an average of 0 09 km2 and a standard deviation of 0 08 km2 additional information about the subcatchments i e percent imperviousness slope elevation may be found in great detail in shireman et al 2022 pcswmm 2d was instrumental in model configurations that included finer scale subcatchments and the addition of virtual dual drainage modeling components such as conduits junctions and nodes to simulate surface flows and connect to the underground pipes the virtual components are not physically present on the landscape but rather used in the software to provide quasi 2d modeling where 1d solutions in many locations aid in a 2d visualization as stated in shireman et al 2022 the spatial model domain was very large so for finer scale modeling a variable sized mesh was constructed in pcswmm 2d a fixed rectangular mesh of 12 m was used for roadways where grided sampling is more apt to be performed and a variable nested hexagonal mesh of 12 m 48 8 m was used with finer resolution components situated closer to the epicenter of the blast subcatchment resolution on the scale of 0 1 m2 is required to approximate 2d modeling in pcswmm 2d and was not tested in this research due to the computational burden not easily implemented during an emergency the 137cs removal was modeled using swmm s exponential washoff curve eq 1 with coefficient c 1 of 2 45 0 56 and 0 13 and c 2 of 1 25 1 23 and 1 20 for buildings roads and other urban areas respectively coefficient selection is explained in greater detail in shireman et al 2022 but in short 137cs was assumed to sorb to solids and the coefficient values represent average values for total suspended solids washoff 1 w t p o e c 1 q t c 2 t where w t is the quantity of constituent remaining on the material surface po is the initial quantity on the surface c1 and c2 are empirically fitted coefficients q t is runoff rate and t is time the following model configurations were examined in this paper 1a glwa model 1b modified glwa model 2 hybrid model and a 3 high resolution mesh hrm model configurations 1a and 2b were 1d pipe models with traditionally sized subcatchment drainage areas fig 1 a the models differed only in that the subcatchments along the boundary of the radiological deposition plume were split to achieve better agreement between the estimated deposited radioactivity by the air model and the total modeled activity when applying the washoff loadings model 1b was used for comparison with models 2 and 3 as it had a similar starting amount of radiological contamination model 2 the hybrid model consisted of the original glwa pipe network the hrm model for the overland flow at the epicenter of the blast and the glwa subcatchments in the distal areas of the model the hrm was connected to the glwa infrastructure at the outlet nodes of the glwa subcatchments fig 1b model 3 was the hrm model alone and did not include an underground pipe network fig 1c or low radioactivity level contours from the ground deposition plume integrated into the stormwater model because they fell outside the model domain for the hrm model 20 outfalls were copied along with their connecting conduits from the glwa model along the river edge of the model domain and connected to the nearest node of the 2d mesh the models were run using three different rainfall configurations based on a real storm that occurred on september 25 2018 the 3 h event totaled 24 mm of precipitation and contained a 1 h peak rainfall intensity equivalent to a 1 h peak rainfall for the 2 year return frequency the 24 h event totaled 50 mm of precipitation and was representative of a 24 h 2 year return frequency storm event for detroit when modeling the 3 h and 24 h events no post rainfall periods were included in the simulation runtime the 72 h simulation was the 24 h rainfall event and 48 h of post rainfall dry weather to track any lagging movements of water and contaminants through the system horton was used as the infiltration method dynamic wave as the flow routing method and extran as the surcharge method these solver methods are explained in detail in rossman 2015 the model was run with a 0 25 s routing time step and a 15 min reporting timestep surface concentrations of 137cs were applied to the subcatchments from a ground deposition air modeling plume using the area weighting tool in pcswmm 2d fig 2 this tool computes a weighted average of an attribute of interest for a feature in a target layer from intersecting features of a source layer this led to more radioactivity being applied to the stormwater model than contained in the plume and differences in starting radioactivity levels between the models model 1a contained 61 more radioactivity than the ground deposition plume model 1b 19 model 2 20 and model 3 10 more due to these discrepancies model 1a was not used for further comparisons and contaminant values were normalized by starting concentration for comparisons between models for accurate conservation of radioactivity levels from the deposition plume to the stormwater model subcatchment resolution needs be modified so that the size of the subcatchments match the border of the plume even if contaminant loadings were manually adjusted along the border of the plume so that concentrations in the stormwater model and air dispersion models matched the runoff generated by the areas of the subcatchments would be scaled inconsistently the runoff water quality error was 0 001 for all three scenarios and all three rain events 3 results 3 1 subcatchments the total fraction of 137cs that washed off the subcatchments varied depending on the model configuration and the simulation event on average the lower accumulation shorter rainfall duration 3 h event had less total washoff than the higher accumulation longer 24 h event for all model configurations the longer system drainage time for the 72 h simulation did not make a discernible difference in surface washoff compared to the 24 h event without drainage time table 1 the modified glwa model 1b consistently washed off less than the hybrid and hrm models for the 3 h rain event model 1b had 14 lower washoff than the hybrid model and 17 lower than the hrm percentages were calculated as differences of subcatchment values in table 1 for the 24 h and 72 h events model 1b washed off 8 and 9 less than the hybrid and hrm respectively overall the hrm model washed off the most 137cs but was only 5 percent higher than the hybrid model regardless of the simulated event the spatial distribution of 137cs remaining after the 24 h event is visualized for each model configuration in fig 3 where the hrm and hybrid models overlap geographically the results were the same the hybrid model used unmodified glwa subcatchments in the distal part of the model domain and therefore does not align with the modified glwa subcatchments for direct comparison 3 2 outfalls the modified glwa model contained 150 outfalls the hrm model 20 outfalls and the hybrid model 161 outfalls 11 of the hrm outfalls remained in the hybrid model but transferred flow into the buried pipe network a lower total percentage of radioactivity was released through the outfalls for the hybrid model than the modified glwa model 14 38 and 36 lower for the 3 24 and 72 h events respectively this was most likely due to more of the 137cs held in the 2d conduits the modified glwa model and the hybrid model contained buried underground pipes that collect water from the entire model area and convey large amounts of water through two terminal outfall locations outside of the bounds of the hrm model this combined with the fact that like the hybrid model the hrm also held 137cs on the surface instead of routing through outfalls resulted in very little release of radioactivity through the outfalls for the 3 hour event and only 1 of the starting deposition for the 24 hr and 72 hr events the location of the highest radioactivity outfall was geographically the same for the hybrid and modified glwa model configurations and represented 43 73 of all radioactivity discharged table 1 when comparing only the 20 outfalls in all three of the models the location of maximum discharge did vary this means that the model configurations do not drastically alter the destination routing in a conveyance system model but can alter localized drainage locations 3 3 pipes each model configuration contained a different combination of virtual and physically present pipes this caused the location exhibiting the highest pipe flux of radioactivity to differ based on model construction compared to the modified glwa model the location exhibiting the highest flux differed by 41 and 75 less radioactivity in the hrm and hybrid models respectively this is related to the hrm and hybrid models having the capacity to hold water on the surface in the virtual pipe network at a finer scale whereas the glwa models must eventually transfer the loads to the underground conveyance system even if ponding is enabled in swmm the pipe exhibiting the highest flux of radioactivity in model 1b had 100 of the 137cs washoff routed through it during the 72 h event simulation for the same simulation the hybrid model pipe exhibiting the highest flux of radioactivity saw 63 of the total washoff flow through it whereas the hrm model only saw 40 of the total radioactivity in addition to holding water on the surface the virtual 2d pipes also allowed additional flow pathways which redistributed flow directions and allowed for the visualization of surface drainage pathways fig 4 b the hybrid model shared surface pipes with the hrm model and underground pipes with the glwa models by comparing the differences in contaminant flux during the simulation between the shared components it is possible to see the locations in the model where the interplay between the surface and the underground pipe flows causes differences in radioactivity fig 4 in certain areas the presence of the underground pipe network causes differences in the flux modeled on the surface and these locations are generally in the same locations when comparing overlapping underground locations with overlapping surface locations for example along the drainage pathway directly southeast of the detonation there is a difference in pipe flux of 13 322 gbq depending on if you use the hybrid vs modified glwa model and a 86 221 gbq surface flux difference along the same pathway when comparing the hybrid vs hrm model overall however the magnitude of these radioactivity differences are isolated to 7 distinct areas of surface drainage pipe interplay and there is a 322 gbq difference in pipe network fluxes excluding the furthest downstream line of the pipe network 4 discussion using ground deposition plumes from atmospheric pollutant models as input into watershed models has historically run into challenges such as differing grid cells and definition of dry deposition removal from the atmosphere vs amount depositing on the watershed available for washoff and is model specific on both the air and water side burian et al 2001 hong et al 2017 stormwater subcatchments are traditionally delineated based on topography and known drainage to infrastructure and will not necessarily overlap precisely with the shape of the deposition plume the research presented in this paper indicated that directly loading a hypothetical ground deposition plume from imaac into an existing swmm subcatchment array has the advantage of a rapid response but there is a potential for overestimating the contaminant load 61 more radioactivity than the air model deposition plume the magnitude of the overestimation is dependent on the size of the subcatchment and the path of the plume but for km2 sized subcatchments typical of many off the shelf stormwater models the error may be substantial over double the amount this mass overestimate can be managed by splitting the stormwater model subcatchments where significant portions of the subcatchment would be outside of the plume footprint and is most important from a mass balance standpoint in areas of highest contamination simply scaling the loading amount would not account for the extra runoff generated by the uncontaminated part of the subcatchment which can lead to inaccurate calculations of contaminant concentrations due to finer spatial detail developing a hrm has the advantage of better representing the contaminant mass loads without needing to split subcatchments along the path of the plume although one could split hrm subcatchments to exactly map the predicted footprint the air deposition results are also from a model so this level of precision is not necessarily physically accurate and potentially only worth the effort if the values are from field measurements e g a gamma spectroscopy aerial monitoring system for 137cs as observed in shireman et al 2022 within each model more total runoff from a subcatchment corresponded to greater washoff total runoff did not correlate with the size of a subcatchment but it did relate to how land use was classified higher impervious values leading to greater runoff greater washoff from a specific subcatchment was also related to land use classification because different runoff coefficients were used for roads buildings and impervious areas between models it was observed that finer spatial resolution resulted in more overall washoff of the contaminant plume washoff parameters have been shown to better hold their physical meaning at the small scale bonhomme and petrucci 2017 and micro delineation of subcatchments has been shown to reduce uncertainty in flow predictions sun et al 2012 peak flows have been shown to be influenced by subcatchment scale with different results for larger storms reduction in peak flow and smaller storms increase of peak flows caused by differences in infiltration overland flow and conduit routing at different scales cao et al 2020 ghosh and hellweger 2012 pollutant predictions are influenced by accuracy of flow predictions e g dilution and have also been shown to be sensitive to spatial scales and vary by pollutant dai et al 2018 with all three model configurations it is possible to visualize areas that will be more or less intensely affected by the release and subsequent washoff but on very different scales areas of finer resolution may be more suitable for remediation activities such as targeted sampling of contamination hotspots and deciding where to stage waste generated during remediation and recovery efforts such that it does not contaminate sensitive areas there is a distinct difference in how runoff is handled between the glwa and hybrid model hrm subcatchments in the glwa model and glwa subcatchments included in the hybrid model runoff is aggregated and enters the swmm stormwater conveyance system directly at the model subcatchment outlet nodes in the hrm mesh areas of the hybrid model and the hrm model runoff for each cell enters the 2d conduit mesh at a 2d node and is conveyed through the 2d system via the 2d conduits simulating overland flow this difference had important implications because the 2d conduit system represents a secondary flow system and behaved as temporary storage during a model run the 2d conduit system contained substantially more components and therefore allowed for more flow paths and lower radioactivity levels per pipe also since the hybrid and hrm model were able to hold contaminant in the surface flow it was observed that less contamination flowed through outfalls 0 1 of total contamination for the hrm and 0 4 63 for the hybrid depending on storm event than in the glwa model configurations 14 99 if the stormwater model is being used to develop an outfall monitoring plan at a set threshold this is a significant finding to be aware of since model structure greatly impacts the number of outfalls indicated by modeling as being contaminated 5 conclusions this research investigated the impact of model structure on transport of radiological deposition from a dirty bomb explosion in an urban area model structure impacted the contaminant loading into the stormwater model from the air deposition modeling and found that unless the original model subcatchments were of sufficiently fine scale adjustments to the stormwater model are necessary to split the original subcatchments along the path of the plume model structure was also found to impact the amount of washoff high resolution modeling produced more washoff flow of contamination through outfalls more contamination flowing through outfalls with the underground pipe network and flux of radioactivity per pipe lower levels in the 2d conduit system the implications of these findings are that few stormwater models are considered ready off the shelf monitoring plans must be field verified with lower resolution models potentially more conservative on the number of impacted outfalls and radioactivity levels per pipe and that hybrid hrm models are necessary to visualize transport at a scale applicable to surface sampling and waste staging while stormwater models in any configuration require field verification and have known shortcomings they can also provide very useful information in predicting impacts of severe weather providing risk assessors with water quality ranges for dose assessments and deciding where to sample that aid in nuclear emergency response disclaimer the research described in this article has been funded wholly or in part by the u s environmental protection agency contract no 68herc19d0009 to aptim government services this manuscript was subject to administrative review but does not necessarily reflect the view of the u s environmental protection agency no official endorsement should be inferred as the epa does not endorse the purchase or sale of any commercial products or services credit authorship contribution statement anne mikelonis conceptualization methodology writing original draft writing review editing supervision project administration funding acquisition jonathan shireman methodology software validation formal analysis data curation writing review editing katherine ratliff conceptualization methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful to numerous contributors including radha krishnan of aptim sherri gee john norton wendy barrott and walter davis of the great lakes water authority and brian kelly of the usepa region 5 thank you to lifeng yuan and michelle simon for internal technical reviews and ramona sherman for quality assurance review 
25464,this paper presents lake operation optimization of nutrient exports loone a python based model with three coupled modules 1 a water balance module to simulate hydrology and operations 2 a nutrient module to simulate phosphorus dynamics in the water column and 3 an optimization tool to design optimal reservoir releases we applied the model to lake okeechobee florida usa a shallow subtropical lake receiving and exporting excessive amounts of nutrients while providing water supply recreation and flood control we evaluated loone under historical operation schedules for model verification and then designed optimal releases that minimized phosphorus exports into the two main distributaries of lake okeechobee we found that p loads could decrease by 12 33 under optimal operations the software package could be used to evaluate management decisions to help control water quality issues such as high nutrient exports and harmful algal blooms in lake okeechobee and reservoirs worldwide keywords reservoir operations phosphorus dynamics optimal discharges shallow lake lake okeechobee data availability data used in this study was extracted from a publicly available database https www sfwmd gov science data dbhydro 1 introduction reservoir operation strategies determine water release and storage to facilitate flood control municipal water supply irrigation and hydropower generation among other uses in the past reservoir operations often ignored environmental objectives hu et al 2014 yielding significant alterations to natural flow regimes downstream ultimately causing severe ecological impacts bunn and arthington 2002 in recent years there has been an increasing attention towards protecting and restoring freshwater ecosystems harwood et al 2018 twardek et al 2021 wineland et al 2022 galelli et al 2022 reservoir operations affect flow velocity water levels and hydraulic retention time which affect the amount of nutrient release and retention associated with biogeochemical processes in the water column and bottom sediment vollenweider 1975 lürling et al 2016 han et al 2018 xu et al 2020b overall reservoir operations affect water quality in the reservoir and downstream rivers thus strategic management of reservoirs is crucial numerous studies have attempted to quantify aquatic environment responses to reservoir operations li et al 2018 considered specific hydrologic metrics e g monthly flow magnitudes magnitude and duration of annual extreme flows that affect ecological functions whereas szemis et al 2012 2013 considered average scores as indicators of the overall health of all species and ecological outcomes other studies hu et al 2014 rangel peraza et al 2012 xu et al 2020b you and zhang 2018 analyzed various explicit water quality indicator responses e g nutrients total dissolved solids dissolved oxygen algae temperature etc to reservoir operation parameters e g inflow release water head water exchange etc for instance aalami et al 2018 rangel peraza et al 2012 and yosefipoor et al 2022 conducted hydrodynamic and water quality simulations using the ce qual w2 model to investigate impacts of water releases on different water quality variables babamiri and marofi 2021 coupled the weap and qual2k models to investigate effects of water releases on water quality xu et al 2020 adopted artificial neural networks ann to quantify the relationship between hydrologic and ecological indicators other studies have optimized reservoir discharges to improve various water quality variables e g water temperature dissolved oxygen total dissolved solids sediment passage in reservoirs and rivers aalami et al 2018 xu et al 2020 yang et al 2012 wild et al 2018 though few studies considered nutrients explicitly babamiri and marofi 2021 xu et al 2020b yosefipoor et al 2022 moreover giuliani et al 2014 and quinn et al 2017 identified reservoir operation and p emission policies depending on critical variables in the system e g water storage water quality and water supply in summary reservoir operations have altered the natural flow regime of rivers and affected water quality in reservoirs as well as downstream water bodies yet reservoir operations could be used to improve water quality if designed appropriately chen and olden 2017 horne et al 2016 sabo et al 2017 szemis et al 2013 phosphorus p concentrations in freshwater lakes often drive water quality and limit ecosystem productivity brett and benjamin 2008 downing et al 2001 thus numerous studies have attempted to determine factors impacting p concentrations in lake water vollenweider 1975 s p mass balance approach a pillar in p modeling determines lake p concentrations based on input p mass loading rate average lake depth phosphorus retention and hydraulic loading predictive p models could be divided into static and dynamic models on the one hand static models assume steady state lake water conditions and like vollenweider 1975 determine p concentration in a lake as an empirical function of inflow p concentrations lake mean depth and water retention time bryhn and håkanson 2007 nurnberg 1984 reckhow 1988 on the other hand dynamic models comprise ordinary or partial differential equations to simulate changes in nutrient load over time but they need more input variables than static models though there have been dynamic models that consider vertical transport settling and resuspension from the bottom sediment layer håkanson and bryhn 2008 kum and burnett 2013 consideration of the effect of how surface phenomena in the bottom sediment i e desorption and adsorption which could also affect phosphorus cycle in the lake water column has been limited in addition p models have considered stratification in deep water lakes e g håkanson and bryhn 2008 komatsu et al 2006 some models have incorporated two dimensional or three dimensional grids to predict p concentrations in lakes komatsu et al 2006 such models may perform well against observations but are computationally intensive and need more resources and are not suitable for evaluating management questions and strategies that need multiple diagnostic tests there are a number of models that compute 1 d hydraulics and reservoir operations e g weap hec ressim vic res etc sei 2012 calamita et al 2021 duc dang et al 2020 klipsch and evans 2006 and others compute complex 2 d and 3 d hydrodynamics e g ce qual w2 delft3d etc baracchini et al 2020 chanudet et al 2016 kobler et al 2018 mateo lazaro et al 2020 some of these hydrodynamic models have been successfully linked to water quality and ecosystem productivity models babamiri and marofi 2021 though 3d models take long computational times and 1 d models focus more on hydrodynamic processes rather than geochemical and management aspects thus this study coupled reservoir operations and a dynamic phosphorus mass balance algorithm to support the evaluation of the impacts of reservoir management policies on phosphorus dynamics in shallow regulated lakes we developed a python based tool capable of simulating reservoirs regulated lakes with multiple inflows outflows and considering multiple processes associated with particulate and dissolved phosphorus this tool can evaluate the effects of lake operations e g outlet discharges on nutrient concentrations and exports in addition an optimization engine was developed to identify operational strategies that minimize phosphorus exports out of reservoirs and improve water quality in distributaries of the reservoirs this paper presents the development of lake operation optimization of nutrient exports loone which is an integrated water balance nutrient cycling and optimization model for regulated lakes reservoirs the water balance module adopts the reservoir continuity equation to simulate daily lake operations it also incorporates detail water balance budget variables inflows outflows rainfall evapotranspiration seepage etc and simulates lake operations based on identified operation rule curves p concentrations and exports are simulated through multiple outlets by using a deterministic mass balance module which also considers phosphorus retention and resuspension from the bottom sediment layer the optimization module is used to design lake releases with the objectives of minimizing nutrient exports into the downstream waters and improve water quality the integrated model is used to evaluate impacts of water management operations on nutrient concentrations and exports in lake okeechobee a subtropical shallow lake with a complex system of inflows and outflows lake okeechobee is a heavily regulated lake and has historically received large amounts of nutrients that have impacted water quality in the lake and receiving waters 2 model overview loone is a comprehensive water balance nutrient optimization model that comprises three coupled modules fig 1 1 a water balance module that simulates the water balance and operations of a reservoir 2 a nutrient module that simulates nutrient phosphorus dynamics in the water column and 3 an optimization engine that optimizes a reservoir s releases into its distributaries with the objective of nutrient export minimization and or water deficit minimization python 3 was chosen to develop the code because of its many high quality libraries and a powerful community support in terms of data inputs the model requires time series of tributaries flow and nutrient mass load rainfall and evapotranspiration et information on the lake and sediment properties sediment particle bulk density moisture content of each sediment type 2 1 the water balance module the water balance module simulates daily stage and storage volume of a reservoir through the release of discharges into distributaries based on the reservoir s operation rules to either fulfill specific target releases or maintain desired water storage in the reservoir the water balance module is based on the continuity equation eqn 1 δ s δ t 1 n i x 1 m o x where δ s change in reservoir storage during time step δ t i x different n inflows into the reservoir surface water i s groundwater i g w rainfall over the reservoir i r and o x different m outflows from the reservoir surface o s groundwater o g w et from the reservoir and wetlands o e t seepage losses o l applying the continuity equation to a reservoir results in the following eqn 2 d s i 1 d s i i s i g w i r i o s o g w o e t o l i where d s i represents reservoir storage volume at time step i i s and o s represent one or multiple surface water inputs and outputs respectively the water balance module simulates daily reservoir storage at each time step and then the storage is converted to reservoir stage by using the reservoir s elevation storage curve this simulated stage is used to determine the regulated releases for the next time step following the reservoir s rule curve the water balance module simulates water releases into distributaries following operation rules that define the desired amount of water release based on reservoir water levels needed to fulfill a desired water demand or to preserve specific water storage in the reservoir the module considers the tributary hydrologic condition i e wet or dry via the palmer drought severity index and the average reservoir net inflow for the previous two weeks 2 2 phosphorus module loone incorporates a phosphorus dynamic module that simulates both particulate and dissolved p concentrations in a reservoir as well as p mass loads into distributaries it also incorporates p releases from the surficial sediment layer in the reservoir bottom to the water column known as internal loading the fundamental p modeling approach follows vollenweider 1975 input output model eqn 3 to predict in lake total phosphorus tp concentrations as function of external loading hydraulic loss with lake outflows and net sedimentation fig 2 b illustrates the phosphorus module conceptually and the following paragraphs discuss the phosphorus simulation processes in detail the vollenweider 1975 p mass balance model is expressed as follows eqn 3 v δ p l a k e δ t l e x t a q o u t σ v p l a k e where p lake is the in lake tp concentration mg p m 3 v is lake volume m3 l ext is the external p loading into the lake per unit of lake area mg p m 2 d 1 a is lake area m2 q o u t is the lake discharges m3 d 1 and σ is the net sedimentation coefficient d 1 which is the first order decay coefficient that represents the net balance of two opposing processes gravitational settling of water column p versus internal loading pollman and thomas james 2011 loone s p module utilizes an input output separation system in modeling lake tp concentrations which proved better results especially for large area lakes as this method reflects the spatial gradients in p concentration fig 2a walker and havens 2003 transport of p within the reservoir was greatly simplified and simulated based on the following equations eqn 4 v i δ p i δ t l e x t a i q i o p i σ a i p i eqn 5 v o δ p o δ t q i o p i q o u t p o σ a o p o where items denoted with a subscript i represent inlet part of the lake and items denoted with the subscript o indicate outlet part of the lake q i o is flow in the lake from inlet part to outlet part m3 d 1 walker and havens 2003 internal loading significantly delays lake recovery particularly in large shallow lakes missimer et al 2021 pollman and thomas james 2011 furthermore the two major processes that likely drive internal sediment phosphorous flux in shallow lakes are diffusion and wind driven sediment resuspension fisher et al 2005 jin et al 2007 thus internal loading in our module is considered explicitly by separating σ into two components eqn 6 σ v s e t z l int a p l a k e v where v set is the gross settling velocity m d 1 z is the lake mean depth m and l int is the internal loading per unit area mg p m 2 d 1 dissolved inorganic phosphorus dip moves across the sediment water interface via diffusion associated with the concentration gradient thus internal loading flux is determined eqn 7 l int v d i f f d i p p o r e d i p l a k e θ where v d i f f is the piston velocity describing the mass transfer of dip across the sediment water interface m d 1 d i p p o r e and d i p l a k e are dip concentrations in the sediment porewater and lake water column respectively mg p m 3 and θ is the sediment s fractional porosity substituting from eqn 7 into eqn 6 and eqn 3 results in the following eqn 8 v δ p l δ t l e x t a v d i f f d i p p o r e d i p l a k e θ a s e d v s e t a p l q o p l e 0 t d e 1 τ τ c τ c e 2 where ased indicates the area of sediment in the lakebed τ c is the critical shear stress dyne cm 2 t d is the time following deposition days and e 0 e 2 are coefficients porewater dip could be in the form of sediment p p adsorbed to sediment surface or p in sediment porewater fig 2b sediment p transforms into porewater p through mineralization while p adsorbed into sediment particle surface moves between sediment and porewater via adsorption desorption phenomena porewater dip dynamics are represented as follows eqn 9 θ z s e d a s e d δ d i p p δ t v d i f f θ a s e d d i p p d i p l j d e s j a d s j d e c o m p j b u r i a l where j d e s indicates the total desorptive flux mg d 1 j a d s indicates the total adsorptive flux mg d 1 j d e c o m p indicates p flux due to mineralization and j b u r i a l indicates p sink into the deep sediment these four fluxes are estimated as described by eqns 10 13 eqn10 j d e s k d e s γ m s e d eqn 11 j a d s k a d s d i p p γ γ m s e d eqn 12 j d e c o m p k d e c o m p p s e d m s e d eqn 13 j b u r i a l v b u r i a l θ a s e d d i p p where k d e s represents the desorption rate constant d 1 m s e d indicates the mass of sediment in the surficial mixed layer kg and γ represents the amount of sorbed solute mg p kg 1 sediment k a d s represents the adsorption rate constant m3 mg 1 d 1 γ indicates the maximum concentration of sorbed p the solid phase could hold mg p kg 1 sediment k d e c o m p represents the first order decay constant d 1 p s e d represents sediment p mg p kg 1 sediment and v b u r i a l indicates burial velocity m d 1 2 3 optimization engine optimization is adopted for reservoir operation aiming at minimizing nutrient exports out of a reservoir and minimizing water demand deficits the optimization problem comprises of two objective functions o b j 1 min p l o a d exp o b j 2 min w a t e r d e m a n d d e f i c i t s where p load exp represents p load exports into distributaries and water demand deficits represent the difference between water demand and water supply the decision variables include monthly water discharges while problem constraints incorporate maintaining lake stage s within defined thresholds as well as discharge o s less than control structure s maximum discharge capacity and are formulated as follows min s s i max s 0 o s d i s c h a r g e c a p a c i t y various algorithms have been used to solve reservoir operation optimization problems aalami et al 2018 bai et al 2015 haddad et al 2006 rani and madalena 2010 saberchenari et al 2016 however evolutionary algorithms e g genetic algorithms dominate the current state of practice in operations research chang et al 2016 kerachian and karamouz 2006 maier et al 2014 yang et al 2012 yosefipoor et al 2022 thus platypus a multi objective evolutionary algorithm moea open source framework for optimization in python was adopted as optimization module hadka 2015 https github com project platypus platypus ε nsga ii moea a common technique to optimize reservoir operation e g ahmad and hossain 2020 cohen et al 2021 dang et al 2020 peñuela et al 2021 was adopted to optimize the lake operations 3 case study lake okeechobee florida usa loone was used to simulate operations and phosphorus mass balance of lake okeechobee the largest reservoir by surface area in the southern us in addition to its dimensions we chose lake okeechobee as a case study for multiple reasons first it is a multi inlet and multi outlet reservoir with a complex system of pumps and locks operated with a regulation schedule which allowed us to evaluate performance of the model in a complex hydrologic system second it is a nutrient impaired lake known to export large amount of nutrients to its surrounding water bodies tarabih and arias 2021 walker 2000 thus loone could aid evaluating impacts of lake regulations on the nutrient status of the regional system lake okeechobee plays a pivotal role in south florida s water resources management supplying water demands for the lake okeechobee service area losa which includes domestic water demand for surrounding communities supplemental irrigation water for the losa and everglades agricultural area eaa environmental flows for the st lucie and caloosahatchee estuaries and the everglades national park paudel et al 2020 cadavid et al 1999 importantly lake okeechobee protects the surrounding regions from flooding lake okeechobee receives its water and nutrients mainly from the northern watersheds through kissimmee river indian prairie canal harney pond canal fisheating creek nubbin slough and taylor creek khare et al 2019 lake okeechobee has two main distributaries to the coast the caloosahatchee river and the st lucie canal which discharge into estuaries in the gulf of mexico to the west and the atlantic ocean to the east respectively lake okeechobee also supplies water for the lower east coast and discharges flood control releases to the south through four major canals miami hillsboro north new river and west palm beach water from these southward canals is treated in large stormwater treatment wetlands which can reduce phosphorus concentrations to a great extent these canals supply water to the water conservation areas and capture excess runoff from the eaa fig 3 illustrates main tributaries and distributaries of lake okeechobee lake okeechobee is surrounded by the 229 km long herbert hoover dike which was built to provide flood protection for the surrounding regions lake stages are regulated by operational schedules and five schedules have been implemented since the construction of the dike the main objectives of all schedules have been to perform flood protection navigation and water supply for surrounding communities natural areas agricultural lands and coastal regions in 1991 run25 was proposed with the purpose of minimizing high volume damaging releases to the estuarine ecosystems in 2000 water supply and environmental wse schedule was implemented to provide better balance of water supply in lake and estuary performance and flood protection in 2008 lake okeechobee regulation schedule 2008 lors began to be implemented where the upper bound was lowered from 18 5 ft 5 64 m to 17 25 ft 5 26 m to protect the weakened dike from high water levels the us army corps of engineers is currently re evaluating lake okeechobee operations to coincide with the completion of the dike rehabilitation project expected to be complete by the end of 2022 the lake okeechobee system operating manual losom aims at balancing the congressionally authorized project purposes for water supply flood control recreation navigation and wildlife resources preservation losom aims at reducing lake okeechobee harmful regulatory releases to both st lucie and caloosahatchee estuaries and increasing flows south to the everglades supplementary materials table s3 operations of lake okeechobee have influenced nutrient concentrations in the lake and nutrient exports into the distributaries tarabih and arias 2021 most data used for the lake okeechobee case came from a single source dbhydro the main data portal of the south florida water management district sfwmd https www sfwmd gov science data dbhydro daily flow data originally in ft3 s 1 were synthesized for every tributary distributary of lake okeechobee where flow stations are available rainfall and et data were synthesized at four different gauge stations spatially distributed over the lake proper l001 l005 l006 and lz40 fig 3 daily cumulative rainfall and evapotranspiration over lake okeechobee surface were considered p concentration data included total phosphate mg l 1 and ortho phosphate dip mg l 1 in the lake water column p data were synthesized at all the inflow locations of the lake and in seven different gauge stations spatially distributed over the lake proper ortho phosphate data were synthesized from seven stations in the lake proper tp loads at different lake okeechobee tributaries were calculated by using tp concentration and flow values in the tributaries locations of all gauge stations are illustrated in the supplementary materials fig s5 losa daily water demand data for the study period 1991 2018 were synthesized from sfwmd reports sfwmd 2018 monthly seasonal multi seasonal net inflow outlook time series data as well as operation rule data threshold elevations for different lake regulatory zones and regulatory releases associated with different regulatory zones were synthesized from usace reports cadavid et al 2006 neidrauer et al 1999 usace 2008 lake sediment data particle bulk density and moisture content for the 4 different sediment types mud sand rock and peat fig 5 initial sediment p concentrations and porewater dip in the four different sediment types were synthesized from the literature fisher et al 2001 2005 olila et al 1995 olila and reddy 1993 a detailed description of how the water and p balance equations were implemented for the lake okeechobee complex system is provided in the supplementary materials 3 1 lake okeechobee model loone was used to simulate lake okeechobee regulatory releases into st lucie canal and caloosahatchee river while observed flows were used for west palm beach canal north new river canal hillsboro canal miami canal and l 8 canal see locations in fig 3 in addition water supply flows were simulated utilizing the continuity equation as mentioned in section 2 1 and lake okeechobee rule curves for the study period fig 4 we simulated three different lake okeechobee schedules during the study period 1991 2018 run25 1991 1999 wse 2000 2007 and 2008 lors 2008 2018 loone s p module simulated phosphorus dynamics in lake okeechobee by applying the equations mentioned in section 2 2 using outflow timeseries output from the water balance module as input we developed a polynomial equation between monthly average dip in the lake water column and tp in the lake for the period 1973 2019 n 560 r2 0 55 though future work could treat this regression relationship stochastically to better captures extreme dip concentrations vogel 2017 eqn 14 d i p l 8 08 0 5 p l 0 0008 p l 2 lake okeechobee bed comprises 4 sediment zones mud peat sand and rock yan and james 2013 delineated lake okeechobee bottom sediment zone boundaries based on sampling surveys high resolution aerial photography and elevation data in 1988 1998 and 2006 lake okeechobee bottom sediment map from yan and james 2013 fig 5 was incorporated into the p module to include the different sediment properties impacts on p internal loading altogether the lake was divided into eight different spatial units four sediment zones in the north and four sediment zones in the south 3 2 sensitivity analysis a sensitivity analysis was performed for lake okeechobee p module to evaluate impacts of the different parameters on p concentrations in the lake for a period of 2008 2010 and to understand the importance of each parameter on simulating p dynamics in the lake the water balance module was not tested because it depends on water mass balance and operation schedules of the lake and therefore there is limited space and need to vary input parameters the sobol method saltelli 2002 was used to evaluate the sensitivity of the phosphorus module to nine parameters burial velocity of sediments and porewater v burial eqn 13 diffusion velocity v diff eqn 7 deposition velocity for p from water column to sediments v set eqn 6 inorganic p desorption rate k des eqn 10 adsorption rate constant k ads eqn 11 first order sediment p decay constant k decomp eqn 12 external loading l e x t eqn 8 critical shear stress τ c eqn 8 and the time following deposition t d eqn 8 all initial values of these parameters were derived from the literature supplementary materials table s2 the sobol method determines a first order second order and total effect indices but only the first order and total effect indices were estimated here the first order sensitivity index si represents the additive effect of the parameter and sti the total effect sensitivity index measures the overall influence of the parameter incorporating interactions with other parameters each variable was sampled 4096 times which ended up generating 57344 individual sample sets to properly represent the global variability among parameter values 3 3 scenario design loone was used to determine the effects of alternative operation rules on phosphorus exports we simulated 2008 lors as a baseline scenario and proposed losom operations as an alternative scenario to evaluate impacts of operational modifications on phosphorus concentrations in the lake as well as exports into the estuaries in an alternative scenario we modified operations such as regulatory and water supply releases from the lake however we used the same hydrologic input data 2008 2018 to be consistent with our comparison between these scenarios 3 4 calibration and validation loone was calibrated and validated for water balance and phosphorus simulations loone s water balance simulations were verified by comparing simulated and observed daily lake okeechobee stages and discharges to northern estuaries for the three operation schedules run25 1991 1999 wse 2000 2007 and 2008 lors 2008 2018 to ensure that lake operations are incorporated correctly in the model the phosphorus module was calibrated by modifying the most sensitive p parameters table 1 to minimize the deviation between simulated and observed p concentrations in the lake water column loone s p module was calibrated by using the platypus package for the most recent operation schedule 2008 lors 2008 2018 and was validated for the two previous operation schedules run25 1991 1999 and wse 2000 2007 on a weekly basis automatic calibration was carried out to minimize the difference between simulated and measured p concentrations by using the ε nsga ii optimization algorithm besides coefficient of determination r2 the nash sutcliffe ns and normalized root mean square error nrmse coefficients were used as indicators of fitness between simulations and observations 3 5 optimization of lake okeechobee releases loone was used to design optimal releases of lake okeechobee into the caloosahatchee river and st lucie canal with the goal of demonstrating an operational schedule that can minimize pollutant exports into the estuaries while minimizing losa water deficits we developed an operation schedule that is dependent on p concentrations in the lake to guide water discharges where two p concentration thresholds were identified one for the summer and one for the winter such that if p concentration in the lake exceeds the threshold no water releases are allowed in addition the optimal operation policy depends on seasonal maximum and minimum water levels in the lake where if water level exceeds the maximum threshold maximum capacities of water control structures are discharged while no water releases are allowed if water level falls below the minimum threshold fig 11 algae blooms in st lucie canal and caloosahatchee river represent a major regional environmental issue and they are greatly influenced by nutrient laden discharges from lake okeechobee hence we focused on optimization of nutrient export from the lake to these estuaries decision variables incorporated monthly discharges into the caloosahatchee river and st lucie canal 24 decision variables while constraints included the seasonal maximum and minimum water levels in the lake and maximum discharge capacities of the control structures in the caloosahatchee river and st lucie canal s77 and s308 respectively ε nsga ii algorithm was used to solve the optimization problem which needed to run 1000 times about 3 h of cpu time on a 3 4 ghz pc to converge 4 results and discussion 4 1 phosphorous module sensitivity analysis sobol sensitivity analysis is best illustrated by the first order s1 and total effect st indexes table 1 and table s4 the sensitivity analysis of the phosphorus module resulted in parameters associated with phosphorus resuspension t d and τ c as the most sensitive parameters first order sensitivity index s1 0 068 and 0 057 respectively which makes sense since lake okeechobee is a shallow lake impacted by wind driven resuspended sediments phosphorus external loading ranked as the third most sensitive parameter s1 0 051 illustrating the influence of watershed phosphorus loadings on water quality within lake okeechobee the model was sensitive to settling adsorption desorption and mineralization parameters inorganic p moves from the pore water to sediment particles via adsorption thus adsorption controls dip in pore water which affects the amount of p released into the water column surprisingly the model was not sensitive to diffusion which could be related to higher p concentrations in the water column that resulted in a low gradient of p concentrations between sediments and water column in addition the model was not sensitive to phosphorus burial which could be because the sediments are already saturated with phosphorus limiting the potential for additional burial although total effect indices followed the same ranking as the first order indices the total order indices were less than the first order indices most probably because the interactions between each parameter does not affect the total phosphorus concentration in the lake water column sensitivity results of our study agreed with sensitivity analyses conducted by james et al 1997 2005 pollman and thomas james 2011 who suggested that tp in lake okeechobee is most sensitive to parameters affecting inorganic solids concentration in the sediments resuspension settling and phosphorus in inflowing water furthermore james et al 2005 suggested that tp in lake okeechobee was not sensitive to burial out of the system 4 2 calibration and validation the water balance module was verified for three different operation schedules fig 6 model verification for lake okeechobee water elevations during run25 operation schedule illustrated very good results with ns 0 90 and r2 0 92 run25 maintained lake okeechobee stages above 12 ft 3 66 m the model overestimated low lake levels in water year 1994 and through mar nov 1997 during wse operation schedule the model performed even better with ns 0 96 and r2 0 96 lake okeechobee stages during wse ranged between 8 and 18 ft 2 44 5 49 m except for one event in may 2001 when the lake okeechobee stage decreased to almost 7 ft 2 13 m during the wse period the model had a tendency to underestimate stages during low stage events during the 2008 lors operation schedule the model performed very good ns 0 90 and r2 0 91 the model fluctuated between over and under estimating water levels during low stage events and peak stage events those mismatches are probably associated with management decisions that are not included in normal operations specifically during emergencies due to extreme events e g droughts floods hurricanes etc water discharges into caloosahatchee river and st lucie canal were also verified for the multiple operational schedules ns and r2 values for flows into caloosahatchee river were 0 68 0 79 and 0 73 0 74 respectively ranged based on operational schedule ns and r2 values for flows into the st lucie canal were 0 17 0 72 and 0 39 0 78 respectively fig 7 illustrates monthly flow duration curves for the entire simulation period 1991 2018 for caloosahatchee river and st lucie canal simulated versus observed releases loone s p module was also calibrated validated for these operation schedules the calibrated values for the eight identified sensitive parameters are shown in table 1 see table s5 for parameters by sediment zone and fig 8 a illustrates the comparison of simulated versus observed average monthly p concentrations in lake okeechobee r2 for the calibration period 2008 2018 was 0 40 and 0 24 for the validation period 1991 2007 the model generally captured increasing and decreasing trends in p concentration though the model missed some p concentration peaks in lake okeechobee the model performed well in capturing the average annual p concentrations in lake okeechobee but underestimated p concentrations in some specific years i e 1993 1994 2004 2006 fig 8b which could be related to p sorbed to calcite in surficial sediments which is released to the water column when calcite dissolves due to thermodynamic conditions pollman and thomas james 2011 4 3 comparison of current versus proposed operating schedules after calibrating and validating the model we used the model to demonstrate its outcomes under the proposed losom operations to evaluate effects of losom on phosphorus concentrations in the lake and phosphorus exports into the caloosahatchee and st lucie estuaries as shown in fig 9 losom would maintain lake okeechobee stages higher than 2008 lors operations during both high and low conditions this increase in water levels was expected as losom would decrease water releases into the estuaries especially into the st lucie losom would decrease p loads into the st lucie canal by 40 fig 10 b associated with reduction in high volume discharges yet p loads into the caloosahatchee river would increase by 33 fig 10a and p loads into the south that would be doubled because of increased flow volumes under the losom run fig 10c 4 4 lake okeechobee optimal releases different algae species in south florida have been strongly affected by alterations in both hydrologic alterations and water quality tarboton et al 2004 harmful algal blooms in lake okeechobee associated with high p concentrations walker and havens 1995 often spread during the summer season within the lake and into the estuaries jin and ji 2013 missimer et al 2021 thus loone was used to determine optimal lake okeechobee discharges to minimize p exports into the two major distributaries during summer season may through october we developed an operation policy fig 11 recommending monthly discharges to the caloosahatchee river and the st lucie canal that are released when water levels in the lake are within the operational band i e between the orange and blue solid lines in fig 11 given that p concentrations in the lake are below p concentration thresholds 0 12 mg l 1 for the summer and 0 22 mg l 1 for the winter dashed red line in fig 11 loone provided a pareto optimal front i e five non dominated solutions connected by black lines in the right boxes of fig 12 among 100 feasible solutions the grey points in the right boxes in fig 12 providing monthly discharges into the caloosahatchee and st lucie distributaries we found that the best solution reduced p exports from 106 9 to 77 6 tons yr 1 27 4 into the caloosahatchee river and from 85 4 to 57 6 tons yr 1 32 6 into the st lucie canal fig 12 while maintaining lake okeechobee stages between 12 6 ft 3 84 m and 17 6 ft 5 36 m in addition losa water demand deficit would decrease from 35 534 ac ft 43 83 million m3 to 31 728 ac ft 39 14 million m3 per year the optimal discharges were found to be less than the historical releases during most of the summer season and maximum monthly discharges were lower especially via st lucie canal moreover the optimal operations were verified against a different historical inflows and phosphorus loads to lake okeechobee i e wse operation schedule 2000 2007 and we found that the optimal operation policy reduced phosphorus exports to caloosahatchee river by 20 4 and to st lucie canal by 12 4 fig s9 loone could be expanded to include other objectives identified by users e g navigation in the st lucie canal and caloosahatchee river in future research 4 5 comparison to other models loone provides users with an efficient tool for optimizing lake levels and discharges considering water quantity and water quality objectives loone simulates the water balance and phosphorus dynamics in multi outlet reservoirs and incorporates a multi objective optimization module to minimize nutrient exports and water demand deficits this model is rather simple by design and excludes complex hydrodynamics formulations which are computationally expensive instead this model can simulate long term operations of a reservoir efficiently computation time for a year of simulation is 4 s which makes it an appropriate tool for reservoir operation screening through which the user could simulate various alternative operations driven by multidecadal hydrologic inputs for screening and planning purposes lake okeechobee phosphorus has been simulated by using various models from simple empirical models to 3 d hydrodynamic models table 2 kratzer and brezonik 1984 and janus et al 1990 used regression empirical and simplified vollenweider type models respectively to predict p concentrations in the lake water column considering hydrology external phosphorus loadings and net phosphorus settling performance of these models was poor to modest most probably because these models do not explicitly consider internal loading of phosphorus which was proven to significantly affect phosphorus concentrations in shallow lakes like okeechobee meanwhile james et al 1997 2005 used the lake okeechobee water quality model a spatially averaged deterministic mass balance model to simulate nitrogen phosphorus oxygen and phytoplankton dynamics in the lake jin et al 1998 adopted wasp eutro a water quality analysis simulation model to simulate the dynamics of nitrogen phosphorus and phytoplankton in lake okeechobee dividing the lake into multiple horizontal segments in addition jin et al 2007 used the lake okeechobee environment model a 3 d hydrodynamic sediment and water quality model to simulate phosphorus nitrogen algae organic carbon and oxygen dynamics in the lake water column even though these models provided much value in understanding p dynamics and linkages to the aquatic ecosystem neither of these models considered effects of lake okeechobee regulation schedules on the p water quality moreover computational time associated with those models is relatively long 6 h of cpu time for 1 year simulation jin et al 2007 evaluating water and nutrient management strategies often requires a large number of model simulations and computationally expensive models are not quite useful for this purpose overall loone is very efficient in simulating reservoir operations and p dynamics in the reservoir in a short period of time 4 s for 1 year simulation making the model appropriate for screening scenario analysis and optimization as illustrated in this paper multiple approaches to optimize reservoir releases for different objectives have been developed and evaluated in multi purpose reservoirs table 3 one common approach is to simulate reservoir operations within the optimization problem using the simple water mass balance deng et al 2020 saadatpour et al 2020 yu et al 2018 xu et al 2021 though loone simulates reservoir operations following a detailed operation rule ann and statistical approaches have been adopted to quantify water quality responses to reservoir operations saadatpour et al 2020 yu et al 2018 xu et al 2021 shaw et al 2017 whereas loone simulates phosphorus process dynamics in a shallow reservoir including vertical exchange between water column and sediment bed moreover ali 2015 presented imodel to optimize flows into and through the river of grass project a restoration project in the florida everglades to achieve spatial and temporal flow and stage characteristics however imodel did not consider water quality explicitly finally loone is an open source model that could be easily modified to adopt different reservoirs various objectives and different water quality parameters 5 conclusions loone provides an effective open access tool for lake water balance and phosphorus dynamic simulation in the lake water column considering phosphorus cycling processes such as settling and internal loading from the surficial sediment layer in the reservoir bottom loone is best described as a screening tool to evaluate impacts of reservoir operations on phosphorus loadings into main distributaries for long term period simulations linked hydrodynamic water quality models often have long runtime and can be prohibitively cumbersome to use for management purpose whereas loone is highly computationally efficient and could simulate multi year reservoir operations in a relatively short period of time facilitating its coupling with an optimization algorithm to optimize reservoir releases to minimize nutrient exports and water demand deficits loone is developed with python making it an open source tool that can be more easily accessible to a wide range of researchers and water managers loone successfully simulated the historical hydrologic conditions of lake okeechobee and the phosphorus dynamics of the lake loone was used to evaluate effects of the newly proposed regulation schedule lake okeechobee system operating manual losom on water levels and phosphorus concentrations in the lake proper and on phosphorus exports into the caloosahatchee river and st lucie canal in addition loone was used to design optimal lake okeechobee discharges into the estuaries estimating a potential reduction of p loads into caloosahatchee river and st lucie canal by 27 4 and 32 6 respectively code availability https github com osamatarabih loone programing language python 3 dependencies platypus software availability name of software loone developer osama m tarabih contact email osama m tarabih gmail com year first available 2022 programming language python dependencies platypus a framework for evolutionary evolution optimization in python salib a sensitivity analysis library in python available from https github com osamatarabih loone declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests mauricio arias reports financial support from the us national academy of sciences us environmental protection agency and the us army corps of engineers osama tarabih reports financial support in the form of a scholarship from the everglades foundation acknowledgements funding for this study initially came from a scholarship from the everglades foundation scholarship to ot and a gulf research program early career fellowship from the us national academy of sciences to mea additional funding was provided by the us environmental protection agency grant number 840090 and the us army corps of engineers engineer research and development center grant number w912hz 21 2 0057 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105603 
25464,this paper presents lake operation optimization of nutrient exports loone a python based model with three coupled modules 1 a water balance module to simulate hydrology and operations 2 a nutrient module to simulate phosphorus dynamics in the water column and 3 an optimization tool to design optimal reservoir releases we applied the model to lake okeechobee florida usa a shallow subtropical lake receiving and exporting excessive amounts of nutrients while providing water supply recreation and flood control we evaluated loone under historical operation schedules for model verification and then designed optimal releases that minimized phosphorus exports into the two main distributaries of lake okeechobee we found that p loads could decrease by 12 33 under optimal operations the software package could be used to evaluate management decisions to help control water quality issues such as high nutrient exports and harmful algal blooms in lake okeechobee and reservoirs worldwide keywords reservoir operations phosphorus dynamics optimal discharges shallow lake lake okeechobee data availability data used in this study was extracted from a publicly available database https www sfwmd gov science data dbhydro 1 introduction reservoir operation strategies determine water release and storage to facilitate flood control municipal water supply irrigation and hydropower generation among other uses in the past reservoir operations often ignored environmental objectives hu et al 2014 yielding significant alterations to natural flow regimes downstream ultimately causing severe ecological impacts bunn and arthington 2002 in recent years there has been an increasing attention towards protecting and restoring freshwater ecosystems harwood et al 2018 twardek et al 2021 wineland et al 2022 galelli et al 2022 reservoir operations affect flow velocity water levels and hydraulic retention time which affect the amount of nutrient release and retention associated with biogeochemical processes in the water column and bottom sediment vollenweider 1975 lürling et al 2016 han et al 2018 xu et al 2020b overall reservoir operations affect water quality in the reservoir and downstream rivers thus strategic management of reservoirs is crucial numerous studies have attempted to quantify aquatic environment responses to reservoir operations li et al 2018 considered specific hydrologic metrics e g monthly flow magnitudes magnitude and duration of annual extreme flows that affect ecological functions whereas szemis et al 2012 2013 considered average scores as indicators of the overall health of all species and ecological outcomes other studies hu et al 2014 rangel peraza et al 2012 xu et al 2020b you and zhang 2018 analyzed various explicit water quality indicator responses e g nutrients total dissolved solids dissolved oxygen algae temperature etc to reservoir operation parameters e g inflow release water head water exchange etc for instance aalami et al 2018 rangel peraza et al 2012 and yosefipoor et al 2022 conducted hydrodynamic and water quality simulations using the ce qual w2 model to investigate impacts of water releases on different water quality variables babamiri and marofi 2021 coupled the weap and qual2k models to investigate effects of water releases on water quality xu et al 2020 adopted artificial neural networks ann to quantify the relationship between hydrologic and ecological indicators other studies have optimized reservoir discharges to improve various water quality variables e g water temperature dissolved oxygen total dissolved solids sediment passage in reservoirs and rivers aalami et al 2018 xu et al 2020 yang et al 2012 wild et al 2018 though few studies considered nutrients explicitly babamiri and marofi 2021 xu et al 2020b yosefipoor et al 2022 moreover giuliani et al 2014 and quinn et al 2017 identified reservoir operation and p emission policies depending on critical variables in the system e g water storage water quality and water supply in summary reservoir operations have altered the natural flow regime of rivers and affected water quality in reservoirs as well as downstream water bodies yet reservoir operations could be used to improve water quality if designed appropriately chen and olden 2017 horne et al 2016 sabo et al 2017 szemis et al 2013 phosphorus p concentrations in freshwater lakes often drive water quality and limit ecosystem productivity brett and benjamin 2008 downing et al 2001 thus numerous studies have attempted to determine factors impacting p concentrations in lake water vollenweider 1975 s p mass balance approach a pillar in p modeling determines lake p concentrations based on input p mass loading rate average lake depth phosphorus retention and hydraulic loading predictive p models could be divided into static and dynamic models on the one hand static models assume steady state lake water conditions and like vollenweider 1975 determine p concentration in a lake as an empirical function of inflow p concentrations lake mean depth and water retention time bryhn and håkanson 2007 nurnberg 1984 reckhow 1988 on the other hand dynamic models comprise ordinary or partial differential equations to simulate changes in nutrient load over time but they need more input variables than static models though there have been dynamic models that consider vertical transport settling and resuspension from the bottom sediment layer håkanson and bryhn 2008 kum and burnett 2013 consideration of the effect of how surface phenomena in the bottom sediment i e desorption and adsorption which could also affect phosphorus cycle in the lake water column has been limited in addition p models have considered stratification in deep water lakes e g håkanson and bryhn 2008 komatsu et al 2006 some models have incorporated two dimensional or three dimensional grids to predict p concentrations in lakes komatsu et al 2006 such models may perform well against observations but are computationally intensive and need more resources and are not suitable for evaluating management questions and strategies that need multiple diagnostic tests there are a number of models that compute 1 d hydraulics and reservoir operations e g weap hec ressim vic res etc sei 2012 calamita et al 2021 duc dang et al 2020 klipsch and evans 2006 and others compute complex 2 d and 3 d hydrodynamics e g ce qual w2 delft3d etc baracchini et al 2020 chanudet et al 2016 kobler et al 2018 mateo lazaro et al 2020 some of these hydrodynamic models have been successfully linked to water quality and ecosystem productivity models babamiri and marofi 2021 though 3d models take long computational times and 1 d models focus more on hydrodynamic processes rather than geochemical and management aspects thus this study coupled reservoir operations and a dynamic phosphorus mass balance algorithm to support the evaluation of the impacts of reservoir management policies on phosphorus dynamics in shallow regulated lakes we developed a python based tool capable of simulating reservoirs regulated lakes with multiple inflows outflows and considering multiple processes associated with particulate and dissolved phosphorus this tool can evaluate the effects of lake operations e g outlet discharges on nutrient concentrations and exports in addition an optimization engine was developed to identify operational strategies that minimize phosphorus exports out of reservoirs and improve water quality in distributaries of the reservoirs this paper presents the development of lake operation optimization of nutrient exports loone which is an integrated water balance nutrient cycling and optimization model for regulated lakes reservoirs the water balance module adopts the reservoir continuity equation to simulate daily lake operations it also incorporates detail water balance budget variables inflows outflows rainfall evapotranspiration seepage etc and simulates lake operations based on identified operation rule curves p concentrations and exports are simulated through multiple outlets by using a deterministic mass balance module which also considers phosphorus retention and resuspension from the bottom sediment layer the optimization module is used to design lake releases with the objectives of minimizing nutrient exports into the downstream waters and improve water quality the integrated model is used to evaluate impacts of water management operations on nutrient concentrations and exports in lake okeechobee a subtropical shallow lake with a complex system of inflows and outflows lake okeechobee is a heavily regulated lake and has historically received large amounts of nutrients that have impacted water quality in the lake and receiving waters 2 model overview loone is a comprehensive water balance nutrient optimization model that comprises three coupled modules fig 1 1 a water balance module that simulates the water balance and operations of a reservoir 2 a nutrient module that simulates nutrient phosphorus dynamics in the water column and 3 an optimization engine that optimizes a reservoir s releases into its distributaries with the objective of nutrient export minimization and or water deficit minimization python 3 was chosen to develop the code because of its many high quality libraries and a powerful community support in terms of data inputs the model requires time series of tributaries flow and nutrient mass load rainfall and evapotranspiration et information on the lake and sediment properties sediment particle bulk density moisture content of each sediment type 2 1 the water balance module the water balance module simulates daily stage and storage volume of a reservoir through the release of discharges into distributaries based on the reservoir s operation rules to either fulfill specific target releases or maintain desired water storage in the reservoir the water balance module is based on the continuity equation eqn 1 δ s δ t 1 n i x 1 m o x where δ s change in reservoir storage during time step δ t i x different n inflows into the reservoir surface water i s groundwater i g w rainfall over the reservoir i r and o x different m outflows from the reservoir surface o s groundwater o g w et from the reservoir and wetlands o e t seepage losses o l applying the continuity equation to a reservoir results in the following eqn 2 d s i 1 d s i i s i g w i r i o s o g w o e t o l i where d s i represents reservoir storage volume at time step i i s and o s represent one or multiple surface water inputs and outputs respectively the water balance module simulates daily reservoir storage at each time step and then the storage is converted to reservoir stage by using the reservoir s elevation storage curve this simulated stage is used to determine the regulated releases for the next time step following the reservoir s rule curve the water balance module simulates water releases into distributaries following operation rules that define the desired amount of water release based on reservoir water levels needed to fulfill a desired water demand or to preserve specific water storage in the reservoir the module considers the tributary hydrologic condition i e wet or dry via the palmer drought severity index and the average reservoir net inflow for the previous two weeks 2 2 phosphorus module loone incorporates a phosphorus dynamic module that simulates both particulate and dissolved p concentrations in a reservoir as well as p mass loads into distributaries it also incorporates p releases from the surficial sediment layer in the reservoir bottom to the water column known as internal loading the fundamental p modeling approach follows vollenweider 1975 input output model eqn 3 to predict in lake total phosphorus tp concentrations as function of external loading hydraulic loss with lake outflows and net sedimentation fig 2 b illustrates the phosphorus module conceptually and the following paragraphs discuss the phosphorus simulation processes in detail the vollenweider 1975 p mass balance model is expressed as follows eqn 3 v δ p l a k e δ t l e x t a q o u t σ v p l a k e where p lake is the in lake tp concentration mg p m 3 v is lake volume m3 l ext is the external p loading into the lake per unit of lake area mg p m 2 d 1 a is lake area m2 q o u t is the lake discharges m3 d 1 and σ is the net sedimentation coefficient d 1 which is the first order decay coefficient that represents the net balance of two opposing processes gravitational settling of water column p versus internal loading pollman and thomas james 2011 loone s p module utilizes an input output separation system in modeling lake tp concentrations which proved better results especially for large area lakes as this method reflects the spatial gradients in p concentration fig 2a walker and havens 2003 transport of p within the reservoir was greatly simplified and simulated based on the following equations eqn 4 v i δ p i δ t l e x t a i q i o p i σ a i p i eqn 5 v o δ p o δ t q i o p i q o u t p o σ a o p o where items denoted with a subscript i represent inlet part of the lake and items denoted with the subscript o indicate outlet part of the lake q i o is flow in the lake from inlet part to outlet part m3 d 1 walker and havens 2003 internal loading significantly delays lake recovery particularly in large shallow lakes missimer et al 2021 pollman and thomas james 2011 furthermore the two major processes that likely drive internal sediment phosphorous flux in shallow lakes are diffusion and wind driven sediment resuspension fisher et al 2005 jin et al 2007 thus internal loading in our module is considered explicitly by separating σ into two components eqn 6 σ v s e t z l int a p l a k e v where v set is the gross settling velocity m d 1 z is the lake mean depth m and l int is the internal loading per unit area mg p m 2 d 1 dissolved inorganic phosphorus dip moves across the sediment water interface via diffusion associated with the concentration gradient thus internal loading flux is determined eqn 7 l int v d i f f d i p p o r e d i p l a k e θ where v d i f f is the piston velocity describing the mass transfer of dip across the sediment water interface m d 1 d i p p o r e and d i p l a k e are dip concentrations in the sediment porewater and lake water column respectively mg p m 3 and θ is the sediment s fractional porosity substituting from eqn 7 into eqn 6 and eqn 3 results in the following eqn 8 v δ p l δ t l e x t a v d i f f d i p p o r e d i p l a k e θ a s e d v s e t a p l q o p l e 0 t d e 1 τ τ c τ c e 2 where ased indicates the area of sediment in the lakebed τ c is the critical shear stress dyne cm 2 t d is the time following deposition days and e 0 e 2 are coefficients porewater dip could be in the form of sediment p p adsorbed to sediment surface or p in sediment porewater fig 2b sediment p transforms into porewater p through mineralization while p adsorbed into sediment particle surface moves between sediment and porewater via adsorption desorption phenomena porewater dip dynamics are represented as follows eqn 9 θ z s e d a s e d δ d i p p δ t v d i f f θ a s e d d i p p d i p l j d e s j a d s j d e c o m p j b u r i a l where j d e s indicates the total desorptive flux mg d 1 j a d s indicates the total adsorptive flux mg d 1 j d e c o m p indicates p flux due to mineralization and j b u r i a l indicates p sink into the deep sediment these four fluxes are estimated as described by eqns 10 13 eqn10 j d e s k d e s γ m s e d eqn 11 j a d s k a d s d i p p γ γ m s e d eqn 12 j d e c o m p k d e c o m p p s e d m s e d eqn 13 j b u r i a l v b u r i a l θ a s e d d i p p where k d e s represents the desorption rate constant d 1 m s e d indicates the mass of sediment in the surficial mixed layer kg and γ represents the amount of sorbed solute mg p kg 1 sediment k a d s represents the adsorption rate constant m3 mg 1 d 1 γ indicates the maximum concentration of sorbed p the solid phase could hold mg p kg 1 sediment k d e c o m p represents the first order decay constant d 1 p s e d represents sediment p mg p kg 1 sediment and v b u r i a l indicates burial velocity m d 1 2 3 optimization engine optimization is adopted for reservoir operation aiming at minimizing nutrient exports out of a reservoir and minimizing water demand deficits the optimization problem comprises of two objective functions o b j 1 min p l o a d exp o b j 2 min w a t e r d e m a n d d e f i c i t s where p load exp represents p load exports into distributaries and water demand deficits represent the difference between water demand and water supply the decision variables include monthly water discharges while problem constraints incorporate maintaining lake stage s within defined thresholds as well as discharge o s less than control structure s maximum discharge capacity and are formulated as follows min s s i max s 0 o s d i s c h a r g e c a p a c i t y various algorithms have been used to solve reservoir operation optimization problems aalami et al 2018 bai et al 2015 haddad et al 2006 rani and madalena 2010 saberchenari et al 2016 however evolutionary algorithms e g genetic algorithms dominate the current state of practice in operations research chang et al 2016 kerachian and karamouz 2006 maier et al 2014 yang et al 2012 yosefipoor et al 2022 thus platypus a multi objective evolutionary algorithm moea open source framework for optimization in python was adopted as optimization module hadka 2015 https github com project platypus platypus ε nsga ii moea a common technique to optimize reservoir operation e g ahmad and hossain 2020 cohen et al 2021 dang et al 2020 peñuela et al 2021 was adopted to optimize the lake operations 3 case study lake okeechobee florida usa loone was used to simulate operations and phosphorus mass balance of lake okeechobee the largest reservoir by surface area in the southern us in addition to its dimensions we chose lake okeechobee as a case study for multiple reasons first it is a multi inlet and multi outlet reservoir with a complex system of pumps and locks operated with a regulation schedule which allowed us to evaluate performance of the model in a complex hydrologic system second it is a nutrient impaired lake known to export large amount of nutrients to its surrounding water bodies tarabih and arias 2021 walker 2000 thus loone could aid evaluating impacts of lake regulations on the nutrient status of the regional system lake okeechobee plays a pivotal role in south florida s water resources management supplying water demands for the lake okeechobee service area losa which includes domestic water demand for surrounding communities supplemental irrigation water for the losa and everglades agricultural area eaa environmental flows for the st lucie and caloosahatchee estuaries and the everglades national park paudel et al 2020 cadavid et al 1999 importantly lake okeechobee protects the surrounding regions from flooding lake okeechobee receives its water and nutrients mainly from the northern watersheds through kissimmee river indian prairie canal harney pond canal fisheating creek nubbin slough and taylor creek khare et al 2019 lake okeechobee has two main distributaries to the coast the caloosahatchee river and the st lucie canal which discharge into estuaries in the gulf of mexico to the west and the atlantic ocean to the east respectively lake okeechobee also supplies water for the lower east coast and discharges flood control releases to the south through four major canals miami hillsboro north new river and west palm beach water from these southward canals is treated in large stormwater treatment wetlands which can reduce phosphorus concentrations to a great extent these canals supply water to the water conservation areas and capture excess runoff from the eaa fig 3 illustrates main tributaries and distributaries of lake okeechobee lake okeechobee is surrounded by the 229 km long herbert hoover dike which was built to provide flood protection for the surrounding regions lake stages are regulated by operational schedules and five schedules have been implemented since the construction of the dike the main objectives of all schedules have been to perform flood protection navigation and water supply for surrounding communities natural areas agricultural lands and coastal regions in 1991 run25 was proposed with the purpose of minimizing high volume damaging releases to the estuarine ecosystems in 2000 water supply and environmental wse schedule was implemented to provide better balance of water supply in lake and estuary performance and flood protection in 2008 lake okeechobee regulation schedule 2008 lors began to be implemented where the upper bound was lowered from 18 5 ft 5 64 m to 17 25 ft 5 26 m to protect the weakened dike from high water levels the us army corps of engineers is currently re evaluating lake okeechobee operations to coincide with the completion of the dike rehabilitation project expected to be complete by the end of 2022 the lake okeechobee system operating manual losom aims at balancing the congressionally authorized project purposes for water supply flood control recreation navigation and wildlife resources preservation losom aims at reducing lake okeechobee harmful regulatory releases to both st lucie and caloosahatchee estuaries and increasing flows south to the everglades supplementary materials table s3 operations of lake okeechobee have influenced nutrient concentrations in the lake and nutrient exports into the distributaries tarabih and arias 2021 most data used for the lake okeechobee case came from a single source dbhydro the main data portal of the south florida water management district sfwmd https www sfwmd gov science data dbhydro daily flow data originally in ft3 s 1 were synthesized for every tributary distributary of lake okeechobee where flow stations are available rainfall and et data were synthesized at four different gauge stations spatially distributed over the lake proper l001 l005 l006 and lz40 fig 3 daily cumulative rainfall and evapotranspiration over lake okeechobee surface were considered p concentration data included total phosphate mg l 1 and ortho phosphate dip mg l 1 in the lake water column p data were synthesized at all the inflow locations of the lake and in seven different gauge stations spatially distributed over the lake proper ortho phosphate data were synthesized from seven stations in the lake proper tp loads at different lake okeechobee tributaries were calculated by using tp concentration and flow values in the tributaries locations of all gauge stations are illustrated in the supplementary materials fig s5 losa daily water demand data for the study period 1991 2018 were synthesized from sfwmd reports sfwmd 2018 monthly seasonal multi seasonal net inflow outlook time series data as well as operation rule data threshold elevations for different lake regulatory zones and regulatory releases associated with different regulatory zones were synthesized from usace reports cadavid et al 2006 neidrauer et al 1999 usace 2008 lake sediment data particle bulk density and moisture content for the 4 different sediment types mud sand rock and peat fig 5 initial sediment p concentrations and porewater dip in the four different sediment types were synthesized from the literature fisher et al 2001 2005 olila et al 1995 olila and reddy 1993 a detailed description of how the water and p balance equations were implemented for the lake okeechobee complex system is provided in the supplementary materials 3 1 lake okeechobee model loone was used to simulate lake okeechobee regulatory releases into st lucie canal and caloosahatchee river while observed flows were used for west palm beach canal north new river canal hillsboro canal miami canal and l 8 canal see locations in fig 3 in addition water supply flows were simulated utilizing the continuity equation as mentioned in section 2 1 and lake okeechobee rule curves for the study period fig 4 we simulated three different lake okeechobee schedules during the study period 1991 2018 run25 1991 1999 wse 2000 2007 and 2008 lors 2008 2018 loone s p module simulated phosphorus dynamics in lake okeechobee by applying the equations mentioned in section 2 2 using outflow timeseries output from the water balance module as input we developed a polynomial equation between monthly average dip in the lake water column and tp in the lake for the period 1973 2019 n 560 r2 0 55 though future work could treat this regression relationship stochastically to better captures extreme dip concentrations vogel 2017 eqn 14 d i p l 8 08 0 5 p l 0 0008 p l 2 lake okeechobee bed comprises 4 sediment zones mud peat sand and rock yan and james 2013 delineated lake okeechobee bottom sediment zone boundaries based on sampling surveys high resolution aerial photography and elevation data in 1988 1998 and 2006 lake okeechobee bottom sediment map from yan and james 2013 fig 5 was incorporated into the p module to include the different sediment properties impacts on p internal loading altogether the lake was divided into eight different spatial units four sediment zones in the north and four sediment zones in the south 3 2 sensitivity analysis a sensitivity analysis was performed for lake okeechobee p module to evaluate impacts of the different parameters on p concentrations in the lake for a period of 2008 2010 and to understand the importance of each parameter on simulating p dynamics in the lake the water balance module was not tested because it depends on water mass balance and operation schedules of the lake and therefore there is limited space and need to vary input parameters the sobol method saltelli 2002 was used to evaluate the sensitivity of the phosphorus module to nine parameters burial velocity of sediments and porewater v burial eqn 13 diffusion velocity v diff eqn 7 deposition velocity for p from water column to sediments v set eqn 6 inorganic p desorption rate k des eqn 10 adsorption rate constant k ads eqn 11 first order sediment p decay constant k decomp eqn 12 external loading l e x t eqn 8 critical shear stress τ c eqn 8 and the time following deposition t d eqn 8 all initial values of these parameters were derived from the literature supplementary materials table s2 the sobol method determines a first order second order and total effect indices but only the first order and total effect indices were estimated here the first order sensitivity index si represents the additive effect of the parameter and sti the total effect sensitivity index measures the overall influence of the parameter incorporating interactions with other parameters each variable was sampled 4096 times which ended up generating 57344 individual sample sets to properly represent the global variability among parameter values 3 3 scenario design loone was used to determine the effects of alternative operation rules on phosphorus exports we simulated 2008 lors as a baseline scenario and proposed losom operations as an alternative scenario to evaluate impacts of operational modifications on phosphorus concentrations in the lake as well as exports into the estuaries in an alternative scenario we modified operations such as regulatory and water supply releases from the lake however we used the same hydrologic input data 2008 2018 to be consistent with our comparison between these scenarios 3 4 calibration and validation loone was calibrated and validated for water balance and phosphorus simulations loone s water balance simulations were verified by comparing simulated and observed daily lake okeechobee stages and discharges to northern estuaries for the three operation schedules run25 1991 1999 wse 2000 2007 and 2008 lors 2008 2018 to ensure that lake operations are incorporated correctly in the model the phosphorus module was calibrated by modifying the most sensitive p parameters table 1 to minimize the deviation between simulated and observed p concentrations in the lake water column loone s p module was calibrated by using the platypus package for the most recent operation schedule 2008 lors 2008 2018 and was validated for the two previous operation schedules run25 1991 1999 and wse 2000 2007 on a weekly basis automatic calibration was carried out to minimize the difference between simulated and measured p concentrations by using the ε nsga ii optimization algorithm besides coefficient of determination r2 the nash sutcliffe ns and normalized root mean square error nrmse coefficients were used as indicators of fitness between simulations and observations 3 5 optimization of lake okeechobee releases loone was used to design optimal releases of lake okeechobee into the caloosahatchee river and st lucie canal with the goal of demonstrating an operational schedule that can minimize pollutant exports into the estuaries while minimizing losa water deficits we developed an operation schedule that is dependent on p concentrations in the lake to guide water discharges where two p concentration thresholds were identified one for the summer and one for the winter such that if p concentration in the lake exceeds the threshold no water releases are allowed in addition the optimal operation policy depends on seasonal maximum and minimum water levels in the lake where if water level exceeds the maximum threshold maximum capacities of water control structures are discharged while no water releases are allowed if water level falls below the minimum threshold fig 11 algae blooms in st lucie canal and caloosahatchee river represent a major regional environmental issue and they are greatly influenced by nutrient laden discharges from lake okeechobee hence we focused on optimization of nutrient export from the lake to these estuaries decision variables incorporated monthly discharges into the caloosahatchee river and st lucie canal 24 decision variables while constraints included the seasonal maximum and minimum water levels in the lake and maximum discharge capacities of the control structures in the caloosahatchee river and st lucie canal s77 and s308 respectively ε nsga ii algorithm was used to solve the optimization problem which needed to run 1000 times about 3 h of cpu time on a 3 4 ghz pc to converge 4 results and discussion 4 1 phosphorous module sensitivity analysis sobol sensitivity analysis is best illustrated by the first order s1 and total effect st indexes table 1 and table s4 the sensitivity analysis of the phosphorus module resulted in parameters associated with phosphorus resuspension t d and τ c as the most sensitive parameters first order sensitivity index s1 0 068 and 0 057 respectively which makes sense since lake okeechobee is a shallow lake impacted by wind driven resuspended sediments phosphorus external loading ranked as the third most sensitive parameter s1 0 051 illustrating the influence of watershed phosphorus loadings on water quality within lake okeechobee the model was sensitive to settling adsorption desorption and mineralization parameters inorganic p moves from the pore water to sediment particles via adsorption thus adsorption controls dip in pore water which affects the amount of p released into the water column surprisingly the model was not sensitive to diffusion which could be related to higher p concentrations in the water column that resulted in a low gradient of p concentrations between sediments and water column in addition the model was not sensitive to phosphorus burial which could be because the sediments are already saturated with phosphorus limiting the potential for additional burial although total effect indices followed the same ranking as the first order indices the total order indices were less than the first order indices most probably because the interactions between each parameter does not affect the total phosphorus concentration in the lake water column sensitivity results of our study agreed with sensitivity analyses conducted by james et al 1997 2005 pollman and thomas james 2011 who suggested that tp in lake okeechobee is most sensitive to parameters affecting inorganic solids concentration in the sediments resuspension settling and phosphorus in inflowing water furthermore james et al 2005 suggested that tp in lake okeechobee was not sensitive to burial out of the system 4 2 calibration and validation the water balance module was verified for three different operation schedules fig 6 model verification for lake okeechobee water elevations during run25 operation schedule illustrated very good results with ns 0 90 and r2 0 92 run25 maintained lake okeechobee stages above 12 ft 3 66 m the model overestimated low lake levels in water year 1994 and through mar nov 1997 during wse operation schedule the model performed even better with ns 0 96 and r2 0 96 lake okeechobee stages during wse ranged between 8 and 18 ft 2 44 5 49 m except for one event in may 2001 when the lake okeechobee stage decreased to almost 7 ft 2 13 m during the wse period the model had a tendency to underestimate stages during low stage events during the 2008 lors operation schedule the model performed very good ns 0 90 and r2 0 91 the model fluctuated between over and under estimating water levels during low stage events and peak stage events those mismatches are probably associated with management decisions that are not included in normal operations specifically during emergencies due to extreme events e g droughts floods hurricanes etc water discharges into caloosahatchee river and st lucie canal were also verified for the multiple operational schedules ns and r2 values for flows into caloosahatchee river were 0 68 0 79 and 0 73 0 74 respectively ranged based on operational schedule ns and r2 values for flows into the st lucie canal were 0 17 0 72 and 0 39 0 78 respectively fig 7 illustrates monthly flow duration curves for the entire simulation period 1991 2018 for caloosahatchee river and st lucie canal simulated versus observed releases loone s p module was also calibrated validated for these operation schedules the calibrated values for the eight identified sensitive parameters are shown in table 1 see table s5 for parameters by sediment zone and fig 8 a illustrates the comparison of simulated versus observed average monthly p concentrations in lake okeechobee r2 for the calibration period 2008 2018 was 0 40 and 0 24 for the validation period 1991 2007 the model generally captured increasing and decreasing trends in p concentration though the model missed some p concentration peaks in lake okeechobee the model performed well in capturing the average annual p concentrations in lake okeechobee but underestimated p concentrations in some specific years i e 1993 1994 2004 2006 fig 8b which could be related to p sorbed to calcite in surficial sediments which is released to the water column when calcite dissolves due to thermodynamic conditions pollman and thomas james 2011 4 3 comparison of current versus proposed operating schedules after calibrating and validating the model we used the model to demonstrate its outcomes under the proposed losom operations to evaluate effects of losom on phosphorus concentrations in the lake and phosphorus exports into the caloosahatchee and st lucie estuaries as shown in fig 9 losom would maintain lake okeechobee stages higher than 2008 lors operations during both high and low conditions this increase in water levels was expected as losom would decrease water releases into the estuaries especially into the st lucie losom would decrease p loads into the st lucie canal by 40 fig 10 b associated with reduction in high volume discharges yet p loads into the caloosahatchee river would increase by 33 fig 10a and p loads into the south that would be doubled because of increased flow volumes under the losom run fig 10c 4 4 lake okeechobee optimal releases different algae species in south florida have been strongly affected by alterations in both hydrologic alterations and water quality tarboton et al 2004 harmful algal blooms in lake okeechobee associated with high p concentrations walker and havens 1995 often spread during the summer season within the lake and into the estuaries jin and ji 2013 missimer et al 2021 thus loone was used to determine optimal lake okeechobee discharges to minimize p exports into the two major distributaries during summer season may through october we developed an operation policy fig 11 recommending monthly discharges to the caloosahatchee river and the st lucie canal that are released when water levels in the lake are within the operational band i e between the orange and blue solid lines in fig 11 given that p concentrations in the lake are below p concentration thresholds 0 12 mg l 1 for the summer and 0 22 mg l 1 for the winter dashed red line in fig 11 loone provided a pareto optimal front i e five non dominated solutions connected by black lines in the right boxes of fig 12 among 100 feasible solutions the grey points in the right boxes in fig 12 providing monthly discharges into the caloosahatchee and st lucie distributaries we found that the best solution reduced p exports from 106 9 to 77 6 tons yr 1 27 4 into the caloosahatchee river and from 85 4 to 57 6 tons yr 1 32 6 into the st lucie canal fig 12 while maintaining lake okeechobee stages between 12 6 ft 3 84 m and 17 6 ft 5 36 m in addition losa water demand deficit would decrease from 35 534 ac ft 43 83 million m3 to 31 728 ac ft 39 14 million m3 per year the optimal discharges were found to be less than the historical releases during most of the summer season and maximum monthly discharges were lower especially via st lucie canal moreover the optimal operations were verified against a different historical inflows and phosphorus loads to lake okeechobee i e wse operation schedule 2000 2007 and we found that the optimal operation policy reduced phosphorus exports to caloosahatchee river by 20 4 and to st lucie canal by 12 4 fig s9 loone could be expanded to include other objectives identified by users e g navigation in the st lucie canal and caloosahatchee river in future research 4 5 comparison to other models loone provides users with an efficient tool for optimizing lake levels and discharges considering water quantity and water quality objectives loone simulates the water balance and phosphorus dynamics in multi outlet reservoirs and incorporates a multi objective optimization module to minimize nutrient exports and water demand deficits this model is rather simple by design and excludes complex hydrodynamics formulations which are computationally expensive instead this model can simulate long term operations of a reservoir efficiently computation time for a year of simulation is 4 s which makes it an appropriate tool for reservoir operation screening through which the user could simulate various alternative operations driven by multidecadal hydrologic inputs for screening and planning purposes lake okeechobee phosphorus has been simulated by using various models from simple empirical models to 3 d hydrodynamic models table 2 kratzer and brezonik 1984 and janus et al 1990 used regression empirical and simplified vollenweider type models respectively to predict p concentrations in the lake water column considering hydrology external phosphorus loadings and net phosphorus settling performance of these models was poor to modest most probably because these models do not explicitly consider internal loading of phosphorus which was proven to significantly affect phosphorus concentrations in shallow lakes like okeechobee meanwhile james et al 1997 2005 used the lake okeechobee water quality model a spatially averaged deterministic mass balance model to simulate nitrogen phosphorus oxygen and phytoplankton dynamics in the lake jin et al 1998 adopted wasp eutro a water quality analysis simulation model to simulate the dynamics of nitrogen phosphorus and phytoplankton in lake okeechobee dividing the lake into multiple horizontal segments in addition jin et al 2007 used the lake okeechobee environment model a 3 d hydrodynamic sediment and water quality model to simulate phosphorus nitrogen algae organic carbon and oxygen dynamics in the lake water column even though these models provided much value in understanding p dynamics and linkages to the aquatic ecosystem neither of these models considered effects of lake okeechobee regulation schedules on the p water quality moreover computational time associated with those models is relatively long 6 h of cpu time for 1 year simulation jin et al 2007 evaluating water and nutrient management strategies often requires a large number of model simulations and computationally expensive models are not quite useful for this purpose overall loone is very efficient in simulating reservoir operations and p dynamics in the reservoir in a short period of time 4 s for 1 year simulation making the model appropriate for screening scenario analysis and optimization as illustrated in this paper multiple approaches to optimize reservoir releases for different objectives have been developed and evaluated in multi purpose reservoirs table 3 one common approach is to simulate reservoir operations within the optimization problem using the simple water mass balance deng et al 2020 saadatpour et al 2020 yu et al 2018 xu et al 2021 though loone simulates reservoir operations following a detailed operation rule ann and statistical approaches have been adopted to quantify water quality responses to reservoir operations saadatpour et al 2020 yu et al 2018 xu et al 2021 shaw et al 2017 whereas loone simulates phosphorus process dynamics in a shallow reservoir including vertical exchange between water column and sediment bed moreover ali 2015 presented imodel to optimize flows into and through the river of grass project a restoration project in the florida everglades to achieve spatial and temporal flow and stage characteristics however imodel did not consider water quality explicitly finally loone is an open source model that could be easily modified to adopt different reservoirs various objectives and different water quality parameters 5 conclusions loone provides an effective open access tool for lake water balance and phosphorus dynamic simulation in the lake water column considering phosphorus cycling processes such as settling and internal loading from the surficial sediment layer in the reservoir bottom loone is best described as a screening tool to evaluate impacts of reservoir operations on phosphorus loadings into main distributaries for long term period simulations linked hydrodynamic water quality models often have long runtime and can be prohibitively cumbersome to use for management purpose whereas loone is highly computationally efficient and could simulate multi year reservoir operations in a relatively short period of time facilitating its coupling with an optimization algorithm to optimize reservoir releases to minimize nutrient exports and water demand deficits loone is developed with python making it an open source tool that can be more easily accessible to a wide range of researchers and water managers loone successfully simulated the historical hydrologic conditions of lake okeechobee and the phosphorus dynamics of the lake loone was used to evaluate effects of the newly proposed regulation schedule lake okeechobee system operating manual losom on water levels and phosphorus concentrations in the lake proper and on phosphorus exports into the caloosahatchee river and st lucie canal in addition loone was used to design optimal lake okeechobee discharges into the estuaries estimating a potential reduction of p loads into caloosahatchee river and st lucie canal by 27 4 and 32 6 respectively code availability https github com osamatarabih loone programing language python 3 dependencies platypus software availability name of software loone developer osama m tarabih contact email osama m tarabih gmail com year first available 2022 programming language python dependencies platypus a framework for evolutionary evolution optimization in python salib a sensitivity analysis library in python available from https github com osamatarabih loone declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests mauricio arias reports financial support from the us national academy of sciences us environmental protection agency and the us army corps of engineers osama tarabih reports financial support in the form of a scholarship from the everglades foundation acknowledgements funding for this study initially came from a scholarship from the everglades foundation scholarship to ot and a gulf research program early career fellowship from the us national academy of sciences to mea additional funding was provided by the us environmental protection agency grant number 840090 and the us army corps of engineers engineer research and development center grant number w912hz 21 2 0057 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105603 
