index,text
3495,the overexploitation of groundwater resource and its delicacy management has gained increasing attentions in recent years worldwide because of causing a series of serious environmental and geological problems currently accurately predicting the groundwater level gwl is an important issue in effective groundwater management across scales in the present study three popularly used data driven models which are an autoregressive integrated moving average arima a back propagation artificial neural network bp ann and long short term memory lstm were established in five zones with different hydrogeological properties to explore the model s accuracy in predicting the gwl at monthly and daily scales in a northern plain in china the developed models were evaluated by both the nash sutcliffe efficiency coefficient nse and root mean square error rmse the results indicate that the performance of the lstm model is best at monthly time scales with the nses greater than 0 76 and rmses smaller than 1 15 m in each zone during the training period and demonstrate a good performance at daily time scales with the nses greater than 0 9 and the rmses smaller than 0 55 m at a local area meanwhile the tempo spatial distribution of the probability of drawdowns from the lstm model was estimated by using the object oriented spatial statistical o2s2 method the results show that cumulative drawdowns greater than 10 m are mainly concentrated in water source areas with probabilities over 0 7 from 2003 to 2010 and declining to less than 0 3 from 2011 to 2014 the gwl rose generally in the study area from 2015 to 2018 but the probability of a drawdown with more than 5 m exceeded 0 8 in zone v because of continuing groundwater exploitation this study formulates a framework on developing effective data driven models for predicting the gwl across scales which have the potential to aid groundwater management keywords groundwater management data driven models lstm o2s2 the beijing plain 1 introduction groundwater is the largest reservoir of liquid freshwater resource on the planet over two billion people across the world rely on groundwater as their primary water resource and half or more of the irrigation water used to grow the world s food is supplied from underground sources famiglietti 2014 affected by climate change and human activities groundwater reserves show many uncertainties and policy management usually ignores groundwater governance resulting in the depletion of global groundwater resource richey et al 2015 according to national aeronautics and space administration s nasa s gravity recovery and climate experiment grace satellite 21 out of the world s 37 largest aquifers have fallen to the critical point of sustainability with most located in china india the united states and north africa döll et al 2014 the recovery speed of underground aquifers has difficulty meeting the growth of human water demand which could lead to a global food crisis and environmental geological problems such as land subsidence reduced river flow saltwater intrusion konikow and kendy 2005 dalin et al 2017 in 2014 the sustainable groundwater management act sgma was passed in california kiparsky 2016 to help protect groundwater resources over the long term in china a series of measures have been taken to ensure the safety of groundwater such as the implement of the middle route of the south to north water diversion snwd in december 2014 action plan for comprehensive treatment of groundwater over withdrawal in north china was enacted in january 2019 the first regulation of groundwater management has been taken effect on december 1 2021 which set out specific rules for groundwater in the areas of survey and planning conservation and protection over exploitation treatment pollution control and supervision and management most importantly the construction of china s groundwater monitoring project was fully completed in 2018 and a total of 20 469 national level groundwater professional monitoring stations were built realizing the automatic wireless collection real time transmission and data reception of groundwater level gwl and water temperature monitoring data in china according to the report of china geological survey the project generates nearly 90 million pieces of gwl water temperature and water quality data each year under such circumstance accurate prediction of the gwl have progressively been a focused question of groundwater management at both regional and local scales physical based models such as modflow feflow and tough have been widely used in gwl prediction and the quantitative evaluation of water resources due to their advantages to simulate complex groundwater systems palma and bentley 2007 feng et al 2011 hu and jiao 2015 the accuracy of the numerical models depends on a massive amount of data related to the groundwater system mainly including the stratigraphic structures boundary conditions hydrogeological parameters sources and sinks generally these data are difficult to accurately obtain especially in areas with complex geological structures it usually takes significant time to construct high precision numerical models at regional scales due to many unexpected problems such as uncertainties of parameters at small scales and effective large scale nonlinear equations solver yonas et al 2008 hu et al 2016 in the era of big data and artificial intelligence delicacy management of groundwater at small spatial scales and at daily time scales is progressively required by policy makers a serious unexpected problem occurs in current numerical models at regional scales which is the mismatch between the growing gwl data at daily or ever smaller time scales and limited information of sources and sinks for an example gwl data is sent 4 times each day by automatic wireless sensors however the amount of groundwater withdrawals can only be estimated monthly by investigation or statistics in each administrator districts instead the development of quick and effective data driven models for predicting the gwl is in urgent need with a great significance with the improvement of data sensor technologies and the advent of artificial intelligence the availability of large amounts of data is shaping a new era for applied geosciences in recent years data driven models by simplifying the complex physical systems have been confirmed to be helpful in hydrological studies shen 2018 sun and scanlon 2019 sit et al 2020 the data driven models provide practical significance for water resource management such as river flow forecasts kratzert et al 2018 yaseen et al 2020 water quality estimation najah et al 2013 castrillo and garcía 2020 water demand forecasts herrera et al 2010 adamowski et al 2012 and parameter estimation mugunthan and shoemaker 2006 traditional data driven models mainly consider the relationship between the gwl and other main controlling factors such as precipitation and groundwater withdrawal which can make highly accurate predictions possible even in a complex system sahu et al 2020 such models include multiple linear regression mlr sahoo and jha 2013 autoregressive integrated moving average arima adhikary et al 2012 mirzavand and ghazavi 2014 back propagation artificial neural network bp ann lallahem et al 2005 sahu et al 2020 support vector machine svm yoon et al 2011 and long short term memory lstm zhang et al 2018a müller et al 2020 most previous data driven models are aiming to predict the gwl at the point spatial scale and at monthly time scales müller et al 2020 however the availabilities of data driven models in groundwater managements at both regional and local spatial scales have received much less attention until recently therefore this study aimed to formulate a framework based on data driven models for effective groundwater management including accurately and quickly predicting gwl and analyzing the tempo spatial changes of gwl first the entire study area is divided into five zones according to the characteristic of groundwater system second the controlling factors that affect the changes in the gwl will be analysed by the mlr model and used to initialize three data driven empirical models third a comparative analysis of three models was conducted for simulating the variation of gwl with multi well at monthly and daily data which are arima and two state of art machine learning methods including bp ann model and lstm model finally object oriented spatial statistics o2s2 was used to analyse the tempo spatial characteristics of the gwl over the study area and evaluate the applicability of models in groundwater management 2 background of study area as the capital of china beijing is the political economic and cultural centre of the country and an international metropolis with a population of nearly 20 million the beijing plain lies in the southeast of beijing with an area of approximately 6 400 km2 and contains 15 districts arid and semiarid continental climates prevail in the study area the mean annual precipitation is 588 mm 1956 2002 and the precipitation from july to september accounts for 80 of the total annual amount of precipitation chen et al 2018 however from 1999 to 2011 beijing suffered a continual drought with an average rainfall of only 481 mm wang et al 2015 in the study area precipitation is the main source of groundwater recharge accounting for approximately 50 of the total recharge zhang et al 2017 lateral inflow from mountain areas the infiltration of surface water and farmland irrigation water are also the contributors of groundwater recharges groundwater withdrawal is the most important discharge item followed by evaporation and discharge to the rivers yang et al 2012 fig 1 shows that the change of annual groundwater withdrawal groundwater levels and precipitation from 1998 to 2018 in 1998 the annual groundwater withdrawals approximately 25 108 m3 accounting for approximately 70 of the total water supply in the study area due to continuing drought and excessive exploitation of groundwater for the period from 1998 to 2014 the average depth to groundwater in the study area dropped from 11 88 m to 25 66 m the snwd project began in december 2014 which replaced groundwater withdrawal and changed the structure of the water supply in beijing zou et al 2016 in december 2018 the amount of annual groundwater withdrawal in beijing was reduced to 16 2 108 m3 whereas the average depth to groundwater in the beijing plain was 23 03 m compared to 2014 depth to groundwater is 25 66 m the amount of groundwater resources increased from 13 8 108 m3 to 21 14 108 m and the average gwl rose by 2 63 m beijing water authority 2018a the beijing plain is mainly composed of alluvial fans that are formed by the interaction of the yongding river chaobai river juma river dashi river ju river and cuo river among them the yongding river and chaobai river alluvial fans are adjacent to one another and intersect almost controlling the entire plain area the terrain is higher in the northwest and lower in the southeast from the front of the mountain to the plain the thickness of the quaternary gradually increases and the aquifer gradually evolves from a single aquifer to multiple layers with particles varying from coarse to fine lin et al 2015 chen et al 2018 regionally the groundwater flows from the northwest to the southeast fig 2 shows the location of the study area and the alluvial fan divisions according to previous studies beijing geological and mineral exploration and development bureau 2008 each zone can be regarded as a sub hydrogeological unit specifically zones i and iv are the main water supply sources of the beijing city the first and second water sources in beijing are located in zone i the eighth water source plant and the huairou emergency water sources are located in zone iv long term overexploitation of groundwater has formed obvious cone of depression in the chaobai alluvial fan in recent years ecological water replenishment projects have been implemented in the chaobai river and the yongding river zones ii and iii mainly gain recharges from precipitation infiltration and discharge mainly in the forms of groundwater withdrawal and underground runoff zone v is distributed in the pinggu emergency water sources and receives recharge from the ju river as well as precipitation infiltration 3 materials and methods the flowchart of this research is described in fig 3 at first data from different sources will be prepared and five sub hydrogeological zones i ii iii iv v will be divided according to quaternary groundwater system in the study area after that the controlling factors that affect the changes of the gwl will be analysed by using the mlr model besides data driven models will be established and evaluated in the study area finally object oriented spatial statistics o2s2 is used to analyse the probability of drawdown changes in 110 observation wells to keep the consistent with the observation data this study replaces the variable of the gwl with the depth to groundwater gwd the details of the data and the adopted methodology are described as follows 3 1 data the data used in this study are listed in table 1 mainly including precipitation groundwater withdrawal and gwd data the precipitation data of monthly scale is obtained from trmm satellite with a spatial resolution of 0 25 0 25 which is proved to be accurate in previous studies yin et al 2018 the monthly groundwater withdrawal in 5 zones and gwds data from 110 observation wells monitoring shallow gwl during 2003 to 2018 are obtained from the beijing hydrological reports the daily data during the ecological restoration period from april 19 2020 to may 31 2020 is from the beijing water authority website 3 2 methods for predicting changes in the groundwater level three commonly used data driven models which are arima model bp ann model and lstm model represents traditional statistical models machine learning models and deep learning models respectively the arima model can identify complex patterns in time series data and generate forecasts widely used in statistical models bp ann method is the simplest neural network model in machine learning in addition with the wide application of deep learning in the field of hydrology and water resources lstm model is used for regression prediction of time series and have shown good performances sit et al 2020 3 2 1 multiple linear regression model multiple regression is a type of statistical method for establishing a linear relationship between several independent or predictor variables and a dependent or criterion variable pedhazur 1982 in this study the mlr model is mainly used to analyse the main factors that affect the changes in the gwd the main factors may include precipitation groundwater withdrawal and historical gwd then a statistical regression equation can be established 1 h t β 0 β 1 p t β 3 q t β 3 q t 1 h t 1 where h is the gwd of one observation well at time t p is the precipitation t is time usually is monthly q is the groundwater withdrawal at a panel particular wells q t 1 is groundwater withdrawal from last month h t 1 is gwd from last month and β is the regression coefficient 3 2 2 autoregressive integrated moving average model the arima methodology has the ability to identify complex patterns in data and generate forecasts box and jenkins 1976 the arima model is used to analyze and forecast univariate time series data the arima model represented by p d q includes three main parameters autoregressive order p difference order d and moving average order q the detailed description please refer to the literature adamowski et al 2012 in this study the method takes the time series of the gwd as a set of random sequences approximately describes them with the associated mathematical models and predicts the future based on the historical values of the gwd and the current status first the order of difference is determined by the trial error method and dickey fuller is used to check whether the difference time series is stationary then the order of autoregressive and moving average is given according to the change of time series data then the established arima model will be trained and used to predict the change of the gwd finally the prediction result is differentially restored to obtain the final simulation result 3 2 3 back propagation artificial neural network model an artificial neural network is an abstraction and simulation of the information processing process of the human brain s neural network and it is a nonlinear information processing method the back propagation model is one of the most widely used artificial neural network models coulibaly et al 2001 yang and chen 2009 the bp ann model consists of three layers an input layer a hidden layer and an output layer each layer is composed of one or several neurons there is no connection between the neurons in the layer and the neurons between the layers are connected according to the connection degree of different weights the state of each neuron is determined by the threshold value and the activation function when the total input exceeds the threshold value the neurons will be activated and the neural network will be active the method of meeting processing information is determined by the activation function haykin 1995 banerjee et al 2008 the training process for bp ann includes the forward propagation of information and the back propagation of error karnin 1990 the input information arrives at the output layer through the input layer and the hidden layer in turn when the error between the output result and the expected value exceeds the given error the error is passed back the model obtains the output value again by modifying the weight value of the neuron connection and trains repeatedly until it meets the accuracy requirements in this study the influencing factors determined by the mlr model are used as the input layer variables of the model and the gwd is used as the output layer variables the number of hidden layers and neurons of each layer is designed by adjusting the relative parameters and rectified linear unit relu is selected as activation function 3 2 4 long short term memory model the lstm neural network hochreiter and schmidhuber 1997 is a special recurrent neural network that can store and correlate the previous information and can effectively solve the problem of gradient disappearance and gradient explosion in the process of long sequence training the lstm model includes four parts the input gate forgetting gate output gate and cell state the input gate determines how much input information is transmitted to the cell state the forgetting gate mainly controls how much information in the cell state of the previous period is forgotten and how much information is transmitted to the current moment the output gate outputs the calculation results based on the cell state updated by the forgetting gate and the input gate the cell state is used to record the current input the last hidden layer state the last cell state and the information in the gate structure this study uses the tensorflow platform in python to develop the lstm model the influencing factors determined by the mlr model are used as the training feature value and adaptive moment estimation adam is used as the optimizer 3 3 evaluation metrics the evaluation criteria in this study include the nash sutcliffe efficiency coefficient nse the root mean square error rmse and the relative error re nse is used to evaluate the degree of fit the regression models the rmse quantifies how closely the predicted values match the observed values re refers to the value obtained by multiplying the ratio of the absolute error caused by the measurement to the true value of the measured conventional by 100 the results where nse is close to 1 0 and rmse and re are close to 0 indicate that the models are more reliable 2 nse 1 i 1 n x i y i 2 i 1 n x i x i 2 3 rmse i 1 n x i y i 2 n 4 re x i y i y i 100 where x i y i is the observed gwd simulated by different models x i is the mean of the observed gwd i is the serial number of observation wells and n is the total number of observation wells 3 4 object oriented spatial statistics for the changes in the groundwater level the analysis of complex spatial objects currently plays a key role in a variety of data driven engineering and geoscience applications menafoglio and secchi 2017 spatial analyses cannot ignore the data heterogeneity and complexity o2s2 is a new branch of statistics that aims to provide a unified view for solving various application challenges of modern spatial statistics by grounding the analysis on a powerful geometrical and topological approach which embraces the idea of object oriented data analysis ooda the seminal name chosen by wang and marron 2007 to baptize a system of ideas o2s2 rooted in the interpretation of the data point e g the curve image or network as the atom of the statistical analysis menafoglio and secchi 2019 and meets the need to analyse populations of spatially dependent object data menafoglio and secchi 2017 described the local distributions of grain sizes at 406 locations along 12 boreholes within the aquifer system by probability density function in the neckar river valley they menafoglio and secchi 2019 also predicted the probability density function of dissolved oxygen within a large estuarine system which is regularly monitored to assess the impact of human activities on aquatic variables deemed critical for its ecosystem the literature menafoglio and secchi 2017 gave more detailed description of the o2s2 method in addition a few r packages are already available for o2s2 allowing for spatial simulation and kriging with different kinds of data grujic and menafoglio 2017 sartori and torriani 2019 in this study data from 110 monitoring wells were used to calculate the monthly cumulative drawdown of gwl the cumulative drawdown probability of each monitoring well represents integral of probability density function of predefined gwl decline for example changes of the area with the decline of gwl less than 5 m receives the serious attention therefore the cumulative probability of gwl decline in wells can be represented by a series of spatially distributed constraint curves in the study area which helps to effectively enhance the understanding of regional gwl changes object oriented kriging is an interpolation technique to estimate gwl for unknown data points within the range of a discrete set of measured data points which gives the best linear unbiased estimation of the intermediate values and is widely used in the domain of spatial analysis kleijnen and mehdad 2014 this study uses the idea of o2s2 to calculate the probability of a gwl cumulative drawdown at each monitoring well in the study area and object oriented kriging to interpolate it into a two dimensional space 4 results different input variables influence the prediction results of the data driven models in this study mlr methods are used to analyse the factors affecting changes in the groundwater long term series data from 2003 to 2018 were used to construct regression models among gwd precipitation and groundwater withdrawal the data from 2003 to 2016 were used to train the models and the data from 2017 to 2018 were used for prediction 4 1 controlling factors affecting changes in the groundwater level first zone i was selected as a typical partition considering the lag characteristics of recharge and discharge three types of models are considered for different input variables the first model uses monthly gwd denoted by h t in the region as the dependent variable and monthly precipitation denoted by p t and monthly withdrawal denoted by q t as independent variables the second model adds groundwater withdrawal at last month denoted by q t 1 based on the first model and the third model adds gwd data at last month denoted by h t 1 based on the second model the established regression models are expressed in table 2 and the f test and t test are performed for each model all three models for different input variables pass the f test among them the r 2 of the third model is 0 89 which is greatly improved compared to the regression effect of the first and second models when the significance level α is set as 0 05 the f test of each model is performed and the overall linear relationship of each model is significant the monthly precipitation and withdrawal failed to pass the t test indicating that the precipitation and groundwater withdrawal data at last month have no obvious influence on the changes in the gwd in the region the independent variables determined in the third model were introduced into the other four zones to establish multiple regression models and both the f test and t test were performed with the results shown in table 3 the r 2 of the models established are greater than 0 85 indicating a good degree of fit the model evaluation results thus show that the four models passed the f test while the groundwater withdrawal at last month of four zones did not pass the t test which indicates that the influence of groundwater withdrawal at last month is not significant considering the high correlation coefficients of the models the third model was used for a further predictive analysis using the established mlr models of the five zones the gwd of the five zones from 2017 to 2018 was predicted fig 4 shows that the selected model has good prediction results in the five zones the relative errors between the predicted and observed values of the five zones are almost less than 5 and the average relative errors from zones i to v are 0 76 2 46 0 73 1 04 and 1 87 respectively considering the strong correlation between the selected variables and the gwd the precipitation and groundwater withdrawal in the current year and the gwd and groundwater withdrawal at last month are selected as the input variables of the data driven models 4 2 comparisons of results from the data driven models the gwd for each observation well in each zone will be simulated by using three data driven models for the same zone the groundwater withdrawals keep the same the input variables are normalized to data range from 0 to 1 before model training which ensures that during the process of iterative weight adjustment a percentage change in the weighted input sample is reflected with a similar percentage change at the nodes of the output layer kanellopoulos and wilkinson 1997 the simulation period including training validation and prediction period are set from 2003 to 2018 after the cross validation the training period validation period and prediction period are obtained from 2003 to 2014 2015 to 2017 and 2018 respectively the settings of each model parameter are obtained after parameter optimization and the setting of hyperparameters in lstm and bp ann models are shown in table 4 the parameters p d q of the arima model is optimized as 1 2 and 1 respectively and thus the arima 1 2 1 model is established table 5 shows comprehensive comparisons among the arima bp ann and lstm models for different study periods and zones lstm model performs best during the training period with the nses in each zone greater than 0 76 the performance of the arima model is the worst with nses only is 0 33 in zone ii and 0 42 in zone iv during the validation period the nses of the lstm model are greater than 0 49 and better than the other two models except for zone v the arima model performed best during the validation period in the zone v however the lstm model is more reliable and stable for both training and validation period fig 5 indicates the distribution of rmses and nses from the three models during the validation period the largest errors are present in zone iv and v for three models probably because of the heterogeneous of river infiltration during ecological water replenishment implemented in zone iv and groundwater withdrawals includes both the porous and karst aquifers in zone v during the validation period which may be alleviated and improved by the amount of data used in the training meanwhile the error of the gwl from the lstm model during the validation period is smaller than that in our previous numerical model zhang et al 2018b the numbers of observation wells with absolute errors of less than 1 m and 3 m account for 92 and 100 in the lstm model whereas those two numbers are 40 and 80 in the numerical model respectively in general the lstm model performs best in the entire research field compared to other two data driven models in this study it is also be noted that the calibration of numerical models usually takes hydrologist months to prepare necessary data and carry out pre processing model calibration model application and post processing whereas the machine learning method only takes a few days to find the optimal setting of the hyperparameters and model construction chen et al 2020 in this study the total time consumption for the training validation of 110 observation wells in our personal computer intel r core tm i7 8700 cpu 3 20 ghz 12 and 31 8 gib memory is 35 seconds 534 seconds and 774 seconds for the arima model the bp ann model and the lstm model respectively although the lstm model takes much time for the total calculation the accuracy of the lstm model ranks the top among the three models and the efficiency may be improved by using super computer 4 3 groundwater level predicted by the lstm model the datasets in 2018 are used for model prediction by the lstm model typical monitoring wells are selected from each zone to compare the prediction results of the gwd with the observed values fig 6 demonstrates the changes in the gwd during training period validation period and the prediction period calculated by lstm model from which it can be observed that the patterns of the simulated gwd keep the same as the observed results average rmse of observation wells in each zone from lstm model during the prediction period is 0 71 0 6 0 64 1 19 and 1 98 for zone i ii iii iv and v respectively the average rmses in zone iv and v are greater than 1 m as discussed above although the lstm model performs best in three data driven models the general bad matches with the observed results occur in zone iv and v for both the validation and prediction periods one of the main cause may be that the period for model training is short after the implement of water replenishment of the snwd project and thus the general gwl rose to some degree during the validation and predication periods which is not captured by the lstm model in the training period another important reason may be that distribution of groundwater withdrawals in zone iv and v is subject to significant changes during the period of validation and prediction because the amount of groundwater withdrawals in the two zones decreases during the period from 2016 to 2018 it should be noted that the lstm model still produced reasonable results for zone i ii and iii during both the validation and prediction period under such circumstance the accuracy of the lstm model may be improved significantly after enough training data is fed to model 5 discussion 5 1 application of the lstm model at daily time scales in order to verify that the developed lstm model can be used for groundwater management zone i is selected as an example to demonstrate the applicability of the model a significant ecological water replenishment project was carried out in zone i during the period of april 19 to may 31 2020 sun et al 2021 the daily data including precipitation groundwater supply river runoff and gwd was prepared and used for the trained lstm model under the condition that the structure and hyperparameters of the trained model remain unchanged according to previous studies hu et al 2020 gwd in 10 observation wells shown in fig 2 have quick response to the ecological water replenishment project and are simulated since the short daily scale data was collected training and validation without prediction were carried out accounting for 80 and 20 respectively fig 7 shows that the changes of gwd during both training period and validation period from the lstm model and observed gwd with time of the 4 observation wells generally the rmses of 10 wells are smaller than 0 5 m and the nses are greater than 0 9 during training period and also showed better performance during validation period with rmses are smaller than 0 55 m the results show that the trained lstm model can be used for the simulation for both monthly and daily time scales 5 2 tempo spatial distribution of the groundwater level the established data driven models can simulate changes in the gwl in each observation well at different time periods whereas the spatial distribution of the gwl may be estimated in december 2010 the state council of china issued the decisions of the central committee of the communist party of china and the state council on accelerating water conservancy reform and development which was aimed at strengthening the management of groundwater resource in china in december 2014 the middle route of the snwd was implemented providing water source guarantees for cities in northern china including the study area therefore obvious change of gwl in the study area may be observed in three time periods which are 2003 2010 2011 2014 and 2015 2018 based on the predict results of the lstm model and then converting the gwd into gwl data the ordinary kriging method is used to analyse the tempo spatial changes in the gwl in the study area fig 8 shows that the groundwater in the study area flowed from the northwest to the southeast from 2003 to 2018 and that the local groundwater flow patterns changed slightly at different times for example from 2003 to 2010 the water source area in zones iv and v the gwl declined and the cone of depression continued to expand the groundwater flow pattern in the study area did not change significantly from 2011 to 2014 comparing to the gwl between 2014 and 2018 it can be determined that the gwl rebounded greatly in 2018 and that the area of the landing cone of depression was reduced in zones i iv and v 5 3 probability statistical distribution of groundwater level change to further understand the law of the gwl change in the study area this study uses the o2s2 idea to perform a statistical analysis on the probability of the gwd change in the study area the gwd in 2003 2011 and 2015 is chosen as initial reference gwd at the different stages during the period of 2003 2010 2011 2014 and 2015 2018 respectively and then the drawdown probability of 110 observation wells was estimated and the probability density function distribution curve of the cumulative drawdown was drawn for each well shown in fig 9 a negative drawdown indicates that the gwd of the observation wells decreases and the gwl rises fig 9 a indicates that the cumulative drawdown of observation wells during this period is likely to be concentrated between 10 m and 10 m from 2003 to 2010 and that observation wells with drawdowns greater than 10 m also occupy a larger part fig 9 b shows that the drawdowns of the observation wells during this period were also concentrated between 10 m and 10 m from 2011 to 2014 however compared with 2003 to 2010 the probability of each monitoring well having a gwl drawdown greater than 10 m is almost zero and the cumulative water level rise of some observation wells is concentrated within 10 m fig 9 c demonstrates that many observation wells were concentrated between 5 m and 5 m from 2015 to 2018 indicating that the numbers and probability of observation wells with a raised gwl also increased the cumulative drawdown probability density curve reflects the probability distribution of a single observation well to observe the spatial distribution of the drawdown probability more intuitively according to the critical value of drawdown probability distribution range in different stages in fig 9 cumulative drawdowns with values of 5 m and 10 m are selected as the critical values during 2003 2010 and 2011 2014 respectively and then 1 m and 5 m are selected as the critical values during 2011 2014 and 2015 2018 respectively the ordinary kriging method is used to draw the cumulative drawdown probability spatial distribution map fig 10 a and b shows that from 2003 to 2010 the probability of cumulative drawdown exceeding 5 m in the study area is mainly located in zone i with a probability of over 0 8 and second distributed in zone v the probability exceeds 0 4 the areas where the cumulative drawdown was greater than 10 m were mainly concentrated in zones iii and v and the probabilities were all over 0 7 the pinggu emergency water source is one of the emergency water source projects constructed to alleviate the increasingly tense water supply situation of the beijing urban area before the water transfer project of the snwd which led to a gwl decline in zone v in zone iv areas with larger cumulative drawdowns were mainly distributed in the water source areas during this period the study area suffered a continuous drought groundwater was the main source of water supply and overexploitation was the main reason for the decline in gwl from 2011 to 2014 fig 10 c and d shows that the areas with a cumulative drawdown greater than 5 m and a probability greater than 0 5 are scattered mainly located in the piedmont plains and near the concentrated water source area the probability is lower than 0 3 with a cumulative drawdown of more than 10 m compared with the results during 2003 2010 the probabilities of a drawdown greater than 5 m and 10 m are much smaller fig 11 a and b reveals that the areas with a drawdown probability greater than 1 m are mainly in zones i ii iii and v indicating that measures to strengthen groundwater resource management are effective and to a large extent alleviate the trend of a drastic decline in the gwl from 2015 to 2018 fig 11 c and d shows that the probability of the gwd drawdown greater than 1 m in zone iv has increased while there is a significant decrease in zone i the areas where the probability exceeds 0 6 of a drawdown greater than 5 m are mainly distributed in zone v and the upper part of zone iv especially in zone v with a probability above 0 8 compared with 2011 to 2014 the probability exceeding 0 6 of drawdown greater than 5 m sharply decreased in zones i iii and iv indicating that the gwl decline was controlled during this time period this has a great relationship with the reduction of groundwater exploitation and the use of water from the snwd for groundwater replenishment the area where the probability of the gwd drawdown in zone v is greater than 5 m has increased significantly indicating that the gwl in zone v still declines from 2015 to 2018 the monitored data shows that gwd of emergency water sources in zone v increased from 29 84 m to 33 83 m from 2014 to 2017 and decreased to 24 63 m from 2017 to 2018 for the implementation of groundwater replenishment measures this fully reveals that the measures for groundwater resource management are effective and that the increase in water replenishment measures has greatly reduced the decreasing trend of gwl 5 4 relationship between water supply and groundwater level changes annual water use data from beijing water resources bulletin beijing water authority 2018b in terms of water source e g groundwater and surface water use and sector e g agricultural and domestic use were examined to understand the variability of gwl spatial distribution fig 12 shows the water supply and water use ratio in the study area domestic water use accounts for the largest share of the total water use with the development of economic and improvement of life the slight increase in domestic water use was driven primarily by increases in water use by households and the service sector long et al 2020 from 2003 to 2010 the agricultural water use is kept at a high level accounting for more than 30 of total water use groundwater as the mainly resource accounting for more than 60 of total water supply groundwater overexploitation results in a continuous increase in gwd after 2011 groundwater resource management was strengthened and key measures such as controlling the total amount of groundwater use improving the water use efficiency were implemented effectively agricultural water use accounting for decreasing from 30 3 in 2011 to 10 7 in 2014 of total water use and groundwater supply accounting for decreasing to 41 2 of total water supply thus the probability of a substantial drawdown in the study area was reduced after 2015 the increase of snwd water and reclaimed water use further instead of groundwater withdrawal and the implementation of groundwater source replenishment projects have effectively slowed the depletion of groundwater resource in addition the emphasis on environmental protection has resulted in a rapid increase in the proportion of environmental water use from 1 7 in 2003 to 34 1 in 2018 which has the most significant changes in the four water use sectors environmental water use mainly from reclaimed water and transfer water the increase of environmental water use also increases the recharge of groundwater resources and to a certain extent can conserve groundwater sources and protect the sustainable development and utilization of groundwater restore the gwl gradually 6 conclusions to meet the needs of groundwater prediction supported by high frequency national gwl monitoring data data driven models were developed to accurately predict gwl changes and analyse their practical significance in groundwater resource management this study took the beijing plain as an example and used mlr models to determine the main factors affecting gwl changes in five zones the accuracy of the three popularly used data driven models was discussed at both monthly and daily time scales the tempo spatial changes in the gwl in the study area were analysed the conclusions of this study are as follows 1 based on the constructed mlr model the main factors that affect the changes of gwd in the study area at regional scales are precipitation and groundwater withdrawal in the current year and the gwd and groundwater withdrawal at last month which are used as input variables of the data driven models 2 after evaluating the accuracy of the three data driven models the lstm model has the best performance with the nses in each zone greater than 0 76 and rmses smaller than 1 15 m during the training period the two numbers are 0 49 and 0 71 m except for zone v during the training period which is better than arima and bp ann models the accuracy of data driven models in prediction period may be affected by the multi layer aquifer and training data 3 the model we trained can not only be used for the simulation of monthly scale data but also has achieved better results at daily scales with the rmses smaller than 0 55 m and the nses greater than 0 9 and the constructed model has the potential for near real time management 4 the study on the spatial distribution of the probability of a drawdown in different stages from 2003 to 2018 reveals that the gwl decreasing trend is different in five zones at different time periods which is related to the utilization of water resources during the period the probability of the gwl in the water source area dropped by more than 10 m all over 0 7 from 2003 to 2010 and declined to less than 0 3 from 2011 to 2014 since 2015 the gwl in zone v has continued to decline while the gwl decline has been gradually controlled to a certain degree in other zones 5 during the continuous drought from 2003 to 2010 overexploitation of groundwater resulted in a rapid decline in the gwl forming a cone of depression in the water source area and expanding continuously from 2011 to 2014 the downward trend of the gwl slowed down with strengthened groundwater resource management after the implementation of the snwd project the water use structure was changed and the gwl in the study area generally increased the accuracy of the data driven models largely depends on the rich training data input the model results from the developed lstm model during the validation and predication period are not well match with observed results in zone iv and v probably because of the limited training data input after the unsteady groundwater system is subject to major changes such as significant reduction of groundwater withdrawals numerical models may be applied to enhance the generalization ability of data driven models applicability of the data driven models at daily time scales is only verified in zone i for certain time periods due to limited data collected however the data driven models can quickly and accurately simulate changes of gwl as well as demonstrating the tempo spatial distribution of gwl changes after applying the appropriate spatial interpolation method the data driven models should be dynamically trained year by year and thus have the potential to better serve near real time groundwater management credit authorship contribution statement jianchong sun methodology writing original draft writing review editing litang hu conceptualization methodology writing review editing dandan li data collection analysis kangning sun writing review editing zhengqiu yang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the national natural science foundation of china grant nos 41877173 and 41831283 we also thanks for constructive comments from editors and anonymous reviewers to improve the quality of our manuscript 
3495,the overexploitation of groundwater resource and its delicacy management has gained increasing attentions in recent years worldwide because of causing a series of serious environmental and geological problems currently accurately predicting the groundwater level gwl is an important issue in effective groundwater management across scales in the present study three popularly used data driven models which are an autoregressive integrated moving average arima a back propagation artificial neural network bp ann and long short term memory lstm were established in five zones with different hydrogeological properties to explore the model s accuracy in predicting the gwl at monthly and daily scales in a northern plain in china the developed models were evaluated by both the nash sutcliffe efficiency coefficient nse and root mean square error rmse the results indicate that the performance of the lstm model is best at monthly time scales with the nses greater than 0 76 and rmses smaller than 1 15 m in each zone during the training period and demonstrate a good performance at daily time scales with the nses greater than 0 9 and the rmses smaller than 0 55 m at a local area meanwhile the tempo spatial distribution of the probability of drawdowns from the lstm model was estimated by using the object oriented spatial statistical o2s2 method the results show that cumulative drawdowns greater than 10 m are mainly concentrated in water source areas with probabilities over 0 7 from 2003 to 2010 and declining to less than 0 3 from 2011 to 2014 the gwl rose generally in the study area from 2015 to 2018 but the probability of a drawdown with more than 5 m exceeded 0 8 in zone v because of continuing groundwater exploitation this study formulates a framework on developing effective data driven models for predicting the gwl across scales which have the potential to aid groundwater management keywords groundwater management data driven models lstm o2s2 the beijing plain 1 introduction groundwater is the largest reservoir of liquid freshwater resource on the planet over two billion people across the world rely on groundwater as their primary water resource and half or more of the irrigation water used to grow the world s food is supplied from underground sources famiglietti 2014 affected by climate change and human activities groundwater reserves show many uncertainties and policy management usually ignores groundwater governance resulting in the depletion of global groundwater resource richey et al 2015 according to national aeronautics and space administration s nasa s gravity recovery and climate experiment grace satellite 21 out of the world s 37 largest aquifers have fallen to the critical point of sustainability with most located in china india the united states and north africa döll et al 2014 the recovery speed of underground aquifers has difficulty meeting the growth of human water demand which could lead to a global food crisis and environmental geological problems such as land subsidence reduced river flow saltwater intrusion konikow and kendy 2005 dalin et al 2017 in 2014 the sustainable groundwater management act sgma was passed in california kiparsky 2016 to help protect groundwater resources over the long term in china a series of measures have been taken to ensure the safety of groundwater such as the implement of the middle route of the south to north water diversion snwd in december 2014 action plan for comprehensive treatment of groundwater over withdrawal in north china was enacted in january 2019 the first regulation of groundwater management has been taken effect on december 1 2021 which set out specific rules for groundwater in the areas of survey and planning conservation and protection over exploitation treatment pollution control and supervision and management most importantly the construction of china s groundwater monitoring project was fully completed in 2018 and a total of 20 469 national level groundwater professional monitoring stations were built realizing the automatic wireless collection real time transmission and data reception of groundwater level gwl and water temperature monitoring data in china according to the report of china geological survey the project generates nearly 90 million pieces of gwl water temperature and water quality data each year under such circumstance accurate prediction of the gwl have progressively been a focused question of groundwater management at both regional and local scales physical based models such as modflow feflow and tough have been widely used in gwl prediction and the quantitative evaluation of water resources due to their advantages to simulate complex groundwater systems palma and bentley 2007 feng et al 2011 hu and jiao 2015 the accuracy of the numerical models depends on a massive amount of data related to the groundwater system mainly including the stratigraphic structures boundary conditions hydrogeological parameters sources and sinks generally these data are difficult to accurately obtain especially in areas with complex geological structures it usually takes significant time to construct high precision numerical models at regional scales due to many unexpected problems such as uncertainties of parameters at small scales and effective large scale nonlinear equations solver yonas et al 2008 hu et al 2016 in the era of big data and artificial intelligence delicacy management of groundwater at small spatial scales and at daily time scales is progressively required by policy makers a serious unexpected problem occurs in current numerical models at regional scales which is the mismatch between the growing gwl data at daily or ever smaller time scales and limited information of sources and sinks for an example gwl data is sent 4 times each day by automatic wireless sensors however the amount of groundwater withdrawals can only be estimated monthly by investigation or statistics in each administrator districts instead the development of quick and effective data driven models for predicting the gwl is in urgent need with a great significance with the improvement of data sensor technologies and the advent of artificial intelligence the availability of large amounts of data is shaping a new era for applied geosciences in recent years data driven models by simplifying the complex physical systems have been confirmed to be helpful in hydrological studies shen 2018 sun and scanlon 2019 sit et al 2020 the data driven models provide practical significance for water resource management such as river flow forecasts kratzert et al 2018 yaseen et al 2020 water quality estimation najah et al 2013 castrillo and garcía 2020 water demand forecasts herrera et al 2010 adamowski et al 2012 and parameter estimation mugunthan and shoemaker 2006 traditional data driven models mainly consider the relationship between the gwl and other main controlling factors such as precipitation and groundwater withdrawal which can make highly accurate predictions possible even in a complex system sahu et al 2020 such models include multiple linear regression mlr sahoo and jha 2013 autoregressive integrated moving average arima adhikary et al 2012 mirzavand and ghazavi 2014 back propagation artificial neural network bp ann lallahem et al 2005 sahu et al 2020 support vector machine svm yoon et al 2011 and long short term memory lstm zhang et al 2018a müller et al 2020 most previous data driven models are aiming to predict the gwl at the point spatial scale and at monthly time scales müller et al 2020 however the availabilities of data driven models in groundwater managements at both regional and local spatial scales have received much less attention until recently therefore this study aimed to formulate a framework based on data driven models for effective groundwater management including accurately and quickly predicting gwl and analyzing the tempo spatial changes of gwl first the entire study area is divided into five zones according to the characteristic of groundwater system second the controlling factors that affect the changes in the gwl will be analysed by the mlr model and used to initialize three data driven empirical models third a comparative analysis of three models was conducted for simulating the variation of gwl with multi well at monthly and daily data which are arima and two state of art machine learning methods including bp ann model and lstm model finally object oriented spatial statistics o2s2 was used to analyse the tempo spatial characteristics of the gwl over the study area and evaluate the applicability of models in groundwater management 2 background of study area as the capital of china beijing is the political economic and cultural centre of the country and an international metropolis with a population of nearly 20 million the beijing plain lies in the southeast of beijing with an area of approximately 6 400 km2 and contains 15 districts arid and semiarid continental climates prevail in the study area the mean annual precipitation is 588 mm 1956 2002 and the precipitation from july to september accounts for 80 of the total annual amount of precipitation chen et al 2018 however from 1999 to 2011 beijing suffered a continual drought with an average rainfall of only 481 mm wang et al 2015 in the study area precipitation is the main source of groundwater recharge accounting for approximately 50 of the total recharge zhang et al 2017 lateral inflow from mountain areas the infiltration of surface water and farmland irrigation water are also the contributors of groundwater recharges groundwater withdrawal is the most important discharge item followed by evaporation and discharge to the rivers yang et al 2012 fig 1 shows that the change of annual groundwater withdrawal groundwater levels and precipitation from 1998 to 2018 in 1998 the annual groundwater withdrawals approximately 25 108 m3 accounting for approximately 70 of the total water supply in the study area due to continuing drought and excessive exploitation of groundwater for the period from 1998 to 2014 the average depth to groundwater in the study area dropped from 11 88 m to 25 66 m the snwd project began in december 2014 which replaced groundwater withdrawal and changed the structure of the water supply in beijing zou et al 2016 in december 2018 the amount of annual groundwater withdrawal in beijing was reduced to 16 2 108 m3 whereas the average depth to groundwater in the beijing plain was 23 03 m compared to 2014 depth to groundwater is 25 66 m the amount of groundwater resources increased from 13 8 108 m3 to 21 14 108 m and the average gwl rose by 2 63 m beijing water authority 2018a the beijing plain is mainly composed of alluvial fans that are formed by the interaction of the yongding river chaobai river juma river dashi river ju river and cuo river among them the yongding river and chaobai river alluvial fans are adjacent to one another and intersect almost controlling the entire plain area the terrain is higher in the northwest and lower in the southeast from the front of the mountain to the plain the thickness of the quaternary gradually increases and the aquifer gradually evolves from a single aquifer to multiple layers with particles varying from coarse to fine lin et al 2015 chen et al 2018 regionally the groundwater flows from the northwest to the southeast fig 2 shows the location of the study area and the alluvial fan divisions according to previous studies beijing geological and mineral exploration and development bureau 2008 each zone can be regarded as a sub hydrogeological unit specifically zones i and iv are the main water supply sources of the beijing city the first and second water sources in beijing are located in zone i the eighth water source plant and the huairou emergency water sources are located in zone iv long term overexploitation of groundwater has formed obvious cone of depression in the chaobai alluvial fan in recent years ecological water replenishment projects have been implemented in the chaobai river and the yongding river zones ii and iii mainly gain recharges from precipitation infiltration and discharge mainly in the forms of groundwater withdrawal and underground runoff zone v is distributed in the pinggu emergency water sources and receives recharge from the ju river as well as precipitation infiltration 3 materials and methods the flowchart of this research is described in fig 3 at first data from different sources will be prepared and five sub hydrogeological zones i ii iii iv v will be divided according to quaternary groundwater system in the study area after that the controlling factors that affect the changes of the gwl will be analysed by using the mlr model besides data driven models will be established and evaluated in the study area finally object oriented spatial statistics o2s2 is used to analyse the probability of drawdown changes in 110 observation wells to keep the consistent with the observation data this study replaces the variable of the gwl with the depth to groundwater gwd the details of the data and the adopted methodology are described as follows 3 1 data the data used in this study are listed in table 1 mainly including precipitation groundwater withdrawal and gwd data the precipitation data of monthly scale is obtained from trmm satellite with a spatial resolution of 0 25 0 25 which is proved to be accurate in previous studies yin et al 2018 the monthly groundwater withdrawal in 5 zones and gwds data from 110 observation wells monitoring shallow gwl during 2003 to 2018 are obtained from the beijing hydrological reports the daily data during the ecological restoration period from april 19 2020 to may 31 2020 is from the beijing water authority website 3 2 methods for predicting changes in the groundwater level three commonly used data driven models which are arima model bp ann model and lstm model represents traditional statistical models machine learning models and deep learning models respectively the arima model can identify complex patterns in time series data and generate forecasts widely used in statistical models bp ann method is the simplest neural network model in machine learning in addition with the wide application of deep learning in the field of hydrology and water resources lstm model is used for regression prediction of time series and have shown good performances sit et al 2020 3 2 1 multiple linear regression model multiple regression is a type of statistical method for establishing a linear relationship between several independent or predictor variables and a dependent or criterion variable pedhazur 1982 in this study the mlr model is mainly used to analyse the main factors that affect the changes in the gwd the main factors may include precipitation groundwater withdrawal and historical gwd then a statistical regression equation can be established 1 h t β 0 β 1 p t β 3 q t β 3 q t 1 h t 1 where h is the gwd of one observation well at time t p is the precipitation t is time usually is monthly q is the groundwater withdrawal at a panel particular wells q t 1 is groundwater withdrawal from last month h t 1 is gwd from last month and β is the regression coefficient 3 2 2 autoregressive integrated moving average model the arima methodology has the ability to identify complex patterns in data and generate forecasts box and jenkins 1976 the arima model is used to analyze and forecast univariate time series data the arima model represented by p d q includes three main parameters autoregressive order p difference order d and moving average order q the detailed description please refer to the literature adamowski et al 2012 in this study the method takes the time series of the gwd as a set of random sequences approximately describes them with the associated mathematical models and predicts the future based on the historical values of the gwd and the current status first the order of difference is determined by the trial error method and dickey fuller is used to check whether the difference time series is stationary then the order of autoregressive and moving average is given according to the change of time series data then the established arima model will be trained and used to predict the change of the gwd finally the prediction result is differentially restored to obtain the final simulation result 3 2 3 back propagation artificial neural network model an artificial neural network is an abstraction and simulation of the information processing process of the human brain s neural network and it is a nonlinear information processing method the back propagation model is one of the most widely used artificial neural network models coulibaly et al 2001 yang and chen 2009 the bp ann model consists of three layers an input layer a hidden layer and an output layer each layer is composed of one or several neurons there is no connection between the neurons in the layer and the neurons between the layers are connected according to the connection degree of different weights the state of each neuron is determined by the threshold value and the activation function when the total input exceeds the threshold value the neurons will be activated and the neural network will be active the method of meeting processing information is determined by the activation function haykin 1995 banerjee et al 2008 the training process for bp ann includes the forward propagation of information and the back propagation of error karnin 1990 the input information arrives at the output layer through the input layer and the hidden layer in turn when the error between the output result and the expected value exceeds the given error the error is passed back the model obtains the output value again by modifying the weight value of the neuron connection and trains repeatedly until it meets the accuracy requirements in this study the influencing factors determined by the mlr model are used as the input layer variables of the model and the gwd is used as the output layer variables the number of hidden layers and neurons of each layer is designed by adjusting the relative parameters and rectified linear unit relu is selected as activation function 3 2 4 long short term memory model the lstm neural network hochreiter and schmidhuber 1997 is a special recurrent neural network that can store and correlate the previous information and can effectively solve the problem of gradient disappearance and gradient explosion in the process of long sequence training the lstm model includes four parts the input gate forgetting gate output gate and cell state the input gate determines how much input information is transmitted to the cell state the forgetting gate mainly controls how much information in the cell state of the previous period is forgotten and how much information is transmitted to the current moment the output gate outputs the calculation results based on the cell state updated by the forgetting gate and the input gate the cell state is used to record the current input the last hidden layer state the last cell state and the information in the gate structure this study uses the tensorflow platform in python to develop the lstm model the influencing factors determined by the mlr model are used as the training feature value and adaptive moment estimation adam is used as the optimizer 3 3 evaluation metrics the evaluation criteria in this study include the nash sutcliffe efficiency coefficient nse the root mean square error rmse and the relative error re nse is used to evaluate the degree of fit the regression models the rmse quantifies how closely the predicted values match the observed values re refers to the value obtained by multiplying the ratio of the absolute error caused by the measurement to the true value of the measured conventional by 100 the results where nse is close to 1 0 and rmse and re are close to 0 indicate that the models are more reliable 2 nse 1 i 1 n x i y i 2 i 1 n x i x i 2 3 rmse i 1 n x i y i 2 n 4 re x i y i y i 100 where x i y i is the observed gwd simulated by different models x i is the mean of the observed gwd i is the serial number of observation wells and n is the total number of observation wells 3 4 object oriented spatial statistics for the changes in the groundwater level the analysis of complex spatial objects currently plays a key role in a variety of data driven engineering and geoscience applications menafoglio and secchi 2017 spatial analyses cannot ignore the data heterogeneity and complexity o2s2 is a new branch of statistics that aims to provide a unified view for solving various application challenges of modern spatial statistics by grounding the analysis on a powerful geometrical and topological approach which embraces the idea of object oriented data analysis ooda the seminal name chosen by wang and marron 2007 to baptize a system of ideas o2s2 rooted in the interpretation of the data point e g the curve image or network as the atom of the statistical analysis menafoglio and secchi 2019 and meets the need to analyse populations of spatially dependent object data menafoglio and secchi 2017 described the local distributions of grain sizes at 406 locations along 12 boreholes within the aquifer system by probability density function in the neckar river valley they menafoglio and secchi 2019 also predicted the probability density function of dissolved oxygen within a large estuarine system which is regularly monitored to assess the impact of human activities on aquatic variables deemed critical for its ecosystem the literature menafoglio and secchi 2017 gave more detailed description of the o2s2 method in addition a few r packages are already available for o2s2 allowing for spatial simulation and kriging with different kinds of data grujic and menafoglio 2017 sartori and torriani 2019 in this study data from 110 monitoring wells were used to calculate the monthly cumulative drawdown of gwl the cumulative drawdown probability of each monitoring well represents integral of probability density function of predefined gwl decline for example changes of the area with the decline of gwl less than 5 m receives the serious attention therefore the cumulative probability of gwl decline in wells can be represented by a series of spatially distributed constraint curves in the study area which helps to effectively enhance the understanding of regional gwl changes object oriented kriging is an interpolation technique to estimate gwl for unknown data points within the range of a discrete set of measured data points which gives the best linear unbiased estimation of the intermediate values and is widely used in the domain of spatial analysis kleijnen and mehdad 2014 this study uses the idea of o2s2 to calculate the probability of a gwl cumulative drawdown at each monitoring well in the study area and object oriented kriging to interpolate it into a two dimensional space 4 results different input variables influence the prediction results of the data driven models in this study mlr methods are used to analyse the factors affecting changes in the groundwater long term series data from 2003 to 2018 were used to construct regression models among gwd precipitation and groundwater withdrawal the data from 2003 to 2016 were used to train the models and the data from 2017 to 2018 were used for prediction 4 1 controlling factors affecting changes in the groundwater level first zone i was selected as a typical partition considering the lag characteristics of recharge and discharge three types of models are considered for different input variables the first model uses monthly gwd denoted by h t in the region as the dependent variable and monthly precipitation denoted by p t and monthly withdrawal denoted by q t as independent variables the second model adds groundwater withdrawal at last month denoted by q t 1 based on the first model and the third model adds gwd data at last month denoted by h t 1 based on the second model the established regression models are expressed in table 2 and the f test and t test are performed for each model all three models for different input variables pass the f test among them the r 2 of the third model is 0 89 which is greatly improved compared to the regression effect of the first and second models when the significance level α is set as 0 05 the f test of each model is performed and the overall linear relationship of each model is significant the monthly precipitation and withdrawal failed to pass the t test indicating that the precipitation and groundwater withdrawal data at last month have no obvious influence on the changes in the gwd in the region the independent variables determined in the third model were introduced into the other four zones to establish multiple regression models and both the f test and t test were performed with the results shown in table 3 the r 2 of the models established are greater than 0 85 indicating a good degree of fit the model evaluation results thus show that the four models passed the f test while the groundwater withdrawal at last month of four zones did not pass the t test which indicates that the influence of groundwater withdrawal at last month is not significant considering the high correlation coefficients of the models the third model was used for a further predictive analysis using the established mlr models of the five zones the gwd of the five zones from 2017 to 2018 was predicted fig 4 shows that the selected model has good prediction results in the five zones the relative errors between the predicted and observed values of the five zones are almost less than 5 and the average relative errors from zones i to v are 0 76 2 46 0 73 1 04 and 1 87 respectively considering the strong correlation between the selected variables and the gwd the precipitation and groundwater withdrawal in the current year and the gwd and groundwater withdrawal at last month are selected as the input variables of the data driven models 4 2 comparisons of results from the data driven models the gwd for each observation well in each zone will be simulated by using three data driven models for the same zone the groundwater withdrawals keep the same the input variables are normalized to data range from 0 to 1 before model training which ensures that during the process of iterative weight adjustment a percentage change in the weighted input sample is reflected with a similar percentage change at the nodes of the output layer kanellopoulos and wilkinson 1997 the simulation period including training validation and prediction period are set from 2003 to 2018 after the cross validation the training period validation period and prediction period are obtained from 2003 to 2014 2015 to 2017 and 2018 respectively the settings of each model parameter are obtained after parameter optimization and the setting of hyperparameters in lstm and bp ann models are shown in table 4 the parameters p d q of the arima model is optimized as 1 2 and 1 respectively and thus the arima 1 2 1 model is established table 5 shows comprehensive comparisons among the arima bp ann and lstm models for different study periods and zones lstm model performs best during the training period with the nses in each zone greater than 0 76 the performance of the arima model is the worst with nses only is 0 33 in zone ii and 0 42 in zone iv during the validation period the nses of the lstm model are greater than 0 49 and better than the other two models except for zone v the arima model performed best during the validation period in the zone v however the lstm model is more reliable and stable for both training and validation period fig 5 indicates the distribution of rmses and nses from the three models during the validation period the largest errors are present in zone iv and v for three models probably because of the heterogeneous of river infiltration during ecological water replenishment implemented in zone iv and groundwater withdrawals includes both the porous and karst aquifers in zone v during the validation period which may be alleviated and improved by the amount of data used in the training meanwhile the error of the gwl from the lstm model during the validation period is smaller than that in our previous numerical model zhang et al 2018b the numbers of observation wells with absolute errors of less than 1 m and 3 m account for 92 and 100 in the lstm model whereas those two numbers are 40 and 80 in the numerical model respectively in general the lstm model performs best in the entire research field compared to other two data driven models in this study it is also be noted that the calibration of numerical models usually takes hydrologist months to prepare necessary data and carry out pre processing model calibration model application and post processing whereas the machine learning method only takes a few days to find the optimal setting of the hyperparameters and model construction chen et al 2020 in this study the total time consumption for the training validation of 110 observation wells in our personal computer intel r core tm i7 8700 cpu 3 20 ghz 12 and 31 8 gib memory is 35 seconds 534 seconds and 774 seconds for the arima model the bp ann model and the lstm model respectively although the lstm model takes much time for the total calculation the accuracy of the lstm model ranks the top among the three models and the efficiency may be improved by using super computer 4 3 groundwater level predicted by the lstm model the datasets in 2018 are used for model prediction by the lstm model typical monitoring wells are selected from each zone to compare the prediction results of the gwd with the observed values fig 6 demonstrates the changes in the gwd during training period validation period and the prediction period calculated by lstm model from which it can be observed that the patterns of the simulated gwd keep the same as the observed results average rmse of observation wells in each zone from lstm model during the prediction period is 0 71 0 6 0 64 1 19 and 1 98 for zone i ii iii iv and v respectively the average rmses in zone iv and v are greater than 1 m as discussed above although the lstm model performs best in three data driven models the general bad matches with the observed results occur in zone iv and v for both the validation and prediction periods one of the main cause may be that the period for model training is short after the implement of water replenishment of the snwd project and thus the general gwl rose to some degree during the validation and predication periods which is not captured by the lstm model in the training period another important reason may be that distribution of groundwater withdrawals in zone iv and v is subject to significant changes during the period of validation and prediction because the amount of groundwater withdrawals in the two zones decreases during the period from 2016 to 2018 it should be noted that the lstm model still produced reasonable results for zone i ii and iii during both the validation and prediction period under such circumstance the accuracy of the lstm model may be improved significantly after enough training data is fed to model 5 discussion 5 1 application of the lstm model at daily time scales in order to verify that the developed lstm model can be used for groundwater management zone i is selected as an example to demonstrate the applicability of the model a significant ecological water replenishment project was carried out in zone i during the period of april 19 to may 31 2020 sun et al 2021 the daily data including precipitation groundwater supply river runoff and gwd was prepared and used for the trained lstm model under the condition that the structure and hyperparameters of the trained model remain unchanged according to previous studies hu et al 2020 gwd in 10 observation wells shown in fig 2 have quick response to the ecological water replenishment project and are simulated since the short daily scale data was collected training and validation without prediction were carried out accounting for 80 and 20 respectively fig 7 shows that the changes of gwd during both training period and validation period from the lstm model and observed gwd with time of the 4 observation wells generally the rmses of 10 wells are smaller than 0 5 m and the nses are greater than 0 9 during training period and also showed better performance during validation period with rmses are smaller than 0 55 m the results show that the trained lstm model can be used for the simulation for both monthly and daily time scales 5 2 tempo spatial distribution of the groundwater level the established data driven models can simulate changes in the gwl in each observation well at different time periods whereas the spatial distribution of the gwl may be estimated in december 2010 the state council of china issued the decisions of the central committee of the communist party of china and the state council on accelerating water conservancy reform and development which was aimed at strengthening the management of groundwater resource in china in december 2014 the middle route of the snwd was implemented providing water source guarantees for cities in northern china including the study area therefore obvious change of gwl in the study area may be observed in three time periods which are 2003 2010 2011 2014 and 2015 2018 based on the predict results of the lstm model and then converting the gwd into gwl data the ordinary kriging method is used to analyse the tempo spatial changes in the gwl in the study area fig 8 shows that the groundwater in the study area flowed from the northwest to the southeast from 2003 to 2018 and that the local groundwater flow patterns changed slightly at different times for example from 2003 to 2010 the water source area in zones iv and v the gwl declined and the cone of depression continued to expand the groundwater flow pattern in the study area did not change significantly from 2011 to 2014 comparing to the gwl between 2014 and 2018 it can be determined that the gwl rebounded greatly in 2018 and that the area of the landing cone of depression was reduced in zones i iv and v 5 3 probability statistical distribution of groundwater level change to further understand the law of the gwl change in the study area this study uses the o2s2 idea to perform a statistical analysis on the probability of the gwd change in the study area the gwd in 2003 2011 and 2015 is chosen as initial reference gwd at the different stages during the period of 2003 2010 2011 2014 and 2015 2018 respectively and then the drawdown probability of 110 observation wells was estimated and the probability density function distribution curve of the cumulative drawdown was drawn for each well shown in fig 9 a negative drawdown indicates that the gwd of the observation wells decreases and the gwl rises fig 9 a indicates that the cumulative drawdown of observation wells during this period is likely to be concentrated between 10 m and 10 m from 2003 to 2010 and that observation wells with drawdowns greater than 10 m also occupy a larger part fig 9 b shows that the drawdowns of the observation wells during this period were also concentrated between 10 m and 10 m from 2011 to 2014 however compared with 2003 to 2010 the probability of each monitoring well having a gwl drawdown greater than 10 m is almost zero and the cumulative water level rise of some observation wells is concentrated within 10 m fig 9 c demonstrates that many observation wells were concentrated between 5 m and 5 m from 2015 to 2018 indicating that the numbers and probability of observation wells with a raised gwl also increased the cumulative drawdown probability density curve reflects the probability distribution of a single observation well to observe the spatial distribution of the drawdown probability more intuitively according to the critical value of drawdown probability distribution range in different stages in fig 9 cumulative drawdowns with values of 5 m and 10 m are selected as the critical values during 2003 2010 and 2011 2014 respectively and then 1 m and 5 m are selected as the critical values during 2011 2014 and 2015 2018 respectively the ordinary kriging method is used to draw the cumulative drawdown probability spatial distribution map fig 10 a and b shows that from 2003 to 2010 the probability of cumulative drawdown exceeding 5 m in the study area is mainly located in zone i with a probability of over 0 8 and second distributed in zone v the probability exceeds 0 4 the areas where the cumulative drawdown was greater than 10 m were mainly concentrated in zones iii and v and the probabilities were all over 0 7 the pinggu emergency water source is one of the emergency water source projects constructed to alleviate the increasingly tense water supply situation of the beijing urban area before the water transfer project of the snwd which led to a gwl decline in zone v in zone iv areas with larger cumulative drawdowns were mainly distributed in the water source areas during this period the study area suffered a continuous drought groundwater was the main source of water supply and overexploitation was the main reason for the decline in gwl from 2011 to 2014 fig 10 c and d shows that the areas with a cumulative drawdown greater than 5 m and a probability greater than 0 5 are scattered mainly located in the piedmont plains and near the concentrated water source area the probability is lower than 0 3 with a cumulative drawdown of more than 10 m compared with the results during 2003 2010 the probabilities of a drawdown greater than 5 m and 10 m are much smaller fig 11 a and b reveals that the areas with a drawdown probability greater than 1 m are mainly in zones i ii iii and v indicating that measures to strengthen groundwater resource management are effective and to a large extent alleviate the trend of a drastic decline in the gwl from 2015 to 2018 fig 11 c and d shows that the probability of the gwd drawdown greater than 1 m in zone iv has increased while there is a significant decrease in zone i the areas where the probability exceeds 0 6 of a drawdown greater than 5 m are mainly distributed in zone v and the upper part of zone iv especially in zone v with a probability above 0 8 compared with 2011 to 2014 the probability exceeding 0 6 of drawdown greater than 5 m sharply decreased in zones i iii and iv indicating that the gwl decline was controlled during this time period this has a great relationship with the reduction of groundwater exploitation and the use of water from the snwd for groundwater replenishment the area where the probability of the gwd drawdown in zone v is greater than 5 m has increased significantly indicating that the gwl in zone v still declines from 2015 to 2018 the monitored data shows that gwd of emergency water sources in zone v increased from 29 84 m to 33 83 m from 2014 to 2017 and decreased to 24 63 m from 2017 to 2018 for the implementation of groundwater replenishment measures this fully reveals that the measures for groundwater resource management are effective and that the increase in water replenishment measures has greatly reduced the decreasing trend of gwl 5 4 relationship between water supply and groundwater level changes annual water use data from beijing water resources bulletin beijing water authority 2018b in terms of water source e g groundwater and surface water use and sector e g agricultural and domestic use were examined to understand the variability of gwl spatial distribution fig 12 shows the water supply and water use ratio in the study area domestic water use accounts for the largest share of the total water use with the development of economic and improvement of life the slight increase in domestic water use was driven primarily by increases in water use by households and the service sector long et al 2020 from 2003 to 2010 the agricultural water use is kept at a high level accounting for more than 30 of total water use groundwater as the mainly resource accounting for more than 60 of total water supply groundwater overexploitation results in a continuous increase in gwd after 2011 groundwater resource management was strengthened and key measures such as controlling the total amount of groundwater use improving the water use efficiency were implemented effectively agricultural water use accounting for decreasing from 30 3 in 2011 to 10 7 in 2014 of total water use and groundwater supply accounting for decreasing to 41 2 of total water supply thus the probability of a substantial drawdown in the study area was reduced after 2015 the increase of snwd water and reclaimed water use further instead of groundwater withdrawal and the implementation of groundwater source replenishment projects have effectively slowed the depletion of groundwater resource in addition the emphasis on environmental protection has resulted in a rapid increase in the proportion of environmental water use from 1 7 in 2003 to 34 1 in 2018 which has the most significant changes in the four water use sectors environmental water use mainly from reclaimed water and transfer water the increase of environmental water use also increases the recharge of groundwater resources and to a certain extent can conserve groundwater sources and protect the sustainable development and utilization of groundwater restore the gwl gradually 6 conclusions to meet the needs of groundwater prediction supported by high frequency national gwl monitoring data data driven models were developed to accurately predict gwl changes and analyse their practical significance in groundwater resource management this study took the beijing plain as an example and used mlr models to determine the main factors affecting gwl changes in five zones the accuracy of the three popularly used data driven models was discussed at both monthly and daily time scales the tempo spatial changes in the gwl in the study area were analysed the conclusions of this study are as follows 1 based on the constructed mlr model the main factors that affect the changes of gwd in the study area at regional scales are precipitation and groundwater withdrawal in the current year and the gwd and groundwater withdrawal at last month which are used as input variables of the data driven models 2 after evaluating the accuracy of the three data driven models the lstm model has the best performance with the nses in each zone greater than 0 76 and rmses smaller than 1 15 m during the training period the two numbers are 0 49 and 0 71 m except for zone v during the training period which is better than arima and bp ann models the accuracy of data driven models in prediction period may be affected by the multi layer aquifer and training data 3 the model we trained can not only be used for the simulation of monthly scale data but also has achieved better results at daily scales with the rmses smaller than 0 55 m and the nses greater than 0 9 and the constructed model has the potential for near real time management 4 the study on the spatial distribution of the probability of a drawdown in different stages from 2003 to 2018 reveals that the gwl decreasing trend is different in five zones at different time periods which is related to the utilization of water resources during the period the probability of the gwl in the water source area dropped by more than 10 m all over 0 7 from 2003 to 2010 and declined to less than 0 3 from 2011 to 2014 since 2015 the gwl in zone v has continued to decline while the gwl decline has been gradually controlled to a certain degree in other zones 5 during the continuous drought from 2003 to 2010 overexploitation of groundwater resulted in a rapid decline in the gwl forming a cone of depression in the water source area and expanding continuously from 2011 to 2014 the downward trend of the gwl slowed down with strengthened groundwater resource management after the implementation of the snwd project the water use structure was changed and the gwl in the study area generally increased the accuracy of the data driven models largely depends on the rich training data input the model results from the developed lstm model during the validation and predication period are not well match with observed results in zone iv and v probably because of the limited training data input after the unsteady groundwater system is subject to major changes such as significant reduction of groundwater withdrawals numerical models may be applied to enhance the generalization ability of data driven models applicability of the data driven models at daily time scales is only verified in zone i for certain time periods due to limited data collected however the data driven models can quickly and accurately simulate changes of gwl as well as demonstrating the tempo spatial distribution of gwl changes after applying the appropriate spatial interpolation method the data driven models should be dynamically trained year by year and thus have the potential to better serve near real time groundwater management credit authorship contribution statement jianchong sun methodology writing original draft writing review editing litang hu conceptualization methodology writing review editing dandan li data collection analysis kangning sun writing review editing zhengqiu yang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the national natural science foundation of china grant nos 41877173 and 41831283 we also thanks for constructive comments from editors and anonymous reviewers to improve the quality of our manuscript 
3496,this corrigendum is related to bretreger et al 2020 where a range of landsat 5 7 and 8 observations were used to monitor irrigated cropping conditions and using these observations in three modelling approaches with meteorological data to quantify the irrigation area level water use i e actual evapotranspiration aet one of the remote sensing aet methods was cmrset guerschman et al 2009 and in a response to bretreger et al 2020 the authors peña arancibia et al 2021 clarified the implementation of cmrset by using the landsat swir2 band 2090 2350 nm wavelength using this landsat band matched the naming of the band used in the cmrset development with modis i e swir2 despite having a different wavelength 1628 1652 nm wavelength between the two remote sensing instruments the swir bands from modis and landsat may match in naming but their wavelengths differ this means that the landsat swir1 band 1550 1750 nm wavelength should have been used when implementing cmrset with landsat to optimally represent the cmrset method rather than using landsat swir2 2090 2350 nm wavelength as performed in bretreger et al 2020 the analysis contained in this corrigendum acknowledges this mistake and shows the impact of this on the estimated irrigation results demonstrating that cmrset is a viable option for the aet component of methodologies that are attempting to quantify regional irrigation water use other results originally documented in bretreger et al 2020 specifically those generated using the irrisat and kamble aet methods were not impacted by the sub optimal use of the landsat swir2 band 1 corrigendum analysis to clarify the outcomes of the band mismatch between landsat swir1 and swir2 we have re analysed these simulations and data inputs to investigate the changes in irrigation output that were highlighted by peña arancibia et al 2021 to do this the cmrset method was re analysed using both the landsat swir1 and swir2 bands additionally the method used in bretreger et al 2020 was expanded to include a recently updated version of cmrset described by guerschman et al 2022 i e cmrset v2 results using the cmrset coefficients reported in guerschman et al 2009 are denoted cmrset v1 this was performed over 4 of the 5 original sites in nsw australia including murray irrigation mulwala murray irrigation wakool murrumbidgee irrigation and coleambally irrigation full site descriptions are provided in bretreger et al 2020 the period analysed was january 2010 to december 2017 for all sites using the landsat series of satellites smaller periods are reported when observed data are the limiting factor while the major intent of this corrigendum is to clarify the results of the cmrset method the results of the irrisat and kamble methods are included where relevant to give an indication of how these methods compare fig 1 shows the time series of estimated irrigation across the study sites of the corrigendum included in the plot are the re analysis of the cmrset method using both landsat swir1 1550 1750 nm wavelength and landsat swir2 2090 2350 nm wavelength as well as cmrset v2 correctly implemented with only landsat swir1 there is a clear improvement in simulation when using the correct landsat swir1 band rather than sub optimal for cmrset landsat swir2 the result from the updated cmrset v2 improved further upon this result the grey shaded areas are the water years that experienced drought conditions bretreger et al 2020 the root mean square error rmse and the coefficient of determination r2 of the re analysis is calculated for each of the sites and each method used to calculate the estimated irrigation a combined analysis using all available observed data and modelled output has also been perfromed this is presented in table 1 this includes a re analysis of the irrisat and kamble methods as a comparison the result here agrees with the time series in fig 1 in that correctly implementing cmrset with landsat swir1 band results show an improvement in estimated irrigation and cmrset v2 reduces the error even further 2 findings differences in the implementation of cmrset using landsat swir1 or using landsat swir2 bands have been explored in this corrigendum the swir bands were implemented incorrectly in the original article due to confusing band naming conventions across multiple remote sensing instruments and any potential users of the method outlined in bretreger et al 2020 should be aware of this like peña arancibia et al 2021 we encourage users to report band names band numbers and wavelengths the latter if only one is reported when documenting remote sensing dependent methods while the major conclusion of bretreger et al 2020 remains the same that is remote sensing is a viable option for quantifying regional irrigation water use we now also conclude that the cmrset method including cmrset v2 is a viable option alongside kamble and irrisat for calculating aet for regional irrigation quantification doi of original article https doi org 10 1016 j jhydrol 2020 125356 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors acknowledge jorge peña arancibia and tim mcvicar both csiro land and water for their discussions and assistance with re analysis of the cmrset and cmrset v2 methods 
3496,this corrigendum is related to bretreger et al 2020 where a range of landsat 5 7 and 8 observations were used to monitor irrigated cropping conditions and using these observations in three modelling approaches with meteorological data to quantify the irrigation area level water use i e actual evapotranspiration aet one of the remote sensing aet methods was cmrset guerschman et al 2009 and in a response to bretreger et al 2020 the authors peña arancibia et al 2021 clarified the implementation of cmrset by using the landsat swir2 band 2090 2350 nm wavelength using this landsat band matched the naming of the band used in the cmrset development with modis i e swir2 despite having a different wavelength 1628 1652 nm wavelength between the two remote sensing instruments the swir bands from modis and landsat may match in naming but their wavelengths differ this means that the landsat swir1 band 1550 1750 nm wavelength should have been used when implementing cmrset with landsat to optimally represent the cmrset method rather than using landsat swir2 2090 2350 nm wavelength as performed in bretreger et al 2020 the analysis contained in this corrigendum acknowledges this mistake and shows the impact of this on the estimated irrigation results demonstrating that cmrset is a viable option for the aet component of methodologies that are attempting to quantify regional irrigation water use other results originally documented in bretreger et al 2020 specifically those generated using the irrisat and kamble aet methods were not impacted by the sub optimal use of the landsat swir2 band 1 corrigendum analysis to clarify the outcomes of the band mismatch between landsat swir1 and swir2 we have re analysed these simulations and data inputs to investigate the changes in irrigation output that were highlighted by peña arancibia et al 2021 to do this the cmrset method was re analysed using both the landsat swir1 and swir2 bands additionally the method used in bretreger et al 2020 was expanded to include a recently updated version of cmrset described by guerschman et al 2022 i e cmrset v2 results using the cmrset coefficients reported in guerschman et al 2009 are denoted cmrset v1 this was performed over 4 of the 5 original sites in nsw australia including murray irrigation mulwala murray irrigation wakool murrumbidgee irrigation and coleambally irrigation full site descriptions are provided in bretreger et al 2020 the period analysed was january 2010 to december 2017 for all sites using the landsat series of satellites smaller periods are reported when observed data are the limiting factor while the major intent of this corrigendum is to clarify the results of the cmrset method the results of the irrisat and kamble methods are included where relevant to give an indication of how these methods compare fig 1 shows the time series of estimated irrigation across the study sites of the corrigendum included in the plot are the re analysis of the cmrset method using both landsat swir1 1550 1750 nm wavelength and landsat swir2 2090 2350 nm wavelength as well as cmrset v2 correctly implemented with only landsat swir1 there is a clear improvement in simulation when using the correct landsat swir1 band rather than sub optimal for cmrset landsat swir2 the result from the updated cmrset v2 improved further upon this result the grey shaded areas are the water years that experienced drought conditions bretreger et al 2020 the root mean square error rmse and the coefficient of determination r2 of the re analysis is calculated for each of the sites and each method used to calculate the estimated irrigation a combined analysis using all available observed data and modelled output has also been perfromed this is presented in table 1 this includes a re analysis of the irrisat and kamble methods as a comparison the result here agrees with the time series in fig 1 in that correctly implementing cmrset with landsat swir1 band results show an improvement in estimated irrigation and cmrset v2 reduces the error even further 2 findings differences in the implementation of cmrset using landsat swir1 or using landsat swir2 bands have been explored in this corrigendum the swir bands were implemented incorrectly in the original article due to confusing band naming conventions across multiple remote sensing instruments and any potential users of the method outlined in bretreger et al 2020 should be aware of this like peña arancibia et al 2021 we encourage users to report band names band numbers and wavelengths the latter if only one is reported when documenting remote sensing dependent methods while the major conclusion of bretreger et al 2020 remains the same that is remote sensing is a viable option for quantifying regional irrigation water use we now also conclude that the cmrset method including cmrset v2 is a viable option alongside kamble and irrisat for calculating aet for regional irrigation quantification doi of original article https doi org 10 1016 j jhydrol 2020 125356 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors acknowledge jorge peña arancibia and tim mcvicar both csiro land and water for their discussions and assistance with re analysis of the cmrset and cmrset v2 methods 
3497,accurate runoff prediction is critical for various fields of hydrology agriculture and environmental studies numerous hydrologic models have been developed and demonstrate good performances in runoff simulation however errors are inherent in forecasted runoff predictions which can cause uncertainty in real time flood warning systems in order to improve the predictive performance of hydrologic modeling this study used a deep learning approach as a post processor to correct for errors associated with hydrologic data the proposed model uses the long short term memory model with sequence to sequence structure as a post processor to improve runoff forecasting specifically the deep learning approach was used to estimate errors in forecasted hourly runoff provided from national water model in russian river basin california united states error prediction in hourly runoff with lead times between 1 and 18 h were developed using observed precipitation and errors from upstream stream gages to improve the predictive performance of national water model the predictive performance of the model was evaluated using numerous statistical metrics and results show that the long short term memory model with sequence to sequence post processor improved runoff predictions compared to standalone results from the national water model statistical values of percent bias decreased from a range of 60 80 to 15 10 when the post processor model was used and similarly root mean square errors of runoff prediction decreased from 120 cms to 20 cms thus this study demonstrates the power of deep learning model to improve hydrologic modeling results especially those with short forecasting lead times keywords deep learning error prediction lstm s2s national water model 1 introduction accurately simulating relationships between rainfall and runoff is important for various water management purposes particularly flood forecasting shrestha and solomatine 2008 neitsch et al 2011 hu et al 2018 fan et al 2020 xiang et al 2020 physical based numerical hydrological models are often used to simulate nonlinear rainfall runoff relationships and these models are subsequently applied in various hydrologic applications including short term simulations for flood prediction and long term simulations for drought analysis and water resources management ott et al 1991 lee et al 2005 wu et al 2011 kang and sridhar 2017 wang et al 2017 these models generally contain multiple parameters used to calibrate hydrological processes and to minimize output residuals or errors in hydrological models errors inherent in the flow prediction results can lead to inaccurate hydrological analyses decision makers operating water related infrastructures for flood and drought forecasting irrigation control and other water management purposes require reliable hydrologic modeling results that consider uncertainties inherent in the results shrestha and solomatine 2008 proper error estimation and prediction can enhance the reliability and credibility of hydrologic modeling outputs for water management and improve our understanding of error propagations within modeling frameworks krzysztofowicz 2001 differences between modeled results and observed data e g differences between modeled streamflow estimates and measured discharge can be caused by four general sources within a hydrologic model a uncertainties or inaccuracies in modeling input data b uncertainties or inaccuracies in modeled outputs used for calibration c uncertainties or inaccuracies in modeling parameters and d uncertainties caused by imperfect model structures refsgaard and storm 1990 shrestha and solomatine 2008 a major contributor to errors caused by any of these types of uncertainties is the difficulty in accurately measuring the spatial and temporal variability in hydrologic modeling inputs across large modeling domains e g large watershed or groundwater aquifers shrestha and solomatine 2008 numerous methods have been used to analyze uncertainties in hydrological models these methods include analytical and approximation methods rosenblueth 1975 harr 1989 melching 1992 tung 2011 maskey et al 2004 montanari 2007 monte carlo based methods kuczera and parent 1998 hong et al 2006 vrugt et al 2008 bayesian methods and generalized likelihood uncertainty estimation glue methods krzysztofowicz 1999 jin et al 2010 li et al 2010 and fuzzy theory based methods maskey and guinot 2003 huang et al 2010 however the majority of these methods deal only with a single source of uncertainty and most methods assume that the model structure is accurate and that input data is free from uncertainty these limitations make it difficult to consider the inherent uncertainties of input data and numerical approaches for limiting the effects of input errors on model outputs according to abebe and price 2003 uncertainties associated with the quality of upstream input data affect the calibration of the model parameters and input errors translate through the model to its outcome as long as there are unaccounted uncertainties in input data it is difficult to expect accurate rainfall runoff simulations through hydrological models for instance in a short term rainfall runoff model which may predict the amount and pattern of runoff for a flood warning system accurate runoff forecasting is essential since the flood warning level is determined according to the forecasted results from the hydrological model it is crucial to maintain high model accuracy moreover it is necessary to manage the errors in real time because the short term hydrological models provide runoff predictions every hour ideally it is possible to minimize the uncertainty of the predicted runoff by predicting the errors that can be generated from various sources in advance many studies have examined how uncertainty from upstream sources affect the errors in model outcomes kobold and sušelj 2005 haydon and deletic 2009 arnaud et al 2011 mcmillan et al 2011 for example muñoz et al 2014 analyzed how the uncertainty in precipitation affects the runoff errors simulated by a hydrological model by using monte carlo simulations and they found that the uncertainty in the precipitation estimation processes had a significant effect on the modeling output errors obviously uncertainty in the precipitation data during a rainy period had a larger impact on output errors compared to periods with less precipitation another study led by datta and bolisetti 2016 used the precipitation multiplier method to study the impacts of precipitation uncertainty on model error and the study concluded that differences between estimated precipitation and true precipitation have significant effects on the errors in model prediction specifically the errors in model prediction resulted in underestimated discharges for high flow events and overestimated discharges for low flow events thus it is essential to consider the impact of uncertainty in upstream data sources when minimizing the output errors of hydrologic models recently deep learning models have been applied for various purposes in the field of hydrology deep learning models use large datasets to analyze and predict non linear relationships between data components such as rainfall runoff relationships such models have significantly contributed to the advancement of hydrological analyses since they provide high quality and cost effect modeling results mosavi et al 2018 in addition the deep learning models have shown predictive powers with fewer parameters compared to the physical based models solomatine and ostfeld 2008 castelletti et al 2010 deep learning models have been widely used in various forecasting applications including predictions of precipitation lin et al 2013 agrawal et al 2019 sønderby et al 2020 runoff yilmaz and muttil 2014 hu et al 2018 fan et al 2020 xiang et al 2020 soil properties feng et al 2019 groundwater levels sahoo et al 2017 and river stages choi et al 2020 in addition deep learning models can be used in combination with two or more models kim et al 2019 and can be used to improve the performance of physical based models as they are used complementarily abebe and price 2003 not only are deep learning models effective in predicting hydrologic variables but they can also be used to analyze and predict how errors in model outputs are affected by various sources of uncertainty several studies have analyzed the uncertainties of hydrological models using deep learning model to improve the predictive performance of the models abebe and price 2003 presented a complementary framework to manage uncertainty in hydrological models using an artificial neural network ann model their study focused on the relationship between input data e g precipitation and error in output e g runoff of the model and they concluded that the usage of ann model for error predictions in hydrological models can significantly improve model performance moreover wu et al 2018 successfully applied a random forest model to predict the errors based on relationships between input variables e g precipitation temperature and output e g runoff error and similarly shrestha and solomatine 2009 applied various machine learning models to estimate the prediction intervals of outputs and concluded that machine learning approaches are effective methods to estimate model uncertainties in addition deep learning models have recently been used as a post processor for minimizing runoff errors and improving the performance of hydrological models frame et al 2020 nearing et al 2020 previous studies only considered error sources related to uncertainties within the model such as choice of input variables parameters and model structure so it is difficult to account for the influences of upstream data uncertainty on errors in the modeling output by building upon previous data driven modeling studies this study developed a post processor framework using deep learning approach and evaluated the framework to reduce hydrologic modeling errors by explicitly considering uncertainties in upstream runoff data the long short term memory approach which is a deep learning model was combined with a sequence to sequence learning structure hereafter referred to as lstm s2s to predict the errors in the forecasted runoff from a hydrological model recently xiang et al 2020 evaluated lstm s2s model to predict the runoff and suggested the lstm s2s model for hydrological predictions also frame et al 2020 applied long short term memory approach lstm to improve the performance of national water model nwm for runoff simulation by considering nwms output as input variables of lstm model however these studies focused on only runoff simulation by using lstm based deep learning approach instead of the uncertainty of modeling results the novelty of this study is to hybrid use of deep learning approach and physical based model for predicting the model errors in outcomes instead of the runoff itself the specific objectives of this were to i analyze errors in forecasted runoff from the physical based hydrologic model ii develop an error correction framework using a deep learning model as a post processor framework and iii investigate how the developed post processing framework improves the predictive performance of hydrological model 2 materials and methods 2 1 study area this study used the russian river basin located in california usa as a study area the russian river basin is located on the west coast of the united states fig 1 and receives an average annual precipitation of 925 mm with more than 80 of the annual precipitation occurring between november and march intense precipitation is often generated in this region from extratropical cyclones or jet streams from the pacific ocean which have historically caused flood damages during the wet season ralph et al 2006 johnson et al 2016 han et al 2019 the basin has a drainage area of approximately 3 850 km2 with elevations ranging from 50 m to nearly 800 m two reservoirs have been constructed in the basin mendocino and sonoma reservoirs which are regulated by the coyote valley and warm springs dams for irrigation and flood control because the russian river is one of the most flood prone rivers in california due to its unique combination of geography climatologically heavy precipitation the basin is ideally suited to evaluate and reduce errors associated with runoff modeling and flood forecasting fig 1 shows the precipitation stations three united states geological survey usgs streamflow gages and the stream channel network within the basin 2 2 national water model the nwm was developed to simulate and forecast hydrological processes including streamflow runoff throughout the continental united states conus the nwm uses observed datasets from more than 8 000 usgs gages to forecast various hydrological variables including soil moisture surface runoff snow water equivalent and other variables for 2 7 million locations in the conus the core system of the nwm contains the national center for atmospheric research ncar supported community weather research and forecasting hydrologic model wrf hydro https water noaa gov about nwm the model provides forecasted outputs for three geospatial features including river channels land types and reservoirs the types of channels and reservoirs are based on the u s nhdplus dataset and the land types are based on a 1 km and 250 m grid system covering the conus for example the nwm provides land surface variables such as evapotranspiration snow water equivalent and snow depth at a 1 km grid resolution and ponded water depth and soil saturation outputs at a 250 m grid resolution han et al 2019 the nwm has three forecasting configurations including short range medium range and long range forecasting table 1 shows the features of each in this study hourly streamflow forecasted from short range configuration from 2019 to 2020 were used to estimate errors from the nwm and compare them with the results from the deep learning model i e lstm s2s the short range configuration produces hourly deterministic forecasts of streamflow and hydrologic states with a lead time of 1 18 h in 1 hour increments 2 3 data descriptions datasets for this study include forecasted runoff data with lead times of 1 to 18 h in 1 hour increments from the nwm and observed runoff at three usgs stations located in the russian river basin usgs 11 463 000 at the russian river basin near cloverdale cloverdale station usgs 11 464 000 at the russian river near healdsburg healdsburg station and usgs 11 467 000 at the russian river near guerneville guerneville station the guerneville station is located at the outlet point of the watershed additionally hourly precipitation data from noaa physical sciences laboratory https www psl noaa gov data at the potter valley central pvc hopland hld rio nido rod and healdsburg hbg stations were used as input data to the lstm s2s model datasets from 2019 were used for training the lstm s2s model and datasets from 2020 were used to test of the model performance table 2 describes the various datasets used in this study 2 4 long short term memory with sequence to sequence learning the lstm model introduced by hochreiter and schmidhuber 1997 is a recurrent neural network rnn deep learning model the lstm model is an effective method for solving gradient vanishing problems generated in rnn models and for handling long term time dependent data since the training process of lstm model is based on numerous parameters the training process may be longer than other data drive models but it has the advantage of providing results with high accuracy hu et al 2018 the lstm model has been applied for linguistic translation speech recognition and time series predictions and recently also to the prediction of several hydrological variables soltau et al 2016 hu et al 2018 kratzert et al 2018 fan et al 2020 xiang et al 2020 fig 2 shows the conceptual diagram of lstm model the lstm model consists of many hidden layers called memory cells and each memory cell maintains the state of information at time t fig 2 each memory cell also includes three nonlinear gates the forget gate ft input gate it and output gate ot that control the flow of information between cells the lstm operates the algorithms from an input sequence data xt to the final outcome o by looping through equations 1 6 with parameter values of c0 0 and h0 0 indicating initial cell state and hidden state generated from first cell adeyemi et al 2018 the equations used to control information flow through the lstm gages are 1 f t σ w f h t 1 x t b f 2 i t σ w i h t 1 x t b i 3 o t σ w o h t 1 x t b o 4 c t t a n h w c h t 1 x t b c 5 c t f t c t 1 i t c t 6 h t o t t a n h c t where σ is the non linear activation function wf wi wo and wc are weight values for the forget gates input gates output gates and memory cells ht 1 denotes output data from a previous cell xt is input data for the current time step and bf bi and bo are bias vectors of each gate respectively in addition c t is the state of any cell generated from the activation function in this study relu was used as the activation function the purpose of the forget gate equation 1 is to determine how much information from the previous block is maintained and the gate determines this value by applying output from the previous cells and current input data to the activation function the input gate equation 2 is able to determine which of the new information to store in a cell and the output gate equation 3 is used to determine the final output value from the information stored in the cell however a limitation of lstm models is that equal time steps for the model inputs and outcomes are required xiang et al 2020 combining the lstm with a sequence to sequence learning model lsmt s2s or encoder decoder model was developed to solve this limitation using a structure based on different time steps of input and output variables cho et al 2014 the main difference between the both of lstm and lstm s2s is the number of time steps considered for the input and output variables therefore there is an explicit benefit that data based on different time sequence can be analyzed more efficiently through an lstm s2s model generally lstm s2s models are applied in such fields as text translation speech recognition and image analysis which require different time sequence xiang et al 2020 similarly lstm s2s models can be used in hydrological time series predictions such as short term runoff and flash flood forecasting kao et al 2020 xiang et al 2020 fig 3 shows the conceptual diagram of a lstm s2s model the lstm s2s model process is similar to the traditional lstm model but it consists of three structures known as the encoder encoded vector and decoder stages an encoder stage uses the information from given datasets as input variables and output from the encoder stage with m time steps can be stored in a encoded vector the encoded vector stage is the final hidden state given from the encoder stage it aims to help the decoder predict output values accurately by condensing the information produced from the encoder part also it can be the initial hidden state of the decoder stage of the model in the decoder stage it is then used to translate data stored in the encoded vector into predictions of output variables with n time steps one of the beneficial features of lstm s2s models is that the time steps of encoder and decoder lstm can be different in this study x1 xm represents the historical errors in runoff from the physical based hydrological model and observed precipitation at upstream stations and o1 on denotes the predicted errors 2 5 model design in this study a lstm s2s model was used to improve runoff forecasting with lead times between 1 and 18 h using runoff errors from upstream and observed precipitation as input data fig 4 illustrates the model design which includes the following five components 1 collecting forecasted and observed runoff time series data 2 calculating errors between forecasted and observed datasets 3 training the lstm s2s model using the calculated errors and observed data 4 predicting future model errors using the lstm s2s model 5 improving the forecasted runoff and evaluating the forecast performance errors were estimated using equation 7 for each time step between 1 and 18 h 8 error runoff obs runoff sim runoff obs 100 where runoffobs and runoffsim denote the observed and forecasted runoff from the nwm estimated errors in each time step are used as input variables of the lstm s2s model for training and error predictions the overall model can be formulated as follows 8 error t m f error upstream t 1 t 2 t n precipitation t 1 t 2 t n in this model algorithm time series input including errors in forecasted runoff at three stations two upstream stations and the outlet point and observed precipitation at four ground based gauges were used to predict the error for each time steps four metrics were used for evaluating the performance of the model design coefficient of correlation cc root mean square error rmse percent bias pbias and nash sutcliffe efficiency nse cc is calculated as 9 cc y e y e y o y o y e y e y o y o where ye and yo indicate the simulated and observed runoff y e and y o denote the average simulated and observed runoff values of cc range from 1 to 1 and describe a measure of how well the outputs are simulated by the model a cc value of 0 indicates that there is no correlation between observed and simulated runoff rmse and pbias are calculated as 10 rmse y o y e 2 m 11 pbias y o y e y o 100 values of rmse ranges from 0 to and describes how well the simulated value matches to the observed value the zero value of rmse means the modelled and observed values are the same the pbias denotes the ratio of difference between sums of simulated and observed values to the sum of observed values nse is calculated as 12 nse 1 y e y o 2 y o y o 2 values of nse represents the predictive power of the model and range from to 1 nse values near 1 indicate excellent model performance 3 results 3 1 national water model performance for runoff forecasting it was necessary to first evaluate the runoff predictions from the nwm before testing prediction improvements using the lstm s2s model a total of 11 647 values of predicted runoff from the nwm were compared with observed runoff data collected at the usgs stations evaluation metrics of cc nse pbias and rsme were used to compare runoff predicted by the nwm and observed runoff at each of the three usgs gages and the cumulative distributions of each metric were plotted to assess the nwm predictive performance fig 5 as shown in the fig 5 it was found that the performance of the nwm according to the evaluation metrics was similar at each usgs station though runoff predictions at the healdsburg station were the most accurate based on nse and pbias distributions overall according to pbias values the nwm runoff predictions at each usgs station tended to overestimate actual runoff indicating that improvements to runoff predictions are necessary the forecasting performance of the nwm for each 1 hour time step between 1 and 18 h was also assessed using the evaluation metrics as shown in fig 6 the predictive performance of the nwm at the guerneville station tends to decrease with growing lead times until the performance stabilizes for lead times greater than approximately 12 h cc 0 66 nse 0 55 pbias 10 and rmse 40 cms these results show the necessity of improving the predictive performance of nwm for effective flood forecasting especially at longer lead times 3 2 error prediction results 3 2 1 model training the lstm s2s model was trained by evaluating its performance after adjusting the parameters for input data lag times number of lstm neuron cells in each layer and batch sizes in each layer fan et al 2020 xiang et al 2020 during model training lag times number of neurons and batch sizes were adjusted to achieve optimal model performance previous studies have also focused on adjusting these parameters during the lstm model training period fan et al 2020 xiang et al 2020 in our model observed hourly precipitation at four stations pvc hld rod and hbg and residuals in runoff at two upstream usgs stations cloverdale and healdsburg were used as input data to evaluate the sensitivity of the model to variations in lag time of input data lag times between 6 and 24 h were tested from the upstream precipitation and usgs stations and model performance was evaluated at the most downstream usgs station guerneville as shown in fig 7 a the results indicate that the model performs best with lag times of 6 h nse 0 95 cc 0 97 pbias 47 and models with lags between 6 and 12 h perform similarly based on cc and nse values but have higher pbias values as lag times increase models with lag times between 12 and 18 h have relatively low performance likely because the time to concentration between the upstream stations and basin outlet varies between 10 and 18 h according to time differences in time to runoff peaks between each station this indicates that other hydrologic processes including soil moisture and evapotranspiration are also important for predicting runoff with longer time steps xiang et al 2020 though not considered in this study the sensitivity of model performance to the number of neurons and batch sizes is shown in fig 7 b and c these two parameters were adjusted from 32 to 256 and tested at the guerneville station the model with 32 number neurons and a batch size of 128 performs best cc 0 96 0 75 nse 0 91 0 71 pbias 59 4 63 9 rmse 1 4 cms 2 4 cms for 32 number neurons 128 batch size the model with a batch size of 32 also showed acceptable performance with high values of cc and nse but it has larger pbias values compared to other models with different batch sizes based on these model training results this study used a lag time of 6 h number of neurons of 32 and batch size of 128 as hyperparameters of the lstm s2s model 3 2 2 error prediction using lstm s2s model the trained lstm s2s model was used to predict runoff errors at the guerneville station with forecasting lead times of 1 to 18 h in 1 hour increments results for each lead time fig 8 indicate a strong model performance with average r2 0 97 distributions of error results range from 100 to 100 for 12 18 h lead times and 50 to 100 for 1 6 h lead times and a high density red points in fig 8 of error predictions between 20 to 0 in addition as a lead time increased the distributions of errors tended to increase the statistical results for the overall predictive performance of the lstm s2s model for lead times between 1 and 18 h are listed in table 3 as shown in table 3 the lstm s2s model provided high performance for error prediction with average cc of 0 95 nse of 0 86 rmse of 5 31 and pbias of 2 11 it was found that the pbias values were positive indicating predicted errors were overestimated compared to the observations also nse values were 0 72 or greater indicating acceptable model results xiang et al 2020 moriasi et al 2007 thus from the statistical results the lstm s2s model showed acceptable error prediction compared to the observed error confirming its applicability for improving model performance as a post processor 3 3 improved runoff forecasting performance predicted errors for each lead time from the lstm s2s model described in section 3 2 were used to improve the accuracy of forecasted runoff measured at the guerneville station it was found that the lstm s2s model based post processor improved predictive performance of the nwm model fig 9 shows results comparing the performance of the nwm with and without incorporating the error predictions from the lstm s2s model distributions of observed grey markers and corrected runoff blue markers were predominantly between 0 and 50 cms the agreement between observed and predicted runoff from the nwm without lstm s2s error predictions ranged between r2 0 75 to 0 95 whereas the agreement with lstm s2s error predictions ranged between r2 0 98 to 0 99 improvements in runoff predictions were shown in all lead times between 1 and 18 h fig 9 results also show that the lstm s2s based post processor improved the predictive performance of the nwm according to the metrics of cc pbias and rmse as shown in fig 10 evaluation metrics for runoff predictions of the nwm with without the lstm s2s were cc 0 1 0 5 0 5 pbias 15 10 60 80 and rmse 0 cms 20 cms 0 cms 120 cms to highlight the effects of using the lstm s2s model to improve runoff forecasting fig 11 shows two observed runoff hydrographs and the forecasting results of the nwm with and without the addition of the lstm s2s model for lead times between 1 and 18 h deviations between the runoff predictions from nwm and observations were large for all lead times whereas the nwm with the lstm s2s model produced results that followed the same patterns of observed runoff in addition it was found that the fluctuation and deviation of the improved runoff were significantly lower during low runoff periods than periods of high runoff as shown in fig 11 a and b the error in the predicted runoff compared to the observations was also larger in the nwm compared to the nwm with the lstm s2s model for example in fig 11 a the range of total errors for 1 18 h in runoff predicted from the nwm model ranged from 25 to 50 whereas the range of errors in predicted runoff from the nwm with the lstm s2s model ranged from 3 to 3 for period of 2020 01 16 to 2020 01 20 4 discussions this study demonstrates improvements in hydrologic runoff forecasting when a deep learning model is used to predict errors in runoff data at various lead times specifically in the russian river basin it was shown that a lstm s2s post processor model has the potential to improve the predictive performance of the nwm at a guerneville usgs station located near the outlet of the basin although this study used a relatively small basin as a case study to demonstrate the application of the lstm s2s model it is expected that the deep learning model can be applied in other basins to improve the forecasting performance of the nwm for instance frame et al 2020 showed that lstm based post processor without the s2s decoder can be applied to improve the nwm simulation performance for daily runoff at 531 basins across the continental united states they focused on predictions of daily runoff using nwm output data as input for a lstm model in addition they directly predicted the runoff which is different from the method of our study which predicts the errors in forecasted runoff in advance however both studies have a same purpose of improving nwm runoff prediction performance using an lstm based post processor both studies show that an lstm post processor can be used to significantly improve the predictive ability of the nwm for daily and hourly runoff forecasting however the major difference between previous work and this study is that we present a method to improve the performance of hydrologic models such as the nwm by predicting and adjusting for errors in runoff through a post processor instead of directly predicting runoff as performed in other previous studies e g xiang et al 2020 and frame et al 2020 in this study only observed precipitation and errors generated from upstream stations were considered as input variables but further work should consider other variables that influence hydrologic routing such as diversion infrastructure reservoirs and other management infrastructure since these variables can impact modeling errors in nwm for example frame et al 2020 used other nwm variables such as air temperature radiation vapor pressure potential evapotranspiration and snow fraction as inputs of their deep learning model to improve the representation of streamflow patterns they showed the deep learning model can improve prediction accuracy of nwm although they used output states of the nwm as input data for their lstm model further work can consider actual errors in input variables and their effects on forecasted runoff the proposed post processing model contains only precipitation and errors in runoff as input data to apply the lstm s2s model to other regions with various meteorological characteristics a new input selection process may be required to consider other hydrologic variables such as snow accumulation which is not observed for the russian river basin thus it is expected that several variables including snow accumulation data soil moisture data and evapotranspiration data can be considered to improve the predictive accuracy of the nwm model in other regions although soil and topography data such as soil properties land use and slope were not considered in this study they can be used to make efficient prediction results in other regions in addition this study considered only short term prediction period between 1 and 18 h provided from nwm the proposed model has the potential to be applied to medium to 10 days and long term to 30 days applications the error correction framework presented in this study can be applied to improve the performance of any hydrologic model and is not limited to the nwm since the framework works as a post processor it can predict new errors based on historic uncertainties that occur in a specific hydrological model in addition as this study has shown the various factors related to the uncertainties in the outcomes can be used together depending on the characteristics of the local observation systems although only hourly based datasets such as precipitation and runoff were used in this study the error correction framework can consider multiple time steps ranging from sub hourly to monthly for forecasting errors and improving models performance 5 conclusions in this study we demonstrated how a lstm s2s model which is a deep learning model can be used to improve the predictive performance of the nwm the proposed method was applied in the russian river basin california and the results indicated the following 1 the performance of the nwm for forecasting runoff from 1 to 18 h was evaluated and the prediction accuracy of the nwm decreased as lead times increased it was shown that average metrics of cc nse pbais and rmse were 0 65 0 55 10 and 40 cms respectively for lead times between 12 and 18 h in addition the overall predictive performances of the nwm at three usgs stations were found to be lower compared to the actual runoff which means that a post processor could improve the prediction performance of the nwm 2 three factors including previous time step of input data number of neurons and batch size effect on the performance of the lstm s2s model the result indicates that the performance of the lstm s2s decreased as sequence length and number of neurons increased in addition the change in batch size did not have a dramatic effect on simulation accuracy the optimized model was based on 6 h for sequence length 32 for number of neurons and 128 for batch size respectively 3 the lstm s2s model obtained desirable results for error forecasting for each time step between 1 and 18 h with average values of cc of 0 95 nse of 0 88 and pbias of 14 respectively the results of prediction errors from lstm s2s model were used for improving the predictive performance of the nwm 4 it was found that using the lstm s2s as a post processor significantly improved the predictive performance of the nwm for lead times of 1 to 18 h compared to the predicted runoff from the nwm the nwm with the lstm s2s post processor improved runoff predictions with r2 values of 0 98 to 0 99 moreover it was found that lstm s2s based post processor improved the predictive performance of nwm in terms of not only temporal pattern of runoff but also the volume of predicted runoff error correction through a post processing framework is essential since many hydrologic models such as the nwm contain inherent uncertainties even after calibration therefore we expect that the proposed post process framework in this study can contribute to improving hydrological modeling performance by minimizing the uncertainty of various hydrological models besides the nwm in addition the suggested framework includes only upstream information such as observed precipitation and errors as input data thus it may be easy to apply the post processor framework to improve the model s predictive ability for numerous regions it is expected that the proposed framework can be applied for different basins and over 2 7 million river reaches around the united states for which the hydrological models i e nwm provides runoff predictions this study also serves to highlight the power of deep learning models for solving hydrological problems credit authorship contribution statement heechan han conceptualization methodology investigation visualization writing original draft ryan r morrison supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
3497,accurate runoff prediction is critical for various fields of hydrology agriculture and environmental studies numerous hydrologic models have been developed and demonstrate good performances in runoff simulation however errors are inherent in forecasted runoff predictions which can cause uncertainty in real time flood warning systems in order to improve the predictive performance of hydrologic modeling this study used a deep learning approach as a post processor to correct for errors associated with hydrologic data the proposed model uses the long short term memory model with sequence to sequence structure as a post processor to improve runoff forecasting specifically the deep learning approach was used to estimate errors in forecasted hourly runoff provided from national water model in russian river basin california united states error prediction in hourly runoff with lead times between 1 and 18 h were developed using observed precipitation and errors from upstream stream gages to improve the predictive performance of national water model the predictive performance of the model was evaluated using numerous statistical metrics and results show that the long short term memory model with sequence to sequence post processor improved runoff predictions compared to standalone results from the national water model statistical values of percent bias decreased from a range of 60 80 to 15 10 when the post processor model was used and similarly root mean square errors of runoff prediction decreased from 120 cms to 20 cms thus this study demonstrates the power of deep learning model to improve hydrologic modeling results especially those with short forecasting lead times keywords deep learning error prediction lstm s2s national water model 1 introduction accurately simulating relationships between rainfall and runoff is important for various water management purposes particularly flood forecasting shrestha and solomatine 2008 neitsch et al 2011 hu et al 2018 fan et al 2020 xiang et al 2020 physical based numerical hydrological models are often used to simulate nonlinear rainfall runoff relationships and these models are subsequently applied in various hydrologic applications including short term simulations for flood prediction and long term simulations for drought analysis and water resources management ott et al 1991 lee et al 2005 wu et al 2011 kang and sridhar 2017 wang et al 2017 these models generally contain multiple parameters used to calibrate hydrological processes and to minimize output residuals or errors in hydrological models errors inherent in the flow prediction results can lead to inaccurate hydrological analyses decision makers operating water related infrastructures for flood and drought forecasting irrigation control and other water management purposes require reliable hydrologic modeling results that consider uncertainties inherent in the results shrestha and solomatine 2008 proper error estimation and prediction can enhance the reliability and credibility of hydrologic modeling outputs for water management and improve our understanding of error propagations within modeling frameworks krzysztofowicz 2001 differences between modeled results and observed data e g differences between modeled streamflow estimates and measured discharge can be caused by four general sources within a hydrologic model a uncertainties or inaccuracies in modeling input data b uncertainties or inaccuracies in modeled outputs used for calibration c uncertainties or inaccuracies in modeling parameters and d uncertainties caused by imperfect model structures refsgaard and storm 1990 shrestha and solomatine 2008 a major contributor to errors caused by any of these types of uncertainties is the difficulty in accurately measuring the spatial and temporal variability in hydrologic modeling inputs across large modeling domains e g large watershed or groundwater aquifers shrestha and solomatine 2008 numerous methods have been used to analyze uncertainties in hydrological models these methods include analytical and approximation methods rosenblueth 1975 harr 1989 melching 1992 tung 2011 maskey et al 2004 montanari 2007 monte carlo based methods kuczera and parent 1998 hong et al 2006 vrugt et al 2008 bayesian methods and generalized likelihood uncertainty estimation glue methods krzysztofowicz 1999 jin et al 2010 li et al 2010 and fuzzy theory based methods maskey and guinot 2003 huang et al 2010 however the majority of these methods deal only with a single source of uncertainty and most methods assume that the model structure is accurate and that input data is free from uncertainty these limitations make it difficult to consider the inherent uncertainties of input data and numerical approaches for limiting the effects of input errors on model outputs according to abebe and price 2003 uncertainties associated with the quality of upstream input data affect the calibration of the model parameters and input errors translate through the model to its outcome as long as there are unaccounted uncertainties in input data it is difficult to expect accurate rainfall runoff simulations through hydrological models for instance in a short term rainfall runoff model which may predict the amount and pattern of runoff for a flood warning system accurate runoff forecasting is essential since the flood warning level is determined according to the forecasted results from the hydrological model it is crucial to maintain high model accuracy moreover it is necessary to manage the errors in real time because the short term hydrological models provide runoff predictions every hour ideally it is possible to minimize the uncertainty of the predicted runoff by predicting the errors that can be generated from various sources in advance many studies have examined how uncertainty from upstream sources affect the errors in model outcomes kobold and sušelj 2005 haydon and deletic 2009 arnaud et al 2011 mcmillan et al 2011 for example muñoz et al 2014 analyzed how the uncertainty in precipitation affects the runoff errors simulated by a hydrological model by using monte carlo simulations and they found that the uncertainty in the precipitation estimation processes had a significant effect on the modeling output errors obviously uncertainty in the precipitation data during a rainy period had a larger impact on output errors compared to periods with less precipitation another study led by datta and bolisetti 2016 used the precipitation multiplier method to study the impacts of precipitation uncertainty on model error and the study concluded that differences between estimated precipitation and true precipitation have significant effects on the errors in model prediction specifically the errors in model prediction resulted in underestimated discharges for high flow events and overestimated discharges for low flow events thus it is essential to consider the impact of uncertainty in upstream data sources when minimizing the output errors of hydrologic models recently deep learning models have been applied for various purposes in the field of hydrology deep learning models use large datasets to analyze and predict non linear relationships between data components such as rainfall runoff relationships such models have significantly contributed to the advancement of hydrological analyses since they provide high quality and cost effect modeling results mosavi et al 2018 in addition the deep learning models have shown predictive powers with fewer parameters compared to the physical based models solomatine and ostfeld 2008 castelletti et al 2010 deep learning models have been widely used in various forecasting applications including predictions of precipitation lin et al 2013 agrawal et al 2019 sønderby et al 2020 runoff yilmaz and muttil 2014 hu et al 2018 fan et al 2020 xiang et al 2020 soil properties feng et al 2019 groundwater levels sahoo et al 2017 and river stages choi et al 2020 in addition deep learning models can be used in combination with two or more models kim et al 2019 and can be used to improve the performance of physical based models as they are used complementarily abebe and price 2003 not only are deep learning models effective in predicting hydrologic variables but they can also be used to analyze and predict how errors in model outputs are affected by various sources of uncertainty several studies have analyzed the uncertainties of hydrological models using deep learning model to improve the predictive performance of the models abebe and price 2003 presented a complementary framework to manage uncertainty in hydrological models using an artificial neural network ann model their study focused on the relationship between input data e g precipitation and error in output e g runoff of the model and they concluded that the usage of ann model for error predictions in hydrological models can significantly improve model performance moreover wu et al 2018 successfully applied a random forest model to predict the errors based on relationships between input variables e g precipitation temperature and output e g runoff error and similarly shrestha and solomatine 2009 applied various machine learning models to estimate the prediction intervals of outputs and concluded that machine learning approaches are effective methods to estimate model uncertainties in addition deep learning models have recently been used as a post processor for minimizing runoff errors and improving the performance of hydrological models frame et al 2020 nearing et al 2020 previous studies only considered error sources related to uncertainties within the model such as choice of input variables parameters and model structure so it is difficult to account for the influences of upstream data uncertainty on errors in the modeling output by building upon previous data driven modeling studies this study developed a post processor framework using deep learning approach and evaluated the framework to reduce hydrologic modeling errors by explicitly considering uncertainties in upstream runoff data the long short term memory approach which is a deep learning model was combined with a sequence to sequence learning structure hereafter referred to as lstm s2s to predict the errors in the forecasted runoff from a hydrological model recently xiang et al 2020 evaluated lstm s2s model to predict the runoff and suggested the lstm s2s model for hydrological predictions also frame et al 2020 applied long short term memory approach lstm to improve the performance of national water model nwm for runoff simulation by considering nwms output as input variables of lstm model however these studies focused on only runoff simulation by using lstm based deep learning approach instead of the uncertainty of modeling results the novelty of this study is to hybrid use of deep learning approach and physical based model for predicting the model errors in outcomes instead of the runoff itself the specific objectives of this were to i analyze errors in forecasted runoff from the physical based hydrologic model ii develop an error correction framework using a deep learning model as a post processor framework and iii investigate how the developed post processing framework improves the predictive performance of hydrological model 2 materials and methods 2 1 study area this study used the russian river basin located in california usa as a study area the russian river basin is located on the west coast of the united states fig 1 and receives an average annual precipitation of 925 mm with more than 80 of the annual precipitation occurring between november and march intense precipitation is often generated in this region from extratropical cyclones or jet streams from the pacific ocean which have historically caused flood damages during the wet season ralph et al 2006 johnson et al 2016 han et al 2019 the basin has a drainage area of approximately 3 850 km2 with elevations ranging from 50 m to nearly 800 m two reservoirs have been constructed in the basin mendocino and sonoma reservoirs which are regulated by the coyote valley and warm springs dams for irrigation and flood control because the russian river is one of the most flood prone rivers in california due to its unique combination of geography climatologically heavy precipitation the basin is ideally suited to evaluate and reduce errors associated with runoff modeling and flood forecasting fig 1 shows the precipitation stations three united states geological survey usgs streamflow gages and the stream channel network within the basin 2 2 national water model the nwm was developed to simulate and forecast hydrological processes including streamflow runoff throughout the continental united states conus the nwm uses observed datasets from more than 8 000 usgs gages to forecast various hydrological variables including soil moisture surface runoff snow water equivalent and other variables for 2 7 million locations in the conus the core system of the nwm contains the national center for atmospheric research ncar supported community weather research and forecasting hydrologic model wrf hydro https water noaa gov about nwm the model provides forecasted outputs for three geospatial features including river channels land types and reservoirs the types of channels and reservoirs are based on the u s nhdplus dataset and the land types are based on a 1 km and 250 m grid system covering the conus for example the nwm provides land surface variables such as evapotranspiration snow water equivalent and snow depth at a 1 km grid resolution and ponded water depth and soil saturation outputs at a 250 m grid resolution han et al 2019 the nwm has three forecasting configurations including short range medium range and long range forecasting table 1 shows the features of each in this study hourly streamflow forecasted from short range configuration from 2019 to 2020 were used to estimate errors from the nwm and compare them with the results from the deep learning model i e lstm s2s the short range configuration produces hourly deterministic forecasts of streamflow and hydrologic states with a lead time of 1 18 h in 1 hour increments 2 3 data descriptions datasets for this study include forecasted runoff data with lead times of 1 to 18 h in 1 hour increments from the nwm and observed runoff at three usgs stations located in the russian river basin usgs 11 463 000 at the russian river basin near cloverdale cloverdale station usgs 11 464 000 at the russian river near healdsburg healdsburg station and usgs 11 467 000 at the russian river near guerneville guerneville station the guerneville station is located at the outlet point of the watershed additionally hourly precipitation data from noaa physical sciences laboratory https www psl noaa gov data at the potter valley central pvc hopland hld rio nido rod and healdsburg hbg stations were used as input data to the lstm s2s model datasets from 2019 were used for training the lstm s2s model and datasets from 2020 were used to test of the model performance table 2 describes the various datasets used in this study 2 4 long short term memory with sequence to sequence learning the lstm model introduced by hochreiter and schmidhuber 1997 is a recurrent neural network rnn deep learning model the lstm model is an effective method for solving gradient vanishing problems generated in rnn models and for handling long term time dependent data since the training process of lstm model is based on numerous parameters the training process may be longer than other data drive models but it has the advantage of providing results with high accuracy hu et al 2018 the lstm model has been applied for linguistic translation speech recognition and time series predictions and recently also to the prediction of several hydrological variables soltau et al 2016 hu et al 2018 kratzert et al 2018 fan et al 2020 xiang et al 2020 fig 2 shows the conceptual diagram of lstm model the lstm model consists of many hidden layers called memory cells and each memory cell maintains the state of information at time t fig 2 each memory cell also includes three nonlinear gates the forget gate ft input gate it and output gate ot that control the flow of information between cells the lstm operates the algorithms from an input sequence data xt to the final outcome o by looping through equations 1 6 with parameter values of c0 0 and h0 0 indicating initial cell state and hidden state generated from first cell adeyemi et al 2018 the equations used to control information flow through the lstm gages are 1 f t σ w f h t 1 x t b f 2 i t σ w i h t 1 x t b i 3 o t σ w o h t 1 x t b o 4 c t t a n h w c h t 1 x t b c 5 c t f t c t 1 i t c t 6 h t o t t a n h c t where σ is the non linear activation function wf wi wo and wc are weight values for the forget gates input gates output gates and memory cells ht 1 denotes output data from a previous cell xt is input data for the current time step and bf bi and bo are bias vectors of each gate respectively in addition c t is the state of any cell generated from the activation function in this study relu was used as the activation function the purpose of the forget gate equation 1 is to determine how much information from the previous block is maintained and the gate determines this value by applying output from the previous cells and current input data to the activation function the input gate equation 2 is able to determine which of the new information to store in a cell and the output gate equation 3 is used to determine the final output value from the information stored in the cell however a limitation of lstm models is that equal time steps for the model inputs and outcomes are required xiang et al 2020 combining the lstm with a sequence to sequence learning model lsmt s2s or encoder decoder model was developed to solve this limitation using a structure based on different time steps of input and output variables cho et al 2014 the main difference between the both of lstm and lstm s2s is the number of time steps considered for the input and output variables therefore there is an explicit benefit that data based on different time sequence can be analyzed more efficiently through an lstm s2s model generally lstm s2s models are applied in such fields as text translation speech recognition and image analysis which require different time sequence xiang et al 2020 similarly lstm s2s models can be used in hydrological time series predictions such as short term runoff and flash flood forecasting kao et al 2020 xiang et al 2020 fig 3 shows the conceptual diagram of a lstm s2s model the lstm s2s model process is similar to the traditional lstm model but it consists of three structures known as the encoder encoded vector and decoder stages an encoder stage uses the information from given datasets as input variables and output from the encoder stage with m time steps can be stored in a encoded vector the encoded vector stage is the final hidden state given from the encoder stage it aims to help the decoder predict output values accurately by condensing the information produced from the encoder part also it can be the initial hidden state of the decoder stage of the model in the decoder stage it is then used to translate data stored in the encoded vector into predictions of output variables with n time steps one of the beneficial features of lstm s2s models is that the time steps of encoder and decoder lstm can be different in this study x1 xm represents the historical errors in runoff from the physical based hydrological model and observed precipitation at upstream stations and o1 on denotes the predicted errors 2 5 model design in this study a lstm s2s model was used to improve runoff forecasting with lead times between 1 and 18 h using runoff errors from upstream and observed precipitation as input data fig 4 illustrates the model design which includes the following five components 1 collecting forecasted and observed runoff time series data 2 calculating errors between forecasted and observed datasets 3 training the lstm s2s model using the calculated errors and observed data 4 predicting future model errors using the lstm s2s model 5 improving the forecasted runoff and evaluating the forecast performance errors were estimated using equation 7 for each time step between 1 and 18 h 8 error runoff obs runoff sim runoff obs 100 where runoffobs and runoffsim denote the observed and forecasted runoff from the nwm estimated errors in each time step are used as input variables of the lstm s2s model for training and error predictions the overall model can be formulated as follows 8 error t m f error upstream t 1 t 2 t n precipitation t 1 t 2 t n in this model algorithm time series input including errors in forecasted runoff at three stations two upstream stations and the outlet point and observed precipitation at four ground based gauges were used to predict the error for each time steps four metrics were used for evaluating the performance of the model design coefficient of correlation cc root mean square error rmse percent bias pbias and nash sutcliffe efficiency nse cc is calculated as 9 cc y e y e y o y o y e y e y o y o where ye and yo indicate the simulated and observed runoff y e and y o denote the average simulated and observed runoff values of cc range from 1 to 1 and describe a measure of how well the outputs are simulated by the model a cc value of 0 indicates that there is no correlation between observed and simulated runoff rmse and pbias are calculated as 10 rmse y o y e 2 m 11 pbias y o y e y o 100 values of rmse ranges from 0 to and describes how well the simulated value matches to the observed value the zero value of rmse means the modelled and observed values are the same the pbias denotes the ratio of difference between sums of simulated and observed values to the sum of observed values nse is calculated as 12 nse 1 y e y o 2 y o y o 2 values of nse represents the predictive power of the model and range from to 1 nse values near 1 indicate excellent model performance 3 results 3 1 national water model performance for runoff forecasting it was necessary to first evaluate the runoff predictions from the nwm before testing prediction improvements using the lstm s2s model a total of 11 647 values of predicted runoff from the nwm were compared with observed runoff data collected at the usgs stations evaluation metrics of cc nse pbias and rsme were used to compare runoff predicted by the nwm and observed runoff at each of the three usgs gages and the cumulative distributions of each metric were plotted to assess the nwm predictive performance fig 5 as shown in the fig 5 it was found that the performance of the nwm according to the evaluation metrics was similar at each usgs station though runoff predictions at the healdsburg station were the most accurate based on nse and pbias distributions overall according to pbias values the nwm runoff predictions at each usgs station tended to overestimate actual runoff indicating that improvements to runoff predictions are necessary the forecasting performance of the nwm for each 1 hour time step between 1 and 18 h was also assessed using the evaluation metrics as shown in fig 6 the predictive performance of the nwm at the guerneville station tends to decrease with growing lead times until the performance stabilizes for lead times greater than approximately 12 h cc 0 66 nse 0 55 pbias 10 and rmse 40 cms these results show the necessity of improving the predictive performance of nwm for effective flood forecasting especially at longer lead times 3 2 error prediction results 3 2 1 model training the lstm s2s model was trained by evaluating its performance after adjusting the parameters for input data lag times number of lstm neuron cells in each layer and batch sizes in each layer fan et al 2020 xiang et al 2020 during model training lag times number of neurons and batch sizes were adjusted to achieve optimal model performance previous studies have also focused on adjusting these parameters during the lstm model training period fan et al 2020 xiang et al 2020 in our model observed hourly precipitation at four stations pvc hld rod and hbg and residuals in runoff at two upstream usgs stations cloverdale and healdsburg were used as input data to evaluate the sensitivity of the model to variations in lag time of input data lag times between 6 and 24 h were tested from the upstream precipitation and usgs stations and model performance was evaluated at the most downstream usgs station guerneville as shown in fig 7 a the results indicate that the model performs best with lag times of 6 h nse 0 95 cc 0 97 pbias 47 and models with lags between 6 and 12 h perform similarly based on cc and nse values but have higher pbias values as lag times increase models with lag times between 12 and 18 h have relatively low performance likely because the time to concentration between the upstream stations and basin outlet varies between 10 and 18 h according to time differences in time to runoff peaks between each station this indicates that other hydrologic processes including soil moisture and evapotranspiration are also important for predicting runoff with longer time steps xiang et al 2020 though not considered in this study the sensitivity of model performance to the number of neurons and batch sizes is shown in fig 7 b and c these two parameters were adjusted from 32 to 256 and tested at the guerneville station the model with 32 number neurons and a batch size of 128 performs best cc 0 96 0 75 nse 0 91 0 71 pbias 59 4 63 9 rmse 1 4 cms 2 4 cms for 32 number neurons 128 batch size the model with a batch size of 32 also showed acceptable performance with high values of cc and nse but it has larger pbias values compared to other models with different batch sizes based on these model training results this study used a lag time of 6 h number of neurons of 32 and batch size of 128 as hyperparameters of the lstm s2s model 3 2 2 error prediction using lstm s2s model the trained lstm s2s model was used to predict runoff errors at the guerneville station with forecasting lead times of 1 to 18 h in 1 hour increments results for each lead time fig 8 indicate a strong model performance with average r2 0 97 distributions of error results range from 100 to 100 for 12 18 h lead times and 50 to 100 for 1 6 h lead times and a high density red points in fig 8 of error predictions between 20 to 0 in addition as a lead time increased the distributions of errors tended to increase the statistical results for the overall predictive performance of the lstm s2s model for lead times between 1 and 18 h are listed in table 3 as shown in table 3 the lstm s2s model provided high performance for error prediction with average cc of 0 95 nse of 0 86 rmse of 5 31 and pbias of 2 11 it was found that the pbias values were positive indicating predicted errors were overestimated compared to the observations also nse values were 0 72 or greater indicating acceptable model results xiang et al 2020 moriasi et al 2007 thus from the statistical results the lstm s2s model showed acceptable error prediction compared to the observed error confirming its applicability for improving model performance as a post processor 3 3 improved runoff forecasting performance predicted errors for each lead time from the lstm s2s model described in section 3 2 were used to improve the accuracy of forecasted runoff measured at the guerneville station it was found that the lstm s2s model based post processor improved predictive performance of the nwm model fig 9 shows results comparing the performance of the nwm with and without incorporating the error predictions from the lstm s2s model distributions of observed grey markers and corrected runoff blue markers were predominantly between 0 and 50 cms the agreement between observed and predicted runoff from the nwm without lstm s2s error predictions ranged between r2 0 75 to 0 95 whereas the agreement with lstm s2s error predictions ranged between r2 0 98 to 0 99 improvements in runoff predictions were shown in all lead times between 1 and 18 h fig 9 results also show that the lstm s2s based post processor improved the predictive performance of the nwm according to the metrics of cc pbias and rmse as shown in fig 10 evaluation metrics for runoff predictions of the nwm with without the lstm s2s were cc 0 1 0 5 0 5 pbias 15 10 60 80 and rmse 0 cms 20 cms 0 cms 120 cms to highlight the effects of using the lstm s2s model to improve runoff forecasting fig 11 shows two observed runoff hydrographs and the forecasting results of the nwm with and without the addition of the lstm s2s model for lead times between 1 and 18 h deviations between the runoff predictions from nwm and observations were large for all lead times whereas the nwm with the lstm s2s model produced results that followed the same patterns of observed runoff in addition it was found that the fluctuation and deviation of the improved runoff were significantly lower during low runoff periods than periods of high runoff as shown in fig 11 a and b the error in the predicted runoff compared to the observations was also larger in the nwm compared to the nwm with the lstm s2s model for example in fig 11 a the range of total errors for 1 18 h in runoff predicted from the nwm model ranged from 25 to 50 whereas the range of errors in predicted runoff from the nwm with the lstm s2s model ranged from 3 to 3 for period of 2020 01 16 to 2020 01 20 4 discussions this study demonstrates improvements in hydrologic runoff forecasting when a deep learning model is used to predict errors in runoff data at various lead times specifically in the russian river basin it was shown that a lstm s2s post processor model has the potential to improve the predictive performance of the nwm at a guerneville usgs station located near the outlet of the basin although this study used a relatively small basin as a case study to demonstrate the application of the lstm s2s model it is expected that the deep learning model can be applied in other basins to improve the forecasting performance of the nwm for instance frame et al 2020 showed that lstm based post processor without the s2s decoder can be applied to improve the nwm simulation performance for daily runoff at 531 basins across the continental united states they focused on predictions of daily runoff using nwm output data as input for a lstm model in addition they directly predicted the runoff which is different from the method of our study which predicts the errors in forecasted runoff in advance however both studies have a same purpose of improving nwm runoff prediction performance using an lstm based post processor both studies show that an lstm post processor can be used to significantly improve the predictive ability of the nwm for daily and hourly runoff forecasting however the major difference between previous work and this study is that we present a method to improve the performance of hydrologic models such as the nwm by predicting and adjusting for errors in runoff through a post processor instead of directly predicting runoff as performed in other previous studies e g xiang et al 2020 and frame et al 2020 in this study only observed precipitation and errors generated from upstream stations were considered as input variables but further work should consider other variables that influence hydrologic routing such as diversion infrastructure reservoirs and other management infrastructure since these variables can impact modeling errors in nwm for example frame et al 2020 used other nwm variables such as air temperature radiation vapor pressure potential evapotranspiration and snow fraction as inputs of their deep learning model to improve the representation of streamflow patterns they showed the deep learning model can improve prediction accuracy of nwm although they used output states of the nwm as input data for their lstm model further work can consider actual errors in input variables and their effects on forecasted runoff the proposed post processing model contains only precipitation and errors in runoff as input data to apply the lstm s2s model to other regions with various meteorological characteristics a new input selection process may be required to consider other hydrologic variables such as snow accumulation which is not observed for the russian river basin thus it is expected that several variables including snow accumulation data soil moisture data and evapotranspiration data can be considered to improve the predictive accuracy of the nwm model in other regions although soil and topography data such as soil properties land use and slope were not considered in this study they can be used to make efficient prediction results in other regions in addition this study considered only short term prediction period between 1 and 18 h provided from nwm the proposed model has the potential to be applied to medium to 10 days and long term to 30 days applications the error correction framework presented in this study can be applied to improve the performance of any hydrologic model and is not limited to the nwm since the framework works as a post processor it can predict new errors based on historic uncertainties that occur in a specific hydrological model in addition as this study has shown the various factors related to the uncertainties in the outcomes can be used together depending on the characteristics of the local observation systems although only hourly based datasets such as precipitation and runoff were used in this study the error correction framework can consider multiple time steps ranging from sub hourly to monthly for forecasting errors and improving models performance 5 conclusions in this study we demonstrated how a lstm s2s model which is a deep learning model can be used to improve the predictive performance of the nwm the proposed method was applied in the russian river basin california and the results indicated the following 1 the performance of the nwm for forecasting runoff from 1 to 18 h was evaluated and the prediction accuracy of the nwm decreased as lead times increased it was shown that average metrics of cc nse pbais and rmse were 0 65 0 55 10 and 40 cms respectively for lead times between 12 and 18 h in addition the overall predictive performances of the nwm at three usgs stations were found to be lower compared to the actual runoff which means that a post processor could improve the prediction performance of the nwm 2 three factors including previous time step of input data number of neurons and batch size effect on the performance of the lstm s2s model the result indicates that the performance of the lstm s2s decreased as sequence length and number of neurons increased in addition the change in batch size did not have a dramatic effect on simulation accuracy the optimized model was based on 6 h for sequence length 32 for number of neurons and 128 for batch size respectively 3 the lstm s2s model obtained desirable results for error forecasting for each time step between 1 and 18 h with average values of cc of 0 95 nse of 0 88 and pbias of 14 respectively the results of prediction errors from lstm s2s model were used for improving the predictive performance of the nwm 4 it was found that using the lstm s2s as a post processor significantly improved the predictive performance of the nwm for lead times of 1 to 18 h compared to the predicted runoff from the nwm the nwm with the lstm s2s post processor improved runoff predictions with r2 values of 0 98 to 0 99 moreover it was found that lstm s2s based post processor improved the predictive performance of nwm in terms of not only temporal pattern of runoff but also the volume of predicted runoff error correction through a post processing framework is essential since many hydrologic models such as the nwm contain inherent uncertainties even after calibration therefore we expect that the proposed post process framework in this study can contribute to improving hydrological modeling performance by minimizing the uncertainty of various hydrological models besides the nwm in addition the suggested framework includes only upstream information such as observed precipitation and errors as input data thus it may be easy to apply the post processor framework to improve the model s predictive ability for numerous regions it is expected that the proposed framework can be applied for different basins and over 2 7 million river reaches around the united states for which the hydrological models i e nwm provides runoff predictions this study also serves to highlight the power of deep learning models for solving hydrological problems credit authorship contribution statement heechan han conceptualization methodology investigation visualization writing original draft ryan r morrison supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
3498,climate change and rapid expansion of urban areas are expected to increase pluvial flood hazard and risk in the near future and particularly so in large developed areas and cities therefore large scale and high resolution pluvial flood hazard mapping is required to identify hotspots where mitigation measures may be applied to reduce flood risk depressions or low points in urban areas where runoff volumes can be stored are prone to pluvial flooding the standard approach based on estimating synthetic design hyetographs assumes in a given depression that the t year design storm generates the t year pluvial flood in addition urban areas usually include several depressions even linked or nested that would require distinct design hyetographs instead of using a unique synthetic design storm in this paper a stochastic methodology is proposed to address the limitations of this standard approach developing large scale 2 m resolution pluvial flood hazard maps in urban areas with multiple depressions the authors present an application of the proposed approach to the city of pamplona in spain 68 26 km2 the safer rain fast processing algorithm based on digital elevation models dems is compared with the iber 2d hydrodynamic model in four real storms by using 10 min precipitation fields precipitation recorded at rainfall gauging stations was merged with continuous fields obtained from a meteorological radar station given the hydrostatic limitations of safer rain the benchmarking results are adequate in terms of water depths in depressions a long set of 10 000 synthetic storms that maintain the statistical properties of observations in pamplona is generated safer rain is used to simulate runoff response and filling and spilling processes in depressions for the 10 000 synthetic storms obtaining the probability distribution of water depths in each cell maps of pluvial flood hazards are developed in the pamplona metropolitan area for 10 return periods in the range from two to 500 years from such pixel based series of simulated water depths bivariate return period curves are estimated in a set of cells showing that several storms can generate a given t year pluvial flood with an increasing precipitation with storm duration that depends on the draining catchment soil characteristics the methodology proposed is useful to develop maps of pluvial flood hazards in large multi depression urban areas in reasonable computation times identifying the main pluvial flood hotspots keywords pluvial floods safer rain flood hazard mapping rapid flood model urban areas bivariate return periods 1 introduction pluvial floods are usually generated by high intensity and short duration storms climate change projections point to an increase in the intensity and frequency of such extreme rainfall events kundzewicz et al 2014 furthermore the expansion of urban areas and the increasing density of assets in cities have amplified the economic and human consequences associated with pluvial floods kaspersen et al 2017 bulti and abebe 2020 pluvial flood hazards are usually assessed by using synthetic design storms krvavica and rubinić 2020 and two dimensional 2d hydrodynamic models in this study a stochastic methodology is proposed to estimate pluvial flood hazards in urban areas overcoming some of the major limitations of deterministic approaches synthetic design storms neglect the influence of both hyetograph shapes and spatial and temporal distributions of rainfall on water depths in addition such an approach assumes that the design storm for a given return period t generates the t year pluvial flood however the t year pluvial flood should be obtained by using a threshold in a variable that characterises flood hazardousness such as water depths moreover a high degree of uncertainty is associated with design hyetograph estimates propagating it through rainfall runoff simulations and pluvial flood hazard calculations as catchment response times are usually estimated with empirical formulae that may not be valid in areas with different conditions from those of the regions used to obtain them as well as the results can differ by up to 500 depending on the formula used grimaldi et al 2012 gericke and smithers 2014 tuyls et al 2018 consequently urban drainage systems could be over or under estimated depending on the empirical formula used in each case in addition urban areas are usually composed of multiple depressions prone to pluvial flooding that can be even linked and nested each depression should be analysed independently when using an approach based on synthetic design storms as varying catchment response times will lead to varying design hyetograph durations therefore multiple simulations are required increasing the complexity of pluvial flood hazard mapping at the scale of a city or municipality the limitations of the standard approach based on synthetic design hyetographs can be improved by using a stochastic analysis that considers the runoff response in a large set of storms maintaining the statistical properties of real rainfall events observed in a given area first design hyetograph estimates are not required as the complete probability distribution of water depths at any point will be obtained addressing the assumption that the t year storm generates the t year pluvial flood second the method can be applied to larger scales with multiple depressions as it is independent of catchment response times at each depression third a stochastic analysis avoids the uncertainties associated with catchment response time estimates by using empirical formulae consequently a stochastic approach will be more adequate for pluvial flood hazard mapping in urban areas with multiple depressions however a rapid tool for simulating the water depths generated by a large set of storms is required for conducting such a stochastic approach 2d hydrodynamic models are recognised to simulate pluvial floods in urban areas accurately henonin et al 2013 based on solving the 2d shallow water equations swes by using numerical methods therefore they require high computation times furthermore studies that use long sets of stochastic synthetic rainfall events are unusual for the delineation of pluvial flood hazard maps apel et al 2016 as they are not affordable with the computation times required by 2d hydrodynamic models either simplified hydrodynamic models are used to reduce simulation times nuswantoro et al 2016 or a reduced set of simulations are considered such as 45 simulations simoes et al 2015 and 160 simulations per probability apel et al 2016 recently computation times of 2d hydrodynamic models have been reduced with parallel computing techniques and graphic processing units gpus guidolin et al 2016 such as the lisflood fp and p dwave models neal et al 2009 leandro et al 2014 with topography simplifications by using sub grid models yu and lane 2006 neelz and pender 2007 and focusing on cells with low porosity values that tend to reduce computational efficiencies guinot et al 2017 bruwier et al 2017 in addition the solving complexity of 2d wses has been reduced by approximating or neglecting inertial and advection terms bates et al 2010 and by using the cellular automata ca approach ghimire et al 2013 guidolin et al 2016 however such approaches would not be enough for conducting probabilistic analyses bernini and franchini 2013 furthermore artificial neural networks anns can be trained with a reduced set of 2d hydrodynamic model outputs bermúdez et al 2018 berkhan et al 2019 though they are unable to interpolate between the rainfall events used to train the network a set of hybrid models has been also developed chang et al 2010 pan et al 2011 and approaches based on support vector machines have been offered lin et al 2013 jhong et al 2017 bermúdez et al 2019 therefore a technique based on a 2d hydrodynamic model with short enough computation times and satisfactory results is not available yet rapid flood models rfms have been developed to shorten the computation times in contrast to 2d hydrodynamic models they identify depressions from a digital elevation model dem considering a set of storage reservoirs connected through links the water balance equation is used to simulate depression filling and spilling processes bulti and abebe 2020 therefore high resolution dems can be used with small pre processing computation times rfms can reduce computation times to few minutes or even seconds up to 1000 times compared with hydrodynamic models teng et al 2007 consequently rfms are adequate for conducting probabilistic analyses some rfms are the rapid flood spreading model rfsm bernini and franchini 2013 the flood connected domain calculation fcdc method zhang et al 2014 the rapid urban flood inundation and damage assessment model rufidam model jamali et al 2018 and the cellular automata fast flood evaluation ca ffé model jamali et al 2019 hierarchical filling and spilling algorithms hfsas or puddle to puddle dynamic filling and spilling approaches p2ps have been recognised as promising techniques for characterising pluvial floods chu et al 2013 zhang and pan 2014 in this respect the safer rain tool has been developed recently samela et al 2020 safer rain is a hfsa that can simulate detailed spatially distributed infiltration processes with the green ampt model supplying flood depths and extents probabilistic approaches are more correct than deterministic approaches based on t year design events as design event estimates have significant uncertainties and 2d models are not perfect di baldassarre et al 2010 probabilistic approaches based on continuous simulation needs a long period of rainfall time series nuswantoro et al 2014 semi continuous simulation approaches have been proposed to reduce simulation times lawrence et al 2014 jamali et al 2020 however stochastic approaches can generate larger samples of storms that can extend the available time series of observed rainfall events in this study a stochastic methodology is proposed to delineate probabilistic pluvial flood hazard maps across large urban areas and cities with 2 m horizontal resolution the fast processing dem based safer rain algorithm samela et al 2020 is used benefiting from its computational efficiency and low runtimes as well as from its ability to simulate spatially distributed infiltration processes therefore the limited number of simulations that can be considered by using a 2d hydrodynamic model simoes et al 2015 apel et al 2016 is overcome by using such a rfm in addition the safer rain algorithm does not need to be coupled to a rainfall runoff model the stochastic approach proposed can generate a long set of synthetic rainfall events with similar statistical properties to real rainfall events by using a methodology based on copulas extending the usual length of rainfall time series utilised in continuous and semi continuous approaches in addition the limitations of using a deterministic approach based on a single synthetic design storm krvavica and rubinić 2020 are improved by considering such a large set of storms the probability distribution of water depths is obtained in each cell of the domain furthermore flood depths and extents for given return periods are estimated from the water depth series obtained in each cell in addition bivariate return period curves are estimated analysing the relationship between the variables that characterise storms and pluvial flood water depths in a given depression the methodology has been applied to the pamplona metropolitan area in spain this study is structured as follows section 2 presents the case study and the models used in the analysis section 3 offers the methodology proposed for the delineation of pluvial flood hazard maps at municipality scales section 4 presents the application of the methodology to the case study of the pamplona metropolitan area section 5 discusses the results and limitations of the methodology proposed section 6 summarises the main conclusions of the study 2 case study and models first this section presents the pamplona metropolitan area case study as well as the data available second the models used are described the safer rain algorithm the 2d iber hydrodynamic model and the infiltration equation used to simulate rainfall losses 2 1 pamplona case study pamplona is located in the navarre region in the northern part of spain fig 1 the pamplona metropolitan area is 68 26 km2 and has around 335 000 inhabitants including the municipalities of barañáin burlada cendea de olza cizur huarte orcoyen pamplona villava and zizur mayor fig 2 it is subject to pluvial floods mainly in the summer months the municipalities of barañáin and zizur mayor have the highest pluvial flood losses according to the database provided by the spanish consorcio de compensación de seguros that compensates the damages produced by natural hazards in spain for the 20 july 2010 pluvial flood event that is the greatest in the pamplona metropolitan area in the period 1996 2020 such a database supplies a total flood loss of 3 319 m updated to the year 2020 70 7 of the flood losses were concentrated in zizur mayor and barañáin areas 1 687 m in zizur maryor and 0 66 m in barañáin and the closest postal code of pamplona to barañáin therefore such areas have been selected for the safer rain benchmark activities described in section 3 1 the 2 m dem considered in the study was supplied by the spanish national geographic institute instituto geográfico nacional ign in spanish precipitation data at four gauging stations were supplied by the real time automatic hydrological information system sistema automático de información hidrológica saih in spanish of the river ebro basin authority the spanish state meteorological agency agencia estatal de meteorología aemet in spanish the regional government of navarre and the universidad pública de navarra upna in spanish see table 1 three real pluvial flood events have been identified in the barañáin and zizur mayor municipalities in recent years table 2 first the day of occurrence of the main flood events in the pamplona metropolitan area were extracted from the database supplied by the spanish consorcio de compensación de seguros such events can be either fluvial or pluvial therefore the type of flood for each event was identified by checking streamflow and precipitation observations as well as pieces of news in the diario de navarra newspaper finally the three main pluvial flood events in the pamplona metropolitan area were selected the return period for each event in table 2 has been estimated from the intensity duration frequency idf curve for the storm duration by using the total storm precipitation depth the 20 july 2010 event had a rainfall of 42 6 mm and a peak intensity of 66 4 mm h in 15 min at the rain gauging station p1 with a return period estimate of 14 3 years on 18 september 2019 two storms were identified a 37 8 mm storm with a peak intensity of 74 4 mm h in 10 min at the p4 station in the morning which corresponds to a return period of 16 3 years and a small scale 25 9 mm storm with a peak intensity of 73 2 mm h in 10 min at the p4 station in the afternoon associated with a return period of 4 37 years the storm of 25 april 2020 was highly localised at the barañáin municipality and characterised by using the precipitation fields described in section 2 1 1 with a peak rainfall of 12 mm in 10 min in the area of zizur mayor and barañáin 2 1 1 quantitative precipitation estimation fields in the pamplona metropolitan area optimised and high resolution quantitative precipitation estimations qpes are required for identifying where localised heavy or torrential storms may take place coping with pluvial flood events properly zhang et al 2016 the combination of surface and remote sensing based radar data can generate a merged product that reduces the errors obtained by using only either rain gauging interpolation or radar data as radar data cover large areas and capture better the spatial variability of rainfall fields mckee and binns 2016 a variety of techniques is available for the merging process ochoa rodríguez et al 2019 though geostatistical methods provide the most satisfactory results such as the kriging with external drift ked technique sharon and gaussiat 2015 ked is an extension of kriging that uses external variables in this case radar qpes as auxiliary information in the interpolation process linear weights employed in the interpolation of point gauge values are further constrained by the spatial association between radar and rain gauge values in addition the real time semivariogram model is obtained with real time radar data describing the spatial correlation between data qpes with a 10 min time step are obtained in the metropolitan area of pamplona for the four storms included in table 2 fig 3 precipitation recorded at automatic weather stations awss from different organisms such as aemet the regional government of navarre and crowdsourced networks are merged with raw continuous fields obtained from the aemet zaragoza regional radar that supplies 10 min surface rainfall intensity sri based on the 0 5 elevation radar reflectivity by using the c band 5 6 ghz the number of rain gauging stations used in each event depends on the data availability crowdsourced real time rainfall data is acquired from the weather underground network that is fed by a global community of people connecting data from personal weather stations for example rainfall data was available for 10 crowdsourced stations in the 25 april 2020 event radar data quality is good enough despite the signal attenuation and the high beam elevation as zaragoza is located 150 km from pamplona furthermore raw radar data are post processed with ground clutter identification correction for vertical profile of reflectivity vpr and reflectivity to rain rate conversion by using the marshall palmer z r relationship marshall and palmer 1948 while radar fields are generated with a time step of 10 min rainfall gauging stations collects data with a time step of five to ten minutes therefore at site rainfall data are synchronised with radar scan time minimising the uncertainty from signal mismatches by using advection schemes tabary 2007 shapiro et al 2010 in addition the linear relationship between primary and auxiliary variables in ked may not remain valid for the whole range of rainfall intensities especially in the case of heavy precipitation events the expected value of the primary variable is the estimated precipitation ked adds information about the secondary or auxiliary fields that may contribute to a better prediction of the geospatial field of precipitation as they are related to the primary variable the radar based sri data are utilised as auxiliary fields in addition logarithmic transformations of the variables are tested to improve the results in the case of dominant non linear relationships between primary and secondary variables such as in heavy or torrential storms therefore qpe fields are selected among four options automatically via cross validation in real time i only sri based qpe ii ked with linear variables in both the primary and auxiliary variables iii ked with the logarithm of the primary variable and iv ked with logarithms in both variables 2 2 models the safer rain algorithm samela et al 2020 has been developed within the saferplaces project safer rain is implemented in the saferplaces platform https platform saferplaces co in order to assess pluvial fluvial and coastal flood hazard and risk in urban environments under current and future climates in this paper safer rain is used to estimate water depths and flood extensions in the stochastic methodology in addition the 2d hydrodynamic iber model bladé et al 2014 was used for the safer rain benchmarking with the four storms selected in table 2 both models consider the green ampt infiltration equation for simulating rainfall losses green and ampt 1911 2 2 1 safer rain algorithm safer rain is a fast processing hfsa that identifies pluvial flooded areas on the basis of nested surface depressions extracted from high resolution lidar dems and a given rainfall depth considering spatially distributed rainfall inputs and infiltration processes samela et al 2020 its main simplifying assumption consists of neglecting overland flow dynamics consequently net rainfall volumes accumulate in the system of nested depressions according to their capacity and hierarchical structure safer rain consists of two main steps in the first dem pre processing aims to identify the hierarchy tree of nested depressions that defines the sequence of depression filling and spilling as well as the filling volumes in each one in the second depression flooding identifies flooded areas and simulates corresponding water depths the flooding phase implemented in safer rain is original in three senses in the first a bottom up level set method is applied for quantifying partial filling in nested higher level depressions for more details see samela et al 2020 in the second either uniform or spatially variable i e gridded rainfall depths can be used as input rainfall volume and in the third infiltration losses can be simulated with a pixel based adaptation of the event based green ampt infiltration model green and ampt 1911 samela et al 2020 reported the first applications of safer rain to two case studies in northern italy where it was benchmarked with a 2d hydrodynamic model the results showed an agreement of 53 for the flooded area in lignano sabbiadoro and accuracy values in the range of 0 96 0 99 with the accuracy correlation coefficient acc in rimini these applications highlighted the fast computation of flooded areas guaranteed by safer rain as the pre processing phase can be run once for a given study area in addition the results revealed the effectiveness in identifying pluvial hazard hotspots with spatially distributed rainfall and different land use scenarios though the simulations are hydrostatic and safer rain cannot consider flow dynamic effects 2 2 2 2d iber hydrodynamic model flood extensions and water depths are obtained for the four storms shown in table 2 by using numerical simulations with the iber code bladé et al 2014 cea et al 2007 iber is free software that solves the 2d depth averaged swes by using a finite volume scheme with the domain being discretized with both structured and unstructured triangular or quadrilateral elements cueto felgueroso et al 2019 santillán et al 2020 it solves the mass conservative equation eq 1 and the momentum balance equations in conservative form with source terms eq 2 and eq 3 1 h t h v x x h v y y 0 2 h v x t x h v x 2 y h v x v y g h z b x τ b x ρ 3 h v y t x h v x 2 y h v y v x g h z b x τ b y ρ where h is the water depth vi is the depth average velocity along the ith direction ρ is the water density zb is the height of the river bed τ b i is the bed friction term along the ith direction and g is the gravitational acceleration 2 2 3 green ampt infiltration equation the green ampt infiltration equation is considered in both safer rain and iber models they consider a pixel based adaptation of the event based green ampt infiltration model green and ampt 1911 that assumes that water infiltrates into relatively dry soil as a sharp wetting front computing the infiltration or rainfall loss with eq 4 4 f t k s 1 φ θ i ψ f t where ft is the infiltration rate or rainfall loss rate at time step t mm h ks is the saturated hydraulic conductivity mm h φ is the soil porosity θ i is the initial water content ψ is the wetting front suction mm and ft is the cumulative infiltration or rainfall loss at time step t mm water infiltrates in soils until the rainfall rate exceeds the soil limited infiltration rate therefore the time to ponding tp is considered specifically in a given pixel with homogeneous land cover and soil type characteristics the green ampt module applied in safer rain computes the overall infiltration depth f at the end of the rainfall event t d with eq 5 5 f i d i f d t p i t p k s d t p ψ δ θ ln ψ δ θ f ψ δ θ i t p i f d t p where i is rainfall intensity mm h δ θ is defined as φ θ i d is the storm duration and tp can be computed with eq 6 6 t p i f i k ψ δ θ k s i i k s i f i k land use and lithology layers have been used to identify soil types in the pamplona metropolitan area values of the green ampt parameters for each soil type have been obtained from literature based on previous experiences see e g chow et al 1988 3 methodology the methodology consists of six steps fig 4 the first step of the methodology consists of benchmarking the safer rain algorithm with the iber model in the barañáin and zizur mayor municipalities by using the precipitation fields for the four storms selected in table 2 the second involves identifying a set of real rainfall events that can generate pluvial flood events in pamplona extracted from the time series recorded at the four gauging stations considered in the study in the third the copula based stochastic generator of rainfall storms that maintain the statistical characteristics of the real storms identified in the previous step is described in the fourth the methodology to estimate t year water depths in each cell of the pamplona metropolitan area is offered in the fifth the methodology to compare the results of the stochastic methodology with the standard approach is presented in the sixth and last step the methodology to assess the uncertainty of t year water depth estimates is shown 3 1 benchmarking of safer rain the safer rain algorithm outputs are benchmarked in the barañáin and zizur mayor municipalities fig 2 with the 2d hydrodynamic iber model the 10 min qpe fields generated for the four storms included in table 2 are used as input data in both models there are no observations available about flood extents and water depths for such four pluvial events therefore the outputs of the 2d hydrodynamic model are considered as the gold standard truth for the sake of simplicity the urban drainage system is not considered in either model while iber is a 2d hydrodynamic model the safer rain tool is hydrostatic and can only simulate water depths and flood extents in depressions where water is stored therefore maximum water depths simulated by iber cannot be used in the comparison and the iber model is run until six hours after the end of the storm when hydrodynamic processes are expected to be smoothed water depth outputs of the iber model at six hours after the end of the storm are compared with outputs of the safer rain algorithm the errors between the models are quantified by using a set of objective functions bennett et al 2013 falter et al 2013 berkhahn et al 2019 samela et al 2020 a given cell is considered as flooded when its water depth is equal or greater than 10 cm first the bias and root mean square error rmse between water depths in the flooded area are calculated second the critical success index csi is used to quantify the agreement between the two models in terms of flood extents measuring the fraction of cells that were correctly modelled and penalizing both misses and false alarms the probability of detection hit rate or true positive rate rtp calculates the number of flooded agreements among the flooded areas in the gold standard truth simulation quantifying the model sensitivity the true negative rate rtn or specificity counts the number of non flooded agreements among the non flooded cells in the benchmark simulation the accuracy correlation coefficient acc quantifies the number of correct assessments between the simulations lastly the mathews correlation coefficient mcc measures the correlation between the binary classification in the simulations 7 csi tp tp f p f n 8 r tp tp tp f n 9 r tn tn tn f p 10 acc tn t p tn t p f n f p 11 mcc t p t n f p f n t p f p t p f n t n f p t n f n where tp true positive or hints is the number of cells where both models delineate them as flooded fp false positive or false alarms is the number of cells that safer rain simulates as flooded and iber simulates as non flooded tn true negative is the number of cells where both models agree in delineating them as non flooded and fn false negative or misses is the number of cells that safer rain simulates as non flooded and iber simulates as flooded csi can adopt values of between zero and one while a value of one points to a perfect simulation of safer rain a value of zero indicates that none of the flooded cells simulated by safer rain are flooded in the iber simulation mcc can provide values between 1 that would mean a complete disagreement between safer rain and iber and one that would point to a perfect simulation of safer rain a value of zero indicates that the safer rain output would not be better than a random prediction in addition the nash sutcliffe efficiency coefficient nse is calculated in each cell for the four pluvial flood events eq 12 12 nse 1 i 1 n wd s a f e r r a i n i wd iber i 2 i 1 n wd iber i w d iber 2 where wdsafer rain i is the water depth simulated by safer rain in the ith pluvial flood event wdiber i is the water depth simulated by iber in the ith pluvial flood event w d iber is the mean value of the water depths simulated by iber in the n flood events and n is the number of flood events considered in cells with a negative value of nse w d iber would be a better prediction than the safer rain output nse has been calculated in the cells with w d iber greater than 0 1 m as smaller values of w d iber can lead to an excessively small value of the denominator in eq 12 resulting in unusual high negative values of nse despite small differences between safer rain and iber simulations 3 2 extraction of rainfall events rainfall events that can drive pluvial floods in the metropolitan area of pamplona are identified in this study a peak over threshold pot approach is selected to consider a larger set of rainfall events that exceed a given threshold regardless of when they occurred in a pot series the arrival of rainfall events that exceed the threshold can be characterised by a poisson process madsen et al 1997 eq 13 13 p m k λ k k e k where p m k is the probability of having a number of exceedances m equal to an integer number k in a given period of t years k is an integer number equal or higher than zero k 0 1 2 and λ is the mean annual number of exceedances that is obtained with eq 14 14 λ m t where m is the number of exceedances over the threshold in a period of t years pluvial floods are generated by high intensity events in short durations consequently long events with high rainfall amounts and low intensities should not be considered in this analysis the pot threshold for pluvial floods is usually selected in terms of rainfall intensity the extreme rainfall alert era establishes a threshold that depends on the storm duration 30 mm in 1 h 40 mm in 3 h and 50 mm in 6 h hurford et al 2012 apel et al 2016 selected a threshold of 18 mm in 1 h blanc et al 2012 identified a decreasing mean rainfall intensity threshold with storm duration from around 3 mm h in 1 h to 2 mm h in 10 24 h mailhot et al 2013 selected a threshold between 2 and 10 mm in 30 min depending on the calendar day in this study a threshold of 10 mm in 30 min that corresponds to a rainfall intensity of 20 mm h is selected considering the characteristics of the storms that have generated pluvial floods in pamplona table 2 3 3 stochastic generation of rainfall events a stochastic methodology to generate synthetic rainfall events that maintain the statistical properties of the rainfall events identified in section 3 2 is presented storms are characterised by rainfall depth and duration which are the input data of the safer rain algorithm synthetic pairs of rainfall depth and storm duration can be generated by using bivariate copulas palla et al 2018 in this exploratory analysis for the sake of simplicity some simplifications have been introduced though safer rain can consider spatial distributions of rainfall and qpe fields have been considered in the benchmarking analysis to simulate four real pluvial flood events accurately precipitation fields have not been considered in the stochastic rainfall generator to reduce the computation times required for calibrating the rainfall generator simulating the precipitation fields and running the safer rain algorithm therefore the stochastic method will consider uniform rainfalls and durations in the pamplona metropolitan area and a longer set of 10 000 synthetic rainfall events will be examined in the study the procedure consists of two steps in the first the observed storms that exceed the threshold of 10 mm in 30 min at the four rainfall gauging stations are aggregated in a unique synthetic gauging site in order to obtain a larger time series and reduce estimate uncertainties this aggregation is justified considering that the distance between the rainfall gauging sites is small enough fig 2 to assume that they have similar characteristics in addition gauging sites are far enough to record varying storm rainfall depths intensities and durations for a given pluvial flood event given that such events are characterised by a high spatial variability in the second a bivariate approach is used to generate synthetic rainfall events in this case the first step consists of fitting the marginal distributions of rainfall depth and duration for the aggregated set of storms that exceed the threshold the magnitude of exceedances over a threshold are usually characterised by a generalised pareto gp distribution function madsen et al 1997 lang et al 1999 as it is a limiting distribution for a series of values that exceed a threshold and it has threshold stability martins and stedinger 2001 the parameters of the gp distribution are estimated by using the l moments method hosking and wallis 1993 1997 the second step of the bivariate approach consists of using a copula to characterise the dependence between storm rainfall depths and durations sklar 1959 in this study three archimedean copula families are considered clayton frank and gumbel in addition the gumbel copula belongs to the extreme value family the cramér von mises statistic sn is selected as the goodness of fit test to identify the copula that can represent better the dependence structure between rainfall depths and durations eq 15 as genest et al 2009 found that sn led to the best results for all copula models requena et al 2013 15 s n i j 1 n i c n i r ij n i 1 s ij n i 1 c i r ij n i 1 s ij n i 1 θ cop i 2 i 1 n where c n i u 1 u 2 is the empirical copula at site i rij and sij are the ranks associated with xij and yij θ cop i are the estimated parameters of the fitted parametric copula ci at site i and ni the record length at site i c n i u 1 u 2 is expressed by eq 16 16 c n i u 1 u 2 1 n i j 1 n i 1 r ij n i 1 u 1 s ij n i 1 u 2 u 1 u 2 0 1 i 1 n where 1 e is the indicator function of the set e valued 1 inside e or 0 outside and n is the observed record length in addition the p value associated with a given goodness of fit test is crucial in determining if a given copula model is suitable to characterise the dependence between two variables genest and rémillard 2008 therefore first the p value is used to reject a given copula model when it is smaller than 0 05 then the value of the sn statistic is used to select the best copula among the non rejected ones the lowest value of sn indicates the best fitting to observations salvadori and de michele 2011 consequently the selected copula should have the lowest value of sn with a p value greater than 0 05 lastly a long set of n synthetic storms characterised by rainfall duration pairs are generated randomly by using the best copula identified 3 4 delineation of pluvial flood hazard maps the series of n synthetic rainfall duration pairs generated by using the methodology described in section 3 3 are the safer rain input data considering infiltration processes with the aforementioned green ampt infiltration equation section 2 2 3 a constant initial soil water content was considered to avoid varying the green ampt parameter layers in the sequential simulation with safer rain the outputs of safer rain are processed to obtain a long set of n water depths in each cell then the t year water depth is estimated by using a frequency analysis in each cell flood hazard maps are usually obtained for a set of return periods that represent the expected exceedance probability in a given year however the n synthetic rainfall duration pairs represent the storms expected to exceed the threshold of 10 mm in 30 min with a mean value of λ exceedances per year eq 14 consequently the n water depths in each cell do not represent the annual maximum water depths expected in n years though the water depths that are expected to exceed a threshold in n λ years the relationship between the return period in an annual maximum series analysis ta and the return period in a pot series tp is given by eq 17 stedinger et al 1992 17 t p 1 l n 1 1 t a where tp is the return period in a pot series and ta is the return period in an ams clearly tp will be smaller than ta as more than one event is expected to exceed the threshold per year in a pot series 3 5 comparison with the standard approach the next step of the methodology consists of a comparison between the results of the stochastic methodology proposed in this study with the standard approach based on catchment response times used in practice the standard approach assumes that the t year pluvial flood in a given depression is generated by the t year storm with a duration equal to the time of concentration of the draining catchment to such a depression three depressions identified in the safer rain pre processing phase in the pamplona metropolitan area have been selected for the comparison a motorway depression at zizur mayor and two depressions in the echavacóiz neighbourhood and at soto aizoáin street at an industrial estate at orcoyen fig 5 the time of concentration of the draining catchment to each depression is estimated by using the velocity method that estimates travel times along the hydraulically most distant flow path nrcs 2010 then the design storms estimated for a duration equal to the time of concentration are used as input data of the safer rain algorithm comparing the outputs with the results of the stochastic methodology in a set of cells selected in each such depression 3 6 t year water depth estimate uncertainty assessment the last step of the methodology consists of quantifying the uncertainty of the t year water depth estimates obtained with the stochastic methodology proposed in this study in section 3 4 t year water depths are estimated from outputs of the safer rain algorithm by using a given set of n storm rainfall duration pairs generated by the copula fitted to the observations section 3 3 therefore such water depth quantiles could be conditioned by the set of rainfall duration pairs used in each case therefore the stochastic methodology has been replicated ten times generating ten sets of n storm rainfall duration pairs with the copula fitted to the observations the safer rain algorithm is run by using each set of n rainfall duration pairs as input data for each set of n rainfall duration pairs t year water depths are estimated in each cell of the pamplona metropolitan area by using the frequency analysis described in section 3 4 the uncertainty analysis has been limited to the draining catchment to the orcoyen depression delineated in section 3 5 fig 5 to reduce the computation times of the safer rain algorithm the stochastic methodology uncertainty is assessed by analysing the dispersion of the ten t year water depth estimates for each return period in five cells located in the orcoyen depression fig 12c 4 results this involves six steps first the results of the benchmarking activities to validate the outputs of the safer rain model with the hydrodynamic simulations of iber in four real storms are presented second the application of the stochastic methodology to generate synthetic rainfall events that maintain the statistic properties of real rainfall events which exceed a given threshold is offered third the results of pluvial flood hazard mapping in the pamplona metropolitan area are shown fourth the results obtained from the comparison between the stochastic methodology and the standard approach are presented fifth bivariate return period curves are estimated in a set of cells located in three depression of the pamplona metropolitan area sixth and lastly the uncertainty in t year water depth estimates is assessed in five cells located in the orcoyen depression 4 1 safer rain benchmarking in pamplona the outputs of the safer rain algorithm are compared with water depths simulated by the iber hydrodynamic model six hours after the end of the rainfall event in the barañáin and zizur mayor municipalities considering the precipitation fields in four real storms table 2 the first storm of 18 september 2019 is not considered in the barañáin municipality as it did not affect the area significantly fig 3 fig 6 shows the outputs of the safer rain algorithm and the iber 2d hydrodynamic model for the pluvial flood event of 20 july 2010 in the barañáin municipality such a figure confirms that safer rain correctly identifies the main depressions prone to flooding in pluvial flood events the results obtained from the other two storms in barañáin and the four storms in zizur mayor are included in the supplementary material fig s1 s6 fig s7 and fig s8 show the errors in water depths between the outputs of safer rain and iber most of the pixels show yellow and orange colours that point to a generally slightly lower estimate of water depths by safer rain it can be explained by the hydrostatic assumptions of safer rain that neglects backwater dynamic curves in overland flows fig 7 shows the nse values in barañáin and zizur mayor eq 12 the highest nse values blue areas are obtained in the main depressions pointing to a good prediction of safer rain in the areas prone to pluvial flooding where water is stored and water depths are greater on the contrary negative values of nse red areas are usually located in zones with small water depths fig 8 shows the histograms of nse in terms of the magnitude of w d iber wd in the figure legend in barañáin and zizur mayor in barañáin most of cells have high values of nse regardless the magnitude of water depths in addition 50 of cells with w d iber greater than 1 m have a nse value greater than 0 6 in zizur mayor the percentage of cells with high values of nse increases with water depths for small water depths with values of w d iber between 0 1 and 0 2 m 30 of cells have nse values between 0 6 and 1 and only 10 between 0 6 and 1 however for the greatest water depths with w d iber values greater than 1 m almost 80 of cells have nse values greater than 0 6 consequently safer rain supplies good predictions in depressions where greater water depths are expected though it supplies worse predictions in zones with small water depths the bias rmse csi rtp rtn acc and mcc objective functions are used to quantify the differences in water depths between iber and safer rain outputs table 3 in barañáin the bias is between 2 and 4 cm that points to small errors in water depths such values close to zero mean that safer rain compensates positive and negative errors and does not tend either to overestimate or underestimate water depths at barañáin the rmse values are greater between 19 and 25 cm as they do not account for compensation between positive and negative errors in terms of flood extents the csi values are 0 42 and 0 45 for the 2010 and 2020 events respectively and close to 0 3 for the second storm of 2019 indicating that safer rain represents correctly the 30 40 of the flood extent simulated by iber rtp is 0 5 for the events of 2010 and 2020 and 0 38 for the second storm of 2019 meaning that safer rain correctly simulates 50 and 38 respectively of the flooded cells simulated by iber these results may show good agreement between the models accounting for the limitations of the hydrostatic assumptions of safer rain regarding the hydrodynamic simulations of iber rtn and acc are close to one in the three events proving that the percentage of agreements in cells is high either flooded or non flooded and in line with that observed in samela et al 2020 lastly mcc values are between 0 45 and 0 63 meaning that safer rain provides a good prediction of the iber simulations at zizur mayor the bias is between 21 and 16 cm with a value close to zero in the second storm of 18 september 2019 the rmse values are between 0 26 and 0 28 in the 18 september 2019 flood event the event of 20 july 2020 provides a rmse value of 0 48 therefore at zizur mayor safer rain tends to underestimate water depths with greater rmse values for the highest rainfall magnitude csi values are between 0 24 and 0 38 which indicates that 24 38 of the flood extents simulated by iber are predicted correctly by safer rain rtp values are between 0 32 and 0 46 revealing agreement of 32 46 of safer rain with the flooded cells of iber as stated above these results provide agreement of safer rain and account for hydrostatic limitations rtn and acc are close to one in the four flood events considered at zizur mayor therefore the percentage of agreements in cells either flooded or non flooded is also high mcc values are equal to 0 52 and 0 57 in the 20 july 2010 and respectively the second storm of 18 september 2019 flood events the mcc values are smaller for the first storm of 18 september 2019 with a value close to 0 4 therefore it is clear that safer rain provides a good prediction of the iber simulations in terms of flood extents mainly in the flood events of 20 july 2010 and the second storm of 18 september 2019 comparing the results in pamplona spain with those obtained in rimini italy samela et al 2020 sensitivity rtp is up to 71 in rimini and up to 50 in pamplona however at pamplona specificity rtn is slightly greater with values of 99 in all the cases accuracy values obtained from acc are similar samela et al 2020 found that while sensitivity increases with storm precipitation accuracy decreases slightly the return periods of the four real storms considered in the benchmark analysis in pamplona are below 20 years therefore the smaller sensitivity values may be explained by smaller precipitations in the storms considered in pamplona in summary small csi values indicates that flood extents predicted by safer rain are usually smaller than expected however high nse values for the greatest water depths point to a good prediction in the main depressions with the highest water depths and the greatest flood hazards therefore the safer rain tool offers adequate results at barañáin and zizur mayor considering the hydrostatic assumptions of safer rain regarding the hydrodynamic behaviour of iber and the high reduction of computation times from hours to around one minute 4 2 stochastic generation of rainfall events the first step in stochastic generation of rainfall events consists of the extraction of real storms that exceed the threshold of 10 mm in 30 min cumulative precipitation in two consecutive time steps is considered in the 15 min rainfall time series and in three consecutive time steps in the 10 min rainfall time series fig s9 in a given storm several consecutive time steps could have a cumulative precipitation over the threshold therefore the beginning and ending time steps of storms were identified table 4 shows that the four rain gauging stations have similar values of λ indicating an analogous rainfall behaviour with a similar probability of storm arrival exceeding the threshold of 10 mm in 30 min at the four stations therefore the storms can be aggregated in a unique series of 138 rainfall events as rainfall gauging sites are close enough to have a similar rainfall behaviour though far enough to record different storms in pluvial flood events that are characterised by precipitations with high spatial variability the mean annual number of exceedances over the threshold for the aggregated data is calculated as the weighted mean of λ by the record length at each station obtaining a value of 2 03 exceedances per year the marginal distributions of storm rainfall depth and duration are obtained by using a gp distribution with the l moments method fig 9 the rainfall depth frequency curve has a lower bound of 10 mm that corresponds to the pot threshold guaranteeing that the synthetic rainfall events generated by the copula based approach will have a precipitation equal or higher to such a threshold and avoiding the generation of events with smaller precipitation that could not drive pluvial floods the duration frequency curve has a lower bound of 30 min which also agrees with the pot threshold and an upper bound close to 3 5 h therefore the synthetic rainfall events will have durations between 0 5 and 3 5 h avoiding the generation of excessively short or long storms that could not drive pluvial floods the clayton frank and gumbel family copulas were fitted to the aggregated 138 precipitation duration pairs fig 10 the sn goodness of fit test and p value were obtained by using 10 000 parametric bootstrap samples for each copula family table 5 the results show that the clayton and frank copulas have a p value lower than 0 05 indicating that they are unable to characterise the dependence relationship between precipitation and duration pairs as only the gumbel copula obtained a p value higher than 0 05 it cannot be rejected in addition the gumbel copula also leads to the lowest value of the sn statistic consequently the gumbel copula is the most appropriate when characterising the dependence structure of the 138 precipitation duration pairs of real rainfall events that can drive pluvial floods in the pamplona metropolitan area 4 3 pluvial flood hazard maps in pamplona a set of 10 000 random precipitation duration pairs was generated with the gumbel copula fitted in section 4 2 obtaining short durations between 0 5 and 3 5 h and rainfall depths equal or greater than 10 mm required to generate pluvial floods in pamplona fig 10c the safer rain algorithm was run with the 10 000 synthetic storms and considered infiltration processes with the green ampt model the safer rain algorithm is embedded in the saferplaces platform in pamplona https platform saferplaces co however a jupiter notebook script in google colaboratory was used to run such a long number of simulations iteratively the safer rain pre process was run once in the pamplona metropolitan area for 121 40 s the flooding phases i e the filling and spilling processes were simulated in around 45 s for each event supplying water depths in each cell in a given cell the water depth distribution can include several zeros that represent the storms that do not generate enough water accumulation in the depression to flood the cell in addition it can show an upper bound that represents 100 filling water depth in the depression therefore no distribution function could fit such data adequately in each cell t year water depths were estimated empirically from the sorted series of 10 000 water depths by using the weibull plotting position formula and considering the return periods in a pot analysis tp eq 17 seven return periods are considered two five 10 25 50 100 and 500 years fig 11 shows the pluvial flood hazard maps for the two 10 100 and 500 year return periods the hazard maps for other three return periods five 25 and 50 years are included in the supplementary material fig s10 s12 the main pluvial flood hotspots in pamplona could be identified from such maps fig 11b i the motorway depression at zizur mayor ii the soto aizoáin street at the industrial estate of orcoyen driven by a building with three industrial units that obstructs a former stream iii echavacóiz neighbourhood driven by a levee to protect the area from river elorz fluvial floods iv depression at the miluze and ermitagaña crossroads v railway underpass at san jorge vi a depression at the sadar street vii a depression between txantrea and burlada and viii a depression at the burlada sports centre driven by a levee to protect the area from river arga fluvial floods 4 4 comparison with the standard approach the results of the stochastic methodology with safer rain are compared with the standard approach that considers the t year storm precipitation for a duration equal to the draining catchment response time a time of concentration of 25 min was estimated at the motorway depression at zizur mayor 50 min at the echavacóiz neighbourhood and 75 min at the orcoyen industrial area by using the velocity method that estimates travel times along the hydraulically most distant flow path nrcs 2010 table 6 includes the t year storm precipitations considered in each depression a set of cells have been selected in each zone to compare the distribution of 10 000 water depths obtained by using the stochastic methodology with the standard approach results fig 12 the cells for the comparison have been selected scattered in main depressions where water is stored to characterise adequately the filling processes in this case 10 return periods are used for the comparison two five 10 20 25 30 50 100 200 and 500 years fig 13 shows the results for the two cells considered in the motorway depression at zizur mayor the distribution of water depths obtained with the stochastic methodology adequately fits the water depths obtained by the standard approach both approaches agree about when the depression begins to fill in cell 2 at a return period of below two years however in cell 1 the filling process begins for the 10 year return period for the stochastic methodology and between two and five years for the standard approach in the filling process water depths are similar in each approach though water depths are greater for the standard approach for low return periods the differences in water depths between the approaches are reduced as the return period increases in cell 1 in cell 2 water depths for the stochastic methodology are slightly greater for the highest return periods in the echavacóiz neighbourhood both methods provide agreement regarding the beginning of the filling process of between two and five years in cell 1 and five and ten years in cell 3 fig 14 in cell 2 the filling process begins for the two year return period for the standard approach and between two and five years for the stochastic methodology in this case the stochastic methodology simulates the filling process faster than the standard approach in the three cells as the return period for the 100 depression filling is smaller for the stochastic methodology however distributions show a similar shape for each methodology and water depth differences for given return periods are small in the range of 10 cm in the orcoyen commercial area fig 15 water depth distributions show similar shapes and point to an adequate simulation of the depression filling processes in each case though the standard approach estimates slightly greater water depths the filling process in the depression begins at lower return periods in the standard approach the greatest differences between water depths for given return periods are for the lowest return periods with the differences becoming smaller for high return periods 4 5 bivariate pluvial flood return periods in addition the stochastic methodology can supply information about the relationship between the variables that characterise storms and pluvial flood water depths in a given depression mediero et al 2010 specifically the set of storms that generate water depths higher than the t year water depths estimated empirically in section 4 3 can be identified in a given depression obtaining the bivariate return period curves of pluvial floods in the three depressions selected as case studies in section 3 5 fig 16 shows that the t year water depth in a given depression is not driven by a unique storm such as the design storm rather than by a set of storms with different combinations of precipitation and storm duration for example the five year water depth in cell 5 of the orcoyen depression boundary between green and blue colours in fig 16c is generated by a set of storms 16 6 mm in 30 min 23 3 mm in one hour 27 5 mm in 1 5 h and 30 5 mm in two hours among others consequently the bivariate return period curve for a given return period t identifies the possible combinations of precipitation and storm duration that generate the t year pluvial flood bivariate return period curves show that precipitation increases with storm duration for a given return period though the relationship is not linear such an increasing relationship is related to both the mean rainfall intensity and precipitation losses generated by infiltration processes in a given storm for a given bivariate return period curve while the precipitation increases with storm duration the mean rainfall intensity decreases with storm duration this indicates that a shorter storm with a higher mean rainfall intensity can generate the same water depth than a longer storm with a smaller mean rainfall intensity in addition rainfall water volumes increase with storm precipitation with greater water depths being expected however for a given storm precipitation water runoff volume decreases as storm duration increases because infiltration processes give place to greater rainfall losses higher rainfall losses are generated in rural catchments with high infiltration rates nevertheless smaller rainfall losses are expected in urban catchments where soils are mostly sealed and infiltration rates are lower therefore bivariate return period curves will have steeper slopes in rural catchments than in urban catchments furthermore return period curves could even be horizontal in a catchment with soils completely sealed where no rainfall losses will be generated in the three depressions selected in section 3 5 the draining catchment at the orcoyen depression is mostly rural the catchment in echavacóiz is a mixture of rural and urban areas and the zizur mayor motorway depression has a draining catchment that is mostly urban therefore the zizur mayor depression will have the mildest bivariate return period curve slope as its draining catchment is mostly urban and rainfall losses are smaller smoothing the increasing relationship between precipitation and storm duration for a given bivariate return period curve however the orcoyen depression will show the steepest bivariate return period curve as its mostly rural draining catchment increases rainfall losses 4 6 uncertainty in t year water depth estimates finally the uncertainty in t year water depth estimates is assessed ten sets of 10 000 storm rainfall duration pairs are generated with the gumbel copula fitted to the observations in section 4 2 the safer rain algorithm is run with the ten sets of rainfall duration pairs in the draining catchment to the orcoyen depression fig 5 five cells are selected for assessing the uncertainty of t year water depth estimates fig 12c by plotting the dispersion of the ten t year water depth estimates in each cell by using boxplots fig 17 fig 17 shows that uncertainty increases with decreasing water depths boxplots are greater in cells 1 and 4 where water depths are smaller in addition uncertainty increases with the return period in cells 2 3 and 5 the greatest uncertainties are found for the highest return periods furthermore in cells 2 3 and 5 high uncertainties are also found for the lowest return periods with smaller water depths this agrees with the findings of the benchmarking activities section 4 1 where higher errors in the safer rain algorithm outputs were found when water depths are small therefore the most reliable estimates are found for return periods between 10 and 100 years where almost all the ten t year water depth estimates agree in addition for high return periods the uncertainty increases for small water depths therefore the stochastic methodology supplies more reliable results in the lowest cells of the main depressions where greater water depths are expected 5 discussion it can be argued that the standard approach used for pluvial flood hazard mapping in practice and based on design storms has certain drawbacks one could be that design storms may not be feasible when considering the real storms that can arrive at the area studied another would be that such an approach assumes that the t year storm generates the t year pluvial flood event and lastly another would be that empirical formulae used to estimate catchment response times and durations of design storms present significant uncertainties when in practice the stochastic methodology proposed in this study considers the return period concept as an exceeding water depth threshold identified from a large set of 10 000 storms with frequencies rainfall depths and durations similar to the observations at rain gauging stations therefore such a methodology improves the standard approach as it estimates t year water depths regardless of catchment response times avoiding the uncertainties by using empirical formulae and design storms that could not be realistic in addition the stochastic methodology shows that the t year water depth in a given depression can be generated by a set of storms with varying combinations of storm precipitation and duration instead of by a single design storm consequently the stochastic methodology can be considered sounder that the standard approach moreover the stochastic methodology can easily be applied to multi depression urban areas as it does not need to estimate a design hyetograph in each depression in terms of catchment response times however a drawback of the proposed methodology could be the need to repeat the simulations when a mitigation measure is applied changing flood hazards nevertheless the dtm can be clipped focusing in the area where flood hazards change with the new mitigation measure reducing the computation time of the new simulations considerably the benchmarking results between the safer rain algorithm outputs and the iber 2d hydrodynamic simulations show that safer rain supplies adequate results in terms of water depths considering its hydrostatic assumptions safer rain supplies better predictions of water depths in depressions prone to pluvial flooding where the greatest water depths are expected in terms of both water depth magnitude and uncertainty however safer rain shows some limitations to reproduce flood hazards in zones where small water depths are expected consequently t year water depth estimates in lower points of the main depression are more reliable than t year estimates in areas with small water depths where such estimates should be taken with caution in addition the exploratory analysis presented in this study has considered some simplifying assumptions the first is that synthetic uniform spatial rainfall events have been considered though the safer rain tool can consider precipitation fields with heterogeneous spatial distribution such a simplification avoids the complexity and computation times required for calibrating a spatially distributed rainfall generator and for simulating safer rain with precipitation fields for such a long set of storms the second is that a constant initial soil water content in the green ampt model has been considered and the urban drainage system in the pamplona metropolitan area could not be considered as the current version of the saferplaces platform does not include such functionalities yet lastly the main limitation of the stochastic methodology proposed arises from the hydrostatic behaviour of the safer rain algorithm that cannot consider overland flow dynamics however the reduction in computation times obtained from such an assumption compared with 2d hydrodynamic models is from several hours to around one minute allowing pluvial flood hazard maps at multi depression cities or municipalities at spatial scales to be generated in reasonable times such a stochastic approach would be unaffordable with 2d hydrodynamic models moreover not only t year water depths are obtained in each cell but also the complete probability distribution of water depths and bivariate return period curves in terms of precipitation and storm duration 6 conclusions a stochastic methodology to delineate pluvial flood hazard maps in urban areas by using the fast processing and dem based safer rain algorithm is presented the methodology was applied to the metropolitan area of pamplona in spain 68 26 km2 the safer rain algorithm was benchmarked with the iber 2d hydrodynamic model at the barañáin and zizur mayor municipalities considering 10 min precipitation qpe fields for four real storms that generated pluvial floods in the last years the benchmarking results showed that the safer rain algorithm offers adequate results in terms of water depths considering its hydrostatic simplifications in addition safer rain offers better predictions in depressions where greater water depths are expected than in flatter areas with smaller water depths a set of 138 real storms in which each one exceeds a threshold of 10 mm in 30 min was identified from the observations at four rainfall gauging stations the gumbel copula was selected as the most suitable to represent the dependence between the 138 real storm precipitation duration pairs a large set of 10 000 synthetic storms with similar statistical characteristics to the real storms was generated randomly and used as input data in the safer rain algorithm considering infiltration processes with the green ampt model the safer rain pre process was run once in 121 40 s and the flooding phase was simulated in around 45 s for each event therefore the computation time was reduced from some hours to around one minute for each event regarding 2d hydrodynamic modelling in each cell t year water depths were estimated empirically from the series of 10 000 simulated water depths pluvial flood hazard maps were developed in pamplona for 10 return periods in the range from two to 500 years identifying the main pluvial flood hotspots in the urban area the stochastic methodology was compared with the standard approach based on t year design storms several points at three depressions in pamplona were selected for the comparison concluding that both methodologies supply similar results in the control points however the stochastic methodology proposed in this paper addresses the drawbacks of the standard approach first the stochastic methodology avoids the uncertainties associated with catchment response time estimates with empirical formulae second the stochastic methodology does not need to consider varying storm durations in terms of the catchment response time for each depression and can be easily implemented in multi depression areas third the assumption that states that the t year storm generates the t year pluvial flood is improved by obtaining the complete probability distribution of water depths in each cell in addition bivariate return periods can be estimated showing that a given t year pluvial flood can be generated by a set of storms with varying combinations of precipitation and duration bivariate return period curves show that precipitation increases with storm duration for a given return period in a given depression the slope exhibited by such curves will depend on infiltration rates at its draining catchment higher infiltration rates will generate greater rainfall losses and smaller runoff volumes leading to a higher increase in precipitation with storm duration therefore rural catchments will have steeper bivariate return period curves than urban catchments in this first exploratory analysis spatially uniform precipitations have been considered the results are limited by the hydrostatic assumption of the safer rain tool identifying water depths only in depressions where runoff water volumes tend to accumulate however the stochastic methodology proposed in this study benefits from the reduced computation times required by the fast processing dem based safer rain algorithm overcoming the limitations associated with the high computation times required by two dimensional hydrodynamic models therefore the methodology proposed is a promising technique to develop consistent pluvial flood hazard maps in urban areas at large scale and multi depression cities or municipalities credit authorship contribution statement luis mediero conceptualization methodology data curation writing original draft writing review editing visualization investigation supervision project administration enrique soriano methodology data curation visualization investigation peio oria conceptualization methodology data curation writing original draft writing review editing visualization investigation stefano bagli software supervision project administration attilio castellarin conceptualization methodology software writing original draft writing review editing investigation supervision project administration luis garrote conceptualization methodology investigation supervision paolo mazzoli software project administration jaroslav mysiak methodology investigation project administration stefania pasetti software project administration simone persiano methodology software writing original draft writing review editing investigation david santillán data curation writing original draft writing review editing investigation kai schröter methodology investigation project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the eit climate kic demonstrator project saferplaces improved assessment of pluvial fluvial and coastal flood hazards and risks in european cities as a mean to build safer and resilient communities task id eit 2 2 37 200246 p401 1b the saferplaces project aims to advice identification and assessment of flood risk mitigation measures and plans inform climate adaptation and disaster risk reduction strategies and help to foster multi stakeholder agreements and partnership for resilience building it has employed innovative climate hydrological and hydraulic topographic and economic modelling techniques to develop the saferplaces platform https platform saferplaces co in order to assess pluvial fluvial and coastal flood hazard and risk in urban environments under current and future climates the authors would like to acknowledge the spanish instituto geográfico nacional ign for supplying the 2 m dem the saih real time system of the river ebro basin authority the spanish agencia estatal de meteorología aemet the regional government of navarre and the universidad pública de navarra upna for supplying the precipitation data and the consorcio de compensación de seguros for supplying a database of direct flood losses in pamplona used to identify the main areas prone to pluvial floods the authors also express their gratitude to andrew selby for the help and guidance received with regard to language usage and expression appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 127649 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3498,climate change and rapid expansion of urban areas are expected to increase pluvial flood hazard and risk in the near future and particularly so in large developed areas and cities therefore large scale and high resolution pluvial flood hazard mapping is required to identify hotspots where mitigation measures may be applied to reduce flood risk depressions or low points in urban areas where runoff volumes can be stored are prone to pluvial flooding the standard approach based on estimating synthetic design hyetographs assumes in a given depression that the t year design storm generates the t year pluvial flood in addition urban areas usually include several depressions even linked or nested that would require distinct design hyetographs instead of using a unique synthetic design storm in this paper a stochastic methodology is proposed to address the limitations of this standard approach developing large scale 2 m resolution pluvial flood hazard maps in urban areas with multiple depressions the authors present an application of the proposed approach to the city of pamplona in spain 68 26 km2 the safer rain fast processing algorithm based on digital elevation models dems is compared with the iber 2d hydrodynamic model in four real storms by using 10 min precipitation fields precipitation recorded at rainfall gauging stations was merged with continuous fields obtained from a meteorological radar station given the hydrostatic limitations of safer rain the benchmarking results are adequate in terms of water depths in depressions a long set of 10 000 synthetic storms that maintain the statistical properties of observations in pamplona is generated safer rain is used to simulate runoff response and filling and spilling processes in depressions for the 10 000 synthetic storms obtaining the probability distribution of water depths in each cell maps of pluvial flood hazards are developed in the pamplona metropolitan area for 10 return periods in the range from two to 500 years from such pixel based series of simulated water depths bivariate return period curves are estimated in a set of cells showing that several storms can generate a given t year pluvial flood with an increasing precipitation with storm duration that depends on the draining catchment soil characteristics the methodology proposed is useful to develop maps of pluvial flood hazards in large multi depression urban areas in reasonable computation times identifying the main pluvial flood hotspots keywords pluvial floods safer rain flood hazard mapping rapid flood model urban areas bivariate return periods 1 introduction pluvial floods are usually generated by high intensity and short duration storms climate change projections point to an increase in the intensity and frequency of such extreme rainfall events kundzewicz et al 2014 furthermore the expansion of urban areas and the increasing density of assets in cities have amplified the economic and human consequences associated with pluvial floods kaspersen et al 2017 bulti and abebe 2020 pluvial flood hazards are usually assessed by using synthetic design storms krvavica and rubinić 2020 and two dimensional 2d hydrodynamic models in this study a stochastic methodology is proposed to estimate pluvial flood hazards in urban areas overcoming some of the major limitations of deterministic approaches synthetic design storms neglect the influence of both hyetograph shapes and spatial and temporal distributions of rainfall on water depths in addition such an approach assumes that the design storm for a given return period t generates the t year pluvial flood however the t year pluvial flood should be obtained by using a threshold in a variable that characterises flood hazardousness such as water depths moreover a high degree of uncertainty is associated with design hyetograph estimates propagating it through rainfall runoff simulations and pluvial flood hazard calculations as catchment response times are usually estimated with empirical formulae that may not be valid in areas with different conditions from those of the regions used to obtain them as well as the results can differ by up to 500 depending on the formula used grimaldi et al 2012 gericke and smithers 2014 tuyls et al 2018 consequently urban drainage systems could be over or under estimated depending on the empirical formula used in each case in addition urban areas are usually composed of multiple depressions prone to pluvial flooding that can be even linked and nested each depression should be analysed independently when using an approach based on synthetic design storms as varying catchment response times will lead to varying design hyetograph durations therefore multiple simulations are required increasing the complexity of pluvial flood hazard mapping at the scale of a city or municipality the limitations of the standard approach based on synthetic design hyetographs can be improved by using a stochastic analysis that considers the runoff response in a large set of storms maintaining the statistical properties of real rainfall events observed in a given area first design hyetograph estimates are not required as the complete probability distribution of water depths at any point will be obtained addressing the assumption that the t year storm generates the t year pluvial flood second the method can be applied to larger scales with multiple depressions as it is independent of catchment response times at each depression third a stochastic analysis avoids the uncertainties associated with catchment response time estimates by using empirical formulae consequently a stochastic approach will be more adequate for pluvial flood hazard mapping in urban areas with multiple depressions however a rapid tool for simulating the water depths generated by a large set of storms is required for conducting such a stochastic approach 2d hydrodynamic models are recognised to simulate pluvial floods in urban areas accurately henonin et al 2013 based on solving the 2d shallow water equations swes by using numerical methods therefore they require high computation times furthermore studies that use long sets of stochastic synthetic rainfall events are unusual for the delineation of pluvial flood hazard maps apel et al 2016 as they are not affordable with the computation times required by 2d hydrodynamic models either simplified hydrodynamic models are used to reduce simulation times nuswantoro et al 2016 or a reduced set of simulations are considered such as 45 simulations simoes et al 2015 and 160 simulations per probability apel et al 2016 recently computation times of 2d hydrodynamic models have been reduced with parallel computing techniques and graphic processing units gpus guidolin et al 2016 such as the lisflood fp and p dwave models neal et al 2009 leandro et al 2014 with topography simplifications by using sub grid models yu and lane 2006 neelz and pender 2007 and focusing on cells with low porosity values that tend to reduce computational efficiencies guinot et al 2017 bruwier et al 2017 in addition the solving complexity of 2d wses has been reduced by approximating or neglecting inertial and advection terms bates et al 2010 and by using the cellular automata ca approach ghimire et al 2013 guidolin et al 2016 however such approaches would not be enough for conducting probabilistic analyses bernini and franchini 2013 furthermore artificial neural networks anns can be trained with a reduced set of 2d hydrodynamic model outputs bermúdez et al 2018 berkhan et al 2019 though they are unable to interpolate between the rainfall events used to train the network a set of hybrid models has been also developed chang et al 2010 pan et al 2011 and approaches based on support vector machines have been offered lin et al 2013 jhong et al 2017 bermúdez et al 2019 therefore a technique based on a 2d hydrodynamic model with short enough computation times and satisfactory results is not available yet rapid flood models rfms have been developed to shorten the computation times in contrast to 2d hydrodynamic models they identify depressions from a digital elevation model dem considering a set of storage reservoirs connected through links the water balance equation is used to simulate depression filling and spilling processes bulti and abebe 2020 therefore high resolution dems can be used with small pre processing computation times rfms can reduce computation times to few minutes or even seconds up to 1000 times compared with hydrodynamic models teng et al 2007 consequently rfms are adequate for conducting probabilistic analyses some rfms are the rapid flood spreading model rfsm bernini and franchini 2013 the flood connected domain calculation fcdc method zhang et al 2014 the rapid urban flood inundation and damage assessment model rufidam model jamali et al 2018 and the cellular automata fast flood evaluation ca ffé model jamali et al 2019 hierarchical filling and spilling algorithms hfsas or puddle to puddle dynamic filling and spilling approaches p2ps have been recognised as promising techniques for characterising pluvial floods chu et al 2013 zhang and pan 2014 in this respect the safer rain tool has been developed recently samela et al 2020 safer rain is a hfsa that can simulate detailed spatially distributed infiltration processes with the green ampt model supplying flood depths and extents probabilistic approaches are more correct than deterministic approaches based on t year design events as design event estimates have significant uncertainties and 2d models are not perfect di baldassarre et al 2010 probabilistic approaches based on continuous simulation needs a long period of rainfall time series nuswantoro et al 2014 semi continuous simulation approaches have been proposed to reduce simulation times lawrence et al 2014 jamali et al 2020 however stochastic approaches can generate larger samples of storms that can extend the available time series of observed rainfall events in this study a stochastic methodology is proposed to delineate probabilistic pluvial flood hazard maps across large urban areas and cities with 2 m horizontal resolution the fast processing dem based safer rain algorithm samela et al 2020 is used benefiting from its computational efficiency and low runtimes as well as from its ability to simulate spatially distributed infiltration processes therefore the limited number of simulations that can be considered by using a 2d hydrodynamic model simoes et al 2015 apel et al 2016 is overcome by using such a rfm in addition the safer rain algorithm does not need to be coupled to a rainfall runoff model the stochastic approach proposed can generate a long set of synthetic rainfall events with similar statistical properties to real rainfall events by using a methodology based on copulas extending the usual length of rainfall time series utilised in continuous and semi continuous approaches in addition the limitations of using a deterministic approach based on a single synthetic design storm krvavica and rubinić 2020 are improved by considering such a large set of storms the probability distribution of water depths is obtained in each cell of the domain furthermore flood depths and extents for given return periods are estimated from the water depth series obtained in each cell in addition bivariate return period curves are estimated analysing the relationship between the variables that characterise storms and pluvial flood water depths in a given depression the methodology has been applied to the pamplona metropolitan area in spain this study is structured as follows section 2 presents the case study and the models used in the analysis section 3 offers the methodology proposed for the delineation of pluvial flood hazard maps at municipality scales section 4 presents the application of the methodology to the case study of the pamplona metropolitan area section 5 discusses the results and limitations of the methodology proposed section 6 summarises the main conclusions of the study 2 case study and models first this section presents the pamplona metropolitan area case study as well as the data available second the models used are described the safer rain algorithm the 2d iber hydrodynamic model and the infiltration equation used to simulate rainfall losses 2 1 pamplona case study pamplona is located in the navarre region in the northern part of spain fig 1 the pamplona metropolitan area is 68 26 km2 and has around 335 000 inhabitants including the municipalities of barañáin burlada cendea de olza cizur huarte orcoyen pamplona villava and zizur mayor fig 2 it is subject to pluvial floods mainly in the summer months the municipalities of barañáin and zizur mayor have the highest pluvial flood losses according to the database provided by the spanish consorcio de compensación de seguros that compensates the damages produced by natural hazards in spain for the 20 july 2010 pluvial flood event that is the greatest in the pamplona metropolitan area in the period 1996 2020 such a database supplies a total flood loss of 3 319 m updated to the year 2020 70 7 of the flood losses were concentrated in zizur mayor and barañáin areas 1 687 m in zizur maryor and 0 66 m in barañáin and the closest postal code of pamplona to barañáin therefore such areas have been selected for the safer rain benchmark activities described in section 3 1 the 2 m dem considered in the study was supplied by the spanish national geographic institute instituto geográfico nacional ign in spanish precipitation data at four gauging stations were supplied by the real time automatic hydrological information system sistema automático de información hidrológica saih in spanish of the river ebro basin authority the spanish state meteorological agency agencia estatal de meteorología aemet in spanish the regional government of navarre and the universidad pública de navarra upna in spanish see table 1 three real pluvial flood events have been identified in the barañáin and zizur mayor municipalities in recent years table 2 first the day of occurrence of the main flood events in the pamplona metropolitan area were extracted from the database supplied by the spanish consorcio de compensación de seguros such events can be either fluvial or pluvial therefore the type of flood for each event was identified by checking streamflow and precipitation observations as well as pieces of news in the diario de navarra newspaper finally the three main pluvial flood events in the pamplona metropolitan area were selected the return period for each event in table 2 has been estimated from the intensity duration frequency idf curve for the storm duration by using the total storm precipitation depth the 20 july 2010 event had a rainfall of 42 6 mm and a peak intensity of 66 4 mm h in 15 min at the rain gauging station p1 with a return period estimate of 14 3 years on 18 september 2019 two storms were identified a 37 8 mm storm with a peak intensity of 74 4 mm h in 10 min at the p4 station in the morning which corresponds to a return period of 16 3 years and a small scale 25 9 mm storm with a peak intensity of 73 2 mm h in 10 min at the p4 station in the afternoon associated with a return period of 4 37 years the storm of 25 april 2020 was highly localised at the barañáin municipality and characterised by using the precipitation fields described in section 2 1 1 with a peak rainfall of 12 mm in 10 min in the area of zizur mayor and barañáin 2 1 1 quantitative precipitation estimation fields in the pamplona metropolitan area optimised and high resolution quantitative precipitation estimations qpes are required for identifying where localised heavy or torrential storms may take place coping with pluvial flood events properly zhang et al 2016 the combination of surface and remote sensing based radar data can generate a merged product that reduces the errors obtained by using only either rain gauging interpolation or radar data as radar data cover large areas and capture better the spatial variability of rainfall fields mckee and binns 2016 a variety of techniques is available for the merging process ochoa rodríguez et al 2019 though geostatistical methods provide the most satisfactory results such as the kriging with external drift ked technique sharon and gaussiat 2015 ked is an extension of kriging that uses external variables in this case radar qpes as auxiliary information in the interpolation process linear weights employed in the interpolation of point gauge values are further constrained by the spatial association between radar and rain gauge values in addition the real time semivariogram model is obtained with real time radar data describing the spatial correlation between data qpes with a 10 min time step are obtained in the metropolitan area of pamplona for the four storms included in table 2 fig 3 precipitation recorded at automatic weather stations awss from different organisms such as aemet the regional government of navarre and crowdsourced networks are merged with raw continuous fields obtained from the aemet zaragoza regional radar that supplies 10 min surface rainfall intensity sri based on the 0 5 elevation radar reflectivity by using the c band 5 6 ghz the number of rain gauging stations used in each event depends on the data availability crowdsourced real time rainfall data is acquired from the weather underground network that is fed by a global community of people connecting data from personal weather stations for example rainfall data was available for 10 crowdsourced stations in the 25 april 2020 event radar data quality is good enough despite the signal attenuation and the high beam elevation as zaragoza is located 150 km from pamplona furthermore raw radar data are post processed with ground clutter identification correction for vertical profile of reflectivity vpr and reflectivity to rain rate conversion by using the marshall palmer z r relationship marshall and palmer 1948 while radar fields are generated with a time step of 10 min rainfall gauging stations collects data with a time step of five to ten minutes therefore at site rainfall data are synchronised with radar scan time minimising the uncertainty from signal mismatches by using advection schemes tabary 2007 shapiro et al 2010 in addition the linear relationship between primary and auxiliary variables in ked may not remain valid for the whole range of rainfall intensities especially in the case of heavy precipitation events the expected value of the primary variable is the estimated precipitation ked adds information about the secondary or auxiliary fields that may contribute to a better prediction of the geospatial field of precipitation as they are related to the primary variable the radar based sri data are utilised as auxiliary fields in addition logarithmic transformations of the variables are tested to improve the results in the case of dominant non linear relationships between primary and secondary variables such as in heavy or torrential storms therefore qpe fields are selected among four options automatically via cross validation in real time i only sri based qpe ii ked with linear variables in both the primary and auxiliary variables iii ked with the logarithm of the primary variable and iv ked with logarithms in both variables 2 2 models the safer rain algorithm samela et al 2020 has been developed within the saferplaces project safer rain is implemented in the saferplaces platform https platform saferplaces co in order to assess pluvial fluvial and coastal flood hazard and risk in urban environments under current and future climates in this paper safer rain is used to estimate water depths and flood extensions in the stochastic methodology in addition the 2d hydrodynamic iber model bladé et al 2014 was used for the safer rain benchmarking with the four storms selected in table 2 both models consider the green ampt infiltration equation for simulating rainfall losses green and ampt 1911 2 2 1 safer rain algorithm safer rain is a fast processing hfsa that identifies pluvial flooded areas on the basis of nested surface depressions extracted from high resolution lidar dems and a given rainfall depth considering spatially distributed rainfall inputs and infiltration processes samela et al 2020 its main simplifying assumption consists of neglecting overland flow dynamics consequently net rainfall volumes accumulate in the system of nested depressions according to their capacity and hierarchical structure safer rain consists of two main steps in the first dem pre processing aims to identify the hierarchy tree of nested depressions that defines the sequence of depression filling and spilling as well as the filling volumes in each one in the second depression flooding identifies flooded areas and simulates corresponding water depths the flooding phase implemented in safer rain is original in three senses in the first a bottom up level set method is applied for quantifying partial filling in nested higher level depressions for more details see samela et al 2020 in the second either uniform or spatially variable i e gridded rainfall depths can be used as input rainfall volume and in the third infiltration losses can be simulated with a pixel based adaptation of the event based green ampt infiltration model green and ampt 1911 samela et al 2020 reported the first applications of safer rain to two case studies in northern italy where it was benchmarked with a 2d hydrodynamic model the results showed an agreement of 53 for the flooded area in lignano sabbiadoro and accuracy values in the range of 0 96 0 99 with the accuracy correlation coefficient acc in rimini these applications highlighted the fast computation of flooded areas guaranteed by safer rain as the pre processing phase can be run once for a given study area in addition the results revealed the effectiveness in identifying pluvial hazard hotspots with spatially distributed rainfall and different land use scenarios though the simulations are hydrostatic and safer rain cannot consider flow dynamic effects 2 2 2 2d iber hydrodynamic model flood extensions and water depths are obtained for the four storms shown in table 2 by using numerical simulations with the iber code bladé et al 2014 cea et al 2007 iber is free software that solves the 2d depth averaged swes by using a finite volume scheme with the domain being discretized with both structured and unstructured triangular or quadrilateral elements cueto felgueroso et al 2019 santillán et al 2020 it solves the mass conservative equation eq 1 and the momentum balance equations in conservative form with source terms eq 2 and eq 3 1 h t h v x x h v y y 0 2 h v x t x h v x 2 y h v x v y g h z b x τ b x ρ 3 h v y t x h v x 2 y h v y v x g h z b x τ b y ρ where h is the water depth vi is the depth average velocity along the ith direction ρ is the water density zb is the height of the river bed τ b i is the bed friction term along the ith direction and g is the gravitational acceleration 2 2 3 green ampt infiltration equation the green ampt infiltration equation is considered in both safer rain and iber models they consider a pixel based adaptation of the event based green ampt infiltration model green and ampt 1911 that assumes that water infiltrates into relatively dry soil as a sharp wetting front computing the infiltration or rainfall loss with eq 4 4 f t k s 1 φ θ i ψ f t where ft is the infiltration rate or rainfall loss rate at time step t mm h ks is the saturated hydraulic conductivity mm h φ is the soil porosity θ i is the initial water content ψ is the wetting front suction mm and ft is the cumulative infiltration or rainfall loss at time step t mm water infiltrates in soils until the rainfall rate exceeds the soil limited infiltration rate therefore the time to ponding tp is considered specifically in a given pixel with homogeneous land cover and soil type characteristics the green ampt module applied in safer rain computes the overall infiltration depth f at the end of the rainfall event t d with eq 5 5 f i d i f d t p i t p k s d t p ψ δ θ ln ψ δ θ f ψ δ θ i t p i f d t p where i is rainfall intensity mm h δ θ is defined as φ θ i d is the storm duration and tp can be computed with eq 6 6 t p i f i k ψ δ θ k s i i k s i f i k land use and lithology layers have been used to identify soil types in the pamplona metropolitan area values of the green ampt parameters for each soil type have been obtained from literature based on previous experiences see e g chow et al 1988 3 methodology the methodology consists of six steps fig 4 the first step of the methodology consists of benchmarking the safer rain algorithm with the iber model in the barañáin and zizur mayor municipalities by using the precipitation fields for the four storms selected in table 2 the second involves identifying a set of real rainfall events that can generate pluvial flood events in pamplona extracted from the time series recorded at the four gauging stations considered in the study in the third the copula based stochastic generator of rainfall storms that maintain the statistical characteristics of the real storms identified in the previous step is described in the fourth the methodology to estimate t year water depths in each cell of the pamplona metropolitan area is offered in the fifth the methodology to compare the results of the stochastic methodology with the standard approach is presented in the sixth and last step the methodology to assess the uncertainty of t year water depth estimates is shown 3 1 benchmarking of safer rain the safer rain algorithm outputs are benchmarked in the barañáin and zizur mayor municipalities fig 2 with the 2d hydrodynamic iber model the 10 min qpe fields generated for the four storms included in table 2 are used as input data in both models there are no observations available about flood extents and water depths for such four pluvial events therefore the outputs of the 2d hydrodynamic model are considered as the gold standard truth for the sake of simplicity the urban drainage system is not considered in either model while iber is a 2d hydrodynamic model the safer rain tool is hydrostatic and can only simulate water depths and flood extents in depressions where water is stored therefore maximum water depths simulated by iber cannot be used in the comparison and the iber model is run until six hours after the end of the storm when hydrodynamic processes are expected to be smoothed water depth outputs of the iber model at six hours after the end of the storm are compared with outputs of the safer rain algorithm the errors between the models are quantified by using a set of objective functions bennett et al 2013 falter et al 2013 berkhahn et al 2019 samela et al 2020 a given cell is considered as flooded when its water depth is equal or greater than 10 cm first the bias and root mean square error rmse between water depths in the flooded area are calculated second the critical success index csi is used to quantify the agreement between the two models in terms of flood extents measuring the fraction of cells that were correctly modelled and penalizing both misses and false alarms the probability of detection hit rate or true positive rate rtp calculates the number of flooded agreements among the flooded areas in the gold standard truth simulation quantifying the model sensitivity the true negative rate rtn or specificity counts the number of non flooded agreements among the non flooded cells in the benchmark simulation the accuracy correlation coefficient acc quantifies the number of correct assessments between the simulations lastly the mathews correlation coefficient mcc measures the correlation between the binary classification in the simulations 7 csi tp tp f p f n 8 r tp tp tp f n 9 r tn tn tn f p 10 acc tn t p tn t p f n f p 11 mcc t p t n f p f n t p f p t p f n t n f p t n f n where tp true positive or hints is the number of cells where both models delineate them as flooded fp false positive or false alarms is the number of cells that safer rain simulates as flooded and iber simulates as non flooded tn true negative is the number of cells where both models agree in delineating them as non flooded and fn false negative or misses is the number of cells that safer rain simulates as non flooded and iber simulates as flooded csi can adopt values of between zero and one while a value of one points to a perfect simulation of safer rain a value of zero indicates that none of the flooded cells simulated by safer rain are flooded in the iber simulation mcc can provide values between 1 that would mean a complete disagreement between safer rain and iber and one that would point to a perfect simulation of safer rain a value of zero indicates that the safer rain output would not be better than a random prediction in addition the nash sutcliffe efficiency coefficient nse is calculated in each cell for the four pluvial flood events eq 12 12 nse 1 i 1 n wd s a f e r r a i n i wd iber i 2 i 1 n wd iber i w d iber 2 where wdsafer rain i is the water depth simulated by safer rain in the ith pluvial flood event wdiber i is the water depth simulated by iber in the ith pluvial flood event w d iber is the mean value of the water depths simulated by iber in the n flood events and n is the number of flood events considered in cells with a negative value of nse w d iber would be a better prediction than the safer rain output nse has been calculated in the cells with w d iber greater than 0 1 m as smaller values of w d iber can lead to an excessively small value of the denominator in eq 12 resulting in unusual high negative values of nse despite small differences between safer rain and iber simulations 3 2 extraction of rainfall events rainfall events that can drive pluvial floods in the metropolitan area of pamplona are identified in this study a peak over threshold pot approach is selected to consider a larger set of rainfall events that exceed a given threshold regardless of when they occurred in a pot series the arrival of rainfall events that exceed the threshold can be characterised by a poisson process madsen et al 1997 eq 13 13 p m k λ k k e k where p m k is the probability of having a number of exceedances m equal to an integer number k in a given period of t years k is an integer number equal or higher than zero k 0 1 2 and λ is the mean annual number of exceedances that is obtained with eq 14 14 λ m t where m is the number of exceedances over the threshold in a period of t years pluvial floods are generated by high intensity events in short durations consequently long events with high rainfall amounts and low intensities should not be considered in this analysis the pot threshold for pluvial floods is usually selected in terms of rainfall intensity the extreme rainfall alert era establishes a threshold that depends on the storm duration 30 mm in 1 h 40 mm in 3 h and 50 mm in 6 h hurford et al 2012 apel et al 2016 selected a threshold of 18 mm in 1 h blanc et al 2012 identified a decreasing mean rainfall intensity threshold with storm duration from around 3 mm h in 1 h to 2 mm h in 10 24 h mailhot et al 2013 selected a threshold between 2 and 10 mm in 30 min depending on the calendar day in this study a threshold of 10 mm in 30 min that corresponds to a rainfall intensity of 20 mm h is selected considering the characteristics of the storms that have generated pluvial floods in pamplona table 2 3 3 stochastic generation of rainfall events a stochastic methodology to generate synthetic rainfall events that maintain the statistical properties of the rainfall events identified in section 3 2 is presented storms are characterised by rainfall depth and duration which are the input data of the safer rain algorithm synthetic pairs of rainfall depth and storm duration can be generated by using bivariate copulas palla et al 2018 in this exploratory analysis for the sake of simplicity some simplifications have been introduced though safer rain can consider spatial distributions of rainfall and qpe fields have been considered in the benchmarking analysis to simulate four real pluvial flood events accurately precipitation fields have not been considered in the stochastic rainfall generator to reduce the computation times required for calibrating the rainfall generator simulating the precipitation fields and running the safer rain algorithm therefore the stochastic method will consider uniform rainfalls and durations in the pamplona metropolitan area and a longer set of 10 000 synthetic rainfall events will be examined in the study the procedure consists of two steps in the first the observed storms that exceed the threshold of 10 mm in 30 min at the four rainfall gauging stations are aggregated in a unique synthetic gauging site in order to obtain a larger time series and reduce estimate uncertainties this aggregation is justified considering that the distance between the rainfall gauging sites is small enough fig 2 to assume that they have similar characteristics in addition gauging sites are far enough to record varying storm rainfall depths intensities and durations for a given pluvial flood event given that such events are characterised by a high spatial variability in the second a bivariate approach is used to generate synthetic rainfall events in this case the first step consists of fitting the marginal distributions of rainfall depth and duration for the aggregated set of storms that exceed the threshold the magnitude of exceedances over a threshold are usually characterised by a generalised pareto gp distribution function madsen et al 1997 lang et al 1999 as it is a limiting distribution for a series of values that exceed a threshold and it has threshold stability martins and stedinger 2001 the parameters of the gp distribution are estimated by using the l moments method hosking and wallis 1993 1997 the second step of the bivariate approach consists of using a copula to characterise the dependence between storm rainfall depths and durations sklar 1959 in this study three archimedean copula families are considered clayton frank and gumbel in addition the gumbel copula belongs to the extreme value family the cramér von mises statistic sn is selected as the goodness of fit test to identify the copula that can represent better the dependence structure between rainfall depths and durations eq 15 as genest et al 2009 found that sn led to the best results for all copula models requena et al 2013 15 s n i j 1 n i c n i r ij n i 1 s ij n i 1 c i r ij n i 1 s ij n i 1 θ cop i 2 i 1 n where c n i u 1 u 2 is the empirical copula at site i rij and sij are the ranks associated with xij and yij θ cop i are the estimated parameters of the fitted parametric copula ci at site i and ni the record length at site i c n i u 1 u 2 is expressed by eq 16 16 c n i u 1 u 2 1 n i j 1 n i 1 r ij n i 1 u 1 s ij n i 1 u 2 u 1 u 2 0 1 i 1 n where 1 e is the indicator function of the set e valued 1 inside e or 0 outside and n is the observed record length in addition the p value associated with a given goodness of fit test is crucial in determining if a given copula model is suitable to characterise the dependence between two variables genest and rémillard 2008 therefore first the p value is used to reject a given copula model when it is smaller than 0 05 then the value of the sn statistic is used to select the best copula among the non rejected ones the lowest value of sn indicates the best fitting to observations salvadori and de michele 2011 consequently the selected copula should have the lowest value of sn with a p value greater than 0 05 lastly a long set of n synthetic storms characterised by rainfall duration pairs are generated randomly by using the best copula identified 3 4 delineation of pluvial flood hazard maps the series of n synthetic rainfall duration pairs generated by using the methodology described in section 3 3 are the safer rain input data considering infiltration processes with the aforementioned green ampt infiltration equation section 2 2 3 a constant initial soil water content was considered to avoid varying the green ampt parameter layers in the sequential simulation with safer rain the outputs of safer rain are processed to obtain a long set of n water depths in each cell then the t year water depth is estimated by using a frequency analysis in each cell flood hazard maps are usually obtained for a set of return periods that represent the expected exceedance probability in a given year however the n synthetic rainfall duration pairs represent the storms expected to exceed the threshold of 10 mm in 30 min with a mean value of λ exceedances per year eq 14 consequently the n water depths in each cell do not represent the annual maximum water depths expected in n years though the water depths that are expected to exceed a threshold in n λ years the relationship between the return period in an annual maximum series analysis ta and the return period in a pot series tp is given by eq 17 stedinger et al 1992 17 t p 1 l n 1 1 t a where tp is the return period in a pot series and ta is the return period in an ams clearly tp will be smaller than ta as more than one event is expected to exceed the threshold per year in a pot series 3 5 comparison with the standard approach the next step of the methodology consists of a comparison between the results of the stochastic methodology proposed in this study with the standard approach based on catchment response times used in practice the standard approach assumes that the t year pluvial flood in a given depression is generated by the t year storm with a duration equal to the time of concentration of the draining catchment to such a depression three depressions identified in the safer rain pre processing phase in the pamplona metropolitan area have been selected for the comparison a motorway depression at zizur mayor and two depressions in the echavacóiz neighbourhood and at soto aizoáin street at an industrial estate at orcoyen fig 5 the time of concentration of the draining catchment to each depression is estimated by using the velocity method that estimates travel times along the hydraulically most distant flow path nrcs 2010 then the design storms estimated for a duration equal to the time of concentration are used as input data of the safer rain algorithm comparing the outputs with the results of the stochastic methodology in a set of cells selected in each such depression 3 6 t year water depth estimate uncertainty assessment the last step of the methodology consists of quantifying the uncertainty of the t year water depth estimates obtained with the stochastic methodology proposed in this study in section 3 4 t year water depths are estimated from outputs of the safer rain algorithm by using a given set of n storm rainfall duration pairs generated by the copula fitted to the observations section 3 3 therefore such water depth quantiles could be conditioned by the set of rainfall duration pairs used in each case therefore the stochastic methodology has been replicated ten times generating ten sets of n storm rainfall duration pairs with the copula fitted to the observations the safer rain algorithm is run by using each set of n rainfall duration pairs as input data for each set of n rainfall duration pairs t year water depths are estimated in each cell of the pamplona metropolitan area by using the frequency analysis described in section 3 4 the uncertainty analysis has been limited to the draining catchment to the orcoyen depression delineated in section 3 5 fig 5 to reduce the computation times of the safer rain algorithm the stochastic methodology uncertainty is assessed by analysing the dispersion of the ten t year water depth estimates for each return period in five cells located in the orcoyen depression fig 12c 4 results this involves six steps first the results of the benchmarking activities to validate the outputs of the safer rain model with the hydrodynamic simulations of iber in four real storms are presented second the application of the stochastic methodology to generate synthetic rainfall events that maintain the statistic properties of real rainfall events which exceed a given threshold is offered third the results of pluvial flood hazard mapping in the pamplona metropolitan area are shown fourth the results obtained from the comparison between the stochastic methodology and the standard approach are presented fifth bivariate return period curves are estimated in a set of cells located in three depression of the pamplona metropolitan area sixth and lastly the uncertainty in t year water depth estimates is assessed in five cells located in the orcoyen depression 4 1 safer rain benchmarking in pamplona the outputs of the safer rain algorithm are compared with water depths simulated by the iber hydrodynamic model six hours after the end of the rainfall event in the barañáin and zizur mayor municipalities considering the precipitation fields in four real storms table 2 the first storm of 18 september 2019 is not considered in the barañáin municipality as it did not affect the area significantly fig 3 fig 6 shows the outputs of the safer rain algorithm and the iber 2d hydrodynamic model for the pluvial flood event of 20 july 2010 in the barañáin municipality such a figure confirms that safer rain correctly identifies the main depressions prone to flooding in pluvial flood events the results obtained from the other two storms in barañáin and the four storms in zizur mayor are included in the supplementary material fig s1 s6 fig s7 and fig s8 show the errors in water depths between the outputs of safer rain and iber most of the pixels show yellow and orange colours that point to a generally slightly lower estimate of water depths by safer rain it can be explained by the hydrostatic assumptions of safer rain that neglects backwater dynamic curves in overland flows fig 7 shows the nse values in barañáin and zizur mayor eq 12 the highest nse values blue areas are obtained in the main depressions pointing to a good prediction of safer rain in the areas prone to pluvial flooding where water is stored and water depths are greater on the contrary negative values of nse red areas are usually located in zones with small water depths fig 8 shows the histograms of nse in terms of the magnitude of w d iber wd in the figure legend in barañáin and zizur mayor in barañáin most of cells have high values of nse regardless the magnitude of water depths in addition 50 of cells with w d iber greater than 1 m have a nse value greater than 0 6 in zizur mayor the percentage of cells with high values of nse increases with water depths for small water depths with values of w d iber between 0 1 and 0 2 m 30 of cells have nse values between 0 6 and 1 and only 10 between 0 6 and 1 however for the greatest water depths with w d iber values greater than 1 m almost 80 of cells have nse values greater than 0 6 consequently safer rain supplies good predictions in depressions where greater water depths are expected though it supplies worse predictions in zones with small water depths the bias rmse csi rtp rtn acc and mcc objective functions are used to quantify the differences in water depths between iber and safer rain outputs table 3 in barañáin the bias is between 2 and 4 cm that points to small errors in water depths such values close to zero mean that safer rain compensates positive and negative errors and does not tend either to overestimate or underestimate water depths at barañáin the rmse values are greater between 19 and 25 cm as they do not account for compensation between positive and negative errors in terms of flood extents the csi values are 0 42 and 0 45 for the 2010 and 2020 events respectively and close to 0 3 for the second storm of 2019 indicating that safer rain represents correctly the 30 40 of the flood extent simulated by iber rtp is 0 5 for the events of 2010 and 2020 and 0 38 for the second storm of 2019 meaning that safer rain correctly simulates 50 and 38 respectively of the flooded cells simulated by iber these results may show good agreement between the models accounting for the limitations of the hydrostatic assumptions of safer rain regarding the hydrodynamic simulations of iber rtn and acc are close to one in the three events proving that the percentage of agreements in cells is high either flooded or non flooded and in line with that observed in samela et al 2020 lastly mcc values are between 0 45 and 0 63 meaning that safer rain provides a good prediction of the iber simulations at zizur mayor the bias is between 21 and 16 cm with a value close to zero in the second storm of 18 september 2019 the rmse values are between 0 26 and 0 28 in the 18 september 2019 flood event the event of 20 july 2020 provides a rmse value of 0 48 therefore at zizur mayor safer rain tends to underestimate water depths with greater rmse values for the highest rainfall magnitude csi values are between 0 24 and 0 38 which indicates that 24 38 of the flood extents simulated by iber are predicted correctly by safer rain rtp values are between 0 32 and 0 46 revealing agreement of 32 46 of safer rain with the flooded cells of iber as stated above these results provide agreement of safer rain and account for hydrostatic limitations rtn and acc are close to one in the four flood events considered at zizur mayor therefore the percentage of agreements in cells either flooded or non flooded is also high mcc values are equal to 0 52 and 0 57 in the 20 july 2010 and respectively the second storm of 18 september 2019 flood events the mcc values are smaller for the first storm of 18 september 2019 with a value close to 0 4 therefore it is clear that safer rain provides a good prediction of the iber simulations in terms of flood extents mainly in the flood events of 20 july 2010 and the second storm of 18 september 2019 comparing the results in pamplona spain with those obtained in rimini italy samela et al 2020 sensitivity rtp is up to 71 in rimini and up to 50 in pamplona however at pamplona specificity rtn is slightly greater with values of 99 in all the cases accuracy values obtained from acc are similar samela et al 2020 found that while sensitivity increases with storm precipitation accuracy decreases slightly the return periods of the four real storms considered in the benchmark analysis in pamplona are below 20 years therefore the smaller sensitivity values may be explained by smaller precipitations in the storms considered in pamplona in summary small csi values indicates that flood extents predicted by safer rain are usually smaller than expected however high nse values for the greatest water depths point to a good prediction in the main depressions with the highest water depths and the greatest flood hazards therefore the safer rain tool offers adequate results at barañáin and zizur mayor considering the hydrostatic assumptions of safer rain regarding the hydrodynamic behaviour of iber and the high reduction of computation times from hours to around one minute 4 2 stochastic generation of rainfall events the first step in stochastic generation of rainfall events consists of the extraction of real storms that exceed the threshold of 10 mm in 30 min cumulative precipitation in two consecutive time steps is considered in the 15 min rainfall time series and in three consecutive time steps in the 10 min rainfall time series fig s9 in a given storm several consecutive time steps could have a cumulative precipitation over the threshold therefore the beginning and ending time steps of storms were identified table 4 shows that the four rain gauging stations have similar values of λ indicating an analogous rainfall behaviour with a similar probability of storm arrival exceeding the threshold of 10 mm in 30 min at the four stations therefore the storms can be aggregated in a unique series of 138 rainfall events as rainfall gauging sites are close enough to have a similar rainfall behaviour though far enough to record different storms in pluvial flood events that are characterised by precipitations with high spatial variability the mean annual number of exceedances over the threshold for the aggregated data is calculated as the weighted mean of λ by the record length at each station obtaining a value of 2 03 exceedances per year the marginal distributions of storm rainfall depth and duration are obtained by using a gp distribution with the l moments method fig 9 the rainfall depth frequency curve has a lower bound of 10 mm that corresponds to the pot threshold guaranteeing that the synthetic rainfall events generated by the copula based approach will have a precipitation equal or higher to such a threshold and avoiding the generation of events with smaller precipitation that could not drive pluvial floods the duration frequency curve has a lower bound of 30 min which also agrees with the pot threshold and an upper bound close to 3 5 h therefore the synthetic rainfall events will have durations between 0 5 and 3 5 h avoiding the generation of excessively short or long storms that could not drive pluvial floods the clayton frank and gumbel family copulas were fitted to the aggregated 138 precipitation duration pairs fig 10 the sn goodness of fit test and p value were obtained by using 10 000 parametric bootstrap samples for each copula family table 5 the results show that the clayton and frank copulas have a p value lower than 0 05 indicating that they are unable to characterise the dependence relationship between precipitation and duration pairs as only the gumbel copula obtained a p value higher than 0 05 it cannot be rejected in addition the gumbel copula also leads to the lowest value of the sn statistic consequently the gumbel copula is the most appropriate when characterising the dependence structure of the 138 precipitation duration pairs of real rainfall events that can drive pluvial floods in the pamplona metropolitan area 4 3 pluvial flood hazard maps in pamplona a set of 10 000 random precipitation duration pairs was generated with the gumbel copula fitted in section 4 2 obtaining short durations between 0 5 and 3 5 h and rainfall depths equal or greater than 10 mm required to generate pluvial floods in pamplona fig 10c the safer rain algorithm was run with the 10 000 synthetic storms and considered infiltration processes with the green ampt model the safer rain algorithm is embedded in the saferplaces platform in pamplona https platform saferplaces co however a jupiter notebook script in google colaboratory was used to run such a long number of simulations iteratively the safer rain pre process was run once in the pamplona metropolitan area for 121 40 s the flooding phases i e the filling and spilling processes were simulated in around 45 s for each event supplying water depths in each cell in a given cell the water depth distribution can include several zeros that represent the storms that do not generate enough water accumulation in the depression to flood the cell in addition it can show an upper bound that represents 100 filling water depth in the depression therefore no distribution function could fit such data adequately in each cell t year water depths were estimated empirically from the sorted series of 10 000 water depths by using the weibull plotting position formula and considering the return periods in a pot analysis tp eq 17 seven return periods are considered two five 10 25 50 100 and 500 years fig 11 shows the pluvial flood hazard maps for the two 10 100 and 500 year return periods the hazard maps for other three return periods five 25 and 50 years are included in the supplementary material fig s10 s12 the main pluvial flood hotspots in pamplona could be identified from such maps fig 11b i the motorway depression at zizur mayor ii the soto aizoáin street at the industrial estate of orcoyen driven by a building with three industrial units that obstructs a former stream iii echavacóiz neighbourhood driven by a levee to protect the area from river elorz fluvial floods iv depression at the miluze and ermitagaña crossroads v railway underpass at san jorge vi a depression at the sadar street vii a depression between txantrea and burlada and viii a depression at the burlada sports centre driven by a levee to protect the area from river arga fluvial floods 4 4 comparison with the standard approach the results of the stochastic methodology with safer rain are compared with the standard approach that considers the t year storm precipitation for a duration equal to the draining catchment response time a time of concentration of 25 min was estimated at the motorway depression at zizur mayor 50 min at the echavacóiz neighbourhood and 75 min at the orcoyen industrial area by using the velocity method that estimates travel times along the hydraulically most distant flow path nrcs 2010 table 6 includes the t year storm precipitations considered in each depression a set of cells have been selected in each zone to compare the distribution of 10 000 water depths obtained by using the stochastic methodology with the standard approach results fig 12 the cells for the comparison have been selected scattered in main depressions where water is stored to characterise adequately the filling processes in this case 10 return periods are used for the comparison two five 10 20 25 30 50 100 200 and 500 years fig 13 shows the results for the two cells considered in the motorway depression at zizur mayor the distribution of water depths obtained with the stochastic methodology adequately fits the water depths obtained by the standard approach both approaches agree about when the depression begins to fill in cell 2 at a return period of below two years however in cell 1 the filling process begins for the 10 year return period for the stochastic methodology and between two and five years for the standard approach in the filling process water depths are similar in each approach though water depths are greater for the standard approach for low return periods the differences in water depths between the approaches are reduced as the return period increases in cell 1 in cell 2 water depths for the stochastic methodology are slightly greater for the highest return periods in the echavacóiz neighbourhood both methods provide agreement regarding the beginning of the filling process of between two and five years in cell 1 and five and ten years in cell 3 fig 14 in cell 2 the filling process begins for the two year return period for the standard approach and between two and five years for the stochastic methodology in this case the stochastic methodology simulates the filling process faster than the standard approach in the three cells as the return period for the 100 depression filling is smaller for the stochastic methodology however distributions show a similar shape for each methodology and water depth differences for given return periods are small in the range of 10 cm in the orcoyen commercial area fig 15 water depth distributions show similar shapes and point to an adequate simulation of the depression filling processes in each case though the standard approach estimates slightly greater water depths the filling process in the depression begins at lower return periods in the standard approach the greatest differences between water depths for given return periods are for the lowest return periods with the differences becoming smaller for high return periods 4 5 bivariate pluvial flood return periods in addition the stochastic methodology can supply information about the relationship between the variables that characterise storms and pluvial flood water depths in a given depression mediero et al 2010 specifically the set of storms that generate water depths higher than the t year water depths estimated empirically in section 4 3 can be identified in a given depression obtaining the bivariate return period curves of pluvial floods in the three depressions selected as case studies in section 3 5 fig 16 shows that the t year water depth in a given depression is not driven by a unique storm such as the design storm rather than by a set of storms with different combinations of precipitation and storm duration for example the five year water depth in cell 5 of the orcoyen depression boundary between green and blue colours in fig 16c is generated by a set of storms 16 6 mm in 30 min 23 3 mm in one hour 27 5 mm in 1 5 h and 30 5 mm in two hours among others consequently the bivariate return period curve for a given return period t identifies the possible combinations of precipitation and storm duration that generate the t year pluvial flood bivariate return period curves show that precipitation increases with storm duration for a given return period though the relationship is not linear such an increasing relationship is related to both the mean rainfall intensity and precipitation losses generated by infiltration processes in a given storm for a given bivariate return period curve while the precipitation increases with storm duration the mean rainfall intensity decreases with storm duration this indicates that a shorter storm with a higher mean rainfall intensity can generate the same water depth than a longer storm with a smaller mean rainfall intensity in addition rainfall water volumes increase with storm precipitation with greater water depths being expected however for a given storm precipitation water runoff volume decreases as storm duration increases because infiltration processes give place to greater rainfall losses higher rainfall losses are generated in rural catchments with high infiltration rates nevertheless smaller rainfall losses are expected in urban catchments where soils are mostly sealed and infiltration rates are lower therefore bivariate return period curves will have steeper slopes in rural catchments than in urban catchments furthermore return period curves could even be horizontal in a catchment with soils completely sealed where no rainfall losses will be generated in the three depressions selected in section 3 5 the draining catchment at the orcoyen depression is mostly rural the catchment in echavacóiz is a mixture of rural and urban areas and the zizur mayor motorway depression has a draining catchment that is mostly urban therefore the zizur mayor depression will have the mildest bivariate return period curve slope as its draining catchment is mostly urban and rainfall losses are smaller smoothing the increasing relationship between precipitation and storm duration for a given bivariate return period curve however the orcoyen depression will show the steepest bivariate return period curve as its mostly rural draining catchment increases rainfall losses 4 6 uncertainty in t year water depth estimates finally the uncertainty in t year water depth estimates is assessed ten sets of 10 000 storm rainfall duration pairs are generated with the gumbel copula fitted to the observations in section 4 2 the safer rain algorithm is run with the ten sets of rainfall duration pairs in the draining catchment to the orcoyen depression fig 5 five cells are selected for assessing the uncertainty of t year water depth estimates fig 12c by plotting the dispersion of the ten t year water depth estimates in each cell by using boxplots fig 17 fig 17 shows that uncertainty increases with decreasing water depths boxplots are greater in cells 1 and 4 where water depths are smaller in addition uncertainty increases with the return period in cells 2 3 and 5 the greatest uncertainties are found for the highest return periods furthermore in cells 2 3 and 5 high uncertainties are also found for the lowest return periods with smaller water depths this agrees with the findings of the benchmarking activities section 4 1 where higher errors in the safer rain algorithm outputs were found when water depths are small therefore the most reliable estimates are found for return periods between 10 and 100 years where almost all the ten t year water depth estimates agree in addition for high return periods the uncertainty increases for small water depths therefore the stochastic methodology supplies more reliable results in the lowest cells of the main depressions where greater water depths are expected 5 discussion it can be argued that the standard approach used for pluvial flood hazard mapping in practice and based on design storms has certain drawbacks one could be that design storms may not be feasible when considering the real storms that can arrive at the area studied another would be that such an approach assumes that the t year storm generates the t year pluvial flood event and lastly another would be that empirical formulae used to estimate catchment response times and durations of design storms present significant uncertainties when in practice the stochastic methodology proposed in this study considers the return period concept as an exceeding water depth threshold identified from a large set of 10 000 storms with frequencies rainfall depths and durations similar to the observations at rain gauging stations therefore such a methodology improves the standard approach as it estimates t year water depths regardless of catchment response times avoiding the uncertainties by using empirical formulae and design storms that could not be realistic in addition the stochastic methodology shows that the t year water depth in a given depression can be generated by a set of storms with varying combinations of storm precipitation and duration instead of by a single design storm consequently the stochastic methodology can be considered sounder that the standard approach moreover the stochastic methodology can easily be applied to multi depression urban areas as it does not need to estimate a design hyetograph in each depression in terms of catchment response times however a drawback of the proposed methodology could be the need to repeat the simulations when a mitigation measure is applied changing flood hazards nevertheless the dtm can be clipped focusing in the area where flood hazards change with the new mitigation measure reducing the computation time of the new simulations considerably the benchmarking results between the safer rain algorithm outputs and the iber 2d hydrodynamic simulations show that safer rain supplies adequate results in terms of water depths considering its hydrostatic assumptions safer rain supplies better predictions of water depths in depressions prone to pluvial flooding where the greatest water depths are expected in terms of both water depth magnitude and uncertainty however safer rain shows some limitations to reproduce flood hazards in zones where small water depths are expected consequently t year water depth estimates in lower points of the main depression are more reliable than t year estimates in areas with small water depths where such estimates should be taken with caution in addition the exploratory analysis presented in this study has considered some simplifying assumptions the first is that synthetic uniform spatial rainfall events have been considered though the safer rain tool can consider precipitation fields with heterogeneous spatial distribution such a simplification avoids the complexity and computation times required for calibrating a spatially distributed rainfall generator and for simulating safer rain with precipitation fields for such a long set of storms the second is that a constant initial soil water content in the green ampt model has been considered and the urban drainage system in the pamplona metropolitan area could not be considered as the current version of the saferplaces platform does not include such functionalities yet lastly the main limitation of the stochastic methodology proposed arises from the hydrostatic behaviour of the safer rain algorithm that cannot consider overland flow dynamics however the reduction in computation times obtained from such an assumption compared with 2d hydrodynamic models is from several hours to around one minute allowing pluvial flood hazard maps at multi depression cities or municipalities at spatial scales to be generated in reasonable times such a stochastic approach would be unaffordable with 2d hydrodynamic models moreover not only t year water depths are obtained in each cell but also the complete probability distribution of water depths and bivariate return period curves in terms of precipitation and storm duration 6 conclusions a stochastic methodology to delineate pluvial flood hazard maps in urban areas by using the fast processing and dem based safer rain algorithm is presented the methodology was applied to the metropolitan area of pamplona in spain 68 26 km2 the safer rain algorithm was benchmarked with the iber 2d hydrodynamic model at the barañáin and zizur mayor municipalities considering 10 min precipitation qpe fields for four real storms that generated pluvial floods in the last years the benchmarking results showed that the safer rain algorithm offers adequate results in terms of water depths considering its hydrostatic simplifications in addition safer rain offers better predictions in depressions where greater water depths are expected than in flatter areas with smaller water depths a set of 138 real storms in which each one exceeds a threshold of 10 mm in 30 min was identified from the observations at four rainfall gauging stations the gumbel copula was selected as the most suitable to represent the dependence between the 138 real storm precipitation duration pairs a large set of 10 000 synthetic storms with similar statistical characteristics to the real storms was generated randomly and used as input data in the safer rain algorithm considering infiltration processes with the green ampt model the safer rain pre process was run once in 121 40 s and the flooding phase was simulated in around 45 s for each event therefore the computation time was reduced from some hours to around one minute for each event regarding 2d hydrodynamic modelling in each cell t year water depths were estimated empirically from the series of 10 000 simulated water depths pluvial flood hazard maps were developed in pamplona for 10 return periods in the range from two to 500 years identifying the main pluvial flood hotspots in the urban area the stochastic methodology was compared with the standard approach based on t year design storms several points at three depressions in pamplona were selected for the comparison concluding that both methodologies supply similar results in the control points however the stochastic methodology proposed in this paper addresses the drawbacks of the standard approach first the stochastic methodology avoids the uncertainties associated with catchment response time estimates with empirical formulae second the stochastic methodology does not need to consider varying storm durations in terms of the catchment response time for each depression and can be easily implemented in multi depression areas third the assumption that states that the t year storm generates the t year pluvial flood is improved by obtaining the complete probability distribution of water depths in each cell in addition bivariate return periods can be estimated showing that a given t year pluvial flood can be generated by a set of storms with varying combinations of precipitation and duration bivariate return period curves show that precipitation increases with storm duration for a given return period in a given depression the slope exhibited by such curves will depend on infiltration rates at its draining catchment higher infiltration rates will generate greater rainfall losses and smaller runoff volumes leading to a higher increase in precipitation with storm duration therefore rural catchments will have steeper bivariate return period curves than urban catchments in this first exploratory analysis spatially uniform precipitations have been considered the results are limited by the hydrostatic assumption of the safer rain tool identifying water depths only in depressions where runoff water volumes tend to accumulate however the stochastic methodology proposed in this study benefits from the reduced computation times required by the fast processing dem based safer rain algorithm overcoming the limitations associated with the high computation times required by two dimensional hydrodynamic models therefore the methodology proposed is a promising technique to develop consistent pluvial flood hazard maps in urban areas at large scale and multi depression cities or municipalities credit authorship contribution statement luis mediero conceptualization methodology data curation writing original draft writing review editing visualization investigation supervision project administration enrique soriano methodology data curation visualization investigation peio oria conceptualization methodology data curation writing original draft writing review editing visualization investigation stefano bagli software supervision project administration attilio castellarin conceptualization methodology software writing original draft writing review editing investigation supervision project administration luis garrote conceptualization methodology investigation supervision paolo mazzoli software project administration jaroslav mysiak methodology investigation project administration stefania pasetti software project administration simone persiano methodology software writing original draft writing review editing investigation david santillán data curation writing original draft writing review editing investigation kai schröter methodology investigation project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the eit climate kic demonstrator project saferplaces improved assessment of pluvial fluvial and coastal flood hazards and risks in european cities as a mean to build safer and resilient communities task id eit 2 2 37 200246 p401 1b the saferplaces project aims to advice identification and assessment of flood risk mitigation measures and plans inform climate adaptation and disaster risk reduction strategies and help to foster multi stakeholder agreements and partnership for resilience building it has employed innovative climate hydrological and hydraulic topographic and economic modelling techniques to develop the saferplaces platform https platform saferplaces co in order to assess pluvial fluvial and coastal flood hazard and risk in urban environments under current and future climates the authors would like to acknowledge the spanish instituto geográfico nacional ign for supplying the 2 m dem the saih real time system of the river ebro basin authority the spanish agencia estatal de meteorología aemet the regional government of navarre and the universidad pública de navarra upna for supplying the precipitation data and the consorcio de compensación de seguros for supplying a database of direct flood losses in pamplona used to identify the main areas prone to pluvial floods the authors also express their gratitude to andrew selby for the help and guidance received with regard to language usage and expression appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 127649 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3499,cable bacteria have been discovered in various marine and freshwater habitats and their unique metabolism named electrogenic sulfide oxidation e sox is of great ecological significance to aquatic ecosystems however the environmental factors that determining the dynamics and abundance of cable bacteria in freshwater sediments are not well understood in this study we examined the activity and growth of cable bacteria in response to the change of oxygen availability in freshwater sediments the metabolic activity of cable bacteria was quantified by microsensor profiling while their abundance was determined by fluorescence in situ hybridization and 16s rrna gene sequencing the increase of oxygen availability greatly promoted the metabolic activity and proliferation of cable bacteria as reflected by a higher rate of e sox and a larger population size as affected by the change of sulfide availability and oxygen demand with the proliferation of cable bacteria these promoting effects were more pronounced at later development stage moreover the elevation of oxygen availability drove the downwards growth of cable bacteria and the increased inventories of dissolved sulfate in deeper sediment layers which may expand the influences of cable bacteria on sediment biogeochemical cycling on the vertical scale our results contribute to further understanding of the correlations between oxygen availability and growth dynamics of cable bacteria in natural environments keywords cable bacteria oxygen availability metabolic activity abundance and distribution freshwater sediment 1 introduction cable bacteria are centimeter long filamentous sulfide oxidizing bacteria belonging to the desulfobulbacea family trojan et al 2016 using their filaments as electron conductors cable bacteria link oxygen reduction at sediment water interface to sulfide oxidation in anoxic regions nielsen et al 2010 pfeffer et al 2012 meysman et al 2019 the combination of these two separated reactions is known as electrogenic sulfur oxidation e sox pfeffer et al 2012 malkin et al 2014 the process of e sox created a distinct suboxic zone without o2 and h2s an alkalization nearby the bottom water and an intense acidification in deeper anoxic sediment schauer et al 2014 meysman et al 2015 these unique geochemical imprints arecommonly used to track the development of cable bacteria in both laboratory and field studies van de velde et al 2016 burdorf et al 2018 marzocchi et al 2018 the metabolism of cable bacteria can greatly accelerate the biogeochemical cycling in sediments especially for iron rao et al 2016 sulu gambari et al 2016a sulfur sandfeld et al 2020 liu et al 2021 carbon müller et al 2016 scholz et al 2020 nitrogen kessler et al 2019 marzocchi et al 2021 and trace elements van de velde et al 2017 the intensified cycling of these elements can induce a series of additional biogeochemical consequences that include the prevention of benthic sulfide release seitaj et al 2015 hermans et al 2019 martin et al 2017 the promotion of sedimentary phosphorus sequestration sulu gambari et al 2016b hermans et al 2021 and the suppression of methane production in sediments scholz et al 2020 this may contribute to the protection of benthicorganisms improvement of water quality and regulation of trophic status seitaj et al 2015 sulu gambari et al 2016b and have implications and greenhousegasreduction in aquatic ecosystems scholz et al 2020 scholz et al 2021 given the great ecological significance of cable bacteria to local environments it is important to explore the environmental factors that determining their development and distribution in natural environments cable bacteria have been discovered in diverse habitats across marine and freshwater environments risgaard petersen et al 2015 burdorf et al 2017 scholz et al 2021 despite ubiquitous in the environment their ecological niche has been documented to be strictly constrained by several key factors such as the availability of electron donors e g fes or free sulfide hermans et al 2019 xu et al 2022 and acceptors e g oxygen or nitrate marzocchi et al 2014 2018 oxygen as the preferred electron acceptor for the metabolic activity of cable bacteria can efficiently harvest the electrons from sulfide oxidation marzocchi et al 2014 phylogenetic analysis suggested cable bacteria from marine and freshwater habitats are belong to different lineage risgaard petersen et al 2015 trojan et al 2016 dam et al 2021 which may have different physiological properties and thus response to environmental factors differently laboratory evidence suggested the increasing of oxygen levels in overlying water can significantly promote the proliferation of cable bacteria and enhance their influences on sedimentary geochemistry burdorf et al 2018 liu et al 2021 e sox activity was immediately disappeared when there was very low or without oxygen risgaard petersen et al 2015 field studies also suggested the population dynamics of cable bacteria were closely related to the temporal variations of bottom water oxygen levels in seasonal hypoxia coastal environments no cable bacteria was observed in persistent anoxic environments marzocchi et al 2018 hermans et al 2019 hermans et al 2021 more recently the occurrence of cable bacteria has been also reported in the root systems of aquatic plants martin et al 2019 scholz et al 2019 scholz et al 2021 and the tubes and burrows sediments of intertidal macro benthos aller et al 2019 li et al 2020 which was directly connected with introducing of oxygen to these microenvironments however most of these previous studies are focus on marine sediments the potential effects of oxygen availability on the development and abundance of cable bacteria in freshwater sediments are seldom explored the present study was performed to investigate how the oxygen availability in overlying water may affect the growth of cable bacteria in freshwater sediments growth dynamics of cable bacteria were monitored using a time series sediment incubation experiment where sediment cores were subjected to a range of bottom water redox conditions from anoxic to fully re oxygenated which were kept constant over time cable bacteria activity was examined by high resolution depth profiling of o2 σh2s and ph in combined with the pore water and sediment geochemical analysis cable filament densities and its depth distribution in sediment were quantified by fluorescence in situ hybridization fish and the relative abundance of cable bacteria was estimated by the high throughput 16s rrna gene sequencing our study will provide valuable information on the understanding of the influence oxygen availability on the distribution and growth dynamics of cable bacteria in natural environments 2 materials and methods 2 1 sediment sampling and preparation surface sediments in the depth of 0 5 cm were collected from a natural freshwater habitat of cable bacteria xu et al 2021 located in the wenyu river 40 07 00 36 n 116 16 27 57 e beijing china in october 2019 the samples of overlying water 30 l were simultaneously collected and filtered by polyether sulfone pes membrane 0 45 µm for further use the physiochemical properties of the sediment and the overlying water are summarized in table 1 before experiment the sediments were pretreated for incubation following the procedures as described in previous studies rao et al 2014 schauer et al 2014 in brief the sediment was firstly asphyxiated and sieved by a mesh 0 5 mm to exclude the potential disturbance of macrofauna after that the sediments were completely homogenized to ensure consistent initial conditions for incubation experiments 2 2 sediment incubation and sampling procedures the prepared sediments were packed into a total of 48 glass beakers diameter 6 0 cm height 13 5 cm as sediment cores and then these packed glass beakers were divided into four groups 12 replicated cores for each group and transferred into different plastic aquaria length 28 5 cm width 20 cm height 17 cm the filtered overlying water was then gently added to these aquaria to construct the sediment water incubation systems dissolved oxygen in the overlying water of these incubation systems were regulated into different levels by controlling the aerationprocess specifically the overlying water in the first incubation system was flushed with pure nitrogen for 2 h and then the incubation aquaria was sealed with three layers of parafilm sigma aldrich usa and tightly fixed by tapes to prevent air permeation referred to as the anoxic an treatment the second incubation aquaria was also sealed with three layers of parafilm in which 20 pinholes with a dimeter of 0 03 cm were pierced for gas exchange with air and the fluctuations of oxygen levels were controlled by flushing nitrogen referred to as the partially re oxygenated pr treatment the plastic pipeline of nitrogen diameter 0 4 cm and o2 sensor diameter 0 5 cm were inserted into the overlaying water through the holes the third incubation system was kept in ambient condition and referred to as the natural re oxygenated nr treatment the overlying water in the fourth aquaria was continuously and gently aerated by a pump to achieve a saturated state of dissolve oxygen referred to as the fully re oxygenated fr treatment except for a slight aeration induced stirring in the fr and pr treatment the other two treatments were incubated under stagnant condition the corresponding dissolved oxygen contents in overlaying water from the four treatments are 1 88 0 57 μmol l 1 an 153 28 7 μmol l 1 pr 276 8 2 μmol l 1 nr and 307 10 9 μmol l 1 fr respectively these aquaria were incubated at 15 c in a darkened chamber for a period of 50 days during the incubation period the dissolved oxygen concentrations in the overlying water of the fr nr and pr treatment were monitored every 2 4 days by o2 microsensor unisense a s denmark while for the anoxic treatment this monitoring was only conducted when sampling to avoid the disturbance of the anaerobic conditions fig s1 in addition 20 50 ml deionized water was gently added to replenish the evaporated water from the incubation aquaria every 2 days except for the sealed anoxic aquaria which has no significant water evaporation before replenishing the deionized water was aerated with air or nitrogen to keep their oxygen concentration comparable to that in the overlaying water of the aquaria the addition of deionized water did not induce a significant impact on the overall oxygen state of the incubation system depth profiles of σh2s ph o2 in pore water and the depth distribution of cable bacteria abundance in sediment were determined at 0 12 28 and 50 days to track the development of cable bacteria three parallel cores were done for each treatment after micro electrode measurements the corresponding sediment cores of each treatment were removed from the incubator their top 5 cm sediment was collected with a cutoff syringe diameter 3 cm and then sliced at 0 5 cm thicknessresolution 0 5 ml sediment of each sediment slice was then fixed with the same volume of 96 ethanol and frozen at 20 c for the fluorescence in situ hybridization fish analysis at the end of the incubation pore water samples of each sediment slice were extracted through centrifugation of the sediment at 4500 g 15 min for measuring the depth profiles of so2 4 no 3 and fe2 the obtained pore water was filtered through 0 45 μm filter mixed cellulose ester membrane and divided into two subsamples one subsample was stored at 20 c for the measurements of so2 4 and no 3 and the other subsample was acidified with 37 hcl 10 μl ml and then stored at 20 c for fe2 analysis sediment slicing and pore water extraction were conducted in an anoxic box continuously purged with nitrogen to prevent the oxidation of fe2 in addition the top 2 cm sediment about 0 1g from each treatment was sampled at the end of incubation the subsamples from three parallel sediment cores were completely mixed and stored at 80 c for the following 16s rrna gene sequencing 2 3 microsensor measurements high resolution depth profiles of o2 σh2s and ph were detected with commercial microsensors unisense a s denmark before each monitoring micro electrodes were calibrated according to the standard operating procedures given by the manufacturer in brief two standard points were used for oxygen sensor calibration air saturated tap water with 100 o2 saturation and the anoxic sediment with 0 o2 saturation a four point standard curve with a concentration gradient from 0 to 20 µm zinc sulfide solutions was used for the calibration of h2s sensor the standard stock solution of zinc sulfide was purchased from shenyang zhixinyun technology co ltd china with a certified concentration of 132 2 μg ml 1 the ph sensor was calibrated using nist certified standard buffer solutions of ph 4 0 7 0 and 10 0 ncl of wisconsin inc usa microprofiles of o2 σh2s and ph were recorded in the top 3 cm sediment at a depth resolution of 100 400 µm three replicate profiles were monitored for each sediment core at each time point and three parallel cores were measured for each treatment the depth profiles of total sulfide concentration σh2s h2s hs s2 were derived from the corresponding h2s contents and ph values measured at each depth jeroschewski et al 1996 the width of suboxic zone δph and cathodic oxygen consumption coc rate were calculated based on the depth profiles of o2 ph and σh2s using the average values in brief the width of suboxic zone in sediments was defined as the depth horizon between the sulfide appearance depth sad σh2s 0 3 μm and the oxygen penetration depth opd o2 1 μm seitaj et al 2015 due to the low sulfide concentration in pore water of the sediment we used the sad was defined as the depth at which σh2s concentration was more than 0 3 µm in this study δph represented as the amplitude of ph change in the pore water which was quantified as the difference between the highest ph value in the oxic zone and the lowest ph value in the suboxic zone burdorf et al 2018 the calculation of coc assumes that except the activity of cable bacteria other reactions are not contributed to the change of alkalinity in surface sediments the coc rate was derived from cathodic proton consumption cpc using the following equitation coc cpc 4 the cpc was calculated as the sum of the upward and downward alkalinity fluxes nearby the ph peak in oxic zone as described by previous studies risgaard petersen et al 2012 malkin et al 2014 schauer et al 2014 müller et al 2016 in brief alkalinity flux j ta was calculated using the following formula j ta j hc o 3 2j c o 3 2 j oh j h the specific assumptions of the cpc calculation and the detailed calculation procedures are provided in our previous studies xu et al 2021 xu et al 2022 2 4 chemical analysis dissolved oxygen ph conductivity and temperature in the overlying water were monitored by an automatic water quality detector ysi professional plus usa sediment porosity was measured following the methods as described by burdige 2007 total organic carbon toc in sediment was determined via an elemental analyzer multin c2100 elementar germany so2 4 and no 3 concentrations in the pore water were determined by ion chromatography ics3000 dionex usa and dissolved fe2 concentrations were determined by spectrophotometry method apha 2005 acid volatile sulfide avs concentrations in sediments were determined using a cold distillation procedure burton et al 2011 sulfide in avs was converted into h2s by 9 m hcl which was then trapped in alkaline zinc acetate solutions the concentrations of sulfide in the fixed solutions were determined by methylene blue spectrophotometry reese et al 2011 2 5 fish analysis and 16s rrna gene sequencing fish analysis was conducted using the standard protocols as reported in previous studies lücker et al 2007 müller et al 2016 briefly the filaments of cable bacteria were identified by the desulfobulbaceae specific oligonucleotide probe dsb706 and the density of cable filaments were observed and counted through epifluorescence microscopy the depth integrated density of bacteria filaments m cm 2 and the vertical density at each sediment depth m cm 3 were calculated according to the method described previously malkin et al 2014 schauer et al 2014 in addition 16s rrna gene sequencing was applied to determine the relative abundance of cable bacteria which was estimated as the ratios of the number of reads of cable bacteria species identified in this study to that of the total bacteria geelhoed et al 2020 the primer pair of 338f 5 actcctacggga ggcagcag 3 and 806r 5 ggactachvgggtwtctaat 3 targeting the region v3 v4 of bacterial 16s rrna gene was used for pcr amplification the details of dna extraction pcr amplification and bioinformatics data processing are provided in detail by xu et al 2021 xu et al 2022 the 16s rrna amplicon data have been uploaded to the database of national center for biotechnology information ncbi the accession number is prjna807585 2 6 statistical analysis statistical analyses were carried out with spss 21 0 one way analysis of variation anova followed by duncan s test was used to compare the differences in the parameters related to e sox activity porewater and sediment geochemistry and cable bacteria abundance p 0 05 was accepted as statistically significant 3 results 3 1 microsenser profiling of h2s o2 and ph as shown in fig 1 the oxygen penetration depth opd o2 1 μm was significantly increased with oxygen contents in overlying water one way anova duncan s test p 0 05 specifically the mean values of opd in the sediments were 2 13 mm 1 83 mm and 0 99 mm for the fr nr and pr treatment respectively at the beginning of incubation day 0 the sulfide appearance depth sad was similar in all treatments in the depth range of 2 6 3 4 mm at day 12 microsensor profiling showed a clear separation of σh2s and o2 accompanied by the formation of a suboxic zone in width of 4 8 6 00 mm in the sediments from these three treatments simultaneously the representative ph extremes were observed with a ph maxima nearby the sediment water interface and minima near the sad indicating the metabolic activity of cable bacteria in the sediments correspondingly the sulfide appearance depth sad in sediments moves downwards to the deeper sediments within the incubation period the sad moved from 3 00 to 3 40 to 3 0 20 6 mm 3 40 to 16 4 mm and 3 20 to 11 9 mm in the fr nr and pr treatment respectively however these typical geochemical imprints were not observed in sediments from the anoxic an treatment during the incubation period indicating the absence of e sox activity the cathodic oxygen consumption coc rate the magnitude of ph change δph and the width of suboxic zone are further calculated to assess the intensity of e sox activity in each treatment fig 2 in general the temporal patterns of coc rate were similar in the three treatments pr nr and fr that have cable bacteria activity which firstly increased and reach the peak value at day 28 and then decreased in the rest incubation period in particular great difference of the coc rates among different treatments was observed at 12 d and 28 d fig 2a specifically the coc rate of the fr 0 71 m 2 d 1 for 12 d and 1 04 mmol m 2 d 1 for 28 d and nr treatment 0 64 mmol m 2 d 1 for 12 d and 0 93 mmol m 2 d 1 for 28 d were significantly higher than those of the pr treatment 0 35 mmol m 2 d 1 for 12 d and 0 72 mmol m 2 d 1 for 28 d one way anova duncan s test p 0 05 similar change patterns were also observed in δph fig 2b and the width of suboxic zone fig 2c in these three treatments both these two indicators increased first until reach the maximum at day 28 and then stabilized in the following incubation time δph and the width of suboxic zone were significantly increased with the overlying water oxygen supply and the variations among different treatments were more obvious in the later incubation stages no significant difference of δph in the fr and nr treatments but both of them were significantly higher than that of the pr treatment one way anova duncan s test p 0 05 within the incubation period 3 2 pore water and sediment geochemistry depth profiles of dissolved fe2 so2 4 and acid volatile sulfide avs concentrations were determined in each treatment at day 50 fig 3 the surface layers of sediment 0 0 5 cm showed very low dissolved fe2 concentrations 3 19 3 81 µm in all treatments in three re oxygenated treatments pr nr fr fe2 showed a significant enrichment in the pore water of the suboxic zone 0 5 2 0 cm with distinct peaks in the depth of 1 0 1 5 cm and then decreased in deeper sediments fig 3a the maximum fe2 concentrations greatly increased with oxygen supply levels followed the order of pr 196 45 µm nr 297 83 µm fr 385 63 µm in contrast dissolved fe2 concentrations in sediment from anoxic treatment an were much lower than the other three treatments and maintained at relatively stable levels 18 0 26 3 µm in the anoxic layers comparing with the anoxic treatment so2 4 concentrations were increased in all sediment layers in three re oxygenated treatments and reach a maximum below the suboxic zone in the depth of 20 25 mm fig 3b the inventories of so2 4 in sediments 0 5 cm from the fr 466 mmol m 2 nr 452 mmol m 2 and pr 366 mmol m 2 treatments were 8 9 fold 8 6 fold and 7 0 fold higher than the an treatment 52 3 mmol m 2 respectively table 2 in accordancewith the enrichment of dissolved fe2 and so2 4 in pore water avs contents were greatly depleted in the surface sediments especially in the fully re oxygenated treatment fig 3c specifically the reduced avs inventory in the top 5 cm sediment of the fr 0 216 mmol m 2 treatment was about 1 1 fold and 1 4 fold as that of the nr 0 198 mmol m 2 and pr 0 152 mmol m 2 treatments respectively table 2 comparing to these three treatments no depletion of avs was observed in the sediments from the anoxic treatment table 3 3 3 cable bacteria abundance and depth distribution in the three re oxygenated treatments the integrated depth density of cable filaments was greatly increased within the incubation period fig 4 a for instance the density of cable bacteria increased by more than 10 folds from day 12 18 2 m cm 2 to day 50 190 m cm 2 in the fr treatments similar results are also observed in the pr and nr treatments comparison between treatments cable bacteria density was remarkably increased with oxygen availability and that was more prominent in the later incubation stage for instance the integrated density of cable filaments in sediments from the fr treatment at day 12 and day 50 was 1 8 fold and 2 9 fold as that of the pr treatment respectively cable bacteria density was comparable in sediments between the fr and nr treatments within the first 28 days but was significantly higher in the fr treatment at day 50 one way anova duncan s test p 0 05 in addition the relative abundance of cable bacteria to total bacteria in sediments was also increased with increasing of oxygen availability following the order of pr 0 02 nr 0 06 fr 0 11 at day 50 fig 4b over the incubation period no cable bacteria distribution was found in the sediments of the anoxic treatment through both fish analysis and 16s rrna gene sequencing which was coincided with the lack of e sox geochemical imprints as shown in fig 5 the depth distributions of cable bacteria are similar in three re oxygenated treatments pr nr and fr and dependent on time at day 12 the highest density of cable bacteria was observed in the top surface layer 0 0 5 cm of the sediment and then gradually decreased with depth whereas in day 28 and day 50 the vertical density of cable filaments was firstly increased with depth until reach the peak value and then decrease substantially in the deeper sediment the vertical densities of cable bacteria and its extended depth were changed with incubation time and varied greatly among different treatments in particular the growth of cable bacteria in sediments from nr 0 5 cm and fr 0 5 cm treatments was deeper than that of the pr treatment 0 4 cm in pr and nr treatment the filament densities of cable bacteria in 0 0 5 cm sediments were increased with the oxygen penetration depth and showed a higher abundance in the pr treatment while this density was not increased with a further increase of oxygen penetration depth from the nr to fr treatment at the end of incubation the filament density in top 0 5 cm sediments of fr treatment 11 5 m cm 3 was significantly lower that of the nr 35 2 m cm 3 and pr 19 9 m cm 3 treatment however higher density of cable bacteria was detected in most of the depth in the 0 5 5 0 cm sediments from the nr and fr treatment especially in the suboxic zones 0 5 2 0 cm for instance the integrated abundance of cable bacteria in the depth of 0 5 2 0 cm sediments from the fr 162 m cm 2 and nr 146 m cm 2 treatment were 2 9 fold and 2 4 fold as that in the pr 58 2 m cm 2 treatment respectively in addition the peak vertical density in the treatments with higher oxygen availability was occurred in the deeper sediment layers which was consistent with the sulfide appearance depths at the end of incubation the sad in sediments were 2 06 cm 1 64 cm and 1 19 cm for the fr nr and pr treatment respectively 4 discussion 4 1 development dynamics of cable bacteria population in this study the e sox activity was tracked by time series monitoring of the depth profiles of ph o2 and h2s in pore water within the first 7 days we did not observe the ph excursion in oxic zone in all treatment fig s2 the small size of cable bacteria community at early incubation stage is not able to exert obvious impacts on the ph of sediments and thus were not observable base on the ph profiles similar results were also observed in our previous incubation study using the sediments from the same site xu et al 2022 after 12 d of incubation typical geochemical fingerprints of cable bacteria including the appearance of a substantial suboxic zone a ph maximum in top oxic layers and a pronounced acidification in the deeper anoxic sediments were detected in the three re oxygenated treatments fr nr pr suggesting the presence of e sox activity pfeffer et al 2012 malkin et al 2014 this considerable development of cable bacteria occurred within a few days day 7 to day 12 was consistent with the previous studies which demonstrated a rapid development of cable bacteria within 1 2 weeks rao et al 2016 van de velde et al 2017 to clearly illustrate the development dynamics of cable bacteria in different treatments the suboxic zone width magnitude of ph excursions δph and cathodic oxygen consumption coc rate were further calculated based on the microsensor data these three parameters are commonly used indicators for the intensity of e sox activity malkin et al 2014 schauer et al 2014 burdorf et al 2018 notably the temporal change trends of coc rate δph and the suboxic zone width were highly consistent in the three re oxygenated treatments pr nr and fr fig 2 according to the coc and ph data all the treatments firstly showed an initial slow growth phase 7days fig s2 then enter a rapid growth phase subsequently declined at the rest of incubation decline phase at the end of incubation the subsurface ph peak was absent in the three treatments thus the coc rate was calculated as zero through the alkalinity balance this may underestimate the metabolic activity of cable bacteria since the featured distribution of fe2 and so2 4 in pore water remain at this moment the absence of ph maximum may be caused by dissipation of alkalinity into the overlying water burdorf et al 2016 van de velde et al 2016 in the sediments with narrow oxic zones opd 3 0 mm in this study the upward export of alkalinity is rapid and thus make the formation of a ph peak difficult in later incubation stage hermans et al 2020 due to the low temporal resolution of microsensor profiling and the intrinsic limitation of the coc determination method the stable phase was not observed in this study and thus the timing of each growth phase cannot be identified an incubation study on marine sediments suggested the time interval of cable bacteria development was greatly dependent on oxygen supply levels in overlying water higher oxygen availability resulted in a faster development of cable bacteria as manifested by a shorter time to reach the peak metabolic activity burdorf et al 2018 in freshwater sediments σh2s contents in pore water are usually very low 5 µmol in this study thus the availability of sulfide and oxygen are probably both the limiting factors for the initial growth of cable bacteria using the sediment from the same site under aeration conditions we found a moderate increase of sulfide availability enable to promote the rapid growth of cable bacteria the timing of slow growth phase was shortened by more than one week in higher sulfide treatment xu et al 2022 while for marine sediments that have high sulfide availability burdorf et al 2018 the growth of cable bacteria in early development stage may be only restricted by oxygen supply more related studies on various cable bacteria habitats with different sedimentary geochemical characteristics are needed to clarify these speculations 4 2 effects of oxygen availability on e sox activity our results demonstrated the enhancement of oxygen availability in the overlying water resulted in higher e sox activity of cable bacteria as revealed in the following aspects firstly higher metabolic activity means a higher rate of the cathodic half reaction as reflected by a higher cathodic oxygen consumption coc rate nielsen et al 2010 schauer et al 2014 as expected we found that the coc rate was increased with the increasing of oxygen supply levels in overlying water fig 2a secondly our results showed a higher ph change δph fig 2b and a broader suboxic zone fig 2c in higher oxygen supply treatments which also indicating the increased intensity of metabolic activity in response to the elevation of oxygen availability burdorf et al 2016 burdorf et al 2018 in addition the promotion effect was also verified by the pore water and solid phase geochemistry analysis anodic sulfide oxidation induced by the metabolism of cable bacteria exerts the acidification and the decrease of h2s saturation levels in pore water and then promotes the dissolution of fes in suboxic zone and stimulated sulfate regeneration and fe2 release risgaard petersen et al 2012 meysman et al 2015 sulu gambari et al 2016a sandfeld et al 2020 in this study the increased accumulation of dissolved fe2 fig 3a and so2 4 fig 3b in pore water and the enhanced depletion of avs was simultaneously observed in higher oxygen supply treatments at the end of incubation which further confirmed the enhanced e sox activity with elevated oxygen supply the promoting effects of increasing oxygen availability on metabolic activity of cable bacteria mainly reflected at two aspects direct effect and indirect effect in terms of direct effect the increasing supply of oxygen will extend the width of the oxic zone and allow more filament cells of cable bacteria to contact with oxygen this further facilitated cable bacteria to harvest more electrons from the anodic reactions and thus enhanced the metabolic activity of cable bacteria malkin and meysman 2015 burdorf et al 2018 liu et al 2021 the indirect effect was mainly achieved by affecting sulfide supply to cable bacteria specifically elevated oxygen supply will create a positive feedback in which high oxygen availability induced a more rapid growth of cable bacteria and result in an even stronger acidification of the pore water δph values in pore water fr nr pr as shown in fig 2c and more sulfate production which promoted fes dissolution sulfate reduction and finally increase sulfide supply and this in turn lead to a higher metabolic activity of cable bacteria meysman et al 2015 burdorf et al 2018 notably the relative importance of these two aspects varied in different incubation stage at early incubation stage the dissolved sulfide in pore water is the only sulfide sources for cable bacteria growth the direct effect was mainly responsible for the elevation of cable bacteria activity in high oxygen supply treatments however with increasing of incubation time the activity of cable bacteria led to the additional supply of sulfide at this moment both the direct and indirect effects will contribute to the promotion of e sox activity as reported previously there are three primary pathways of sulfide supply to cable bacteria growth meysman et al 2015 we further calculated the sedimentary balance of sulfur and iron at the end of the incubation to estimate the effects of oxygen availability on each sulfide supply source table 2 the calculation assumed that the activity of cable bacteria is restricted to the suboxic zone three sulfide supply pathways from the diffusion of free sulfide produced in deeper sediments or via the dissolution of fes or sulfate reduction in the suboxic zone van de velde et al 2016 were investigated the upward flux of sulfide from deeper sediment was calculated based on the σh2s profiles using fick s first law as reported by burdorf et al 2018 the part of the curves at the bottom of suboxic zone with the fastest slope change was used to calculate the sulfide flux the rate of fes dissolution in suboxic zone was estimated as the rate of avs consumption van de velde et al 2016 as avs was interpreted to be mostly fes in the sediment seitaj et al 2015 sulfate reduction rate in suboxic zone was calculated as the total sulfate reduction rate subtract the sulfate reduction rate in deeper sulfidic zone the detailed calculation processes were provided in burdorf et al 2018 and van de velde et al 2016 the relative importance of three supply pathway of sulfide to cable bacteria activity is similar in different treatments at the end of incubation sulfate reduction in the suboxic region 74 77 was the most important source followed by the dissolution of fes 22 25 the contribution of upward h2s diffusion from sulfate reduction in deeper sediments is negligible 1 across all treatments table 2 our results demonstrated that the increased sulfide supply in high oxygen supply treatment fr was mainly coming from locally fes dissolution 69 and sulfate reduction in suboxic zone 31 in contrast burdorf et al 2018 found that sulfate reduction in deeper sediments 67 82 was the most important pathway of sulfide supply to cable bacteria in a seasonally hypoxic marine sediment in addition their incubation study showed a significant increase of upward h2s flux with the increasing of oxygen supply whereas in our study the upward h2s flux was not affected by oxygen availability and keep at a very low level 0 02 0 03 mmol s m 2 d 1 these results suggested elevation of oxygen availability can greatly increase sulfide supply to cable bacteria growth and displayed in different ways in different sediments future studies on different types of sediment habitats with contrasting geochemical features are needed to explore how the interactions between these two major environmental factors affecting the distribution and dynamics of cable bacteria population in the field 4 3 effects of oxygen availability on cable bacteria abundance and depth distribution in general our results demonstrated oxygen availability in the overlying water can greatly affected cable bacteria abundance higher oxygen availability resulted in larger population size of cable bacteria the depth integrated density of cable bacteria in fully re oxygenated treatment fr is up to 190 m cm 2 at the end of incubation which was almost 3 fold as the partially re oxygenated treatment pr 65 8 m cm 2 consistent with this 16s rrna gene sequencing showed similar trends of increases in the relative abundance of cable bacteria in response to the elevation of oxygen supply both these results strongly demonstrated the stimulation effects of elevated oxygen supply on the proliferation of cable bacteria in freshwater sediment the unique electrogenic metabolism of cable bacteria enables them to scavenge electron donors more efficiently than other sulfur oxidizing microorganisms and exhibited a strong competitive advantage in the environments with high oxygen availability nielsen et al 2010 rao et al 2016 similarly liu et al 2021 also demonstrated a large population size of cable bacteria in the sediments incubated in aerated conditions with higher oxygen supply in addition cable bacteria density has no difference between the two higher oxygen treatments fr and nr within the first 28 d but significantly higher in the fr treatment at the end of incubation this was consistent with the above e sox activity data that shown comparable coc rate between these two treatments at early incubation stage day 12 and varied greatly in later development stage at early incubation stage cable bacteria abundance is relatively low and the oxygen level of the nr treatment already can satisfy the metabolic requirement however with the proliferation of cable bacteria more oxygen is needed to sustain a larger population size and thus higher cable bacteria abundance was observed in the fr treatment at the end of incubation the oxygen concentration in the fr treatment 306 µmol l is only about 10 higher than the nr treatment 275 µmol l these results suggested the dependence of cable bacteria growth on oxygen availability is related to the population size when oxygen supply levels below the metabolic requirement of the population a slight elevation of oxygen supply can significantly promote the growth of cable bacteria similar vertical distribution pattern of cable bacteria was observed in the three re oxygenated treatments at the early development stage day 12 the highest filament density was observed in the oxic zone of the sediment and then declined in the deeper the anoxic layers with the increasing of incubation time cable bacteria grew downwards in the anoxic sediment with the development of suboxic zone and showed a peak vertical density nearby the sulfide appearance depth this distribution confirmed the previous speculations that the initial growth of cable bacteria occurred in the oxic anoxic interface nearby the surface sediment and then developed downwards to capture free sulfide in deeper pore water schauer et al 2014 nielsen et al 2015 meanwhile our results demonstrated the depth distribution of cable bacteria also affected by oxygen availability comparing to the nr and pr treatment the top 0 5 cm sediments of fr treatment showed the lowest filament density at the end of incubation this may due to the detrimental effects of high oxygen availability on the cathodic cells of cable bacteria in surface oxic zone according to previous studies kjeldsen et al 2019 cable bacteria are evolutionarily obligate anaerobes the abundant reactive oxygen species derived from high oxygen reduction may induced oxidative stress on filament cells located in oxic zone however cable bacteria possess an efficient strategy to reduce this damage on the entire population by a division of labor and cooperation between cathodic and anodic filament cells for an individual filament only a few cells are located in the oxic zone served as electron sink for the rapid dispersion of electrons while more than 90 filament cells located in the anoxic zone are responsible for gaining energy and growth and those cells did not suffer from oxidative stress geerlings et al 2020 scilipoti et al 2021 consistent with these studies we found the filament densities in deeper sediments were significantly increased with oxygen availability and the relative abundance and integrated density of cable bacteria in the fully re oxygenated treatment fr were significantly higher than that of nr and pr treatment at the end of incubation these results demonstrated that the detrimental effects of high oxygen exposure are limited to the filament cells in oxic zone and not induced adverse impacts on the whole cable bacteria population in addition the downwards expansion of cable bacteria was more significant in the treatments with higher oxygen supply levels previous studies also documented an increased growth of cable bacteria in deeper sediments with the increasing of oxygen availability malkin and meysman 2015 liu et al 2021 this downwards growth may be in order to harvest more electron donors free sulfide from the deeper sediment to support a larger cable population under sufficient oxygen conditions schauer et al 2014 marzocchi et al 2020 liu et al 2021 moreover at the end of incubation we found the inventories of dissolved so2 4 in the deeper layers 4 5 cm of sediments from the fully re oxygenated treatment fr 7 17 mmol m 2 were higher than that in the partially re oxygenated treatment pr 5 79 mmol m 2 one way anova duncan s test p 0 05 this result suggested the elevation of oxygen availability led to a higher enrichment of cable bacteria in deeper sediments which may further expand their impacts on the sediment biogeochemistry on the vertical scale such as sulfur cycling 5 conclusions in conclusion our study provided clear evidence of the promotion effects of increasing oxygen availability on metabolic activity and population growth of cable bacteria in freshwater sediments cable bacteria populations from high oxygen supply treatments exhibited a higher e sox activity and a higher population density as affected by the temporal changes of sulfide availability and population size the promotion effects of increase oxygen supply on the proliferation of cable bacteria are varied at different incubation stage a higher enrichment of cable bacteria and a significant increase of dissolved sulfate concentration were simultaneously observed in deeper sediments from high oxygen supply treatments indicating the elevation of oxygen availability may expand the vertical impacts of cable bacteria on elemental biogeochemical cycling in sediments however due to the limitations in experimental design the derivation of the minimumthreshold of oxygen supply for cable bacteria growth in freshwater sediments was not achieved in this study an incubation study on marine sediment reported the minimum oxygen limit for cable bacteria growth is 50 µm burdorf et al 2018 the lowest in situ o2 levels that allow for the presence of cable bacteria was recorded as 5 µm in the hypoxic basins of the baltic sea marzocchi et al 2018 hermans et al 2020 however there is no available data on freshwater sediments currently exploring the minimum o2 threshold for cable bacteria development in freshwater sediment will promote our understanding of theirnaturegeographicdistribution and evolution moreover further studies on diverse sediment habitats with different geochemical features are necessary to illustrate how the interactions of oxygen availability with other environmental factors such as sulfide supply controls the distribution and growth dynamics of cable bacteria in natural environments credit authorship contribution statement shouliang huo conceptualization methodology writing original draft xiaoling xu writing original draft nanyan weng writing original draft hanxiao zhang writing original draft fengchang wu writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by grants from the national natural science foundation of china nos 91751114 51922010 and the national key research and development program of china 2017yfa0605003 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 127666 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3499,cable bacteria have been discovered in various marine and freshwater habitats and their unique metabolism named electrogenic sulfide oxidation e sox is of great ecological significance to aquatic ecosystems however the environmental factors that determining the dynamics and abundance of cable bacteria in freshwater sediments are not well understood in this study we examined the activity and growth of cable bacteria in response to the change of oxygen availability in freshwater sediments the metabolic activity of cable bacteria was quantified by microsensor profiling while their abundance was determined by fluorescence in situ hybridization and 16s rrna gene sequencing the increase of oxygen availability greatly promoted the metabolic activity and proliferation of cable bacteria as reflected by a higher rate of e sox and a larger population size as affected by the change of sulfide availability and oxygen demand with the proliferation of cable bacteria these promoting effects were more pronounced at later development stage moreover the elevation of oxygen availability drove the downwards growth of cable bacteria and the increased inventories of dissolved sulfate in deeper sediment layers which may expand the influences of cable bacteria on sediment biogeochemical cycling on the vertical scale our results contribute to further understanding of the correlations between oxygen availability and growth dynamics of cable bacteria in natural environments keywords cable bacteria oxygen availability metabolic activity abundance and distribution freshwater sediment 1 introduction cable bacteria are centimeter long filamentous sulfide oxidizing bacteria belonging to the desulfobulbacea family trojan et al 2016 using their filaments as electron conductors cable bacteria link oxygen reduction at sediment water interface to sulfide oxidation in anoxic regions nielsen et al 2010 pfeffer et al 2012 meysman et al 2019 the combination of these two separated reactions is known as electrogenic sulfur oxidation e sox pfeffer et al 2012 malkin et al 2014 the process of e sox created a distinct suboxic zone without o2 and h2s an alkalization nearby the bottom water and an intense acidification in deeper anoxic sediment schauer et al 2014 meysman et al 2015 these unique geochemical imprints arecommonly used to track the development of cable bacteria in both laboratory and field studies van de velde et al 2016 burdorf et al 2018 marzocchi et al 2018 the metabolism of cable bacteria can greatly accelerate the biogeochemical cycling in sediments especially for iron rao et al 2016 sulu gambari et al 2016a sulfur sandfeld et al 2020 liu et al 2021 carbon müller et al 2016 scholz et al 2020 nitrogen kessler et al 2019 marzocchi et al 2021 and trace elements van de velde et al 2017 the intensified cycling of these elements can induce a series of additional biogeochemical consequences that include the prevention of benthic sulfide release seitaj et al 2015 hermans et al 2019 martin et al 2017 the promotion of sedimentary phosphorus sequestration sulu gambari et al 2016b hermans et al 2021 and the suppression of methane production in sediments scholz et al 2020 this may contribute to the protection of benthicorganisms improvement of water quality and regulation of trophic status seitaj et al 2015 sulu gambari et al 2016b and have implications and greenhousegasreduction in aquatic ecosystems scholz et al 2020 scholz et al 2021 given the great ecological significance of cable bacteria to local environments it is important to explore the environmental factors that determining their development and distribution in natural environments cable bacteria have been discovered in diverse habitats across marine and freshwater environments risgaard petersen et al 2015 burdorf et al 2017 scholz et al 2021 despite ubiquitous in the environment their ecological niche has been documented to be strictly constrained by several key factors such as the availability of electron donors e g fes or free sulfide hermans et al 2019 xu et al 2022 and acceptors e g oxygen or nitrate marzocchi et al 2014 2018 oxygen as the preferred electron acceptor for the metabolic activity of cable bacteria can efficiently harvest the electrons from sulfide oxidation marzocchi et al 2014 phylogenetic analysis suggested cable bacteria from marine and freshwater habitats are belong to different lineage risgaard petersen et al 2015 trojan et al 2016 dam et al 2021 which may have different physiological properties and thus response to environmental factors differently laboratory evidence suggested the increasing of oxygen levels in overlying water can significantly promote the proliferation of cable bacteria and enhance their influences on sedimentary geochemistry burdorf et al 2018 liu et al 2021 e sox activity was immediately disappeared when there was very low or without oxygen risgaard petersen et al 2015 field studies also suggested the population dynamics of cable bacteria were closely related to the temporal variations of bottom water oxygen levels in seasonal hypoxia coastal environments no cable bacteria was observed in persistent anoxic environments marzocchi et al 2018 hermans et al 2019 hermans et al 2021 more recently the occurrence of cable bacteria has been also reported in the root systems of aquatic plants martin et al 2019 scholz et al 2019 scholz et al 2021 and the tubes and burrows sediments of intertidal macro benthos aller et al 2019 li et al 2020 which was directly connected with introducing of oxygen to these microenvironments however most of these previous studies are focus on marine sediments the potential effects of oxygen availability on the development and abundance of cable bacteria in freshwater sediments are seldom explored the present study was performed to investigate how the oxygen availability in overlying water may affect the growth of cable bacteria in freshwater sediments growth dynamics of cable bacteria were monitored using a time series sediment incubation experiment where sediment cores were subjected to a range of bottom water redox conditions from anoxic to fully re oxygenated which were kept constant over time cable bacteria activity was examined by high resolution depth profiling of o2 σh2s and ph in combined with the pore water and sediment geochemical analysis cable filament densities and its depth distribution in sediment were quantified by fluorescence in situ hybridization fish and the relative abundance of cable bacteria was estimated by the high throughput 16s rrna gene sequencing our study will provide valuable information on the understanding of the influence oxygen availability on the distribution and growth dynamics of cable bacteria in natural environments 2 materials and methods 2 1 sediment sampling and preparation surface sediments in the depth of 0 5 cm were collected from a natural freshwater habitat of cable bacteria xu et al 2021 located in the wenyu river 40 07 00 36 n 116 16 27 57 e beijing china in october 2019 the samples of overlying water 30 l were simultaneously collected and filtered by polyether sulfone pes membrane 0 45 µm for further use the physiochemical properties of the sediment and the overlying water are summarized in table 1 before experiment the sediments were pretreated for incubation following the procedures as described in previous studies rao et al 2014 schauer et al 2014 in brief the sediment was firstly asphyxiated and sieved by a mesh 0 5 mm to exclude the potential disturbance of macrofauna after that the sediments were completely homogenized to ensure consistent initial conditions for incubation experiments 2 2 sediment incubation and sampling procedures the prepared sediments were packed into a total of 48 glass beakers diameter 6 0 cm height 13 5 cm as sediment cores and then these packed glass beakers were divided into four groups 12 replicated cores for each group and transferred into different plastic aquaria length 28 5 cm width 20 cm height 17 cm the filtered overlying water was then gently added to these aquaria to construct the sediment water incubation systems dissolved oxygen in the overlying water of these incubation systems were regulated into different levels by controlling the aerationprocess specifically the overlying water in the first incubation system was flushed with pure nitrogen for 2 h and then the incubation aquaria was sealed with three layers of parafilm sigma aldrich usa and tightly fixed by tapes to prevent air permeation referred to as the anoxic an treatment the second incubation aquaria was also sealed with three layers of parafilm in which 20 pinholes with a dimeter of 0 03 cm were pierced for gas exchange with air and the fluctuations of oxygen levels were controlled by flushing nitrogen referred to as the partially re oxygenated pr treatment the plastic pipeline of nitrogen diameter 0 4 cm and o2 sensor diameter 0 5 cm were inserted into the overlaying water through the holes the third incubation system was kept in ambient condition and referred to as the natural re oxygenated nr treatment the overlying water in the fourth aquaria was continuously and gently aerated by a pump to achieve a saturated state of dissolve oxygen referred to as the fully re oxygenated fr treatment except for a slight aeration induced stirring in the fr and pr treatment the other two treatments were incubated under stagnant condition the corresponding dissolved oxygen contents in overlaying water from the four treatments are 1 88 0 57 μmol l 1 an 153 28 7 μmol l 1 pr 276 8 2 μmol l 1 nr and 307 10 9 μmol l 1 fr respectively these aquaria were incubated at 15 c in a darkened chamber for a period of 50 days during the incubation period the dissolved oxygen concentrations in the overlying water of the fr nr and pr treatment were monitored every 2 4 days by o2 microsensor unisense a s denmark while for the anoxic treatment this monitoring was only conducted when sampling to avoid the disturbance of the anaerobic conditions fig s1 in addition 20 50 ml deionized water was gently added to replenish the evaporated water from the incubation aquaria every 2 days except for the sealed anoxic aquaria which has no significant water evaporation before replenishing the deionized water was aerated with air or nitrogen to keep their oxygen concentration comparable to that in the overlaying water of the aquaria the addition of deionized water did not induce a significant impact on the overall oxygen state of the incubation system depth profiles of σh2s ph o2 in pore water and the depth distribution of cable bacteria abundance in sediment were determined at 0 12 28 and 50 days to track the development of cable bacteria three parallel cores were done for each treatment after micro electrode measurements the corresponding sediment cores of each treatment were removed from the incubator their top 5 cm sediment was collected with a cutoff syringe diameter 3 cm and then sliced at 0 5 cm thicknessresolution 0 5 ml sediment of each sediment slice was then fixed with the same volume of 96 ethanol and frozen at 20 c for the fluorescence in situ hybridization fish analysis at the end of the incubation pore water samples of each sediment slice were extracted through centrifugation of the sediment at 4500 g 15 min for measuring the depth profiles of so2 4 no 3 and fe2 the obtained pore water was filtered through 0 45 μm filter mixed cellulose ester membrane and divided into two subsamples one subsample was stored at 20 c for the measurements of so2 4 and no 3 and the other subsample was acidified with 37 hcl 10 μl ml and then stored at 20 c for fe2 analysis sediment slicing and pore water extraction were conducted in an anoxic box continuously purged with nitrogen to prevent the oxidation of fe2 in addition the top 2 cm sediment about 0 1g from each treatment was sampled at the end of incubation the subsamples from three parallel sediment cores were completely mixed and stored at 80 c for the following 16s rrna gene sequencing 2 3 microsensor measurements high resolution depth profiles of o2 σh2s and ph were detected with commercial microsensors unisense a s denmark before each monitoring micro electrodes were calibrated according to the standard operating procedures given by the manufacturer in brief two standard points were used for oxygen sensor calibration air saturated tap water with 100 o2 saturation and the anoxic sediment with 0 o2 saturation a four point standard curve with a concentration gradient from 0 to 20 µm zinc sulfide solutions was used for the calibration of h2s sensor the standard stock solution of zinc sulfide was purchased from shenyang zhixinyun technology co ltd china with a certified concentration of 132 2 μg ml 1 the ph sensor was calibrated using nist certified standard buffer solutions of ph 4 0 7 0 and 10 0 ncl of wisconsin inc usa microprofiles of o2 σh2s and ph were recorded in the top 3 cm sediment at a depth resolution of 100 400 µm three replicate profiles were monitored for each sediment core at each time point and three parallel cores were measured for each treatment the depth profiles of total sulfide concentration σh2s h2s hs s2 were derived from the corresponding h2s contents and ph values measured at each depth jeroschewski et al 1996 the width of suboxic zone δph and cathodic oxygen consumption coc rate were calculated based on the depth profiles of o2 ph and σh2s using the average values in brief the width of suboxic zone in sediments was defined as the depth horizon between the sulfide appearance depth sad σh2s 0 3 μm and the oxygen penetration depth opd o2 1 μm seitaj et al 2015 due to the low sulfide concentration in pore water of the sediment we used the sad was defined as the depth at which σh2s concentration was more than 0 3 µm in this study δph represented as the amplitude of ph change in the pore water which was quantified as the difference between the highest ph value in the oxic zone and the lowest ph value in the suboxic zone burdorf et al 2018 the calculation of coc assumes that except the activity of cable bacteria other reactions are not contributed to the change of alkalinity in surface sediments the coc rate was derived from cathodic proton consumption cpc using the following equitation coc cpc 4 the cpc was calculated as the sum of the upward and downward alkalinity fluxes nearby the ph peak in oxic zone as described by previous studies risgaard petersen et al 2012 malkin et al 2014 schauer et al 2014 müller et al 2016 in brief alkalinity flux j ta was calculated using the following formula j ta j hc o 3 2j c o 3 2 j oh j h the specific assumptions of the cpc calculation and the detailed calculation procedures are provided in our previous studies xu et al 2021 xu et al 2022 2 4 chemical analysis dissolved oxygen ph conductivity and temperature in the overlying water were monitored by an automatic water quality detector ysi professional plus usa sediment porosity was measured following the methods as described by burdige 2007 total organic carbon toc in sediment was determined via an elemental analyzer multin c2100 elementar germany so2 4 and no 3 concentrations in the pore water were determined by ion chromatography ics3000 dionex usa and dissolved fe2 concentrations were determined by spectrophotometry method apha 2005 acid volatile sulfide avs concentrations in sediments were determined using a cold distillation procedure burton et al 2011 sulfide in avs was converted into h2s by 9 m hcl which was then trapped in alkaline zinc acetate solutions the concentrations of sulfide in the fixed solutions were determined by methylene blue spectrophotometry reese et al 2011 2 5 fish analysis and 16s rrna gene sequencing fish analysis was conducted using the standard protocols as reported in previous studies lücker et al 2007 müller et al 2016 briefly the filaments of cable bacteria were identified by the desulfobulbaceae specific oligonucleotide probe dsb706 and the density of cable filaments were observed and counted through epifluorescence microscopy the depth integrated density of bacteria filaments m cm 2 and the vertical density at each sediment depth m cm 3 were calculated according to the method described previously malkin et al 2014 schauer et al 2014 in addition 16s rrna gene sequencing was applied to determine the relative abundance of cable bacteria which was estimated as the ratios of the number of reads of cable bacteria species identified in this study to that of the total bacteria geelhoed et al 2020 the primer pair of 338f 5 actcctacggga ggcagcag 3 and 806r 5 ggactachvgggtwtctaat 3 targeting the region v3 v4 of bacterial 16s rrna gene was used for pcr amplification the details of dna extraction pcr amplification and bioinformatics data processing are provided in detail by xu et al 2021 xu et al 2022 the 16s rrna amplicon data have been uploaded to the database of national center for biotechnology information ncbi the accession number is prjna807585 2 6 statistical analysis statistical analyses were carried out with spss 21 0 one way analysis of variation anova followed by duncan s test was used to compare the differences in the parameters related to e sox activity porewater and sediment geochemistry and cable bacteria abundance p 0 05 was accepted as statistically significant 3 results 3 1 microsenser profiling of h2s o2 and ph as shown in fig 1 the oxygen penetration depth opd o2 1 μm was significantly increased with oxygen contents in overlying water one way anova duncan s test p 0 05 specifically the mean values of opd in the sediments were 2 13 mm 1 83 mm and 0 99 mm for the fr nr and pr treatment respectively at the beginning of incubation day 0 the sulfide appearance depth sad was similar in all treatments in the depth range of 2 6 3 4 mm at day 12 microsensor profiling showed a clear separation of σh2s and o2 accompanied by the formation of a suboxic zone in width of 4 8 6 00 mm in the sediments from these three treatments simultaneously the representative ph extremes were observed with a ph maxima nearby the sediment water interface and minima near the sad indicating the metabolic activity of cable bacteria in the sediments correspondingly the sulfide appearance depth sad in sediments moves downwards to the deeper sediments within the incubation period the sad moved from 3 00 to 3 40 to 3 0 20 6 mm 3 40 to 16 4 mm and 3 20 to 11 9 mm in the fr nr and pr treatment respectively however these typical geochemical imprints were not observed in sediments from the anoxic an treatment during the incubation period indicating the absence of e sox activity the cathodic oxygen consumption coc rate the magnitude of ph change δph and the width of suboxic zone are further calculated to assess the intensity of e sox activity in each treatment fig 2 in general the temporal patterns of coc rate were similar in the three treatments pr nr and fr that have cable bacteria activity which firstly increased and reach the peak value at day 28 and then decreased in the rest incubation period in particular great difference of the coc rates among different treatments was observed at 12 d and 28 d fig 2a specifically the coc rate of the fr 0 71 m 2 d 1 for 12 d and 1 04 mmol m 2 d 1 for 28 d and nr treatment 0 64 mmol m 2 d 1 for 12 d and 0 93 mmol m 2 d 1 for 28 d were significantly higher than those of the pr treatment 0 35 mmol m 2 d 1 for 12 d and 0 72 mmol m 2 d 1 for 28 d one way anova duncan s test p 0 05 similar change patterns were also observed in δph fig 2b and the width of suboxic zone fig 2c in these three treatments both these two indicators increased first until reach the maximum at day 28 and then stabilized in the following incubation time δph and the width of suboxic zone were significantly increased with the overlying water oxygen supply and the variations among different treatments were more obvious in the later incubation stages no significant difference of δph in the fr and nr treatments but both of them were significantly higher than that of the pr treatment one way anova duncan s test p 0 05 within the incubation period 3 2 pore water and sediment geochemistry depth profiles of dissolved fe2 so2 4 and acid volatile sulfide avs concentrations were determined in each treatment at day 50 fig 3 the surface layers of sediment 0 0 5 cm showed very low dissolved fe2 concentrations 3 19 3 81 µm in all treatments in three re oxygenated treatments pr nr fr fe2 showed a significant enrichment in the pore water of the suboxic zone 0 5 2 0 cm with distinct peaks in the depth of 1 0 1 5 cm and then decreased in deeper sediments fig 3a the maximum fe2 concentrations greatly increased with oxygen supply levels followed the order of pr 196 45 µm nr 297 83 µm fr 385 63 µm in contrast dissolved fe2 concentrations in sediment from anoxic treatment an were much lower than the other three treatments and maintained at relatively stable levels 18 0 26 3 µm in the anoxic layers comparing with the anoxic treatment so2 4 concentrations were increased in all sediment layers in three re oxygenated treatments and reach a maximum below the suboxic zone in the depth of 20 25 mm fig 3b the inventories of so2 4 in sediments 0 5 cm from the fr 466 mmol m 2 nr 452 mmol m 2 and pr 366 mmol m 2 treatments were 8 9 fold 8 6 fold and 7 0 fold higher than the an treatment 52 3 mmol m 2 respectively table 2 in accordancewith the enrichment of dissolved fe2 and so2 4 in pore water avs contents were greatly depleted in the surface sediments especially in the fully re oxygenated treatment fig 3c specifically the reduced avs inventory in the top 5 cm sediment of the fr 0 216 mmol m 2 treatment was about 1 1 fold and 1 4 fold as that of the nr 0 198 mmol m 2 and pr 0 152 mmol m 2 treatments respectively table 2 comparing to these three treatments no depletion of avs was observed in the sediments from the anoxic treatment table 3 3 3 cable bacteria abundance and depth distribution in the three re oxygenated treatments the integrated depth density of cable filaments was greatly increased within the incubation period fig 4 a for instance the density of cable bacteria increased by more than 10 folds from day 12 18 2 m cm 2 to day 50 190 m cm 2 in the fr treatments similar results are also observed in the pr and nr treatments comparison between treatments cable bacteria density was remarkably increased with oxygen availability and that was more prominent in the later incubation stage for instance the integrated density of cable filaments in sediments from the fr treatment at day 12 and day 50 was 1 8 fold and 2 9 fold as that of the pr treatment respectively cable bacteria density was comparable in sediments between the fr and nr treatments within the first 28 days but was significantly higher in the fr treatment at day 50 one way anova duncan s test p 0 05 in addition the relative abundance of cable bacteria to total bacteria in sediments was also increased with increasing of oxygen availability following the order of pr 0 02 nr 0 06 fr 0 11 at day 50 fig 4b over the incubation period no cable bacteria distribution was found in the sediments of the anoxic treatment through both fish analysis and 16s rrna gene sequencing which was coincided with the lack of e sox geochemical imprints as shown in fig 5 the depth distributions of cable bacteria are similar in three re oxygenated treatments pr nr and fr and dependent on time at day 12 the highest density of cable bacteria was observed in the top surface layer 0 0 5 cm of the sediment and then gradually decreased with depth whereas in day 28 and day 50 the vertical density of cable filaments was firstly increased with depth until reach the peak value and then decrease substantially in the deeper sediment the vertical densities of cable bacteria and its extended depth were changed with incubation time and varied greatly among different treatments in particular the growth of cable bacteria in sediments from nr 0 5 cm and fr 0 5 cm treatments was deeper than that of the pr treatment 0 4 cm in pr and nr treatment the filament densities of cable bacteria in 0 0 5 cm sediments were increased with the oxygen penetration depth and showed a higher abundance in the pr treatment while this density was not increased with a further increase of oxygen penetration depth from the nr to fr treatment at the end of incubation the filament density in top 0 5 cm sediments of fr treatment 11 5 m cm 3 was significantly lower that of the nr 35 2 m cm 3 and pr 19 9 m cm 3 treatment however higher density of cable bacteria was detected in most of the depth in the 0 5 5 0 cm sediments from the nr and fr treatment especially in the suboxic zones 0 5 2 0 cm for instance the integrated abundance of cable bacteria in the depth of 0 5 2 0 cm sediments from the fr 162 m cm 2 and nr 146 m cm 2 treatment were 2 9 fold and 2 4 fold as that in the pr 58 2 m cm 2 treatment respectively in addition the peak vertical density in the treatments with higher oxygen availability was occurred in the deeper sediment layers which was consistent with the sulfide appearance depths at the end of incubation the sad in sediments were 2 06 cm 1 64 cm and 1 19 cm for the fr nr and pr treatment respectively 4 discussion 4 1 development dynamics of cable bacteria population in this study the e sox activity was tracked by time series monitoring of the depth profiles of ph o2 and h2s in pore water within the first 7 days we did not observe the ph excursion in oxic zone in all treatment fig s2 the small size of cable bacteria community at early incubation stage is not able to exert obvious impacts on the ph of sediments and thus were not observable base on the ph profiles similar results were also observed in our previous incubation study using the sediments from the same site xu et al 2022 after 12 d of incubation typical geochemical fingerprints of cable bacteria including the appearance of a substantial suboxic zone a ph maximum in top oxic layers and a pronounced acidification in the deeper anoxic sediments were detected in the three re oxygenated treatments fr nr pr suggesting the presence of e sox activity pfeffer et al 2012 malkin et al 2014 this considerable development of cable bacteria occurred within a few days day 7 to day 12 was consistent with the previous studies which demonstrated a rapid development of cable bacteria within 1 2 weeks rao et al 2016 van de velde et al 2017 to clearly illustrate the development dynamics of cable bacteria in different treatments the suboxic zone width magnitude of ph excursions δph and cathodic oxygen consumption coc rate were further calculated based on the microsensor data these three parameters are commonly used indicators for the intensity of e sox activity malkin et al 2014 schauer et al 2014 burdorf et al 2018 notably the temporal change trends of coc rate δph and the suboxic zone width were highly consistent in the three re oxygenated treatments pr nr and fr fig 2 according to the coc and ph data all the treatments firstly showed an initial slow growth phase 7days fig s2 then enter a rapid growth phase subsequently declined at the rest of incubation decline phase at the end of incubation the subsurface ph peak was absent in the three treatments thus the coc rate was calculated as zero through the alkalinity balance this may underestimate the metabolic activity of cable bacteria since the featured distribution of fe2 and so2 4 in pore water remain at this moment the absence of ph maximum may be caused by dissipation of alkalinity into the overlying water burdorf et al 2016 van de velde et al 2016 in the sediments with narrow oxic zones opd 3 0 mm in this study the upward export of alkalinity is rapid and thus make the formation of a ph peak difficult in later incubation stage hermans et al 2020 due to the low temporal resolution of microsensor profiling and the intrinsic limitation of the coc determination method the stable phase was not observed in this study and thus the timing of each growth phase cannot be identified an incubation study on marine sediments suggested the time interval of cable bacteria development was greatly dependent on oxygen supply levels in overlying water higher oxygen availability resulted in a faster development of cable bacteria as manifested by a shorter time to reach the peak metabolic activity burdorf et al 2018 in freshwater sediments σh2s contents in pore water are usually very low 5 µmol in this study thus the availability of sulfide and oxygen are probably both the limiting factors for the initial growth of cable bacteria using the sediment from the same site under aeration conditions we found a moderate increase of sulfide availability enable to promote the rapid growth of cable bacteria the timing of slow growth phase was shortened by more than one week in higher sulfide treatment xu et al 2022 while for marine sediments that have high sulfide availability burdorf et al 2018 the growth of cable bacteria in early development stage may be only restricted by oxygen supply more related studies on various cable bacteria habitats with different sedimentary geochemical characteristics are needed to clarify these speculations 4 2 effects of oxygen availability on e sox activity our results demonstrated the enhancement of oxygen availability in the overlying water resulted in higher e sox activity of cable bacteria as revealed in the following aspects firstly higher metabolic activity means a higher rate of the cathodic half reaction as reflected by a higher cathodic oxygen consumption coc rate nielsen et al 2010 schauer et al 2014 as expected we found that the coc rate was increased with the increasing of oxygen supply levels in overlying water fig 2a secondly our results showed a higher ph change δph fig 2b and a broader suboxic zone fig 2c in higher oxygen supply treatments which also indicating the increased intensity of metabolic activity in response to the elevation of oxygen availability burdorf et al 2016 burdorf et al 2018 in addition the promotion effect was also verified by the pore water and solid phase geochemistry analysis anodic sulfide oxidation induced by the metabolism of cable bacteria exerts the acidification and the decrease of h2s saturation levels in pore water and then promotes the dissolution of fes in suboxic zone and stimulated sulfate regeneration and fe2 release risgaard petersen et al 2012 meysman et al 2015 sulu gambari et al 2016a sandfeld et al 2020 in this study the increased accumulation of dissolved fe2 fig 3a and so2 4 fig 3b in pore water and the enhanced depletion of avs was simultaneously observed in higher oxygen supply treatments at the end of incubation which further confirmed the enhanced e sox activity with elevated oxygen supply the promoting effects of increasing oxygen availability on metabolic activity of cable bacteria mainly reflected at two aspects direct effect and indirect effect in terms of direct effect the increasing supply of oxygen will extend the width of the oxic zone and allow more filament cells of cable bacteria to contact with oxygen this further facilitated cable bacteria to harvest more electrons from the anodic reactions and thus enhanced the metabolic activity of cable bacteria malkin and meysman 2015 burdorf et al 2018 liu et al 2021 the indirect effect was mainly achieved by affecting sulfide supply to cable bacteria specifically elevated oxygen supply will create a positive feedback in which high oxygen availability induced a more rapid growth of cable bacteria and result in an even stronger acidification of the pore water δph values in pore water fr nr pr as shown in fig 2c and more sulfate production which promoted fes dissolution sulfate reduction and finally increase sulfide supply and this in turn lead to a higher metabolic activity of cable bacteria meysman et al 2015 burdorf et al 2018 notably the relative importance of these two aspects varied in different incubation stage at early incubation stage the dissolved sulfide in pore water is the only sulfide sources for cable bacteria growth the direct effect was mainly responsible for the elevation of cable bacteria activity in high oxygen supply treatments however with increasing of incubation time the activity of cable bacteria led to the additional supply of sulfide at this moment both the direct and indirect effects will contribute to the promotion of e sox activity as reported previously there are three primary pathways of sulfide supply to cable bacteria growth meysman et al 2015 we further calculated the sedimentary balance of sulfur and iron at the end of the incubation to estimate the effects of oxygen availability on each sulfide supply source table 2 the calculation assumed that the activity of cable bacteria is restricted to the suboxic zone three sulfide supply pathways from the diffusion of free sulfide produced in deeper sediments or via the dissolution of fes or sulfate reduction in the suboxic zone van de velde et al 2016 were investigated the upward flux of sulfide from deeper sediment was calculated based on the σh2s profiles using fick s first law as reported by burdorf et al 2018 the part of the curves at the bottom of suboxic zone with the fastest slope change was used to calculate the sulfide flux the rate of fes dissolution in suboxic zone was estimated as the rate of avs consumption van de velde et al 2016 as avs was interpreted to be mostly fes in the sediment seitaj et al 2015 sulfate reduction rate in suboxic zone was calculated as the total sulfate reduction rate subtract the sulfate reduction rate in deeper sulfidic zone the detailed calculation processes were provided in burdorf et al 2018 and van de velde et al 2016 the relative importance of three supply pathway of sulfide to cable bacteria activity is similar in different treatments at the end of incubation sulfate reduction in the suboxic region 74 77 was the most important source followed by the dissolution of fes 22 25 the contribution of upward h2s diffusion from sulfate reduction in deeper sediments is negligible 1 across all treatments table 2 our results demonstrated that the increased sulfide supply in high oxygen supply treatment fr was mainly coming from locally fes dissolution 69 and sulfate reduction in suboxic zone 31 in contrast burdorf et al 2018 found that sulfate reduction in deeper sediments 67 82 was the most important pathway of sulfide supply to cable bacteria in a seasonally hypoxic marine sediment in addition their incubation study showed a significant increase of upward h2s flux with the increasing of oxygen supply whereas in our study the upward h2s flux was not affected by oxygen availability and keep at a very low level 0 02 0 03 mmol s m 2 d 1 these results suggested elevation of oxygen availability can greatly increase sulfide supply to cable bacteria growth and displayed in different ways in different sediments future studies on different types of sediment habitats with contrasting geochemical features are needed to explore how the interactions between these two major environmental factors affecting the distribution and dynamics of cable bacteria population in the field 4 3 effects of oxygen availability on cable bacteria abundance and depth distribution in general our results demonstrated oxygen availability in the overlying water can greatly affected cable bacteria abundance higher oxygen availability resulted in larger population size of cable bacteria the depth integrated density of cable bacteria in fully re oxygenated treatment fr is up to 190 m cm 2 at the end of incubation which was almost 3 fold as the partially re oxygenated treatment pr 65 8 m cm 2 consistent with this 16s rrna gene sequencing showed similar trends of increases in the relative abundance of cable bacteria in response to the elevation of oxygen supply both these results strongly demonstrated the stimulation effects of elevated oxygen supply on the proliferation of cable bacteria in freshwater sediment the unique electrogenic metabolism of cable bacteria enables them to scavenge electron donors more efficiently than other sulfur oxidizing microorganisms and exhibited a strong competitive advantage in the environments with high oxygen availability nielsen et al 2010 rao et al 2016 similarly liu et al 2021 also demonstrated a large population size of cable bacteria in the sediments incubated in aerated conditions with higher oxygen supply in addition cable bacteria density has no difference between the two higher oxygen treatments fr and nr within the first 28 d but significantly higher in the fr treatment at the end of incubation this was consistent with the above e sox activity data that shown comparable coc rate between these two treatments at early incubation stage day 12 and varied greatly in later development stage at early incubation stage cable bacteria abundance is relatively low and the oxygen level of the nr treatment already can satisfy the metabolic requirement however with the proliferation of cable bacteria more oxygen is needed to sustain a larger population size and thus higher cable bacteria abundance was observed in the fr treatment at the end of incubation the oxygen concentration in the fr treatment 306 µmol l is only about 10 higher than the nr treatment 275 µmol l these results suggested the dependence of cable bacteria growth on oxygen availability is related to the population size when oxygen supply levels below the metabolic requirement of the population a slight elevation of oxygen supply can significantly promote the growth of cable bacteria similar vertical distribution pattern of cable bacteria was observed in the three re oxygenated treatments at the early development stage day 12 the highest filament density was observed in the oxic zone of the sediment and then declined in the deeper the anoxic layers with the increasing of incubation time cable bacteria grew downwards in the anoxic sediment with the development of suboxic zone and showed a peak vertical density nearby the sulfide appearance depth this distribution confirmed the previous speculations that the initial growth of cable bacteria occurred in the oxic anoxic interface nearby the surface sediment and then developed downwards to capture free sulfide in deeper pore water schauer et al 2014 nielsen et al 2015 meanwhile our results demonstrated the depth distribution of cable bacteria also affected by oxygen availability comparing to the nr and pr treatment the top 0 5 cm sediments of fr treatment showed the lowest filament density at the end of incubation this may due to the detrimental effects of high oxygen availability on the cathodic cells of cable bacteria in surface oxic zone according to previous studies kjeldsen et al 2019 cable bacteria are evolutionarily obligate anaerobes the abundant reactive oxygen species derived from high oxygen reduction may induced oxidative stress on filament cells located in oxic zone however cable bacteria possess an efficient strategy to reduce this damage on the entire population by a division of labor and cooperation between cathodic and anodic filament cells for an individual filament only a few cells are located in the oxic zone served as electron sink for the rapid dispersion of electrons while more than 90 filament cells located in the anoxic zone are responsible for gaining energy and growth and those cells did not suffer from oxidative stress geerlings et al 2020 scilipoti et al 2021 consistent with these studies we found the filament densities in deeper sediments were significantly increased with oxygen availability and the relative abundance and integrated density of cable bacteria in the fully re oxygenated treatment fr were significantly higher than that of nr and pr treatment at the end of incubation these results demonstrated that the detrimental effects of high oxygen exposure are limited to the filament cells in oxic zone and not induced adverse impacts on the whole cable bacteria population in addition the downwards expansion of cable bacteria was more significant in the treatments with higher oxygen supply levels previous studies also documented an increased growth of cable bacteria in deeper sediments with the increasing of oxygen availability malkin and meysman 2015 liu et al 2021 this downwards growth may be in order to harvest more electron donors free sulfide from the deeper sediment to support a larger cable population under sufficient oxygen conditions schauer et al 2014 marzocchi et al 2020 liu et al 2021 moreover at the end of incubation we found the inventories of dissolved so2 4 in the deeper layers 4 5 cm of sediments from the fully re oxygenated treatment fr 7 17 mmol m 2 were higher than that in the partially re oxygenated treatment pr 5 79 mmol m 2 one way anova duncan s test p 0 05 this result suggested the elevation of oxygen availability led to a higher enrichment of cable bacteria in deeper sediments which may further expand their impacts on the sediment biogeochemistry on the vertical scale such as sulfur cycling 5 conclusions in conclusion our study provided clear evidence of the promotion effects of increasing oxygen availability on metabolic activity and population growth of cable bacteria in freshwater sediments cable bacteria populations from high oxygen supply treatments exhibited a higher e sox activity and a higher population density as affected by the temporal changes of sulfide availability and population size the promotion effects of increase oxygen supply on the proliferation of cable bacteria are varied at different incubation stage a higher enrichment of cable bacteria and a significant increase of dissolved sulfate concentration were simultaneously observed in deeper sediments from high oxygen supply treatments indicating the elevation of oxygen availability may expand the vertical impacts of cable bacteria on elemental biogeochemical cycling in sediments however due to the limitations in experimental design the derivation of the minimumthreshold of oxygen supply for cable bacteria growth in freshwater sediments was not achieved in this study an incubation study on marine sediment reported the minimum oxygen limit for cable bacteria growth is 50 µm burdorf et al 2018 the lowest in situ o2 levels that allow for the presence of cable bacteria was recorded as 5 µm in the hypoxic basins of the baltic sea marzocchi et al 2018 hermans et al 2020 however there is no available data on freshwater sediments currently exploring the minimum o2 threshold for cable bacteria development in freshwater sediment will promote our understanding of theirnaturegeographicdistribution and evolution moreover further studies on diverse sediment habitats with different geochemical features are necessary to illustrate how the interactions of oxygen availability with other environmental factors such as sulfide supply controls the distribution and growth dynamics of cable bacteria in natural environments credit authorship contribution statement shouliang huo conceptualization methodology writing original draft xiaoling xu writing original draft nanyan weng writing original draft hanxiao zhang writing original draft fengchang wu writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by grants from the national natural science foundation of china nos 91751114 51922010 and the national key research and development program of china 2017yfa0605003 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 127666 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
