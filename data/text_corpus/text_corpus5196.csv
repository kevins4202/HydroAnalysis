index,text
25980,the nasa unified weather research and forecasting model has been coupled with the carnegie ames stanford approach biogeochemical model global fire emissions database and the parameterized chemistry transport model nu wrf casa to simulate co2 transport and variability at fine spatial resolutions which is anticipated to assist in better understanding the mechanisms and processes controlling carbon sources sinks and in reducing uncertainty in carbon climate interactions nu wrf casa is designed to operate in an internally consistent manner in which nasa s goddard earth observing system model version 5 can be applied to drive the entire coupled modeling system a 3 year simulation was carried out for north america to evaluate nu wrf casa s performance under various regions and environmental conditions observational co2 data from tower flask aircraft and satellite measurements were used for model evaluations the results showed that nu wrf casa correctly distributed co2 fluxes and reproduced spatial temporal co2 transport and variability reasonably well with overall bias within 1 ppmv 1 introduction approximately one half of the anthropogenic emission of carbon dioxide co2 to the atmosphere is currently removed from the atmosphere by the air sea flux and land atmosphere flux interannual variability in this flux is large and the magnitude location and mechanisms producing the co2 sink are not well determined ipcc 2018 usgcrp 2018 as a result carbon climate interaction is among the leading sources of uncertainty in prediction of future climate cox et al 2000 dufresne et al 2002 friedlingstein et al 2014 in particular the inferred terrestrial carbon sink and its variability spatial distribution and dependence on environmental conditions must be better characterized in order to assess how surface uptake and release of co2 will evolve with changing fossil fuel emissions land use and climate in coming decades ipcc 2018 and references within part of the problem in understanding these mechanisms is that key processes controlling co2 fluxes and hence local co2 mixing ratio variability occur at relatively small spatial and or rapid temporal scales examples include photosynthetic dependence on sunlight and soil moisture temperature dependence of above ground and soil respiration and plant cover and phenology small scales are similarly important for fossil fuel emissions from point sources urban and or industrial areas and transportation as well as biomass burning because surface fluxes are generally inferred from gradients in atmospheric co2 the problem is further complicated by transport in atmospheric boundary layers and frontal systems that can concentrate or reduce mixing ratio gradients produced by fluxes which are themselves often correlated with the local weather e g parazoo et al 2011 co2 flux and abundance heterogeneity are so large that it becomes very difficult to integrate the net of these processes in time and space to global regional or even landscape scales yet it is precisely the small shifts in balance between photosynthesis and respiration during the mid latitude seasonal cycle that are hypothesized to drive a large part of the global net terrestrial sink friedlingstein et al 1995 as a result global prognostic models which have been developed from sparse observational evidence diverge greatly and bottom up flux inferences don t agree with top down ones e g gourdji et al 2012 inferring fluxes from observations in either forward or inverse models is hindered by the stiffness of the system owing to limited model resolution in addressing the aforementioned scale problems efforts have been made to develop high resolution regional or mesoscale carbon flux transport models ahmadov et al 2007 coupled a diagnostic biosphere model vegetation photosynthesis and respiration model vprm mahadevan et al 2008 to the community weather research and forecasting wrf model skamarock et al 2008 to investigate the co2 flux and transport at resolutions of a few tens of kilometers km vprm is an empirical model calibrated to measurements of eddy covariance flux towers and extrapolated across the landscape in wrf vprm the co2 net ecosystem exchange nee flux is derived using the enhanced vegetation index and land surface water index from the moderate resolution imaging spectroradiometer modis and the shortwave radiation and air temperature from wrf wrf vprm has shown better performance than two global models in reproducing the coastal tower co2 measurements ahmadov et al 2009 using wrf as the modeling framework ballav et al 2012 simulated the co2 concentration over east asia at the 27 27 km horizontal resolution and diaz isaac et al 2014 compared the simulated co2 at a 10 10 km resolution with the measurements from the mid continental intensive mci field campaign both of which utilized the terrestrial carbon flux from stand alone biosphere models similar efforts employing other mesoscale transport models to investigate co2 at high spatial resolutions include the online atmosphere biosphere coupling works by sarrat et al 2007a b and the offline coupling works by ter maat et al 2010 and uebel et al 2017 all of which are based on short episodic simulations geared toward various field campaigns inverse modeling has also been progressing toward higher resolutions especially for regional models for example wang et al 2014 used a north american regional inversion system with meteorology from wrf run at 40 km resolution coupled to the stochastic time inverted lagrangian transport stilt model run at 1 1 flux resolution to evaluate the flux uncertainty reduction that could be provided by the proposed active sensing of co2 emissions over nights days and seasons ascends satellite mission the works by different research teams demonstrate the value of high resolution co2 simulation in characterizing carbon sources sinks and improving carbon flux inversions this paper presents a model development and evaluation effort to integrate the carnegie ames stanford approach casa biogeochemical model randerson et al 1996 1997 van der werf et al 2006 2010 to nasa unified wrf nu wrf z tao et al 2013 2016 peters lidard et al 2015 it is similar to the studies by ballav et al 2012 and diaz isaac et al 2014 but with a greater spatial temporal scale and scope of analysis the goal is to extend the capability of nu wrf an institutional regional modeling and assimilation system to investigate carbon fluxes and transport at satellite resolved spatial scales 1 20 km the paper is organized as follows section 2 describes the modeling system and development followed by model evaluation in section 3 different from most existing studies whose experiments focus on relatively small regions at weeks to months temporal scales this work evaluates the model performance over a relatively large region covering various landscapes topographies and climate zones over a relatively long period 3 years including various environmental conditions such as drought conclusions are summarized in section 4 2 model development there are four major components in the modeling system 1 nasa s goddard earth observing system model version 5 geos 5 that includes the production of nasa s modern era retrospective analysis for research and applications version 2 merra 2 2 casa and global fire emissions database casa gfed 3 parameterized chemistry transport model pctm and 4 nu wrf supplement figure 1 s shows the relationship of four components in which geos 5 merra 2 provides meteorology to drive casa gfed and pctm as well as furnishes meteorological lateral boundary conditions lbcs to constrain nu wrf casa gfed generates terrestrial carbon fluxes to feed into both pctm and nu wrf for co2 simulations since they had been well developed and evaluated only brief introduction was given to each component in sections 2 1 through 2 4 the interface and modules newly developed to couple these four components were presented with details in sections 2 5 and 2 6 2 1 geos 5 merra 2 geos 5 consists of an atmospheric general circulation model agcm a data assimilation system a catchment based land surface model and an ocean model all coupled together using the earth system modeling framework esmf rienecker et al 2011 molod et al 2012 ott et al 2015 the agcm utilizes the finite volume dynamic core developed by lin 2004 and the geos 5 column physics package rienecker et al 2008 geos 5 has been employed to produce the merra 2 rienecker et al 2011 gelaro and coauthors 2017 a reanalysis meteorological data product constrained by various observations merra 2 bears a horizontal resolution of 0 5 latitude by 0 625 longitude with 72 vertical levels extending from surface to 0 01 hpa for the present study surface temperature precipitation and solar radiation from merra 2 were used to drive casa to compute terrestrial carbon fluxes meteorological fields such as wind temperature and radiation out of merra 2 together with the carbon fluxes out of casa and other sources were used to drive pctm to generate the temporospatial distributions of co2 mixing ratio over the globe merra 2 meteorology was also utilized to provide the initial and boundary conditions to constrain nu wrf this way the entire modeling system was operated under an internally consistent driving force 2 2 casa gfed casa gfed model is derived from potter et al 1993 and evolved into the current state through multiple additions and improvements e g randerson et al 1996 1997 van der werf et al 2006 2010 the current casa gfed includes physiological processes of carbon uptake by photosynthesis and carbon release through respiration and fires respiration and fires are coupled to plant productivity through carbon pool buildup drought impact on carbon flux is dealt with by a parameterization dependent mostly on precipitation casa gfed is driven by monthly meteorological data from merra 2 and a number of satellite derived products including modis vegetation classification and burned areas giglio et al 2006 2010 as well as the fraction of photosynthetically active radiation fpar out of the advanced very high resolution radiometer avhrr los et al 2000 pinzon and tucker 2014 at a 0 5 by 0 5 resolution the generated monthly carbon fluxes are then disaggregated to 3 hourly values using merra 2 s 3 h averaged temperature at 2 m and surface downward solar radiation olsen and randerson 2004 together with daily satellite fire detections mu et al 2011 casa gfed outputs modeling results have extensively been evaluated in the community and employed widely in global co2 analysis e g peters et al 2007 feng et al 2011 casa gfed model conserves carbon mass balance and is highly constrained by satellite observations of vegetation activity and coverage in general the biosphere is close to neutral in the casa gfed simulation i e there is no long term net carbon sink although there can be interannual variations in the balance between uptake and release randerson et al 1996 in the version of casa used here a sink of 100 tg c yr 1 is induced by crop harvest in the u s midwest that is prescribed based on national agriculture statistics service data on crop area and harvest 2 3 pctm nasa s goddard space flight center gsfc parameterized chemistry and transport model pctm is a proven robust model in forward simulation of global co2 distributions kawa et al 2004 2010 law et al 2008 and inverse estimation of co2 fluxes baker et al 2006 2010 butler et al 2010 wang et al 2018 the forward simulation used in this study bore the same horizontal resolutions 0 50 latitude x 0 625 longitude and vertical pressure level 56 grids as merra 2 some of the most upper levels are merged to reduce computation with little impact on co2 mixing ratio the pctm outputs are used together with merra 2 meteorology as the initial and lateral boundary conditions to drive the regional model the inputs to pctm include the 3 h averaged winds convective mass fluxes and scalar diffusivity from merra 2 the terrestrial 3 h averaged co2 fluxes modulated from the casa gfed monthly net primary production and heterotrophic respiration the biomass burning co2 fluxes based on modis and casa gfed the monthly fossil fuel co2 fluxes from the carbon dioxide information analysis center cdiac at 1 1 resolution andres et al 2016 and oceanic co2 fluxes from takahashi et al 2009 all of above carbon flux components were combined together into one single input file for the forward simulation in this study note that pctm in this configuration simulates only atmospheric co2 transport without the chemical transformations of volatile organic compounds and co into co2 2 4 nu wrf the nu wrf modeling system has been developed at gsfc under continuous institutional support z tao et al 2013 peters lidard et al 2015 it is an observation driven integrated regional modeling system that treats chemistry aerosol cloud precipitation and land processes explicitly at satellite resolved spatial scales typically 1 20 km to bridge the continuum among local microscale regional mesoscale and global synoptic processes it is constructed as the superset of the community wrf skamarock et al 2008 that unifies and incorporates the gsfc land information system lis kumar et al 2006 peters lidard et al 2007 the goddard chemistry aerosol radiation and transport gocart with dynamic dust emissions chin et al 2002 kim et al 2017 the goddard radiation and microphysics schemes shi et al 2014 and the goddard satellite data simulator unit g sdsu matsui et al 2013 2014 lis provides a flexible and satellite based representation of land surface physics and states e g soil and vegetation that are directly coupled to the atmosphere g sdsu allows conversions of modeled parameters to radiance and backscatter for direct comparison with satellite level 1 data at relevant scales similar to community wrf nu wrf employs a eulerian dynamical core and supports a wide array of microphysics radiation land surface planetary boundary layer pbl and cumulus schemes as well as gas aerosol chemistry and their interactions with cloud and radiation the modular design provides the flexibility and expansibility to couple with casa gfed to simulate co2 flux and transport 2 5 model coupling the coupling of casa gfed pctm with nu wrf follows the same nu wrf design philosophy peters lidard et al 2015 i e 1 meet the scientific requirement that is to investigate carbon flux transport at high resolution to better characterize carbon sources sinks for more reliable climate projections 2 maintain computational efficacy and efficiency and 3 optimize code changes to ease merges into the community wrf following this strategy we have implemented the offline coupling approach in which nu wrf receives casa gfed and other source i e oceanic and anthropogenic co2 fluxes via the grid data structure co2 fluxes are passed to advection convection and diffusion schemes through the grid data structure as well similar to pctm neither carbon chemical transformation nor carbon vegetation ocean uptake has been considered explicitly in nu wrf these changes are reflected implicitly with the co2 fluxes as input we developed several modules to ingest and transport co2 fluxes in nu wrf as shown in fig 1 casa2wrf module takes the global casa gfed terrestrial and fire co2 fluxes along with the oceanic and anthropogenic including both fossil fuel and biofuel fluxes to map them into each nu wrf grid casa2wrf supports all the map projections that are currently supported in the community wrf including lambert conformal mercator and polar stereographic ones bilinear interpolation is used for spatial mapping the co2 flux tendency field is calculated with a linear interpolation this tendency field is used to linearly allocate co2 fluxes into each nu wrf time step the results are written to a file ready for nu wrf to use casa2wrf also takes the global pctm co2 concentration data and maps them into nu wrf grids spatially and temporally with bilinear and linear interpolation respectively the co2 concentration tendency field is calculated as well the results are written to the initial and lateral boundary condition lbc files respectively for nu wrf simulation similar to the community wrf a namelist file is used for process controls merra2wrf module is a monolithic program customized to process the 6 hourly reanalysis data from merra 2 peters lidard et al 2015 it extracts various 2 dimensional and 3 dimensional meteorological fields from 5 merra 2 output files including air skin temperature pressure geopotential humidity and wind subsequently it writes out the results in binary ready for wrf preprocessing system wps to process this way the merra 2 meteorology is interpolated into nu wrf initial and lateral boundary conditions module casaco2 fluxes is developed within the nu wrf framework to process the co2 fluxes and calculate concentrations it utilizes the same grid data structure as that in the community wrf a new chemical mechanism number to represent this casa gfed and nu wrf coupling is added in the program registry and modification has been done in various existing modules within nu wrf to reflect the change the chemistry driver emissions driver and initial input data driver are all updated to take and process co2 data from various files processed by modules casa2wrf and merra2wrf all the newly developed modules have been migrated into the nu wrf modeling system and managed via the subversion revision control system for future update the enhanced modeling system is called nu wrf casa hereafter 2 6 computational performance we made the following nu wrf simulation to demonstrate the computational cost of running nu wrf casa the test case run was carried out using a 282 x 237 x 50 grid with the 21 km horizontal resolution starting at 00 utc of december 1 and ending at 00 utc of december 3 over the contiguous u s the running time step was 60 s the model was configured with the goddard microphysics and radiation schemes shi et al 2014 lis noah land surface scheme ek et al 2003 kumar et al 2006 peters lidard et al 2007 monin obukhov surface layer scheme the yonsei university ysu planetary boundary layer scheme hong et al 2006 hong 2010 and new grell cumulus scheme an improved version of grell and devenyi 2002 the initial and lateral boundary conditions were from merra 2 rienecker et al 2011 all runs were conducted on the nasa discover supercomputer using 14 core nodes with the 2 6 ghz intel xeon haswell processor and interconnected with fdr infiniband fig 2 illustrates the model execution time as a function of number of processors for two nu wrf simulations with and without casa gfed coupling it showed that in either case the execution time reduced almost linearly in log scale as the number of processors increased from 28 to 140 in comparison with the basic nu wrf run the run with the casa gfed coupling added approximately 4 6 7 5 overhead with an average of 5 9 more execution time 3 model evaluation nu wrf has been applied to many investigations e g santanello et al 2013a b z tao et al 2013 2015 2016 2018 zaitchik et al 2013 shi et al 2014 w k tao et al 2013 2016 wu et al 2016 and has performed well in meteorological hydrological and atmospheric chemistry simulations therefore this study focused on the evaluation of nu wrf casa performance in simulating spatial and temporal including diurnal seasonal and interannual co2 distributions correlation coefficient bias modeled observation and root mean square error rmse statistics were calculated for model evaluations 3 1 case study setup we conducted a 3 year 2010 2012 simulation over the contiguous u s for evaluations of nu wrf casa the simulation period covers both drought and normal conditions based on observations from the u s drought monitor https www drought gov drought data gallery us drought monitor which allows us to evaluate the model performance on not only diurnal to seasonal variations but also inter annual variations under different environmental stresses the simulation domain is laid on a 282 x 237 21 km resolution grid 50 vertical layers extended from the terrain following surface to 30 hpa the physics package for the simulation was the same as the one used in the computational performance experiment described in section 2 6 the meteorological initial and boundary conditions were derived from merra 2 using the merra2wrf module co2 initial and lateral boundary conditions were based on pctm generated with the casa2wrf module the co2 emissions fluxes were the same as used in pctm section 2 3 the simulation started at 00z december 1st of 2009 and ended on 00z january 1 of 2013 allowing 1 month model spin up to minimize the impact of initial conditions fig 3 illustrates the 3 year average monthly total co2 fluxes from all available sources i e terrestrial oceanic fire and anthropogenic projected to the regional domain using casa2wrf the high co2 emission fluxes were found in major metropolitan areas e g the northeast corridor from boston to washington dc chicago houston los angles and san francisco these fluxes mainly originated from anthropogenic sources and were persistently high throughout the year terrestrial co2 fluxes seen in the great plains were dominated by croplands and various forests and showed a distinct monthly variation reflecting the source sink cycle of the underlying terrestrial system oceanic co2 fluxes fluctuated around zero with a much smaller magnitude than the terrestrial ones areas of fire emissions can be seen in july west of hudson s bay canada fig 4 shows the distribution of 2010 2012 3 year average surface co2 concentrations except for the pacific coastal areas surface co2 generally displayed an increasing gradient from u s west 395 ppmv to east 425 ppmv in response to the anthropogenic carbon flux hot spots surface co2 concentrations appeared to be high in metropolitan areas and along the ohio valley that houses many power plants 3 2 co2 observations the co2 observational data are from the observation package obspack cooperative global atmospheric data integration project 2017 prepared by noaa in consultation with data providers the obspack collects direct atmospheric greenhouse gas measurements from multiple national and university laboratories in a consistent format with care given to make the data as accurate as possible masarie et al 2014 co2 observations from different platforms including in situ tower flask and airplane were selected for model evaluations the locations of each data product are highlighted in fig 4 more detailed site information is listed in tables 1 and 2 a total of 16 stationary sites were selected for model evaluations out of which 12 were tower in situ hourly measurements and 4 were flask observations the sites are located at various altitudes ranging from near sea level to more than 3500 m above sea level masl and represent a spectrum of land covers including water croplands grasslands and forests 13 aircraft flight locations covering different vegetation types were also used for evaluation of modeled vertical profiles and evolution in addition the total column co2 xco2 measurements from the greenhouse gases observing satellite gosat were used for model evaluation the atmospheric co2 observations from space acos gosat tanso fts level 2 bias corrected xco2 v7 3 oco 2 2016 was used in comparison to nu wrf casa xco2 3 3 comparison with observations the modeled either based on pctm or nu wrf casa co2 concentrations at each in situ tower location 3 dimensional based on their latitudes longitudes and intake heights listed in table 1 and sampling time were extracted and compared to the respective observations all hourly values were compared for the continuous in situ measurements and used in the statistical calculation over the years of 2010 2012 fig 5 shows the statistics of comparisons between simulated and observed co2 for pctm and nu wrf casa models it can be seen that in comparison with pctm nu wrf casa reduced co2 bias in 9 out of 12 sites and decreased rmse in 3 sites nu wrf casa and pctm showed comparable correlations with the observations in 9 out of 12 sites with less than 5 difference in correlation coefficients between each other however nu wrf casa as compared with pctm correlated much better to the observations in fsd sgp and snp sites with the respective increases of 15 1 15 4 and 34 in correlation coefficients when considering all site data together the bias out of nu wrf casa was 0 48 ppmv versus 0 71 ppmv for pctm the rmse from nu wrf casa was 5 81 ppmv an approximately 9 decrease from 6 39 ppmv for pctm and the respective correlation coefficients for nu wrf casa and pctm were 0 79 and 0 74 the difference in the simulated results from nu wrf casa and pctm was mainly caused by the underlying meteorology that was responsible for atmospheric co2 transports overall nu wrf casa displays a noticeable improvement over pctm in simulating co2 and much less difference than seen for tm5 and wrf in diaz isaac et al 2014 in the next several sections a more detailed evaluation of nu wrf casa is discussed 3 3 1 in situ tower measurements fig 6 illustrates the three statistical measures in evaluating the nu wrf casa performance at each site categorized by season for all 3 years all 24 h data were utilized for the calculation we defined winter as december january and february spring as march april and may summer as june july and august and fall as september october and november it can be seen that there were variations in bias across the sites and seasons the bias at the same site but in different seasons can be of opposite sign bringing the total annual bias low the sites at various forest locations i e amt etl fsd lef nwr and snp experienced positive biases in winter and spring ranging from 0 44 to 4 61 ppmv except for nwr and snp that are located on mountain ridges the biases in summer and fall for amt etl fsd and lef were either negative 3 21 0 40 ppmv or small positive 0 51 for fsd in fall bringing the annual bias as 0 06 0 85 1 18 and 0 86 ppmv respectively the summer and fall biases at nwr and snp were positive giving these two mountaintop sites positive bias across all four seasons the respective annual biases for nwr and snp were 0 64 and 3 89 ppmv the cropland sites i e bao sct sgp and wbi saw negative biases 2 62 0 18 ppmv in both summer and fall in winter all but bao experienced positive bias 0 33 3 88 ppmv the winter bias of bao was 2 11 ppmv while the ones of bao and sct were negative 0 5 ppmv the spring biases of sgp and wbi were 2 66 and 0 54 ppmv respectively the respective annual biases for bao sct sgp and wbi were 1 66 0 61 0 90 and 0 03 ppmv the large annual negative bias at bao was thought to be linked to the location of the site bao though classified as cropland in the model received a lot of urban pollution plumes from the nearby cities the elevated local anthropogenic fluxes might not be captured the biases among four seasons for the grassland site wkt showed a phase shift with positive negative positive negative bias for winter spring summer fall respectively with an annual bias at 0 38 ppmv nu wrf casa overestimated co2 over the water site wsa in winter spring and summer but underestimated it in fall leading to a total annual bias of 0 36 ppmv taking together all data from 12 sites the largest bias of 1 89 ppmv occurred in winter followed by spring 1 38 ppmv summer 0 70 ppmv and fall 0 66 ppmv the observed biases were comparable to the results by other studies ahmadov et al 2009 uebel et al 2017 in simulations to compare with the coastal tower measurements of co2 over may june using wrf vprm ahmadov et al 2009 reported a bias of 0 67 ppmv in their investigation of co2 variability over a summer week using a high resolution model uebel et al 2017 reported biases ranging from 0 70 ppmv sugar beet cropland to 5 52 ppmv winter barley cropland the annual rmse for the 12 tower sites varied from 2 03 nwr to 8 01 fsd ppmv with the overall rmse i e including data points from all 12 sites at 5 81 ppmv seasonally the largest rmse typically occurred in summer except for bao and wkt that saw the largest rmse in winter and fall respectively this probably was related to the strong photosynthesis drawdown in summer taking together all data from 12 sites the rmses for winter spring summer and fall were 4 98 5 00 7 58 and 5 29 ppmv respectively in general the nu wrf casa modeled co2 time series correlated well with the observations at each tower site the annual correlation coefficients ranged from 0 56 bao to 0 89 lef with the 12 site overall value at 0 79 seasonally the highest correlation typically happened in fall except for amt nwr and sgp all of which found the best correlation in winter the 12 site overall correlation coefficients for winter spring summer and fall were 0 71 0 70 0 73 and 0 79 respectively the nu wrf casa performance at different hours of the day was investigated as well for this purpose we divided each day into 4 periods midnight 23 04 local standard time lst morning 05 10 lst midday 11 16 lst and evening 17 22 lst table 3 lists the results at each time period summarized based on the statistics from each of 12 sites and overall statistics that takes 12 site data together it appeared that nu wrf casa performed the best in terms of the overall rmse and correlation coefficient through the midday into evening hours without exception the smallest rmse and highest correlation coefficients always appeared during the midday or the evening the overall smallest 0 00 largest 0 95 bias however occurred in the midnight midday respectively for individual sites except wkt where the largest negative bias of 0 89 ppmv took place during midnight other sites saw their largest bias occurring either in the morning or in the midday during midday and evening with generally deep pbl the air was relatively well mixed leading to less co2 variability and thus smaller rmse and better correlation at night on the other hand the very shallow pbl and near surface co2 gradient were likely not well resolved by the lowest model layers this explained the larger rmse at midnight and early morning 3 3 2 flask measurements unlike online co2 measurements by in situ instruments deployed at a tower flask measurements are collected and subsequently analyzed in a laboratory they are typically taken near midday at a location representative of large scale background conditions about once per week in this study we compared the nu wrf casa results to the flask measurements from 4 sites table 1 this experiment represented testing the model performance in a more controlled environment according to the noaa protocol flask sampling selects for as much as possible wind directions that avoid local fluxes fig 6 also displays the bias rmse and correlation coefficient at 4 sites with suffix flask for different seasons it was found that the smallest absolute seasonal biases took place during spring across 4 sites while the largest absolute biases appeared in different seasons winter for pta and uta summer for nwr and fall for thd the annual biases were 1 32 2 19 0 15 and 0 59 ppmv for pta thd uta and nwr respectively although not always the smallest the rmses in spring were all relatively small the largest rmse however took place in fall at pta and thd two sites on the pacific coast in winter at uta and in summer at nwr the annual rmses were 4 84 4 99 2 27 and 2 12 ppmv for pta thd uta and nwr respectively the correlation coefficients at two coastal sites i e pta and thd were relatively small as 0 62 and 0 68 they were 0 80 and 0 82 for uta and nwr respectively it was worth noting that the nu wrf casa performances at two inland sites uta and nwr were superior to those at two coastal sites pta and thd the sampling problem might cause this the modeling grid cells along coastal areas generally include both land and water portions therefore the modeled co2 at coastal sites might be affected by significant terrestrial fluxes which were typically not reflected in the flask measurements this may explain the large negative biases found in pta and thd 3 3 3 aircraft flight profiles aircraft measurements provide not only horizontal but also vertical co2 distributions with time this 4 dimensional dataset is extremely valuable in a full fledged evaluation of model performance on co2 transport simulations note that the aircraft measurements are almost all taken near midday when the pbl is vertically well mixed and the impact of sub grid scale fluxes is diminished in this study a total of 13 flight measurement locations table 2 were selected for nu wrf casa evaluation these measurements covered various landscapes at different seasons and times within the years of 2010 2012 the modeled co2 concentrations were interpolated to match the flight latitude longitude altitude and time the results are summarized in table 4 it can be found that statistically nu wrf casa simulated the 4 dimensional co2 distribution well across the 13 flight locations the modeled concentrations were highly correlated to the measured ones with correlation coefficients ranging from 0 79 sgp and thd to 0 94 etl the model biases were small varying between 0 35 tgc and 0 50 thd ppmv the rmse fluctuated from 1 45 car to 5 51 ppmv inx taking together all data from the 13 flight locations the bias rmse and correlation coefficients were 0 03 ppmv 2 61 ppmv and 0 88 respectively there were 4 locations etl lef sgp and thd with both ground in situ tower or flask and aircraft measurements without exception statistically the modeled results compared better to the aircraft observations than to the ground measurements this was expected since many flight observations were taken in the free troposphere where co2 were well mixed and more represented as the background concentration the model performance along altitudes was evaluated as well we grouped the flight measurements into 17 altitude intervals from surface to above 7000 masl there were 8 levels with 250 m intervals from surface to 2000 masl 6 levels with 500 m intervals from 2000 to 5000 masl 2 levels with 1000 m intervals from 5000 to 7000 masl and the one above 7000 masl the measurement numbers at each sampling level ranged from 240 below 250 masl level to 800 2000 2500 masl level the averages of flight measurements within each level were compared with the same level average modeled results the statistics of bias rmse and correlation coefficient are illustrated in fig 7 nu wrf casa generally yielded positive biases up to 0 9 ppmv at the low altitude within 1000 masl agreeing with the finding by ter maat et al 2010 who reported higher modeled co2 concentrations near surface likely due to a lower estimated pblh the bias turned to negative between 1000 and 3500 masl before oscillating around 0 from 3500 to 5000 masl very small negative biases less than 0 1 ppmv then appeared above 6000 masl in any altitude though the model bias was small as a comparison sarrat et al 2007 a b reported a model observation co2 discrepancy of up to 4 ppmv along the vertical profile when evaluating their regional model using the carboeurope regional experiment strategy ceres flight observations rmse decreased with altitude from around 5 ppmv near surface to about 1 8 ppmv at 3500 masl and leveled off at approximately 1 6 ppmv above 4000 masl this was anticipated because the long lived co2 was more homogeneously distributed at high altitudes where there were no major co2 sources and sinks the correlation coefficient ranged between 0 83 and 0 91 across the altitudes 3 3 4 gosat measurements a two step method was employed to derive the nu wrf casa xco2 first the collocated nu wrf casa co2 profiles were vertically extended using the interpolated profiles from pctm grafted above the top of the nu wrf casa model domain 30 hpa then the modeled xco2 was calculated from the resulting profiles using the gosat reported averaging kernels and a priori profile vertical weighting fig 8 illustrates the comparison of nu wrf casa modeled xco2 to the gosat xco2 in july in general the model gradients were well correlated r 0 768 with the observations across the domain and the bias 0 35 ppmv was small the gosat data were more variable than the model xco2 and the range of values was larger the correlations were poor in january figure not shown however when the synoptic gradients in xco2 were small and the variability in gosat appeared to be dominated by small scale processes controlling co2 fluxes and or noise 3 3 5 drought vs normal conditions drought puts stress on plant morphology physiology and photosynthesis fahad et al 2017 and the references therein which consequently impacts the vegetation carbon fluxes and atmospheric co2 concentration the model s capability to simulate atmospheric co2 under drought vs normal conditions was also evaluated in this study according to the data by u s drought monitor https www drought gov drought data gallery us drought monitor texas and the southern great plain areas suffered extreme to exceptional drought starting from early 2011 and throughout the entire year another extreme drought hit the southern great plain areas in the summer of 2012 and lasted into 2013 within the 12 tower sites selected for the model evaluation sgp and wkt were in the areas impacted by the aforementioned drought spells net ecosystem exchange nee or net vegetation carbon flux variability can be driven by either ecosystem production or respiration a recent study by z liu et al 2018 finds that the inter annual variability of nee is primarily controlled by production if the annual precipitation is below 950 mm in the contiguous u s the climatological annual precipitation at both sgp and wkt is approximately 900 mm https www bestplaces net climate and thus the inter annual change of vegetation carbon flux is likely driven by production at these two sites therefore the growing season april october has been selected for analysis we chose the periods marked as extreme or exceptional drought to represent the drought spells as such the correlation coefficient bias and rmse statistics were computed for wkt in the period between april 1 and october 31 2011 and for spg in the periods including may 15 october 31 of 2011 and july 20 october 31 of 2012 the results were then compared to the same statistics computed for the normal non drought conditions periods in the growing season for wkt april october of 2010 and 2012 and sgp april october of 2010 and april june of 2012 although the driving forces for these two droughts varied e g j liu et al 2018 their impacts on terrestrial fluxes were apparent as shown in fig 9 at sgp the average vegetation carbon flux over the drought period of the growing season was 3 14 x 10 9 kg m 2 s 1 with a standard deviation of 7 83 x 10 9 kg m 2 s 1 while the average and standard deviation were 5 99 x 10 9 kg m 2 s 1 and 1 81 x 10 8 kg m 2 s 1 respectively over the normal period at wkt the respective average carbon fluxes over the drought and normal periods were 1 43 x 10 9 and 2 91 x 10 9 kg m 2 s 1 while the respective standard deviations were 1 16 x 10 8 and 1 71 x 10 8 kg m 2 s 1 it can be seen that drought increased the net carbon fluxes at both sites which was likely due to less vegetation primary production according to z liu et al 2018 the results also revealed that net carbon flux was less variable during drought spells than during normal growth periods fig 10 displays the comparisons of daily average co2 concentrations from nu wrf casa and observations during drought and normal conditions for sgp and wkt the model performance was similar under two different environmental conditions the correlation coefficients were slightly better under the drought condition than under the normal condition for both sgp 0 58 vs 0 54 and wkt 0 83 vs 0 72 at sgp both bias 1 43 vs 1 79 ppmv and rmse 5 51 vs 6 26 ppmv were slightly better under drought than under normal conditions at wkt while the rmse was lower under drought 2 93 ppmv than under normal 3 12 ppmv conditions the absolute bias was smaller under normal 0 49 ppmv than under drought 1 67 ppmv conditions data from wkt showed two distinct clusters reflecting the drought and normal conditions while data from sgp lacked such feature fig 10 this may relate to the land use type wkt was classified as grasslands as such it was more susceptible to drought stress and thus the resulting ambient atmospheric co2 concentrations would more reflect the distinct micro environment under drought and normal conditions on the other hand sgp was classified as croplands and subject to human interference e g irrigation when drought struck as such the resulting ambient co2 concentrations would be less responsive to the drought stress and thus less distinguishable from drought to normal conditions 3 3 6 meteorology and model observation discrepancy to explore the possible causes of co2 model measurement discrepancies we analyzed the 3 year meteorology at selected sites it should be noted that this analysis was not intended to be exhaustive but rather to provide some insights into the model deficiencies and suggestions to improve the model we selected the local climatological data lcd archived by noaa s national center for environmental information ncei https www ncdc noaa gov cdo web datatools lcd for nu wrf casa meteorology evaluation the selected 4 sites wkt sgp wbi and lef span from the south to north with land covers of grasslands croplands croplands and mixed forest respectively since co2 is a long lived atmospheric trace gas its temporal and spatial variations at a synoptic scale are mainly driven by surface source sink variations and pbl processes particularly pblh and wind play important roles in determining co2 distributions in the boundary layer pblh is not readily available from observations but its growth is primarily driven by buoyancy resulting from surface sensible heating as confirmed by the observations from the amma campaign kohler et al 2010 in nu wrf casa temperature at 2 m t2 is a diagnostic variable and estimated as a function of sensible heat flux w k tao et al 2013 z tao et al 2013 therefore t2 can be viewed as a proxy of sensible heating and thus of pblh figs 11 and 12 display the comparisons between nu wrf casa modeled and observed t2 and wind speed at the selected sites respectively it was readily seen that nu wrf casa captured the temporal and spatial variations of both wind and t2 reasonably well the correlation coefficients for wind ranged from 0 76 sgp to 0 87 wkt with the bias within 0 7 m s 1 modeled t2 correlated with the observations especially well with the correlation coefficients being at least 0 98 however nu wrf casa showed a cold bias across all 4 sites with the underestimated t2 ranging from 0 96 wkt and wbi to 1 18 k sgp and lef this cold t2 bias was expected to lead to a shallower pblh indeed the nu wrf casa simulated pblh was less than that from merra 2 at these sites and times we further carried out meteorology and co2 flux analyses for each season to understand the model observation discrepancies shown in section 3 3 1 we compared the season when the seasonal average total carbon flux was positive source and when the seasonal average total carbon flux was negative sink table 5 summarizes the seasonal bias of t2 pblh wind speed co2 level and source sink classification at the selected sites it was interesting to find that pblh was at least partially responsible for the co2 model measurement discrepancies in most cases approximately 70 of 16 cases showing in red in table 5 for example wkt was a carbon source in winter as such the shallower pblh would allow more co2 accumulation in pbl resulting in a positive 0 81 ppmv bias in spring wkt changed to a carbon sink the shallower plbh would reduce the co2 mixing down from the free troposphere leading to a negative 0 95 ppmv bias in summer wkt became a carbon source the modeled shallower plbh again caused a positive 0 49 ppmv co2 bias a similar explanation was applicable to the other sites seasons highlighted in red in table 5 with the aforementioned reasoning the fall season across the 4 sites and spring in sgp should have had the biases in the opposite direction this indicated that sources other than pblh uncertainty may play a larger role in determining the model observation discrepancy here one such uncertainty was the variability of carbon sources sinks fall and spring were the transition seasons in which the net carbon source or sink was in transition for example we defined spring as a carbon sink season at sgp based on the seasonal average total carbon flux a close look revealed that among 9 spring months throughout 2010 2012 there were 5 carbon source months and 4 carbon sink months at sgp similar carbon source sink switches among months in fall season across 4 sites were found as well the uncertainty in carbon source sink variations would definitely cause variations in atmospheric co2 simulation in summary uncertainties in both carbon source sink variations and meteorology lead to uncertainties in co2 simulation in nu wrf particularly pblh representation had a prominent and direct impact on atmospheric co2 distributions pblh strongly depends on the sensible heat flux which highlights the importance to improve modeling of the land surface processes 4 summary modeling co2 at fine spatial resolution assists in better understanding the mechanisms and processes controlling carbon sources sinks and atmospheric co2 transport variability since many of these processes occur at relatively small spatial and or rapid temporal scales knowledge gained from such mechanism process studies is anticipated to lead to reduction of uncertainty in carbon climate interactions in this study we expanded the capability of the nu wrf model to simulate co2 transport and variability at fine spatial scales to achieve this goal nu wrf has been coupled with casa gfed and pctm in an offline manner together with the already built in capability of nu wrf coupling with merra 2 the newly developed nu wrf casa modeling system is capable of simulating co2 fluxes and transport in an internally consistent manner in which the global geos 5 merra 2 meteorology drives casa gfed pctm and nu wrf taking advantage of the process based casa gfed and high resolution nu wrf the new modeling system provides a flexible platform to investigate carbon fluxes and transport at fine scales and at process levels various interfaces modules and command options have been developed under the nu wrf framework coupling of casa gfed added approximately 4 6 7 5 computational overhead with an average of 5 9 more execution time in comparison with the stand alone nu wrf simulation a 3 year simulation covering multiple landscapes topographies and environmental conditions has been carried out to evaluate the model performance observational data collected from in situ tower flask aircraft and satellite measurements were employed to gauge the capability in simulating inter annual seasonal and diurnal variabilities of atmospheric co2 across a range of sites under both drought and normal conditions the results showed that nu wrf casa performed reasonably well in reproducing co2 transport and spatial temporal variabilities with overall bias less than 1 ppmv and correlation coefficient more than 0 6 it was further found that pblh had a prominent impact on atmospheric co2 levels and could directly link to approximately 70 of the cases of the co2 model observation discrepancies the close relationship between pblh and sensible heating emphasized the importance in improving model land surface processes to improve simulations of regional co2 distribution in the near future and taking advantage of the rich observational data already and to be collected from the ongoing and upcoming satellite missions critical to earth observing such as joint polar satellite system jpss orbiting carbon observatory 2 and 3 oco 2 3 geostationary carbon cycle observatory geocarb and tropospheric emissions monitoring pollution tempo nu wrf casa will continuously be improved on its physics and prediction skills the ultimate goal is to reduce uncertainties in simulating carbon climate interactions through the knowledge gained from simulation of co2 fluxes and transport using the fine scale and more detailed process representations available in nu wrf casa e g land surface physics soil moisture boundary layer dynamics and convective transport declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the development and evaluation of nu wrf casa was funded by nasa s modeling analysis and prediction map program solicitation nnh12zda001n map the authors would like to thank nasa center for climate simulation nccs for supercomputing and data storage support as well as noaa s cooperative global atmospheric data integration project ncgadip who organizes and distributes co2 data thanks are also due to each individual co2 data providers who submit their data to ncgadip including beth anderson nws arlyn andrews noaa dan baumann usgs sebastien biraud lbnl arm ken davis psu ankur desai uofwi ed dlugokencky noaa ralph keeling sio john lee uofme natasha miles psu jen morse instaar matt parker srnl scott richardson psu charles stanier uofia britton stephens ncar colm sweeney noaa margaret torn lbnl stephan de wekker uofva dan wolfe noaa psd doug worthy ec and point arena lighthouse keepers inc last but not least thanks go to gosat project and goddard earth sciences data and information services center ges disc for data collection and archive in compliance with fair data standards all data collected and generated for this research are archived and stored on nccs servers due to the sheer size of data 3 tb it is impractical to upload data to a public domain repository however the authors will be happy to share data on an individual request basis appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104785 
25980,the nasa unified weather research and forecasting model has been coupled with the carnegie ames stanford approach biogeochemical model global fire emissions database and the parameterized chemistry transport model nu wrf casa to simulate co2 transport and variability at fine spatial resolutions which is anticipated to assist in better understanding the mechanisms and processes controlling carbon sources sinks and in reducing uncertainty in carbon climate interactions nu wrf casa is designed to operate in an internally consistent manner in which nasa s goddard earth observing system model version 5 can be applied to drive the entire coupled modeling system a 3 year simulation was carried out for north america to evaluate nu wrf casa s performance under various regions and environmental conditions observational co2 data from tower flask aircraft and satellite measurements were used for model evaluations the results showed that nu wrf casa correctly distributed co2 fluxes and reproduced spatial temporal co2 transport and variability reasonably well with overall bias within 1 ppmv 1 introduction approximately one half of the anthropogenic emission of carbon dioxide co2 to the atmosphere is currently removed from the atmosphere by the air sea flux and land atmosphere flux interannual variability in this flux is large and the magnitude location and mechanisms producing the co2 sink are not well determined ipcc 2018 usgcrp 2018 as a result carbon climate interaction is among the leading sources of uncertainty in prediction of future climate cox et al 2000 dufresne et al 2002 friedlingstein et al 2014 in particular the inferred terrestrial carbon sink and its variability spatial distribution and dependence on environmental conditions must be better characterized in order to assess how surface uptake and release of co2 will evolve with changing fossil fuel emissions land use and climate in coming decades ipcc 2018 and references within part of the problem in understanding these mechanisms is that key processes controlling co2 fluxes and hence local co2 mixing ratio variability occur at relatively small spatial and or rapid temporal scales examples include photosynthetic dependence on sunlight and soil moisture temperature dependence of above ground and soil respiration and plant cover and phenology small scales are similarly important for fossil fuel emissions from point sources urban and or industrial areas and transportation as well as biomass burning because surface fluxes are generally inferred from gradients in atmospheric co2 the problem is further complicated by transport in atmospheric boundary layers and frontal systems that can concentrate or reduce mixing ratio gradients produced by fluxes which are themselves often correlated with the local weather e g parazoo et al 2011 co2 flux and abundance heterogeneity are so large that it becomes very difficult to integrate the net of these processes in time and space to global regional or even landscape scales yet it is precisely the small shifts in balance between photosynthesis and respiration during the mid latitude seasonal cycle that are hypothesized to drive a large part of the global net terrestrial sink friedlingstein et al 1995 as a result global prognostic models which have been developed from sparse observational evidence diverge greatly and bottom up flux inferences don t agree with top down ones e g gourdji et al 2012 inferring fluxes from observations in either forward or inverse models is hindered by the stiffness of the system owing to limited model resolution in addressing the aforementioned scale problems efforts have been made to develop high resolution regional or mesoscale carbon flux transport models ahmadov et al 2007 coupled a diagnostic biosphere model vegetation photosynthesis and respiration model vprm mahadevan et al 2008 to the community weather research and forecasting wrf model skamarock et al 2008 to investigate the co2 flux and transport at resolutions of a few tens of kilometers km vprm is an empirical model calibrated to measurements of eddy covariance flux towers and extrapolated across the landscape in wrf vprm the co2 net ecosystem exchange nee flux is derived using the enhanced vegetation index and land surface water index from the moderate resolution imaging spectroradiometer modis and the shortwave radiation and air temperature from wrf wrf vprm has shown better performance than two global models in reproducing the coastal tower co2 measurements ahmadov et al 2009 using wrf as the modeling framework ballav et al 2012 simulated the co2 concentration over east asia at the 27 27 km horizontal resolution and diaz isaac et al 2014 compared the simulated co2 at a 10 10 km resolution with the measurements from the mid continental intensive mci field campaign both of which utilized the terrestrial carbon flux from stand alone biosphere models similar efforts employing other mesoscale transport models to investigate co2 at high spatial resolutions include the online atmosphere biosphere coupling works by sarrat et al 2007a b and the offline coupling works by ter maat et al 2010 and uebel et al 2017 all of which are based on short episodic simulations geared toward various field campaigns inverse modeling has also been progressing toward higher resolutions especially for regional models for example wang et al 2014 used a north american regional inversion system with meteorology from wrf run at 40 km resolution coupled to the stochastic time inverted lagrangian transport stilt model run at 1 1 flux resolution to evaluate the flux uncertainty reduction that could be provided by the proposed active sensing of co2 emissions over nights days and seasons ascends satellite mission the works by different research teams demonstrate the value of high resolution co2 simulation in characterizing carbon sources sinks and improving carbon flux inversions this paper presents a model development and evaluation effort to integrate the carnegie ames stanford approach casa biogeochemical model randerson et al 1996 1997 van der werf et al 2006 2010 to nasa unified wrf nu wrf z tao et al 2013 2016 peters lidard et al 2015 it is similar to the studies by ballav et al 2012 and diaz isaac et al 2014 but with a greater spatial temporal scale and scope of analysis the goal is to extend the capability of nu wrf an institutional regional modeling and assimilation system to investigate carbon fluxes and transport at satellite resolved spatial scales 1 20 km the paper is organized as follows section 2 describes the modeling system and development followed by model evaluation in section 3 different from most existing studies whose experiments focus on relatively small regions at weeks to months temporal scales this work evaluates the model performance over a relatively large region covering various landscapes topographies and climate zones over a relatively long period 3 years including various environmental conditions such as drought conclusions are summarized in section 4 2 model development there are four major components in the modeling system 1 nasa s goddard earth observing system model version 5 geos 5 that includes the production of nasa s modern era retrospective analysis for research and applications version 2 merra 2 2 casa and global fire emissions database casa gfed 3 parameterized chemistry transport model pctm and 4 nu wrf supplement figure 1 s shows the relationship of four components in which geos 5 merra 2 provides meteorology to drive casa gfed and pctm as well as furnishes meteorological lateral boundary conditions lbcs to constrain nu wrf casa gfed generates terrestrial carbon fluxes to feed into both pctm and nu wrf for co2 simulations since they had been well developed and evaluated only brief introduction was given to each component in sections 2 1 through 2 4 the interface and modules newly developed to couple these four components were presented with details in sections 2 5 and 2 6 2 1 geos 5 merra 2 geos 5 consists of an atmospheric general circulation model agcm a data assimilation system a catchment based land surface model and an ocean model all coupled together using the earth system modeling framework esmf rienecker et al 2011 molod et al 2012 ott et al 2015 the agcm utilizes the finite volume dynamic core developed by lin 2004 and the geos 5 column physics package rienecker et al 2008 geos 5 has been employed to produce the merra 2 rienecker et al 2011 gelaro and coauthors 2017 a reanalysis meteorological data product constrained by various observations merra 2 bears a horizontal resolution of 0 5 latitude by 0 625 longitude with 72 vertical levels extending from surface to 0 01 hpa for the present study surface temperature precipitation and solar radiation from merra 2 were used to drive casa to compute terrestrial carbon fluxes meteorological fields such as wind temperature and radiation out of merra 2 together with the carbon fluxes out of casa and other sources were used to drive pctm to generate the temporospatial distributions of co2 mixing ratio over the globe merra 2 meteorology was also utilized to provide the initial and boundary conditions to constrain nu wrf this way the entire modeling system was operated under an internally consistent driving force 2 2 casa gfed casa gfed model is derived from potter et al 1993 and evolved into the current state through multiple additions and improvements e g randerson et al 1996 1997 van der werf et al 2006 2010 the current casa gfed includes physiological processes of carbon uptake by photosynthesis and carbon release through respiration and fires respiration and fires are coupled to plant productivity through carbon pool buildup drought impact on carbon flux is dealt with by a parameterization dependent mostly on precipitation casa gfed is driven by monthly meteorological data from merra 2 and a number of satellite derived products including modis vegetation classification and burned areas giglio et al 2006 2010 as well as the fraction of photosynthetically active radiation fpar out of the advanced very high resolution radiometer avhrr los et al 2000 pinzon and tucker 2014 at a 0 5 by 0 5 resolution the generated monthly carbon fluxes are then disaggregated to 3 hourly values using merra 2 s 3 h averaged temperature at 2 m and surface downward solar radiation olsen and randerson 2004 together with daily satellite fire detections mu et al 2011 casa gfed outputs modeling results have extensively been evaluated in the community and employed widely in global co2 analysis e g peters et al 2007 feng et al 2011 casa gfed model conserves carbon mass balance and is highly constrained by satellite observations of vegetation activity and coverage in general the biosphere is close to neutral in the casa gfed simulation i e there is no long term net carbon sink although there can be interannual variations in the balance between uptake and release randerson et al 1996 in the version of casa used here a sink of 100 tg c yr 1 is induced by crop harvest in the u s midwest that is prescribed based on national agriculture statistics service data on crop area and harvest 2 3 pctm nasa s goddard space flight center gsfc parameterized chemistry and transport model pctm is a proven robust model in forward simulation of global co2 distributions kawa et al 2004 2010 law et al 2008 and inverse estimation of co2 fluxes baker et al 2006 2010 butler et al 2010 wang et al 2018 the forward simulation used in this study bore the same horizontal resolutions 0 50 latitude x 0 625 longitude and vertical pressure level 56 grids as merra 2 some of the most upper levels are merged to reduce computation with little impact on co2 mixing ratio the pctm outputs are used together with merra 2 meteorology as the initial and lateral boundary conditions to drive the regional model the inputs to pctm include the 3 h averaged winds convective mass fluxes and scalar diffusivity from merra 2 the terrestrial 3 h averaged co2 fluxes modulated from the casa gfed monthly net primary production and heterotrophic respiration the biomass burning co2 fluxes based on modis and casa gfed the monthly fossil fuel co2 fluxes from the carbon dioxide information analysis center cdiac at 1 1 resolution andres et al 2016 and oceanic co2 fluxes from takahashi et al 2009 all of above carbon flux components were combined together into one single input file for the forward simulation in this study note that pctm in this configuration simulates only atmospheric co2 transport without the chemical transformations of volatile organic compounds and co into co2 2 4 nu wrf the nu wrf modeling system has been developed at gsfc under continuous institutional support z tao et al 2013 peters lidard et al 2015 it is an observation driven integrated regional modeling system that treats chemistry aerosol cloud precipitation and land processes explicitly at satellite resolved spatial scales typically 1 20 km to bridge the continuum among local microscale regional mesoscale and global synoptic processes it is constructed as the superset of the community wrf skamarock et al 2008 that unifies and incorporates the gsfc land information system lis kumar et al 2006 peters lidard et al 2007 the goddard chemistry aerosol radiation and transport gocart with dynamic dust emissions chin et al 2002 kim et al 2017 the goddard radiation and microphysics schemes shi et al 2014 and the goddard satellite data simulator unit g sdsu matsui et al 2013 2014 lis provides a flexible and satellite based representation of land surface physics and states e g soil and vegetation that are directly coupled to the atmosphere g sdsu allows conversions of modeled parameters to radiance and backscatter for direct comparison with satellite level 1 data at relevant scales similar to community wrf nu wrf employs a eulerian dynamical core and supports a wide array of microphysics radiation land surface planetary boundary layer pbl and cumulus schemes as well as gas aerosol chemistry and their interactions with cloud and radiation the modular design provides the flexibility and expansibility to couple with casa gfed to simulate co2 flux and transport 2 5 model coupling the coupling of casa gfed pctm with nu wrf follows the same nu wrf design philosophy peters lidard et al 2015 i e 1 meet the scientific requirement that is to investigate carbon flux transport at high resolution to better characterize carbon sources sinks for more reliable climate projections 2 maintain computational efficacy and efficiency and 3 optimize code changes to ease merges into the community wrf following this strategy we have implemented the offline coupling approach in which nu wrf receives casa gfed and other source i e oceanic and anthropogenic co2 fluxes via the grid data structure co2 fluxes are passed to advection convection and diffusion schemes through the grid data structure as well similar to pctm neither carbon chemical transformation nor carbon vegetation ocean uptake has been considered explicitly in nu wrf these changes are reflected implicitly with the co2 fluxes as input we developed several modules to ingest and transport co2 fluxes in nu wrf as shown in fig 1 casa2wrf module takes the global casa gfed terrestrial and fire co2 fluxes along with the oceanic and anthropogenic including both fossil fuel and biofuel fluxes to map them into each nu wrf grid casa2wrf supports all the map projections that are currently supported in the community wrf including lambert conformal mercator and polar stereographic ones bilinear interpolation is used for spatial mapping the co2 flux tendency field is calculated with a linear interpolation this tendency field is used to linearly allocate co2 fluxes into each nu wrf time step the results are written to a file ready for nu wrf to use casa2wrf also takes the global pctm co2 concentration data and maps them into nu wrf grids spatially and temporally with bilinear and linear interpolation respectively the co2 concentration tendency field is calculated as well the results are written to the initial and lateral boundary condition lbc files respectively for nu wrf simulation similar to the community wrf a namelist file is used for process controls merra2wrf module is a monolithic program customized to process the 6 hourly reanalysis data from merra 2 peters lidard et al 2015 it extracts various 2 dimensional and 3 dimensional meteorological fields from 5 merra 2 output files including air skin temperature pressure geopotential humidity and wind subsequently it writes out the results in binary ready for wrf preprocessing system wps to process this way the merra 2 meteorology is interpolated into nu wrf initial and lateral boundary conditions module casaco2 fluxes is developed within the nu wrf framework to process the co2 fluxes and calculate concentrations it utilizes the same grid data structure as that in the community wrf a new chemical mechanism number to represent this casa gfed and nu wrf coupling is added in the program registry and modification has been done in various existing modules within nu wrf to reflect the change the chemistry driver emissions driver and initial input data driver are all updated to take and process co2 data from various files processed by modules casa2wrf and merra2wrf all the newly developed modules have been migrated into the nu wrf modeling system and managed via the subversion revision control system for future update the enhanced modeling system is called nu wrf casa hereafter 2 6 computational performance we made the following nu wrf simulation to demonstrate the computational cost of running nu wrf casa the test case run was carried out using a 282 x 237 x 50 grid with the 21 km horizontal resolution starting at 00 utc of december 1 and ending at 00 utc of december 3 over the contiguous u s the running time step was 60 s the model was configured with the goddard microphysics and radiation schemes shi et al 2014 lis noah land surface scheme ek et al 2003 kumar et al 2006 peters lidard et al 2007 monin obukhov surface layer scheme the yonsei university ysu planetary boundary layer scheme hong et al 2006 hong 2010 and new grell cumulus scheme an improved version of grell and devenyi 2002 the initial and lateral boundary conditions were from merra 2 rienecker et al 2011 all runs were conducted on the nasa discover supercomputer using 14 core nodes with the 2 6 ghz intel xeon haswell processor and interconnected with fdr infiniband fig 2 illustrates the model execution time as a function of number of processors for two nu wrf simulations with and without casa gfed coupling it showed that in either case the execution time reduced almost linearly in log scale as the number of processors increased from 28 to 140 in comparison with the basic nu wrf run the run with the casa gfed coupling added approximately 4 6 7 5 overhead with an average of 5 9 more execution time 3 model evaluation nu wrf has been applied to many investigations e g santanello et al 2013a b z tao et al 2013 2015 2016 2018 zaitchik et al 2013 shi et al 2014 w k tao et al 2013 2016 wu et al 2016 and has performed well in meteorological hydrological and atmospheric chemistry simulations therefore this study focused on the evaluation of nu wrf casa performance in simulating spatial and temporal including diurnal seasonal and interannual co2 distributions correlation coefficient bias modeled observation and root mean square error rmse statistics were calculated for model evaluations 3 1 case study setup we conducted a 3 year 2010 2012 simulation over the contiguous u s for evaluations of nu wrf casa the simulation period covers both drought and normal conditions based on observations from the u s drought monitor https www drought gov drought data gallery us drought monitor which allows us to evaluate the model performance on not only diurnal to seasonal variations but also inter annual variations under different environmental stresses the simulation domain is laid on a 282 x 237 21 km resolution grid 50 vertical layers extended from the terrain following surface to 30 hpa the physics package for the simulation was the same as the one used in the computational performance experiment described in section 2 6 the meteorological initial and boundary conditions were derived from merra 2 using the merra2wrf module co2 initial and lateral boundary conditions were based on pctm generated with the casa2wrf module the co2 emissions fluxes were the same as used in pctm section 2 3 the simulation started at 00z december 1st of 2009 and ended on 00z january 1 of 2013 allowing 1 month model spin up to minimize the impact of initial conditions fig 3 illustrates the 3 year average monthly total co2 fluxes from all available sources i e terrestrial oceanic fire and anthropogenic projected to the regional domain using casa2wrf the high co2 emission fluxes were found in major metropolitan areas e g the northeast corridor from boston to washington dc chicago houston los angles and san francisco these fluxes mainly originated from anthropogenic sources and were persistently high throughout the year terrestrial co2 fluxes seen in the great plains were dominated by croplands and various forests and showed a distinct monthly variation reflecting the source sink cycle of the underlying terrestrial system oceanic co2 fluxes fluctuated around zero with a much smaller magnitude than the terrestrial ones areas of fire emissions can be seen in july west of hudson s bay canada fig 4 shows the distribution of 2010 2012 3 year average surface co2 concentrations except for the pacific coastal areas surface co2 generally displayed an increasing gradient from u s west 395 ppmv to east 425 ppmv in response to the anthropogenic carbon flux hot spots surface co2 concentrations appeared to be high in metropolitan areas and along the ohio valley that houses many power plants 3 2 co2 observations the co2 observational data are from the observation package obspack cooperative global atmospheric data integration project 2017 prepared by noaa in consultation with data providers the obspack collects direct atmospheric greenhouse gas measurements from multiple national and university laboratories in a consistent format with care given to make the data as accurate as possible masarie et al 2014 co2 observations from different platforms including in situ tower flask and airplane were selected for model evaluations the locations of each data product are highlighted in fig 4 more detailed site information is listed in tables 1 and 2 a total of 16 stationary sites were selected for model evaluations out of which 12 were tower in situ hourly measurements and 4 were flask observations the sites are located at various altitudes ranging from near sea level to more than 3500 m above sea level masl and represent a spectrum of land covers including water croplands grasslands and forests 13 aircraft flight locations covering different vegetation types were also used for evaluation of modeled vertical profiles and evolution in addition the total column co2 xco2 measurements from the greenhouse gases observing satellite gosat were used for model evaluation the atmospheric co2 observations from space acos gosat tanso fts level 2 bias corrected xco2 v7 3 oco 2 2016 was used in comparison to nu wrf casa xco2 3 3 comparison with observations the modeled either based on pctm or nu wrf casa co2 concentrations at each in situ tower location 3 dimensional based on their latitudes longitudes and intake heights listed in table 1 and sampling time were extracted and compared to the respective observations all hourly values were compared for the continuous in situ measurements and used in the statistical calculation over the years of 2010 2012 fig 5 shows the statistics of comparisons between simulated and observed co2 for pctm and nu wrf casa models it can be seen that in comparison with pctm nu wrf casa reduced co2 bias in 9 out of 12 sites and decreased rmse in 3 sites nu wrf casa and pctm showed comparable correlations with the observations in 9 out of 12 sites with less than 5 difference in correlation coefficients between each other however nu wrf casa as compared with pctm correlated much better to the observations in fsd sgp and snp sites with the respective increases of 15 1 15 4 and 34 in correlation coefficients when considering all site data together the bias out of nu wrf casa was 0 48 ppmv versus 0 71 ppmv for pctm the rmse from nu wrf casa was 5 81 ppmv an approximately 9 decrease from 6 39 ppmv for pctm and the respective correlation coefficients for nu wrf casa and pctm were 0 79 and 0 74 the difference in the simulated results from nu wrf casa and pctm was mainly caused by the underlying meteorology that was responsible for atmospheric co2 transports overall nu wrf casa displays a noticeable improvement over pctm in simulating co2 and much less difference than seen for tm5 and wrf in diaz isaac et al 2014 in the next several sections a more detailed evaluation of nu wrf casa is discussed 3 3 1 in situ tower measurements fig 6 illustrates the three statistical measures in evaluating the nu wrf casa performance at each site categorized by season for all 3 years all 24 h data were utilized for the calculation we defined winter as december january and february spring as march april and may summer as june july and august and fall as september october and november it can be seen that there were variations in bias across the sites and seasons the bias at the same site but in different seasons can be of opposite sign bringing the total annual bias low the sites at various forest locations i e amt etl fsd lef nwr and snp experienced positive biases in winter and spring ranging from 0 44 to 4 61 ppmv except for nwr and snp that are located on mountain ridges the biases in summer and fall for amt etl fsd and lef were either negative 3 21 0 40 ppmv or small positive 0 51 for fsd in fall bringing the annual bias as 0 06 0 85 1 18 and 0 86 ppmv respectively the summer and fall biases at nwr and snp were positive giving these two mountaintop sites positive bias across all four seasons the respective annual biases for nwr and snp were 0 64 and 3 89 ppmv the cropland sites i e bao sct sgp and wbi saw negative biases 2 62 0 18 ppmv in both summer and fall in winter all but bao experienced positive bias 0 33 3 88 ppmv the winter bias of bao was 2 11 ppmv while the ones of bao and sct were negative 0 5 ppmv the spring biases of sgp and wbi were 2 66 and 0 54 ppmv respectively the respective annual biases for bao sct sgp and wbi were 1 66 0 61 0 90 and 0 03 ppmv the large annual negative bias at bao was thought to be linked to the location of the site bao though classified as cropland in the model received a lot of urban pollution plumes from the nearby cities the elevated local anthropogenic fluxes might not be captured the biases among four seasons for the grassland site wkt showed a phase shift with positive negative positive negative bias for winter spring summer fall respectively with an annual bias at 0 38 ppmv nu wrf casa overestimated co2 over the water site wsa in winter spring and summer but underestimated it in fall leading to a total annual bias of 0 36 ppmv taking together all data from 12 sites the largest bias of 1 89 ppmv occurred in winter followed by spring 1 38 ppmv summer 0 70 ppmv and fall 0 66 ppmv the observed biases were comparable to the results by other studies ahmadov et al 2009 uebel et al 2017 in simulations to compare with the coastal tower measurements of co2 over may june using wrf vprm ahmadov et al 2009 reported a bias of 0 67 ppmv in their investigation of co2 variability over a summer week using a high resolution model uebel et al 2017 reported biases ranging from 0 70 ppmv sugar beet cropland to 5 52 ppmv winter barley cropland the annual rmse for the 12 tower sites varied from 2 03 nwr to 8 01 fsd ppmv with the overall rmse i e including data points from all 12 sites at 5 81 ppmv seasonally the largest rmse typically occurred in summer except for bao and wkt that saw the largest rmse in winter and fall respectively this probably was related to the strong photosynthesis drawdown in summer taking together all data from 12 sites the rmses for winter spring summer and fall were 4 98 5 00 7 58 and 5 29 ppmv respectively in general the nu wrf casa modeled co2 time series correlated well with the observations at each tower site the annual correlation coefficients ranged from 0 56 bao to 0 89 lef with the 12 site overall value at 0 79 seasonally the highest correlation typically happened in fall except for amt nwr and sgp all of which found the best correlation in winter the 12 site overall correlation coefficients for winter spring summer and fall were 0 71 0 70 0 73 and 0 79 respectively the nu wrf casa performance at different hours of the day was investigated as well for this purpose we divided each day into 4 periods midnight 23 04 local standard time lst morning 05 10 lst midday 11 16 lst and evening 17 22 lst table 3 lists the results at each time period summarized based on the statistics from each of 12 sites and overall statistics that takes 12 site data together it appeared that nu wrf casa performed the best in terms of the overall rmse and correlation coefficient through the midday into evening hours without exception the smallest rmse and highest correlation coefficients always appeared during the midday or the evening the overall smallest 0 00 largest 0 95 bias however occurred in the midnight midday respectively for individual sites except wkt where the largest negative bias of 0 89 ppmv took place during midnight other sites saw their largest bias occurring either in the morning or in the midday during midday and evening with generally deep pbl the air was relatively well mixed leading to less co2 variability and thus smaller rmse and better correlation at night on the other hand the very shallow pbl and near surface co2 gradient were likely not well resolved by the lowest model layers this explained the larger rmse at midnight and early morning 3 3 2 flask measurements unlike online co2 measurements by in situ instruments deployed at a tower flask measurements are collected and subsequently analyzed in a laboratory they are typically taken near midday at a location representative of large scale background conditions about once per week in this study we compared the nu wrf casa results to the flask measurements from 4 sites table 1 this experiment represented testing the model performance in a more controlled environment according to the noaa protocol flask sampling selects for as much as possible wind directions that avoid local fluxes fig 6 also displays the bias rmse and correlation coefficient at 4 sites with suffix flask for different seasons it was found that the smallest absolute seasonal biases took place during spring across 4 sites while the largest absolute biases appeared in different seasons winter for pta and uta summer for nwr and fall for thd the annual biases were 1 32 2 19 0 15 and 0 59 ppmv for pta thd uta and nwr respectively although not always the smallest the rmses in spring were all relatively small the largest rmse however took place in fall at pta and thd two sites on the pacific coast in winter at uta and in summer at nwr the annual rmses were 4 84 4 99 2 27 and 2 12 ppmv for pta thd uta and nwr respectively the correlation coefficients at two coastal sites i e pta and thd were relatively small as 0 62 and 0 68 they were 0 80 and 0 82 for uta and nwr respectively it was worth noting that the nu wrf casa performances at two inland sites uta and nwr were superior to those at two coastal sites pta and thd the sampling problem might cause this the modeling grid cells along coastal areas generally include both land and water portions therefore the modeled co2 at coastal sites might be affected by significant terrestrial fluxes which were typically not reflected in the flask measurements this may explain the large negative biases found in pta and thd 3 3 3 aircraft flight profiles aircraft measurements provide not only horizontal but also vertical co2 distributions with time this 4 dimensional dataset is extremely valuable in a full fledged evaluation of model performance on co2 transport simulations note that the aircraft measurements are almost all taken near midday when the pbl is vertically well mixed and the impact of sub grid scale fluxes is diminished in this study a total of 13 flight measurement locations table 2 were selected for nu wrf casa evaluation these measurements covered various landscapes at different seasons and times within the years of 2010 2012 the modeled co2 concentrations were interpolated to match the flight latitude longitude altitude and time the results are summarized in table 4 it can be found that statistically nu wrf casa simulated the 4 dimensional co2 distribution well across the 13 flight locations the modeled concentrations were highly correlated to the measured ones with correlation coefficients ranging from 0 79 sgp and thd to 0 94 etl the model biases were small varying between 0 35 tgc and 0 50 thd ppmv the rmse fluctuated from 1 45 car to 5 51 ppmv inx taking together all data from the 13 flight locations the bias rmse and correlation coefficients were 0 03 ppmv 2 61 ppmv and 0 88 respectively there were 4 locations etl lef sgp and thd with both ground in situ tower or flask and aircraft measurements without exception statistically the modeled results compared better to the aircraft observations than to the ground measurements this was expected since many flight observations were taken in the free troposphere where co2 were well mixed and more represented as the background concentration the model performance along altitudes was evaluated as well we grouped the flight measurements into 17 altitude intervals from surface to above 7000 masl there were 8 levels with 250 m intervals from surface to 2000 masl 6 levels with 500 m intervals from 2000 to 5000 masl 2 levels with 1000 m intervals from 5000 to 7000 masl and the one above 7000 masl the measurement numbers at each sampling level ranged from 240 below 250 masl level to 800 2000 2500 masl level the averages of flight measurements within each level were compared with the same level average modeled results the statistics of bias rmse and correlation coefficient are illustrated in fig 7 nu wrf casa generally yielded positive biases up to 0 9 ppmv at the low altitude within 1000 masl agreeing with the finding by ter maat et al 2010 who reported higher modeled co2 concentrations near surface likely due to a lower estimated pblh the bias turned to negative between 1000 and 3500 masl before oscillating around 0 from 3500 to 5000 masl very small negative biases less than 0 1 ppmv then appeared above 6000 masl in any altitude though the model bias was small as a comparison sarrat et al 2007 a b reported a model observation co2 discrepancy of up to 4 ppmv along the vertical profile when evaluating their regional model using the carboeurope regional experiment strategy ceres flight observations rmse decreased with altitude from around 5 ppmv near surface to about 1 8 ppmv at 3500 masl and leveled off at approximately 1 6 ppmv above 4000 masl this was anticipated because the long lived co2 was more homogeneously distributed at high altitudes where there were no major co2 sources and sinks the correlation coefficient ranged between 0 83 and 0 91 across the altitudes 3 3 4 gosat measurements a two step method was employed to derive the nu wrf casa xco2 first the collocated nu wrf casa co2 profiles were vertically extended using the interpolated profiles from pctm grafted above the top of the nu wrf casa model domain 30 hpa then the modeled xco2 was calculated from the resulting profiles using the gosat reported averaging kernels and a priori profile vertical weighting fig 8 illustrates the comparison of nu wrf casa modeled xco2 to the gosat xco2 in july in general the model gradients were well correlated r 0 768 with the observations across the domain and the bias 0 35 ppmv was small the gosat data were more variable than the model xco2 and the range of values was larger the correlations were poor in january figure not shown however when the synoptic gradients in xco2 were small and the variability in gosat appeared to be dominated by small scale processes controlling co2 fluxes and or noise 3 3 5 drought vs normal conditions drought puts stress on plant morphology physiology and photosynthesis fahad et al 2017 and the references therein which consequently impacts the vegetation carbon fluxes and atmospheric co2 concentration the model s capability to simulate atmospheric co2 under drought vs normal conditions was also evaluated in this study according to the data by u s drought monitor https www drought gov drought data gallery us drought monitor texas and the southern great plain areas suffered extreme to exceptional drought starting from early 2011 and throughout the entire year another extreme drought hit the southern great plain areas in the summer of 2012 and lasted into 2013 within the 12 tower sites selected for the model evaluation sgp and wkt were in the areas impacted by the aforementioned drought spells net ecosystem exchange nee or net vegetation carbon flux variability can be driven by either ecosystem production or respiration a recent study by z liu et al 2018 finds that the inter annual variability of nee is primarily controlled by production if the annual precipitation is below 950 mm in the contiguous u s the climatological annual precipitation at both sgp and wkt is approximately 900 mm https www bestplaces net climate and thus the inter annual change of vegetation carbon flux is likely driven by production at these two sites therefore the growing season april october has been selected for analysis we chose the periods marked as extreme or exceptional drought to represent the drought spells as such the correlation coefficient bias and rmse statistics were computed for wkt in the period between april 1 and october 31 2011 and for spg in the periods including may 15 october 31 of 2011 and july 20 october 31 of 2012 the results were then compared to the same statistics computed for the normal non drought conditions periods in the growing season for wkt april october of 2010 and 2012 and sgp april october of 2010 and april june of 2012 although the driving forces for these two droughts varied e g j liu et al 2018 their impacts on terrestrial fluxes were apparent as shown in fig 9 at sgp the average vegetation carbon flux over the drought period of the growing season was 3 14 x 10 9 kg m 2 s 1 with a standard deviation of 7 83 x 10 9 kg m 2 s 1 while the average and standard deviation were 5 99 x 10 9 kg m 2 s 1 and 1 81 x 10 8 kg m 2 s 1 respectively over the normal period at wkt the respective average carbon fluxes over the drought and normal periods were 1 43 x 10 9 and 2 91 x 10 9 kg m 2 s 1 while the respective standard deviations were 1 16 x 10 8 and 1 71 x 10 8 kg m 2 s 1 it can be seen that drought increased the net carbon fluxes at both sites which was likely due to less vegetation primary production according to z liu et al 2018 the results also revealed that net carbon flux was less variable during drought spells than during normal growth periods fig 10 displays the comparisons of daily average co2 concentrations from nu wrf casa and observations during drought and normal conditions for sgp and wkt the model performance was similar under two different environmental conditions the correlation coefficients were slightly better under the drought condition than under the normal condition for both sgp 0 58 vs 0 54 and wkt 0 83 vs 0 72 at sgp both bias 1 43 vs 1 79 ppmv and rmse 5 51 vs 6 26 ppmv were slightly better under drought than under normal conditions at wkt while the rmse was lower under drought 2 93 ppmv than under normal 3 12 ppmv conditions the absolute bias was smaller under normal 0 49 ppmv than under drought 1 67 ppmv conditions data from wkt showed two distinct clusters reflecting the drought and normal conditions while data from sgp lacked such feature fig 10 this may relate to the land use type wkt was classified as grasslands as such it was more susceptible to drought stress and thus the resulting ambient atmospheric co2 concentrations would more reflect the distinct micro environment under drought and normal conditions on the other hand sgp was classified as croplands and subject to human interference e g irrigation when drought struck as such the resulting ambient co2 concentrations would be less responsive to the drought stress and thus less distinguishable from drought to normal conditions 3 3 6 meteorology and model observation discrepancy to explore the possible causes of co2 model measurement discrepancies we analyzed the 3 year meteorology at selected sites it should be noted that this analysis was not intended to be exhaustive but rather to provide some insights into the model deficiencies and suggestions to improve the model we selected the local climatological data lcd archived by noaa s national center for environmental information ncei https www ncdc noaa gov cdo web datatools lcd for nu wrf casa meteorology evaluation the selected 4 sites wkt sgp wbi and lef span from the south to north with land covers of grasslands croplands croplands and mixed forest respectively since co2 is a long lived atmospheric trace gas its temporal and spatial variations at a synoptic scale are mainly driven by surface source sink variations and pbl processes particularly pblh and wind play important roles in determining co2 distributions in the boundary layer pblh is not readily available from observations but its growth is primarily driven by buoyancy resulting from surface sensible heating as confirmed by the observations from the amma campaign kohler et al 2010 in nu wrf casa temperature at 2 m t2 is a diagnostic variable and estimated as a function of sensible heat flux w k tao et al 2013 z tao et al 2013 therefore t2 can be viewed as a proxy of sensible heating and thus of pblh figs 11 and 12 display the comparisons between nu wrf casa modeled and observed t2 and wind speed at the selected sites respectively it was readily seen that nu wrf casa captured the temporal and spatial variations of both wind and t2 reasonably well the correlation coefficients for wind ranged from 0 76 sgp to 0 87 wkt with the bias within 0 7 m s 1 modeled t2 correlated with the observations especially well with the correlation coefficients being at least 0 98 however nu wrf casa showed a cold bias across all 4 sites with the underestimated t2 ranging from 0 96 wkt and wbi to 1 18 k sgp and lef this cold t2 bias was expected to lead to a shallower pblh indeed the nu wrf casa simulated pblh was less than that from merra 2 at these sites and times we further carried out meteorology and co2 flux analyses for each season to understand the model observation discrepancies shown in section 3 3 1 we compared the season when the seasonal average total carbon flux was positive source and when the seasonal average total carbon flux was negative sink table 5 summarizes the seasonal bias of t2 pblh wind speed co2 level and source sink classification at the selected sites it was interesting to find that pblh was at least partially responsible for the co2 model measurement discrepancies in most cases approximately 70 of 16 cases showing in red in table 5 for example wkt was a carbon source in winter as such the shallower pblh would allow more co2 accumulation in pbl resulting in a positive 0 81 ppmv bias in spring wkt changed to a carbon sink the shallower plbh would reduce the co2 mixing down from the free troposphere leading to a negative 0 95 ppmv bias in summer wkt became a carbon source the modeled shallower plbh again caused a positive 0 49 ppmv co2 bias a similar explanation was applicable to the other sites seasons highlighted in red in table 5 with the aforementioned reasoning the fall season across the 4 sites and spring in sgp should have had the biases in the opposite direction this indicated that sources other than pblh uncertainty may play a larger role in determining the model observation discrepancy here one such uncertainty was the variability of carbon sources sinks fall and spring were the transition seasons in which the net carbon source or sink was in transition for example we defined spring as a carbon sink season at sgp based on the seasonal average total carbon flux a close look revealed that among 9 spring months throughout 2010 2012 there were 5 carbon source months and 4 carbon sink months at sgp similar carbon source sink switches among months in fall season across 4 sites were found as well the uncertainty in carbon source sink variations would definitely cause variations in atmospheric co2 simulation in summary uncertainties in both carbon source sink variations and meteorology lead to uncertainties in co2 simulation in nu wrf particularly pblh representation had a prominent and direct impact on atmospheric co2 distributions pblh strongly depends on the sensible heat flux which highlights the importance to improve modeling of the land surface processes 4 summary modeling co2 at fine spatial resolution assists in better understanding the mechanisms and processes controlling carbon sources sinks and atmospheric co2 transport variability since many of these processes occur at relatively small spatial and or rapid temporal scales knowledge gained from such mechanism process studies is anticipated to lead to reduction of uncertainty in carbon climate interactions in this study we expanded the capability of the nu wrf model to simulate co2 transport and variability at fine spatial scales to achieve this goal nu wrf has been coupled with casa gfed and pctm in an offline manner together with the already built in capability of nu wrf coupling with merra 2 the newly developed nu wrf casa modeling system is capable of simulating co2 fluxes and transport in an internally consistent manner in which the global geos 5 merra 2 meteorology drives casa gfed pctm and nu wrf taking advantage of the process based casa gfed and high resolution nu wrf the new modeling system provides a flexible platform to investigate carbon fluxes and transport at fine scales and at process levels various interfaces modules and command options have been developed under the nu wrf framework coupling of casa gfed added approximately 4 6 7 5 computational overhead with an average of 5 9 more execution time in comparison with the stand alone nu wrf simulation a 3 year simulation covering multiple landscapes topographies and environmental conditions has been carried out to evaluate the model performance observational data collected from in situ tower flask aircraft and satellite measurements were employed to gauge the capability in simulating inter annual seasonal and diurnal variabilities of atmospheric co2 across a range of sites under both drought and normal conditions the results showed that nu wrf casa performed reasonably well in reproducing co2 transport and spatial temporal variabilities with overall bias less than 1 ppmv and correlation coefficient more than 0 6 it was further found that pblh had a prominent impact on atmospheric co2 levels and could directly link to approximately 70 of the cases of the co2 model observation discrepancies the close relationship between pblh and sensible heating emphasized the importance in improving model land surface processes to improve simulations of regional co2 distribution in the near future and taking advantage of the rich observational data already and to be collected from the ongoing and upcoming satellite missions critical to earth observing such as joint polar satellite system jpss orbiting carbon observatory 2 and 3 oco 2 3 geostationary carbon cycle observatory geocarb and tropospheric emissions monitoring pollution tempo nu wrf casa will continuously be improved on its physics and prediction skills the ultimate goal is to reduce uncertainties in simulating carbon climate interactions through the knowledge gained from simulation of co2 fluxes and transport using the fine scale and more detailed process representations available in nu wrf casa e g land surface physics soil moisture boundary layer dynamics and convective transport declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the development and evaluation of nu wrf casa was funded by nasa s modeling analysis and prediction map program solicitation nnh12zda001n map the authors would like to thank nasa center for climate simulation nccs for supercomputing and data storage support as well as noaa s cooperative global atmospheric data integration project ncgadip who organizes and distributes co2 data thanks are also due to each individual co2 data providers who submit their data to ncgadip including beth anderson nws arlyn andrews noaa dan baumann usgs sebastien biraud lbnl arm ken davis psu ankur desai uofwi ed dlugokencky noaa ralph keeling sio john lee uofme natasha miles psu jen morse instaar matt parker srnl scott richardson psu charles stanier uofia britton stephens ncar colm sweeney noaa margaret torn lbnl stephan de wekker uofva dan wolfe noaa psd doug worthy ec and point arena lighthouse keepers inc last but not least thanks go to gosat project and goddard earth sciences data and information services center ges disc for data collection and archive in compliance with fair data standards all data collected and generated for this research are archived and stored on nccs servers due to the sheer size of data 3 tb it is impractical to upload data to a public domain repository however the authors will be happy to share data on an individual request basis appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104785 
25981,operational ecological forecasting is an emerging field that leverages ecological models in a new cross disciplinary way using a real time or nearly real time climate forecast to project near term ecosystem states these applications give decision makers lead time to anticipate and manage state changes that degrade ecosystem functions or directly impact humans the everglades forecasting model everforecast is an operational forecast model designed specifically for conservation management purposes including water management it provides up to six month forecasts of daily projected spatially continuous stage values across the everglades we validated everforecast quarterly to measured historical values at 207 gages jan 1 2000 dec 31 2019 the everforecast hindcasts of water stage accurately captured measured stage variation with a low percentage of measured stages exceeding hindcasted values over the whole spatial extent the mean rmse is 20 98 cm the mean mae is 14 42 cm and the mean mbe is 0 91 cm keywords operational forecasting spatial position analysis near term forecasts everglades florida 1 introduction 1 1 background scientists decision makers and managers use ecological models to synthesize our best current understanding and respond effectively to current and anticipated environmental change increasingly users are seeking explicit descriptions of uncertainty in the models and how uncertainty matters in determining outcomes and management responses burgman et al 2005 regan et al 2005 wilson et al 2005 nicholson and possingham 2007 operational forecasts refer to short term forecasts used to inform operations such as water management operational ecological forecasting is an emerging cross disciplinary field that leverages ecological models with a real time or nearly real time climate forecast to project near term ecosystem states these applications give decision makers lead time to anticipate and manage ecosystem state changes across relevant time intervals clark et al 2001 allen et al 2015 further operational forecasts are continuously updated to reflect real time conditions providing an effective means to reduce uncertainty and manage adverse responses however accurate ecological forecasts are complicated by environmental stochasticity and other sources of uncertainty in both the input data and from the models themselves leading to forecasts with potentially little useable information stumpf et al 2009 nevertheless ecological forecasting is increasingly important considering increasing pressures on natural resources including extreme climatic events allen et al 2015 operational hydrologic forecasting has a long history in the field of weather forecasting with most of the models constructed using numerical methods brooks et al 1992 allen et al 2015 these mechanistic models are based on an understanding of the physical biological or chemical processes that underlie and control the predicted hydrologic values brown et al 2013 the national oceanic and atmospheric administration noaa has dedicated forecasting capabilities toward national operational ecological forecasts to provide early warnings of the possible effects of ecosystem changes on coastal systems with sufficient lead time for mitigation strategies national oceanic and atmospheric administration 2014 noaa operational hydrodynamic models include those for temperature prediction salinity and water current velocity allen et al 2015 these models often integrate empirical data to estimate uncertainty constrain model parameters and choose initial values hydrologic and ecological models may forecast future conditions as realizations of averaged plausible conditions or ranges of scenario conditions however initializing the models to be constrained with best estimates of current data will better mimic and forecast actual futures luo et al 2011 liu et al 2012 operational water stage forecasting for south florida s everglades provides managers with documented models that can be used in decisions for near term operations in as short a time frame as possible everforecast described in this paper is among a few models that use a statistical approach brown et al 2013 allen et al 2015 numerical process models require a detailed understanding of the mechanics of a system as well as rich streams of fine scaled input data blauw et al 2006 hipsey et al 2015 because mechanistic models require complex integration of multiple variables and first principles of understanding these models often take many years to develop and test before they can be used additionally assumptions about mechanistic responses are often necessary bringing further uncertainty compounded in the final model output on the other hand statistical relationships can be developed and confidently used within their validated range given well known uncertainties and take considerably less time to develop while statistical models do not incorporate a general theoretical framework of the modeled system they require large datasets to construct and validate blauw et al 2006 we use current data to initiate the forecasts and we model uncertainty using monte carlo statistical methods drawn from seasonal distributions of previous water level change to improve forecasted output in contrast to mechanistic models statistical models are built from available data and can use a wide array of statistical techniques such as generalized linear modeling or artificial neural networks brown et al 2013 most of this work has been conducted in coastal or marine habitats with much of it being supported by noaa and their habitat science and ecological forecasting technical team national oceanic and atmospheric administration 2016 examples of operational ecological forecasts using statistical methods include a model of bird migration intensities to reduce collisions with planes from the royal netherlands air force van belle et al 2007 the chesapeake bay ecological prediction system s forecasts for jellyfish blooms and dangerous bacteria in harmful algal blooms habs decker et al 2007 brown et al 2013 and seasonal forecasts of southern bluefin tuna for both reducing bycatch and increasing catch efficiency in australia hobday et al 2011 eveson et al 2015 measurement of uncertainty has traditionally been a difficult task in ecological and operational forecasting luo et al 2011 liu et al 2012 increasing prediction skill requires the refinement and development of model validation and uncertainty measurement failure to account for environmental stochasticity leads to model estimates that underestimate uncertainty which in turn lead to suboptimal decisions when developing management strategies clark et al 2001 nicholson and possingham 2007 many ecological operational forecasts target environmental issues that directly impact human populations such as economic loss from decreases in fish populations or tourism and public health impacts e g jellyfish blooms decker et al 2007 habs blauw et al 2006 stumpf et al 2009 seawater in drinking water supply yu et al 2017 and fire severity chen et al 2011 other ecological forecasts are used to inform conservation and natural resource management e g bison population management hobbs et al 2015 waterfowl population management u s fish and wildlife service 2019 many ecological forecasts do not regularly iterate with real time or near real time data and may therefore be slow to improve their forecasts with feedback on how well the models perform white et al 2019 dietze et al 2018 forecasts that continually integrate the newest information available and iteratively update predictions with new evidence dietze et al 2018 are incredibly valuable for decision making dietze and lynch 2019 and can be used to support multiple species objectives the florida everglades is a wetland ecosystem particularly vulnerable to changing climatic conditions water flows depths distribution and quality are fundamental to the integrity of the ecosystem yet are managed by an extensive suite of water control structures and storm water treatment areas whose operation is under frequent review with respect to weather conditions and perceived ecological needs michener et al 1997 erwin 2009 pearlstine et al 2010 we developed an operational forecasting application the everglades forecasting model everforecast which generates near term up to six months spatially continuous simulations of forecasted water levels from more than 200 gages in south florida haider et al 2020 data assimilation and ensemble forecasting a type of monte carlo analysis are methods used in numerical prediction to measure uncertainty leutbecher and palmer 2008 luo et al 2011 liu et al 2012 we replicate this method of capturing natural variation in water levels and forecasting uncertainty by using monte carlo simulations with a spatial position analysis these forecasted water levels then project outcomes for a suite of ecological responses using established spatiotemporal models of species distribution e g endangered species wading birds and prey fish everforecast is unique in that it is the first example to our knowledge of a spatially explicit hydrologic ecological operational forecast using empirical methods to inform conservation management decisions producing near term forecasts of water stage for florida s greater everglades ecosystem will improve the ability of federal state and local authorities to make informed management decisions and increase efficacy of management actions 1 2 objectives current positional analyses in the everglades provide point or area averaged stage forecasts that are invaluable for water management cadavid et al 1999 our goal in this research was to generate probable spatially explicit changes in water depth to forecast near term species distributions and ecological responses spatial distribution and patterns of water stage and depth change in florida s everglades drive ecosystem responses at numerous spatial and temporal scales shaping landscape topography clark et al 2009 givnish et al 2008 wu et al 2006 primary production iwaniec et al 2006 koch et al 2012 and a complex food web beerens et al 2017 sokol et al 2014 williams and trexler 2006 estimates of those spatial arrangements are necessary for resource managers to identify management actions that can benefit a suite of ecological communities while explicitly quantifying the potential costs to others thus planning for rather than reacting to environmental change the objective of this paper is to focus on generation of the hydrologic near term forecasts of everforecast everforecast development is motivated by application to ecological modeling which will be the subject of another paper 2 methods 2 1 model domain everforecast was developed for the managed everglades south of lake okeehobee and including water conservation areas wca the miccosukee federal indian reservation the big cypress seminole indian reservation everglades national park enp and big cypress national preserve bcnp fig 1 southern florida s everglades are a wet grassland ecosystem that rises from florida bay at a gradual gradient of approximately 5 cm km in a broad depression with freshwater flows down a gentle gradient as sheetflow not restricted to channels or streambeds the depression is bounded by the atlantic coastal ridge to the east and the big cypress limestone ridge to the west everglades national park is the largest congressionally designated wilderness area east of the rocky mountains and is internationally recognized by united nations education scientific and cultural organization unesco as a world heritage site in 1979 and as an international biosphere reserve in 1976 and by the ramsar convention of 1987 the everglades ecosystem creates a unique complex of habitats because of its position at the interface of the temperate and subtropical regions and of fresh and brackish water davis et al 1994 mcvoy et al 2011 these habitats include the largest sawgrass cladium jamaicense and mangrove ecosystems in north america short hydroperiod prairies tree islands ridge and slough habitat significant foraging and breeding grounds for wading birds and a high diversity of threatened endangered candidate and endemic species iucn 1979 the impacts of urbanization compartmentalization and water management over the last century on the everglades extent natural hydrology and ecosystems culminated in the comprehensive everglades restoration plan cerp for the restoration preservation and protection of the south florida ecosystem while providing for the water related needs of the region water resources development act 2000 2 2 overview everforecast s near term spatial hydrologic forecasting uses the trajectory of a historic analog to spatially predict the trajectory of near future water levels the process is composed of three elements fig 1 1 description of a water stage central tendency across the modeled domain up to six months into the future based on how water stage trended historically under similar initial conditions and forecasted precipitation 2 monte carlo simulations at each gage of near term future water stages based on the joint probabilities of distance from the central tendency and daily change in stage and 3 interpolation of the water stage simulations from the modeled gages to a continuous surface of water stages and water depths everforecast java software code is freely available from the u s geological survey wetlands and aquatic research center as detailed in the software availability section at the end of this paper the six month length of the forecasts resulted from interaction with users including the multi agency everglades multispecies water coordination team who expressed a need to look ahead into the next season but also by the limitations of attempting to forecast weather too far into the future water stage is collected from u s geological survey everglades depth estimation network eden version 3 the eden digital elevation model dem is subtracted from stage to estimate water depth eden is an integrated real time water level monitoring network that provides 1991 present water stage across the freshwater everglades management areas continuous daily spatial interpolations of surface water level gage data are provided on a 400 400 m resolution grid from over 200 gage stations palaseanu and pearlstine 2008 telis et al 2015 the source of data for precipitation is the national oceanic and atmospheric administration national weather service climate prediction center cpc 3 month average precipitation outlooks which forecast to 12 months out climate prediction center 2007 o lenic et al 2008 detail the methods and model skills for the operational long range climate predictions table 1 lists the everforecast data inputs to generate central tendencies and monte carlo simulations described further in sections 2 3 and 2 4 the central tendencies and simulations are forecast daily for six months from the starting date of the forecast and result in spatially continuous surfaces over the model domain at 400 400 m resolution 2 3 spatial position analysis central tendency for each run everforecast initializes a hydrologic spatial position analysis to construct a central tendency water stage forecast for each 400 400 m cell in the model domain the forecast is a series of daily stage values at the location of each of the water stage gages across the everglades landscape for the up to six month length of the desired simulation period to generate the forecast a routine searches month by month for analog data from historical stage and precipitation records analogs are past spatial distributions of water stage that most closely match current stage distribution and precipitation to ensure a spatially coherent forecasting trend each analog month is selected from available eden water surfaces for finding the best fit across 18 spatially distributed gages rather than independently at each individual gage the procedure is to 1 calculate the root mean squared error rmse between recent stage values at the set of pre selected water gages fig 1 and values from those gages on the same day of the year in past years 2 identify the subset of years in which the rmse falls within a threshold of acceptance 3 calculate the difference between current cpc forecasted shifts in monthly regional precipitation totals and actual historic regional precipitation totals in that same month for each year in the subset and 4 identify the analog year with the smallest difference between forecasted and historic precipitation data from the appropriate month in each selected past year are considered a likely analog for conditions in the forecast current year fig 1 the subset of eden analog months is selected by calculating the rmse of stage for the last four days of the month before the forecast start date to each of the eden water surfaces at those dates for the last 20 years this step is repeated at each of the 18 central tendency analysis gages fig 1 for an overall rmse across all gages such that rmse 1 m n i 1 m j 1 n r i j s i j 2 1 2 where m 4 the last four days in the previous month n is the number of gages rij are the modeled eden analogs water stage values and sij are the gage water stage values the 18 gages were selected across the central everglades to cover interior marsh away from structures well distributed particularly north to south because that is the direction of regional slope and to capture broad physiographic regions such as are reflected by lower and higher hydroperiod marsh in everglades national park a subset of eden analog months with a rmse match of 20 cm is selected from the full date range fig 2 a the 20 cm threshold value was chosen after repeated testing to maximize selection precision while still allowing enough flexibility for the model to select multiple potential analogs at this step for most simulations the historic month used as an analog to forecast the coming month is selected from the rmse selected subset of months by matching to the closest monthly rainfall forecasts fig 2b and c cpc issues 3 month rainfall forecasts for each month e g jan feb mar feb mar apr o lenic et al 2008 therefore forecast periods overlap by two months taking advantage of the overlap a linear programming model is used to dis aggregate the 3 month cpc forecasts into monthly rainfall that preserves the 3 month totals linear programming is a standard method for mathematical optimization every 3 month forecast overlaps the prior and following forecast periods for two of the three months the objective is to produce monthly forecasts that preserve all the 3 month rainfall forecast totals inputs to the linear programming model are the overlapping 3 month forecasts and additional constraints used to optimize the solution based on seasonal variability of rainfall these constraints on the solution are based on the expected distribution of rainfall within each 3 month period for example within the apr may jun period april is typically very dry while on average june is the rainiest month of the year these constraints are constructed from next generation radar nexrad rainfall data south florida water management district 2008 nexrad data are available for south florida on a 2 2 km grid that overlaps the 400 400 m eden grid daily nexrad rainfall values from overlapping grid cells are summed into monthly total rainfall across the model domain minimum and maximum rainfall totals are used as constraints for the expected range of rainfall in each month together the overlapping 3 month rainfall totals and the expected distribution of rainfall within each 3 month period are used to provide an optimized solution for the monthly rainfall forecasts table 2 illustrates the linear programming model and results generated for july december 2013 dis aggregated monthly forecast values fall within the expected distribution constrained by the upper and lower bounds while also respecting the 3 month cpc forecasts near term 3 month cpc forecasts typically fall within the expected distributions developed from historical monthly rainfall totals resulting in a straightforward solution to the linear programming model in other cases forecasts diverge widely enough from historical patterns to render the model infeasible to control for this scenario and in recognition of the unreliability of far term rainfall forecasts the process is limited in practice to a six month simulation period when the linear programming solver cannot provide an optimized solution to a given model the constraints on the solution are iteratively widened by 1 in a stepwise fashion from the last month of the simulation period back to the third month rarely when these constraints are widened by at least 15 without success the program will incorporate the constraints of the second month into the iterative widening process to find a solvable model analysis is repeated until an analog has been selected for each month of the simulation period in its first iteration the analog selection process targets eden stage values immediately preceding the simulation start date the first selected analog month will contain the eden data that best aligns with the targeted stage values after correcting for differences between shifts in measured and forecasted precipitation each subsequent iteration of the analog selection process targets the analog s most recent forecast from the preceding month once an analog has been selected the water stage of the analog is shifted up or down in each grid cell so that day one of the analog matches the stage of the end date of the preceding month fig 2d the daily change in stage from the analog month is added to the previous day s stage forecast starting with measured stages at the beginning of the forecast period this calculation is made for each cell preserving a natural rate of change in stage spatially across the domain over the forecast period the analog months are combined into a continuous daily dataset adjacent analog months are nearly always chosen from different years despite the apparent continuity of stage values in consecutive months of the same year difference in projected precipitation is often an overriding factor forecasts are based on the daily change in stage from the analog months through the forecast period from analog months that may be from different years but which had rainfall closely matching the cpc forecast the forecast surfaces are used as measures of central tendency in the construction of stochastic forecasts of stage 2 4 monte carlo simulations monte carlo simulations are constructed about the central tendency at each of 207 gage locations monte carlo simulations of daily stage are driven by 1 previous biweekly hydrologic variability 1991 2019 for this case study 2 future cpc precipitation classification dry median wet for each biweekly period and 3 a joint probability between the central tendency and the biweekly distribution of historical variability future precipitation classification and near term central tendency were provided by the spatial position analysis described above the goal of the simulations is to encompass the range of probable water stages a measure of that range is the median variance of a very large number of stochastic simulations to reduce demands on computing data management and storage of spatial outcomes for operations that are being repeated monthly we wanted to find the minimum number of simulations that would be needed to result in a reasonable chance that they will converge on the median variance of many simulations the procedure was to draw a subset of 6 month simulations with replacement from a full set of 500 at 16 of our 18 spa gage locations two of the gage locations 3ane and ne5 were dropped because they are not included in the eden database for surface interpolations we drew 10 000 subsets of 25 50 100 150 200 300 and 400 we repeated this routine for three representative years 2007 a dry year 2010 a median year and 2013 a wet year based on water stage quantiles at each biweek of the simulations we found the mean and variance of each draw of a subset and found the mean of the biweekly variances this results in 10 000 means of biweekly stage variance for each subset draw these results were used to plot the median interquartile range iqr of mean stage variances by gage and by all gages for each year and each subset draw fig 3 is the plot of median iqr of water stage variance at each subset draw for the three representative years we determined that the change between draws for all the years approaches an asymptote at approximately 100 150 draws and selected 150 for generating monte carlo simulations in our application cpc precipitation forecasts for south florida climate division 68 are compared to monthly precipitation totals and quartiles from the florida climate center for 1998 2017 florida climate center 2020 to classify each simulation month as dry median or wet dry simulations are below the 25th percentile from florida climate center data and wet simulations are above the 75th percentile for each simulation the initial condition starts at the most recent real time stage value then a joint probability distribution is used to select each subsequent daily stage value maintaining a consistent directional change across the landscape with the magnitude varying depending on each gage s past variability the joint probability distribution used to pick new stage values is the product of two input probability distributions fig 4 the first input distribution is biweekly stage change probability about the central tendency the biweekly change distribution for each gage irrespective of simulation is a normal distribution with mean equal to the central tendency value for the current time step and standard deviation equal to the biweekly historic stage standard deviation derived from eden stage data for the appropriate category of dry median or wet fig 4a the second input distribution is daily stage change probability or how much stage is likely to change in a single day the daily change distribution varies with respect to gage biweek and simulation it is normally distributed with mean equal to the sum of current stage and mean stage change for the current biweek the standard deviation is equal to the absolute value of the sum of current stage and the biweekly historic stage change standard deviation derived from eden stage data the product of the two input probabilities is the probability distribution for the next day s stage fig 4b a random value is taken from this distribution as the stage value used for the next day in this individual simulation the value is calculated as a standard normal random value multiplied by two standard deviations plus the mean of the next day s stage probability distribution finally a random uniform number between 0 and 1 is selected and compared to the trend probability value for the gage the trend probability value is the number of days expressed as a percent on 0 1 scale which a gage moves in the same direction over a given biweek whether that be increasing or decreasing stage and gives an indication of how stable or jittery a gage s water levels will be for a particular biweekly period if the selected random number is greater than the trend probability and the previous stage change was moving in the same direction or if the random number is less than the trend probability and the previous stage change was not moving in the same direction the direction of change is reversed and applied to the old stage value to arrive at the new stage value this is done to keep the movement of stage in line with historical trends e g drydown periods an example of the central tendency and 150 monte carlo simulations are displayed in fig 4c for gage site 62 located in water conservation area 3a the figure illustrates an example of the changing variability around the central tendency resulting from different seasons the variance in the simulations widens as the simulations move away from the starting date because we are moving from the dry season to the wet season and variability about the central tendency during the wet season is greater there may also be some increased variability from increasing temporal uncertainty away from the starting date but that effect is lessened since the methods favor simulation movements toward the central tendency the effect of season and time since start of a simulation is further explored in section 3 2 monte carlo assessments interpolation to continuous water depth maps under different probable near futures are illustrated in fig 4d 2 5 spatial interpolation the forecasted daily stage values at the 207 gages are used to generate water surfaces using an adaptation of a surfacing application developed and used in the eden program eden uses the gage values to simulate canal and levee boundaries that impede continuous flow across the everglades and then uses a radial basis function to interpolate water surfaces independently for subareas that have little to no influence on each other and last stitches these surfaces together to create a continuous water surface over the eden extent telis et al 2015 water depths are calculated using the eden dem that was developed from a combination of the high accuracy elevation data haed electronic atlas and aerial height finder ahf data telis et al 2015 2 6 central tendency error we ran everforecast hindcasts quarterly to compare the daily central tendency surfaces to eden historical surfaces january 2000 december 2019 we calculated mean daily deviation from eden water stage and summarized differences with root mean square error rmse mean absolute error mae and mean biased error mbe over the period of record to analyze the overall performance of the forecasts rmse and mae measure residual errors and rmse measures error magnitude and gives greater weight to larger errors mae measures model bias and mbe shows over and under predictions of the water stage as positive and negative values rmse mae and mbe were also generated for the 3 years selected earlier as representative of dry 2007 median 2010 and wet 2013 years during the 2000 2019 time period of the hindcasts these measures gave an evaluation of the effects of extremes on the observed errors 2 7 monte carlo simulation evaluations the 2000 2019 hindcast data set and the representative dry median and wet years were also used to evaluate the performance of the stochastic simulations about the central tendency for each day at each gage we determined the maximum and minimum hindcasted water stage from the set of simulations to determine how well the observed variation was captured by the range of monte carlo simulations we calculated the percentage of days that the eden stage value exceeded either the maximum or minimum simulated stage the percent of days with exceedances was mapped by gage to evaluate the spatial distribution time since the start of a simulation may affect the spread of the simulations and their deviation from observed water levels to evaluate that effect we calculated the absolute deviation relative to the start of a run the absolute deviation is equivalent to the mae but is not referred to as an error because the monte carlo simulations are purposely a spread around the expected so they are not errors so much as possibilities time of the year may also affect the spread of the simulations absolute deviation by time of the year was also calculated 3 results assessments of the accuracy of the central tendency and conditions effecting the spread of the monte carlo simulations are presented 3 1 central tendency assessments fig 5 plots everforecast hydrology and precipitation for comparison to eden measured stage hindcasts were started once every quarter and run for six months the hindcasts follow the eden water surface most of the time when they deviate is most often is at the tops or bottoms of stage peaks as the model lags the observed water levels change in direction over the whole spatial extent the mean rmse is 20 98 cm the mean mae is 14 42 cm and the mean mbe is 0 91 cm table 3 forecast that start at the beginning of quarters one three and four increase rmse and mae from 3 months into the simulation to 6 months table 4 the second quarter is a transitional period between the south florida wet and dry seasons and displays higher error earlier in the forecasts before reducing to more predictable values associated with the third quarter rmse of the daily difference in water stage between the central tendency and eden historical values is mapped for all days from 2000 to 2019 in fig 6 we see the highest differences from eden historical stage at the northwestern corner of water conservation area 3a along the levees between water conservation area 1 2a and 3a on the western border of big cypress national preserve and in the south central to southeastern area of everglades national park these are areas of higher elevation such as the marl prairie and pine rocklands in everglades national park and the levees are areas of abrupt change 3 2 monte carlo assessments fig 7 presents the quantile distribution of monte carlo simulations at six gage locations distributed spatially across the everglades starting in the second quarter of 2010 the second quarter forecast is shown because as previously noted that is a transitional period between the south florida wet and dry seasons when we expect the most variance 2010 is a median year for water levels we might expect when forecasting with the influence of precipitation that the earlier hindcast dates will be closer to eden measured water depths than later dates in these examples we see close tracking across the entire 6 months at some locations and other locations where deviation from eden begins after about the first 3 months though remaining within the 10 90 bounding quantiles gage do1 a short hydroperiod marl prairie location has markedly higher variance among the simulations than the other examples this is consistent with the spatial distribution of central tendency error that we discussed above spatial differences demonstrate where forecasting bias under or over prediction occurred in the greater everglades fig 8 the everforecast hindcasts of water stage accurately captured measured stage variation with a low percentage of measured stage exceeding hindcasted values the range of everforecast simulations better captured lower water stage values than higher stage values i e the historical stage values exceeded monte carlo minimums less often than exceeded monte carlo maximums eden historical water stage was more likely to exceed maximum simulated stage at gages located in northern big cypress national preserve in the drier marl prairie pine rockland in everglades national park and when located on or near a water structure no spatial pattern was recognized for eden historical water stage exceeding the simulated minimum the largest mean percentage of days exceeding extrema was 3 41 of days exceeding a maximum in 2004 the percent for all other years was less than 1 00 taking the average over all gages historical eden water stage exceeded the maximum stage of simulations most often in the third quarter july september 1 16 of days the wet season and exceeded the minimum stage most often in the fourth quarter october december 0 20 of days a transitional period to the dry season the absolute difference ad from eden water stage in each of the 150 simulations in relation to the time of year is plotted in fig 9 a by julian date and by the quarter in which they start because the simulations are six months long the fourth quarter simulations start in october but continue through march so they plot at both julian days 274 365 and days 1 90 the simulations start at current know water stages and therefore zero ad and typically reach a relatively stable maximum typically within the first 2 months where the second quarter maximum ad values exceed the other quarters it may peak and then drop quickly into the third quarter consistent with what we have seen previously the do1 location in marl prairie has the largest range of simulation values during the second quarter as expected the range of the stochastic simulations closely relates to the magnitude of the standard deviation value used as input for biweekly stage change probability about the central tendency fig 9b the pattern tracks both throughout the year and per gage fig 10 is a closer look at differences from eden across all the spa gages at time since start of the simulation we again observe the pattern this time with rmse of leveling off at approximately two months and rmse peaking during the second quarter and then dropping to third quarter values 4 discussion the everforecast is a novel statistical approach to spatially continuous water stage forecasting for resource and operational management the modeled simulations present scientists and managers with the likely range of possible future conditions from which ecological response and sensitivity over probable ranges of water stage can be evaluated across the landscape and within specific regions percentages of historical measured water stage exceeding everforecast simulations were low fig 8 the second quarter in south florida is a transitional period between the dry and wet season the shorter hydroperiod landscapes such as the marl prairie are the most challenging to forecast during these transitional periods as water levels are typically below ground but rising to a ponded condition these conditions require further investigation to improve the model the south florida hydrodynamic models also struggle to simulate below ground depths in this landscape new six month forecasts were generated quarterly for testing purposes in use however everforecast generates new six month forecasts each month instead of quarterly to achieve greater accuracy in forecasts as the season progresses while there are some limitations stemming from the use of a statistical model framework we believe that we have addressed these potential weaknesses in a variety of ways one common criticism is that because statistical models are tailored to historical data factors such as increased variation and an increase in extreme events caused by climate change are not captured in the models because the data that the model is based upon do not contain the variation that is expected in the future we address this issue by using appropriate model inputs in our forecast such as cpc s numerical weather forecast furthermore while extreme events and fundamental changes in the way parts of the system operate are important considerations for long term and moderate term forecasting everforecast is specifically restricted to short term forecasting here we have demonstrated an accurate forecast time frame using variability over the last two decades and new daily data will continue to inform distributions each consecutive year precipitation forecasts are critical inputs to the success and future improvements of everforecast near term water stage forecasting downscaled climate models such the north american multi model ensemble nmme may provide an effective new source of precipitation forecasts in the future improvements to downscaled climate models for application to shorter time mean predictions of local precipitation patterns have been reported jung et al 2012 kirtman et al 2012 siqueira and kirtman 2016 laurindo et al 2019 currently while south florida precipitation forecasts have improved considerably from past modeling they still have poor performance o lenic et al 2008 that may partly be because the current downscaled resolution is still inadequate to represent convective precipitation over the south florida peninsula infanti and kirtman 2019 everforecast is being introduced to everglades restoration modeling for a suite of species distribution models wading birds alligator cape sable seaside sparrow small fishes apple snail and snail kite the multi species ensemble of models is also one of the attempts in the everglades restoration programs at introducing uncertainty from a range of probable inputs to the ecological models rather than treating each species separately everforecast allows users to investigate the consequences of water management that favor a particular species or group of species on the habitat of the other species other processes such as peat fire risk are also under development for inclusion as are on line decision support tools for spatial valuation of competing restoration objectives romaach et al 2018 presentation of the results may be simplified and potentially made more useful by categorization of the many monte carlo simulations into groups such as 1 median stage and recession rate 2 high stage low recession rate 3 high stage high recession rate 4 low stage high recession rate 5 low stage low recession rate fig 11 is an example of the everforecast application to a species distribution model that is in development with everglades multispecies water coordination team biologists and other shareholders evaluations such as these provide supporting information for multi agency management recommendations spatially explicit near term forecasting in the everglades is possible because of a data rich environment with a network of daily near real time water stage stations not all watersheds have the advantage of a data rich environment to apply the same methods however many riverine systems for example are sufficiently gaged for water managers to apply low flow metrics and calculations to deal with complex water supply endangered species and water quality issues one example the truckee river reservoir operations in california and nevada hong et al 2016 rely heavily on forecasting for medium range management to meet minimum environmental and water supply flow requirements through the duration of droughts such critical decisions for adaptive management of a natural resource can be further informed by explicitly addressing uncertainty with regularly updated probabilistic monte carlo simulations as we have applied in the everglades everglades national park has taken the everforecast hydrologic forecast and applied them to several early operational applications hill and redwine personal communication quantile evaluations such as in fig 6 are being used in presentations to resource managers to forecast soil oxidation and fire risk fig 12 illustrates an additional capacity of everforecast in which in addition to using eden surfaces as our analog months surfaces generated in a hydrodynamic model south florida water management district 2005 as restoration scenario alternatives were used as the analog months the figure plots the difference between the eden forecast and the alternative scenario forecast for the example location shown lower quantile dryer simulations are wetter in the alternative and higher quantile wetter simulations exhibit lower maximum water depths in the alternative the implication for everglades restoration is that this alternative meets an objective at this location of narrowing the extremes of high and low stages experienced 5 conclusions in a continuing close collaboration with agency natural resource managers everforecast provides near term mapped hydrologic forecasts that are necessary for species distribution modeling the spatial distribution of forecasts is valuable to managers for assessing current and modeled conditions for the potential to modify operations and improve ecological conditions more robust management decisions have been shown to result when uncertainties are incorporated in decision making problems regan et al 2005 nicholson and possingham 2007 everforecast offers the user a spatially explicit hydrologic forecast which includes a range of outcomes to reflect uncertainty that influences natural resource management decisions software availability name of software or dataset everforecast developer joint ecosystem modeling usgs warc contact information stephanie s romaach phd u s geological survey wetland and aquatic research center 3321 college ave davie fl 33314 phone 954 377 5921 email sromanach usgs gov year first available 2017 hardware required 50 gb disk 8 gb ram x86 64 cpu software required os ubuntu 18 04 64 bit osx 10 15 windows 10 64 bit java runtime environment 8 mixed integer linear programming solver lp solve netcdf 4 docker optional availability https jem gov everforecast cost free program language java program size 150 mb web declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national park service interagency agreement p16pg00252 and the u s geological survey greater everglades priority ecosystems science data for historic water stage and nexrad rainfall used for model development and evaluations is supplied by the u s geological survey s everglades depth estimation network thank you to caitlin hackett for figure contributions we are grateful to the anonymous reviewers and julien martin for helpful comments on a previous version of this manuscript any use of trade firm or product names is for descriptive purposes and does not imply endorsement by the u s government 
25981,operational ecological forecasting is an emerging field that leverages ecological models in a new cross disciplinary way using a real time or nearly real time climate forecast to project near term ecosystem states these applications give decision makers lead time to anticipate and manage state changes that degrade ecosystem functions or directly impact humans the everglades forecasting model everforecast is an operational forecast model designed specifically for conservation management purposes including water management it provides up to six month forecasts of daily projected spatially continuous stage values across the everglades we validated everforecast quarterly to measured historical values at 207 gages jan 1 2000 dec 31 2019 the everforecast hindcasts of water stage accurately captured measured stage variation with a low percentage of measured stages exceeding hindcasted values over the whole spatial extent the mean rmse is 20 98 cm the mean mae is 14 42 cm and the mean mbe is 0 91 cm keywords operational forecasting spatial position analysis near term forecasts everglades florida 1 introduction 1 1 background scientists decision makers and managers use ecological models to synthesize our best current understanding and respond effectively to current and anticipated environmental change increasingly users are seeking explicit descriptions of uncertainty in the models and how uncertainty matters in determining outcomes and management responses burgman et al 2005 regan et al 2005 wilson et al 2005 nicholson and possingham 2007 operational forecasts refer to short term forecasts used to inform operations such as water management operational ecological forecasting is an emerging cross disciplinary field that leverages ecological models with a real time or nearly real time climate forecast to project near term ecosystem states these applications give decision makers lead time to anticipate and manage ecosystem state changes across relevant time intervals clark et al 2001 allen et al 2015 further operational forecasts are continuously updated to reflect real time conditions providing an effective means to reduce uncertainty and manage adverse responses however accurate ecological forecasts are complicated by environmental stochasticity and other sources of uncertainty in both the input data and from the models themselves leading to forecasts with potentially little useable information stumpf et al 2009 nevertheless ecological forecasting is increasingly important considering increasing pressures on natural resources including extreme climatic events allen et al 2015 operational hydrologic forecasting has a long history in the field of weather forecasting with most of the models constructed using numerical methods brooks et al 1992 allen et al 2015 these mechanistic models are based on an understanding of the physical biological or chemical processes that underlie and control the predicted hydrologic values brown et al 2013 the national oceanic and atmospheric administration noaa has dedicated forecasting capabilities toward national operational ecological forecasts to provide early warnings of the possible effects of ecosystem changes on coastal systems with sufficient lead time for mitigation strategies national oceanic and atmospheric administration 2014 noaa operational hydrodynamic models include those for temperature prediction salinity and water current velocity allen et al 2015 these models often integrate empirical data to estimate uncertainty constrain model parameters and choose initial values hydrologic and ecological models may forecast future conditions as realizations of averaged plausible conditions or ranges of scenario conditions however initializing the models to be constrained with best estimates of current data will better mimic and forecast actual futures luo et al 2011 liu et al 2012 operational water stage forecasting for south florida s everglades provides managers with documented models that can be used in decisions for near term operations in as short a time frame as possible everforecast described in this paper is among a few models that use a statistical approach brown et al 2013 allen et al 2015 numerical process models require a detailed understanding of the mechanics of a system as well as rich streams of fine scaled input data blauw et al 2006 hipsey et al 2015 because mechanistic models require complex integration of multiple variables and first principles of understanding these models often take many years to develop and test before they can be used additionally assumptions about mechanistic responses are often necessary bringing further uncertainty compounded in the final model output on the other hand statistical relationships can be developed and confidently used within their validated range given well known uncertainties and take considerably less time to develop while statistical models do not incorporate a general theoretical framework of the modeled system they require large datasets to construct and validate blauw et al 2006 we use current data to initiate the forecasts and we model uncertainty using monte carlo statistical methods drawn from seasonal distributions of previous water level change to improve forecasted output in contrast to mechanistic models statistical models are built from available data and can use a wide array of statistical techniques such as generalized linear modeling or artificial neural networks brown et al 2013 most of this work has been conducted in coastal or marine habitats with much of it being supported by noaa and their habitat science and ecological forecasting technical team national oceanic and atmospheric administration 2016 examples of operational ecological forecasts using statistical methods include a model of bird migration intensities to reduce collisions with planes from the royal netherlands air force van belle et al 2007 the chesapeake bay ecological prediction system s forecasts for jellyfish blooms and dangerous bacteria in harmful algal blooms habs decker et al 2007 brown et al 2013 and seasonal forecasts of southern bluefin tuna for both reducing bycatch and increasing catch efficiency in australia hobday et al 2011 eveson et al 2015 measurement of uncertainty has traditionally been a difficult task in ecological and operational forecasting luo et al 2011 liu et al 2012 increasing prediction skill requires the refinement and development of model validation and uncertainty measurement failure to account for environmental stochasticity leads to model estimates that underestimate uncertainty which in turn lead to suboptimal decisions when developing management strategies clark et al 2001 nicholson and possingham 2007 many ecological operational forecasts target environmental issues that directly impact human populations such as economic loss from decreases in fish populations or tourism and public health impacts e g jellyfish blooms decker et al 2007 habs blauw et al 2006 stumpf et al 2009 seawater in drinking water supply yu et al 2017 and fire severity chen et al 2011 other ecological forecasts are used to inform conservation and natural resource management e g bison population management hobbs et al 2015 waterfowl population management u s fish and wildlife service 2019 many ecological forecasts do not regularly iterate with real time or near real time data and may therefore be slow to improve their forecasts with feedback on how well the models perform white et al 2019 dietze et al 2018 forecasts that continually integrate the newest information available and iteratively update predictions with new evidence dietze et al 2018 are incredibly valuable for decision making dietze and lynch 2019 and can be used to support multiple species objectives the florida everglades is a wetland ecosystem particularly vulnerable to changing climatic conditions water flows depths distribution and quality are fundamental to the integrity of the ecosystem yet are managed by an extensive suite of water control structures and storm water treatment areas whose operation is under frequent review with respect to weather conditions and perceived ecological needs michener et al 1997 erwin 2009 pearlstine et al 2010 we developed an operational forecasting application the everglades forecasting model everforecast which generates near term up to six months spatially continuous simulations of forecasted water levels from more than 200 gages in south florida haider et al 2020 data assimilation and ensemble forecasting a type of monte carlo analysis are methods used in numerical prediction to measure uncertainty leutbecher and palmer 2008 luo et al 2011 liu et al 2012 we replicate this method of capturing natural variation in water levels and forecasting uncertainty by using monte carlo simulations with a spatial position analysis these forecasted water levels then project outcomes for a suite of ecological responses using established spatiotemporal models of species distribution e g endangered species wading birds and prey fish everforecast is unique in that it is the first example to our knowledge of a spatially explicit hydrologic ecological operational forecast using empirical methods to inform conservation management decisions producing near term forecasts of water stage for florida s greater everglades ecosystem will improve the ability of federal state and local authorities to make informed management decisions and increase efficacy of management actions 1 2 objectives current positional analyses in the everglades provide point or area averaged stage forecasts that are invaluable for water management cadavid et al 1999 our goal in this research was to generate probable spatially explicit changes in water depth to forecast near term species distributions and ecological responses spatial distribution and patterns of water stage and depth change in florida s everglades drive ecosystem responses at numerous spatial and temporal scales shaping landscape topography clark et al 2009 givnish et al 2008 wu et al 2006 primary production iwaniec et al 2006 koch et al 2012 and a complex food web beerens et al 2017 sokol et al 2014 williams and trexler 2006 estimates of those spatial arrangements are necessary for resource managers to identify management actions that can benefit a suite of ecological communities while explicitly quantifying the potential costs to others thus planning for rather than reacting to environmental change the objective of this paper is to focus on generation of the hydrologic near term forecasts of everforecast everforecast development is motivated by application to ecological modeling which will be the subject of another paper 2 methods 2 1 model domain everforecast was developed for the managed everglades south of lake okeehobee and including water conservation areas wca the miccosukee federal indian reservation the big cypress seminole indian reservation everglades national park enp and big cypress national preserve bcnp fig 1 southern florida s everglades are a wet grassland ecosystem that rises from florida bay at a gradual gradient of approximately 5 cm km in a broad depression with freshwater flows down a gentle gradient as sheetflow not restricted to channels or streambeds the depression is bounded by the atlantic coastal ridge to the east and the big cypress limestone ridge to the west everglades national park is the largest congressionally designated wilderness area east of the rocky mountains and is internationally recognized by united nations education scientific and cultural organization unesco as a world heritage site in 1979 and as an international biosphere reserve in 1976 and by the ramsar convention of 1987 the everglades ecosystem creates a unique complex of habitats because of its position at the interface of the temperate and subtropical regions and of fresh and brackish water davis et al 1994 mcvoy et al 2011 these habitats include the largest sawgrass cladium jamaicense and mangrove ecosystems in north america short hydroperiod prairies tree islands ridge and slough habitat significant foraging and breeding grounds for wading birds and a high diversity of threatened endangered candidate and endemic species iucn 1979 the impacts of urbanization compartmentalization and water management over the last century on the everglades extent natural hydrology and ecosystems culminated in the comprehensive everglades restoration plan cerp for the restoration preservation and protection of the south florida ecosystem while providing for the water related needs of the region water resources development act 2000 2 2 overview everforecast s near term spatial hydrologic forecasting uses the trajectory of a historic analog to spatially predict the trajectory of near future water levels the process is composed of three elements fig 1 1 description of a water stage central tendency across the modeled domain up to six months into the future based on how water stage trended historically under similar initial conditions and forecasted precipitation 2 monte carlo simulations at each gage of near term future water stages based on the joint probabilities of distance from the central tendency and daily change in stage and 3 interpolation of the water stage simulations from the modeled gages to a continuous surface of water stages and water depths everforecast java software code is freely available from the u s geological survey wetlands and aquatic research center as detailed in the software availability section at the end of this paper the six month length of the forecasts resulted from interaction with users including the multi agency everglades multispecies water coordination team who expressed a need to look ahead into the next season but also by the limitations of attempting to forecast weather too far into the future water stage is collected from u s geological survey everglades depth estimation network eden version 3 the eden digital elevation model dem is subtracted from stage to estimate water depth eden is an integrated real time water level monitoring network that provides 1991 present water stage across the freshwater everglades management areas continuous daily spatial interpolations of surface water level gage data are provided on a 400 400 m resolution grid from over 200 gage stations palaseanu and pearlstine 2008 telis et al 2015 the source of data for precipitation is the national oceanic and atmospheric administration national weather service climate prediction center cpc 3 month average precipitation outlooks which forecast to 12 months out climate prediction center 2007 o lenic et al 2008 detail the methods and model skills for the operational long range climate predictions table 1 lists the everforecast data inputs to generate central tendencies and monte carlo simulations described further in sections 2 3 and 2 4 the central tendencies and simulations are forecast daily for six months from the starting date of the forecast and result in spatially continuous surfaces over the model domain at 400 400 m resolution 2 3 spatial position analysis central tendency for each run everforecast initializes a hydrologic spatial position analysis to construct a central tendency water stage forecast for each 400 400 m cell in the model domain the forecast is a series of daily stage values at the location of each of the water stage gages across the everglades landscape for the up to six month length of the desired simulation period to generate the forecast a routine searches month by month for analog data from historical stage and precipitation records analogs are past spatial distributions of water stage that most closely match current stage distribution and precipitation to ensure a spatially coherent forecasting trend each analog month is selected from available eden water surfaces for finding the best fit across 18 spatially distributed gages rather than independently at each individual gage the procedure is to 1 calculate the root mean squared error rmse between recent stage values at the set of pre selected water gages fig 1 and values from those gages on the same day of the year in past years 2 identify the subset of years in which the rmse falls within a threshold of acceptance 3 calculate the difference between current cpc forecasted shifts in monthly regional precipitation totals and actual historic regional precipitation totals in that same month for each year in the subset and 4 identify the analog year with the smallest difference between forecasted and historic precipitation data from the appropriate month in each selected past year are considered a likely analog for conditions in the forecast current year fig 1 the subset of eden analog months is selected by calculating the rmse of stage for the last four days of the month before the forecast start date to each of the eden water surfaces at those dates for the last 20 years this step is repeated at each of the 18 central tendency analysis gages fig 1 for an overall rmse across all gages such that rmse 1 m n i 1 m j 1 n r i j s i j 2 1 2 where m 4 the last four days in the previous month n is the number of gages rij are the modeled eden analogs water stage values and sij are the gage water stage values the 18 gages were selected across the central everglades to cover interior marsh away from structures well distributed particularly north to south because that is the direction of regional slope and to capture broad physiographic regions such as are reflected by lower and higher hydroperiod marsh in everglades national park a subset of eden analog months with a rmse match of 20 cm is selected from the full date range fig 2 a the 20 cm threshold value was chosen after repeated testing to maximize selection precision while still allowing enough flexibility for the model to select multiple potential analogs at this step for most simulations the historic month used as an analog to forecast the coming month is selected from the rmse selected subset of months by matching to the closest monthly rainfall forecasts fig 2b and c cpc issues 3 month rainfall forecasts for each month e g jan feb mar feb mar apr o lenic et al 2008 therefore forecast periods overlap by two months taking advantage of the overlap a linear programming model is used to dis aggregate the 3 month cpc forecasts into monthly rainfall that preserves the 3 month totals linear programming is a standard method for mathematical optimization every 3 month forecast overlaps the prior and following forecast periods for two of the three months the objective is to produce monthly forecasts that preserve all the 3 month rainfall forecast totals inputs to the linear programming model are the overlapping 3 month forecasts and additional constraints used to optimize the solution based on seasonal variability of rainfall these constraints on the solution are based on the expected distribution of rainfall within each 3 month period for example within the apr may jun period april is typically very dry while on average june is the rainiest month of the year these constraints are constructed from next generation radar nexrad rainfall data south florida water management district 2008 nexrad data are available for south florida on a 2 2 km grid that overlaps the 400 400 m eden grid daily nexrad rainfall values from overlapping grid cells are summed into monthly total rainfall across the model domain minimum and maximum rainfall totals are used as constraints for the expected range of rainfall in each month together the overlapping 3 month rainfall totals and the expected distribution of rainfall within each 3 month period are used to provide an optimized solution for the monthly rainfall forecasts table 2 illustrates the linear programming model and results generated for july december 2013 dis aggregated monthly forecast values fall within the expected distribution constrained by the upper and lower bounds while also respecting the 3 month cpc forecasts near term 3 month cpc forecasts typically fall within the expected distributions developed from historical monthly rainfall totals resulting in a straightforward solution to the linear programming model in other cases forecasts diverge widely enough from historical patterns to render the model infeasible to control for this scenario and in recognition of the unreliability of far term rainfall forecasts the process is limited in practice to a six month simulation period when the linear programming solver cannot provide an optimized solution to a given model the constraints on the solution are iteratively widened by 1 in a stepwise fashion from the last month of the simulation period back to the third month rarely when these constraints are widened by at least 15 without success the program will incorporate the constraints of the second month into the iterative widening process to find a solvable model analysis is repeated until an analog has been selected for each month of the simulation period in its first iteration the analog selection process targets eden stage values immediately preceding the simulation start date the first selected analog month will contain the eden data that best aligns with the targeted stage values after correcting for differences between shifts in measured and forecasted precipitation each subsequent iteration of the analog selection process targets the analog s most recent forecast from the preceding month once an analog has been selected the water stage of the analog is shifted up or down in each grid cell so that day one of the analog matches the stage of the end date of the preceding month fig 2d the daily change in stage from the analog month is added to the previous day s stage forecast starting with measured stages at the beginning of the forecast period this calculation is made for each cell preserving a natural rate of change in stage spatially across the domain over the forecast period the analog months are combined into a continuous daily dataset adjacent analog months are nearly always chosen from different years despite the apparent continuity of stage values in consecutive months of the same year difference in projected precipitation is often an overriding factor forecasts are based on the daily change in stage from the analog months through the forecast period from analog months that may be from different years but which had rainfall closely matching the cpc forecast the forecast surfaces are used as measures of central tendency in the construction of stochastic forecasts of stage 2 4 monte carlo simulations monte carlo simulations are constructed about the central tendency at each of 207 gage locations monte carlo simulations of daily stage are driven by 1 previous biweekly hydrologic variability 1991 2019 for this case study 2 future cpc precipitation classification dry median wet for each biweekly period and 3 a joint probability between the central tendency and the biweekly distribution of historical variability future precipitation classification and near term central tendency were provided by the spatial position analysis described above the goal of the simulations is to encompass the range of probable water stages a measure of that range is the median variance of a very large number of stochastic simulations to reduce demands on computing data management and storage of spatial outcomes for operations that are being repeated monthly we wanted to find the minimum number of simulations that would be needed to result in a reasonable chance that they will converge on the median variance of many simulations the procedure was to draw a subset of 6 month simulations with replacement from a full set of 500 at 16 of our 18 spa gage locations two of the gage locations 3ane and ne5 were dropped because they are not included in the eden database for surface interpolations we drew 10 000 subsets of 25 50 100 150 200 300 and 400 we repeated this routine for three representative years 2007 a dry year 2010 a median year and 2013 a wet year based on water stage quantiles at each biweek of the simulations we found the mean and variance of each draw of a subset and found the mean of the biweekly variances this results in 10 000 means of biweekly stage variance for each subset draw these results were used to plot the median interquartile range iqr of mean stage variances by gage and by all gages for each year and each subset draw fig 3 is the plot of median iqr of water stage variance at each subset draw for the three representative years we determined that the change between draws for all the years approaches an asymptote at approximately 100 150 draws and selected 150 for generating monte carlo simulations in our application cpc precipitation forecasts for south florida climate division 68 are compared to monthly precipitation totals and quartiles from the florida climate center for 1998 2017 florida climate center 2020 to classify each simulation month as dry median or wet dry simulations are below the 25th percentile from florida climate center data and wet simulations are above the 75th percentile for each simulation the initial condition starts at the most recent real time stage value then a joint probability distribution is used to select each subsequent daily stage value maintaining a consistent directional change across the landscape with the magnitude varying depending on each gage s past variability the joint probability distribution used to pick new stage values is the product of two input probability distributions fig 4 the first input distribution is biweekly stage change probability about the central tendency the biweekly change distribution for each gage irrespective of simulation is a normal distribution with mean equal to the central tendency value for the current time step and standard deviation equal to the biweekly historic stage standard deviation derived from eden stage data for the appropriate category of dry median or wet fig 4a the second input distribution is daily stage change probability or how much stage is likely to change in a single day the daily change distribution varies with respect to gage biweek and simulation it is normally distributed with mean equal to the sum of current stage and mean stage change for the current biweek the standard deviation is equal to the absolute value of the sum of current stage and the biweekly historic stage change standard deviation derived from eden stage data the product of the two input probabilities is the probability distribution for the next day s stage fig 4b a random value is taken from this distribution as the stage value used for the next day in this individual simulation the value is calculated as a standard normal random value multiplied by two standard deviations plus the mean of the next day s stage probability distribution finally a random uniform number between 0 and 1 is selected and compared to the trend probability value for the gage the trend probability value is the number of days expressed as a percent on 0 1 scale which a gage moves in the same direction over a given biweek whether that be increasing or decreasing stage and gives an indication of how stable or jittery a gage s water levels will be for a particular biweekly period if the selected random number is greater than the trend probability and the previous stage change was moving in the same direction or if the random number is less than the trend probability and the previous stage change was not moving in the same direction the direction of change is reversed and applied to the old stage value to arrive at the new stage value this is done to keep the movement of stage in line with historical trends e g drydown periods an example of the central tendency and 150 monte carlo simulations are displayed in fig 4c for gage site 62 located in water conservation area 3a the figure illustrates an example of the changing variability around the central tendency resulting from different seasons the variance in the simulations widens as the simulations move away from the starting date because we are moving from the dry season to the wet season and variability about the central tendency during the wet season is greater there may also be some increased variability from increasing temporal uncertainty away from the starting date but that effect is lessened since the methods favor simulation movements toward the central tendency the effect of season and time since start of a simulation is further explored in section 3 2 monte carlo assessments interpolation to continuous water depth maps under different probable near futures are illustrated in fig 4d 2 5 spatial interpolation the forecasted daily stage values at the 207 gages are used to generate water surfaces using an adaptation of a surfacing application developed and used in the eden program eden uses the gage values to simulate canal and levee boundaries that impede continuous flow across the everglades and then uses a radial basis function to interpolate water surfaces independently for subareas that have little to no influence on each other and last stitches these surfaces together to create a continuous water surface over the eden extent telis et al 2015 water depths are calculated using the eden dem that was developed from a combination of the high accuracy elevation data haed electronic atlas and aerial height finder ahf data telis et al 2015 2 6 central tendency error we ran everforecast hindcasts quarterly to compare the daily central tendency surfaces to eden historical surfaces january 2000 december 2019 we calculated mean daily deviation from eden water stage and summarized differences with root mean square error rmse mean absolute error mae and mean biased error mbe over the period of record to analyze the overall performance of the forecasts rmse and mae measure residual errors and rmse measures error magnitude and gives greater weight to larger errors mae measures model bias and mbe shows over and under predictions of the water stage as positive and negative values rmse mae and mbe were also generated for the 3 years selected earlier as representative of dry 2007 median 2010 and wet 2013 years during the 2000 2019 time period of the hindcasts these measures gave an evaluation of the effects of extremes on the observed errors 2 7 monte carlo simulation evaluations the 2000 2019 hindcast data set and the representative dry median and wet years were also used to evaluate the performance of the stochastic simulations about the central tendency for each day at each gage we determined the maximum and minimum hindcasted water stage from the set of simulations to determine how well the observed variation was captured by the range of monte carlo simulations we calculated the percentage of days that the eden stage value exceeded either the maximum or minimum simulated stage the percent of days with exceedances was mapped by gage to evaluate the spatial distribution time since the start of a simulation may affect the spread of the simulations and their deviation from observed water levels to evaluate that effect we calculated the absolute deviation relative to the start of a run the absolute deviation is equivalent to the mae but is not referred to as an error because the monte carlo simulations are purposely a spread around the expected so they are not errors so much as possibilities time of the year may also affect the spread of the simulations absolute deviation by time of the year was also calculated 3 results assessments of the accuracy of the central tendency and conditions effecting the spread of the monte carlo simulations are presented 3 1 central tendency assessments fig 5 plots everforecast hydrology and precipitation for comparison to eden measured stage hindcasts were started once every quarter and run for six months the hindcasts follow the eden water surface most of the time when they deviate is most often is at the tops or bottoms of stage peaks as the model lags the observed water levels change in direction over the whole spatial extent the mean rmse is 20 98 cm the mean mae is 14 42 cm and the mean mbe is 0 91 cm table 3 forecast that start at the beginning of quarters one three and four increase rmse and mae from 3 months into the simulation to 6 months table 4 the second quarter is a transitional period between the south florida wet and dry seasons and displays higher error earlier in the forecasts before reducing to more predictable values associated with the third quarter rmse of the daily difference in water stage between the central tendency and eden historical values is mapped for all days from 2000 to 2019 in fig 6 we see the highest differences from eden historical stage at the northwestern corner of water conservation area 3a along the levees between water conservation area 1 2a and 3a on the western border of big cypress national preserve and in the south central to southeastern area of everglades national park these are areas of higher elevation such as the marl prairie and pine rocklands in everglades national park and the levees are areas of abrupt change 3 2 monte carlo assessments fig 7 presents the quantile distribution of monte carlo simulations at six gage locations distributed spatially across the everglades starting in the second quarter of 2010 the second quarter forecast is shown because as previously noted that is a transitional period between the south florida wet and dry seasons when we expect the most variance 2010 is a median year for water levels we might expect when forecasting with the influence of precipitation that the earlier hindcast dates will be closer to eden measured water depths than later dates in these examples we see close tracking across the entire 6 months at some locations and other locations where deviation from eden begins after about the first 3 months though remaining within the 10 90 bounding quantiles gage do1 a short hydroperiod marl prairie location has markedly higher variance among the simulations than the other examples this is consistent with the spatial distribution of central tendency error that we discussed above spatial differences demonstrate where forecasting bias under or over prediction occurred in the greater everglades fig 8 the everforecast hindcasts of water stage accurately captured measured stage variation with a low percentage of measured stage exceeding hindcasted values the range of everforecast simulations better captured lower water stage values than higher stage values i e the historical stage values exceeded monte carlo minimums less often than exceeded monte carlo maximums eden historical water stage was more likely to exceed maximum simulated stage at gages located in northern big cypress national preserve in the drier marl prairie pine rockland in everglades national park and when located on or near a water structure no spatial pattern was recognized for eden historical water stage exceeding the simulated minimum the largest mean percentage of days exceeding extrema was 3 41 of days exceeding a maximum in 2004 the percent for all other years was less than 1 00 taking the average over all gages historical eden water stage exceeded the maximum stage of simulations most often in the third quarter july september 1 16 of days the wet season and exceeded the minimum stage most often in the fourth quarter october december 0 20 of days a transitional period to the dry season the absolute difference ad from eden water stage in each of the 150 simulations in relation to the time of year is plotted in fig 9 a by julian date and by the quarter in which they start because the simulations are six months long the fourth quarter simulations start in october but continue through march so they plot at both julian days 274 365 and days 1 90 the simulations start at current know water stages and therefore zero ad and typically reach a relatively stable maximum typically within the first 2 months where the second quarter maximum ad values exceed the other quarters it may peak and then drop quickly into the third quarter consistent with what we have seen previously the do1 location in marl prairie has the largest range of simulation values during the second quarter as expected the range of the stochastic simulations closely relates to the magnitude of the standard deviation value used as input for biweekly stage change probability about the central tendency fig 9b the pattern tracks both throughout the year and per gage fig 10 is a closer look at differences from eden across all the spa gages at time since start of the simulation we again observe the pattern this time with rmse of leveling off at approximately two months and rmse peaking during the second quarter and then dropping to third quarter values 4 discussion the everforecast is a novel statistical approach to spatially continuous water stage forecasting for resource and operational management the modeled simulations present scientists and managers with the likely range of possible future conditions from which ecological response and sensitivity over probable ranges of water stage can be evaluated across the landscape and within specific regions percentages of historical measured water stage exceeding everforecast simulations were low fig 8 the second quarter in south florida is a transitional period between the dry and wet season the shorter hydroperiod landscapes such as the marl prairie are the most challenging to forecast during these transitional periods as water levels are typically below ground but rising to a ponded condition these conditions require further investigation to improve the model the south florida hydrodynamic models also struggle to simulate below ground depths in this landscape new six month forecasts were generated quarterly for testing purposes in use however everforecast generates new six month forecasts each month instead of quarterly to achieve greater accuracy in forecasts as the season progresses while there are some limitations stemming from the use of a statistical model framework we believe that we have addressed these potential weaknesses in a variety of ways one common criticism is that because statistical models are tailored to historical data factors such as increased variation and an increase in extreme events caused by climate change are not captured in the models because the data that the model is based upon do not contain the variation that is expected in the future we address this issue by using appropriate model inputs in our forecast such as cpc s numerical weather forecast furthermore while extreme events and fundamental changes in the way parts of the system operate are important considerations for long term and moderate term forecasting everforecast is specifically restricted to short term forecasting here we have demonstrated an accurate forecast time frame using variability over the last two decades and new daily data will continue to inform distributions each consecutive year precipitation forecasts are critical inputs to the success and future improvements of everforecast near term water stage forecasting downscaled climate models such the north american multi model ensemble nmme may provide an effective new source of precipitation forecasts in the future improvements to downscaled climate models for application to shorter time mean predictions of local precipitation patterns have been reported jung et al 2012 kirtman et al 2012 siqueira and kirtman 2016 laurindo et al 2019 currently while south florida precipitation forecasts have improved considerably from past modeling they still have poor performance o lenic et al 2008 that may partly be because the current downscaled resolution is still inadequate to represent convective precipitation over the south florida peninsula infanti and kirtman 2019 everforecast is being introduced to everglades restoration modeling for a suite of species distribution models wading birds alligator cape sable seaside sparrow small fishes apple snail and snail kite the multi species ensemble of models is also one of the attempts in the everglades restoration programs at introducing uncertainty from a range of probable inputs to the ecological models rather than treating each species separately everforecast allows users to investigate the consequences of water management that favor a particular species or group of species on the habitat of the other species other processes such as peat fire risk are also under development for inclusion as are on line decision support tools for spatial valuation of competing restoration objectives romaach et al 2018 presentation of the results may be simplified and potentially made more useful by categorization of the many monte carlo simulations into groups such as 1 median stage and recession rate 2 high stage low recession rate 3 high stage high recession rate 4 low stage high recession rate 5 low stage low recession rate fig 11 is an example of the everforecast application to a species distribution model that is in development with everglades multispecies water coordination team biologists and other shareholders evaluations such as these provide supporting information for multi agency management recommendations spatially explicit near term forecasting in the everglades is possible because of a data rich environment with a network of daily near real time water stage stations not all watersheds have the advantage of a data rich environment to apply the same methods however many riverine systems for example are sufficiently gaged for water managers to apply low flow metrics and calculations to deal with complex water supply endangered species and water quality issues one example the truckee river reservoir operations in california and nevada hong et al 2016 rely heavily on forecasting for medium range management to meet minimum environmental and water supply flow requirements through the duration of droughts such critical decisions for adaptive management of a natural resource can be further informed by explicitly addressing uncertainty with regularly updated probabilistic monte carlo simulations as we have applied in the everglades everglades national park has taken the everforecast hydrologic forecast and applied them to several early operational applications hill and redwine personal communication quantile evaluations such as in fig 6 are being used in presentations to resource managers to forecast soil oxidation and fire risk fig 12 illustrates an additional capacity of everforecast in which in addition to using eden surfaces as our analog months surfaces generated in a hydrodynamic model south florida water management district 2005 as restoration scenario alternatives were used as the analog months the figure plots the difference between the eden forecast and the alternative scenario forecast for the example location shown lower quantile dryer simulations are wetter in the alternative and higher quantile wetter simulations exhibit lower maximum water depths in the alternative the implication for everglades restoration is that this alternative meets an objective at this location of narrowing the extremes of high and low stages experienced 5 conclusions in a continuing close collaboration with agency natural resource managers everforecast provides near term mapped hydrologic forecasts that are necessary for species distribution modeling the spatial distribution of forecasts is valuable to managers for assessing current and modeled conditions for the potential to modify operations and improve ecological conditions more robust management decisions have been shown to result when uncertainties are incorporated in decision making problems regan et al 2005 nicholson and possingham 2007 everforecast offers the user a spatially explicit hydrologic forecast which includes a range of outcomes to reflect uncertainty that influences natural resource management decisions software availability name of software or dataset everforecast developer joint ecosystem modeling usgs warc contact information stephanie s romaach phd u s geological survey wetland and aquatic research center 3321 college ave davie fl 33314 phone 954 377 5921 email sromanach usgs gov year first available 2017 hardware required 50 gb disk 8 gb ram x86 64 cpu software required os ubuntu 18 04 64 bit osx 10 15 windows 10 64 bit java runtime environment 8 mixed integer linear programming solver lp solve netcdf 4 docker optional availability https jem gov everforecast cost free program language java program size 150 mb web declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national park service interagency agreement p16pg00252 and the u s geological survey greater everglades priority ecosystems science data for historic water stage and nexrad rainfall used for model development and evaluations is supplied by the u s geological survey s everglades depth estimation network thank you to caitlin hackett for figure contributions we are grateful to the anonymous reviewers and julien martin for helpful comments on a previous version of this manuscript any use of trade firm or product names is for descriptive purposes and does not imply endorsement by the u s government 
25982,realtime information on sediment and nutrients generated and transported from catchments is essential to inform management decisions aimed to improve the ecosystem health of the great barrier reef gbr a water quality modelling methodology is developed to provide accurate and reliable estimates of sediments dissolved and particulate nutrients i e nitrogen and phosphorus for historical simulations realtime status and forecasts a water quality model is built for seven key water quality constituents at ten locations in eight gbr catchments using a non linear multivariate regression technique covariates used in the multivariate regression models were derived from either streamflow baseflow or time based cyclical processes the performance of models varied by site location and constituent and out of 67 models developed here 27 models have nse values 0 5 and 57 models have nse values 0 3 these 67 hourly models have been used to generate historical simulations and forecasts of concentration and load keywords water quality constituent covariate multivariate regression model great barrier reef gbr 1 introduction in a stream reach solutes or particulates are transported along with the water the rate of transport is non linearly related mainly to the velocity of the water and the velocities vary in a less uniform manner than pressures in the water flow furthermore the concentration of sediment and water depth can also be a driver there are also delays in the solutes and particulates reaching the stream as they can end up being deposited on flood plains or in sediments before ultimately reaching the stream or being released by the surface and sub surface water in subsequent events the main pathways for the transport of water or solutes hereafter this will also imply particulates from the soil in a catchment to a stream are surface runoff sub surface flow or interflow and groundwater flow the solutes may take more than one of these pathways i e the sub surface flow may become surface flow further down the slope where exfiltration can take place once in the river system the solutes are subject to further modifications due to reactions volatilization dispersion deposition and resuspension and have been incorporated in models of varying complexity such as physically based distributed models semi distributed models and statistically based models qual2e brown and barnwell 1987 is a physically based distributed model and this and similar models have been reviewed by cox 2003 the development of process based distributed catchment hydrology models is reviewed by paniconi and putti 2015 and shows the progress and difficulties associated with such modelling the addition of solutes adds an extra layer of complexity in the process based models because the solute transport in the soil profile and slope needs to be modelled along with the erosion process for particulates at the soil surface further the exchange of solutes in the surface soil with any ponded water also needs to be modelled one of the limitations of the distributed hydrologic models is that these models are difficult to parameterise when only limited data is available to inform the modelled processes the resulting equifinality makes selection of a unique set of parameters ill posed beven and freer 2001 fully distributed solute catchment models pose even greater complexities in parameterisation that can add uncertainties for similar reasons shetran is an example of a fully distributed hydrological and solute transport model ewen et al 2000 semi distributed hydrological and solute transport models also exist such as source which is formerly known as watercast cook et al 2009 dougall and carroll 2013 swat arnold et al 1998 and catsalt tuteja et al 2003 the source and swat models use point scale processes which are then scaled up to the functional units in source and hydrologic response units in swat the qual model has been used for river system transport when combined with swat debele et al 2008 water and solutes are routed through the channel before transporting through the river system these models have not been able to accurately simulate day to day variations in the concentration of nutrients robson and dourdet 2015 statistical models have been widely used to model water quality cohn 1995 cohn et al 1992 colby 1956 miller 1951 and have been used in australia joo et al 2014 kroon et al 2012 kuhnert et al 2013 2012 robson and dourdet 2015 rustomji and wilkinson 2008 wang et al 2011 these models relate flow rate and other factors such as the amount of flow prior to the estimated time seasonal factors and the limb of the hydrograph to the concentration and or load this form of modelling avoids the difficulties inherent in scaling up processes from the point scale to the catchment scale grizzetti et al 2005 haan et al 1995 they generally result in a better fit around the mean of the flow concentration relationship venables and dichmont 2004 than streamflow solute models furthermore a statistical based modelling approach provides a natural way to estimate model uncertainty kuhnert et al 2013 2012 wang et al 2011 the major limitation to statistical model development is the requirement of long term historical data additionally performing the dynamic modelling and scenario analysis by changing the topographical physiographical geomorphological attributes and climatological data are other limitations of statistical modelling if long term historical data is not available then the model parametrisation is difficult which adversely affects the model performance for several catchments that discharge to the great barrier reef there is sufficient data to develop statistical models these models once developed can be computationally very efficient as they only require computation of a single equation for each constituent following a review of modelling approaches cook 2014 a statistical approach to modelling the water quality in targeted great barrier reef catchments was selected as this method could deliver results at an hourly time scale required for the marine models and would be computationally efficient all types of water quality models i e distributed semi distributed and statistical have some errors in the outputs which can be quantified using various metrics and the same has been done in this study in this study we developed statistical models for estimating the concentration of seven solutes constituents total suspended solids tss dissolved inorganic nitrogen din dissolved inorganic phosphorus dip dissolved organic nitrogen don dissolved organic phosphorus dop particulate nitrogen pn and particulate phosphorus pp the method used has unique features including the use of integrals and differentials of streamflow and derived series and a fixed value power transform to normalise and stabilise variance of constituents and covariates the aim of the study was to develop computationally efficient statistical water quality models that only require easily available data such as streamflow and its components an additional aim was to enable the efficient deployment of the method for other locations and constituents subject to suitable data availability the models developed have the capability to be used on a daily updating operational context for generating hourly status and forecasts of water quality concentration and load the water quality models developed are designed to provide inputs into the marine hydrodynamic and biogeochemical models implemented for the great barrier reef gbr lagoon baird et al 2016 herzfeld 2015 steven et al 2019 these marine models require sub daily inputs from the catchment models the data and process used to develop the statistical models along with the results and discussion are presented in the following sections 2 study area and data the great barrier reef gbr region extends more than 2300 km along the queensland coast of eastern australia there are 35 major catchments which drain to the gbr lagoon and the total area of these catchments is 423 070 km2 furnas 2003 steven et al 2014 of the 35 catchments 8 are considered in this study due to the availability of water quality monitoring observations long term hourly streamflow data and hourly streamflow forecasting models the details of these 8 catchments are discussed in the next section 2 1 study area the 35 major catchments of gbr are divided into 6 natural resource management nrm regions i e cape york wet tropics burdekin mackay whitsunday fitzroy and burnett mary the 8 catchments considered in this study cover 83 of the 35 gbr catchments area and at least one catchment is located in each nrm region table 1 fig 1 of these 8 catchments water quality models were developed at water quality monitoring locations given that the streamflow monitoring locations end of system eos most downstream streamflow gauging station are either located at the same location or upstream on the same river and free from tidal influence additional models were also developed for two sub catchments in the fitzroy catchment dawson and comet table 1 the contributing areas upstream of the water quality monitoring for these locations vary from 327 km2 to 139 230 km2 table 1 in a few catchments the water quality monitoring locations are downstream of streamflow gauging stations therefore the contributing areas at water quality monitoring locations are slightly larger than the streamflow gauging locations table 1 2 2 water quality constituent observations the water quality constituent observations are obtained from the department of environment and science des queensland government https www des qld gov au des has an ongoing program to take observations as part of the great barrier reef catchment loads monitoring program and publishes the results in the annual reports turner et al 2013 2012 wallace et al 2015 2014 this dataset contains instantaneous and irregularly sampled measurements of water quality with generally more observations during high flow periods than dry periods the duration of water quality sample observations used is from 2006 to 2014 but vary between locations table 1 as water quality observations are instantaneous values recorded at irregular time intervals the total number of samples is limited 2 3 streamflow observations the hourly streamflow observations used in this study are taken from the water data online product of the australian bureau of meteorology http www bom gov au waterdata the water data online product contains historical streamflow observations from water level stations across australia the dataset is kept current through a process of daily updates collated from the state and local government agencies responsible for monitoring water levels each agency also supplies rating tables for each gauging station which are used to convert recorded water level to streamflow the duration of streamflow data used in this study is from the start date of the observation record of each site to june 2016 table 1 2 4 data preparation and quality assurance checks some of the values in the water quality samples used are marked as lower than the detection limit of the instrument standard practice is followed and these values are replaced with half of the detection limit croghan and egeghy 2003 for example in water quality sample observations a value marked as 0 03 mg l indicates that the actual value is lower than 0 03 mg l but it cannot be measured due to the detection limit of the instrument therefore this value 0 03 mg l is replaced with 0 015 mg l the number of water quality samples lower than the detection limit vary with the constituent and sites here the minimum to maximum percentages of the data lower than the detection limit for all sites is mentioned in the bracket for each constituent tss 0 2 din 0 12 dip 0 8 don 0 5 dop 21 81 pn 1 15 and pp 0 35 for the dop the percentage of data lower than the detection limit is much higher than the other constituents it is mainly because the dop concentration is generally lower than the other constituents additionally some of the instances are flagged as not available or not recorded these instances are treated as missing data as the water quality modelling is performed at an hourly time scale a continuous time series of constituent concentration is required and this was prepared from the available quality assured instantaneous and irregular water quality sample dataset consequently water quality samples are only available for specific time steps of the continuous time series and the vast majority of the time steps correspond to missing data if there were more than one sample recorded during an hour then the values are arithmetically averaged to prepare a single value for that time step a parameterised quality assurance semi automated process was applied to the hourly streamflow observations to remove i outliers i e spikes or very high flows not supported by nearby values ii linearly interpolated values i e long linear interpolation iii constant values i e same values for long duration and iv negative numbers i e any values lower than zero 3 methodology the aim here is to develop water quality models for historical simulation and forecasting of concentrations and loads for the following water quality constituents tss din dip don dop pn and pp as these models will be used in the historical simulation current status update and forecasting computational efficiency is an important criterion to achieve these aims statistical models are used in this study as these models are computationally efficient and match the available data the key steps involved in the development of statistical models are presented in fig 2 and each one is discussed in detail in the following sections 3 1 covariates the main pathways for the transport of water quality constituents from the soil in a catchment to a stream are either streamflow or its components such as surface runoff sub surface flow or interflow groundwater or baseflow fig 3 therefore streamflow and its components are considered in this study as the covariates cook et al 2017 further the water quality constituents can take one or several of these pathways i e the subsurface flow may become surface runoff further down the slope where exfiltration can take place fig 3 to develop water quality models various streamflow based covariates are generated which are discussed in the following sections 3 1 1 streamflow the streamflow q is a convolution of the point scale processes that are or have occurred in the catchment at each point in the catchment the flow of water during a rainfall event either infiltrates the soil or runs off the surface the quantity of water starting from each point in the catchment can be considered a time series with the volume decreasing as evaporation and infiltration processes along its travel path take place thus the end of catchment streamflow is a convolution of all of these time series integrated across the catchment we initially considered using the rainfall data at stations across the catchments as this could add information about whether flow from different parts of the catchment would result in different responses we found that this added a level of complexity mainly because of the poor geographical distribution of rain gauges in many of the catchments whereas good quality streamflow data is available for these catchments therefore instead of climate variables the quality assured hourly streamflow data at the closest gauge to the water quality measurement location is used as a covariate to develop statistical water quality models cohn et al 1992 colby 1956 miller 1951 hourly streamflow time series are generated at water quality monitoring stations for the 10 locations considered in this study out of all of them the streamflow observations at 4 locations table 2 are either not available or available for short duration at water quality monitoring locations therefore streamflow is transferred from the upstream water level gauging stations to the downstream water quality monitoring locations for the development of water quality models at those water quality monitoring locations the flow transfer method is based on the proportional increase in the catchment area water volume and flow travel time from the upstream water level gauging station to the downstream water quality monitoring locations the methodology is described in gbr catchment loads monitoring annual reports turner et al 2013 2012 wallace et al 2015 2014 and the flow transfer equations developed at 4 locations following the above method are presented in table 2 3 1 2 baseflow the water that infiltrates into the ground surface can contribute via deep drainage to the groundwater flow and can also contribute to the streamflow as either exfiltration flow or baseflow q bf the baseflow component of the streamflow is determined by a baseflow filtering approach developed by nathan and mcmahon 1990 the baseflow is calculated on a daily time scale and uniformly disaggregated to an hourly time scale the baseflow is used as a covariate because it could be the pathway of dissolved solutes such as nitrate and nitrite the hourly baseflow time series are separated from observed streamflow at water quality monitoring stations for the 10 locations considered in this study 3 1 3 cumulative difference between streamflow and mean another streamflow component that is considered in the study is the cumulative difference between streamflow and the mean q m which is a measure of the catchment hydrological conditions when this value is negative it means that the catchment is drier than the average and the magnitude gives an estimate of how dry when the values are positive the catchment is wetter than average and the magnitude gives an estimate of how wet cook et al 2017 presented data for the fitzroy catchment which showed very clearly the millennium drought from 2000 to 2009 the q m is calculated at a particular time by accumulating the difference between streamflow and long term mean eq 1 1 q m t i 0 t q i q where q i is the streamflow at ith time step q is the long term streamflow mean calculated from available streamflow observations for the entire period and t is the time since the start of the long term streamflow record q m varies with time from positive to negative depending on the climatic conditions that have occurred since the start of the record and will return to zero at the end of the record the positive and negative values of q m indicate the wet and dry conditions of catchment respectively particularly during the period for which q m is calculated the start date of the streamflow record varies from site to site table 1 the hourly cumulative difference between streamflow time series and the mean are generated at water quality monitoring stations for all locations considered in this study 3 1 4 differential of variables the differential of variables streamflow baseflow and the cumulative difference between streamflow and mean is the change in the variable rate with time which is approximated using the difference approximation eq 2 2 d x d t t i c x i 1 x i 1 t i 1 t i 1 t i 1 t i t i t i 1 1 where x is the variable q q bf and q m in m3 s and t is the time in hour the index i is the point in the time series record with i 1 and i 1 the next and previous record values and c is a constant to convert the output to m3 s2 which is 1 3600 in this case the differential of streamflow baseflow and the cumulative difference between streamflow and mean are used as covariates in developing the multivariate regression models it has often been considered that the changes in the concentration of water quality constituent depend on whether the flow is increasing or decreasing marsh and waters 2009 the differential of the streamflow provides a convenient method for determining whether the streamflow is increasing with the time dx dt 0 steady with time dx dt 0 or decreasing with time dx dt 0 the magnitude of the values 0 the rising limb of hydrograph indicates how quickly streamflow is increasing and possibly the increase in the area of contribution rainfall and or spectrum of velocities of surface runoff in the catchment this relates to constituent concentration change due to the erosion on hillslopes and gullies and the removal of sediments from the storages within the catchment and stream the magnitude of values 0 the falling limb of hydrograph indicates the rate of decrease in the contributing area rainfall and or decrease in the spectrum of velocities of surface runoff in the catchment this could relate to the constituent concentration changes due to the deposition processes in the catchment on floodplains and streams for both dissolved and particulate constituents for some of the catchments the differential of the baseflow shows good relationship with the dissolved constituents such as don and din the hourly time series of differentials are generated for streamflow baseflow and the cumulative difference between streamflow and mean for 10 locations considered in this study 3 1 5 integral of variables the total amount of streamflow passing through the river channel in the previous period can significantly influence the constituent concentration this is because the duration of small or low flows can result in the storage of water quality constituents on flow planes the build up of concentration in the soil or riparian zones the generation of erodible materials in gullies and storage or depletion in water storages such as dams and weirs periods of extended high flows can result in opposite changes in the constituent concentration due to the processes listed above in previous studies a discounted rate whereby the past flow has less effect with time has been used kuhnert et al 2013 2012 robson and dourdet 2015 wang et al 2011 here we will not discount the rate as this assumes the past events have less effect with time instead we will integrate the streamflow into the past for fixed periods of time and consider the effect of this integral on the constituent eq 3 3 i x t t a x d t where x is the variable q q bf and q m t is time and  is the time period into the past of the integral various values of  such as 1 2 5 10 20 50 and 75 days are considered in this study due to different sizes and response times of catchments note that here streamflows are integrated at each time step for the number of days mentioned above and not restricted only during an event the hourly time series of integrals of various durations mentioned above are generated for streamflow baseflow and the cumulative difference between streamflow and mean for all 10 locations considered in this study if there is missing data during the integration period then integrals are only calculated if 90 or greater than 90 of the data is available during that period the same 90 threshold is used by the streamflow data providers to generate hourly streamflow data 3 1 6 time based covariates the water quality constituent data may have short range periodic time dependent variation due to annual changes in temperature crop growth patterns and land management practices such as fertilization and harvesting cohn et al 1992 have modelled these short ranges temporal variations by adding both sine and cosine terms of their modelling approach these sine and cosine terms are also considered in this study eq 4 4 s t sin 2  t s p c t cos 2  t s p where st and ct are sine and cosine time based covariate values t s is the time in days since the water quality constituent observation is available and p is the period of 365 days for non leap year and 366 days for leap year the use of both sin and cosine which are orthogonal means that this approach can detect if a time based periodic signal is in phase or out of phase the hourly time series of time based covariates are generated for all seven constituents for the 10 locations of the study 3 2 covariate selection all constituents and covariates except time based for the 10 locations are transformed using box cox transformation with fixed 0 2 power function to stabilise variance and normalise the range this fixed value of power function is used instead of a log transformation as the long exponential tail of the log transform create problems in the calculation of the uncertainty mcinerney et al 2017 to determine the general predictability of each covariate linear and polynomial order 2 regressions were performed between each water quality constituent and covariate pair for each site this is repeated for all combinations of transformed constituents and covariates the covariates which show an r2 coefficient of determination 0 1 are selected to develop water quality statistical models the r2 value of 0 1 may be considered low in terms of predictability but when these covariates are used together with other covariates in the statistical model the combined performance may be improved at this stage of covariate selection the poor predictability covariates are filtered out if none of the covariate show r2 0 1 for any particular constituents at a particular site then no water quality model is developed for that constituent and site the selected covariates in both normal and transformed form are used to develop the statistical model the squared form of selected normal and transformed covariates is also used to develop statistical models as squared form shows improvement in predictive power in some of the models when used in polynomial order 2 regression the development of statistical models multivariate regression is presented in the next section 3 3 multivariate regression model the intent of this study is to develop simple statistical models which are computationally efficient and can be used in realtime forecasting therefore a standard form of multivariate regression is used to develop the relationship between water quality constituents and covariates for all locations eq 5 rather than an alternative approach such as a process based model 5 y f x 1 x 2 x n c a 1 x 1 a 2 x 2 a n x n  where y is the seven water quality constituents tss din dip don dop pn and pp in normal or transformed form x 1 to x n are covariates in normal transformed and squared form the n is the maximum number of covariates used in the multivariate regression which is restricted to 5 in this study c is the intercept and a 1 to a n are the coefficients the residual standard error  is assumed to be normally distributed in either normal or transformed space and it is used to estimate the model uncertainty the method of calculation of residual standard error and model uncertainty is explained in section 3 5 a water quality constituent for a particular site in normal and transformed form are used as the dependent variable in the multivariate regression equation for developing statistical water quality models the selected covariates in normal transformed and squared form are used as independent variables in all possible combinations in the multivariate regression equation fig 4 based on the authors experience the number of covariates in the multivariate regression equation is restricted to five to avoid over parametrisation and overfitting of the model whereas all possible combinations of covariates between one to five are tried fig 4 if the number of covariates selected in the previous step is significantly high then in some cases millions of combinations of multivariate regression models are developed which are evaluated to find the best model the regression models are evaluated according to the r2 and adjusted r2 generation of a large number of multivariate regression models is a computationally intensive exercise and the australian national computational infrastructure nci was an essential resource to complete this study 3 4 model stability the complete set of multivariate regression models generated in the previous step is parameterised in two different manners i using the whole record of the water quality sample data set and using ii two thirds sample of data set the latter approach is used to check the stability of the models here model stability means that the performance of the model using two thirds data set does not significantly change in comparison to the model using the whole data set when the same covariates are used in both types of models the models which use the whole record of the water quality sample data set are called mod all from here onwards it is essential to check the model stability when applied with forcing data outside the calibration step as they will be used to forecast water quality constituent concentration and load therefore the models are validated by separating the water quality samples into two sets one third and two thirds of the samples selected at random without repeats splitting the sample into two thirds for calibration and one third for validation is quite common in hydrological modelling particularly when the number of data samples is limited the same concept is used here the multivariate regression models are generated again in all possible combinations with remaining two thirds data sets fig 4 these models are called mod 2 3 from here onwards to check the model performance r2 and adjusted r2 are calculated for the validation datasets using the same process as used for the entire dataset fig 4 other model performance measures are also used in this study and are discussed in the following sections 3 5 model uncertainty the residual standard errors  are generated for all combinations of multivariate regression models for all constituents and locations this residual standard error is used to estimate the model uncertainty through the deviation from the mean prediction to calculate model uncertainty the confidence limits used in this study are 5 and 95 which is calculated by the mean prediction 2 the predicted constituents at the 5 confidence limit which are negative are replaced with zero this results in the uncertainty limit being truncated to the physical limit of zero 3 6 model selection the multivariate regression models for each water quality constituent and location are generated for all possible combinations of covariates using the entire record of the water quality data set mod all and the two third data set mod 2 3 there are a few constituents and location combinations where no multivariate regression model is generated due to no covariates been found to have significant predictive skill after the generation of multivariate regression models the next critical task is to select the best model out of all generated models for each site and constituent which is described in the following sections and in fig 5 3 6 1 shortlisting models on the basis of adjusted r2 to select the best water quality model first the multivariate regression models for each constituent and location are arranged in decreasing order of adjusted r2 and then the best 100 models where the constituents are in normal space and best 100 models where the constituents are in transformed space are selected for further analysis and short listing fig 5 these 200 models are selected from mod all and on the basis of covariates of these 200 models another 200 models are selected from mod 2 3 which have the same covariates but coefficients are different fig 5 the r2 of these 200 models are also overviewed to confirm that the differences between r2 and adjusted r2 are not significant the mod 2 3 are used to check the stability of models here the model stability means that the performance of mod 2 3 does not significantly change in comparison to the mod all when the same covariates are used in both types of models if the total number of generated models is less than 100 in normal space and 100 in transformed space then whatever number of models are generated are used for further analysis and short listing in the majority of constituents and locations the number of generated models is much higher than 200 the adjusted r2 is preferred over r2 for selecting the best 200 models as it also accounts for the total number of variables in the regression the adjusted r2 increases only if the new variable improves the model more than would be expected by chance it decreases when a variable improves the model by less than expected by chance therefore it is helpful to avoid overfitting in the regression model the adjusted r2 is always less than or equal to r2 3 6 2 further shortlisting of models on the basis of time series plots and nse values the selected 100 models where the constituents in normal space and 100 models where the constituents in the transformed space are used to simulate continuous hourly water quality constituents time series and compared with the water quality constituent observations the nash sutcliffe efficiency nse is also calculated for these 200 models nash and sutcliffe 1970 when the constituents are in transformed space in the model then the simulated time series is back transformed before comparing with the model observations and calculating the nse values the comparison of simulated time series with the observed samples is performed for both sets of 200 models i e mod all and mod 2 3 in a similar way the nse values are also compared for both sets of 200 models i e mod all and mod 2 3 the stability of models can be checked at the same time while selecting the best model based on nse values model stability and visual comparison of simulated and observed constituent values the best 10 models are selected out of the 200 models to select the best 10 models we have started with the highest nse value model then check the stability of that model and if the model is stable then visually compared the simulated and observed constituent values and also check the consistency in the pattern of streamflow with the pattern of simulated constituent time series if the model looks satisfactory in visual comparison then the model was shortlisted but if the model was not satisfactory either in the stability check or in the visual inspection then that model was not considered and the next model with the second highest nse value was investigated we continue this process until the best 10 models are shortlisted the main reason of performing the visual inspection of time series is that the observed water quality sample data is discrete therefore nse or other statistical measures i e adjusted r2 or r2 evaluate the model performance only at the time of observations but not throughout the entire time series visual inspection of the time series of the simulated constituent concentration enables checking for consistency with the observed water quality samples and the consistency in pattern with the observed streamflow for example that it does not show very high concentration during low flows or very low concentration with high flows the 10 models shortlisted can be a combination of transformed and normal space constituent models or could also belong to a single category they are all selected from mod all and on the basis of covariates of these 10 models another 10 models are selected from mod 2 3 the models of each of these categories are used for further analysis and short listing fig 5 the models considered in the selection are those with the smallest differences in adjusted r2 and nse between mod 2 3 and mod all we found that in the majority of selected models the adjusted r2 and nse for both categories of models are very close 3 6 3 further shortlisting of models on the basis of additional metrics and plots for the selected 10 models in each of the above mentioned categories further additional indices such as root mean square error rmse and mean absolute error mae are generated using the simulated and observed constituents data fig 5 the nse values were already computed in the previous step additional plots are generated to analyse the results in more detail and decide on the best model fig 5 these are i scatter plot between observed and simulated constituent concentration ii similar scatter plot as i but for a load iii scatter plot of simulated concentration between the two categories of the model i e mod all and mod 2 3 iv similar scatter plot as iii but only for the remaining one third data and v the time series plots of observed and simulated constituent concentration with 5 and 95 percentile confidence band for both types of model categories i e mod all and mod 2 3 on the basis of the visual inspection of the above mentioned plots and an analysis of the statistical indices computed in this step the 3 best performing models are selected which are further analysed and short listed to get the best water quality model fig 5 3 6 4 selection of single best model for each constituent and locations the performance of the 3 best models at the hourly time scale selected in the previous section is very similar for most of the constituents and locations therefore selecting one best model is not trivial we have added one further criterion i e the annual load comparison to the process to assist with choosing the final model the annual load integrates the mass of the constituent over a large time step which ensures that an hourly model when integrated up to an annual mass produces acceptable results the procedure is described below to find the best model the simulated constituent concentrations are converted into a load by multiplying with the hourly streamflow with the estimated constituent for the same duration which is then aggregated up to compute the annual load these annual loads are compared with the loads given in the gbr catchment load monitoring reports turner et al 2013 2012 wallace et al 2015 2014 the simulated annual loads are also calculated for 5 and 95 percentile confidence band the load year considered in the load monitoring report is from 1st july to 30th june after investigating model performance at annual scale and re examining all the statistical indices and various time series and scatter plots described previously one best model is selected fig 5 in the model selection approach described in this section not all performance plots and statistical indices are generated for all the multivariate regression models as visual inspection of all of them is not feasible therefore a systematic narrow down approach is used in this study to find out the one best model i e from all multivariate regression models to 200 models then 10 models then 3 models and then finally one best model based on various criteria described throughout this section by following this systematic approach we only need to generate a limited number of plots that are practically feasible to analyse the analysis described here is repeated for all 7 constituents and 10 locations covered in this study a total of 67 water quality models is successfully generated the remaining 3 models are not generated due to very poor relationship between constituents and covariate and also due to the high percentage of water quality samples lower than the detection limit and these are mainly for dop the results of these models are presented and discussed in section 4 3 7 forecasting the adopted water quality models are also used with streamflow forecast to generate the water quality constituent concentration forecast a brief description of the streamflow forecasting model and the water quality constituent forecasting process used are presented in the following sections 3 7 1 streamflow forecast in this study we have generated 3 days lead time hourly deterministic streamflow forecasts daily this streamflow forecast is used in water quality models to generate water quality constituents forecast which is discussed in detail in the next section the streamflow forecasts are generated using the short term water information and forecasting tools swift hapuarachchi et al 2017 kabir et al 2018 pagano et al 2011 perraud et al 2015 swift is a streamflow modelling package designed for both operational streamflow forecasting and scientific research it is a collection of hydrologic modelling components and utilities that support model calibration validation and forecast verification hapuarachchi et al 2017 kabir et al 2018 pagano et al 2011 perraud et al 2015 the gr4h rainfall runoff conceptual model is used in swift as the performance of this model is satisfactory across various catchments in australia hapuarachchi et al 2017 perrin et al 2003 note that the swift modelling system with gr4h model is used to generate 7 day streamflow forecasts service http www bom gov au water 7daystreamflow by the australian bureau of meteorology the australian community climate and earth system simulator access r http www bom gov au australia charts about about access shtml forecast rainfall data for the period 2013 to june 2016 is used to generate hourly streamflow forecast from swift for 3 days lead time this streamflow forecast data is used with water quality models to generate water quality constituent forecasts 3 7 2 water quality constituent forecast the daily generated hourly streamflow forecast data for 3 days lead time is combined with the historical observed streamflow data to generate the streamflow and its component based covariates these covariates are used in adopted water quality models to generate the water quality constituent concentration forecast these constituent concentrations are converted into a load by multiplying the streamflow values of that duration the results of the water quality forecast for selected constituents and locations are discussed in section 4 4 results and discussion the covariates used to develop each water quality model and the performance of these models vary for the constituents and locations and are discussed in detail in the following sections 4 1 covariates of the adopted water quality models the covariates of the adopted water quality models after the selection process for all seven constituents in ten locations are presented in table 3 for some of the models the dependent variables constituents are in transformed space and highlighted by a grey colour table 3 for dop there are no models at three locations south johnstone river at upstream central mill herbert river at ingham and sandy creek at homebush due to very poor relationships between constituents and covariates table 3 and to the high percentage of water quality samples lower than the detection limit for these three locations none of the covariates have r2 0 1 and therefore no covariates were selected the streamflow its differential and integral either in normal or transformed form appear as the covariates for 56 out of 67 water quality models table 3 streamflow appears to be a strong driver for generating both particulates and dissolved constituents the streamflow differentials and integrals are also strongly correlated with several constituents as they provide information about the rate of change of streamflow and cumulative streamflow in the past which we can assume is predictive of water quality concentration the cumulative difference between streamflow and mean its differential and integral either in normal or transformed form appear as the covariates in 49 out of 67 water quality models table 3 these covariates appeared as highly correlated covariates with several constituents as these covariates are indicators of catchment hydrological conditions to the best of the authors knowledge this is the first time the authors are aware that the cumulative difference between streamflow and its mean its differential and integrals have been used as covariates in the development of water quality models the baseflow based covariates either in normal or transformed form appear as the covariates in 41 models out of 67 water quality models table 3 the baseflow based covariates appear in both particulate as well as dissolved constituents but for din and dip they appear as covariates in 16 out of 20 models which shows that the baseflow is the strongest driver for dissolved constituents the time based covariates only appear in 24 out of 67 water quality models where they improved the multivariate regression performance further but do not appear as strong covariates in the majority of the models we presume this is because there is no strong seasonality pattern for the majority of constituents table 3 note that all possible combinations of covariates between one to five are tried here to develop the multivariate regression model therefore if an individual covariate improves the model performance then it is considered in the finally selected model otherwise the model without that covariate would have been considered finally for don and dop models for the burdekin river at home hill only time based covariates appear in the adopted model but the performance of these models is poor the performance of all models is discussed in detail in the next section 4 2 model performance checks using statistical indices the adjusted r2 for adopted water quality models for all seven constituents and for ten locations are presented in table 4 it includes adjusted r2 for both categories of models i e mod all and mod 2 3 note that the water quality modelling here is performed at an hourly time scale therefore adjusted r2 are also calculated at the same time scale the adjusted r2 for water quality models for tss are better than other constituents and all the values are 0 5 table 4 the highest adjusted r2 for tss is 0 83 for burnett river at ben anderson barrage head water and drops to 0 82 only for mod 2 3 for other six constituents the adjusted r2 vary significantly with the constituents and locations for pn and pp some models have high adjusted r2 up to 0 7 whereas some have low value only 0 26 table 4 the adjusted r2 for din and dip range between 0 23 and 0 88 and this upper range is also the highest adjusted r2 among all the models the adjusted r2 for don and dop models are generally low the majority of models have values lower than 0 5 table 4 models of dop are not developed for three locations due to very poor relationship between constituents and covariates the model performance in the historical simulation period and one third validation period using other statistical indices are discussed in the following sub sections 4 2 1 model performance in historical simulation period the adopted water quality models are used to generate a continuous hourly time series of constituent concentrations for the historical period during which constituent observations are available the simulated time series in which the dependent variable i e the constituent concentration is in the transformed form is back transformed using power 5 to generate the hourly time series of constituent concentration in the normal form note that the box cox transformation with the fixed 0 2 power function is used in this study as mentioned in section 3 2 as the observed water quality samples are discrete the simulations corresponding to observation time periods are extracted from the simulated continuous hourly time series and various statistical indices i e nse rmse and mae are calculated tables 5 7 these additional indices are helpful to check the model performance more thoroughly the nse values of all water quality models in historical simulation period vary by constituent and location table 5 the highest value of nse is 0 9 corresponding to dip for dawson river at taroom and lowest value is 0 01 for dop at burdekin river at home hill table 5 the performance of the models for dop are generally poorest among other constituents and the major reason is because of a large number of constant values in the observed record it is due to the measured values being recorded as below the detection limit of the measuring instrument which ranges 21 81 of the values for dop the nse values can be compared among different constituents and locations as it is dimensionless number and ranges to 1 but the rmse and mae cannot be compared as these are in mg l and scale of observation of various constituents and locations are significantly different tables 5 7 generally the tss concentration is higher than other constituents i e din don dip dop pn and pp therefore the rmse and mae are also higher for tss tables 6 7 further the rmse and mae values are low for most of the models for dop because the dop concentration is quite low the rmse and mae values are used to check the difference between the historical simulation and validation period which is discussed in the next section 4 2 2 model performance during validation period the difference in nse rmse and mae between historical simulation period and the one third validation period is negligible in most of the models see upper and lower sections of tables 5 7 the upper section indicates the model performance in the simulation period whereas the lower section indicates the model performance in one third validation period tables 5 7 this indicates that most of the models can be considered stable over the validation period tables 5 7 this is because the model stability is one of the selection criteria in adopting the model from the large number of candidate models generated for each constituent and location the highest difference in nse values between the historical simulation period and the validation period is for pp for normanby river at kalpowar crossing in this case the nse value during the simulation period is 0 29 but for the validation period it is 0 07 table 5 it indicates this model is not stable enough though it is the best model out of all other models generated for this site and constituent further the nse is sensitive to high values therefore showing large differences whereas the rmse and mae values for same constituent and location do not show the same order of difference tables 6 and 7 therefore for checking the stability of models during validation period the rmse and mae are also used the performance of the model is also checked through visual inspection of a variety of plots which are discussed in the next section 4 3 model performance checks using various plots the performance of all 67 water quality models is assessed using various plots such as time series scatter and annual load comparison plots these plots for all 67 models cannot be presented here therefore plots for one good performing model i e a tss model for burnett river at ben anderson barrage head water and one poorly performed models i e a dop model for the burdekin river at home hill are shown in the next section the plots for other models are available from the authors on request 4 3 1 comparison of observed and simulated time series the hourly simulated time series of tss concentration for the burnett river at ben anderson barrage head water and dop concentration for the burdekin river at home hill with 5 and 95 confidence band are compared with the average observed values for the same time period in fig 6 a and b the observed streamflows are also presented to compare the trend with constituent concentrations fig 6a and b the simulated tss concentrations for the burnett river at ben anderson barrage head water match the observations throughout the simulation period well as this model was considered a well performing model with an nse value of 0 84 fig 6a table 5 further most observations are falling within 5 and 95 confidence band fig 6a the temporal pattern of the observed tss concentrations is similar to the temporal pattern of the streamflow which suggests that streamflow and its components are strong drivers for tss the simulated dop concentrations for the burdekin river at home hill poorly match the observations and this is considered to be a poorly performing model with nse value only 0 01 fig 6b table 5 further several observations are outside the 5 and 95 confidence band fig 6b the simulated time series shows the cyclical pattern because the covariates in this model are only time based covariates table 3 the main reason for the poor performance of this model is the inability to select covariates which can reproduce the large number of constant values these constant values are due to measurements being sampled below the detection limit of the measuring instrument which is 62 of the dop measurements for burdekin river at home hill fig 6b further the temporal pattern of dop observations does not show a similar temporal pattern as that of the streamflow and therefore streamflow or its components are not good predictors of the dop concentration samples at this location fig 6b table 3 the performance of most of the other models for other constituents and locations lies between these two extreme cases these plots for all models cannot be presented here but are available from the authors on request the performance of the models is also assessed using scatter plots between observation and simulation which are presented in the next section 4 3 2 scatter between observations and simulations the scatter plots between hourly observations and simulations are presented for tss concentration and load for the burnett river at ben anderson barrage head water and dop concentration and load for the burdekin river at home hill fig 7 a d for tss hourly concentration for the burnett river at ben anderson barrage head water the majority of the scatter points are close to 1 1 line irrespective of low or high concentration values which further confirms this is a good performing model fig 7a the simulated hourly concentrations of tss at burnett river at ben anderson barrage head water are converted to hourly load by multiplying the observed streamflow for the same time period the tss load shows a similar performance in fact the spread is less in load scatter plot than the concentration scatter plot because of the multiplication of observed streamflows in both simulated and observed tss concentrations fig 7b if the variation in hourly observed and simulated tss concentration is low then after multiplication of observed streamflow the variation in hourly observed and simulated tss load becomes insignificant and more scatter points are on 1 1 line fig 7b most points in the scatter plot for hourly dop concentration for the burdekin river at home hill are significantly deviating from 1 1 line which clearly shows this model is unable to capture high or low concentration values fig 7c the model heavily underestimates high values of the dop concentration fig 7c this is expected as the model only uses the time based covariates as discussed in the last section the simulated hourly concentrations of dop for the burdekin river at home hill are converted to hourly load by multiplying the observed streamflow for the same time period the performance improved slightly for load calculation due to multiplication by streamflows fig 7d the estimation of low values of load is better than the higher values as low values have deviated on both sides of 1 1 line whereas high loads only have deviated below 1 1 line showing consistent underestimation of high loads fig 7c and d 4 3 3 scatter plots for model stability checks the water quality models are calibrated using all water quality sample observations mod all and using two third water quality sample observations mod 2 3 to check the stability of the model during the entire observation sample period as well as one third observation sample period or validation period the scatter plots are generated between simulated constituent concentrations from both types of models i e mod all and mod 2 3 fig 8 a d these plots are generated to compare simulations during the entire observation sample period fig 8a and c and during one third observation sample period fig 8b and d these plots are presented for simulated tss concentration for the burnett river at ben anderson barrage head water fig 8a and b and for simulated dop concentration for the burdekin river at home hill fig 8c and d note that the covariates of both sets of models i e mod all and mod 2 3 are the same section 3 4 3 6 but the coefficients are different the simulated tss concentration for burnett river at ben anderson barrage head water from both types of models i e mod all and mod 2 3 are perfectly aligned on 1 1 line for all of the observed sample period fig 8a and one third observed sample period validation period fig 8b which shows that the performance of model has not changed when only two third data is used for calibration and the model is stable enough it is also happening because during the model selection process models which show similar performance in calibration and validation periods are preferred the simulated dop concentration for burdekin river at home hill from both types of models i e mod all and mod 2 3 show significant deviation from 1 1 line for all observation sample periods fig 8c and one third observation sample period or validation period fig 8d the mod 2 3 is consistently underestimating the dop concentration during the entire simulation as well as the validation period fig 8c and d it is happening because of poor correlation between the constituent and covariates resulting in low nse values as discussed in section 4 3 1 in the model selection process of dop at burdekin river at home hill none of the models showed stability between calibration and validation periods 4 3 4 comparison of annual loads the simulated hourly concentrations of tss for the burnett river at ben anderson barrage head water and dop for the burdekin river at home hill are converted to hourly load by multiplying the observed streamflow for the same time period these hourly loads are aggregated to load year i e 1st july to 30th june and compared with the annual loads given in gbr catchment load monitoring reports fig 9 a and b turner et al 2013 2012 wallace et al 2015 2014 the annual loads corresponding to 5 and 95 confidence interval are also calculated and compared with the loads given in the load report fig 9a and b in gbr catchment load monitoring reports the annual loads of constituents are calculated on the basis of average load derived from the linear interpolation of concentration within the year turner et al 2013 2012 wallace et al 2015 2014 this method is used when events are adequately sampled or at least with a reasonably representative sampling including the peak concentration joo et al 2012 for poorly sampled and or complex events the beale ratio method is used to calculate the annual load in gbr catchment load monitoring reports joo et al 2012 the simulated annual loads for tss for the burnett river at ben anderson barrage head water are closely matched with the loads given in the load report for all five load years i e 1st july 2009 to 30th june 2014 fig 9a for three years july 2009 to june 2010 july 2011 to june 2012 and july 2013 and june 2014 the simulated loads the loads from the load reports and the loads corresponding to 5 and 95 confidence band overlap as these years are comparatively drier for the burnett catchment relative to the other two years i e july 2010 to june 2011 and july 2012 to june 2013 fig 9a the annual simulated loads closely match the load report which indicates the model has good performance at an annual accumulated time scale turner et al 2013 2012 wallace et al 2015 2014 the simulated annual loads for dop for the burdekin river at home hill do not match closely with the loads report because this model is a poorly performing model fig 9a and b the differences between simulated annual loads and loads given in load report are high particularly for the period july 2010 to june 2011 as this is reasonably wet year for burdekin catchment fig 9b despite the higher difference the loads given in load reports are within 5 and 95 confidence band for all five years though the dop model of the burdekin river at home hill is a poorly performing model with low nse values the results at the annual scale are not as poor as at the hourly time scale it is because the loads are aggregated at annual scale and both simulated loads calculated here and given in the load reports used the same observed streamflow for multiplication with a concentration that improved the performance at the annual scale turner et al 2013 2012 wallace et al 2015 2014 4 4 performance in forecast the 3 days lead time hourly streamflow forecast are used in the selected water quality models to generate the retrospective forecast of water quality concentration and load these forecasts are generated daily for three days lead time the performance of water quality forecast varies between constituents sites and individual events further the performance of water quality forecasts is highly dependent on the skills of water quality models and streamflow forecast due to limited overlap between rainfall forecast period and water quality constituents sample observations i e jan 2013 to jun 2014 a proper statistic could not be generated but certain individual events are analysed visually and assessed at annual scale individual event analysis is further constrained due to a limited number of discrete samples during the forecast period therefore annual assessment with the loads given in the load report and comparison with the simulated load are presented in fig 10 a and b for tss for the burnett river at ben anderson barrage head water and dop for the burdekin river at home hill further the estimation and evaluation of constituents load at the annual scale are useful for reef report card perspective turner et al 2013 2012 wallace et al 2015 2014 the annual forecast loads at 1 2 and 3 days lead time are calculated by aggregating the hourly load time series for each 1 2 and 3 days lead time to the annual scale note that the simulated annual loads are calculated on the basis of observed streamflow data the annual loads are shown in logarithmic scale due to significant variation in dry and wet years and minor differences in forecast loads at different lead times fig 10a and b the annual forecast load for all 3 days lead times are close to the simulated loads and the loads given in the load report fig 10 a b turner et al 2013 2012 wallace et al 2015 2014 the variation in annual load among 3 days lead time is not significant but as the lead time increases the deviation from the load report increases for the year 2013 14 for tss for the burnett river at ben anderson barrage head water and for dop for the burdekin river at home hill fig 10 a b as discussed in the previous section the dop model for the burdekin river at home hill is a poorly performing model with low nse values but even then the results at the annual scale are not as poor as at hourly time scale because of multiplication with the observed streamflows for calculation of loads as discussed in the previous section 4 5 model performance summary the performance of most of the other models developed lies between the two models presented above i e tss model for burnett river at ben anderson barrage head water and dop model for burdekin river at home hill out of 67 models developed in this study 27 have nse values 0 5 and 57 have nse values 0 3 achieving high values of nse in water quality modelling is challenging because some of the constituents do not show good relationships with the covariates derived from the streamflow or its components such as dop in this study despite that a high number of good performing and reliable models are developed in this study all the models are computationally efficient and possible to use in daily water quality status and forecasting and will provide reliable continuous hourly simulation and 3 days lead time hourly forecast our aim here is to provide the water quality hindcast forecast for all 67 models with the model performance information so that the potential users can use the models for their decision making by considering the model performance further providing the water quality hindcast forecast with the model performance is better than providing no information because even the poorly performing models at hourly time scale are providing reasonably accurate loads at the annual scale which are useful for various potential users the nse r2 adjusted r2 rmse and mae use the observed and simulated water quality concentration at the same timestamp the water quality observed samples used in this study are discrete whereas the simulated water quality concentration generated from the models are continuous hourly time series as all the covariates have continuous hourly time series it is quite likely the water quality model fits very well at the time of observations but does not fit well outside of that range and the performance based only on the metric could be misleading therefore we have not categorised the models i e good moderate and poor based on metrics to properly categorise the model we need to investigate a new metric that will consider the multiple criteria to assess the model performance throughout the time range the development of a new metric and then categorisation of the model will be the scope of future research further the variety of covariates in various combinations used in this study have not been explored in other published research to the best of the authors knowledge as most of the covariates are derived from the streamflow or its components this methodology can be applied at any other location where the constituents and streamflow observations are available further these models do not require large spatial data sets as required in distributed process based modelling the modelling methodology and an automated system are successfully developed in this study which can be used to develop water quality models at other locations 5 conclusion a statistical water quality modelling methodology to predict constituents concentration and load has been developed and described various covariates are used in unique combinations including streamflow baseflow the cumulative difference between streamflow and mean their differentials integrals and transformations time based covariates are also used the physical basis for using these co variates is demonstrated in a consistent manner the water quality modelling methodology has been investigated for 7 constituents at 10 locations and 67 water quality models are developed for 3 locations for dop the models are not developed due to poor relationship between constituents and covariates and also due to the high percentage of water quality samples found to be lower than the detection limit the performance of the models varies between sites and constituents out of 67 models 27 models have nse values 0 5 and 57 models have nse values 0 3 validation is carried out by splitting the data sets into two thirds for the development of the model and one third for testing the model this worked well for large data sets as model performances are very similar when full data and two thirds data are used however for small datasets model performances for full and two thirds data can vary significantly further validation was performed by using models to predict the loads in a historical simulation mode and these loads were then compared to previously published loads these demonstrated that the loads calculated with the models developed from this study were generally similar to those published previously the water quality modelling methodology and associated automated system successfully developed in this study could be used to develop water quality models at other locations in the future the water quality modelling method presented here is computationally intensive therefore supercomputing platform is an essential requirement to develop these types of models at other locations in the future the water quality models developed here will be used in the future in the realtime system to generate the simulations for the preceding 14 days current status and hourly forecasts for the next three days of concentration and load these models are designed to provide inputs into the marine hydrodynamic and biogeochemical models implemented for the gbr lagoon and can also be provided to other potential users who could benefit from this data set declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this project was led by the water forecasting services team at the australian bureau of meteorology bureau the broader ereefs project which is a collaboration between the great barrier reef foundation bureau csiro australian institute of marine sciences aims australian government queensland government bhp billiton science and industry endowment fund and bhp billiton mitsubishi alliance provided financial support for this work cedric robillot and greg stuart guided this project through various stages ryan turner reinier mann belinda thomson and the broader team from queensland government s loads monitoring program and miles furnas and richard brinksman from the aims supported with water quality data and expert advice tony jakeman miles furnas and ashish sharma supported independent scientific peer review of the modelling methodology and project outcomes dasarath jayasuriya graham hawke and rob vertessy provided high level guidance and assisted with resourcing the national computational infrastructure nci supported by the australian government was used to perform large computations in this research 
25982,realtime information on sediment and nutrients generated and transported from catchments is essential to inform management decisions aimed to improve the ecosystem health of the great barrier reef gbr a water quality modelling methodology is developed to provide accurate and reliable estimates of sediments dissolved and particulate nutrients i e nitrogen and phosphorus for historical simulations realtime status and forecasts a water quality model is built for seven key water quality constituents at ten locations in eight gbr catchments using a non linear multivariate regression technique covariates used in the multivariate regression models were derived from either streamflow baseflow or time based cyclical processes the performance of models varied by site location and constituent and out of 67 models developed here 27 models have nse values 0 5 and 57 models have nse values 0 3 these 67 hourly models have been used to generate historical simulations and forecasts of concentration and load keywords water quality constituent covariate multivariate regression model great barrier reef gbr 1 introduction in a stream reach solutes or particulates are transported along with the water the rate of transport is non linearly related mainly to the velocity of the water and the velocities vary in a less uniform manner than pressures in the water flow furthermore the concentration of sediment and water depth can also be a driver there are also delays in the solutes and particulates reaching the stream as they can end up being deposited on flood plains or in sediments before ultimately reaching the stream or being released by the surface and sub surface water in subsequent events the main pathways for the transport of water or solutes hereafter this will also imply particulates from the soil in a catchment to a stream are surface runoff sub surface flow or interflow and groundwater flow the solutes may take more than one of these pathways i e the sub surface flow may become surface flow further down the slope where exfiltration can take place once in the river system the solutes are subject to further modifications due to reactions volatilization dispersion deposition and resuspension and have been incorporated in models of varying complexity such as physically based distributed models semi distributed models and statistically based models qual2e brown and barnwell 1987 is a physically based distributed model and this and similar models have been reviewed by cox 2003 the development of process based distributed catchment hydrology models is reviewed by paniconi and putti 2015 and shows the progress and difficulties associated with such modelling the addition of solutes adds an extra layer of complexity in the process based models because the solute transport in the soil profile and slope needs to be modelled along with the erosion process for particulates at the soil surface further the exchange of solutes in the surface soil with any ponded water also needs to be modelled one of the limitations of the distributed hydrologic models is that these models are difficult to parameterise when only limited data is available to inform the modelled processes the resulting equifinality makes selection of a unique set of parameters ill posed beven and freer 2001 fully distributed solute catchment models pose even greater complexities in parameterisation that can add uncertainties for similar reasons shetran is an example of a fully distributed hydrological and solute transport model ewen et al 2000 semi distributed hydrological and solute transport models also exist such as source which is formerly known as watercast cook et al 2009 dougall and carroll 2013 swat arnold et al 1998 and catsalt tuteja et al 2003 the source and swat models use point scale processes which are then scaled up to the functional units in source and hydrologic response units in swat the qual model has been used for river system transport when combined with swat debele et al 2008 water and solutes are routed through the channel before transporting through the river system these models have not been able to accurately simulate day to day variations in the concentration of nutrients robson and dourdet 2015 statistical models have been widely used to model water quality cohn 1995 cohn et al 1992 colby 1956 miller 1951 and have been used in australia joo et al 2014 kroon et al 2012 kuhnert et al 2013 2012 robson and dourdet 2015 rustomji and wilkinson 2008 wang et al 2011 these models relate flow rate and other factors such as the amount of flow prior to the estimated time seasonal factors and the limb of the hydrograph to the concentration and or load this form of modelling avoids the difficulties inherent in scaling up processes from the point scale to the catchment scale grizzetti et al 2005 haan et al 1995 they generally result in a better fit around the mean of the flow concentration relationship venables and dichmont 2004 than streamflow solute models furthermore a statistical based modelling approach provides a natural way to estimate model uncertainty kuhnert et al 2013 2012 wang et al 2011 the major limitation to statistical model development is the requirement of long term historical data additionally performing the dynamic modelling and scenario analysis by changing the topographical physiographical geomorphological attributes and climatological data are other limitations of statistical modelling if long term historical data is not available then the model parametrisation is difficult which adversely affects the model performance for several catchments that discharge to the great barrier reef there is sufficient data to develop statistical models these models once developed can be computationally very efficient as they only require computation of a single equation for each constituent following a review of modelling approaches cook 2014 a statistical approach to modelling the water quality in targeted great barrier reef catchments was selected as this method could deliver results at an hourly time scale required for the marine models and would be computationally efficient all types of water quality models i e distributed semi distributed and statistical have some errors in the outputs which can be quantified using various metrics and the same has been done in this study in this study we developed statistical models for estimating the concentration of seven solutes constituents total suspended solids tss dissolved inorganic nitrogen din dissolved inorganic phosphorus dip dissolved organic nitrogen don dissolved organic phosphorus dop particulate nitrogen pn and particulate phosphorus pp the method used has unique features including the use of integrals and differentials of streamflow and derived series and a fixed value power transform to normalise and stabilise variance of constituents and covariates the aim of the study was to develop computationally efficient statistical water quality models that only require easily available data such as streamflow and its components an additional aim was to enable the efficient deployment of the method for other locations and constituents subject to suitable data availability the models developed have the capability to be used on a daily updating operational context for generating hourly status and forecasts of water quality concentration and load the water quality models developed are designed to provide inputs into the marine hydrodynamic and biogeochemical models implemented for the great barrier reef gbr lagoon baird et al 2016 herzfeld 2015 steven et al 2019 these marine models require sub daily inputs from the catchment models the data and process used to develop the statistical models along with the results and discussion are presented in the following sections 2 study area and data the great barrier reef gbr region extends more than 2300 km along the queensland coast of eastern australia there are 35 major catchments which drain to the gbr lagoon and the total area of these catchments is 423 070 km2 furnas 2003 steven et al 2014 of the 35 catchments 8 are considered in this study due to the availability of water quality monitoring observations long term hourly streamflow data and hourly streamflow forecasting models the details of these 8 catchments are discussed in the next section 2 1 study area the 35 major catchments of gbr are divided into 6 natural resource management nrm regions i e cape york wet tropics burdekin mackay whitsunday fitzroy and burnett mary the 8 catchments considered in this study cover 83 of the 35 gbr catchments area and at least one catchment is located in each nrm region table 1 fig 1 of these 8 catchments water quality models were developed at water quality monitoring locations given that the streamflow monitoring locations end of system eos most downstream streamflow gauging station are either located at the same location or upstream on the same river and free from tidal influence additional models were also developed for two sub catchments in the fitzroy catchment dawson and comet table 1 the contributing areas upstream of the water quality monitoring for these locations vary from 327 km2 to 139 230 km2 table 1 in a few catchments the water quality monitoring locations are downstream of streamflow gauging stations therefore the contributing areas at water quality monitoring locations are slightly larger than the streamflow gauging locations table 1 2 2 water quality constituent observations the water quality constituent observations are obtained from the department of environment and science des queensland government https www des qld gov au des has an ongoing program to take observations as part of the great barrier reef catchment loads monitoring program and publishes the results in the annual reports turner et al 2013 2012 wallace et al 2015 2014 this dataset contains instantaneous and irregularly sampled measurements of water quality with generally more observations during high flow periods than dry periods the duration of water quality sample observations used is from 2006 to 2014 but vary between locations table 1 as water quality observations are instantaneous values recorded at irregular time intervals the total number of samples is limited 2 3 streamflow observations the hourly streamflow observations used in this study are taken from the water data online product of the australian bureau of meteorology http www bom gov au waterdata the water data online product contains historical streamflow observations from water level stations across australia the dataset is kept current through a process of daily updates collated from the state and local government agencies responsible for monitoring water levels each agency also supplies rating tables for each gauging station which are used to convert recorded water level to streamflow the duration of streamflow data used in this study is from the start date of the observation record of each site to june 2016 table 1 2 4 data preparation and quality assurance checks some of the values in the water quality samples used are marked as lower than the detection limit of the instrument standard practice is followed and these values are replaced with half of the detection limit croghan and egeghy 2003 for example in water quality sample observations a value marked as 0 03 mg l indicates that the actual value is lower than 0 03 mg l but it cannot be measured due to the detection limit of the instrument therefore this value 0 03 mg l is replaced with 0 015 mg l the number of water quality samples lower than the detection limit vary with the constituent and sites here the minimum to maximum percentages of the data lower than the detection limit for all sites is mentioned in the bracket for each constituent tss 0 2 din 0 12 dip 0 8 don 0 5 dop 21 81 pn 1 15 and pp 0 35 for the dop the percentage of data lower than the detection limit is much higher than the other constituents it is mainly because the dop concentration is generally lower than the other constituents additionally some of the instances are flagged as not available or not recorded these instances are treated as missing data as the water quality modelling is performed at an hourly time scale a continuous time series of constituent concentration is required and this was prepared from the available quality assured instantaneous and irregular water quality sample dataset consequently water quality samples are only available for specific time steps of the continuous time series and the vast majority of the time steps correspond to missing data if there were more than one sample recorded during an hour then the values are arithmetically averaged to prepare a single value for that time step a parameterised quality assurance semi automated process was applied to the hourly streamflow observations to remove i outliers i e spikes or very high flows not supported by nearby values ii linearly interpolated values i e long linear interpolation iii constant values i e same values for long duration and iv negative numbers i e any values lower than zero 3 methodology the aim here is to develop water quality models for historical simulation and forecasting of concentrations and loads for the following water quality constituents tss din dip don dop pn and pp as these models will be used in the historical simulation current status update and forecasting computational efficiency is an important criterion to achieve these aims statistical models are used in this study as these models are computationally efficient and match the available data the key steps involved in the development of statistical models are presented in fig 2 and each one is discussed in detail in the following sections 3 1 covariates the main pathways for the transport of water quality constituents from the soil in a catchment to a stream are either streamflow or its components such as surface runoff sub surface flow or interflow groundwater or baseflow fig 3 therefore streamflow and its components are considered in this study as the covariates cook et al 2017 further the water quality constituents can take one or several of these pathways i e the subsurface flow may become surface runoff further down the slope where exfiltration can take place fig 3 to develop water quality models various streamflow based covariates are generated which are discussed in the following sections 3 1 1 streamflow the streamflow q is a convolution of the point scale processes that are or have occurred in the catchment at each point in the catchment the flow of water during a rainfall event either infiltrates the soil or runs off the surface the quantity of water starting from each point in the catchment can be considered a time series with the volume decreasing as evaporation and infiltration processes along its travel path take place thus the end of catchment streamflow is a convolution of all of these time series integrated across the catchment we initially considered using the rainfall data at stations across the catchments as this could add information about whether flow from different parts of the catchment would result in different responses we found that this added a level of complexity mainly because of the poor geographical distribution of rain gauges in many of the catchments whereas good quality streamflow data is available for these catchments therefore instead of climate variables the quality assured hourly streamflow data at the closest gauge to the water quality measurement location is used as a covariate to develop statistical water quality models cohn et al 1992 colby 1956 miller 1951 hourly streamflow time series are generated at water quality monitoring stations for the 10 locations considered in this study out of all of them the streamflow observations at 4 locations table 2 are either not available or available for short duration at water quality monitoring locations therefore streamflow is transferred from the upstream water level gauging stations to the downstream water quality monitoring locations for the development of water quality models at those water quality monitoring locations the flow transfer method is based on the proportional increase in the catchment area water volume and flow travel time from the upstream water level gauging station to the downstream water quality monitoring locations the methodology is described in gbr catchment loads monitoring annual reports turner et al 2013 2012 wallace et al 2015 2014 and the flow transfer equations developed at 4 locations following the above method are presented in table 2 3 1 2 baseflow the water that infiltrates into the ground surface can contribute via deep drainage to the groundwater flow and can also contribute to the streamflow as either exfiltration flow or baseflow q bf the baseflow component of the streamflow is determined by a baseflow filtering approach developed by nathan and mcmahon 1990 the baseflow is calculated on a daily time scale and uniformly disaggregated to an hourly time scale the baseflow is used as a covariate because it could be the pathway of dissolved solutes such as nitrate and nitrite the hourly baseflow time series are separated from observed streamflow at water quality monitoring stations for the 10 locations considered in this study 3 1 3 cumulative difference between streamflow and mean another streamflow component that is considered in the study is the cumulative difference between streamflow and the mean q m which is a measure of the catchment hydrological conditions when this value is negative it means that the catchment is drier than the average and the magnitude gives an estimate of how dry when the values are positive the catchment is wetter than average and the magnitude gives an estimate of how wet cook et al 2017 presented data for the fitzroy catchment which showed very clearly the millennium drought from 2000 to 2009 the q m is calculated at a particular time by accumulating the difference between streamflow and long term mean eq 1 1 q m t i 0 t q i q where q i is the streamflow at ith time step q is the long term streamflow mean calculated from available streamflow observations for the entire period and t is the time since the start of the long term streamflow record q m varies with time from positive to negative depending on the climatic conditions that have occurred since the start of the record and will return to zero at the end of the record the positive and negative values of q m indicate the wet and dry conditions of catchment respectively particularly during the period for which q m is calculated the start date of the streamflow record varies from site to site table 1 the hourly cumulative difference between streamflow time series and the mean are generated at water quality monitoring stations for all locations considered in this study 3 1 4 differential of variables the differential of variables streamflow baseflow and the cumulative difference between streamflow and mean is the change in the variable rate with time which is approximated using the difference approximation eq 2 2 d x d t t i c x i 1 x i 1 t i 1 t i 1 t i 1 t i t i t i 1 1 where x is the variable q q bf and q m in m3 s and t is the time in hour the index i is the point in the time series record with i 1 and i 1 the next and previous record values and c is a constant to convert the output to m3 s2 which is 1 3600 in this case the differential of streamflow baseflow and the cumulative difference between streamflow and mean are used as covariates in developing the multivariate regression models it has often been considered that the changes in the concentration of water quality constituent depend on whether the flow is increasing or decreasing marsh and waters 2009 the differential of the streamflow provides a convenient method for determining whether the streamflow is increasing with the time dx dt 0 steady with time dx dt 0 or decreasing with time dx dt 0 the magnitude of the values 0 the rising limb of hydrograph indicates how quickly streamflow is increasing and possibly the increase in the area of contribution rainfall and or spectrum of velocities of surface runoff in the catchment this relates to constituent concentration change due to the erosion on hillslopes and gullies and the removal of sediments from the storages within the catchment and stream the magnitude of values 0 the falling limb of hydrograph indicates the rate of decrease in the contributing area rainfall and or decrease in the spectrum of velocities of surface runoff in the catchment this could relate to the constituent concentration changes due to the deposition processes in the catchment on floodplains and streams for both dissolved and particulate constituents for some of the catchments the differential of the baseflow shows good relationship with the dissolved constituents such as don and din the hourly time series of differentials are generated for streamflow baseflow and the cumulative difference between streamflow and mean for 10 locations considered in this study 3 1 5 integral of variables the total amount of streamflow passing through the river channel in the previous period can significantly influence the constituent concentration this is because the duration of small or low flows can result in the storage of water quality constituents on flow planes the build up of concentration in the soil or riparian zones the generation of erodible materials in gullies and storage or depletion in water storages such as dams and weirs periods of extended high flows can result in opposite changes in the constituent concentration due to the processes listed above in previous studies a discounted rate whereby the past flow has less effect with time has been used kuhnert et al 2013 2012 robson and dourdet 2015 wang et al 2011 here we will not discount the rate as this assumes the past events have less effect with time instead we will integrate the streamflow into the past for fixed periods of time and consider the effect of this integral on the constituent eq 3 3 i x t t a x d t where x is the variable q q bf and q m t is time and  is the time period into the past of the integral various values of  such as 1 2 5 10 20 50 and 75 days are considered in this study due to different sizes and response times of catchments note that here streamflows are integrated at each time step for the number of days mentioned above and not restricted only during an event the hourly time series of integrals of various durations mentioned above are generated for streamflow baseflow and the cumulative difference between streamflow and mean for all 10 locations considered in this study if there is missing data during the integration period then integrals are only calculated if 90 or greater than 90 of the data is available during that period the same 90 threshold is used by the streamflow data providers to generate hourly streamflow data 3 1 6 time based covariates the water quality constituent data may have short range periodic time dependent variation due to annual changes in temperature crop growth patterns and land management practices such as fertilization and harvesting cohn et al 1992 have modelled these short ranges temporal variations by adding both sine and cosine terms of their modelling approach these sine and cosine terms are also considered in this study eq 4 4 s t sin 2  t s p c t cos 2  t s p where st and ct are sine and cosine time based covariate values t s is the time in days since the water quality constituent observation is available and p is the period of 365 days for non leap year and 366 days for leap year the use of both sin and cosine which are orthogonal means that this approach can detect if a time based periodic signal is in phase or out of phase the hourly time series of time based covariates are generated for all seven constituents for the 10 locations of the study 3 2 covariate selection all constituents and covariates except time based for the 10 locations are transformed using box cox transformation with fixed 0 2 power function to stabilise variance and normalise the range this fixed value of power function is used instead of a log transformation as the long exponential tail of the log transform create problems in the calculation of the uncertainty mcinerney et al 2017 to determine the general predictability of each covariate linear and polynomial order 2 regressions were performed between each water quality constituent and covariate pair for each site this is repeated for all combinations of transformed constituents and covariates the covariates which show an r2 coefficient of determination 0 1 are selected to develop water quality statistical models the r2 value of 0 1 may be considered low in terms of predictability but when these covariates are used together with other covariates in the statistical model the combined performance may be improved at this stage of covariate selection the poor predictability covariates are filtered out if none of the covariate show r2 0 1 for any particular constituents at a particular site then no water quality model is developed for that constituent and site the selected covariates in both normal and transformed form are used to develop the statistical model the squared form of selected normal and transformed covariates is also used to develop statistical models as squared form shows improvement in predictive power in some of the models when used in polynomial order 2 regression the development of statistical models multivariate regression is presented in the next section 3 3 multivariate regression model the intent of this study is to develop simple statistical models which are computationally efficient and can be used in realtime forecasting therefore a standard form of multivariate regression is used to develop the relationship between water quality constituents and covariates for all locations eq 5 rather than an alternative approach such as a process based model 5 y f x 1 x 2 x n c a 1 x 1 a 2 x 2 a n x n  where y is the seven water quality constituents tss din dip don dop pn and pp in normal or transformed form x 1 to x n are covariates in normal transformed and squared form the n is the maximum number of covariates used in the multivariate regression which is restricted to 5 in this study c is the intercept and a 1 to a n are the coefficients the residual standard error  is assumed to be normally distributed in either normal or transformed space and it is used to estimate the model uncertainty the method of calculation of residual standard error and model uncertainty is explained in section 3 5 a water quality constituent for a particular site in normal and transformed form are used as the dependent variable in the multivariate regression equation for developing statistical water quality models the selected covariates in normal transformed and squared form are used as independent variables in all possible combinations in the multivariate regression equation fig 4 based on the authors experience the number of covariates in the multivariate regression equation is restricted to five to avoid over parametrisation and overfitting of the model whereas all possible combinations of covariates between one to five are tried fig 4 if the number of covariates selected in the previous step is significantly high then in some cases millions of combinations of multivariate regression models are developed which are evaluated to find the best model the regression models are evaluated according to the r2 and adjusted r2 generation of a large number of multivariate regression models is a computationally intensive exercise and the australian national computational infrastructure nci was an essential resource to complete this study 3 4 model stability the complete set of multivariate regression models generated in the previous step is parameterised in two different manners i using the whole record of the water quality sample data set and using ii two thirds sample of data set the latter approach is used to check the stability of the models here model stability means that the performance of the model using two thirds data set does not significantly change in comparison to the model using the whole data set when the same covariates are used in both types of models the models which use the whole record of the water quality sample data set are called mod all from here onwards it is essential to check the model stability when applied with forcing data outside the calibration step as they will be used to forecast water quality constituent concentration and load therefore the models are validated by separating the water quality samples into two sets one third and two thirds of the samples selected at random without repeats splitting the sample into two thirds for calibration and one third for validation is quite common in hydrological modelling particularly when the number of data samples is limited the same concept is used here the multivariate regression models are generated again in all possible combinations with remaining two thirds data sets fig 4 these models are called mod 2 3 from here onwards to check the model performance r2 and adjusted r2 are calculated for the validation datasets using the same process as used for the entire dataset fig 4 other model performance measures are also used in this study and are discussed in the following sections 3 5 model uncertainty the residual standard errors  are generated for all combinations of multivariate regression models for all constituents and locations this residual standard error is used to estimate the model uncertainty through the deviation from the mean prediction to calculate model uncertainty the confidence limits used in this study are 5 and 95 which is calculated by the mean prediction 2 the predicted constituents at the 5 confidence limit which are negative are replaced with zero this results in the uncertainty limit being truncated to the physical limit of zero 3 6 model selection the multivariate regression models for each water quality constituent and location are generated for all possible combinations of covariates using the entire record of the water quality data set mod all and the two third data set mod 2 3 there are a few constituents and location combinations where no multivariate regression model is generated due to no covariates been found to have significant predictive skill after the generation of multivariate regression models the next critical task is to select the best model out of all generated models for each site and constituent which is described in the following sections and in fig 5 3 6 1 shortlisting models on the basis of adjusted r2 to select the best water quality model first the multivariate regression models for each constituent and location are arranged in decreasing order of adjusted r2 and then the best 100 models where the constituents are in normal space and best 100 models where the constituents are in transformed space are selected for further analysis and short listing fig 5 these 200 models are selected from mod all and on the basis of covariates of these 200 models another 200 models are selected from mod 2 3 which have the same covariates but coefficients are different fig 5 the r2 of these 200 models are also overviewed to confirm that the differences between r2 and adjusted r2 are not significant the mod 2 3 are used to check the stability of models here the model stability means that the performance of mod 2 3 does not significantly change in comparison to the mod all when the same covariates are used in both types of models if the total number of generated models is less than 100 in normal space and 100 in transformed space then whatever number of models are generated are used for further analysis and short listing in the majority of constituents and locations the number of generated models is much higher than 200 the adjusted r2 is preferred over r2 for selecting the best 200 models as it also accounts for the total number of variables in the regression the adjusted r2 increases only if the new variable improves the model more than would be expected by chance it decreases when a variable improves the model by less than expected by chance therefore it is helpful to avoid overfitting in the regression model the adjusted r2 is always less than or equal to r2 3 6 2 further shortlisting of models on the basis of time series plots and nse values the selected 100 models where the constituents in normal space and 100 models where the constituents in the transformed space are used to simulate continuous hourly water quality constituents time series and compared with the water quality constituent observations the nash sutcliffe efficiency nse is also calculated for these 200 models nash and sutcliffe 1970 when the constituents are in transformed space in the model then the simulated time series is back transformed before comparing with the model observations and calculating the nse values the comparison of simulated time series with the observed samples is performed for both sets of 200 models i e mod all and mod 2 3 in a similar way the nse values are also compared for both sets of 200 models i e mod all and mod 2 3 the stability of models can be checked at the same time while selecting the best model based on nse values model stability and visual comparison of simulated and observed constituent values the best 10 models are selected out of the 200 models to select the best 10 models we have started with the highest nse value model then check the stability of that model and if the model is stable then visually compared the simulated and observed constituent values and also check the consistency in the pattern of streamflow with the pattern of simulated constituent time series if the model looks satisfactory in visual comparison then the model was shortlisted but if the model was not satisfactory either in the stability check or in the visual inspection then that model was not considered and the next model with the second highest nse value was investigated we continue this process until the best 10 models are shortlisted the main reason of performing the visual inspection of time series is that the observed water quality sample data is discrete therefore nse or other statistical measures i e adjusted r2 or r2 evaluate the model performance only at the time of observations but not throughout the entire time series visual inspection of the time series of the simulated constituent concentration enables checking for consistency with the observed water quality samples and the consistency in pattern with the observed streamflow for example that it does not show very high concentration during low flows or very low concentration with high flows the 10 models shortlisted can be a combination of transformed and normal space constituent models or could also belong to a single category they are all selected from mod all and on the basis of covariates of these 10 models another 10 models are selected from mod 2 3 the models of each of these categories are used for further analysis and short listing fig 5 the models considered in the selection are those with the smallest differences in adjusted r2 and nse between mod 2 3 and mod all we found that in the majority of selected models the adjusted r2 and nse for both categories of models are very close 3 6 3 further shortlisting of models on the basis of additional metrics and plots for the selected 10 models in each of the above mentioned categories further additional indices such as root mean square error rmse and mean absolute error mae are generated using the simulated and observed constituents data fig 5 the nse values were already computed in the previous step additional plots are generated to analyse the results in more detail and decide on the best model fig 5 these are i scatter plot between observed and simulated constituent concentration ii similar scatter plot as i but for a load iii scatter plot of simulated concentration between the two categories of the model i e mod all and mod 2 3 iv similar scatter plot as iii but only for the remaining one third data and v the time series plots of observed and simulated constituent concentration with 5 and 95 percentile confidence band for both types of model categories i e mod all and mod 2 3 on the basis of the visual inspection of the above mentioned plots and an analysis of the statistical indices computed in this step the 3 best performing models are selected which are further analysed and short listed to get the best water quality model fig 5 3 6 4 selection of single best model for each constituent and locations the performance of the 3 best models at the hourly time scale selected in the previous section is very similar for most of the constituents and locations therefore selecting one best model is not trivial we have added one further criterion i e the annual load comparison to the process to assist with choosing the final model the annual load integrates the mass of the constituent over a large time step which ensures that an hourly model when integrated up to an annual mass produces acceptable results the procedure is described below to find the best model the simulated constituent concentrations are converted into a load by multiplying with the hourly streamflow with the estimated constituent for the same duration which is then aggregated up to compute the annual load these annual loads are compared with the loads given in the gbr catchment load monitoring reports turner et al 2013 2012 wallace et al 2015 2014 the simulated annual loads are also calculated for 5 and 95 percentile confidence band the load year considered in the load monitoring report is from 1st july to 30th june after investigating model performance at annual scale and re examining all the statistical indices and various time series and scatter plots described previously one best model is selected fig 5 in the model selection approach described in this section not all performance plots and statistical indices are generated for all the multivariate regression models as visual inspection of all of them is not feasible therefore a systematic narrow down approach is used in this study to find out the one best model i e from all multivariate regression models to 200 models then 10 models then 3 models and then finally one best model based on various criteria described throughout this section by following this systematic approach we only need to generate a limited number of plots that are practically feasible to analyse the analysis described here is repeated for all 7 constituents and 10 locations covered in this study a total of 67 water quality models is successfully generated the remaining 3 models are not generated due to very poor relationship between constituents and covariate and also due to the high percentage of water quality samples lower than the detection limit and these are mainly for dop the results of these models are presented and discussed in section 4 3 7 forecasting the adopted water quality models are also used with streamflow forecast to generate the water quality constituent concentration forecast a brief description of the streamflow forecasting model and the water quality constituent forecasting process used are presented in the following sections 3 7 1 streamflow forecast in this study we have generated 3 days lead time hourly deterministic streamflow forecasts daily this streamflow forecast is used in water quality models to generate water quality constituents forecast which is discussed in detail in the next section the streamflow forecasts are generated using the short term water information and forecasting tools swift hapuarachchi et al 2017 kabir et al 2018 pagano et al 2011 perraud et al 2015 swift is a streamflow modelling package designed for both operational streamflow forecasting and scientific research it is a collection of hydrologic modelling components and utilities that support model calibration validation and forecast verification hapuarachchi et al 2017 kabir et al 2018 pagano et al 2011 perraud et al 2015 the gr4h rainfall runoff conceptual model is used in swift as the performance of this model is satisfactory across various catchments in australia hapuarachchi et al 2017 perrin et al 2003 note that the swift modelling system with gr4h model is used to generate 7 day streamflow forecasts service http www bom gov au water 7daystreamflow by the australian bureau of meteorology the australian community climate and earth system simulator access r http www bom gov au australia charts about about access shtml forecast rainfall data for the period 2013 to june 2016 is used to generate hourly streamflow forecast from swift for 3 days lead time this streamflow forecast data is used with water quality models to generate water quality constituent forecasts 3 7 2 water quality constituent forecast the daily generated hourly streamflow forecast data for 3 days lead time is combined with the historical observed streamflow data to generate the streamflow and its component based covariates these covariates are used in adopted water quality models to generate the water quality constituent concentration forecast these constituent concentrations are converted into a load by multiplying the streamflow values of that duration the results of the water quality forecast for selected constituents and locations are discussed in section 4 4 results and discussion the covariates used to develop each water quality model and the performance of these models vary for the constituents and locations and are discussed in detail in the following sections 4 1 covariates of the adopted water quality models the covariates of the adopted water quality models after the selection process for all seven constituents in ten locations are presented in table 3 for some of the models the dependent variables constituents are in transformed space and highlighted by a grey colour table 3 for dop there are no models at three locations south johnstone river at upstream central mill herbert river at ingham and sandy creek at homebush due to very poor relationships between constituents and covariates table 3 and to the high percentage of water quality samples lower than the detection limit for these three locations none of the covariates have r2 0 1 and therefore no covariates were selected the streamflow its differential and integral either in normal or transformed form appear as the covariates for 56 out of 67 water quality models table 3 streamflow appears to be a strong driver for generating both particulates and dissolved constituents the streamflow differentials and integrals are also strongly correlated with several constituents as they provide information about the rate of change of streamflow and cumulative streamflow in the past which we can assume is predictive of water quality concentration the cumulative difference between streamflow and mean its differential and integral either in normal or transformed form appear as the covariates in 49 out of 67 water quality models table 3 these covariates appeared as highly correlated covariates with several constituents as these covariates are indicators of catchment hydrological conditions to the best of the authors knowledge this is the first time the authors are aware that the cumulative difference between streamflow and its mean its differential and integrals have been used as covariates in the development of water quality models the baseflow based covariates either in normal or transformed form appear as the covariates in 41 models out of 67 water quality models table 3 the baseflow based covariates appear in both particulate as well as dissolved constituents but for din and dip they appear as covariates in 16 out of 20 models which shows that the baseflow is the strongest driver for dissolved constituents the time based covariates only appear in 24 out of 67 water quality models where they improved the multivariate regression performance further but do not appear as strong covariates in the majority of the models we presume this is because there is no strong seasonality pattern for the majority of constituents table 3 note that all possible combinations of covariates between one to five are tried here to develop the multivariate regression model therefore if an individual covariate improves the model performance then it is considered in the finally selected model otherwise the model without that covariate would have been considered finally for don and dop models for the burdekin river at home hill only time based covariates appear in the adopted model but the performance of these models is poor the performance of all models is discussed in detail in the next section 4 2 model performance checks using statistical indices the adjusted r2 for adopted water quality models for all seven constituents and for ten locations are presented in table 4 it includes adjusted r2 for both categories of models i e mod all and mod 2 3 note that the water quality modelling here is performed at an hourly time scale therefore adjusted r2 are also calculated at the same time scale the adjusted r2 for water quality models for tss are better than other constituents and all the values are 0 5 table 4 the highest adjusted r2 for tss is 0 83 for burnett river at ben anderson barrage head water and drops to 0 82 only for mod 2 3 for other six constituents the adjusted r2 vary significantly with the constituents and locations for pn and pp some models have high adjusted r2 up to 0 7 whereas some have low value only 0 26 table 4 the adjusted r2 for din and dip range between 0 23 and 0 88 and this upper range is also the highest adjusted r2 among all the models the adjusted r2 for don and dop models are generally low the majority of models have values lower than 0 5 table 4 models of dop are not developed for three locations due to very poor relationship between constituents and covariates the model performance in the historical simulation period and one third validation period using other statistical indices are discussed in the following sub sections 4 2 1 model performance in historical simulation period the adopted water quality models are used to generate a continuous hourly time series of constituent concentrations for the historical period during which constituent observations are available the simulated time series in which the dependent variable i e the constituent concentration is in the transformed form is back transformed using power 5 to generate the hourly time series of constituent concentration in the normal form note that the box cox transformation with the fixed 0 2 power function is used in this study as mentioned in section 3 2 as the observed water quality samples are discrete the simulations corresponding to observation time periods are extracted from the simulated continuous hourly time series and various statistical indices i e nse rmse and mae are calculated tables 5 7 these additional indices are helpful to check the model performance more thoroughly the nse values of all water quality models in historical simulation period vary by constituent and location table 5 the highest value of nse is 0 9 corresponding to dip for dawson river at taroom and lowest value is 0 01 for dop at burdekin river at home hill table 5 the performance of the models for dop are generally poorest among other constituents and the major reason is because of a large number of constant values in the observed record it is due to the measured values being recorded as below the detection limit of the measuring instrument which ranges 21 81 of the values for dop the nse values can be compared among different constituents and locations as it is dimensionless number and ranges to 1 but the rmse and mae cannot be compared as these are in mg l and scale of observation of various constituents and locations are significantly different tables 5 7 generally the tss concentration is higher than other constituents i e din don dip dop pn and pp therefore the rmse and mae are also higher for tss tables 6 7 further the rmse and mae values are low for most of the models for dop because the dop concentration is quite low the rmse and mae values are used to check the difference between the historical simulation and validation period which is discussed in the next section 4 2 2 model performance during validation period the difference in nse rmse and mae between historical simulation period and the one third validation period is negligible in most of the models see upper and lower sections of tables 5 7 the upper section indicates the model performance in the simulation period whereas the lower section indicates the model performance in one third validation period tables 5 7 this indicates that most of the models can be considered stable over the validation period tables 5 7 this is because the model stability is one of the selection criteria in adopting the model from the large number of candidate models generated for each constituent and location the highest difference in nse values between the historical simulation period and the validation period is for pp for normanby river at kalpowar crossing in this case the nse value during the simulation period is 0 29 but for the validation period it is 0 07 table 5 it indicates this model is not stable enough though it is the best model out of all other models generated for this site and constituent further the nse is sensitive to high values therefore showing large differences whereas the rmse and mae values for same constituent and location do not show the same order of difference tables 6 and 7 therefore for checking the stability of models during validation period the rmse and mae are also used the performance of the model is also checked through visual inspection of a variety of plots which are discussed in the next section 4 3 model performance checks using various plots the performance of all 67 water quality models is assessed using various plots such as time series scatter and annual load comparison plots these plots for all 67 models cannot be presented here therefore plots for one good performing model i e a tss model for burnett river at ben anderson barrage head water and one poorly performed models i e a dop model for the burdekin river at home hill are shown in the next section the plots for other models are available from the authors on request 4 3 1 comparison of observed and simulated time series the hourly simulated time series of tss concentration for the burnett river at ben anderson barrage head water and dop concentration for the burdekin river at home hill with 5 and 95 confidence band are compared with the average observed values for the same time period in fig 6 a and b the observed streamflows are also presented to compare the trend with constituent concentrations fig 6a and b the simulated tss concentrations for the burnett river at ben anderson barrage head water match the observations throughout the simulation period well as this model was considered a well performing model with an nse value of 0 84 fig 6a table 5 further most observations are falling within 5 and 95 confidence band fig 6a the temporal pattern of the observed tss concentrations is similar to the temporal pattern of the streamflow which suggests that streamflow and its components are strong drivers for tss the simulated dop concentrations for the burdekin river at home hill poorly match the observations and this is considered to be a poorly performing model with nse value only 0 01 fig 6b table 5 further several observations are outside the 5 and 95 confidence band fig 6b the simulated time series shows the cyclical pattern because the covariates in this model are only time based covariates table 3 the main reason for the poor performance of this model is the inability to select covariates which can reproduce the large number of constant values these constant values are due to measurements being sampled below the detection limit of the measuring instrument which is 62 of the dop measurements for burdekin river at home hill fig 6b further the temporal pattern of dop observations does not show a similar temporal pattern as that of the streamflow and therefore streamflow or its components are not good predictors of the dop concentration samples at this location fig 6b table 3 the performance of most of the other models for other constituents and locations lies between these two extreme cases these plots for all models cannot be presented here but are available from the authors on request the performance of the models is also assessed using scatter plots between observation and simulation which are presented in the next section 4 3 2 scatter between observations and simulations the scatter plots between hourly observations and simulations are presented for tss concentration and load for the burnett river at ben anderson barrage head water and dop concentration and load for the burdekin river at home hill fig 7 a d for tss hourly concentration for the burnett river at ben anderson barrage head water the majority of the scatter points are close to 1 1 line irrespective of low or high concentration values which further confirms this is a good performing model fig 7a the simulated hourly concentrations of tss at burnett river at ben anderson barrage head water are converted to hourly load by multiplying the observed streamflow for the same time period the tss load shows a similar performance in fact the spread is less in load scatter plot than the concentration scatter plot because of the multiplication of observed streamflows in both simulated and observed tss concentrations fig 7b if the variation in hourly observed and simulated tss concentration is low then after multiplication of observed streamflow the variation in hourly observed and simulated tss load becomes insignificant and more scatter points are on 1 1 line fig 7b most points in the scatter plot for hourly dop concentration for the burdekin river at home hill are significantly deviating from 1 1 line which clearly shows this model is unable to capture high or low concentration values fig 7c the model heavily underestimates high values of the dop concentration fig 7c this is expected as the model only uses the time based covariates as discussed in the last section the simulated hourly concentrations of dop for the burdekin river at home hill are converted to hourly load by multiplying the observed streamflow for the same time period the performance improved slightly for load calculation due to multiplication by streamflows fig 7d the estimation of low values of load is better than the higher values as low values have deviated on both sides of 1 1 line whereas high loads only have deviated below 1 1 line showing consistent underestimation of high loads fig 7c and d 4 3 3 scatter plots for model stability checks the water quality models are calibrated using all water quality sample observations mod all and using two third water quality sample observations mod 2 3 to check the stability of the model during the entire observation sample period as well as one third observation sample period or validation period the scatter plots are generated between simulated constituent concentrations from both types of models i e mod all and mod 2 3 fig 8 a d these plots are generated to compare simulations during the entire observation sample period fig 8a and c and during one third observation sample period fig 8b and d these plots are presented for simulated tss concentration for the burnett river at ben anderson barrage head water fig 8a and b and for simulated dop concentration for the burdekin river at home hill fig 8c and d note that the covariates of both sets of models i e mod all and mod 2 3 are the same section 3 4 3 6 but the coefficients are different the simulated tss concentration for burnett river at ben anderson barrage head water from both types of models i e mod all and mod 2 3 are perfectly aligned on 1 1 line for all of the observed sample period fig 8a and one third observed sample period validation period fig 8b which shows that the performance of model has not changed when only two third data is used for calibration and the model is stable enough it is also happening because during the model selection process models which show similar performance in calibration and validation periods are preferred the simulated dop concentration for burdekin river at home hill from both types of models i e mod all and mod 2 3 show significant deviation from 1 1 line for all observation sample periods fig 8c and one third observation sample period or validation period fig 8d the mod 2 3 is consistently underestimating the dop concentration during the entire simulation as well as the validation period fig 8c and d it is happening because of poor correlation between the constituent and covariates resulting in low nse values as discussed in section 4 3 1 in the model selection process of dop at burdekin river at home hill none of the models showed stability between calibration and validation periods 4 3 4 comparison of annual loads the simulated hourly concentrations of tss for the burnett river at ben anderson barrage head water and dop for the burdekin river at home hill are converted to hourly load by multiplying the observed streamflow for the same time period these hourly loads are aggregated to load year i e 1st july to 30th june and compared with the annual loads given in gbr catchment load monitoring reports fig 9 a and b turner et al 2013 2012 wallace et al 2015 2014 the annual loads corresponding to 5 and 95 confidence interval are also calculated and compared with the loads given in the load report fig 9a and b in gbr catchment load monitoring reports the annual loads of constituents are calculated on the basis of average load derived from the linear interpolation of concentration within the year turner et al 2013 2012 wallace et al 2015 2014 this method is used when events are adequately sampled or at least with a reasonably representative sampling including the peak concentration joo et al 2012 for poorly sampled and or complex events the beale ratio method is used to calculate the annual load in gbr catchment load monitoring reports joo et al 2012 the simulated annual loads for tss for the burnett river at ben anderson barrage head water are closely matched with the loads given in the load report for all five load years i e 1st july 2009 to 30th june 2014 fig 9a for three years july 2009 to june 2010 july 2011 to june 2012 and july 2013 and june 2014 the simulated loads the loads from the load reports and the loads corresponding to 5 and 95 confidence band overlap as these years are comparatively drier for the burnett catchment relative to the other two years i e july 2010 to june 2011 and july 2012 to june 2013 fig 9a the annual simulated loads closely match the load report which indicates the model has good performance at an annual accumulated time scale turner et al 2013 2012 wallace et al 2015 2014 the simulated annual loads for dop for the burdekin river at home hill do not match closely with the loads report because this model is a poorly performing model fig 9a and b the differences between simulated annual loads and loads given in load report are high particularly for the period july 2010 to june 2011 as this is reasonably wet year for burdekin catchment fig 9b despite the higher difference the loads given in load reports are within 5 and 95 confidence band for all five years though the dop model of the burdekin river at home hill is a poorly performing model with low nse values the results at the annual scale are not as poor as at the hourly time scale it is because the loads are aggregated at annual scale and both simulated loads calculated here and given in the load reports used the same observed streamflow for multiplication with a concentration that improved the performance at the annual scale turner et al 2013 2012 wallace et al 2015 2014 4 4 performance in forecast the 3 days lead time hourly streamflow forecast are used in the selected water quality models to generate the retrospective forecast of water quality concentration and load these forecasts are generated daily for three days lead time the performance of water quality forecast varies between constituents sites and individual events further the performance of water quality forecasts is highly dependent on the skills of water quality models and streamflow forecast due to limited overlap between rainfall forecast period and water quality constituents sample observations i e jan 2013 to jun 2014 a proper statistic could not be generated but certain individual events are analysed visually and assessed at annual scale individual event analysis is further constrained due to a limited number of discrete samples during the forecast period therefore annual assessment with the loads given in the load report and comparison with the simulated load are presented in fig 10 a and b for tss for the burnett river at ben anderson barrage head water and dop for the burdekin river at home hill further the estimation and evaluation of constituents load at the annual scale are useful for reef report card perspective turner et al 2013 2012 wallace et al 2015 2014 the annual forecast loads at 1 2 and 3 days lead time are calculated by aggregating the hourly load time series for each 1 2 and 3 days lead time to the annual scale note that the simulated annual loads are calculated on the basis of observed streamflow data the annual loads are shown in logarithmic scale due to significant variation in dry and wet years and minor differences in forecast loads at different lead times fig 10a and b the annual forecast load for all 3 days lead times are close to the simulated loads and the loads given in the load report fig 10 a b turner et al 2013 2012 wallace et al 2015 2014 the variation in annual load among 3 days lead time is not significant but as the lead time increases the deviation from the load report increases for the year 2013 14 for tss for the burnett river at ben anderson barrage head water and for dop for the burdekin river at home hill fig 10 a b as discussed in the previous section the dop model for the burdekin river at home hill is a poorly performing model with low nse values but even then the results at the annual scale are not as poor as at hourly time scale because of multiplication with the observed streamflows for calculation of loads as discussed in the previous section 4 5 model performance summary the performance of most of the other models developed lies between the two models presented above i e tss model for burnett river at ben anderson barrage head water and dop model for burdekin river at home hill out of 67 models developed in this study 27 have nse values 0 5 and 57 have nse values 0 3 achieving high values of nse in water quality modelling is challenging because some of the constituents do not show good relationships with the covariates derived from the streamflow or its components such as dop in this study despite that a high number of good performing and reliable models are developed in this study all the models are computationally efficient and possible to use in daily water quality status and forecasting and will provide reliable continuous hourly simulation and 3 days lead time hourly forecast our aim here is to provide the water quality hindcast forecast for all 67 models with the model performance information so that the potential users can use the models for their decision making by considering the model performance further providing the water quality hindcast forecast with the model performance is better than providing no information because even the poorly performing models at hourly time scale are providing reasonably accurate loads at the annual scale which are useful for various potential users the nse r2 adjusted r2 rmse and mae use the observed and simulated water quality concentration at the same timestamp the water quality observed samples used in this study are discrete whereas the simulated water quality concentration generated from the models are continuous hourly time series as all the covariates have continuous hourly time series it is quite likely the water quality model fits very well at the time of observations but does not fit well outside of that range and the performance based only on the metric could be misleading therefore we have not categorised the models i e good moderate and poor based on metrics to properly categorise the model we need to investigate a new metric that will consider the multiple criteria to assess the model performance throughout the time range the development of a new metric and then categorisation of the model will be the scope of future research further the variety of covariates in various combinations used in this study have not been explored in other published research to the best of the authors knowledge as most of the covariates are derived from the streamflow or its components this methodology can be applied at any other location where the constituents and streamflow observations are available further these models do not require large spatial data sets as required in distributed process based modelling the modelling methodology and an automated system are successfully developed in this study which can be used to develop water quality models at other locations 5 conclusion a statistical water quality modelling methodology to predict constituents concentration and load has been developed and described various covariates are used in unique combinations including streamflow baseflow the cumulative difference between streamflow and mean their differentials integrals and transformations time based covariates are also used the physical basis for using these co variates is demonstrated in a consistent manner the water quality modelling methodology has been investigated for 7 constituents at 10 locations and 67 water quality models are developed for 3 locations for dop the models are not developed due to poor relationship between constituents and covariates and also due to the high percentage of water quality samples found to be lower than the detection limit the performance of the models varies between sites and constituents out of 67 models 27 models have nse values 0 5 and 57 models have nse values 0 3 validation is carried out by splitting the data sets into two thirds for the development of the model and one third for testing the model this worked well for large data sets as model performances are very similar when full data and two thirds data are used however for small datasets model performances for full and two thirds data can vary significantly further validation was performed by using models to predict the loads in a historical simulation mode and these loads were then compared to previously published loads these demonstrated that the loads calculated with the models developed from this study were generally similar to those published previously the water quality modelling methodology and associated automated system successfully developed in this study could be used to develop water quality models at other locations in the future the water quality modelling method presented here is computationally intensive therefore supercomputing platform is an essential requirement to develop these types of models at other locations in the future the water quality models developed here will be used in the future in the realtime system to generate the simulations for the preceding 14 days current status and hourly forecasts for the next three days of concentration and load these models are designed to provide inputs into the marine hydrodynamic and biogeochemical models implemented for the gbr lagoon and can also be provided to other potential users who could benefit from this data set declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this project was led by the water forecasting services team at the australian bureau of meteorology bureau the broader ereefs project which is a collaboration between the great barrier reef foundation bureau csiro australian institute of marine sciences aims australian government queensland government bhp billiton science and industry endowment fund and bhp billiton mitsubishi alliance provided financial support for this work cedric robillot and greg stuart guided this project through various stages ryan turner reinier mann belinda thomson and the broader team from queensland government s loads monitoring program and miles furnas and richard brinksman from the aims supported with water quality data and expert advice tony jakeman miles furnas and ashish sharma supported independent scientific peer review of the modelling methodology and project outcomes dasarath jayasuriya graham hawke and rob vertessy provided high level guidance and assisted with resourcing the national computational infrastructure nci supported by the australian government was used to perform large computations in this research 
25983,a hot spot region of climate change is the middle east where temperatures actually have a rising tendency and this will increase in future to mitigate the progressing thermal burden urban planning has to develop adaptation measures on the basis of micrometeorological simulations for a quarter in cairo we suggest a decomposition of air temperatures into two temporal and two spatial patterns respectively explaining 97 and 94 of the temperature variability we find that land use has a significant impact on the spatial temperature distribution and should be modified for the purpose of heat adaptation however just 13 of the spatial temperature variability are explained by land use which is a quite limited impact regional weather conditions are the dominant factor for the spatial as well as the temporal development of urban heat keywords urban air temperature empirical orthogonal functions emvi met climate change adaptation 1 introduction average global temperatures tend to increase but regional climatic conditions can develop in very diverse manner due to differences in land use feedback processes atmospheric stability and global circulations if we in particular consider the exposure to extreme heat the changes of both regional climate and population have to be taken into account for example liu et al 2017 found that for the higher emission scenario rcp8 5 the heat exposure for europe will increase by a factor of four by 2100 while the heat exposure for africa will be over 118 times greater than it has been historically exposure refers to people in the considered region that are subject to potential risks due to heat the middle east and north africa region has an arid hot desert climate where precipitation is low rainfall is only during december february and primarily over coastal areas and vegetation scanty lelieveld j et al 2016 already the 4th ipcc assessment report 2011 had indicated that temperatures in africa are projected to rise faster than the global average increase during the 21st century james and washington 2013 an amplified warming of the sahara desert was observed for 1979 2012 by cook and vizy 2015 strengthening the summertime heat low pressure area associated with subsidence and the low level harmattan trade winds during winter lelieveld et al 2012 predicted likewise that by the middle of this century the middle east and north africa will become hotspots where days of extreme heat will have doubled since 1970 the authors argue that the expected extremes are of up to 45 50 c heat waves occur with significantly prolonged durations waha et al 2017 the mean air temperature of egypt has a positive trend of 0 017 c decade egypt second climate communication confirming a result of hansen and sato 2016 such extreme thermal discomfort could jeopardize the existence of the area s 500 million inhabitants extreme heat has important consequences for health kalkstein and smoyer 1993 and society and might even trigger a climate exodus lelieveld et al 2016 most people live in cities where compared to the rural surroundings the regional thermal conditions are strongly modified on the other hand the design of the urban structures offers potential for the implementation of heat adaptation measures elnabawi 2015 a modification of building constructions building materials green and blue infrastructure as well as surface sealing can change the energy balance of a city and mitigate the occurrence and effects of local heat as local heat is the result of regional climate interacting with urban structures the question arises which part of the temperature variability can be attributed to land use and therefore is available for mitigation measures and which part of temperature variability is due to regional climate and cannot be influenced by a climate optimized urban development the urban microclimate has characteristic temporal and spatial features and the distribution of air temperature t forms patterns that are related to the urban structure while pattern recognition in atmospheric data is quite common on the global and regional scales the application of such methods at the urban scale is scarce the aim of our study is to analyse spatiotemporal temperature patterns to interpret them in terms of land use and to suggest strategies for the adaptation to urban heat for that purpose we consider temperature data generated with micrometeorological simulations of the software envi met bruse and fleer 1998 for an urban quarter in cairo egypt these data do not have a great many degrees of freedom as they are highly correlated empirical orthogonal function eof analysis is applied to reduce the dimensionality of the data and to identify leading patterns these patterns are interpreted and utilized for recommendations for adaptation to urban heat we investigate how temporal temperature patterns are related to the radiation balance how spatial temperature patterns are related to land use and whether strategies for heat adaptation are limited this paper explains the details of our study and provides reasoned interpretations for the patterns of urban heat it is the first study estimating the maximum feasible percentage of adaptation to urban heat moreover we recommend adequate strategies to mitigate urban heat and discuss the limits of urban climate adaptation in this sense the paper casts a new light on the urban potential for climate change adaptation the study was exemplified for a region that is prone to extreme heat however the suggested methods are transferrable to urban quarters in any region of the world 2 study area the helwan university campus region of cairo governorate was specified as study area helwan 650 000 inhabitants is located between the mokattam heights and the nile and characterized by residential buildings and heavy industries and located about 24 km south east of central cairo fig 1 founded in the second part of the 19th century as a spa town due to nearby warm mineral springs from 1960 the urbanization and industrialization processes have been growing very rapidly in and around helwan between 2008 and 2011 it was the capital of an own governorate but now it can be considered a suburb of the cairo metropolitan area helwan has a warm dry desert climate with more than 330 days of sunshine per year an annual average temperature of 21 7 c maximum monthly temperature in june and july is 34 9 c and 29 mm annual precipitation koeppen geiger climate zone bwh a previous study robaa 2013 considering the urban climate at five different districts in cairo during the period 1990 2010 found in helwan the highest annual average air temperature and the lowest annual values of relative humidity and wind speed compared to the other districts of cairo robba 2013 concluded that these results are due to the strong impact of land use changes the helwan university campus was established in 1975 planned by the internationally renowned architectural office skidmore owings merrill llp at an area of ca 1400 km2 and has been under construction until now all buildings in the northern part student dormitories have the same architectural design fig 1 in the central part there are sport fields desert landscape and some plants e g hedges trees and palms arranged as inner gardens attia 2018 with the aim to form a bio climatic zone protected from wind and sun buildings in the south faculties library and administration areas are designed differently particular features of nearly all buildings are one or more vertically open slots fig 1 right hand side for ventilation purposes and to improve the thermal comfort most of the buildings are surrounded by vegetation the main streets are sealed with asphalt while pavements narrow canyons and some open squares are constructed by varieties of paving stones e g concrete ceramic and granite this skilful designed area gives rise to the question of how this landscape architecture influences the thermal conditions at the campus the micro climate results from interactions of atmospheric conditions with local land use and therefore the air temperature is not constant but characterised by temporal modes and spatial patterns in the following we identify these modes and patterns and analyse them in terms of their causes and assess the impact of several factors we hypothesize that h1 the variability of urban air temperatures can be described by very few temporal modes and spatial patterns h2 there are physical explanations for each of the modes patterns and h3 the impact of each mode pattern can be used to suggest climate change adaptation measures and to assess their effectiveness 3 methodology 3 1 envi met model envi met includes a three dimensional prognostic flow model based on the navier stokes equations simulates the surface vegetation air interactions the production and dissipation of turbulent energy the short and long wave radiation fluxes as well as the heat exchange of the atmosphere with buildings roofs and the urban ground the envi met model was previously considerably advanced hutter 2012 simon 2016 and validated resulting in version 4 for example in a residential district in freiburg germany for a heat wave day the simulations constantly overestimated air temperature t by approximately 0 2 k regardless of different t values lee h et al 2016 such a bias was not observed with envi met simulations in a sao paulo study that were comparable with the measurements gusson 2016 an application of the old version 3 of envi met for the helwan campus attia s 2018 demonstrated the potential to investigate possible impacts of the bioclimatic zones concept in hot arid areas elnabawi 2015 discusses for the old version 3 the overall agreement of simulations and measurements and the potential for integrating envi met into urban design processes to better account for microclimate effects assessing the thermal comfort by means of envi met 4 and the bio meteorological module salata et al 2016 recommended a twelve step procedure including thorough evaluation of model input parameters consideration of air temperature together with mean radiant temperature and physiologically equivalent temperature comparison of simulations to experimental data and utilization of simulation outputs to provide the thermal comfort in urban regions of cairo although many previous attempts utilized the envi met model acero and herranz pascual 2015 most of them used outdated versions of envi met lee h et al 2016 our results are based on simulations of envi met version 4 4 4 3 2 implementation of the helwan campus region in envi met in our study of the urban quarter helwan campus we used a raster grid size 4 m 4 m covering an area of 900 m 900 m 225 225 grid cells plus additional 12 nesting grid cells at each side to avoid boundary effects fig 2 an image from google earth has been used to provide a background map for the input of land use components into the model area input file all roofs are flat no inclined roof surfaces and all buildings are assumed to have the same thermal properties construction materials and albedo colours out of the total 50 625 cells 9 127 cells i e 18 are covered by buildings while 41 498 grid cells are free of buildings building heights vary between 3m and 30m simulated vegetation is classified into the three main types grass 14 2 of grid cells trees 0 3 and hedges 17 8 all types have the same short wave albedo 0 2 and cover natural unsealed soil vegetation is located around the open spaces and has a discrete presence around buildings 67 7 of the area is without vegetation soils are categorized into six classes loamy soil 63 48 asphalt road 15 48 sandy soil 12 79 brick road 6 88 and concrete pavement 1 36 simulations were made for a period of two days 15th and 16th august 2014 starting midnight 00 00 a m and stored in time steps of  hour the first day was used as a spin up for the model and excluded from further analyses while the second day contained 48 half hourly spatial distributions of air temperature 1 5m above ground and was utilized for the analysis of temporal and spatial patterns weather data for the initialization of the model were provided from weather underground database for cairo airport station latitude 30 7 19 n and longitude 31 24 20 e elevation 75 m wmo id 62366 which is the closest available meteorological station wind speed 10 m above ground was 3 09 m s wind direction nnw 340 initial air temperature 30 c air humidity 48 and roughness length was selected as 0 1 m using atmospheric soundings http weather uwyo edu upperair sounding html gathered directly at helwan observatory 62378 the air humidity at 2500m altitude was 0 6 g water kg air the temperature and humidity conditions at the boundary change during the course of the day for which we utilized hourly meteorological observations temperature minimum 05 00 h and maximum 16 00 h local time humidity vice versa as input data to calibrate the run of envi met 4 4 4 so called forcing 3 3 empirical orthogonal functions empirical orthogonal function eof analysis is a commonly used method in multivariate statistics for the analysis of spatial and temporal variability in a dataset storch and zwiers 2001 eof aims at the decomposition of a space time field t t s t 1 n s 1 p of air temperature t where t and s denote time and space positions into a set of m spatial basis functions v s m the eofs and expansion functions of time c t m according to t t s m 1 m c t m v s m for the data anomalies 1 t t s t t s 1 n t t t s this expansion can be realized by eigenvalue or singular value svd decomposition of the covariance matrix 2 c o v t 1 n 1 t 1 n t s t t t t s the covariance matrix is symmetrical and therefore diagonalisable assuming there is no perfect multi collinearity the covariance matrix is semi definite hence all its eigenvalues  m are positive and  m 2 represents the variance of the mth eigenvector v s m hannachi et al 2007 the percentage of the total variance explained by v s m m 1 m is  m l 1 m  l 100 the eigenvector v s m is also called the mth eof pattern of the anomalies t and these patterns are sorted in ascending order of the eigenvalues the set of eigenvectors forms an orthogonal basis in space i e by construction the eofs are orthogonal and uncorrelated and provide a complete basis in which the time varying field can be expanded the projection of the anomaly field t onto the mth eof pattern depends on the time point t 1 n and is given by 3  t m s 1 p t t s v s m finally the decomposition 1 can be written as 4 t t s m 1 m  m  t m v s m as r a n k c o v t p if the spatial fields are not perfectly multi collinear the eof analysis will result in m p patterns of which the first patterns explain the major part of variance and the contribution of the final patterns can be neglected an important step of the analysis is the specification of the cut off for the selection of the most important patterns for that purpose we will apply the kaiser guttmann criterion abandoning patterns of less significance reduces the noise and the degrees of freedom of the data set not every data set is suitable for an eof analysis ill conditioned data e g random data sets without any mutual correlations cannot be decomposed into meaningful orthogonal patterns the kaiser meyer olkin kmo test measures how suitable the data are for an eof analysis the higher the kmo value from 0 to 1 is the more suitable are the data for eof analysis as a rule of thumb kmo 0 8 1 0 indicates that sampling is adequate kmo 0 6 indicates that remedial action should be taken cerny and kaiser 1977 in the current study we considered envi met simulations of air temperature 1 5m above ground for a spatial region of p p 1 p 2 225 225 50 625 grid cells and a period of n 48 half hourly steps which results for the field t t s in a matrix dimension of 48 50 625 our eof analysis consisted of two steps firstly decomposing the data matrix of anomalies t by eof we identified characteristic time courses temporal modes after removing the diurnal time course from the data f is a reconstructed anomalies matrix from the retained temporal modes we applied the eof analysis to decompose f t into spatial patterns spatial eofs finally we applied regression analyses to each of these resulting spatial patterns against land use characteristics and atmospheric parameters to gather arguments for the interpretation of the spatial eofs 3 4 analysis on the r platform eof data analysis was made using the r programming environment and function prcomp venables and ripley 2010 which is based on singular value decomposition svd of the centred data sets after svd we applied three tests for the selection of most significant eofs correlation analysis kaiser guttman criterion and scree plot numerical accuracy is higher when analysis is carried out by r s function prcomp higher numerical accuracies can be an essential requirement for large data sets like the ones are being investigated here for regressions we applied the function lm which was extended by hierarchical partitioning hier part 4 results 4 1 simulated air temperature the air temperature t at 1 5m above ground fig 3 peaks at around 13 00 while the minimum value can be observed at 05 30 h local time note that the spatial average of t follows a diurnal cycle shaping from low temperatures in the morning to high values in the afternoon and a temperature decrease in the evening solid line in fig 9 this daily oscillation is a basic feature of temperature records and prior to an analysis of spatial temperature patterns this temporal cycle needs to be removed for that purpose in the following we firstly apply the eof for an identification of temporal variations and removed the daily cycle before the decomposition into spatial patterns by help of a second eof analysis 4 2 temporal modes the kmo test suggested that the simulated temperatures clearly contain temporal patterns that can be extracted by eof kmo 0 94 the decomposition into temporal eofs resulted in 48 time series the first four time variations are plotted in fig 4 their associated spatial distributions are plotted in the appendix fig a1 and we applied three different criteria to select the most important temporal modes the first criterion assesses the correlation between the temporal development of spatially averaged air temperatures fig 9 and the temporal modes fig 4 the correlation values are 1 00 0 69 0 03 0 006 for mode 1 2 3 and 4 respectively bold values indicate statistical significance at the 95 level they suggest that most of the variability in time is explained by the first two temporal modes secondly we applied the kaiser guttman criterion guttman 1954 that states that only factors with eigenvalues greater than one if the variables are standardized i e eof calculated from the correlation matrix or greater than the mean of the eigenvalues if the variables are unstandardized i e eof calculated from the covariance matrix are retained and the other eigenvalues are discarded fig 5 left the first eigenvalue  30 17 seems to be separated much larger from the rest and explained the maximum variability of the temporal temperature cycles only the first two eigenvalues exceed the mean value of 0 86 a third criterion d agostino and russell 2005 considers the explained variance associated with each temporal cycle fig 5 right hand side and illustrates that the first two temporal cycles explained 73 and 24 of the total variance respectively the most important first two temporal modes represent the daily course of the temperature cf fig 4 to further analyse only the spatial variations we removed these two cycles from the data identical with the reconstructioin of the data from all eofs except the first two the resulting half hourly temperature distributions are called filtered temperatures f and represent only the spatial variability fig 6 each of these distributions is the anomaly to the daily average temperature of t 33 64 c during the course of the day the spatial patterns change in result of different properties of the ground albedo heat capacity as well as due to generated air flows while the temporal variability comprised 10 12k fig 3 the spatial variation of air temperature was in a range of only ca 2 4k fig 6 4 3 spatial patterns for an analysis of spatial patterns of urban air temperatures the eof technique was applied to the filtered temperature data f fig 6 from the resulting spatial eofs fig 7 the magnitude and heterogeneity of the first four patterns is obvious considering the associated temporal development of these patterns fig a2 the first three series have the shortest periods of 12 and 24 h in particular the time series of the second spatial pattern is an oscillation reciprocal to the daily temperature cycle the time series of the fourth spatial pattern fig a2 makes more irregular oscillations to identify the most important spatial patterns we applied a selection strategy similar to that described in section 4 2 a first test was based on the correlation over 50 625 grid cells between the identified spatial patterns eofs fig 7 and the temperature fields generated by envi met simulations filtered data f fig 6 for selected time points table 1 we find highest and significant correlations for the first three spatial patterns the kaiser criterion for the explained variance of each spatial eof fig 8 left indicates that the first eigenvalue  949 is much larger than the remaining a comparison with the mean eigenvalue  suggests that especially the first two spatial patterns are relevant the third eigenvalue 29 0 is just slightly above  24 4 the scree plot assesses the variance explained by each spatial eof fig 8 right hand side the first two spatial patterns explain 81 and 13 of the total variance respectively a cumulative contribution of 94 in result the first two spatial patterns proved to be most important and they will be discussed in detail in the interpretation section 5 interpretation and discussion 5 1 simulated air temperature in an urban quarter simulated air temperatures t have a pronounced variability in space and time fig 3 the latter is dominated by the diurnal cycle fig 9 solid bold line for the area average of simulated air temperatures as a consequence of the solar heating in the morning the incoming short wave radiation rises rapidly until its maximum at noon and decreases in the afternoon symmetrically to the increase in the morning fig 9 dotted line for the average amount of short wave radiation in w m 2 the short wave radiation is absorbed by surfaces according to their albedo resulting in heat up as is known from remote sensing the radiant temperature curve is characteristic for the surface cover lillesand 2015 and determines the output of long wave radiation fig 9 thin solid line representing an average over the simulated area atmospheric gases are relatively good absorbers of long wave radiation together with turbulent heat diffusion this rises the air temperature t outgoing radiation increases after sunrise but lags somewhat behind the insolation curve air temperature follows the same pattern that has a daily temperature lag of the maximum behind incoming radiation the temperature time series simulated by envi met show a typical daily cycle indicating that the simulations are plausible fig 9 air temperature has a minimum at 05 30 and peaks at 13 00 slightly differing from the min max of forcing see section 3 2 surface warming depends on the material albedo heat conductivity heat capacity and in this way local warming of air is determined by the structure of the ground not shown here this effect can be observed from the air temperature increase in the morning that is very steep for rocks having low heat capacity and conductivity as compared to water for example therefore air temperature rates of heating and cooling can often provide significant acquaintance about the conditions and components of the urban land use e g buildings and vegetation and soils surface temperatures of buildings normally exceed those of vegetation during the day the process is conversely during the night because of the emission of long wave terrestrial radiation and absence of sunlight the time points where the temperature curves not shown here for both vegetation and other components intersect are the thermal crossovers they indicate times at which no radiant temperature difference exists between two materials and take place shortly after dawn and near sunset lillesand et al 2015 at day night local warming cooling generates baroclinic conditions initiating local air circulations all these local processes explain the spatial patterns in air temperature fig 3 5 2 temporal modes the first two temporal eofs characterize the daily temperature cycle while the first mode fig 4 1 represents the general daily development of air temperature compare to t in fig 9 the second temporal mode has a line curvature that is sharper and associated with a small pronounced protrusion before the trough and after the peak crest as the simulated t refers to a height of 1 5 m above ground these protrusions could be due to the shadowing effects of trees buildings and some other topographic features moreover the heating strength depends on ground material see discussion in 5 1 and the orientation of these urban components to the sun change during the course of the day the amplitudes of the temporal modes fig 4 4 75k for 1 and 1 98k for 2 correspond to the temporal variability section 4 2 and result from radiative heating as well as advection of heat that is involved in the envi met 4 4 4 procedure of external forcing each temporal mode has a spatial representation the spatial weights fig a1 in appendix the weights of the first mode exhibit strong positive values in the middle of the domain characterized by trees cf fig a1 top left and fig a4 this mode accounts for up to 74 of the temporal variance especially in the region of the largest amplitude the fraction of local variance explained decreases towards the south and west the spatial representation of the second temporal mode displays high positive weights at buildings in the south western area and negative values at a sandy vegetated ground in the north eastern part here the phase of 2 fig 4 is delayed because in this area the evaporation of moisture produces a slight cooling in the morning in comparison to the temperature rise at build up areas this mode explains up to 24 of the variance near its centre of action the associated time series consists of a combination of inter hourly and lower frequency oscillations 5 3 spatial patterns the first two spatial patterns fig 7 spatial pattern 1 with 2 2k and spatial pattern 2 with 1 5k amplitude together explained 94 of the study area s spatial temperature variability the temporal development of these spatial patterns is represented by the corresponding time series projections in fig a2 both patterns have maximum weight at 09 00 a time when the contrast between warm and cold spots becomes remarkable see fig 6 as a superposition of pattern 1 and pattern 2 at vegetation spots fig a4 the t variability can be qualitatively attributed to the evapotranspiration process in open spaces that is modified by the albedo and emissivity sand albedo is calculated inside envi met using the soil wetness and the sun zenith angle respectively of their natural unsealed soils this was very obvious over sand soil sd roughly most of the zones of high temperature amplitudes are found to be covered by sandy soil or close to such zones coakley 2003 positive temperature anomalies are associated to lower evapotranspiration in the off vegetation region beside the thermal absorption and discharge of the land use structure spatial patterns of air temperature in an urban quarter can be explained by the impact of meteorological and land use parameters the effects of these factors are studied more in detail by means of regression analyses a regression on meteorological parameters the first spatial pattern fig 7 clearly represents the field of air temperature anomalies averaged over time f s with s t d f s 0 23 k as can be seen from the fitted regression models s e o f 1 s 1 02 f s r 2 0 94 and s e o f 2 s 0 001 f s r 2 0 07 this perfect agreement of seof1 and poor fit of seof2 with average temperature anomalies suggests that seof1 but not seof2 is determined by atmospheric conditions in a more detailed analysis we considered meteorological parameters fig 10 including time averages of wind speed ws in m s specific humidity sh in g kg turbulent kinetic energy tke in m2 s2 and long wave radiation emitted from the surface lwre in w m2 in the fitted regression models 5 s e o f 1 s 0 6 k 0 14 k s m w s s 3 56 k k g g s h s 0 60 k s 2 m 2 t k e s 0 73 k m 2 w l w r e s with r 2 0 84 and 6 s e o f 2 s 0 3 k 0 13 k s m w s s 0 91 k k g g s h s 0 06 k s 2 m 2 t k e s 0 47 k m 2 w l w r e s with r 2 0 09 all impact factors are statistically significant 95 level along the buildings long wave radiation and tke are important fig 10 c d in the open regions the wind plays an important role fig 10 a while the specific humidity fig 10 b is large in a vegetated sandy region fig a4 in the north eastern part of the quarter as a result of these regressions we clearly identify seof 1 as the pattern that mostly at 84 represents the meteorological conditions of the urban quarter the impact of meteorology on the second spatial pattern is just 9 which means that most of this pattern s variability is explained by other parameters this is studied by help of land use regressions in the following section b regression on land use parameters the anthropogenic impact on spatial patterns of air temperature is investigated by means of land use regressions table 2 we included three main vegetation types grass trees and hedges as well as five soil classes asphalt road pavement concrete loamy soil sandy soil and brick road a regression analysis was applied to identify the most important impact factors for the spatial air temperature patterns this regression comprises just land use parameters table 2 but not all details of the physical parameterizations including surface albedo and soil moisture memory that are integrated in the soil atmosphere processes of a grid model merrifield 2016 such as envi met in result the urban land use parameters alone explained 41 table 2 and the atmospheric parameters alone explained 84 of seof1 eq 5 by multi linear regression models in contrast the coefficient of determination for seof2 was 19 table 2 for land use and just 9 eq 6 for atmospheric parameters considering all factors together in one model explains 85 of seof1 and 27 of seof2 see table a1 interactions of atmospheric and urban parameters did not have any significant effects we observed that vegetation components have an inverse impact as compared to soil components table a1 the results suggest that seof1 represents the natural variability of the meteorological parameters either the effect of average temperature r2 94 or the effects of the atmospheric parameters ws sh tke and lwre r2 84 while seof2 represents the variability due to land use a much clearer separation of the impact of both groups of factors results from a hierarchical partitioning analysis which determines the relative importance of each independent factor and makes the results more directly comparable chevan and sutherland 1991 hierarchical partitioning clearly supports the dominant impact of atmospheric parameters on seof1 and land use on seof2 table a2 6 conclusions and outlook we conclude that the land use atmospheric coupling feedback is responsible for urban spatiotemporal patterns of air temperature 1 5m above ground in our study this was exemplified for the campus of helwan university and demonstrated that the first temporal eof represents the typical daily temperature cycle and explains ca 73 of the daily temperature variation the second eof explains 24 of the daily temperature variation and characterizes modifications due to land use components such as buildings and associated shading effects the first spatial pattern seof1 nearly equals the mean spatial temperature distribution hierarchical partitioning analysis table a2 demonstrates that atmospheric parameters including wind speed specific humidity turbulent kinetic energy and long wave radiation environment account for 68 of seof1 and 32 are influenced by land use the second spatial pattern seof2 can be interpreted as the effect of land use because 73 of seof2 is influenced by land use components including different buildings neighbourhood vegetation and soils and 27 are allocated to atmospheric parameters table a2 the occurrence of patterns expresses the strong auto correlation of urban atmospheric data our analysis demonstrates that the elimination of the daily cycle is needed prior to the recognition of spatial patterns in result 97 of the temporal variations are explained by the identified two daily cycles and 94 of the spatial variability is explained by the two identified spatial patterns the high representativeness of these four patterns suggests that they are essential for the spatiotemporal distribution of air temperature in the urban quarter confirming hypothesis h1 and they can be utilized for the recommendation of adaptation measures our investigation ascertained that the temporal modes as well as seof1 depend on the prevailing weather situation only the second spatial pattern seof2 can be influenced by land use changes confirming h2 in general vegetation grass hedges and trees is conducive to reduced temperatures any sealing pavement asphalt bricks concrete acts to increase the temperature h3 however the effectiveness of land use changes to mitigate temperature burden is limited because the pattern seof2 accounts for just 13 of the temperature variability as the suggested decomposition of urban temperature distributions into patters was exemplified here for an urban quarter in a hot and arid clime that is moreover a hot spot region of climate change the impact of the weather situation might be stronger compared to other regions in the world further studies might demonstrate how the effectiveness of adaptation measures can change with the climate zone while the presented approach assesses the amount of temperature variability attributed to land use changes a direct prediction of the temperature change resulting from a specific adaptation measure would be desirable for that purpose an approach for the decomposition of surface temperature values was recently developed hertel and schlink 2019 this technique might be extended to urban air temperatures in future research work both techniques attribution of temperature variability as well as of temperature increments could be useful tools for planners optimizing adaptation measures in general physical patterns tend to be non orthogonal hannachi 2007 a limitation of the eof analysis is the assumption of orthogonal patterns because these patterns might result from a superposition of different physical phenomena dommenget and latif 2002 on the other hand it might be possible that an individual eof does not represent the effect of a specific atmosphere land coupling parameter on the air temperature roundy 2014 for example in our study grass vegetation has a significant impact on both spatial temperature patterns table a1 7 software and technical notes for micrometeorological simulations the model envi met version 4 4 4 science license http www envi met com was used together with the visualization program leonardo input data are retrieved from a google earth image of helwan university campus from the weather underground database for cairo airport station latitude 30 7 19 n and longitude 31 24 20 e elevation 75 m wmo id 62366 and from the atmospheric soundings website at university of wyoming http weather uwyo edu upperair sounding html visualization conversion of envi met output data from binary format and the eof analysis were done with programs developed in r r core team 2015 https www r project org hierarchical partitioning was calculated using the hier part package walsh and mac nally 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the work was partly made in frame of the helmholtz climate initiative hi cam that is funded by the helmholtz associations initiative and networking fund the authors are responsible for the content of this publication appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104773 
25983,a hot spot region of climate change is the middle east where temperatures actually have a rising tendency and this will increase in future to mitigate the progressing thermal burden urban planning has to develop adaptation measures on the basis of micrometeorological simulations for a quarter in cairo we suggest a decomposition of air temperatures into two temporal and two spatial patterns respectively explaining 97 and 94 of the temperature variability we find that land use has a significant impact on the spatial temperature distribution and should be modified for the purpose of heat adaptation however just 13 of the spatial temperature variability are explained by land use which is a quite limited impact regional weather conditions are the dominant factor for the spatial as well as the temporal development of urban heat keywords urban air temperature empirical orthogonal functions emvi met climate change adaptation 1 introduction average global temperatures tend to increase but regional climatic conditions can develop in very diverse manner due to differences in land use feedback processes atmospheric stability and global circulations if we in particular consider the exposure to extreme heat the changes of both regional climate and population have to be taken into account for example liu et al 2017 found that for the higher emission scenario rcp8 5 the heat exposure for europe will increase by a factor of four by 2100 while the heat exposure for africa will be over 118 times greater than it has been historically exposure refers to people in the considered region that are subject to potential risks due to heat the middle east and north africa region has an arid hot desert climate where precipitation is low rainfall is only during december february and primarily over coastal areas and vegetation scanty lelieveld j et al 2016 already the 4th ipcc assessment report 2011 had indicated that temperatures in africa are projected to rise faster than the global average increase during the 21st century james and washington 2013 an amplified warming of the sahara desert was observed for 1979 2012 by cook and vizy 2015 strengthening the summertime heat low pressure area associated with subsidence and the low level harmattan trade winds during winter lelieveld et al 2012 predicted likewise that by the middle of this century the middle east and north africa will become hotspots where days of extreme heat will have doubled since 1970 the authors argue that the expected extremes are of up to 45 50 c heat waves occur with significantly prolonged durations waha et al 2017 the mean air temperature of egypt has a positive trend of 0 017 c decade egypt second climate communication confirming a result of hansen and sato 2016 such extreme thermal discomfort could jeopardize the existence of the area s 500 million inhabitants extreme heat has important consequences for health kalkstein and smoyer 1993 and society and might even trigger a climate exodus lelieveld et al 2016 most people live in cities where compared to the rural surroundings the regional thermal conditions are strongly modified on the other hand the design of the urban structures offers potential for the implementation of heat adaptation measures elnabawi 2015 a modification of building constructions building materials green and blue infrastructure as well as surface sealing can change the energy balance of a city and mitigate the occurrence and effects of local heat as local heat is the result of regional climate interacting with urban structures the question arises which part of the temperature variability can be attributed to land use and therefore is available for mitigation measures and which part of temperature variability is due to regional climate and cannot be influenced by a climate optimized urban development the urban microclimate has characteristic temporal and spatial features and the distribution of air temperature t forms patterns that are related to the urban structure while pattern recognition in atmospheric data is quite common on the global and regional scales the application of such methods at the urban scale is scarce the aim of our study is to analyse spatiotemporal temperature patterns to interpret them in terms of land use and to suggest strategies for the adaptation to urban heat for that purpose we consider temperature data generated with micrometeorological simulations of the software envi met bruse and fleer 1998 for an urban quarter in cairo egypt these data do not have a great many degrees of freedom as they are highly correlated empirical orthogonal function eof analysis is applied to reduce the dimensionality of the data and to identify leading patterns these patterns are interpreted and utilized for recommendations for adaptation to urban heat we investigate how temporal temperature patterns are related to the radiation balance how spatial temperature patterns are related to land use and whether strategies for heat adaptation are limited this paper explains the details of our study and provides reasoned interpretations for the patterns of urban heat it is the first study estimating the maximum feasible percentage of adaptation to urban heat moreover we recommend adequate strategies to mitigate urban heat and discuss the limits of urban climate adaptation in this sense the paper casts a new light on the urban potential for climate change adaptation the study was exemplified for a region that is prone to extreme heat however the suggested methods are transferrable to urban quarters in any region of the world 2 study area the helwan university campus region of cairo governorate was specified as study area helwan 650 000 inhabitants is located between the mokattam heights and the nile and characterized by residential buildings and heavy industries and located about 24 km south east of central cairo fig 1 founded in the second part of the 19th century as a spa town due to nearby warm mineral springs from 1960 the urbanization and industrialization processes have been growing very rapidly in and around helwan between 2008 and 2011 it was the capital of an own governorate but now it can be considered a suburb of the cairo metropolitan area helwan has a warm dry desert climate with more than 330 days of sunshine per year an annual average temperature of 21 7 c maximum monthly temperature in june and july is 34 9 c and 29 mm annual precipitation koeppen geiger climate zone bwh a previous study robaa 2013 considering the urban climate at five different districts in cairo during the period 1990 2010 found in helwan the highest annual average air temperature and the lowest annual values of relative humidity and wind speed compared to the other districts of cairo robba 2013 concluded that these results are due to the strong impact of land use changes the helwan university campus was established in 1975 planned by the internationally renowned architectural office skidmore owings merrill llp at an area of ca 1400 km2 and has been under construction until now all buildings in the northern part student dormitories have the same architectural design fig 1 in the central part there are sport fields desert landscape and some plants e g hedges trees and palms arranged as inner gardens attia 2018 with the aim to form a bio climatic zone protected from wind and sun buildings in the south faculties library and administration areas are designed differently particular features of nearly all buildings are one or more vertically open slots fig 1 right hand side for ventilation purposes and to improve the thermal comfort most of the buildings are surrounded by vegetation the main streets are sealed with asphalt while pavements narrow canyons and some open squares are constructed by varieties of paving stones e g concrete ceramic and granite this skilful designed area gives rise to the question of how this landscape architecture influences the thermal conditions at the campus the micro climate results from interactions of atmospheric conditions with local land use and therefore the air temperature is not constant but characterised by temporal modes and spatial patterns in the following we identify these modes and patterns and analyse them in terms of their causes and assess the impact of several factors we hypothesize that h1 the variability of urban air temperatures can be described by very few temporal modes and spatial patterns h2 there are physical explanations for each of the modes patterns and h3 the impact of each mode pattern can be used to suggest climate change adaptation measures and to assess their effectiveness 3 methodology 3 1 envi met model envi met includes a three dimensional prognostic flow model based on the navier stokes equations simulates the surface vegetation air interactions the production and dissipation of turbulent energy the short and long wave radiation fluxes as well as the heat exchange of the atmosphere with buildings roofs and the urban ground the envi met model was previously considerably advanced hutter 2012 simon 2016 and validated resulting in version 4 for example in a residential district in freiburg germany for a heat wave day the simulations constantly overestimated air temperature t by approximately 0 2 k regardless of different t values lee h et al 2016 such a bias was not observed with envi met simulations in a sao paulo study that were comparable with the measurements gusson 2016 an application of the old version 3 of envi met for the helwan campus attia s 2018 demonstrated the potential to investigate possible impacts of the bioclimatic zones concept in hot arid areas elnabawi 2015 discusses for the old version 3 the overall agreement of simulations and measurements and the potential for integrating envi met into urban design processes to better account for microclimate effects assessing the thermal comfort by means of envi met 4 and the bio meteorological module salata et al 2016 recommended a twelve step procedure including thorough evaluation of model input parameters consideration of air temperature together with mean radiant temperature and physiologically equivalent temperature comparison of simulations to experimental data and utilization of simulation outputs to provide the thermal comfort in urban regions of cairo although many previous attempts utilized the envi met model acero and herranz pascual 2015 most of them used outdated versions of envi met lee h et al 2016 our results are based on simulations of envi met version 4 4 4 3 2 implementation of the helwan campus region in envi met in our study of the urban quarter helwan campus we used a raster grid size 4 m 4 m covering an area of 900 m 900 m 225 225 grid cells plus additional 12 nesting grid cells at each side to avoid boundary effects fig 2 an image from google earth has been used to provide a background map for the input of land use components into the model area input file all roofs are flat no inclined roof surfaces and all buildings are assumed to have the same thermal properties construction materials and albedo colours out of the total 50 625 cells 9 127 cells i e 18 are covered by buildings while 41 498 grid cells are free of buildings building heights vary between 3m and 30m simulated vegetation is classified into the three main types grass 14 2 of grid cells trees 0 3 and hedges 17 8 all types have the same short wave albedo 0 2 and cover natural unsealed soil vegetation is located around the open spaces and has a discrete presence around buildings 67 7 of the area is without vegetation soils are categorized into six classes loamy soil 63 48 asphalt road 15 48 sandy soil 12 79 brick road 6 88 and concrete pavement 1 36 simulations were made for a period of two days 15th and 16th august 2014 starting midnight 00 00 a m and stored in time steps of  hour the first day was used as a spin up for the model and excluded from further analyses while the second day contained 48 half hourly spatial distributions of air temperature 1 5m above ground and was utilized for the analysis of temporal and spatial patterns weather data for the initialization of the model were provided from weather underground database for cairo airport station latitude 30 7 19 n and longitude 31 24 20 e elevation 75 m wmo id 62366 which is the closest available meteorological station wind speed 10 m above ground was 3 09 m s wind direction nnw 340 initial air temperature 30 c air humidity 48 and roughness length was selected as 0 1 m using atmospheric soundings http weather uwyo edu upperair sounding html gathered directly at helwan observatory 62378 the air humidity at 2500m altitude was 0 6 g water kg air the temperature and humidity conditions at the boundary change during the course of the day for which we utilized hourly meteorological observations temperature minimum 05 00 h and maximum 16 00 h local time humidity vice versa as input data to calibrate the run of envi met 4 4 4 so called forcing 3 3 empirical orthogonal functions empirical orthogonal function eof analysis is a commonly used method in multivariate statistics for the analysis of spatial and temporal variability in a dataset storch and zwiers 2001 eof aims at the decomposition of a space time field t t s t 1 n s 1 p of air temperature t where t and s denote time and space positions into a set of m spatial basis functions v s m the eofs and expansion functions of time c t m according to t t s m 1 m c t m v s m for the data anomalies 1 t t s t t s 1 n t t t s this expansion can be realized by eigenvalue or singular value svd decomposition of the covariance matrix 2 c o v t 1 n 1 t 1 n t s t t t t s the covariance matrix is symmetrical and therefore diagonalisable assuming there is no perfect multi collinearity the covariance matrix is semi definite hence all its eigenvalues  m are positive and  m 2 represents the variance of the mth eigenvector v s m hannachi et al 2007 the percentage of the total variance explained by v s m m 1 m is  m l 1 m  l 100 the eigenvector v s m is also called the mth eof pattern of the anomalies t and these patterns are sorted in ascending order of the eigenvalues the set of eigenvectors forms an orthogonal basis in space i e by construction the eofs are orthogonal and uncorrelated and provide a complete basis in which the time varying field can be expanded the projection of the anomaly field t onto the mth eof pattern depends on the time point t 1 n and is given by 3  t m s 1 p t t s v s m finally the decomposition 1 can be written as 4 t t s m 1 m  m  t m v s m as r a n k c o v t p if the spatial fields are not perfectly multi collinear the eof analysis will result in m p patterns of which the first patterns explain the major part of variance and the contribution of the final patterns can be neglected an important step of the analysis is the specification of the cut off for the selection of the most important patterns for that purpose we will apply the kaiser guttmann criterion abandoning patterns of less significance reduces the noise and the degrees of freedom of the data set not every data set is suitable for an eof analysis ill conditioned data e g random data sets without any mutual correlations cannot be decomposed into meaningful orthogonal patterns the kaiser meyer olkin kmo test measures how suitable the data are for an eof analysis the higher the kmo value from 0 to 1 is the more suitable are the data for eof analysis as a rule of thumb kmo 0 8 1 0 indicates that sampling is adequate kmo 0 6 indicates that remedial action should be taken cerny and kaiser 1977 in the current study we considered envi met simulations of air temperature 1 5m above ground for a spatial region of p p 1 p 2 225 225 50 625 grid cells and a period of n 48 half hourly steps which results for the field t t s in a matrix dimension of 48 50 625 our eof analysis consisted of two steps firstly decomposing the data matrix of anomalies t by eof we identified characteristic time courses temporal modes after removing the diurnal time course from the data f is a reconstructed anomalies matrix from the retained temporal modes we applied the eof analysis to decompose f t into spatial patterns spatial eofs finally we applied regression analyses to each of these resulting spatial patterns against land use characteristics and atmospheric parameters to gather arguments for the interpretation of the spatial eofs 3 4 analysis on the r platform eof data analysis was made using the r programming environment and function prcomp venables and ripley 2010 which is based on singular value decomposition svd of the centred data sets after svd we applied three tests for the selection of most significant eofs correlation analysis kaiser guttman criterion and scree plot numerical accuracy is higher when analysis is carried out by r s function prcomp higher numerical accuracies can be an essential requirement for large data sets like the ones are being investigated here for regressions we applied the function lm which was extended by hierarchical partitioning hier part 4 results 4 1 simulated air temperature the air temperature t at 1 5m above ground fig 3 peaks at around 13 00 while the minimum value can be observed at 05 30 h local time note that the spatial average of t follows a diurnal cycle shaping from low temperatures in the morning to high values in the afternoon and a temperature decrease in the evening solid line in fig 9 this daily oscillation is a basic feature of temperature records and prior to an analysis of spatial temperature patterns this temporal cycle needs to be removed for that purpose in the following we firstly apply the eof for an identification of temporal variations and removed the daily cycle before the decomposition into spatial patterns by help of a second eof analysis 4 2 temporal modes the kmo test suggested that the simulated temperatures clearly contain temporal patterns that can be extracted by eof kmo 0 94 the decomposition into temporal eofs resulted in 48 time series the first four time variations are plotted in fig 4 their associated spatial distributions are plotted in the appendix fig a1 and we applied three different criteria to select the most important temporal modes the first criterion assesses the correlation between the temporal development of spatially averaged air temperatures fig 9 and the temporal modes fig 4 the correlation values are 1 00 0 69 0 03 0 006 for mode 1 2 3 and 4 respectively bold values indicate statistical significance at the 95 level they suggest that most of the variability in time is explained by the first two temporal modes secondly we applied the kaiser guttman criterion guttman 1954 that states that only factors with eigenvalues greater than one if the variables are standardized i e eof calculated from the correlation matrix or greater than the mean of the eigenvalues if the variables are unstandardized i e eof calculated from the covariance matrix are retained and the other eigenvalues are discarded fig 5 left the first eigenvalue  30 17 seems to be separated much larger from the rest and explained the maximum variability of the temporal temperature cycles only the first two eigenvalues exceed the mean value of 0 86 a third criterion d agostino and russell 2005 considers the explained variance associated with each temporal cycle fig 5 right hand side and illustrates that the first two temporal cycles explained 73 and 24 of the total variance respectively the most important first two temporal modes represent the daily course of the temperature cf fig 4 to further analyse only the spatial variations we removed these two cycles from the data identical with the reconstructioin of the data from all eofs except the first two the resulting half hourly temperature distributions are called filtered temperatures f and represent only the spatial variability fig 6 each of these distributions is the anomaly to the daily average temperature of t 33 64 c during the course of the day the spatial patterns change in result of different properties of the ground albedo heat capacity as well as due to generated air flows while the temporal variability comprised 10 12k fig 3 the spatial variation of air temperature was in a range of only ca 2 4k fig 6 4 3 spatial patterns for an analysis of spatial patterns of urban air temperatures the eof technique was applied to the filtered temperature data f fig 6 from the resulting spatial eofs fig 7 the magnitude and heterogeneity of the first four patterns is obvious considering the associated temporal development of these patterns fig a2 the first three series have the shortest periods of 12 and 24 h in particular the time series of the second spatial pattern is an oscillation reciprocal to the daily temperature cycle the time series of the fourth spatial pattern fig a2 makes more irregular oscillations to identify the most important spatial patterns we applied a selection strategy similar to that described in section 4 2 a first test was based on the correlation over 50 625 grid cells between the identified spatial patterns eofs fig 7 and the temperature fields generated by envi met simulations filtered data f fig 6 for selected time points table 1 we find highest and significant correlations for the first three spatial patterns the kaiser criterion for the explained variance of each spatial eof fig 8 left indicates that the first eigenvalue  949 is much larger than the remaining a comparison with the mean eigenvalue  suggests that especially the first two spatial patterns are relevant the third eigenvalue 29 0 is just slightly above  24 4 the scree plot assesses the variance explained by each spatial eof fig 8 right hand side the first two spatial patterns explain 81 and 13 of the total variance respectively a cumulative contribution of 94 in result the first two spatial patterns proved to be most important and they will be discussed in detail in the interpretation section 5 interpretation and discussion 5 1 simulated air temperature in an urban quarter simulated air temperatures t have a pronounced variability in space and time fig 3 the latter is dominated by the diurnal cycle fig 9 solid bold line for the area average of simulated air temperatures as a consequence of the solar heating in the morning the incoming short wave radiation rises rapidly until its maximum at noon and decreases in the afternoon symmetrically to the increase in the morning fig 9 dotted line for the average amount of short wave radiation in w m 2 the short wave radiation is absorbed by surfaces according to their albedo resulting in heat up as is known from remote sensing the radiant temperature curve is characteristic for the surface cover lillesand 2015 and determines the output of long wave radiation fig 9 thin solid line representing an average over the simulated area atmospheric gases are relatively good absorbers of long wave radiation together with turbulent heat diffusion this rises the air temperature t outgoing radiation increases after sunrise but lags somewhat behind the insolation curve air temperature follows the same pattern that has a daily temperature lag of the maximum behind incoming radiation the temperature time series simulated by envi met show a typical daily cycle indicating that the simulations are plausible fig 9 air temperature has a minimum at 05 30 and peaks at 13 00 slightly differing from the min max of forcing see section 3 2 surface warming depends on the material albedo heat conductivity heat capacity and in this way local warming of air is determined by the structure of the ground not shown here this effect can be observed from the air temperature increase in the morning that is very steep for rocks having low heat capacity and conductivity as compared to water for example therefore air temperature rates of heating and cooling can often provide significant acquaintance about the conditions and components of the urban land use e g buildings and vegetation and soils surface temperatures of buildings normally exceed those of vegetation during the day the process is conversely during the night because of the emission of long wave terrestrial radiation and absence of sunlight the time points where the temperature curves not shown here for both vegetation and other components intersect are the thermal crossovers they indicate times at which no radiant temperature difference exists between two materials and take place shortly after dawn and near sunset lillesand et al 2015 at day night local warming cooling generates baroclinic conditions initiating local air circulations all these local processes explain the spatial patterns in air temperature fig 3 5 2 temporal modes the first two temporal eofs characterize the daily temperature cycle while the first mode fig 4 1 represents the general daily development of air temperature compare to t in fig 9 the second temporal mode has a line curvature that is sharper and associated with a small pronounced protrusion before the trough and after the peak crest as the simulated t refers to a height of 1 5 m above ground these protrusions could be due to the shadowing effects of trees buildings and some other topographic features moreover the heating strength depends on ground material see discussion in 5 1 and the orientation of these urban components to the sun change during the course of the day the amplitudes of the temporal modes fig 4 4 75k for 1 and 1 98k for 2 correspond to the temporal variability section 4 2 and result from radiative heating as well as advection of heat that is involved in the envi met 4 4 4 procedure of external forcing each temporal mode has a spatial representation the spatial weights fig a1 in appendix the weights of the first mode exhibit strong positive values in the middle of the domain characterized by trees cf fig a1 top left and fig a4 this mode accounts for up to 74 of the temporal variance especially in the region of the largest amplitude the fraction of local variance explained decreases towards the south and west the spatial representation of the second temporal mode displays high positive weights at buildings in the south western area and negative values at a sandy vegetated ground in the north eastern part here the phase of 2 fig 4 is delayed because in this area the evaporation of moisture produces a slight cooling in the morning in comparison to the temperature rise at build up areas this mode explains up to 24 of the variance near its centre of action the associated time series consists of a combination of inter hourly and lower frequency oscillations 5 3 spatial patterns the first two spatial patterns fig 7 spatial pattern 1 with 2 2k and spatial pattern 2 with 1 5k amplitude together explained 94 of the study area s spatial temperature variability the temporal development of these spatial patterns is represented by the corresponding time series projections in fig a2 both patterns have maximum weight at 09 00 a time when the contrast between warm and cold spots becomes remarkable see fig 6 as a superposition of pattern 1 and pattern 2 at vegetation spots fig a4 the t variability can be qualitatively attributed to the evapotranspiration process in open spaces that is modified by the albedo and emissivity sand albedo is calculated inside envi met using the soil wetness and the sun zenith angle respectively of their natural unsealed soils this was very obvious over sand soil sd roughly most of the zones of high temperature amplitudes are found to be covered by sandy soil or close to such zones coakley 2003 positive temperature anomalies are associated to lower evapotranspiration in the off vegetation region beside the thermal absorption and discharge of the land use structure spatial patterns of air temperature in an urban quarter can be explained by the impact of meteorological and land use parameters the effects of these factors are studied more in detail by means of regression analyses a regression on meteorological parameters the first spatial pattern fig 7 clearly represents the field of air temperature anomalies averaged over time f s with s t d f s 0 23 k as can be seen from the fitted regression models s e o f 1 s 1 02 f s r 2 0 94 and s e o f 2 s 0 001 f s r 2 0 07 this perfect agreement of seof1 and poor fit of seof2 with average temperature anomalies suggests that seof1 but not seof2 is determined by atmospheric conditions in a more detailed analysis we considered meteorological parameters fig 10 including time averages of wind speed ws in m s specific humidity sh in g kg turbulent kinetic energy tke in m2 s2 and long wave radiation emitted from the surface lwre in w m2 in the fitted regression models 5 s e o f 1 s 0 6 k 0 14 k s m w s s 3 56 k k g g s h s 0 60 k s 2 m 2 t k e s 0 73 k m 2 w l w r e s with r 2 0 84 and 6 s e o f 2 s 0 3 k 0 13 k s m w s s 0 91 k k g g s h s 0 06 k s 2 m 2 t k e s 0 47 k m 2 w l w r e s with r 2 0 09 all impact factors are statistically significant 95 level along the buildings long wave radiation and tke are important fig 10 c d in the open regions the wind plays an important role fig 10 a while the specific humidity fig 10 b is large in a vegetated sandy region fig a4 in the north eastern part of the quarter as a result of these regressions we clearly identify seof 1 as the pattern that mostly at 84 represents the meteorological conditions of the urban quarter the impact of meteorology on the second spatial pattern is just 9 which means that most of this pattern s variability is explained by other parameters this is studied by help of land use regressions in the following section b regression on land use parameters the anthropogenic impact on spatial patterns of air temperature is investigated by means of land use regressions table 2 we included three main vegetation types grass trees and hedges as well as five soil classes asphalt road pavement concrete loamy soil sandy soil and brick road a regression analysis was applied to identify the most important impact factors for the spatial air temperature patterns this regression comprises just land use parameters table 2 but not all details of the physical parameterizations including surface albedo and soil moisture memory that are integrated in the soil atmosphere processes of a grid model merrifield 2016 such as envi met in result the urban land use parameters alone explained 41 table 2 and the atmospheric parameters alone explained 84 of seof1 eq 5 by multi linear regression models in contrast the coefficient of determination for seof2 was 19 table 2 for land use and just 9 eq 6 for atmospheric parameters considering all factors together in one model explains 85 of seof1 and 27 of seof2 see table a1 interactions of atmospheric and urban parameters did not have any significant effects we observed that vegetation components have an inverse impact as compared to soil components table a1 the results suggest that seof1 represents the natural variability of the meteorological parameters either the effect of average temperature r2 94 or the effects of the atmospheric parameters ws sh tke and lwre r2 84 while seof2 represents the variability due to land use a much clearer separation of the impact of both groups of factors results from a hierarchical partitioning analysis which determines the relative importance of each independent factor and makes the results more directly comparable chevan and sutherland 1991 hierarchical partitioning clearly supports the dominant impact of atmospheric parameters on seof1 and land use on seof2 table a2 6 conclusions and outlook we conclude that the land use atmospheric coupling feedback is responsible for urban spatiotemporal patterns of air temperature 1 5m above ground in our study this was exemplified for the campus of helwan university and demonstrated that the first temporal eof represents the typical daily temperature cycle and explains ca 73 of the daily temperature variation the second eof explains 24 of the daily temperature variation and characterizes modifications due to land use components such as buildings and associated shading effects the first spatial pattern seof1 nearly equals the mean spatial temperature distribution hierarchical partitioning analysis table a2 demonstrates that atmospheric parameters including wind speed specific humidity turbulent kinetic energy and long wave radiation environment account for 68 of seof1 and 32 are influenced by land use the second spatial pattern seof2 can be interpreted as the effect of land use because 73 of seof2 is influenced by land use components including different buildings neighbourhood vegetation and soils and 27 are allocated to atmospheric parameters table a2 the occurrence of patterns expresses the strong auto correlation of urban atmospheric data our analysis demonstrates that the elimination of the daily cycle is needed prior to the recognition of spatial patterns in result 97 of the temporal variations are explained by the identified two daily cycles and 94 of the spatial variability is explained by the two identified spatial patterns the high representativeness of these four patterns suggests that they are essential for the spatiotemporal distribution of air temperature in the urban quarter confirming hypothesis h1 and they can be utilized for the recommendation of adaptation measures our investigation ascertained that the temporal modes as well as seof1 depend on the prevailing weather situation only the second spatial pattern seof2 can be influenced by land use changes confirming h2 in general vegetation grass hedges and trees is conducive to reduced temperatures any sealing pavement asphalt bricks concrete acts to increase the temperature h3 however the effectiveness of land use changes to mitigate temperature burden is limited because the pattern seof2 accounts for just 13 of the temperature variability as the suggested decomposition of urban temperature distributions into patters was exemplified here for an urban quarter in a hot and arid clime that is moreover a hot spot region of climate change the impact of the weather situation might be stronger compared to other regions in the world further studies might demonstrate how the effectiveness of adaptation measures can change with the climate zone while the presented approach assesses the amount of temperature variability attributed to land use changes a direct prediction of the temperature change resulting from a specific adaptation measure would be desirable for that purpose an approach for the decomposition of surface temperature values was recently developed hertel and schlink 2019 this technique might be extended to urban air temperatures in future research work both techniques attribution of temperature variability as well as of temperature increments could be useful tools for planners optimizing adaptation measures in general physical patterns tend to be non orthogonal hannachi 2007 a limitation of the eof analysis is the assumption of orthogonal patterns because these patterns might result from a superposition of different physical phenomena dommenget and latif 2002 on the other hand it might be possible that an individual eof does not represent the effect of a specific atmosphere land coupling parameter on the air temperature roundy 2014 for example in our study grass vegetation has a significant impact on both spatial temperature patterns table a1 7 software and technical notes for micrometeorological simulations the model envi met version 4 4 4 science license http www envi met com was used together with the visualization program leonardo input data are retrieved from a google earth image of helwan university campus from the weather underground database for cairo airport station latitude 30 7 19 n and longitude 31 24 20 e elevation 75 m wmo id 62366 and from the atmospheric soundings website at university of wyoming http weather uwyo edu upperair sounding html visualization conversion of envi met output data from binary format and the eof analysis were done with programs developed in r r core team 2015 https www r project org hierarchical partitioning was calculated using the hier part package walsh and mac nally 2020 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the work was partly made in frame of the helmholtz climate initiative hi cam that is funded by the helmholtz associations initiative and networking fund the authors are responsible for the content of this publication appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104773 
25984,wave attenuation is a key process that impacts activities at the coastal land margin and is an ecosystem service provided by many natural landscapes traditional modeling tools for wind wave attenuation require advanced expertise to apply we present an alternative gis based option for estimating wave attenuation the wave attenuation toolbox watte the outputs are a map of wave height transmission as a percentage of the original wave height and a line demarking the extent of wave exposure onshore watte supports a variety of inputs ranging from outputs of ecological and landscape evolution models to remote sensing data and past present and future conditions can be analyzed the present version of watte models wave attenuation as an exponential decay process and a recommendation table for exponential decay constants is derived from previous studies three examples of applying watte to marsh environments are described keywords wave attenuation gis toolbox coastal process marsh ecosystem services participatory modeling software availability name wave attenuation toolbox developer m foster martinez contact information above year 2020 software required arcmap 10 3 or later with spatial analyst extension optional features require the advanced license language python size 32 kb availability doi https doi org 10 31390 civil engineering data 01 gnu general public license v3 cost free 1 introduction wind waves largely determine the form and function of the coastal land margin about half the total energy for all natural coastal processes i e biological chemical and physical processes come from waves leigh et al 1987 short 2012 wave energy is dissipated in the nearshore primarily through breaking and encountering frictional elements natural coastal ecosystems and structures attenuate wave action including mangroves abuodha and kairo 2001 ilman et al 2016 coral reefs hughes et al 2018 seagrass beds guannel et al 2016 waycott et al 2009 marshes crosby et al 2016 k bromberg gedan et al 2009 gedan et al 2011 and oyster reefs wiberg et al 2018 anthropogenic activity has indirectly and directly altered the patterns of wave energy marine structures e g breakwaters sea walls revetments etc directly prevent wave penetration in some areas while enhancing wave reflection and increasing wave energy in others reeve et al 2018 land conversion for shoreline development and the effects of climate change have damaged and decreased the area of vegetated coastal ecosystems by as much as 50 since mid twentieth century duarte et al 2013 at the same time the use of natural and nature based features nnbfs for coastal protection is gaining recognition for being cost effective and providing multiple benefits bridges et al 2015 narayan et al 2016 sutton grier and sandifer 2018 as changes in the coastal land margin continue to occur it is important to understand and communicate the impacts on wave energy here we have created a geographic information systems gis toolbox that estimates and maps wave attenuation the present version of the toolbox is built for estimating wave attenuation through marshes however it could be readily adapted to other coastline types e g mangroves seagrass beds kelp forests the toolbox leverages existing datasets from a range of sources including remote sensing and coastal landscape evolution models allowing for analysis of past present and future conditions by using a simple algorithm the toolbox does not require advanced expertise to apply and remains accessible to a range of stakeholder groups e g local resource managers agencies and non profits the following sections provide background on wave attenuation and modeling tools and describe the toolbox algorithm we present three examples that illustrate potential uses of the toolbox followed by discussion and conclusions 2 background 2 1 wave attenuation through marsh vegetation processes measurement and modelling wave energy is a function of wave height i e the vertical distance between the crest and trough of a wave waves with larger wave heights have greater energy as waves move onshore many dissipative processes are at work such as wave breaking and friction due to bottom roughness but on healthy vegetated shorelines encountering vegetation is the primary energy dissipation mechanism under normal conditions i e non storm guannel et al 2015 i mller et al 1999 vegetation has the greatest impact on waves with shorter wave periods i e 1 min paquier et al 2016 van rooijen et al 2016 which include wind generated sea swell and infragravity waves munk 1950 the amount waves are attenuated across a marsh can vary greatly between sites or even at the same site under different conditions koch et al 2009 pinsky et al 2013 greater attenuation is observed when vegetation occupies a greater proportion of the water column which can be due to increased stem density width or height peruzzo et al 2018 that proportion can change over the course of a tidal cycle as the ratio of the water depth to vegetation height changes or between vegetation patches with varying characteristics e g stem density width and stiffness therefore a range of attenuation that can be expected at a site is more meaningful than one single value field measurements of wave attenuation through marshes measure wave heights at multiple points along a transect perpendicular to the shoreline e g jadhav and chen 2013 i mller and spencer 2002 with known locations of instruments the decrease in wave height per unit length along the transect can be calculated underlying this method is the assumption that wave refraction causes the waves to cross the marsh parallel to the shoreline komar 1998 the way wave attenuation is modeled depends on the application here we chose to model it as an exponential decay kobayashi et al 1993 1 w t h 1 h 0 e k x where w t is the fraction of wave height transmission h is wave height 1 and 0 subscripts denote the location closer to onshore and offshore respectively k is the exponential decay constant and x is the cross shore distance between h 1 and h 0 this model requires minimal input i e only k and describes field measurements well tempest et al 2015 and references therein there are multiple other hydrodynamic models that include the effects of vegetation with varying levels of complexity and applicable spatial scales large scale models typically represent vegetation as bottom friction and parameterize it via manning s n e g adcirc luettich et al 1992 this representation is appropriate for storm surge but is not as useful for wind waves where vegetation is more directly impactful models that do address wind wave vegetation interaction often model vegetation as a drag force as originally described by dalrymple et al 1984 examples include xbeach veg roelvink et al 2009 van rooijen et al 2015 swan veg simulating waves nearshore vegetation suzuki et al 2012 and the invest integrated valuation of ecosystem services and tradeoffs coastal protection model guannel et al 2015 these models require a drag coefficient for the vegetation which has been shown to vary with hydrodynamic conditions e g reynolds number mller et al 2014 and keulegan carpenter number jadhav and chen 2013 other required inputs include vegetation parameters e g stem count and wave parameters e g wave number and height most of these models operate on a mesh grid and need advanced expertise to run they can be run to simulate past present or future conditions e g hijuelos et al 2019 the gis toolbox presented in this paper is in no way intended to replace any of the aforementioned models but rather it is a simpler alternative that may be more suitable for some communities modeling and information needs 2 2 related tools for coastal planning method frameworks have been created specifically for the assessment of natural systems for coastal protection for example osorio cano et al 2017 describe a four stage process beginning with parameterization of the natural system and ending with a coastal management plan the framework includes an assessment of wave attenuation using advanced numerical models e g xbeach roelvink et al 2010 similarly van zanten et al 2014 present a framework that uses information on the wave attenuation capacity of coral reefs to calculate their coastal protection value for both frameworks a lack of site data or of model expertise could prevent a user group from completing the critical step of assessing wave attenuation capacity and progressing in the framework the coastal protection nearshore wave and erosion module of invest guannel et al 2015 tallis et al 2010 provides a framework and toolboxes that operate in a gis environment invest estimates the value of coastal protection services for a range of coastal environments e g doughty et al 2017 ruckelshaus et al 2016 by using the damage averted i e erosion to assess value it produces wave attenuation estimates along a single cross shore transect and requires detailed information such as cross shore bathymetry and wave parameters to run since its initial development invest has evolved and this module is no longer being maintained by developers however the broad use of the module demonstrates the overall need for this type of information 3 methods this paper describes the wave attenuation watte toolbox a custom arcgis toolbox that is run within arcmap arcgis 2018 the code is open source and available for download foster martinez and hagen 2020 the present version of watte estimates the decrease in wave height that occurs as waves cross though marsh vegetation using an exponential wave decay formulation i e eq 1 watte uses an image of the region of interest and simulates a process similar to how wave attenuation is measured in the field the output is an estimate of wave height transmission throughout the entire marsh 3 1 toolbox algorithm watte executes the following steps which are illustrated in fig 1 inputs the main inputs are a raster of the area of interest classified by land type fig 1a and exponential decay constants for each marsh classification 1 identify the marsh water interface each cell classification in the raster is converted to a polygon and the edges of the polygons are converted to polylines a new line the marsh water interface line is created where marsh and water polylines intersect black line fig 1b 2 draw cross shore transects transects polyline features are drawn perpendicular to the marsh water interface line extending in both onshore and offshore directions the transect length and spacing are user specified white lines fig 1b ferreira and cooley 2013 3 generate point features along each transect the onshore direction of the transect is identified by examining the classifications of the raster cells around the marsh water interface point features are generated along the onshore side at the user specified interval white dots fig 1c 4 calculate wave height transmission for each transect starting at the point closest to the marsh water interface the amount of wave height transmission is calculated at each point feature as follows 2 w t 1 w t 0 e k x where 1 and 0 subscripts denote the point on the transect closer to onshore and offshore respectively and x is the distance between the two points the classification of the onshore point location determines the k value used e g the yellow and green areas have different k values in fig 1 the information is passed down each point on the transect so that the previous w t 1 becomes w t 0 shaded dots fig 1d 5 repeat step 4 for each transect fig 1e 6 remove point clustering the ends of transects along a convex coast can become bunched together to reduce point clustering points closer than half the point spacing interval are compared in a pair wise fashion the point farther from the marsh water interface is removed this process continues until all points remaining are separated by at least half the point spacing interval this step is optional and is only performed if this user has the advanced arcgis license 7 interpolate the w t values of all points inverse distance weighting or kriging is used to interpolate between all points fig 1f the interpolation method is selected by the user the underlying equation eq 2 is one dimensional but by executing it at many points and interpolating between them the results appear two dimensional outputs the outputs are a raster of the percent wave height transmission and if the user has the advanced license of arcgis a polyline bounding the area of wave influence i e complete attenuation line orange line fig 1f each run of the watte toolbox simulates one set of conditions we recommend performing multiple runs altering the exponential decay constants each time to study a variety of conditions and bound the wave attenuation estimates 3 2 wave attenuation toolbox development 3 2 1 inputs the cells of the input raster must be classified as marsh water or other examples shown in figs 1a and 3 marsh is defined as vegetated areas within the intertidal zone and water is defined as areas within or below the intertidal zone that are void of vegetation other can be any non inundated landcover such as forest or upland that is elevated above the tidal prism at a minimum the raster must contain two classifications marsh and water but there can be unlimited classifications within the three categories of marsh water and other classifications could correspond to any characteristics such as different vegetation species or different levels of biomass productivity possible sources of input rasters include the national wetland inventory nwi data from remote sensing or the output of land evolution models e g marsh migration models like hydro mem alizad et al 2016a and slamm park et al 1986 nwi maps and output from marsh migration models have the advantage of already being classified data from remote sensing needs to be processed to classify each raster cell and there are existing methods for doing so e g farris et al 2019 ozesmi and bauer 2002 the raster is not required to be a digital elevation model dem or to contain any elevation information for the current version of watte each marsh classification must be assigned an exponential decay constant k which is input by the user ideally k should be calculated from measurements of wave attenuation at the site or an area with similar characteristics if limited site information is available the guidance table described in section 3 3 can be used to set the values although the transects are generated within the toolbox the user must specify the length of the transects the distance between them and the spacing of points along the transect fig 2 shows a screenshot of the watte user interface where this information is specified by the user the spacing of the transects and of the points should be determined by the site characteristics and the resolution of the raster if a site has many different marsh classifications closer spacing is needed as compared to a site with only one or two marsh classifications one point per raster cell is generally recommended to be conservative the total length of the transect should be longer than what is expected for complete attenuation for example if observations at the site indicate wave action dies off about 100 m into the marsh then the transects should be at least 120 m 20 greater the wave calculation step 4 will continue moving inland along the points on the transect until one of the following occurs wave transmission w t 1 eq 1 is below the user defined threshold the transect point is on a non marsh raster cell or the full length of the transect is reached in the example shown in fig 1 the wave transmission threshold is 3 in other words once the wave height is attenuated by 97 the wave is considered completely attenuated and further decrease in wave height is not calculated there is no information on bathymetry or topography within watte and all areas classified as marsh are possible areas for wave propagation this simplification implies the water level scenario should be sufficient to fully inundate the marsh so the waves could theoretically propagate throughout the marsh 3 2 2 outputs the watte outputs are a raster of wave height transmission i e the percent of original wave height and the complete attenuation line this line demarks the extent of wave exposure onshore if the wave transmission reaches the threshold the wave is considered completely attenuated and the complete attenuation line is generated any marsh farther onshore is not considered to be influenced by wave action the absence of this line in an area indicates it is within wave exposure if the user does not have the advanced arcgis license the complete wave attenuation line must be generated manually using the watte output raster instructions for manually producing this line are included with the code the output raster values are 100 at the marsh water interface and decrease moving inland this percentage of wave height transmission can then be multiplied by an initial wave height to show the attenuation in terms of wave height while two locations may have the same transmission percentage they may not experience the same wave energy due to their location within the area shores exposed to greater fetch are likely to receive larger waves as compared to more sheltered areas the transmission calculation does not consider whether the area is exposed or sheltered 3 2 3 assumptions and uncertainty within watte it is assumed that wave height transformation through vegetation can be modeled as an exponential decay addressed in section 2 1 and that waves are parallel to the marsh water interface the second assumption is common in attenuation field measurements when the measurement locations can be selected to better ensure it is true i e locations where the bed slope does not vary greatly in the cross shore direction here we extend it to all points along the marsh water interface which includes coastlines with small channels wave attenuation results in these areas should be interpreted cautiously due to limited fetch it is unlikely that narrow channels contain sizable waves however flow in marsh channels does tend to be perpendicular to the marsh edge temmerman et al 2012 which supports applying this assumption watte has three main types of uncertainty initial conditions model and parameter dietze 2017 initial condition uncertainty refers to how accurately the input raster represents conditions on the ground this uncertainty will vary greatly if the input is based on past or current conditions versus projected future conditions the model uncertainty is set by the formulation chosen for watte exponential decay this model was chosen in part because it minimizes the number of parameters thereby constraining the parameter uncertainty we focus on parameter uncertainty for the remainder of the document because it comes from the choice of the exponential decay constant k which users can manipulate easily 3 3 selection of exponential decay constants as described in section 3 2 1 an exponential decay constant is required for each marsh classification the following guidance table is provided to aid with this selection when wave attenuation measurements or observations are not available table 1 since k is the only parameter in the exponential decay model it contains all of the complexity of vegetation wave interactions that cause differences in wave attenuation for this recommendation table we distilled this complexity to two metrics biomass and inundation both are often reported as influencing wave attenuation capacity shepard et al 2011 within each of these two metrics there are three classes low medium and high creating a total of nine values of k biomass is influenced by a range of factors the low medium and high biomass categories capture differences due to physical biological and geochemical processes for example seasonal shifts schoutens et al 2019 differing hydroperiods morris et al 2002 and subsurface characteristics wilson et al 2015 have all been shown to create differences in biomass productivity and are represented by this metric the inundation metric does not refer to the hydroperiod but rather to the water depth at which the model will be run low inundation is when the water depth is low relative to the height of the vegetation and the vegetation remains emergent at the other end high inundation is when the vegetation is deeply submerged ratios of vegetation height to water depth h w a t e r h v e g have been given for each category to guide the selection to populate table 1 we compiled measured values of k from field studies of species within the spartina genus due to limited data availability all species within the spartina genus are grouped together while there are many non spartina species in marshes there is more available spartina data because this vegetation is common in the low marsh and interacts with waves laboratory studies were excluded because by design they often do not capture all of the physical processes at work the available studies were sorted as containing low medium or high biomass density the decay constants in each biomass category were informed by three studies cited in table 1 web plot digitizer rohatgi 2018 was used to extract data from published figures the results from each study were organized by increasing water depth to vegetation height ratio if this information was not provided the ratio was calculated from the given vegetation height water depth and or site slope even if the given values were averages the measured values of k were averaged in each of the nine categories to provide the recommendations there is an upper limit of 4 for the ratio of water depth to vegetation height there are limited measurements beyond this depth as the impact of vegetation diminishes when deeply submerged the k values provided span a wide range using the values for medium biomass the distance of marsh needed to reach 50 attenuation i e half the wave height at the shoreline varies from 13 m to 116 m for low and high inundation conditions respectively table 1 is not required to run watte the user can input any value for the exponential decay constant we recommend using location specific data if it is available regardless of the source of the values we suggest performing multiple runs using the upper and lower bounds of probable values for example the combination of low productivity and high inundation gives the lowest amount of wave attenuation the most conservative result 4 examples of watte application 4 1 example 1 validation of exponential decay constant selection the performance of watte results heavily depends on the selection of the exponential decay constants in an effort to validate the provided guidance table the values of k for the following example were selected from table 1 and were based on a description of vegetation and measurement conditions alone the results are then compared to the measured wave attenuation at the site morgan et al 2009 examined differences in functions between meadow and fringing salt marshes in maine and new hampshire us we use their measurements of wave attenuation at the mousam river site where the low marsh is dominated by spartina alterniflora these measurements were not used to inform the values in table 1 making it an independent validation to apply watte we acquired an aerial image from 2009 of the study area taken by the national agriculture imagery program naip the image was classified with a k means approach duda and canty 2002 using earth resources data analysis system erdas imagine software erdas imagine 2018 by visual inspection we determined that classifying the image with five categories accurately distinguished water marsh and non marsh the five classes included two classes that were judged to be water two classes of marsh and one class of non marsh fig 3 we selected two sets of k values to bound the wave attenuation estimate informed by descriptions of the vegetation and of the site given in morgan et al 2009 the biomass level of the two marsh classes was varied and the inundation level was kept the same table 2 we estimated the marsh class along the perimeter of the area to be higher productivity than the marsh class more common in the interior for the wave height transmission calculation points were generated every 1 m along each transect due to the naip image resolution of 1 m the transects were set to be 100 m long and to occur every 10 m along the marsh water interface the average of three wave attenuation measurements were provided in morgan et al 2009 the results from watte were extracted at the measurement location for comparison the measured wave height decreased from an average of 10 9 cm at the shore to 8 4 cm at a point 5 m from the marsh edge giving a wave height transmission of 77 morgan et al 2009 using watte the lower estimate of wave height transmission higher attenuation was 73 and the upper estimate was 84 accurately bounding the measured value this result demonstrates the utility of table 1 as well as the necessity of running multiple cases to provide bounds on the results note this case is over a short distance of 5 m the estimated distance needed to decay to a wave height of 5 cm about 46 of original wave height is 12 m for the upper bound and 21 m for the lower bound 4 2 example 2 application to grand bay usa watte can be applied to large areas of coast in this example watte was used to analyze 274 km of marsh water interface in the grand bay estuary which spans mississippi and alabama in the us the input raster is the output from hydro mem alizad et al 2018 hydro mem is a marsh migration model it couples adcirc a hydrodynamic model with the marsh equilibrium model mem a biological model to project maps of marsh productivity alizad et al 2016a 2016b the hydro mem maps show five area classifications water upland which is land elevated above the tidal prism and low medium and high productivity marsh exponential decay constants were selected to simulate conditions at mean higher high water mhhw and were based on vegetation and elevation measurements taken to run hydro mem the results show that complete attenuation is reached about 70 m into the vegetated marsh fig 4 this high rate of attenuation can be attributed to the high productivity vegetation along the marsh water interface however many small islands or marsh fragments are less than 70 m wide and are completely subjected to wave action the wave action varies throughout the estuary with more exposed areas having a longer fetch and receiving larger waves the marsh in the western portion is more protected than the eastern side and the waves will differ even if the wave transmission percentage is the same if typical wave heights are known for the different areas the percentages of wave transmission can be converted to wave heights by multiplying them by the known wave height or wave statistic i e root mean square or significant wave height at the shoreline note these results have not been validated with in situ measurements but rather are provided to show that watte can be applied at larger scales stakeholders can gain more information from the hydro mem results by using them in watte 4 3 example 3 projections with future sea level rise there is a large effort in the scientific community to understand and predict marsh change with sea level rise slr and other future environmental conditions passeri et al 2015 schuerch et al 2018 and references therein as marshes change the associated ecosystem services also change applying watte to these future projections allows for better understanding of how the ecosystem service of wave attenuation will change in the future hydro mem is one example of a model that projects marsh migration with slr we applied watte to hydro mem results from intermediate high and high rates of slr in an area of grand bay alabama alizad et al 2018 the complete attenuation line is shown for years 2010 fig 5 a no slr 2050 fig 5b intermediate high slr fig 5c high slr and 2100 fig 5d high slr the rate of slr impacts the marsh migration pattern and therefore the wave attenuation pattern with intermediate high slr in 2050 the marsh retains high productivity but the width of marsh narrows for high slr in 2050 and 2100 the marsh is largely composed of low productivity biomass however it increases in width and the inland portions of marsh are not as exposed to wave action the areas where the upland is exposed to wave action indicated by white arrows in fig 5 are more erosion prone and could be areas of targeted intervention fig 5d shows the marsh edge and complete attenuation line for 2010 overlaid on the high slr results for 2100 the marsh retreat is apparent as well as the widening of the area influenced by waves hydro mem results alizad et al 2018 show that ponding occurs with high slr in 2050 fig 5c the edges of these ponds are considered part of the marsh water interface and are treated the same as the marsh water interface at the shoreline although waves being generated in interior ponds are likely small previous studies have shown that wind generated shear stresses can lead to pond expansion and therefore pond edges are included in watte day et al 2011 5 discussion 5 1 evaluation of watte as a participatory modeling tool by focusing on only the process of wave attenuation through marsh vegetation watte can be used within a variety of assessment methods and modeling frameworks its simplicity makes it suitable for participatory modeling where stakeholders are formally engaged in problem identification and analysis voinov et al 2018 due to the opportunity to learn and revise through process iteration participatory modeling is well suited for addressing complex environmental management decisions stave 2010 such as coastal protection plans and restoration activities tool selection should be tailored for the particular project and stakeholders as it is an important step in a successful application of the participatory modeling process voinov and bousquet 2010 voinov et al 2018 compared 17 types of participatory modeling tools grouped by four modeling methods gis was categorized as a quantitative modeling method with aggregated results compared to other tools the greatest strength of gis was the ability to represent spatial results as could be expected it was also judged to be strong in ease of communication and modification gis tended to be weak in areas where many tools were weak such as in handling uncertainty and representing temporal results voinov et al 2018 we address the lack of uncertainty by recommending users run watte multiple times to bound the results temporal results can be achieved if data throughout landscape changes is available watte input the time scale would then be that of the land evolution changes we evaluated watte based on the eight criteria detailed in bagstad et al 2013 table 3 these criteria were developed to evaluate the usability of decision support tools for quantification of ecosystem services and have been used to evaluate tools similar to watte overall watte performs well as it is publicly available and provides quantitative results it can be applied to a wide range of spatial scales increasing the scale simply increases the run time since it requires arcgis software we judged it to be moderate cost it does not have a valuation component and it would therefore need to be used in conjunction with another tool in order to make economic assessments see barbier et al 2013 for an example we expect watte to be useful for a range of groups performing research at the coastal land margin and interested in assessing patterns of wave attenuation by coastal vegetation including academic institutions industry community groups and non profits e g the nature conservancy climate central 5 2 strengths and limitations a core strength of watte is that it can provide reasonable estimates of wave attenuation with limited information as illustrated in example 1 section 4 1 the input raster can be derived from remote sensing data which is more available for coasts across the globe as compared to in situ measurements remote sensing is a valuable resource for work in under studied locations and is gaining recognition for its usefulness in wetland restoration and management ganju 2019 the exponential decay constants can be sourced from measurements in a different area with similar conditions and they can be varied in multiple runs of watte to test bounding cases having only one parameter to manipulate simplifies this process of course the more that is known about a site the more can be done with the watte results for example converting from percentages of wave transmission to wave heights requires an understanding of typical wave conditions throughout the site it should be noted that users must be careful not to extrapolate results to extreme conditions e g storm surge unless data from these conditions was used to calculate the exponential decay constants the benefit of flexibility also comes at the price of not being mechanistic in the measurements of exponential decay constants all processes are bundled together for example if the stem density changes that parameter cannot be adjusted independently the way it can be in a drag model the same is true for changes in slope which impact shoaling and bed friction processes instead of being directly adjusted the effect of these changes on the exponential decay constant first needs to be judged similarly not requiring a dem or any elevation information removes a barrier to usage but it creates additional factors to consider in the interpretation of the results as stated in section 3 2 1 wave propagation can occur in any area classified as marsh and cannot occur in areas classified as other that is why it is important to run a scenario where the marsh is fully inundated or to designate non inundated marsh areas as other if running a low inundation scenario unrealistic wave propagation could occur where the marsh is dry this result is likely not common because high attenuation occurs at low inundation and waves are often completely attenuated before reaching higher elevations another limitation is that each transect is independent there is no interaction between waves along different transects even if those transects overlap when points along different transects are clustered together the points closer to the marsh water interface are preferentially kept in the interpolation to produce a conservative estimate of attenuation step 6 in section 3 1 5 3 future applications wave attenuation is just one of many ecosystem services performed by marshes coastal projects incorporating marshes as nnbfs are widely promoted bridges et al 2015 shepard et al 2011 sutton grier et al 2018 yet to properly evaluate nnbfs against other options formal recognition of the ecosystem services is necessary sutton grier et al 2015 watte can quantify the service of wave attenuation to provide a more complete picture future iterations of watte can expand applicable shoreline types the exponential decay model is appropriate for some for example pinsky et al 2013 calculated exponential decay constants for previous studies of wave attenuation through kelp mangroves and seagrasses and lacy and macvean 2016 found wave attenuation over mudflats follows exponential decay with decay constants that are inversely related to depth squared however it is likely other models will need to be added to watte to address all shorelines of interest since the code is open source we are hopeful others will modify and improve it to suit particular project needs 6 conclusions the accelerating rate of sea level rise creates additional impetus to study how coastlines will change and to communicate the findings to a broader audience it is important to communicate not only how marshes will change but also how the associated ecosystem services will change some of which may have direct impacts on the lives of coastal residents here we focus on wave attenuation by marsh vegetation and have presented an arcgis toolbox watte to estimate wave height transmission along any given marsh shoreline while vegetation wave interaction is highly complex there is a general consensus that wave heights exponentially decay as they pass through or over vegetation tempest et al 2015 and references therein we model wave attenuation as an exponential decay to keep the process and its application simple watte can be applied to create location specific estimates without extensive field measurements or advanced numerical models the results from watte add information to existing datasets enhancing their value these datasets may be sourced from observations collected previously current data or projections of future conditions datasets can also be manipulated to reflect anticipated changes resulting from coastal projects and the results can help evaluate these projects estimating their impact on wave energy at the site this information can be used to identify priority areas for conservation and restoration declaration of competing interest none acknowledgements this work was supported by awards no na10nos4780146 and na16nos4780208 from the national oceanic and atmospheric administration noaa ecological effects of sea level rise eeslr program and the louisiana sea grant laborde chair the authors would like to acknowledge dr denise delorme for assistance with the literature review as well as w m lauve for classifying the naip imagery the manuscript significantly benefited from three anonymous reviews the statements and conclusions do not necessarily reflect the views of noaa eeslr louisiana sea grant louisiana state university university of south carolina or their affiliates appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104788 
25984,wave attenuation is a key process that impacts activities at the coastal land margin and is an ecosystem service provided by many natural landscapes traditional modeling tools for wind wave attenuation require advanced expertise to apply we present an alternative gis based option for estimating wave attenuation the wave attenuation toolbox watte the outputs are a map of wave height transmission as a percentage of the original wave height and a line demarking the extent of wave exposure onshore watte supports a variety of inputs ranging from outputs of ecological and landscape evolution models to remote sensing data and past present and future conditions can be analyzed the present version of watte models wave attenuation as an exponential decay process and a recommendation table for exponential decay constants is derived from previous studies three examples of applying watte to marsh environments are described keywords wave attenuation gis toolbox coastal process marsh ecosystem services participatory modeling software availability name wave attenuation toolbox developer m foster martinez contact information above year 2020 software required arcmap 10 3 or later with spatial analyst extension optional features require the advanced license language python size 32 kb availability doi https doi org 10 31390 civil engineering data 01 gnu general public license v3 cost free 1 introduction wind waves largely determine the form and function of the coastal land margin about half the total energy for all natural coastal processes i e biological chemical and physical processes come from waves leigh et al 1987 short 2012 wave energy is dissipated in the nearshore primarily through breaking and encountering frictional elements natural coastal ecosystems and structures attenuate wave action including mangroves abuodha and kairo 2001 ilman et al 2016 coral reefs hughes et al 2018 seagrass beds guannel et al 2016 waycott et al 2009 marshes crosby et al 2016 k bromberg gedan et al 2009 gedan et al 2011 and oyster reefs wiberg et al 2018 anthropogenic activity has indirectly and directly altered the patterns of wave energy marine structures e g breakwaters sea walls revetments etc directly prevent wave penetration in some areas while enhancing wave reflection and increasing wave energy in others reeve et al 2018 land conversion for shoreline development and the effects of climate change have damaged and decreased the area of vegetated coastal ecosystems by as much as 50 since mid twentieth century duarte et al 2013 at the same time the use of natural and nature based features nnbfs for coastal protection is gaining recognition for being cost effective and providing multiple benefits bridges et al 2015 narayan et al 2016 sutton grier and sandifer 2018 as changes in the coastal land margin continue to occur it is important to understand and communicate the impacts on wave energy here we have created a geographic information systems gis toolbox that estimates and maps wave attenuation the present version of the toolbox is built for estimating wave attenuation through marshes however it could be readily adapted to other coastline types e g mangroves seagrass beds kelp forests the toolbox leverages existing datasets from a range of sources including remote sensing and coastal landscape evolution models allowing for analysis of past present and future conditions by using a simple algorithm the toolbox does not require advanced expertise to apply and remains accessible to a range of stakeholder groups e g local resource managers agencies and non profits the following sections provide background on wave attenuation and modeling tools and describe the toolbox algorithm we present three examples that illustrate potential uses of the toolbox followed by discussion and conclusions 2 background 2 1 wave attenuation through marsh vegetation processes measurement and modelling wave energy is a function of wave height i e the vertical distance between the crest and trough of a wave waves with larger wave heights have greater energy as waves move onshore many dissipative processes are at work such as wave breaking and friction due to bottom roughness but on healthy vegetated shorelines encountering vegetation is the primary energy dissipation mechanism under normal conditions i e non storm guannel et al 2015 i mller et al 1999 vegetation has the greatest impact on waves with shorter wave periods i e 1 min paquier et al 2016 van rooijen et al 2016 which include wind generated sea swell and infragravity waves munk 1950 the amount waves are attenuated across a marsh can vary greatly between sites or even at the same site under different conditions koch et al 2009 pinsky et al 2013 greater attenuation is observed when vegetation occupies a greater proportion of the water column which can be due to increased stem density width or height peruzzo et al 2018 that proportion can change over the course of a tidal cycle as the ratio of the water depth to vegetation height changes or between vegetation patches with varying characteristics e g stem density width and stiffness therefore a range of attenuation that can be expected at a site is more meaningful than one single value field measurements of wave attenuation through marshes measure wave heights at multiple points along a transect perpendicular to the shoreline e g jadhav and chen 2013 i mller and spencer 2002 with known locations of instruments the decrease in wave height per unit length along the transect can be calculated underlying this method is the assumption that wave refraction causes the waves to cross the marsh parallel to the shoreline komar 1998 the way wave attenuation is modeled depends on the application here we chose to model it as an exponential decay kobayashi et al 1993 1 w t h 1 h 0 e k x where w t is the fraction of wave height transmission h is wave height 1 and 0 subscripts denote the location closer to onshore and offshore respectively k is the exponential decay constant and x is the cross shore distance between h 1 and h 0 this model requires minimal input i e only k and describes field measurements well tempest et al 2015 and references therein there are multiple other hydrodynamic models that include the effects of vegetation with varying levels of complexity and applicable spatial scales large scale models typically represent vegetation as bottom friction and parameterize it via manning s n e g adcirc luettich et al 1992 this representation is appropriate for storm surge but is not as useful for wind waves where vegetation is more directly impactful models that do address wind wave vegetation interaction often model vegetation as a drag force as originally described by dalrymple et al 1984 examples include xbeach veg roelvink et al 2009 van rooijen et al 2015 swan veg simulating waves nearshore vegetation suzuki et al 2012 and the invest integrated valuation of ecosystem services and tradeoffs coastal protection model guannel et al 2015 these models require a drag coefficient for the vegetation which has been shown to vary with hydrodynamic conditions e g reynolds number mller et al 2014 and keulegan carpenter number jadhav and chen 2013 other required inputs include vegetation parameters e g stem count and wave parameters e g wave number and height most of these models operate on a mesh grid and need advanced expertise to run they can be run to simulate past present or future conditions e g hijuelos et al 2019 the gis toolbox presented in this paper is in no way intended to replace any of the aforementioned models but rather it is a simpler alternative that may be more suitable for some communities modeling and information needs 2 2 related tools for coastal planning method frameworks have been created specifically for the assessment of natural systems for coastal protection for example osorio cano et al 2017 describe a four stage process beginning with parameterization of the natural system and ending with a coastal management plan the framework includes an assessment of wave attenuation using advanced numerical models e g xbeach roelvink et al 2010 similarly van zanten et al 2014 present a framework that uses information on the wave attenuation capacity of coral reefs to calculate their coastal protection value for both frameworks a lack of site data or of model expertise could prevent a user group from completing the critical step of assessing wave attenuation capacity and progressing in the framework the coastal protection nearshore wave and erosion module of invest guannel et al 2015 tallis et al 2010 provides a framework and toolboxes that operate in a gis environment invest estimates the value of coastal protection services for a range of coastal environments e g doughty et al 2017 ruckelshaus et al 2016 by using the damage averted i e erosion to assess value it produces wave attenuation estimates along a single cross shore transect and requires detailed information such as cross shore bathymetry and wave parameters to run since its initial development invest has evolved and this module is no longer being maintained by developers however the broad use of the module demonstrates the overall need for this type of information 3 methods this paper describes the wave attenuation watte toolbox a custom arcgis toolbox that is run within arcmap arcgis 2018 the code is open source and available for download foster martinez and hagen 2020 the present version of watte estimates the decrease in wave height that occurs as waves cross though marsh vegetation using an exponential wave decay formulation i e eq 1 watte uses an image of the region of interest and simulates a process similar to how wave attenuation is measured in the field the output is an estimate of wave height transmission throughout the entire marsh 3 1 toolbox algorithm watte executes the following steps which are illustrated in fig 1 inputs the main inputs are a raster of the area of interest classified by land type fig 1a and exponential decay constants for each marsh classification 1 identify the marsh water interface each cell classification in the raster is converted to a polygon and the edges of the polygons are converted to polylines a new line the marsh water interface line is created where marsh and water polylines intersect black line fig 1b 2 draw cross shore transects transects polyline features are drawn perpendicular to the marsh water interface line extending in both onshore and offshore directions the transect length and spacing are user specified white lines fig 1b ferreira and cooley 2013 3 generate point features along each transect the onshore direction of the transect is identified by examining the classifications of the raster cells around the marsh water interface point features are generated along the onshore side at the user specified interval white dots fig 1c 4 calculate wave height transmission for each transect starting at the point closest to the marsh water interface the amount of wave height transmission is calculated at each point feature as follows 2 w t 1 w t 0 e k x where 1 and 0 subscripts denote the point on the transect closer to onshore and offshore respectively and x is the distance between the two points the classification of the onshore point location determines the k value used e g the yellow and green areas have different k values in fig 1 the information is passed down each point on the transect so that the previous w t 1 becomes w t 0 shaded dots fig 1d 5 repeat step 4 for each transect fig 1e 6 remove point clustering the ends of transects along a convex coast can become bunched together to reduce point clustering points closer than half the point spacing interval are compared in a pair wise fashion the point farther from the marsh water interface is removed this process continues until all points remaining are separated by at least half the point spacing interval this step is optional and is only performed if this user has the advanced arcgis license 7 interpolate the w t values of all points inverse distance weighting or kriging is used to interpolate between all points fig 1f the interpolation method is selected by the user the underlying equation eq 2 is one dimensional but by executing it at many points and interpolating between them the results appear two dimensional outputs the outputs are a raster of the percent wave height transmission and if the user has the advanced license of arcgis a polyline bounding the area of wave influence i e complete attenuation line orange line fig 1f each run of the watte toolbox simulates one set of conditions we recommend performing multiple runs altering the exponential decay constants each time to study a variety of conditions and bound the wave attenuation estimates 3 2 wave attenuation toolbox development 3 2 1 inputs the cells of the input raster must be classified as marsh water or other examples shown in figs 1a and 3 marsh is defined as vegetated areas within the intertidal zone and water is defined as areas within or below the intertidal zone that are void of vegetation other can be any non inundated landcover such as forest or upland that is elevated above the tidal prism at a minimum the raster must contain two classifications marsh and water but there can be unlimited classifications within the three categories of marsh water and other classifications could correspond to any characteristics such as different vegetation species or different levels of biomass productivity possible sources of input rasters include the national wetland inventory nwi data from remote sensing or the output of land evolution models e g marsh migration models like hydro mem alizad et al 2016a and slamm park et al 1986 nwi maps and output from marsh migration models have the advantage of already being classified data from remote sensing needs to be processed to classify each raster cell and there are existing methods for doing so e g farris et al 2019 ozesmi and bauer 2002 the raster is not required to be a digital elevation model dem or to contain any elevation information for the current version of watte each marsh classification must be assigned an exponential decay constant k which is input by the user ideally k should be calculated from measurements of wave attenuation at the site or an area with similar characteristics if limited site information is available the guidance table described in section 3 3 can be used to set the values although the transects are generated within the toolbox the user must specify the length of the transects the distance between them and the spacing of points along the transect fig 2 shows a screenshot of the watte user interface where this information is specified by the user the spacing of the transects and of the points should be determined by the site characteristics and the resolution of the raster if a site has many different marsh classifications closer spacing is needed as compared to a site with only one or two marsh classifications one point per raster cell is generally recommended to be conservative the total length of the transect should be longer than what is expected for complete attenuation for example if observations at the site indicate wave action dies off about 100 m into the marsh then the transects should be at least 120 m 20 greater the wave calculation step 4 will continue moving inland along the points on the transect until one of the following occurs wave transmission w t 1 eq 1 is below the user defined threshold the transect point is on a non marsh raster cell or the full length of the transect is reached in the example shown in fig 1 the wave transmission threshold is 3 in other words once the wave height is attenuated by 97 the wave is considered completely attenuated and further decrease in wave height is not calculated there is no information on bathymetry or topography within watte and all areas classified as marsh are possible areas for wave propagation this simplification implies the water level scenario should be sufficient to fully inundate the marsh so the waves could theoretically propagate throughout the marsh 3 2 2 outputs the watte outputs are a raster of wave height transmission i e the percent of original wave height and the complete attenuation line this line demarks the extent of wave exposure onshore if the wave transmission reaches the threshold the wave is considered completely attenuated and the complete attenuation line is generated any marsh farther onshore is not considered to be influenced by wave action the absence of this line in an area indicates it is within wave exposure if the user does not have the advanced arcgis license the complete wave attenuation line must be generated manually using the watte output raster instructions for manually producing this line are included with the code the output raster values are 100 at the marsh water interface and decrease moving inland this percentage of wave height transmission can then be multiplied by an initial wave height to show the attenuation in terms of wave height while two locations may have the same transmission percentage they may not experience the same wave energy due to their location within the area shores exposed to greater fetch are likely to receive larger waves as compared to more sheltered areas the transmission calculation does not consider whether the area is exposed or sheltered 3 2 3 assumptions and uncertainty within watte it is assumed that wave height transformation through vegetation can be modeled as an exponential decay addressed in section 2 1 and that waves are parallel to the marsh water interface the second assumption is common in attenuation field measurements when the measurement locations can be selected to better ensure it is true i e locations where the bed slope does not vary greatly in the cross shore direction here we extend it to all points along the marsh water interface which includes coastlines with small channels wave attenuation results in these areas should be interpreted cautiously due to limited fetch it is unlikely that narrow channels contain sizable waves however flow in marsh channels does tend to be perpendicular to the marsh edge temmerman et al 2012 which supports applying this assumption watte has three main types of uncertainty initial conditions model and parameter dietze 2017 initial condition uncertainty refers to how accurately the input raster represents conditions on the ground this uncertainty will vary greatly if the input is based on past or current conditions versus projected future conditions the model uncertainty is set by the formulation chosen for watte exponential decay this model was chosen in part because it minimizes the number of parameters thereby constraining the parameter uncertainty we focus on parameter uncertainty for the remainder of the document because it comes from the choice of the exponential decay constant k which users can manipulate easily 3 3 selection of exponential decay constants as described in section 3 2 1 an exponential decay constant is required for each marsh classification the following guidance table is provided to aid with this selection when wave attenuation measurements or observations are not available table 1 since k is the only parameter in the exponential decay model it contains all of the complexity of vegetation wave interactions that cause differences in wave attenuation for this recommendation table we distilled this complexity to two metrics biomass and inundation both are often reported as influencing wave attenuation capacity shepard et al 2011 within each of these two metrics there are three classes low medium and high creating a total of nine values of k biomass is influenced by a range of factors the low medium and high biomass categories capture differences due to physical biological and geochemical processes for example seasonal shifts schoutens et al 2019 differing hydroperiods morris et al 2002 and subsurface characteristics wilson et al 2015 have all been shown to create differences in biomass productivity and are represented by this metric the inundation metric does not refer to the hydroperiod but rather to the water depth at which the model will be run low inundation is when the water depth is low relative to the height of the vegetation and the vegetation remains emergent at the other end high inundation is when the vegetation is deeply submerged ratios of vegetation height to water depth h w a t e r h v e g have been given for each category to guide the selection to populate table 1 we compiled measured values of k from field studies of species within the spartina genus due to limited data availability all species within the spartina genus are grouped together while there are many non spartina species in marshes there is more available spartina data because this vegetation is common in the low marsh and interacts with waves laboratory studies were excluded because by design they often do not capture all of the physical processes at work the available studies were sorted as containing low medium or high biomass density the decay constants in each biomass category were informed by three studies cited in table 1 web plot digitizer rohatgi 2018 was used to extract data from published figures the results from each study were organized by increasing water depth to vegetation height ratio if this information was not provided the ratio was calculated from the given vegetation height water depth and or site slope even if the given values were averages the measured values of k were averaged in each of the nine categories to provide the recommendations there is an upper limit of 4 for the ratio of water depth to vegetation height there are limited measurements beyond this depth as the impact of vegetation diminishes when deeply submerged the k values provided span a wide range using the values for medium biomass the distance of marsh needed to reach 50 attenuation i e half the wave height at the shoreline varies from 13 m to 116 m for low and high inundation conditions respectively table 1 is not required to run watte the user can input any value for the exponential decay constant we recommend using location specific data if it is available regardless of the source of the values we suggest performing multiple runs using the upper and lower bounds of probable values for example the combination of low productivity and high inundation gives the lowest amount of wave attenuation the most conservative result 4 examples of watte application 4 1 example 1 validation of exponential decay constant selection the performance of watte results heavily depends on the selection of the exponential decay constants in an effort to validate the provided guidance table the values of k for the following example were selected from table 1 and were based on a description of vegetation and measurement conditions alone the results are then compared to the measured wave attenuation at the site morgan et al 2009 examined differences in functions between meadow and fringing salt marshes in maine and new hampshire us we use their measurements of wave attenuation at the mousam river site where the low marsh is dominated by spartina alterniflora these measurements were not used to inform the values in table 1 making it an independent validation to apply watte we acquired an aerial image from 2009 of the study area taken by the national agriculture imagery program naip the image was classified with a k means approach duda and canty 2002 using earth resources data analysis system erdas imagine software erdas imagine 2018 by visual inspection we determined that classifying the image with five categories accurately distinguished water marsh and non marsh the five classes included two classes that were judged to be water two classes of marsh and one class of non marsh fig 3 we selected two sets of k values to bound the wave attenuation estimate informed by descriptions of the vegetation and of the site given in morgan et al 2009 the biomass level of the two marsh classes was varied and the inundation level was kept the same table 2 we estimated the marsh class along the perimeter of the area to be higher productivity than the marsh class more common in the interior for the wave height transmission calculation points were generated every 1 m along each transect due to the naip image resolution of 1 m the transects were set to be 100 m long and to occur every 10 m along the marsh water interface the average of three wave attenuation measurements were provided in morgan et al 2009 the results from watte were extracted at the measurement location for comparison the measured wave height decreased from an average of 10 9 cm at the shore to 8 4 cm at a point 5 m from the marsh edge giving a wave height transmission of 77 morgan et al 2009 using watte the lower estimate of wave height transmission higher attenuation was 73 and the upper estimate was 84 accurately bounding the measured value this result demonstrates the utility of table 1 as well as the necessity of running multiple cases to provide bounds on the results note this case is over a short distance of 5 m the estimated distance needed to decay to a wave height of 5 cm about 46 of original wave height is 12 m for the upper bound and 21 m for the lower bound 4 2 example 2 application to grand bay usa watte can be applied to large areas of coast in this example watte was used to analyze 274 km of marsh water interface in the grand bay estuary which spans mississippi and alabama in the us the input raster is the output from hydro mem alizad et al 2018 hydro mem is a marsh migration model it couples adcirc a hydrodynamic model with the marsh equilibrium model mem a biological model to project maps of marsh productivity alizad et al 2016a 2016b the hydro mem maps show five area classifications water upland which is land elevated above the tidal prism and low medium and high productivity marsh exponential decay constants were selected to simulate conditions at mean higher high water mhhw and were based on vegetation and elevation measurements taken to run hydro mem the results show that complete attenuation is reached about 70 m into the vegetated marsh fig 4 this high rate of attenuation can be attributed to the high productivity vegetation along the marsh water interface however many small islands or marsh fragments are less than 70 m wide and are completely subjected to wave action the wave action varies throughout the estuary with more exposed areas having a longer fetch and receiving larger waves the marsh in the western portion is more protected than the eastern side and the waves will differ even if the wave transmission percentage is the same if typical wave heights are known for the different areas the percentages of wave transmission can be converted to wave heights by multiplying them by the known wave height or wave statistic i e root mean square or significant wave height at the shoreline note these results have not been validated with in situ measurements but rather are provided to show that watte can be applied at larger scales stakeholders can gain more information from the hydro mem results by using them in watte 4 3 example 3 projections with future sea level rise there is a large effort in the scientific community to understand and predict marsh change with sea level rise slr and other future environmental conditions passeri et al 2015 schuerch et al 2018 and references therein as marshes change the associated ecosystem services also change applying watte to these future projections allows for better understanding of how the ecosystem service of wave attenuation will change in the future hydro mem is one example of a model that projects marsh migration with slr we applied watte to hydro mem results from intermediate high and high rates of slr in an area of grand bay alabama alizad et al 2018 the complete attenuation line is shown for years 2010 fig 5 a no slr 2050 fig 5b intermediate high slr fig 5c high slr and 2100 fig 5d high slr the rate of slr impacts the marsh migration pattern and therefore the wave attenuation pattern with intermediate high slr in 2050 the marsh retains high productivity but the width of marsh narrows for high slr in 2050 and 2100 the marsh is largely composed of low productivity biomass however it increases in width and the inland portions of marsh are not as exposed to wave action the areas where the upland is exposed to wave action indicated by white arrows in fig 5 are more erosion prone and could be areas of targeted intervention fig 5d shows the marsh edge and complete attenuation line for 2010 overlaid on the high slr results for 2100 the marsh retreat is apparent as well as the widening of the area influenced by waves hydro mem results alizad et al 2018 show that ponding occurs with high slr in 2050 fig 5c the edges of these ponds are considered part of the marsh water interface and are treated the same as the marsh water interface at the shoreline although waves being generated in interior ponds are likely small previous studies have shown that wind generated shear stresses can lead to pond expansion and therefore pond edges are included in watte day et al 2011 5 discussion 5 1 evaluation of watte as a participatory modeling tool by focusing on only the process of wave attenuation through marsh vegetation watte can be used within a variety of assessment methods and modeling frameworks its simplicity makes it suitable for participatory modeling where stakeholders are formally engaged in problem identification and analysis voinov et al 2018 due to the opportunity to learn and revise through process iteration participatory modeling is well suited for addressing complex environmental management decisions stave 2010 such as coastal protection plans and restoration activities tool selection should be tailored for the particular project and stakeholders as it is an important step in a successful application of the participatory modeling process voinov and bousquet 2010 voinov et al 2018 compared 17 types of participatory modeling tools grouped by four modeling methods gis was categorized as a quantitative modeling method with aggregated results compared to other tools the greatest strength of gis was the ability to represent spatial results as could be expected it was also judged to be strong in ease of communication and modification gis tended to be weak in areas where many tools were weak such as in handling uncertainty and representing temporal results voinov et al 2018 we address the lack of uncertainty by recommending users run watte multiple times to bound the results temporal results can be achieved if data throughout landscape changes is available watte input the time scale would then be that of the land evolution changes we evaluated watte based on the eight criteria detailed in bagstad et al 2013 table 3 these criteria were developed to evaluate the usability of decision support tools for quantification of ecosystem services and have been used to evaluate tools similar to watte overall watte performs well as it is publicly available and provides quantitative results it can be applied to a wide range of spatial scales increasing the scale simply increases the run time since it requires arcgis software we judged it to be moderate cost it does not have a valuation component and it would therefore need to be used in conjunction with another tool in order to make economic assessments see barbier et al 2013 for an example we expect watte to be useful for a range of groups performing research at the coastal land margin and interested in assessing patterns of wave attenuation by coastal vegetation including academic institutions industry community groups and non profits e g the nature conservancy climate central 5 2 strengths and limitations a core strength of watte is that it can provide reasonable estimates of wave attenuation with limited information as illustrated in example 1 section 4 1 the input raster can be derived from remote sensing data which is more available for coasts across the globe as compared to in situ measurements remote sensing is a valuable resource for work in under studied locations and is gaining recognition for its usefulness in wetland restoration and management ganju 2019 the exponential decay constants can be sourced from measurements in a different area with similar conditions and they can be varied in multiple runs of watte to test bounding cases having only one parameter to manipulate simplifies this process of course the more that is known about a site the more can be done with the watte results for example converting from percentages of wave transmission to wave heights requires an understanding of typical wave conditions throughout the site it should be noted that users must be careful not to extrapolate results to extreme conditions e g storm surge unless data from these conditions was used to calculate the exponential decay constants the benefit of flexibility also comes at the price of not being mechanistic in the measurements of exponential decay constants all processes are bundled together for example if the stem density changes that parameter cannot be adjusted independently the way it can be in a drag model the same is true for changes in slope which impact shoaling and bed friction processes instead of being directly adjusted the effect of these changes on the exponential decay constant first needs to be judged similarly not requiring a dem or any elevation information removes a barrier to usage but it creates additional factors to consider in the interpretation of the results as stated in section 3 2 1 wave propagation can occur in any area classified as marsh and cannot occur in areas classified as other that is why it is important to run a scenario where the marsh is fully inundated or to designate non inundated marsh areas as other if running a low inundation scenario unrealistic wave propagation could occur where the marsh is dry this result is likely not common because high attenuation occurs at low inundation and waves are often completely attenuated before reaching higher elevations another limitation is that each transect is independent there is no interaction between waves along different transects even if those transects overlap when points along different transects are clustered together the points closer to the marsh water interface are preferentially kept in the interpolation to produce a conservative estimate of attenuation step 6 in section 3 1 5 3 future applications wave attenuation is just one of many ecosystem services performed by marshes coastal projects incorporating marshes as nnbfs are widely promoted bridges et al 2015 shepard et al 2011 sutton grier et al 2018 yet to properly evaluate nnbfs against other options formal recognition of the ecosystem services is necessary sutton grier et al 2015 watte can quantify the service of wave attenuation to provide a more complete picture future iterations of watte can expand applicable shoreline types the exponential decay model is appropriate for some for example pinsky et al 2013 calculated exponential decay constants for previous studies of wave attenuation through kelp mangroves and seagrasses and lacy and macvean 2016 found wave attenuation over mudflats follows exponential decay with decay constants that are inversely related to depth squared however it is likely other models will need to be added to watte to address all shorelines of interest since the code is open source we are hopeful others will modify and improve it to suit particular project needs 6 conclusions the accelerating rate of sea level rise creates additional impetus to study how coastlines will change and to communicate the findings to a broader audience it is important to communicate not only how marshes will change but also how the associated ecosystem services will change some of which may have direct impacts on the lives of coastal residents here we focus on wave attenuation by marsh vegetation and have presented an arcgis toolbox watte to estimate wave height transmission along any given marsh shoreline while vegetation wave interaction is highly complex there is a general consensus that wave heights exponentially decay as they pass through or over vegetation tempest et al 2015 and references therein we model wave attenuation as an exponential decay to keep the process and its application simple watte can be applied to create location specific estimates without extensive field measurements or advanced numerical models the results from watte add information to existing datasets enhancing their value these datasets may be sourced from observations collected previously current data or projections of future conditions datasets can also be manipulated to reflect anticipated changes resulting from coastal projects and the results can help evaluate these projects estimating their impact on wave energy at the site this information can be used to identify priority areas for conservation and restoration declaration of competing interest none acknowledgements this work was supported by awards no na10nos4780146 and na16nos4780208 from the national oceanic and atmospheric administration noaa ecological effects of sea level rise eeslr program and the louisiana sea grant laborde chair the authors would like to acknowledge dr denise delorme for assistance with the literature review as well as w m lauve for classifying the naip imagery the manuscript significantly benefited from three anonymous reviews the statements and conclusions do not necessarily reflect the views of noaa eeslr louisiana sea grant louisiana state university university of south carolina or their affiliates appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104788 
