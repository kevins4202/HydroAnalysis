index,text
435,richards equation for variably saturated flow can exhibit stability problems due to its nonlinearity the challenge is to resolve sharp wetting fronts without introducing spurious oscillations especially for simulations with very dry initial conditions flow instabilities at sharp wetting fronts may arise particularly in anisotropic and heterogeneous media leading to oscillations or convergence problems the focus of this work is to evaluate and minimize instability problems for simulations with unstructured meshes to this end numerical experiments were performed to investigate the accuracy monotonicity and convergence behavior of numerical solutions for variably saturated flow based on different control volume methods piecewise gradient reconstruction methods and flux approximation methods in particular nonphysical oscillations at the wetting front were investigated a novel multi point flux approximation with multi point upstream weighting based on piecewise gradient reconstruction was developed numerical simulations in both homogeneous and heterogeneous domains with isotropic and anisotropic conductivity tensors demonstrate that the proposed multi point flux approximation with multi point upstream weighting avoids spurious oscillations and improves the results for challenging sharp wetting front problems using general unstructured meshes as well as meshes with distortion the revised voronoi dual control volume method has also been found to provide more flexibility than commonly used center dual and median dual control volume methods keywords multi point flux approximation mpfa multi point upstream weighting mpups variably saturated flow unstructured mesh piecewise gradient reconstruction 1 introduction richards equation has been widely used for the simulation of water flow in unsaturated porous media driven by the actions of gravity and capillarity due to the nonlinearity of richards equation numerical solution of the variably saturated flow problem remains computationally expensive time consuming and in certain circumstances can lead to poor numerical stability and performance zha et al 2019 numerical methods may fail to produce reliable convergence behavior especially for higher order methods farthing and ogden 2017 the numerical accuracy and convergence of solutions for richards equation depend on numerous factors in particular the spatial discretization of the computational mesh which can be characterized through a characteristic length scale and a measure of the aspect ratio or distortion farthing and ogden 2017 the most accurate method is to discretize the solution domain using a structured mesh or orthogonal mesh without distortion however this is not always possible or expensive to do for complex simulation domains in such cases unstructured meshes are beneficial because they can more easily replicate irregular geometries and provide local refinements stability problems including spurious oscillations and convergence problems may be triggered by various aspects such as permeability contrasts in heterogeneous domains anisotropy and mesh effects in unstructured discretization christou et al 2019 spatial discretization based on a structured mesh is straightforward in that the elements of the mesh e g edge face are parallel or perpendicular to the cartesian axes taking the finite volume method as an example the flux over the control volume interface can be easily calculated based on two point flux approximation tpfa lunde 2007 however tpfa is only correct for grids that are orthogonal with respect to the permeability tensor k so called k orthogonal grids aavatsmark i 2007 for non k orthogonal grids tpfa produces an error in the solution that does not vanish as the grids are refined aavatsmark i 2007 for a general unstructured mesh the conventional tpfa method does not provide sufficient accuracy especially for simulations with anisotropic porous media the tpfa method will not converge because of distorted grid cells which often exist in grids honoring physical permeability contrasts eigestad and klausen 2005 special treatment such as local global nonlinear tpfa techniques should be used to account for anisotropy effects chen et al 2008 møyner and lie 2014 nikitin et al 2014 instead of applying the tpfa method a more general approach is to use multi point flux approximation mpfa techniques aavatsmark et al 1998a aavatsmark et al 1998b aavatsmark 2007 eigestad klausen 2005 aavatsmark et al 2008 edwards zheng 2010 zha et al 2013a when mpfa methods are applied to unstructured meshes some conditions need to be satisfied to avoid oscillations and nonphysical solutions for a delaunay grid and isotropic permeability it can be guaranteed that the transmissibility coefficient matrix computed by a mpfa discretization is a monotonic matrix m matrix however for anisotropic conditions the transmissibility coefficients are not necessarily positive and will therefore not generally lead to a m matrix mlacnik and durlofsky 2006 monotonicity is a sufficient condition to ensure that nonphysical local maxima and minima are not produced in the discrete nonlinear unsaturated flow equation forsyth and kropinski 1997 to ensure monotonicity the positive transmissibility condition is not sufficient and upstream weighting must be applied forsyth kropinski 1997 however the standard upstream mpfa cannot always satisfy the positive transmissibility condition that ensures monotonicity of the unstructured discretization younes et al 2013 the main aim of this work is to investigate the development of instabilities for variably saturated flow problems with emphasis on challenging sharp wetting fronts using a general purpose mpfa scheme a series of numerical factors such as spatial weighting piecewise gradient reconstruction method dual control volume method and edge face based flux calculation methods are investigated a revised voronoi dual control volume method and multi point upstream mpups weighting are proposed numerical tests demonstrate that the mpfa together with mpups is capable of producing stable solutions for variably saturated flow in porous media with a high level of heterogeneity and anisotropy the proposed method has also been applied to distorted meshes for which conventional methods are unable to produce convergent solutions or accurate results the combination of these numerical approaches and their adaptability to distorted meshes in the context of variably saturated flow is unique and has not been investigated previously the main outcome of this study is the identification of the most promising approach for solving challenging sharp wetting front problems for variably saturated flow using unstructured meshes the paper is organized as follows section 2 is devoted to the description of the variably saturated flow model and its discretization for different cell types gradient reconstruction and spatial weighting schemes in section 3 the versatility and validation of the different numerical methods are studied for variably saturated flow in heterogeneous and anisotropic domains in section 4 the monotonicity and convergence behavior of the different numerical methods are described in section 5 the adaptability of the proposed method to distorted meshes is investigated finally a discussion and conclusions are provided in sections 6 and 7 2 variably saturated flow model and numerical discretization in this section the governing equations for variably saturated flow are summarized followed by an outline of the methods used for spatial discretization gradient reconstruction and multi point upstream weighting 2 1 governing equation for variably saturated flow water flow in variably saturated media is here simulated based on richards equation richards 1931 which was derived from earlier work by richardson 1922 this equation and modified forms have been widely used to simulate water flow in variably saturated porous media in the past four decades huyakorn et al 1984 huyakorn et al 1986 clement et al 1994 paniconi 1992 huang et al 1996 therrien and sudicky 1996 vogel et al 2000 mayer et al 2002 kollet and maxwell 2006 trefry and muffels 2007 using hydraulic head as the primary dependent variable the mass conservation equation can be written as 1 s a s s h t ϕ s a t k r a k h q a where sa l3 water l 3 void is the water saturation ss l 1 is the specific storage coefficient h l is the hydraulic head t t is time φ is porosity kra is the relative permeability k l t 1 is the hydraulic conductivity tensor and qa m l 3 t 1 is a source sink term the saturation and relative permeability are a function of pressure based on the relationships given by wösten and van genuchten 1988 2 s a s r a 1 s r a 1 α ψ a n m 3 k r a s e a l 1 1 s e a 1 m m 2 where sra defines the residual saturation α n m and l are the soil hydraulic function parameters with m defined by 4 m 1 1 n ψ a l is the pressure head which can be obtained from 5 ψ a h z where z l defines the elevation with respect to a given datum sea is the effective saturation and is given by 6 s e a s a s r a 1 s r a 2 2 numerical discretization of the node centered mpfa scheme analytical solutions of richards equation exist only for simplified cases and in many practical situations flexible numerical solution approaches are required farthing and ogden 2017 among the majority of richards equation solvers the finite volume method is one of the most popular methods due to its natural ability to provide local mass conservation depending on the spatial location of solution values finite volume methods can be classified into cell centered and node centered also known as vertex centered methods diskin et al 2009 zha et al 2013b given the same mesh the solutions of cell centered methods are assumed to be located at the centroid of the cell and the solutions of node centered method are assumed to be located at the nodes of the mesh in this work we focus on the node centered approach in a 2d cartesian coordinate system for the node centered approach a dual mesh which represents the control volume of each node must be created three different dual mesh methods are evaluated in this work the first dual mesh method is referred as the median dual method blazek 2015 which joins the midpoint of each edge of the mesh with the center of the cell the second dual mesh method is referred as the voronoi dual method or containment dual method blazek 2015 hu and nicolaides 1992 in which case the segment that joins the two adjacent cell centers is perpendicular to their common edge and crosses this edge at the midpoint the third dual mesh method is referred to as the center dual method which joins the centers of adjacent cells illustrations of the three dual mesh methods for a triangular mesh are shown in figs 1 a c most mesh generation software cannot generate obtuse free meshes in this case the general voronoi dual control volume method cannot be applied since the circumcenter of a cell is outside of the cell here we implement a revised voronoi dual control volume method to allow for the use of meshes with obtuse cells so that users do not need to modify meshes the midpoint of the largest edge is treated as the cell center to build the dual mesh as shown in fig 1 b the center dual and median dual control volume methods work for a triangular mesh with obtuse cells without redefining the cell center however in a strongly distorted mesh the center dual mesh method may not work because the segment that joins two adjacent cell centers may not cross the shared edge depending on the selected interface center point representing the gradient of the control volume interface the flux calculation can be face based when point p is selected or edge based when point q is selected as shown in fig 1 d for the defined control volume vi enclosed by its boundary γ i the governing eq 1 can be rewritten in the integral conservative form by applying the gauss divergence theorem 7 s a s s t v i hdv ϕ t v i s a dv γ i k ra k h n d γ v j q a dv where n is the unit outward normal of the control volume interface by applying fully implicit time weighting eq 7 can be rewritten as 8 s a i n 1 s s h i n 1 h i n δ t v i ϕ i s a i n 1 s a i n δ t v i l γ i q a l n 1 n l δ s l q a i n 1 v i where the subscript i defines the ith control volume l represents the lth face of control volume vi δt is the time increment n 1 defines the new time level and n represents the old time level δsl l in 2d and l2 in 3d is the area of the lth face of control volume vi and q a l n 1 l t 1 is the water flux through the lth face of the control volume which can be expressed as 9 q a l n 1 k r a l n 1 k l h n 1 where k r a l n 1 is the relative permeability coefficient of the lth face of the control volume at the new time level which can be calculated based on the relative permeability coefficients of adjacent nodes using central weighting harmonic weighting or upstream weighting methods eq 1 is nonlinear because the saturation and the relative permeability are a function of fluid pressure wösten and van genuchten 1988 the hydraulic head gradient h n 1 applies to the lth face of a control volume at the new time level compared to the discretization based on a structured mesh eq 9 needs special treatment while the other terms in eq 8 remain the same or are similar for both structured and unstructured meshes substituting eq 9 into the first term on the right hand side of eq 8 yields the flux through the lth face of the control volume during the time increment δt 10 q a l n 1 n l δ s l k r a l n 1 k l h n 1 n l δ s l since k r a l n 1 and δsl are all scalars that can be calculated based on the provided simulation parameters and geometric features of the mesh we only need to focus on the calculation of the term k l h n 1 n l for the different meshes without loss of generality this term can be rewritten as 11 k l h n l h k l t n l h w l where w l k l t n l as shown in fig 1 d w l can be further decomposed to 12 w l w l u l u l w l n l n l where u l is the unit vector of w l projecting on the lth face of the control volume and n l is the unit outward normal of the lth face of the control volume because k l is a symmetric diagonally dominant matrix it is positive definite and w l is on the same side as the unit outward normal vector n l similarly the edge ij can be written as 13 v l v l u l u l v l n l n l rearranging eqs 12 and 13 we obtain 14 u l w l w l n l n l w l w l n l n l 15 n l v l v l u l u l v l n l substituting eqs 12 14 and 15 into eq 11 yields 16 k l h n l w l n l v l n l h v l p r i m a r y t e r m w l u l w l n l v l u l v l n l h u l s e c o n d a r y t e r m in eq 16 w l v l n l and u l are the physical and geometric parameters that can be calculated based on the provided mesh and material properties the flux across the lth face of the control volume consists of a primary term and a secondary term also known as cross diffusion term for k orthogonal meshes e g structured meshes both w l u l and v l u l equal zero and the secondary term can be ignored in the primary term w l n l v l n l is the transmissibility coefficient also referred to as the influence coefficient in some literature here we provide the approximation of h v l as shown in fig 1 d let p denote the location of center point p of the lth interface of the control volume δx be the vector from point p to node i and δx be the vector from point p to node j the m th order taylor expansion of function yields 17 h p δ x k 0 m 1 k δ x p k h p p p r m 1 where the remainder r has the lagrangian form which can be expressed as 18 r 1 m 1 δ x p k h p p p we have 19 h v l h p δ x p δ x h p δ x h p δ x h j h i ε ji where ε ji is the m th order taylor expansion remainder which can be expressed as 20 ε j i k 2 m 1 k δ x p k h p δ x p k h p p p it can be noted that the high order taylor expansion remainder term ε ji cancels out for a structured mesh when an edge based flux approximation is used for either unstructured mesh or face based flux approximations this term does not cancel out for a good quality mesh it is possible to assume ε ji 0 however this may not hold for a mesh with large expansion factors or radius ratios substituting eq 19 into eq 16 yields 21 k l h n l w l n l v l n l h j h i ε j i w l u l w l n l v l u l v l n l h p u l 2 3 gradient reconstruction and multi point flux approximation several different mpfa methods have been developed in the last two decades including mpfa o aavatsmark et al 1998a aavatsmark et al 1998b eigestad klausen 2005 mpfa o η edwards rogers 1998 mpfa u aavatsmark et al 1998a aavatsmark et al 1998b mpfa l aavatsmark et al 2008 and other enhanced mpfa methods chen et al 2008 edwards zheng 2010 moog 2013 the major difference between the various mpfa methods is the choice of the grid points that are used to calculate the gradient for flux estimation at the control volume interface for the mpfa o method all grid points located in the interaction region are included in the flux approximation while for the mpfa u and mpfa l methods only the grid points adjacent to the interface and specific neighbors are considered for rough grids with large aspect ratios and strong anisotropy ratios convergence may be poor for the mpfa o method while the mpfa l method has generally a larger monotonicity range and continues to provide convergence although at a reduced rate klausen winther 2006 aavatsmark et al 2008 keilegavlen aavatsmark 2011 mohammadnia et al 2017 additional details of the gradient reconstruction methods are provided in the supplementary material s 1 to s 3 for k orthogonal meshes without considering second and higher order taylor remainders eq 21 is reduced to k l h n l w l n l v l n l h j h i a general form of the tpfa method for other meshes the mth order gradient of h is required for eqs 20 and 21 in the present study we focus on the calculation of the second and higher order gradients of h at the center point p of the lth face of the control volume in a 2d coordinate system for the piecewise linear gradient reconstruction we use green gauss and least squares methods and for the piecewise quadratic gradient reconstruction we use higher order least squares methods it is worth mentioning that the proposed piecewise gradient reconstruction satisfies flux continuity as well as potential continuity hydraulic head is linear over the control volume for green gauss or least squares gradient reconstruction methods but quadratic for high order least squares gradient reconstruction methods 2 4 multi point upstream weighting in a fully implicit scheme simple upstream weighting can be obtained if the flux at the control volume interface is calculated as an explicit function of the fluid potential in the neighboring cells aavatsmark eigestad 2006 however for nonlinear unsaturated flow in an unstructured mesh the standard mpfa approach using upstream weighting cannot always satisfy the positive transmissibility condition which ensures monotonicity causing strong nonphysical oscillations near sharp wetting fronts younes et al 2013 monotonicity is here provided when the actual upstream point i e point with higher hydraulic head coincides with flux direction during flux calculation the calculated flux direction should always be from the location with higher hydraulic head to location with lower hydraulic head generally speaking central weighting requires lower numerical effort since there is no need to identify the upstream point while upstream weighting is better suited to deal with discontinuities blazek 2015 for second or higher order upstream weighting methods limiters have to be employed in order to prevent the development of spurious oscillations near strong discontinuities blazek 2015 water fluxes are usually treated with two point upstream weighting and it is well known that when simulating processes with adverse mobility ratios this method suffers from grid orientation effects keilegavlen et al 2012 to account for the mesh effects a novel multi point upstream mpups weighting with multi point flux approximation has been implemented in this research substituting eq 21 is into eq 10 and ignoring the timestep notation we obtain 22 q a l n l δ s l k r a l k l h n l δ s l k r a l a δ s l w l n l v l n l h j h i ε j i k r a l b δ s l w l u l w l n l v l u l v l n l h p u l where k r a l a and k r a l b are the relative permeabilities for the primary and secondary terms at the control volume interface for the calculation of the primary term conventional upstream weighting is applied however the secondary term may have a flux contribution with different direction compared to the primary term when the mpfa method is used especially in the wetting front zone where sharp gradients exist the proposed multi point upstream mpups weighting is based on the sign of the secondary term as shown in fig 1 d when the secondary term is positive the cross term flux is from node j to node i and node j is selected as the upstream point similarly node i is selected as the upstream point when the secondary term is negative as shown below 23 k r a l a k r a j i f w l n l v l n l h j h i ε j i 0 k r a l a k r a i i f w l n l v l n l h j h i ε j i 0 and 24 k r a l b k r a j i f w l u l w l n l v l u l v l n l h p u l 0 k r a l b k r a i i f w l u l w l n l v l u l v l n l h p u l 0 by using mpups the primary and secondary terms may have different upstream points depending on the directions of the flux contributions this rule guarantees monotonicity for all flux components across control volume interfaces 2 5 adaptive time stepping and update modification schemes the numerical difficulties in simulating variably saturated flow with sharp wetting infiltration fronts mainly arise from the nonlinearity of the head based form richards equation krabbenhøft 2007 zha et al 2017 zha et al 2019 an adaptive time stepping scheme and a modified newton s method with an underrelaxation scheme is included in the present model to ensure a reliable and robust solution of the nonlinear equations mayer et al 2002 mayer and macquarrie 2010 the scheme uses upstream weighting for the relative permeability term which leads to improved convergence behavior for sharp infiltration fronts forsyth et al 1995 in addition the code includes an option to limit the update of the head values at each newton iteration to avoid overshoot issues and guide convergence towards the correct solution this scheme has proven to be effective and has allowed to solve unsaturated flow problems in steady state mode which is a difficult problem since the initial conditions can be far from the solution to achieve this performance special treatment for low moisture contents was not required for transient solutions the timesteps are determined adaptively based on a projected maximum grid wide saturation change derived from saturation changes that occurred during the previous time step this method avoids timestep increases that can cause convergence problems 3 numerical experiments in this section a series of numerical tests are presented to investigate the performance of the proposed mpfa scheme this scheme has been implemented in the well established code min3p a process based numerical model designed for the investigation of subsurface fluid flow and reactive transport in variably saturated media mayer et al 2002 henderson et al 2009 bea et al 2012 bea et al 2018 su et al 2017 mayer and macquarrie 2010 we use results obtained from a structured mesh as the reference solution which is justified by previous verification of the code in addition the structured mesh is k orthogonal with a relatively small error resulting from numerical discretization even though an underrelaxation scheme is included in the code this feature was not used in all the tested cases the performance of the proposed approaches were analyzed considering a wide range of conditions and numerical methods including heterogeneity anisotropy control volume methods gradient reconstruction methods flux approximation methods and spatial weighting methods as shown in table 1 detailed information on the numerical approaches used for all simulations can be found in the supplementary material s 4 boundary conditions are imposed strongly for all numerical experiments the edge based discretization is replaced by an algebraic equation such as u j u b 0 where u j is a solution value at the boundary node j and u b is a value specified by the boundary condition in this case no boundary flux quadrature is required as outlined by nishikawa 2015a to provide a quantitative error analysis for the results obtained from unstructured mesh and structured mesh simulations the structural similarity ssim index has been determined and applied facilitating the error comparison for results with equivalent mesh resolution wang et al 2004 in this context the results obtained using the structured mesh are referred to as the reference solution the ssim index has a range of 0 to 1 a value of 1 implies that both solutions are identical and a value of 0 indicates no structural similarity for this analysis a sliding gaussian window of size 11 11 is displaced pixel by pixel on the image of the spatial results wang et al 2004 3 1 case i water table mounding in isotropic soil in this case the proposed methods are used to simulate variably saturated flow based on a laboratory experiment vauclin et al 1979 clement et al 1994 the problem domain is 6 00 m x 2 00 m with an initial horizontal water table located at a height of 0 65 m due to symmetry the simulated portion of the domain is 3 00 m x 2 00 m with no flow boundaries on the bottom and on the left side at the surface of the domain a constant flux 3 55 m day is applied over a width of 0 5 m on the right side of the domain a constant head boundary is applied up to an elevation of 0 65 m the remaining surface and the right side of the domain above the water table are defined as no flow boundaries observation point p1 is located at 0 3 m from the left boundary and 0 2 m below the top surface and observation point p2 is located close to the water table 0 8 m from the left boundary and 0 8 m below the top surface the simulation domain is shown in fig 2 the soil properties used in the model are homogeneous and isotropic with a saturated hydraulic conductivity of 8 40 m day a porosity of 0 30 and a residual water content of 0 01 the values of the van genuchten soil parameters are α 3 3 m 1 n 4 1 and specific storage is ignored clement et al 1994 vauclin et al 1979 three meshes with different resolutions were used for both structured and unstructured grids the parameters for spatial discretization time stepping and newton iteration settings are summarized in table 2 a comparison of transient water table positions produced by the current model using the structured grid mesh1 with results obtained by clement et al 1994 using the same spatial discretization and the experimental data collected by vauclin et al 1979 is shown in fig 2 the results indicate that there is good agreement between the numerical models and the experimental data as discussed above the lack of monotonicity in unsaturated flow can cause nonphysical oscillations near sharp wetting fronts these oscillations can have a dramatic effect on the convergence behavior of the numerical model we are therefore most interested in comparing results within the wetting front zone where sharp gradients exist and oscillation may occur water saturations after a period of 8 hours obtained using the various numerical schemes and mesh resolutions are generally in good agreement with each other however for center dual and median dual control volume methods oscillations are observed at the wetting front for both central weighting and multi point upstream weighting as shown in figs 3 a e for the revised voronoi dual control volume method the simulation results indicate excellent agreement with results obtained from the structured mesh as shown in figs 3 g i in this case there is no significant difference between central weighting and multi point upstream weighting for center dual and median dual control volume methods increased mesh resolution can improve numerical results as indicated by better ssim values however it cannot completely avoid oscillations similar results are obtained for the scenarios using other gradient reconstruction methods as well as mpfa edge based and tpfa edge based flux approximations additional details on the performance of the various methods can be found in the supporting material s 5 3 2 case ii water table mounding in anisotropic soil in this section case i is modified to demonstrate the capability of the gradient based mpfa method for variably saturated flow in porous media with anisotropic material properties the physical parameters spatial discretization parameters of newton iterations initial conditions and boundary conditions remain the same as those used in case i except that the horizontal hydraulic conductivity was modified from 8 4 m day to 84 0 m day while the vertical hydraulic conductivity remained unchanged contour plots of water saturations for case ii after a period of 8 hours using the various numerical schemes are shown in fig 4 results obtained using different control volume methods gradient reconstruction methods and face or edge based flux approximations are similar and thus not included in this figure for this case the spatial distributions of water saturations do not show nonphysical oscillations for any of the mpfa methods however for the tpfa method significant nonphysical oscillations are observed and the results lose accuracy compared to mpfa and structured grid methods this case indicates that the mpfa methods are preferred over the tpfa method for applications to unstructured meshes with anisotropic material properties the transient evolution of water saturations at the observation points can be found in the supporting material s 6 3 3 case iii water infiltration into a heterogeneous dry soil this test case considers water infiltration into a very dry heterogeneous soil with initial pressure of 880 0 kpa forsyth kropinski 1997 the spatial domain is 8 0 m x 6 5 m and consists of four zones with different soil properties as shown in fig 5 all boundaries are impermeable except the infiltration zone at the top left the observation point p1 is located at 2 0 m from the left boundary and 0 2 m below the top surface and observation point p2 is located 0 4 m below p1 the soil properties used in the model are shown in table 3 three meshes with different resolutions and local refinement in the sharp wetting front zone are used for both structured and unstructured grids respectively the parameters for spatial discretization time stepping and newton iteration settings are summarized in table 4 as shown in fig 6 for the median dual control volume method with mpfa flux approximation significant oscillations are observed at the wetting front for both central and upstream weighting for the revised voronoi dual control volume method water saturations are oscillation free and the results are in good agreement with those obtained from the structured mesh similar results were obtained for the two point flux approximation method edge based flux approximation and other gradient reconstruction methods the results obtained from the center dual control volume method are similar to those of the median dual control volume method and are therefore not shown in fig 6 this example indicates that for heterogeneous material with very dry initial conditions although the material is isotropic the median dual and center dual control volume methods are more likely to experience oscillation problems while the revised voronoi dual control volume method can prevent oscillations the transient evolution of water saturation at the observation points for this case can be found in the supporting material s 7 3 4 case iv water infiltration into a heterogeneous and anisotropic dry soil for case iv the physical parameters and boundary conditions are the same as those used in case iii except that the vertical hydraulic conductivities k z in all four zones were reduced by one order of magnitude with reduced k z the wetting front is expected to be much sharper causing conventional central weighting and two point upstream weighting schemes to experience convergence difficulties to compensate for the reduced hydraulic conductivity the initial pressure was changed to 88 0 kpa so that simulation results from conventional methods can be compared against the proposed methods two unstructured meshes with resolutions of 0 1 m mesh 1 and 0 025 m mesh 2 were used to analyze the effect of mesh resolution on the accuracy of the results the number of nodes for the unstructured meshes are 6955 and 110847 respectively for completeness the simulation results with reduced k z but subject to the same initial conditions can be found in the supplementary material s 8 the spatial distributions of water saturations after a period of 30 days are shown in fig 7 as expected the tpfa method does not work well for anisotropic media as shown in fig 7 c oscillations at the wetting front are pronounced for central weighting especially for median dual control volume methods as shown in fig 7 d compared to results obtained using the structured mesh and results obtained with the voronoi dual control volume method shown in figs 7 g and i similar oscillations were observed for the center dual control volume method increasing the mesh resolution does not help to prevent oscillations with central weighting in addition simulations with central weighting using median dual or center dual control volume methods experience convergence problems while the simulation using the voronoi dual control volume method has no difficulty with convergence for mpups weighting similar results were obtained using different control volume methods as shown in figs 7 e and h with the refinement of mesh resolution simulations using voronoi dual median dual or center dual control volume methods generate similar results when mpups weighting is used generally the spatial distribution of water saturations obtained using the voronoi dual control volume method produces better accuracy than the median dual control volume method as shown in figs 7 f and j compared to fig 7 b it can also be observed that oscillations at the wetting front can be improved and more accurate results can be obtained for both central weighting and mpups weighting using the voronoi dual control volume method additional details on the performance of the various methods can be found in the supporting material s 8 4 monotonicity and convergence both anisotropy and heterogeneity increase the complexity of simulating flow in variably saturated media simulations performed for cases i to iv show that hydraulic conductivity variations with direction are more likely to cause oscillations and convergence problems than hydraulic conductivity variations with location particularly near sharp wetting fronts in the simulations with isotropic conductivity tensors despite the presence of heterogeneity the results are similar for the various numerical methods including different control volume methods center dual median dual voronoi dual different gradient reconstruction methods green gauss least squares and high order least squares different flux approximation methods tpfa mpfa edge face based and different spatial weighting schemes central upstream mpups however for simulations with anisotropic conductivity tensors the various methods exhibit quite different performance affecting the quality of the simulation results in this section monotonicity and convergence behavior are analyzed for simulations involving both isotropic and anisotropic material properties without loss of generality the numerical experiments with anisotropic material properties case ii are used here the monotonicity and convergence behavior of case ii are analyzed based on a mesh with average nodal spacing 0 05 m mesh1 with a total of 126 simulation cases analyzed for each gradient reconstruction method 12 simulations using different control volume methods flux approximation methods and spatial weighting schemes were carried out all simulations converged well except those using the fourth order least squares method the failure of the fourth order gradient reconstruction is attributed to nonlinearity introduced when additional neighboring nodes are used simultaneously it is notable that the higher order taylor expansion remainder does not always enhance the convergence for center dual and median dual control volume methods the third order least squares method with higher order remainder requires more iterations for the mpfa face based than edge based method as shown in figs 8 a and b for the center dual method and the third order least squares method with higher order remainder the average number of nonlinear iterations is 26285 for mpfa face based method compared to 7587 for mpfa edge based method similarly for the median dual method this value is 35047 for the mpfa face based method and 8710 for the mpfa edge based method the voronoi dual method required the least number of iterations where the average number of nonlinear iterations is 11082 for mpfa face based method and 5929 for mpfa edge based method we use relative global mass balance error to evaluate the accuracy of the proposed methods the global mass balance error is calculated based on the influx across the model boundary the corresponding outflux across the model boundary and the change in storage within the simulation domain the relative global mass balance error of simulations using higher order taylor expansion is also significantly higher than the error of other methods compared to the center dual and median dual control volume methods the relative global mass balance error based on the revised voronoi dual control volume method is slightly smaller and it takes fewer iterations and less total runtime even though the tpfa method meets the positive transmissibility condition we find that it is not sufficient to ensure monotonicity for the nonlinear unsaturated flow with anisotropic material properties for the median dual and center dual control volume methods there is no significant difference between face based and edge based mpfa methods in the number of nonlinear iterations time steps global mass balance error and total runtime however for the revised voronoi dual control volume method the edge based flux approximation method generally yields better performance than the face based flux approximation method for gradient reconstruction with a higher order taylor expansion remainder as shown in figs 8 c e the third order least squares method with higher order taylor expansion remainder generates much larger errors and a lower similarity index than the other gradient reconstruction methods when the face based method is used indicating that the higher order taylor expansion remainder does not work well with the face based mpfa method the relative global mass balance error obtained with and without higher order taylor expansion remainder for the third order least squares method is almost identical when the edge based flux approximation method is used implying that the the higher order taylor expansion remainder does not reduce the numerical error significantly in this case in this case compared to the piecewise linear gradient reconstruction using green gauss and least squares methods the piecewise quadratic gradient reconstruction using second order or the third order least squares yield smaller global mass balance errors and better similarity indices as shown in figs 8 c and d compared to the median dual and center dual control volume methods the revised voronoi dual control volume method is the most robust method which requires fewer linear and nonlinear iterations it should also be noted that the ssim index may produce misleading results when mpups weighting is used together with the face based mpfa method and third order least squares with higher order taylor expansion remainder as shown in figs 8 c and d the global mass balance error is large but the ssim index of simulation results at 8 hours indicates the results are better which implies that high order taylor expansion remainder should be used cautiously for challenging sharp wetting front problems correlations of ssim indices with the gradient reconstruction method dual control volume method flux approximation method and spatial weighting schemes of the above 126 simulation cases are summarized in table 5 the simulations are grouped by the various numerical schemes the fourth order least squares method produces much lower ssim indices and larger errors compared to the other gradient reconstruction methods with high order taylor expansion remainder added the fourth order least squares method fails or has difficulties converging the third order least squares method with high order taylor expansion remainder produces the best ssim indices compared to other methods for the three spatial weighting methods standard two point upstream weighting produces much lower ssim indices and larger errors compared to central weighting and mpups weighting by using central or mpups weighting it can be found that the edge based and face based flux approximations produce similar results with the face based method performing slightly better as shown by the average ssim indices similarly the revised voronoi dual control volume method with central or mpups weighting generates better ssim indices compared to the center dual and median dual control volume methods detailed information on the 126 simulation cases can be found in the supplementary material s 9 5 adaptability to distorted meshes in many modeling scenarios involving coupled physical processes such as coupling between flow and stress mesh distortion can occur during the solution process even if a highly orthogonal mesh can be used to represent the initial physical domain mcbride et al 2007 to avoid mesh regeneration during simulation numerical methods should be able to use the distorted mesh without losing accuracy in this section the proposed approaches are evaluated for distorted meshes by comparing the results obtained using a regular unstructured mesh with those from a distorted unstructured mesh the numerical experiment is based on case iv with the simulation time extended to 50 days to capture the long term effect of mesh quality for the mesh without distortion the average resolution is 0 05 m and the number of nodes is 27570 mesh 3 for the mesh with distortion the coordinates of nodes inside each zone fig 5 are randomly adjusted using the following formula 25 x i ω x i ω i x d x z i ω z i ω i z d z where xi and zi are the coordinates of i th node without distortion x i ω and z i ω are the coordinates of the i th node with distortion ω i x and ω i z are the random distortion coefficients in x and z directions dx and dz are the average resolution in x and z directions which is 0 05 m in this case for the weakly distorted mesh ω i x and ω i z range from 0 4 to 0 4 with mean zero and variance 0 0545 for the strongly distorted mesh ω i x and ω i z range from 0 5 to 0 5 with mean zero and variance 0 0851 the simulation domains with different mesh qualities are shown in fig 9 the area defined by the black box in figure 9 a has been enlarged and is shown in figs 9 c e and g for different meshes the radius ratio is used to describe the mesh distortion with a larger ratio indicating higher mesh distortion for a fully orthogonal mesh e g equilateral triangle the radius ratio is 1 the spatial distribution of the radius ratio for the three meshes is shown in figs 9 d f and h the largest radius ratio for the normal mesh without distortion is 1 68 while this value increases to 10 53 for the mesh with weak distortion and 127 83 for the mesh with strong distortion the histogram of the radius ratio for the three meshes is shown in fig 9 b to evaluate the capability of the proposed methods for distorted meshes without loss of generality the mpfa flux approximation is applied with green gauss gradient reconstruction and edge based control volume method simulation results obtained from the structured mesh mesh without distortion mesh with weak distortion and mesh with strong distortion are compared both central weighting and mpups weighting are considered for center dual median dual and revised voronoi dual control volume methods as shown in fig 10 compared to the results obtained from structured meshes a and b the proposed mpups weighting works well for all the three control volume methods however for central weighting the simulation using the center dual or median dual control volume method fails to generate accurate results while for the revised voronoi dual control volume method acceptable results are obtained the results obtained using the revised voronoi dual control volume method typically produces much lower numerical errors compared to either the center dual or median dual control volume methods compare fig 10 l to d and h and fig 10 n to f and j in addition the center dual and median dual control volume methods are more sensitive to mesh distortion when central weighting is used compare fig 10 e to c and fig 10 i to g these tests indicate that the revised voronoi dual control volume method is characterized by a high degree of flexibility when used with meshes of varying distortion as well as different spatial weighting the proposed mpups weighting also proved to be a more reliable multi point upstream weighting method compared to two point upstream weighting when applied to distorted meshes 6 discussion for the test cases with isotropic conductivity tensors the results are similar for the various numerical methods however different numerical aspects affect the accuracy of the results and solution efficiency in the simulations with anisotropic conductivity tensors our numerical experiments demonstrate that for isotropic material properties the revised voronoi dual control volume method yields the best accuracy and avoids nonphysical oscillations for meshes with various resolutions the center dual and median dual control volume methods generally require higher resolution meshes to minimize nonphysical oscillations the differences in gradient reconstruction methods spatial weighting and flux approximation methods do not play an important role for the simulation cases with an isotropic conductivity tensor and differences in the numerical results are not significant in this case the standard two point flux approximation can still generate reasonable results for anisotropic material properties the numerical experiments have shown that the control volume methods have a large effect on monotonicity convergence behavior and global mass balance error the revised voronoi dual control volume method is more robust and accurate compared to center dual and median dual control volume methods the standard two point flux approximation tpfa does not generate accurate results for anisotropic hydraulic conductivities the multi point flux approximation mpfa and multi point upstream mpups weighting play a critical role in the accuracy and monotonicity that guarantee convergence compared to tpfa and standard two point upstream weighting both central and mpups weighting are acceptable for different control volume methods and gradient reconstruction methods compared to the results using the structured mesh results obtained using central weighting are generally identical for different numerical schemes with either coarse mesh or fine mesh however nonphysical oscillations can occur or convergence failure is possible when using central weighting at sharp wetting fronts for either central weighting or mpups weighting an unstructured mesh with fine resolution is recommended to ensure accuracy for center dual or median dual control volume methods in contrast the revised voronoi dual control volume method has the advantage that a relatively coarse mesh can be used in addition the numerical experiments show that the proposed approaches can overcome stability issues for challenging sharp wetting front problems using general meshes as well as distorted meshes the results also demonstrate that the high order taylor expansion remainder can reduce the global mass balance error if used appropriately however this method tends to negatively affect convergence behavior for sharp wetting front problems for general unstructured meshes piecewise linear gradient reconstruction provides first order accuracy and piecewise quadratic gradient reconstruction provides second order accuracy blazek 2015 diskin thomas 2008 nishikawa 2015b nishikawa 2018 jalali gooch 2013 although higher order accuracy methods are often preferred we found that the first order scheme with piecewise linear gradient reconstruction has the advantage of avoiding oscillations around sharp wetting fronts 7 conclusions the goal of this work was to develop and investigate the stability of a multi point flux approximation mpfa and multi point upstream mpups weighting method for fully unstructured grids different numerical methods including control volume methods piecewise gradient reconstruction methods spatial weighting schemes and flux approximation methods were analyzed for the challenging wetting front problem of variably saturated flow the accuracy efficiency and convergence behaviors of the candidate set of methods described above were compared by solving variably saturated flow problems using unstructured meshes the proposed approaches were further evaluated for the simulation of variably saturated flow using distorted meshes generally the center dual control volume method and median dual control volume method are widely used due to their flexibility when combined with triangular meshes while the regular voronoi dual control volume method is not a preferable choice for triangular meshes when obtuse angles exist however it was found that the center dual and median dual control volume methods are more likely to cause nonphysical oscillations in contrast the revised voronoi dual control volume method can deal with triangular cells with obtuse angles and nonphysical oscillations can be avoided our results show that the revised voronoi dual control volume method can produce reasonably accurate results and simultaneously avoids nonphysical oscillations if piecewise linear gradient reconstruction is used although this method is not second order accurate it is suitable for the applications presented in this contribution because it does provide a high level of numerical stability for unstructured meshes the standard two point upstream weighting may violate the monotonicity that causes convergence problems and the proposed mpups weighting can be applied in this case it was demonstrated that the proposed mpfa and mpups work for different dual control volume methods including voronoi dual median dual and center dual and various gradient reconstruction methods including green gauss least squares and high order least squares although there is no significant difference in the results for the various gradient reconstruction methods this study shows that the piecewise quadratic gradient reconstruction method provides better accuracy with smaller mass balance errors compared to piecewise linear gradient reconstruction however the limitation of the high order gradient reconstruction method is the occurrence of convergence problems when applied to complex problems with heterogeneous and anisotropic material properties in addition the piecewise quadratic gradient reconstruction method is more computationally intensive than the piecewise linear gradient reconstruction method for meshes with good orthogonality it was observed that the various numerical methods work well without loss of accuracy and convergence however for the distorted meshes the revised voronoi dual control volume method has an advantage over conventional center dual and median control volume methods producing more accurate results it is concluded that the proposed mpfa method with mpups weighting and revised voronoi dual control volume method is a stable and robust method for solving challenging variably saturated flow problems with sharp wetting fronts credit authorship contribution statement danyang su conceptualization methodology writing review editing k ulrich mayer supervision writing review editing kerry t b macquarrie writing review editing declaration of competing interest none acknowledgements this research was supported by the nuclear waste management organization nwmo canada and a mitacs accelerate fellowship it07518 warded to d su the authors would also like to acknowledge westgrid and computecanada for providing computing hardware software and technical support supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103673 appendix supplementary materials image application 1 
435,richards equation for variably saturated flow can exhibit stability problems due to its nonlinearity the challenge is to resolve sharp wetting fronts without introducing spurious oscillations especially for simulations with very dry initial conditions flow instabilities at sharp wetting fronts may arise particularly in anisotropic and heterogeneous media leading to oscillations or convergence problems the focus of this work is to evaluate and minimize instability problems for simulations with unstructured meshes to this end numerical experiments were performed to investigate the accuracy monotonicity and convergence behavior of numerical solutions for variably saturated flow based on different control volume methods piecewise gradient reconstruction methods and flux approximation methods in particular nonphysical oscillations at the wetting front were investigated a novel multi point flux approximation with multi point upstream weighting based on piecewise gradient reconstruction was developed numerical simulations in both homogeneous and heterogeneous domains with isotropic and anisotropic conductivity tensors demonstrate that the proposed multi point flux approximation with multi point upstream weighting avoids spurious oscillations and improves the results for challenging sharp wetting front problems using general unstructured meshes as well as meshes with distortion the revised voronoi dual control volume method has also been found to provide more flexibility than commonly used center dual and median dual control volume methods keywords multi point flux approximation mpfa multi point upstream weighting mpups variably saturated flow unstructured mesh piecewise gradient reconstruction 1 introduction richards equation has been widely used for the simulation of water flow in unsaturated porous media driven by the actions of gravity and capillarity due to the nonlinearity of richards equation numerical solution of the variably saturated flow problem remains computationally expensive time consuming and in certain circumstances can lead to poor numerical stability and performance zha et al 2019 numerical methods may fail to produce reliable convergence behavior especially for higher order methods farthing and ogden 2017 the numerical accuracy and convergence of solutions for richards equation depend on numerous factors in particular the spatial discretization of the computational mesh which can be characterized through a characteristic length scale and a measure of the aspect ratio or distortion farthing and ogden 2017 the most accurate method is to discretize the solution domain using a structured mesh or orthogonal mesh without distortion however this is not always possible or expensive to do for complex simulation domains in such cases unstructured meshes are beneficial because they can more easily replicate irregular geometries and provide local refinements stability problems including spurious oscillations and convergence problems may be triggered by various aspects such as permeability contrasts in heterogeneous domains anisotropy and mesh effects in unstructured discretization christou et al 2019 spatial discretization based on a structured mesh is straightforward in that the elements of the mesh e g edge face are parallel or perpendicular to the cartesian axes taking the finite volume method as an example the flux over the control volume interface can be easily calculated based on two point flux approximation tpfa lunde 2007 however tpfa is only correct for grids that are orthogonal with respect to the permeability tensor k so called k orthogonal grids aavatsmark i 2007 for non k orthogonal grids tpfa produces an error in the solution that does not vanish as the grids are refined aavatsmark i 2007 for a general unstructured mesh the conventional tpfa method does not provide sufficient accuracy especially for simulations with anisotropic porous media the tpfa method will not converge because of distorted grid cells which often exist in grids honoring physical permeability contrasts eigestad and klausen 2005 special treatment such as local global nonlinear tpfa techniques should be used to account for anisotropy effects chen et al 2008 møyner and lie 2014 nikitin et al 2014 instead of applying the tpfa method a more general approach is to use multi point flux approximation mpfa techniques aavatsmark et al 1998a aavatsmark et al 1998b aavatsmark 2007 eigestad klausen 2005 aavatsmark et al 2008 edwards zheng 2010 zha et al 2013a when mpfa methods are applied to unstructured meshes some conditions need to be satisfied to avoid oscillations and nonphysical solutions for a delaunay grid and isotropic permeability it can be guaranteed that the transmissibility coefficient matrix computed by a mpfa discretization is a monotonic matrix m matrix however for anisotropic conditions the transmissibility coefficients are not necessarily positive and will therefore not generally lead to a m matrix mlacnik and durlofsky 2006 monotonicity is a sufficient condition to ensure that nonphysical local maxima and minima are not produced in the discrete nonlinear unsaturated flow equation forsyth and kropinski 1997 to ensure monotonicity the positive transmissibility condition is not sufficient and upstream weighting must be applied forsyth kropinski 1997 however the standard upstream mpfa cannot always satisfy the positive transmissibility condition that ensures monotonicity of the unstructured discretization younes et al 2013 the main aim of this work is to investigate the development of instabilities for variably saturated flow problems with emphasis on challenging sharp wetting fronts using a general purpose mpfa scheme a series of numerical factors such as spatial weighting piecewise gradient reconstruction method dual control volume method and edge face based flux calculation methods are investigated a revised voronoi dual control volume method and multi point upstream mpups weighting are proposed numerical tests demonstrate that the mpfa together with mpups is capable of producing stable solutions for variably saturated flow in porous media with a high level of heterogeneity and anisotropy the proposed method has also been applied to distorted meshes for which conventional methods are unable to produce convergent solutions or accurate results the combination of these numerical approaches and their adaptability to distorted meshes in the context of variably saturated flow is unique and has not been investigated previously the main outcome of this study is the identification of the most promising approach for solving challenging sharp wetting front problems for variably saturated flow using unstructured meshes the paper is organized as follows section 2 is devoted to the description of the variably saturated flow model and its discretization for different cell types gradient reconstruction and spatial weighting schemes in section 3 the versatility and validation of the different numerical methods are studied for variably saturated flow in heterogeneous and anisotropic domains in section 4 the monotonicity and convergence behavior of the different numerical methods are described in section 5 the adaptability of the proposed method to distorted meshes is investigated finally a discussion and conclusions are provided in sections 6 and 7 2 variably saturated flow model and numerical discretization in this section the governing equations for variably saturated flow are summarized followed by an outline of the methods used for spatial discretization gradient reconstruction and multi point upstream weighting 2 1 governing equation for variably saturated flow water flow in variably saturated media is here simulated based on richards equation richards 1931 which was derived from earlier work by richardson 1922 this equation and modified forms have been widely used to simulate water flow in variably saturated porous media in the past four decades huyakorn et al 1984 huyakorn et al 1986 clement et al 1994 paniconi 1992 huang et al 1996 therrien and sudicky 1996 vogel et al 2000 mayer et al 2002 kollet and maxwell 2006 trefry and muffels 2007 using hydraulic head as the primary dependent variable the mass conservation equation can be written as 1 s a s s h t ϕ s a t k r a k h q a where sa l3 water l 3 void is the water saturation ss l 1 is the specific storage coefficient h l is the hydraulic head t t is time φ is porosity kra is the relative permeability k l t 1 is the hydraulic conductivity tensor and qa m l 3 t 1 is a source sink term the saturation and relative permeability are a function of pressure based on the relationships given by wösten and van genuchten 1988 2 s a s r a 1 s r a 1 α ψ a n m 3 k r a s e a l 1 1 s e a 1 m m 2 where sra defines the residual saturation α n m and l are the soil hydraulic function parameters with m defined by 4 m 1 1 n ψ a l is the pressure head which can be obtained from 5 ψ a h z where z l defines the elevation with respect to a given datum sea is the effective saturation and is given by 6 s e a s a s r a 1 s r a 2 2 numerical discretization of the node centered mpfa scheme analytical solutions of richards equation exist only for simplified cases and in many practical situations flexible numerical solution approaches are required farthing and ogden 2017 among the majority of richards equation solvers the finite volume method is one of the most popular methods due to its natural ability to provide local mass conservation depending on the spatial location of solution values finite volume methods can be classified into cell centered and node centered also known as vertex centered methods diskin et al 2009 zha et al 2013b given the same mesh the solutions of cell centered methods are assumed to be located at the centroid of the cell and the solutions of node centered method are assumed to be located at the nodes of the mesh in this work we focus on the node centered approach in a 2d cartesian coordinate system for the node centered approach a dual mesh which represents the control volume of each node must be created three different dual mesh methods are evaluated in this work the first dual mesh method is referred as the median dual method blazek 2015 which joins the midpoint of each edge of the mesh with the center of the cell the second dual mesh method is referred as the voronoi dual method or containment dual method blazek 2015 hu and nicolaides 1992 in which case the segment that joins the two adjacent cell centers is perpendicular to their common edge and crosses this edge at the midpoint the third dual mesh method is referred to as the center dual method which joins the centers of adjacent cells illustrations of the three dual mesh methods for a triangular mesh are shown in figs 1 a c most mesh generation software cannot generate obtuse free meshes in this case the general voronoi dual control volume method cannot be applied since the circumcenter of a cell is outside of the cell here we implement a revised voronoi dual control volume method to allow for the use of meshes with obtuse cells so that users do not need to modify meshes the midpoint of the largest edge is treated as the cell center to build the dual mesh as shown in fig 1 b the center dual and median dual control volume methods work for a triangular mesh with obtuse cells without redefining the cell center however in a strongly distorted mesh the center dual mesh method may not work because the segment that joins two adjacent cell centers may not cross the shared edge depending on the selected interface center point representing the gradient of the control volume interface the flux calculation can be face based when point p is selected or edge based when point q is selected as shown in fig 1 d for the defined control volume vi enclosed by its boundary γ i the governing eq 1 can be rewritten in the integral conservative form by applying the gauss divergence theorem 7 s a s s t v i hdv ϕ t v i s a dv γ i k ra k h n d γ v j q a dv where n is the unit outward normal of the control volume interface by applying fully implicit time weighting eq 7 can be rewritten as 8 s a i n 1 s s h i n 1 h i n δ t v i ϕ i s a i n 1 s a i n δ t v i l γ i q a l n 1 n l δ s l q a i n 1 v i where the subscript i defines the ith control volume l represents the lth face of control volume vi δt is the time increment n 1 defines the new time level and n represents the old time level δsl l in 2d and l2 in 3d is the area of the lth face of control volume vi and q a l n 1 l t 1 is the water flux through the lth face of the control volume which can be expressed as 9 q a l n 1 k r a l n 1 k l h n 1 where k r a l n 1 is the relative permeability coefficient of the lth face of the control volume at the new time level which can be calculated based on the relative permeability coefficients of adjacent nodes using central weighting harmonic weighting or upstream weighting methods eq 1 is nonlinear because the saturation and the relative permeability are a function of fluid pressure wösten and van genuchten 1988 the hydraulic head gradient h n 1 applies to the lth face of a control volume at the new time level compared to the discretization based on a structured mesh eq 9 needs special treatment while the other terms in eq 8 remain the same or are similar for both structured and unstructured meshes substituting eq 9 into the first term on the right hand side of eq 8 yields the flux through the lth face of the control volume during the time increment δt 10 q a l n 1 n l δ s l k r a l n 1 k l h n 1 n l δ s l since k r a l n 1 and δsl are all scalars that can be calculated based on the provided simulation parameters and geometric features of the mesh we only need to focus on the calculation of the term k l h n 1 n l for the different meshes without loss of generality this term can be rewritten as 11 k l h n l h k l t n l h w l where w l k l t n l as shown in fig 1 d w l can be further decomposed to 12 w l w l u l u l w l n l n l where u l is the unit vector of w l projecting on the lth face of the control volume and n l is the unit outward normal of the lth face of the control volume because k l is a symmetric diagonally dominant matrix it is positive definite and w l is on the same side as the unit outward normal vector n l similarly the edge ij can be written as 13 v l v l u l u l v l n l n l rearranging eqs 12 and 13 we obtain 14 u l w l w l n l n l w l w l n l n l 15 n l v l v l u l u l v l n l substituting eqs 12 14 and 15 into eq 11 yields 16 k l h n l w l n l v l n l h v l p r i m a r y t e r m w l u l w l n l v l u l v l n l h u l s e c o n d a r y t e r m in eq 16 w l v l n l and u l are the physical and geometric parameters that can be calculated based on the provided mesh and material properties the flux across the lth face of the control volume consists of a primary term and a secondary term also known as cross diffusion term for k orthogonal meshes e g structured meshes both w l u l and v l u l equal zero and the secondary term can be ignored in the primary term w l n l v l n l is the transmissibility coefficient also referred to as the influence coefficient in some literature here we provide the approximation of h v l as shown in fig 1 d let p denote the location of center point p of the lth interface of the control volume δx be the vector from point p to node i and δx be the vector from point p to node j the m th order taylor expansion of function yields 17 h p δ x k 0 m 1 k δ x p k h p p p r m 1 where the remainder r has the lagrangian form which can be expressed as 18 r 1 m 1 δ x p k h p p p we have 19 h v l h p δ x p δ x h p δ x h p δ x h j h i ε ji where ε ji is the m th order taylor expansion remainder which can be expressed as 20 ε j i k 2 m 1 k δ x p k h p δ x p k h p p p it can be noted that the high order taylor expansion remainder term ε ji cancels out for a structured mesh when an edge based flux approximation is used for either unstructured mesh or face based flux approximations this term does not cancel out for a good quality mesh it is possible to assume ε ji 0 however this may not hold for a mesh with large expansion factors or radius ratios substituting eq 19 into eq 16 yields 21 k l h n l w l n l v l n l h j h i ε j i w l u l w l n l v l u l v l n l h p u l 2 3 gradient reconstruction and multi point flux approximation several different mpfa methods have been developed in the last two decades including mpfa o aavatsmark et al 1998a aavatsmark et al 1998b eigestad klausen 2005 mpfa o η edwards rogers 1998 mpfa u aavatsmark et al 1998a aavatsmark et al 1998b mpfa l aavatsmark et al 2008 and other enhanced mpfa methods chen et al 2008 edwards zheng 2010 moog 2013 the major difference between the various mpfa methods is the choice of the grid points that are used to calculate the gradient for flux estimation at the control volume interface for the mpfa o method all grid points located in the interaction region are included in the flux approximation while for the mpfa u and mpfa l methods only the grid points adjacent to the interface and specific neighbors are considered for rough grids with large aspect ratios and strong anisotropy ratios convergence may be poor for the mpfa o method while the mpfa l method has generally a larger monotonicity range and continues to provide convergence although at a reduced rate klausen winther 2006 aavatsmark et al 2008 keilegavlen aavatsmark 2011 mohammadnia et al 2017 additional details of the gradient reconstruction methods are provided in the supplementary material s 1 to s 3 for k orthogonal meshes without considering second and higher order taylor remainders eq 21 is reduced to k l h n l w l n l v l n l h j h i a general form of the tpfa method for other meshes the mth order gradient of h is required for eqs 20 and 21 in the present study we focus on the calculation of the second and higher order gradients of h at the center point p of the lth face of the control volume in a 2d coordinate system for the piecewise linear gradient reconstruction we use green gauss and least squares methods and for the piecewise quadratic gradient reconstruction we use higher order least squares methods it is worth mentioning that the proposed piecewise gradient reconstruction satisfies flux continuity as well as potential continuity hydraulic head is linear over the control volume for green gauss or least squares gradient reconstruction methods but quadratic for high order least squares gradient reconstruction methods 2 4 multi point upstream weighting in a fully implicit scheme simple upstream weighting can be obtained if the flux at the control volume interface is calculated as an explicit function of the fluid potential in the neighboring cells aavatsmark eigestad 2006 however for nonlinear unsaturated flow in an unstructured mesh the standard mpfa approach using upstream weighting cannot always satisfy the positive transmissibility condition which ensures monotonicity causing strong nonphysical oscillations near sharp wetting fronts younes et al 2013 monotonicity is here provided when the actual upstream point i e point with higher hydraulic head coincides with flux direction during flux calculation the calculated flux direction should always be from the location with higher hydraulic head to location with lower hydraulic head generally speaking central weighting requires lower numerical effort since there is no need to identify the upstream point while upstream weighting is better suited to deal with discontinuities blazek 2015 for second or higher order upstream weighting methods limiters have to be employed in order to prevent the development of spurious oscillations near strong discontinuities blazek 2015 water fluxes are usually treated with two point upstream weighting and it is well known that when simulating processes with adverse mobility ratios this method suffers from grid orientation effects keilegavlen et al 2012 to account for the mesh effects a novel multi point upstream mpups weighting with multi point flux approximation has been implemented in this research substituting eq 21 is into eq 10 and ignoring the timestep notation we obtain 22 q a l n l δ s l k r a l k l h n l δ s l k r a l a δ s l w l n l v l n l h j h i ε j i k r a l b δ s l w l u l w l n l v l u l v l n l h p u l where k r a l a and k r a l b are the relative permeabilities for the primary and secondary terms at the control volume interface for the calculation of the primary term conventional upstream weighting is applied however the secondary term may have a flux contribution with different direction compared to the primary term when the mpfa method is used especially in the wetting front zone where sharp gradients exist the proposed multi point upstream mpups weighting is based on the sign of the secondary term as shown in fig 1 d when the secondary term is positive the cross term flux is from node j to node i and node j is selected as the upstream point similarly node i is selected as the upstream point when the secondary term is negative as shown below 23 k r a l a k r a j i f w l n l v l n l h j h i ε j i 0 k r a l a k r a i i f w l n l v l n l h j h i ε j i 0 and 24 k r a l b k r a j i f w l u l w l n l v l u l v l n l h p u l 0 k r a l b k r a i i f w l u l w l n l v l u l v l n l h p u l 0 by using mpups the primary and secondary terms may have different upstream points depending on the directions of the flux contributions this rule guarantees monotonicity for all flux components across control volume interfaces 2 5 adaptive time stepping and update modification schemes the numerical difficulties in simulating variably saturated flow with sharp wetting infiltration fronts mainly arise from the nonlinearity of the head based form richards equation krabbenhøft 2007 zha et al 2017 zha et al 2019 an adaptive time stepping scheme and a modified newton s method with an underrelaxation scheme is included in the present model to ensure a reliable and robust solution of the nonlinear equations mayer et al 2002 mayer and macquarrie 2010 the scheme uses upstream weighting for the relative permeability term which leads to improved convergence behavior for sharp infiltration fronts forsyth et al 1995 in addition the code includes an option to limit the update of the head values at each newton iteration to avoid overshoot issues and guide convergence towards the correct solution this scheme has proven to be effective and has allowed to solve unsaturated flow problems in steady state mode which is a difficult problem since the initial conditions can be far from the solution to achieve this performance special treatment for low moisture contents was not required for transient solutions the timesteps are determined adaptively based on a projected maximum grid wide saturation change derived from saturation changes that occurred during the previous time step this method avoids timestep increases that can cause convergence problems 3 numerical experiments in this section a series of numerical tests are presented to investigate the performance of the proposed mpfa scheme this scheme has been implemented in the well established code min3p a process based numerical model designed for the investigation of subsurface fluid flow and reactive transport in variably saturated media mayer et al 2002 henderson et al 2009 bea et al 2012 bea et al 2018 su et al 2017 mayer and macquarrie 2010 we use results obtained from a structured mesh as the reference solution which is justified by previous verification of the code in addition the structured mesh is k orthogonal with a relatively small error resulting from numerical discretization even though an underrelaxation scheme is included in the code this feature was not used in all the tested cases the performance of the proposed approaches were analyzed considering a wide range of conditions and numerical methods including heterogeneity anisotropy control volume methods gradient reconstruction methods flux approximation methods and spatial weighting methods as shown in table 1 detailed information on the numerical approaches used for all simulations can be found in the supplementary material s 4 boundary conditions are imposed strongly for all numerical experiments the edge based discretization is replaced by an algebraic equation such as u j u b 0 where u j is a solution value at the boundary node j and u b is a value specified by the boundary condition in this case no boundary flux quadrature is required as outlined by nishikawa 2015a to provide a quantitative error analysis for the results obtained from unstructured mesh and structured mesh simulations the structural similarity ssim index has been determined and applied facilitating the error comparison for results with equivalent mesh resolution wang et al 2004 in this context the results obtained using the structured mesh are referred to as the reference solution the ssim index has a range of 0 to 1 a value of 1 implies that both solutions are identical and a value of 0 indicates no structural similarity for this analysis a sliding gaussian window of size 11 11 is displaced pixel by pixel on the image of the spatial results wang et al 2004 3 1 case i water table mounding in isotropic soil in this case the proposed methods are used to simulate variably saturated flow based on a laboratory experiment vauclin et al 1979 clement et al 1994 the problem domain is 6 00 m x 2 00 m with an initial horizontal water table located at a height of 0 65 m due to symmetry the simulated portion of the domain is 3 00 m x 2 00 m with no flow boundaries on the bottom and on the left side at the surface of the domain a constant flux 3 55 m day is applied over a width of 0 5 m on the right side of the domain a constant head boundary is applied up to an elevation of 0 65 m the remaining surface and the right side of the domain above the water table are defined as no flow boundaries observation point p1 is located at 0 3 m from the left boundary and 0 2 m below the top surface and observation point p2 is located close to the water table 0 8 m from the left boundary and 0 8 m below the top surface the simulation domain is shown in fig 2 the soil properties used in the model are homogeneous and isotropic with a saturated hydraulic conductivity of 8 40 m day a porosity of 0 30 and a residual water content of 0 01 the values of the van genuchten soil parameters are α 3 3 m 1 n 4 1 and specific storage is ignored clement et al 1994 vauclin et al 1979 three meshes with different resolutions were used for both structured and unstructured grids the parameters for spatial discretization time stepping and newton iteration settings are summarized in table 2 a comparison of transient water table positions produced by the current model using the structured grid mesh1 with results obtained by clement et al 1994 using the same spatial discretization and the experimental data collected by vauclin et al 1979 is shown in fig 2 the results indicate that there is good agreement between the numerical models and the experimental data as discussed above the lack of monotonicity in unsaturated flow can cause nonphysical oscillations near sharp wetting fronts these oscillations can have a dramatic effect on the convergence behavior of the numerical model we are therefore most interested in comparing results within the wetting front zone where sharp gradients exist and oscillation may occur water saturations after a period of 8 hours obtained using the various numerical schemes and mesh resolutions are generally in good agreement with each other however for center dual and median dual control volume methods oscillations are observed at the wetting front for both central weighting and multi point upstream weighting as shown in figs 3 a e for the revised voronoi dual control volume method the simulation results indicate excellent agreement with results obtained from the structured mesh as shown in figs 3 g i in this case there is no significant difference between central weighting and multi point upstream weighting for center dual and median dual control volume methods increased mesh resolution can improve numerical results as indicated by better ssim values however it cannot completely avoid oscillations similar results are obtained for the scenarios using other gradient reconstruction methods as well as mpfa edge based and tpfa edge based flux approximations additional details on the performance of the various methods can be found in the supporting material s 5 3 2 case ii water table mounding in anisotropic soil in this section case i is modified to demonstrate the capability of the gradient based mpfa method for variably saturated flow in porous media with anisotropic material properties the physical parameters spatial discretization parameters of newton iterations initial conditions and boundary conditions remain the same as those used in case i except that the horizontal hydraulic conductivity was modified from 8 4 m day to 84 0 m day while the vertical hydraulic conductivity remained unchanged contour plots of water saturations for case ii after a period of 8 hours using the various numerical schemes are shown in fig 4 results obtained using different control volume methods gradient reconstruction methods and face or edge based flux approximations are similar and thus not included in this figure for this case the spatial distributions of water saturations do not show nonphysical oscillations for any of the mpfa methods however for the tpfa method significant nonphysical oscillations are observed and the results lose accuracy compared to mpfa and structured grid methods this case indicates that the mpfa methods are preferred over the tpfa method for applications to unstructured meshes with anisotropic material properties the transient evolution of water saturations at the observation points can be found in the supporting material s 6 3 3 case iii water infiltration into a heterogeneous dry soil this test case considers water infiltration into a very dry heterogeneous soil with initial pressure of 880 0 kpa forsyth kropinski 1997 the spatial domain is 8 0 m x 6 5 m and consists of four zones with different soil properties as shown in fig 5 all boundaries are impermeable except the infiltration zone at the top left the observation point p1 is located at 2 0 m from the left boundary and 0 2 m below the top surface and observation point p2 is located 0 4 m below p1 the soil properties used in the model are shown in table 3 three meshes with different resolutions and local refinement in the sharp wetting front zone are used for both structured and unstructured grids respectively the parameters for spatial discretization time stepping and newton iteration settings are summarized in table 4 as shown in fig 6 for the median dual control volume method with mpfa flux approximation significant oscillations are observed at the wetting front for both central and upstream weighting for the revised voronoi dual control volume method water saturations are oscillation free and the results are in good agreement with those obtained from the structured mesh similar results were obtained for the two point flux approximation method edge based flux approximation and other gradient reconstruction methods the results obtained from the center dual control volume method are similar to those of the median dual control volume method and are therefore not shown in fig 6 this example indicates that for heterogeneous material with very dry initial conditions although the material is isotropic the median dual and center dual control volume methods are more likely to experience oscillation problems while the revised voronoi dual control volume method can prevent oscillations the transient evolution of water saturation at the observation points for this case can be found in the supporting material s 7 3 4 case iv water infiltration into a heterogeneous and anisotropic dry soil for case iv the physical parameters and boundary conditions are the same as those used in case iii except that the vertical hydraulic conductivities k z in all four zones were reduced by one order of magnitude with reduced k z the wetting front is expected to be much sharper causing conventional central weighting and two point upstream weighting schemes to experience convergence difficulties to compensate for the reduced hydraulic conductivity the initial pressure was changed to 88 0 kpa so that simulation results from conventional methods can be compared against the proposed methods two unstructured meshes with resolutions of 0 1 m mesh 1 and 0 025 m mesh 2 were used to analyze the effect of mesh resolution on the accuracy of the results the number of nodes for the unstructured meshes are 6955 and 110847 respectively for completeness the simulation results with reduced k z but subject to the same initial conditions can be found in the supplementary material s 8 the spatial distributions of water saturations after a period of 30 days are shown in fig 7 as expected the tpfa method does not work well for anisotropic media as shown in fig 7 c oscillations at the wetting front are pronounced for central weighting especially for median dual control volume methods as shown in fig 7 d compared to results obtained using the structured mesh and results obtained with the voronoi dual control volume method shown in figs 7 g and i similar oscillations were observed for the center dual control volume method increasing the mesh resolution does not help to prevent oscillations with central weighting in addition simulations with central weighting using median dual or center dual control volume methods experience convergence problems while the simulation using the voronoi dual control volume method has no difficulty with convergence for mpups weighting similar results were obtained using different control volume methods as shown in figs 7 e and h with the refinement of mesh resolution simulations using voronoi dual median dual or center dual control volume methods generate similar results when mpups weighting is used generally the spatial distribution of water saturations obtained using the voronoi dual control volume method produces better accuracy than the median dual control volume method as shown in figs 7 f and j compared to fig 7 b it can also be observed that oscillations at the wetting front can be improved and more accurate results can be obtained for both central weighting and mpups weighting using the voronoi dual control volume method additional details on the performance of the various methods can be found in the supporting material s 8 4 monotonicity and convergence both anisotropy and heterogeneity increase the complexity of simulating flow in variably saturated media simulations performed for cases i to iv show that hydraulic conductivity variations with direction are more likely to cause oscillations and convergence problems than hydraulic conductivity variations with location particularly near sharp wetting fronts in the simulations with isotropic conductivity tensors despite the presence of heterogeneity the results are similar for the various numerical methods including different control volume methods center dual median dual voronoi dual different gradient reconstruction methods green gauss least squares and high order least squares different flux approximation methods tpfa mpfa edge face based and different spatial weighting schemes central upstream mpups however for simulations with anisotropic conductivity tensors the various methods exhibit quite different performance affecting the quality of the simulation results in this section monotonicity and convergence behavior are analyzed for simulations involving both isotropic and anisotropic material properties without loss of generality the numerical experiments with anisotropic material properties case ii are used here the monotonicity and convergence behavior of case ii are analyzed based on a mesh with average nodal spacing 0 05 m mesh1 with a total of 126 simulation cases analyzed for each gradient reconstruction method 12 simulations using different control volume methods flux approximation methods and spatial weighting schemes were carried out all simulations converged well except those using the fourth order least squares method the failure of the fourth order gradient reconstruction is attributed to nonlinearity introduced when additional neighboring nodes are used simultaneously it is notable that the higher order taylor expansion remainder does not always enhance the convergence for center dual and median dual control volume methods the third order least squares method with higher order remainder requires more iterations for the mpfa face based than edge based method as shown in figs 8 a and b for the center dual method and the third order least squares method with higher order remainder the average number of nonlinear iterations is 26285 for mpfa face based method compared to 7587 for mpfa edge based method similarly for the median dual method this value is 35047 for the mpfa face based method and 8710 for the mpfa edge based method the voronoi dual method required the least number of iterations where the average number of nonlinear iterations is 11082 for mpfa face based method and 5929 for mpfa edge based method we use relative global mass balance error to evaluate the accuracy of the proposed methods the global mass balance error is calculated based on the influx across the model boundary the corresponding outflux across the model boundary and the change in storage within the simulation domain the relative global mass balance error of simulations using higher order taylor expansion is also significantly higher than the error of other methods compared to the center dual and median dual control volume methods the relative global mass balance error based on the revised voronoi dual control volume method is slightly smaller and it takes fewer iterations and less total runtime even though the tpfa method meets the positive transmissibility condition we find that it is not sufficient to ensure monotonicity for the nonlinear unsaturated flow with anisotropic material properties for the median dual and center dual control volume methods there is no significant difference between face based and edge based mpfa methods in the number of nonlinear iterations time steps global mass balance error and total runtime however for the revised voronoi dual control volume method the edge based flux approximation method generally yields better performance than the face based flux approximation method for gradient reconstruction with a higher order taylor expansion remainder as shown in figs 8 c e the third order least squares method with higher order taylor expansion remainder generates much larger errors and a lower similarity index than the other gradient reconstruction methods when the face based method is used indicating that the higher order taylor expansion remainder does not work well with the face based mpfa method the relative global mass balance error obtained with and without higher order taylor expansion remainder for the third order least squares method is almost identical when the edge based flux approximation method is used implying that the the higher order taylor expansion remainder does not reduce the numerical error significantly in this case in this case compared to the piecewise linear gradient reconstruction using green gauss and least squares methods the piecewise quadratic gradient reconstruction using second order or the third order least squares yield smaller global mass balance errors and better similarity indices as shown in figs 8 c and d compared to the median dual and center dual control volume methods the revised voronoi dual control volume method is the most robust method which requires fewer linear and nonlinear iterations it should also be noted that the ssim index may produce misleading results when mpups weighting is used together with the face based mpfa method and third order least squares with higher order taylor expansion remainder as shown in figs 8 c and d the global mass balance error is large but the ssim index of simulation results at 8 hours indicates the results are better which implies that high order taylor expansion remainder should be used cautiously for challenging sharp wetting front problems correlations of ssim indices with the gradient reconstruction method dual control volume method flux approximation method and spatial weighting schemes of the above 126 simulation cases are summarized in table 5 the simulations are grouped by the various numerical schemes the fourth order least squares method produces much lower ssim indices and larger errors compared to the other gradient reconstruction methods with high order taylor expansion remainder added the fourth order least squares method fails or has difficulties converging the third order least squares method with high order taylor expansion remainder produces the best ssim indices compared to other methods for the three spatial weighting methods standard two point upstream weighting produces much lower ssim indices and larger errors compared to central weighting and mpups weighting by using central or mpups weighting it can be found that the edge based and face based flux approximations produce similar results with the face based method performing slightly better as shown by the average ssim indices similarly the revised voronoi dual control volume method with central or mpups weighting generates better ssim indices compared to the center dual and median dual control volume methods detailed information on the 126 simulation cases can be found in the supplementary material s 9 5 adaptability to distorted meshes in many modeling scenarios involving coupled physical processes such as coupling between flow and stress mesh distortion can occur during the solution process even if a highly orthogonal mesh can be used to represent the initial physical domain mcbride et al 2007 to avoid mesh regeneration during simulation numerical methods should be able to use the distorted mesh without losing accuracy in this section the proposed approaches are evaluated for distorted meshes by comparing the results obtained using a regular unstructured mesh with those from a distorted unstructured mesh the numerical experiment is based on case iv with the simulation time extended to 50 days to capture the long term effect of mesh quality for the mesh without distortion the average resolution is 0 05 m and the number of nodes is 27570 mesh 3 for the mesh with distortion the coordinates of nodes inside each zone fig 5 are randomly adjusted using the following formula 25 x i ω x i ω i x d x z i ω z i ω i z d z where xi and zi are the coordinates of i th node without distortion x i ω and z i ω are the coordinates of the i th node with distortion ω i x and ω i z are the random distortion coefficients in x and z directions dx and dz are the average resolution in x and z directions which is 0 05 m in this case for the weakly distorted mesh ω i x and ω i z range from 0 4 to 0 4 with mean zero and variance 0 0545 for the strongly distorted mesh ω i x and ω i z range from 0 5 to 0 5 with mean zero and variance 0 0851 the simulation domains with different mesh qualities are shown in fig 9 the area defined by the black box in figure 9 a has been enlarged and is shown in figs 9 c e and g for different meshes the radius ratio is used to describe the mesh distortion with a larger ratio indicating higher mesh distortion for a fully orthogonal mesh e g equilateral triangle the radius ratio is 1 the spatial distribution of the radius ratio for the three meshes is shown in figs 9 d f and h the largest radius ratio for the normal mesh without distortion is 1 68 while this value increases to 10 53 for the mesh with weak distortion and 127 83 for the mesh with strong distortion the histogram of the radius ratio for the three meshes is shown in fig 9 b to evaluate the capability of the proposed methods for distorted meshes without loss of generality the mpfa flux approximation is applied with green gauss gradient reconstruction and edge based control volume method simulation results obtained from the structured mesh mesh without distortion mesh with weak distortion and mesh with strong distortion are compared both central weighting and mpups weighting are considered for center dual median dual and revised voronoi dual control volume methods as shown in fig 10 compared to the results obtained from structured meshes a and b the proposed mpups weighting works well for all the three control volume methods however for central weighting the simulation using the center dual or median dual control volume method fails to generate accurate results while for the revised voronoi dual control volume method acceptable results are obtained the results obtained using the revised voronoi dual control volume method typically produces much lower numerical errors compared to either the center dual or median dual control volume methods compare fig 10 l to d and h and fig 10 n to f and j in addition the center dual and median dual control volume methods are more sensitive to mesh distortion when central weighting is used compare fig 10 e to c and fig 10 i to g these tests indicate that the revised voronoi dual control volume method is characterized by a high degree of flexibility when used with meshes of varying distortion as well as different spatial weighting the proposed mpups weighting also proved to be a more reliable multi point upstream weighting method compared to two point upstream weighting when applied to distorted meshes 6 discussion for the test cases with isotropic conductivity tensors the results are similar for the various numerical methods however different numerical aspects affect the accuracy of the results and solution efficiency in the simulations with anisotropic conductivity tensors our numerical experiments demonstrate that for isotropic material properties the revised voronoi dual control volume method yields the best accuracy and avoids nonphysical oscillations for meshes with various resolutions the center dual and median dual control volume methods generally require higher resolution meshes to minimize nonphysical oscillations the differences in gradient reconstruction methods spatial weighting and flux approximation methods do not play an important role for the simulation cases with an isotropic conductivity tensor and differences in the numerical results are not significant in this case the standard two point flux approximation can still generate reasonable results for anisotropic material properties the numerical experiments have shown that the control volume methods have a large effect on monotonicity convergence behavior and global mass balance error the revised voronoi dual control volume method is more robust and accurate compared to center dual and median dual control volume methods the standard two point flux approximation tpfa does not generate accurate results for anisotropic hydraulic conductivities the multi point flux approximation mpfa and multi point upstream mpups weighting play a critical role in the accuracy and monotonicity that guarantee convergence compared to tpfa and standard two point upstream weighting both central and mpups weighting are acceptable for different control volume methods and gradient reconstruction methods compared to the results using the structured mesh results obtained using central weighting are generally identical for different numerical schemes with either coarse mesh or fine mesh however nonphysical oscillations can occur or convergence failure is possible when using central weighting at sharp wetting fronts for either central weighting or mpups weighting an unstructured mesh with fine resolution is recommended to ensure accuracy for center dual or median dual control volume methods in contrast the revised voronoi dual control volume method has the advantage that a relatively coarse mesh can be used in addition the numerical experiments show that the proposed approaches can overcome stability issues for challenging sharp wetting front problems using general meshes as well as distorted meshes the results also demonstrate that the high order taylor expansion remainder can reduce the global mass balance error if used appropriately however this method tends to negatively affect convergence behavior for sharp wetting front problems for general unstructured meshes piecewise linear gradient reconstruction provides first order accuracy and piecewise quadratic gradient reconstruction provides second order accuracy blazek 2015 diskin thomas 2008 nishikawa 2015b nishikawa 2018 jalali gooch 2013 although higher order accuracy methods are often preferred we found that the first order scheme with piecewise linear gradient reconstruction has the advantage of avoiding oscillations around sharp wetting fronts 7 conclusions the goal of this work was to develop and investigate the stability of a multi point flux approximation mpfa and multi point upstream mpups weighting method for fully unstructured grids different numerical methods including control volume methods piecewise gradient reconstruction methods spatial weighting schemes and flux approximation methods were analyzed for the challenging wetting front problem of variably saturated flow the accuracy efficiency and convergence behaviors of the candidate set of methods described above were compared by solving variably saturated flow problems using unstructured meshes the proposed approaches were further evaluated for the simulation of variably saturated flow using distorted meshes generally the center dual control volume method and median dual control volume method are widely used due to their flexibility when combined with triangular meshes while the regular voronoi dual control volume method is not a preferable choice for triangular meshes when obtuse angles exist however it was found that the center dual and median dual control volume methods are more likely to cause nonphysical oscillations in contrast the revised voronoi dual control volume method can deal with triangular cells with obtuse angles and nonphysical oscillations can be avoided our results show that the revised voronoi dual control volume method can produce reasonably accurate results and simultaneously avoids nonphysical oscillations if piecewise linear gradient reconstruction is used although this method is not second order accurate it is suitable for the applications presented in this contribution because it does provide a high level of numerical stability for unstructured meshes the standard two point upstream weighting may violate the monotonicity that causes convergence problems and the proposed mpups weighting can be applied in this case it was demonstrated that the proposed mpfa and mpups work for different dual control volume methods including voronoi dual median dual and center dual and various gradient reconstruction methods including green gauss least squares and high order least squares although there is no significant difference in the results for the various gradient reconstruction methods this study shows that the piecewise quadratic gradient reconstruction method provides better accuracy with smaller mass balance errors compared to piecewise linear gradient reconstruction however the limitation of the high order gradient reconstruction method is the occurrence of convergence problems when applied to complex problems with heterogeneous and anisotropic material properties in addition the piecewise quadratic gradient reconstruction method is more computationally intensive than the piecewise linear gradient reconstruction method for meshes with good orthogonality it was observed that the various numerical methods work well without loss of accuracy and convergence however for the distorted meshes the revised voronoi dual control volume method has an advantage over conventional center dual and median control volume methods producing more accurate results it is concluded that the proposed mpfa method with mpups weighting and revised voronoi dual control volume method is a stable and robust method for solving challenging variably saturated flow problems with sharp wetting fronts credit authorship contribution statement danyang su conceptualization methodology writing review editing k ulrich mayer supervision writing review editing kerry t b macquarrie writing review editing declaration of competing interest none acknowledgements this research was supported by the nuclear waste management organization nwmo canada and a mitacs accelerate fellowship it07518 warded to d su the authors would also like to acknowledge westgrid and computecanada for providing computing hardware software and technical support supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103673 appendix supplementary materials image application 1 
436,accurate simulation of multiphase flow in subsurface formations is challenging as the formations span large length scales km with high resolution heterogeneous properties to deal with this challenge different multiscale methods have been developed such methods construct coarse scale systems based on a given high resolution fine scale system furthermore they are amenable to parallel computing and allow for a posteriori error control the multiscale methods differ from each other in the way the transition between the different scales is made multiscale finite element and finite volume methods compute local basis functions to map the solutions e g pressure between coarse and fine scales instead homogenization methods solve local periodic problems to determine effective models and parameters e g permeability at a coarser scale it is yet unknown how these two methods compare with each other especially when applied to complex geological formations with no clear scale separation in the property fields this paper develops the first comparison benchmark study of these two methods and extends their applicability to fully implicit simulations using the algebraic dynamic multilevel adm method at each time step on the given fine scale mesh and based on an error analysis the fully implicit system is solved on a dynamic multilevel grid the entries of this system are obtained by using multiscale local basis functions adm ms and respectively by homogenization over local domains adm ho both sets of local basis functions adm ms and local effective parameters adm ho are computed at the beginning of the simulation with no further updates during the multiphase flow simulation the two methods are extended and implemented in the same open source darsim2 simulator https gitlab com darsim2simulator to provide fair quality comparisons the results reveal insightful understanding of the two approaches and qualitatively benchmark their performance it is re emphasized that the test cases considered here include permeability fields with no clear scale separation the development of this paper sheds new lights on advanced multiscale methods for simulation of coupled processes in porous media keywords multiscale homogenization algebraic dynamic multilevel adaptive mesh refinement flow in porous media fully implicit simulation 1 introduction geological formations span large km length scales having heterogeneous properties characterized at high resolutions cm and below as for the uncertainty within the integrated field data typically several equiprobable realizations of the property fields are generated to study and simulate the fluid flow and transport classical simulation approaches are too expensive for such studies therefore advanced simulation methods are required to allow for an accurate representation of the heterogeneous properties at the same time they should provide an efficient simulation framework to study multiple realizations jansen et al 2009 wachspress 1966 model order reduction techniques have been developed to provide a meaningful approximate simulation framework such techniques have to be fast enough to be applied to large scale computational domains in this sense any advanced method of this type can be seen as field applicable only if it allows for reducing the error below any desired threshold value hajibeygi et al 2012 here we only consider numerical model order reduction techniques among which multiscale efendiev and hou 2009 hou and wu 1997 and homogenization weinan 2011 methods stand very promising these approaches are different in the sense that the multiscale method deals with crossing the solution e g the pressure across the scales aarnes and hou 2002 jenny et al 2003 hajibeygi et al 2008 chung et al 2015 whereas in the latter effective lower resolution parameters and functions like the permeability or the transmissibility are derived weinan and yue 2004 abdulle et al 2012 weinan et al 2007 li et al 2020 singh and wheeler 2018 vasilyeva et al 2020 moreover while the multiscale basis functions have been expressed in a purely algebraic formulation wang et al 2014 the same does not hold for the homogenization approach specially the integration of homogenized parameters within the fully implicit framework in an algebraic manner has not yet been developed so far the present work is a first step in this direction at the same time the two methods have many similarities both find their mapping strategy via local solutions of the original governing equations with local boundary conditions multiscale basis functions often employ reduced dimensional boundary conditions tene et al 2015 møyner and lie 2016 while homogenization schemes impose periodic boundary conditions on local problems and consider local representative micro structures even in the case of non periodic properties allaire 1992 abdulle and weinan 2003 arbogast and xiao 2013 bastidas et al 2019 brown et al 2013 both methods are effective for global equations within the fully coupled system of local global unknowns e g the global pressure and the local saturation both have been extended to nonlinear and geologically complex models amanbek et al 2019a hosseinimehr et al 2018 singh et al 2019a recent developments of these two classes of approaches have introduced a fully implicit dynamic multilevel simulation framework adm in which heterogeneous detailed geo models are mapped into adaptive dynamic coarser mesh cusini et al 2018 faigle et al 2014 klemetsdal et al 2020 carciopolo et al 2020 the adm method develops a fully implicit discrete system for coupled flow and transport system of equations in which each equation can be represented at a different resolution than the defined fine scale one more importantly the procedure can be done fully algebraic with the dynamic mesh resolution defined based on a front tracking strategy in contrast to the rich existing literature of adaptive mesh refinement amr methods pau et al 2009 2012 berger and oliger 1984 schmidt and jacobs 1988 edwards 1996 sammon 2003 klemetsdal and lie 2020 adm can be defined as an adaptive mesh coarsening strategy which is conveniently applicable for heterogeneous and nonlinear coupled systems cusini et al 2016 irrespective of the choice of the dynamic mesh strategy it is always a challenge to construct adaptive multiscale entries of the implicit systems the adm method so far has included multiscale basis functions cusini et al 2016 in addition homogenization methods have also been developed for multiphase simulations on dynamic grids amanbek et al 2019a cusini et al 2019 in this context two aspects can be of interest the study of the homogenization based coarser system entries and the development of a benchmark study of the quality of the two approaches of adm multiscale adm ms and adm homogenized adm ho for coupled implicit multiphase flow scenarios this paper develops such a unified framework in which the adm method is extended to account for both multiscale and homogenization schemes for multiphase flow simulations this development makes it possible to allow for different coarse scale entries for dynamic simulations and importantly to benchmark the two classes of multiscale and homogenization strategies noteworthy is that once the effective parameters are computed all other homogenization procedures are implemented algebraically this is done by introducing constant unity local basis with the support of primal non overlapping coarse scale partitions the multiscale adm is implemented fully algebraic since local basis functions are also solved algebraically over the overlapping dual coarse grid domains zhou and tchelepi 2012 the outcome of this development is made available to the public via an open source darsim2 simulator https gitlab com darsim2simulator numerical test cases are considered for the challenging highly heterogeneous spe10 christie and blunt 2001 the number of active grid cells pressure and saturation errors and the solution maps are all reported in detail the development of this paper sheds new lights in the application of multiscale and homogenization approaches in advanced next generation environments for field relevant simulation scenarios the paper is structured as follows next in section 2 the mathematical model is stated briefly section 3 presents the computational framework for both multiscale and homogenization adm methods section 4 presents the test cases and conclusions are drawn in section 5 the appendix gives more details on the multiscale and the homogenization approaches 2 governing equations we consider flow of two immiscible and incompressible phases of α and β through a heterogeneous porous medium at the darcy scale mass balance for the phase i α β reads 1 t ϕ ρ i s i ρ i λ i p ρ i g z ρ i q i here ϕ is the porosity of the medium ρi kg m3 and si are the density and saturation of the phase i respectively the phase mobility tensor λi is equal to k k r i μ i where k m2 is the rock absolute permeability tensor and k r i k r i s i is the saturation dependent relative permeability of phase i moreover μ pa s is the phase viscosity for the ease of presentation the two phase pressures are assumed equal p p α p β pa see e g aziz and settari 2002 however the extension to models involving a saturation dependent capillary pressure is also possible in addition g m s2 is the gravitational acceleration which acts on z direction and q 1 s is the phase source term here it is assumed that the two fluids are occupying completely the pore space and no other fluid phase is present this gives the constraint s α s β 1 which reduces the number of unknowns in the above equations to two sα in short from here on s and p finally the model is completed by initial conditions for the saturation and with boundary conditions we do not specify them explicitly since none of them play a role in the multiscale strategy the fully implicit coupled simulation approach aziz and settari 2002 estimates all the parameters at next time step n 1 as such the semi discrete nonlinear residual for the phase i α β reads 2 r i n 1 ρ i q i n 1 ϕ ρ i s i n 1 ϕ ρ i s i n δ t ρ i λ i p ρ i g z n 1 for finding the solution pair p n 1 s n 1 one needs to employ a linearization scheme here we restrict the discussion to the newton scheme which is 2nd order convergent but requires a starting point that is close enough to the solution in other words the time step may be subject to restrictions also depending on the mesh size alternatively one may consider approaches like the modified picard celia et al 1990 or the l scheme radu et al 2017 which are less demanding from the computational point of view or more robust w r t the starting point and mesh resolution but converge slower than the newton scheme bastidas et al 2019 applied to 2 the newton linearization reads 3 r n 1 r ν r p ν δ p ν 1 r s ν δ s ν 1 which can be expressed algebraically as j ν δ x ν 1 r ν i e 4 r α p r α s r β p r β s j ν δ p δ s δ x ν 1 r α r β r ν in each time step the linear eq 4 is solved iteratively inner loop several times until nonlinear convergence outer loop is reached the overall computational complexity of the simulation depends highly on the complexity of the solution of this linear system advanced multiscale and homogenization methods aim at solving this linear system on a dynamic multilevel mesh note that as shown before cusini et al 2018 the overall efficiency of any advanced method should include not only the speedup of solving the linear eq 4 but also the count of the newton outer loops next the adm method based on multiscale and homogenization formulations is presented 3 dynamic multilevel simulation based on multiscale and homogenization methods 3 1 adm framework formulation the fully implicit linear system 4 is too expensive to be solved for real field scenarios a multilevel dynamic mesh as shown in fig 1 is generated within the adm framework based on an error estimate strategy the error estimate is developed based on a front tracking criterion which applies fine scale grids only at sub regions with sharp gradients the fine scale system is then algebraically reduced into this multilevel grid through sequences of restriction and prolongation operators to obtain the adm grid first sets of n l n x l n y l hierarchically nested coarse grids are imposed on the fine mesh here l indicates the coarsening level moreover γl is the coarsening ratio which is defined as 5 γ l γ x l γ y l n x l 1 n x l n y l 1 n y l for two dimensional 2d domains the adm grid is constructed by assembling a combination of cells at different resolutions within the computational domain by using the sequence of restriction r and prolongation p operators one can express the adm system as 6 r l l 1 r 1 0 j 0 p 0 1 p l 1 l j adm δ x adm r l l 1 r 1 0 r 0 r adm here r l l 1 is the restriction operator which maps the parts of the solution vector that are at level l 1 to level l similarly the prolongation operator p l 1 l maps the parts of the solution vector that are at level l to level l 1 once the adm system 6 is solved the approximated fine scale solution δ x 0 can be acquired by prolonging the adm solution δ x adm i e 7 δ x 0 δ x 0 p 0 1 p l 1 l δ x adm the adm restriction r l l 1 and prolongation p l 1 l operators are assembled using the static multilevel multiscale restriction r l l 1 and prolongation p l 1 l operators respectively they are constructed only at the beginning of the simulation and are kept unchanged throughout the entire simulation the static prolongation operator p l 1 l is constructed as an assembly of the locally computed basis functions at each coarsening level l and reads 8 p l 1 l p p l 1 l 0 0 p s l 1 l n l 1 n l here p p l 1 l and p s l 1 l are the two main diagonal blocks corresponding to main unknowns i e pressure p and saturation s in the case of using the homogenization scheme i e adm ho as will be described in section 3 3 constant basis functions for pressure are used however for the multiscale based adm i e adm ms as will be described in section 3 2 locally computed basis functions are used note that the saturation prolongation operator for both approaches is constant unity function at all coarsening levels which represents the conservative finite volume integration the static restriction operator r l l 1 reads 9 r l l 1 r l l 1 0 0 r l l 1 n l n l 1 in this work a finite volume restriction operator is used to guarantee local mass conservation i e 10 r l l 1 i j 1 if cell i is inside coarser cell j 0 otherwise 3 2 adm using multiscale adm ms in the adm ms method the prolongation operator for pressure is found based on multiscale basis functions these local basis functions are computed algebraically wang et al 2014 based on the steady state pressure equation in this study the incompressible flow equation elliptic pressure equation is used to construct the multiscale basis functions tene et al 2015 an example of a basis function is shown in fig 2 the coarse grid construction and computation of multiscale basis functions are explained in more details in appendix a 3 3 adm using homogenization adm ho homogenization is another method that can be applied to problems involving multiple scales in this method one uses the mathematical models at micro fine scale 1 to derive effective upscaled models and parameters in which the rapidly oscillating characterstics are averaged out in doing so the upscaled model may have a different structure than the ones at the fine scale we refer to amaziane et al 2017 bourgeat et al 1996 hornung 1997 van duijn et al 2007 for theoretical details the goal of this work is to build a unified adm platform where the multiscale and homogenization methods can be compared therefore here the homogenization method is used only to construct effective properties at the dynamic multilevel mesh in this setup the homogenized properties of adm ho at multilevel mesh are found as in adm ms by solving local flow pressure equations based on an incompressible elliptic equation more precisely one assumes that a scale separation holds and doubles the spatial variable into a fast and a slow one the method relies on the homogenization ansatz meaning that all quantities in 1 can be expanded regularly in terms of a scale separation parameter such ideas are employed in bastidas et al 2019 abdulle and nonnenmacher 2009 amanbek et al 2019b amaziane et al 1991 singh et al 2019b szymkiewicz et al 2011 henning et al 2015 2013 to develop effective numerical simulation schemes even in case of non periodic media more details about the homogenization procedure can be found in appendix b in the present context for a given fine scale effective permeability k and for each coarsening level l an effective permeability tensor k l is computed locally in a pre processing step first the domain ω r 2 is divided into coarse cells ω l that correspond to a partition of the domain ω as shown in fig 3 for each coarse cell ω l at level l the components of the effective permeability tensor are calculate as 11 k i j l ω l ω l k e j ω j e i d y for i j 1 2 here ωj are the periodic solutions of the pressure equation on local domains known as micro cell equation in ho literature i e 12 k y ω j e j 0 for all y ω l we remark that e j j 1 2 is the canonical basis of dimension 2 and k is the above mentioned permeability tensor to guarantee the uniqueness of the solution ωj one assumes that its average value over the local coarse cell ω l is 0 to determine the value of the effective permeability tensor at each coarse cell ω l two local micro cell problems 12 are solved for each spatial direction in 2d fig 4 provides an illustration of these local solutions for a coarse element note that the local problems 12 capture the rapidly oscillating characteristics within a coarse element completely decoupled from other coarse elements the homogenized parameters like multiscale bases are computed at the beginning of the simulation fig 5 illustrates the calculation of the effective permeability at different levels the homogenized parameters are used to construct the coarse system entries more precisely the homogenized value in a coarse cell is distributed equally to the fine cells constructing it then the fine scale jacobian and residual are computed with the fine scale saturation field this system is then mapped to the adm resolution by setting prolongation operators in 6 to unity this is a convenient procedure developed in this work to integrate the numerical homogenization method with an existing advanced simulator notice that based on the features of the permeability tensor k the resulting effective parameter k may depend on the macro scale location and the size of the coarse scale partition nevertheless one can show that in practice the adaptive refinement of the mesh is an important aspect that improve the calculation of the effective parameters see bastidas et al 2019 and fig 5 the adm procedure is sketched in fig 6 more details about the role of the homogenization in the offline stage and the complete algorithm of adm ms and adm ho can be found in algorithm 1 4 simulation results to benchmark the homogenization and multiscale based solutions for the dynamic mesh on heterogeneous media two heterogeneous non periodic permeability fields from the top and bottom layers of the spe 10th comparative solution project christie and blunt 2001 are considered for both test cases the computational domain entails 216 54 grid cells at fine scale with δ x δ y 1 m a no flow condition is imposed on all boundaries initially the reservoir contains only the 2nd phase e g oil i e s 0 the 1st phase e g water is injected from an injection well while the reservoir fluid is produced from the production well the locations of the injection and production wells are specified in each test case table 1 shows the input parameters of the fluid and rock properties used in all test cases note that the density and viscosity ratios are assumed to be 1 since compressibility and gravitational forces are both neglected the density values have no influence on the results the numerical results provided by the adm ms and adm ho methods are compared to those obtained from simulation at fine scale reference both adm methods employ the coarsening ratio of 3 3 with two coarsening levels this is set according to the size of the domain 4 1 test case 1 spe10 top layer in this test case one injection well and one production well are placed in the bottom left corner and top right corner of the domain respectively the simulation time is t 1000 days and the results are reported on 100 equidistant time intervals the permeability distribution of the spe10 top layer is shown in fig 7 fig 8 shows the homogenized version of the permeability at two different levels we highlight that the homogenized permeability at both coarse levels preserves the structure of the original fine scale permeability the high and low permeable zones remain clearly detectable the saturation and pressure fields at the final time step are shown in fig 9 and fig 10 respectively from these results it is understood that adm ho on a coarse cell containing high and low permeable fine cells can lead to a higher flow leakage as compared to fine scale and adm ms approaches this effect can be seen in fig 9 fig 11 illustrating the adaptive mesh at 2000 days after injection notice that the refinement of the permeability is most dominant at the saturation front due to the chosen mesh refinement criterion for this figure the coarsening threshold value is δ s 0 3 i e a cell is successively coarsened if δs is lower than 0 3 the error history maps for both adm ms and adm ho are shown in fig 12 the relative errors presented in fig 12 and fig 14 are expressed in terms of the l 2 norm over the entire medium calculated with respect to the fine scale solution as 13 e r r o r s s ref s adm 2 s ref 2 14 e r r o r p p ref p adm 2 p ref 2 the results indicate that the homogenization based simulations have higher errors compared with the multiscale based simulations they both have similar average usage of active grid cells with adm ms having slightly fewer grid cells this is shown in fig 13 note that the grid cells around wells are kept at the fine scale resolution permanently furthermore for tighter error tolerance values the quality of both approaches become comparable fig 14 provides the average pressure and saturation errors together with the average percentage of active grid cells during the whole simulation time as functions of the coarsening criterion threshold 4 2 test case 2 spe10 bottom layer in the second test case the permeability distribution of the spe10 bottom layer presented in fig 15 is considered the location of the injection and production wells are the top left and the bottom right corners respectively the simulation time is 20 days all other simulation parameters remain unchanged fig 16 shows the homogenized permeability values at two different levels due to the many high contrast channels more active cells are employed compared with the spe top layer as shown in fig 17 the saturation and pressure maps at the final time step are shown in fig 18 and fig 19 respectively similar to the previous test cases fig 20 compares the error between the two adm approaches moreover in fig 21 the percentage of active grid cells per each time step is shown fig 22 illustrates the average values of the errors in the pressure and the saturation and the percentage of the active grid cells for each coarsening criterion threshold the results indicate a noticeable difference in the errors of adm ms and adm ho the pressure error in adm ho is significantly higher since adm ho uses homogenized effective parameters this aspect can be improved by employing first order corrections however such an approach would deviate from the adm framework and requires more computational effort therefore it is not adopted here adm ms instead employs multiscale basis functions due to the more accurate pressure calculations the adm ms saturation error is also lower than that of adm ho the difference in the percentage of active grid cells used in the two approaches is less noticeable than the difference in the errors however the adm ho uses more active grid cells especially in this spe10 bottom layer test case 5 conclusion homogenization and multiscale methods have been developed and evolved during the past decade as promising advanced simulation approaches for large scale heterogeneous systems in this work the two methods were investigated extended into a unified fully implicit framework and benchmarked for simulation of multiphase flow in porous media it was shown that the two methods allow the construction of coarser level systems and both rely on local solutions to find their corresponding maps while homogenization methods deliver effective models and parameters multiscale methods find an interpolation of the solution pressure across scales this is the main difference between the two approaches for highly heterogeneous test cases it was shown that the two approaches provide accurate solutions with the developed multiscale numerical strategies the adm ms solutions are more accurate when compared to adm ho the use of a constant effective parameter instead of local multiscale basis functions results in relatively higher errors in addition using constant unity prolongation operator along with the effective coarse scale parameters allows for straightforward implementation of the adm ho method for domains with non periodic permeability fields the study of this paper sheds new light on the application of multiscale and homogenization methods for real field simulation of multiphase flow in porous media note that the computational costs of the two approaches were comparable as they applied almost the same active cells during the simulation ongoing study includes benchmark studies of adm ho and adm ms for 3d fractured porous media on compilable simulation platform which allows scientific cpu comparison study credit authorship contribution statement hadi hajibeygi conceptualization supervision writing original draft manuela bastidas olivares methodology software writing original draft mousa hosseinimehr methodology software writing original draft sorin pop conceptualization writing review editing supervision mary wheeler conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements hadi hajibeygi was sponsored through the dutch science foundation nwo grant 17509 under innovational research incentives scheme vidi project admire sorin pop and manuela bastidas acknowledge the research foundation flanders fwo through the odysseus program project gog1316n all authors acknowledge t u delft darsim group members for the fruitful discussions specially matteo cusini and jeroen rijntjes for their help regarding the darsim2 simulator darsim2 open source simulator can be accessed via https gitlab com darsim2simulator link appendix a adm based on multiscale method in the adm ms approach multiscale finite volume method msfv jenny et al 2003 cortinovis and jenny 2014 is used to compute local basis functions at multiple coarsening levels the computation of basis functions φ is done by solving the incompressible fluid flow equation elliptic part of the mass balance cortinovis and jenny 2017 which reads 15 λ φ 0 the incompressible basis functions are found to be the most efficient ones compared with the compressible and more complex formulations tene et al 2015 the first step is to impose coarse grids on top of the fine mesh for the coarse level 1 here to simplify the visualization a 2d 15 15 discrete domain is considered see fig 23 by connecting the centers of the coarse cells the dual coarse grid is obtained the dual grid makes an overlapping partitioning of the fine scale domain with 3 categories of interior white edge green and vertex blue cells the coarsening ratio in the illustrated example of fig 23 is 5 5 the eq 15 is solved at each dual coarse grid h and for each coarse node vertex k i e λ φ k h 0 in order to solve this local system dirichlet boundary conditions of 1 0 for the corresponding coarse node and 0 0 for the other three coarse nodes are imposed these dirichlet values allow to solve the basis functions on the edges if a reduced dimensional 1d elliptic problem is considered the solution at the edge and vertex cells are then imposed as dirichlet boundary condition for the full 2d problem the solution of this well posed system is the basis function of the corresponding coarse node at the corresponding dual coarse grid fig 24 shows a schematic of the mentioned dual coarse grid h and an example of a basis function belonging to the bottom left coarse node φ 1 h fig 25 shows all the four basis functions for the mentioned dual coarse grid h the combination of the basis functions at all the dual coarse grid cells surrounding the corresponding coarse node forms the basis function belonging to that coarse node fig 26 illustrates an example of a basis function belonging to the bottom left coarse node of an example heterogeneous 2d domai to obtain the basis functions at higher coarsening levels the hierarchically nested coarse grid is constructed on the same domain the same procedure is followed to compute the basis functions at higher coarsening levels fig 27 shows the coarse grid construction at 2 consequent coarsening levels for a 2d domain with 75 75 fine cells note that according to the vast multiscale literature construction of basis functions can be done purely algebraic once the wire basket decomposition of the fine cells into vertex edge face and interior is known tene et al 2016 a partitioning method should be applied for complex mesh møyner and lie 2016 parramore et al 2016 shah et al 2016 gulbransen et al 2010 bosma et al 2017 mehrdoost 2019 mehrdoost and bahrainian 2016 appendix b adm based on homogenization theory the main idea of the adm ho is to use a homogenized version of the permeability k instead of volume averaged permeabilities at different coarse levels in doing so two assumptions are commonly made the permeability is periodic at each of the coarsening levels and the scales are well separated we refer to allaire 1992 for the rigorous mathematical support of this approach although the test cases considered here do not satisfy the two assumptions stated before the homogenization idea can still be considered for developing multiscale simulation tools and in this sense we refer to bastidas et al 2019 amanbek et al 2019a singh et al 2019a amanbek et al 2019b singh et al 2019b at each coarsening level l we call micro scale cell i e local coarse cell the region ω l wherein the parameters change rapidly for each ω l the characteristic length is ℓ where l is the characteristic length for the macro scale domain ω the factor ε ℓ l reflects the scale separation to identify the fast changes in the parameters we double the variables and define the fast variable y x ε in the non dimensional setting ω can be written as the finite union of the local cells ω l we let ω i ω l for some set of indices i for calculating the homogenized permeability we consider an auxiliary elliptic problem auxiliary problem given a fine scale permeability k find a function u ϵ that satisfies 16 k u ϵ 0 in ω u ϵ 0 on ω here the boundary conditions are specified for completeness we employ the homogenization ansatz meaning that the unknown u ϵ can be written as 17 u ϵ x u 0 ϵ u 1 ϵ 2 u 2 where each u i x y is periodic due to the doubling of variables the gradient and divergence operators become x 1 ϵ y and div div x 1 ϵ div y inserting 17 and the two scale operators above in the auxiliary problem 16 one gets div x 1 ϵ div y x 1 ϵ y u 0 ϵ u 1 o ϵ 2 0 u 0 ϵ u 1 o ϵ 2 0 collecting the terms with factor ϵ 2 we obtain for each domain ω l 18 y k u 0 0 for all y ω l clearly any u 0 u 0 x which does not depend on the fast variable y is a solution of the problem 18 thus one can prove that all solutions depend only on x the function u 0 is in fact the macro scale approximation of the original unknown u ϵ on the other hand for the ϵ 1 terms one has y k x u 0 y u 1 0 for all y y one can determine u 1 as a function of u 0 and eliminate it from the system to this end u 1 is written as a linear combination of functions ωj and with u 0 x j as coefficients u 1 x y j 1 dim u 0 x x j ω j x y the functions ωj are solutions of the micro cell local domain problems defined on each ω l y a w y j e j 0 for all y y w j is periodic in y here e j j 1 d is the canonical basis of dimension 2 to guarantee the uniqueness of the solution one requires that ω l ω j 0 d y for all ω l the homogenized permeability in the auxiliary problem is obtained by considering the terms of order ϵ0 in which u 2 appears averaging these terms and using the periodicity of the functions on the right hand side of 17 one obtains that the auxiliary unknown u 0 x solves the homogenized problem 19 k l u 0 0 in ω u 0 0 on ω here the matrix valued function k l ω r 2 2 has the elements k i j l ω l ω l k e j y ω j e i d y note that these steps are carried out only to determine the effective permeability tensor k l the solution u 0 of the effective problem 19 is not of interest here in other words we use the auxiliary problem for the sole purpose of defining an effective parameter that could be used at each level of the dynamic multilevel algorithm supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103674 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
436,accurate simulation of multiphase flow in subsurface formations is challenging as the formations span large length scales km with high resolution heterogeneous properties to deal with this challenge different multiscale methods have been developed such methods construct coarse scale systems based on a given high resolution fine scale system furthermore they are amenable to parallel computing and allow for a posteriori error control the multiscale methods differ from each other in the way the transition between the different scales is made multiscale finite element and finite volume methods compute local basis functions to map the solutions e g pressure between coarse and fine scales instead homogenization methods solve local periodic problems to determine effective models and parameters e g permeability at a coarser scale it is yet unknown how these two methods compare with each other especially when applied to complex geological formations with no clear scale separation in the property fields this paper develops the first comparison benchmark study of these two methods and extends their applicability to fully implicit simulations using the algebraic dynamic multilevel adm method at each time step on the given fine scale mesh and based on an error analysis the fully implicit system is solved on a dynamic multilevel grid the entries of this system are obtained by using multiscale local basis functions adm ms and respectively by homogenization over local domains adm ho both sets of local basis functions adm ms and local effective parameters adm ho are computed at the beginning of the simulation with no further updates during the multiphase flow simulation the two methods are extended and implemented in the same open source darsim2 simulator https gitlab com darsim2simulator to provide fair quality comparisons the results reveal insightful understanding of the two approaches and qualitatively benchmark their performance it is re emphasized that the test cases considered here include permeability fields with no clear scale separation the development of this paper sheds new lights on advanced multiscale methods for simulation of coupled processes in porous media keywords multiscale homogenization algebraic dynamic multilevel adaptive mesh refinement flow in porous media fully implicit simulation 1 introduction geological formations span large km length scales having heterogeneous properties characterized at high resolutions cm and below as for the uncertainty within the integrated field data typically several equiprobable realizations of the property fields are generated to study and simulate the fluid flow and transport classical simulation approaches are too expensive for such studies therefore advanced simulation methods are required to allow for an accurate representation of the heterogeneous properties at the same time they should provide an efficient simulation framework to study multiple realizations jansen et al 2009 wachspress 1966 model order reduction techniques have been developed to provide a meaningful approximate simulation framework such techniques have to be fast enough to be applied to large scale computational domains in this sense any advanced method of this type can be seen as field applicable only if it allows for reducing the error below any desired threshold value hajibeygi et al 2012 here we only consider numerical model order reduction techniques among which multiscale efendiev and hou 2009 hou and wu 1997 and homogenization weinan 2011 methods stand very promising these approaches are different in the sense that the multiscale method deals with crossing the solution e g the pressure across the scales aarnes and hou 2002 jenny et al 2003 hajibeygi et al 2008 chung et al 2015 whereas in the latter effective lower resolution parameters and functions like the permeability or the transmissibility are derived weinan and yue 2004 abdulle et al 2012 weinan et al 2007 li et al 2020 singh and wheeler 2018 vasilyeva et al 2020 moreover while the multiscale basis functions have been expressed in a purely algebraic formulation wang et al 2014 the same does not hold for the homogenization approach specially the integration of homogenized parameters within the fully implicit framework in an algebraic manner has not yet been developed so far the present work is a first step in this direction at the same time the two methods have many similarities both find their mapping strategy via local solutions of the original governing equations with local boundary conditions multiscale basis functions often employ reduced dimensional boundary conditions tene et al 2015 møyner and lie 2016 while homogenization schemes impose periodic boundary conditions on local problems and consider local representative micro structures even in the case of non periodic properties allaire 1992 abdulle and weinan 2003 arbogast and xiao 2013 bastidas et al 2019 brown et al 2013 both methods are effective for global equations within the fully coupled system of local global unknowns e g the global pressure and the local saturation both have been extended to nonlinear and geologically complex models amanbek et al 2019a hosseinimehr et al 2018 singh et al 2019a recent developments of these two classes of approaches have introduced a fully implicit dynamic multilevel simulation framework adm in which heterogeneous detailed geo models are mapped into adaptive dynamic coarser mesh cusini et al 2018 faigle et al 2014 klemetsdal et al 2020 carciopolo et al 2020 the adm method develops a fully implicit discrete system for coupled flow and transport system of equations in which each equation can be represented at a different resolution than the defined fine scale one more importantly the procedure can be done fully algebraic with the dynamic mesh resolution defined based on a front tracking strategy in contrast to the rich existing literature of adaptive mesh refinement amr methods pau et al 2009 2012 berger and oliger 1984 schmidt and jacobs 1988 edwards 1996 sammon 2003 klemetsdal and lie 2020 adm can be defined as an adaptive mesh coarsening strategy which is conveniently applicable for heterogeneous and nonlinear coupled systems cusini et al 2016 irrespective of the choice of the dynamic mesh strategy it is always a challenge to construct adaptive multiscale entries of the implicit systems the adm method so far has included multiscale basis functions cusini et al 2016 in addition homogenization methods have also been developed for multiphase simulations on dynamic grids amanbek et al 2019a cusini et al 2019 in this context two aspects can be of interest the study of the homogenization based coarser system entries and the development of a benchmark study of the quality of the two approaches of adm multiscale adm ms and adm homogenized adm ho for coupled implicit multiphase flow scenarios this paper develops such a unified framework in which the adm method is extended to account for both multiscale and homogenization schemes for multiphase flow simulations this development makes it possible to allow for different coarse scale entries for dynamic simulations and importantly to benchmark the two classes of multiscale and homogenization strategies noteworthy is that once the effective parameters are computed all other homogenization procedures are implemented algebraically this is done by introducing constant unity local basis with the support of primal non overlapping coarse scale partitions the multiscale adm is implemented fully algebraic since local basis functions are also solved algebraically over the overlapping dual coarse grid domains zhou and tchelepi 2012 the outcome of this development is made available to the public via an open source darsim2 simulator https gitlab com darsim2simulator numerical test cases are considered for the challenging highly heterogeneous spe10 christie and blunt 2001 the number of active grid cells pressure and saturation errors and the solution maps are all reported in detail the development of this paper sheds new lights in the application of multiscale and homogenization approaches in advanced next generation environments for field relevant simulation scenarios the paper is structured as follows next in section 2 the mathematical model is stated briefly section 3 presents the computational framework for both multiscale and homogenization adm methods section 4 presents the test cases and conclusions are drawn in section 5 the appendix gives more details on the multiscale and the homogenization approaches 2 governing equations we consider flow of two immiscible and incompressible phases of α and β through a heterogeneous porous medium at the darcy scale mass balance for the phase i α β reads 1 t ϕ ρ i s i ρ i λ i p ρ i g z ρ i q i here ϕ is the porosity of the medium ρi kg m3 and si are the density and saturation of the phase i respectively the phase mobility tensor λi is equal to k k r i μ i where k m2 is the rock absolute permeability tensor and k r i k r i s i is the saturation dependent relative permeability of phase i moreover μ pa s is the phase viscosity for the ease of presentation the two phase pressures are assumed equal p p α p β pa see e g aziz and settari 2002 however the extension to models involving a saturation dependent capillary pressure is also possible in addition g m s2 is the gravitational acceleration which acts on z direction and q 1 s is the phase source term here it is assumed that the two fluids are occupying completely the pore space and no other fluid phase is present this gives the constraint s α s β 1 which reduces the number of unknowns in the above equations to two sα in short from here on s and p finally the model is completed by initial conditions for the saturation and with boundary conditions we do not specify them explicitly since none of them play a role in the multiscale strategy the fully implicit coupled simulation approach aziz and settari 2002 estimates all the parameters at next time step n 1 as such the semi discrete nonlinear residual for the phase i α β reads 2 r i n 1 ρ i q i n 1 ϕ ρ i s i n 1 ϕ ρ i s i n δ t ρ i λ i p ρ i g z n 1 for finding the solution pair p n 1 s n 1 one needs to employ a linearization scheme here we restrict the discussion to the newton scheme which is 2nd order convergent but requires a starting point that is close enough to the solution in other words the time step may be subject to restrictions also depending on the mesh size alternatively one may consider approaches like the modified picard celia et al 1990 or the l scheme radu et al 2017 which are less demanding from the computational point of view or more robust w r t the starting point and mesh resolution but converge slower than the newton scheme bastidas et al 2019 applied to 2 the newton linearization reads 3 r n 1 r ν r p ν δ p ν 1 r s ν δ s ν 1 which can be expressed algebraically as j ν δ x ν 1 r ν i e 4 r α p r α s r β p r β s j ν δ p δ s δ x ν 1 r α r β r ν in each time step the linear eq 4 is solved iteratively inner loop several times until nonlinear convergence outer loop is reached the overall computational complexity of the simulation depends highly on the complexity of the solution of this linear system advanced multiscale and homogenization methods aim at solving this linear system on a dynamic multilevel mesh note that as shown before cusini et al 2018 the overall efficiency of any advanced method should include not only the speedup of solving the linear eq 4 but also the count of the newton outer loops next the adm method based on multiscale and homogenization formulations is presented 3 dynamic multilevel simulation based on multiscale and homogenization methods 3 1 adm framework formulation the fully implicit linear system 4 is too expensive to be solved for real field scenarios a multilevel dynamic mesh as shown in fig 1 is generated within the adm framework based on an error estimate strategy the error estimate is developed based on a front tracking criterion which applies fine scale grids only at sub regions with sharp gradients the fine scale system is then algebraically reduced into this multilevel grid through sequences of restriction and prolongation operators to obtain the adm grid first sets of n l n x l n y l hierarchically nested coarse grids are imposed on the fine mesh here l indicates the coarsening level moreover γl is the coarsening ratio which is defined as 5 γ l γ x l γ y l n x l 1 n x l n y l 1 n y l for two dimensional 2d domains the adm grid is constructed by assembling a combination of cells at different resolutions within the computational domain by using the sequence of restriction r and prolongation p operators one can express the adm system as 6 r l l 1 r 1 0 j 0 p 0 1 p l 1 l j adm δ x adm r l l 1 r 1 0 r 0 r adm here r l l 1 is the restriction operator which maps the parts of the solution vector that are at level l 1 to level l similarly the prolongation operator p l 1 l maps the parts of the solution vector that are at level l to level l 1 once the adm system 6 is solved the approximated fine scale solution δ x 0 can be acquired by prolonging the adm solution δ x adm i e 7 δ x 0 δ x 0 p 0 1 p l 1 l δ x adm the adm restriction r l l 1 and prolongation p l 1 l operators are assembled using the static multilevel multiscale restriction r l l 1 and prolongation p l 1 l operators respectively they are constructed only at the beginning of the simulation and are kept unchanged throughout the entire simulation the static prolongation operator p l 1 l is constructed as an assembly of the locally computed basis functions at each coarsening level l and reads 8 p l 1 l p p l 1 l 0 0 p s l 1 l n l 1 n l here p p l 1 l and p s l 1 l are the two main diagonal blocks corresponding to main unknowns i e pressure p and saturation s in the case of using the homogenization scheme i e adm ho as will be described in section 3 3 constant basis functions for pressure are used however for the multiscale based adm i e adm ms as will be described in section 3 2 locally computed basis functions are used note that the saturation prolongation operator for both approaches is constant unity function at all coarsening levels which represents the conservative finite volume integration the static restriction operator r l l 1 reads 9 r l l 1 r l l 1 0 0 r l l 1 n l n l 1 in this work a finite volume restriction operator is used to guarantee local mass conservation i e 10 r l l 1 i j 1 if cell i is inside coarser cell j 0 otherwise 3 2 adm using multiscale adm ms in the adm ms method the prolongation operator for pressure is found based on multiscale basis functions these local basis functions are computed algebraically wang et al 2014 based on the steady state pressure equation in this study the incompressible flow equation elliptic pressure equation is used to construct the multiscale basis functions tene et al 2015 an example of a basis function is shown in fig 2 the coarse grid construction and computation of multiscale basis functions are explained in more details in appendix a 3 3 adm using homogenization adm ho homogenization is another method that can be applied to problems involving multiple scales in this method one uses the mathematical models at micro fine scale 1 to derive effective upscaled models and parameters in which the rapidly oscillating characterstics are averaged out in doing so the upscaled model may have a different structure than the ones at the fine scale we refer to amaziane et al 2017 bourgeat et al 1996 hornung 1997 van duijn et al 2007 for theoretical details the goal of this work is to build a unified adm platform where the multiscale and homogenization methods can be compared therefore here the homogenization method is used only to construct effective properties at the dynamic multilevel mesh in this setup the homogenized properties of adm ho at multilevel mesh are found as in adm ms by solving local flow pressure equations based on an incompressible elliptic equation more precisely one assumes that a scale separation holds and doubles the spatial variable into a fast and a slow one the method relies on the homogenization ansatz meaning that all quantities in 1 can be expanded regularly in terms of a scale separation parameter such ideas are employed in bastidas et al 2019 abdulle and nonnenmacher 2009 amanbek et al 2019b amaziane et al 1991 singh et al 2019b szymkiewicz et al 2011 henning et al 2015 2013 to develop effective numerical simulation schemes even in case of non periodic media more details about the homogenization procedure can be found in appendix b in the present context for a given fine scale effective permeability k and for each coarsening level l an effective permeability tensor k l is computed locally in a pre processing step first the domain ω r 2 is divided into coarse cells ω l that correspond to a partition of the domain ω as shown in fig 3 for each coarse cell ω l at level l the components of the effective permeability tensor are calculate as 11 k i j l ω l ω l k e j ω j e i d y for i j 1 2 here ωj are the periodic solutions of the pressure equation on local domains known as micro cell equation in ho literature i e 12 k y ω j e j 0 for all y ω l we remark that e j j 1 2 is the canonical basis of dimension 2 and k is the above mentioned permeability tensor to guarantee the uniqueness of the solution ωj one assumes that its average value over the local coarse cell ω l is 0 to determine the value of the effective permeability tensor at each coarse cell ω l two local micro cell problems 12 are solved for each spatial direction in 2d fig 4 provides an illustration of these local solutions for a coarse element note that the local problems 12 capture the rapidly oscillating characteristics within a coarse element completely decoupled from other coarse elements the homogenized parameters like multiscale bases are computed at the beginning of the simulation fig 5 illustrates the calculation of the effective permeability at different levels the homogenized parameters are used to construct the coarse system entries more precisely the homogenized value in a coarse cell is distributed equally to the fine cells constructing it then the fine scale jacobian and residual are computed with the fine scale saturation field this system is then mapped to the adm resolution by setting prolongation operators in 6 to unity this is a convenient procedure developed in this work to integrate the numerical homogenization method with an existing advanced simulator notice that based on the features of the permeability tensor k the resulting effective parameter k may depend on the macro scale location and the size of the coarse scale partition nevertheless one can show that in practice the adaptive refinement of the mesh is an important aspect that improve the calculation of the effective parameters see bastidas et al 2019 and fig 5 the adm procedure is sketched in fig 6 more details about the role of the homogenization in the offline stage and the complete algorithm of adm ms and adm ho can be found in algorithm 1 4 simulation results to benchmark the homogenization and multiscale based solutions for the dynamic mesh on heterogeneous media two heterogeneous non periodic permeability fields from the top and bottom layers of the spe 10th comparative solution project christie and blunt 2001 are considered for both test cases the computational domain entails 216 54 grid cells at fine scale with δ x δ y 1 m a no flow condition is imposed on all boundaries initially the reservoir contains only the 2nd phase e g oil i e s 0 the 1st phase e g water is injected from an injection well while the reservoir fluid is produced from the production well the locations of the injection and production wells are specified in each test case table 1 shows the input parameters of the fluid and rock properties used in all test cases note that the density and viscosity ratios are assumed to be 1 since compressibility and gravitational forces are both neglected the density values have no influence on the results the numerical results provided by the adm ms and adm ho methods are compared to those obtained from simulation at fine scale reference both adm methods employ the coarsening ratio of 3 3 with two coarsening levels this is set according to the size of the domain 4 1 test case 1 spe10 top layer in this test case one injection well and one production well are placed in the bottom left corner and top right corner of the domain respectively the simulation time is t 1000 days and the results are reported on 100 equidistant time intervals the permeability distribution of the spe10 top layer is shown in fig 7 fig 8 shows the homogenized version of the permeability at two different levels we highlight that the homogenized permeability at both coarse levels preserves the structure of the original fine scale permeability the high and low permeable zones remain clearly detectable the saturation and pressure fields at the final time step are shown in fig 9 and fig 10 respectively from these results it is understood that adm ho on a coarse cell containing high and low permeable fine cells can lead to a higher flow leakage as compared to fine scale and adm ms approaches this effect can be seen in fig 9 fig 11 illustrating the adaptive mesh at 2000 days after injection notice that the refinement of the permeability is most dominant at the saturation front due to the chosen mesh refinement criterion for this figure the coarsening threshold value is δ s 0 3 i e a cell is successively coarsened if δs is lower than 0 3 the error history maps for both adm ms and adm ho are shown in fig 12 the relative errors presented in fig 12 and fig 14 are expressed in terms of the l 2 norm over the entire medium calculated with respect to the fine scale solution as 13 e r r o r s s ref s adm 2 s ref 2 14 e r r o r p p ref p adm 2 p ref 2 the results indicate that the homogenization based simulations have higher errors compared with the multiscale based simulations they both have similar average usage of active grid cells with adm ms having slightly fewer grid cells this is shown in fig 13 note that the grid cells around wells are kept at the fine scale resolution permanently furthermore for tighter error tolerance values the quality of both approaches become comparable fig 14 provides the average pressure and saturation errors together with the average percentage of active grid cells during the whole simulation time as functions of the coarsening criterion threshold 4 2 test case 2 spe10 bottom layer in the second test case the permeability distribution of the spe10 bottom layer presented in fig 15 is considered the location of the injection and production wells are the top left and the bottom right corners respectively the simulation time is 20 days all other simulation parameters remain unchanged fig 16 shows the homogenized permeability values at two different levels due to the many high contrast channels more active cells are employed compared with the spe top layer as shown in fig 17 the saturation and pressure maps at the final time step are shown in fig 18 and fig 19 respectively similar to the previous test cases fig 20 compares the error between the two adm approaches moreover in fig 21 the percentage of active grid cells per each time step is shown fig 22 illustrates the average values of the errors in the pressure and the saturation and the percentage of the active grid cells for each coarsening criterion threshold the results indicate a noticeable difference in the errors of adm ms and adm ho the pressure error in adm ho is significantly higher since adm ho uses homogenized effective parameters this aspect can be improved by employing first order corrections however such an approach would deviate from the adm framework and requires more computational effort therefore it is not adopted here adm ms instead employs multiscale basis functions due to the more accurate pressure calculations the adm ms saturation error is also lower than that of adm ho the difference in the percentage of active grid cells used in the two approaches is less noticeable than the difference in the errors however the adm ho uses more active grid cells especially in this spe10 bottom layer test case 5 conclusion homogenization and multiscale methods have been developed and evolved during the past decade as promising advanced simulation approaches for large scale heterogeneous systems in this work the two methods were investigated extended into a unified fully implicit framework and benchmarked for simulation of multiphase flow in porous media it was shown that the two methods allow the construction of coarser level systems and both rely on local solutions to find their corresponding maps while homogenization methods deliver effective models and parameters multiscale methods find an interpolation of the solution pressure across scales this is the main difference between the two approaches for highly heterogeneous test cases it was shown that the two approaches provide accurate solutions with the developed multiscale numerical strategies the adm ms solutions are more accurate when compared to adm ho the use of a constant effective parameter instead of local multiscale basis functions results in relatively higher errors in addition using constant unity prolongation operator along with the effective coarse scale parameters allows for straightforward implementation of the adm ho method for domains with non periodic permeability fields the study of this paper sheds new light on the application of multiscale and homogenization methods for real field simulation of multiphase flow in porous media note that the computational costs of the two approaches were comparable as they applied almost the same active cells during the simulation ongoing study includes benchmark studies of adm ho and adm ms for 3d fractured porous media on compilable simulation platform which allows scientific cpu comparison study credit authorship contribution statement hadi hajibeygi conceptualization supervision writing original draft manuela bastidas olivares methodology software writing original draft mousa hosseinimehr methodology software writing original draft sorin pop conceptualization writing review editing supervision mary wheeler conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements hadi hajibeygi was sponsored through the dutch science foundation nwo grant 17509 under innovational research incentives scheme vidi project admire sorin pop and manuela bastidas acknowledge the research foundation flanders fwo through the odysseus program project gog1316n all authors acknowledge t u delft darsim group members for the fruitful discussions specially matteo cusini and jeroen rijntjes for their help regarding the darsim2 simulator darsim2 open source simulator can be accessed via https gitlab com darsim2simulator link appendix a adm based on multiscale method in the adm ms approach multiscale finite volume method msfv jenny et al 2003 cortinovis and jenny 2014 is used to compute local basis functions at multiple coarsening levels the computation of basis functions φ is done by solving the incompressible fluid flow equation elliptic part of the mass balance cortinovis and jenny 2017 which reads 15 λ φ 0 the incompressible basis functions are found to be the most efficient ones compared with the compressible and more complex formulations tene et al 2015 the first step is to impose coarse grids on top of the fine mesh for the coarse level 1 here to simplify the visualization a 2d 15 15 discrete domain is considered see fig 23 by connecting the centers of the coarse cells the dual coarse grid is obtained the dual grid makes an overlapping partitioning of the fine scale domain with 3 categories of interior white edge green and vertex blue cells the coarsening ratio in the illustrated example of fig 23 is 5 5 the eq 15 is solved at each dual coarse grid h and for each coarse node vertex k i e λ φ k h 0 in order to solve this local system dirichlet boundary conditions of 1 0 for the corresponding coarse node and 0 0 for the other three coarse nodes are imposed these dirichlet values allow to solve the basis functions on the edges if a reduced dimensional 1d elliptic problem is considered the solution at the edge and vertex cells are then imposed as dirichlet boundary condition for the full 2d problem the solution of this well posed system is the basis function of the corresponding coarse node at the corresponding dual coarse grid fig 24 shows a schematic of the mentioned dual coarse grid h and an example of a basis function belonging to the bottom left coarse node φ 1 h fig 25 shows all the four basis functions for the mentioned dual coarse grid h the combination of the basis functions at all the dual coarse grid cells surrounding the corresponding coarse node forms the basis function belonging to that coarse node fig 26 illustrates an example of a basis function belonging to the bottom left coarse node of an example heterogeneous 2d domai to obtain the basis functions at higher coarsening levels the hierarchically nested coarse grid is constructed on the same domain the same procedure is followed to compute the basis functions at higher coarsening levels fig 27 shows the coarse grid construction at 2 consequent coarsening levels for a 2d domain with 75 75 fine cells note that according to the vast multiscale literature construction of basis functions can be done purely algebraic once the wire basket decomposition of the fine cells into vertex edge face and interior is known tene et al 2016 a partitioning method should be applied for complex mesh møyner and lie 2016 parramore et al 2016 shah et al 2016 gulbransen et al 2010 bosma et al 2017 mehrdoost 2019 mehrdoost and bahrainian 2016 appendix b adm based on homogenization theory the main idea of the adm ho is to use a homogenized version of the permeability k instead of volume averaged permeabilities at different coarse levels in doing so two assumptions are commonly made the permeability is periodic at each of the coarsening levels and the scales are well separated we refer to allaire 1992 for the rigorous mathematical support of this approach although the test cases considered here do not satisfy the two assumptions stated before the homogenization idea can still be considered for developing multiscale simulation tools and in this sense we refer to bastidas et al 2019 amanbek et al 2019a singh et al 2019a amanbek et al 2019b singh et al 2019b at each coarsening level l we call micro scale cell i e local coarse cell the region ω l wherein the parameters change rapidly for each ω l the characteristic length is ℓ where l is the characteristic length for the macro scale domain ω the factor ε ℓ l reflects the scale separation to identify the fast changes in the parameters we double the variables and define the fast variable y x ε in the non dimensional setting ω can be written as the finite union of the local cells ω l we let ω i ω l for some set of indices i for calculating the homogenized permeability we consider an auxiliary elliptic problem auxiliary problem given a fine scale permeability k find a function u ϵ that satisfies 16 k u ϵ 0 in ω u ϵ 0 on ω here the boundary conditions are specified for completeness we employ the homogenization ansatz meaning that the unknown u ϵ can be written as 17 u ϵ x u 0 ϵ u 1 ϵ 2 u 2 where each u i x y is periodic due to the doubling of variables the gradient and divergence operators become x 1 ϵ y and div div x 1 ϵ div y inserting 17 and the two scale operators above in the auxiliary problem 16 one gets div x 1 ϵ div y x 1 ϵ y u 0 ϵ u 1 o ϵ 2 0 u 0 ϵ u 1 o ϵ 2 0 collecting the terms with factor ϵ 2 we obtain for each domain ω l 18 y k u 0 0 for all y ω l clearly any u 0 u 0 x which does not depend on the fast variable y is a solution of the problem 18 thus one can prove that all solutions depend only on x the function u 0 is in fact the macro scale approximation of the original unknown u ϵ on the other hand for the ϵ 1 terms one has y k x u 0 y u 1 0 for all y y one can determine u 1 as a function of u 0 and eliminate it from the system to this end u 1 is written as a linear combination of functions ωj and with u 0 x j as coefficients u 1 x y j 1 dim u 0 x x j ω j x y the functions ωj are solutions of the micro cell local domain problems defined on each ω l y a w y j e j 0 for all y y w j is periodic in y here e j j 1 d is the canonical basis of dimension 2 to guarantee the uniqueness of the solution one requires that ω l ω j 0 d y for all ω l the homogenized permeability in the auxiliary problem is obtained by considering the terms of order ϵ0 in which u 2 appears averaging these terms and using the periodicity of the functions on the right hand side of 17 one obtains that the auxiliary unknown u 0 x solves the homogenized problem 19 k l u 0 0 in ω u 0 0 on ω here the matrix valued function k l ω r 2 2 has the elements k i j l ω l ω l k e j y ω j e i d y note that these steps are carried out only to determine the effective permeability tensor k l the solution u 0 of the effective problem 19 is not of interest here in other words we use the auxiliary problem for the sole purpose of defining an effective parameter that could be used at each level of the dynamic multilevel algorithm supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103674 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
437,vegetation is a key source of flow resistance in natural channels and floodplains it is therefore important to accurately model the flow resistance to inform decision makers and managers however it is challenging to predict the resistance of real vegetation because vegetation models are based on relatively small scale lab experiments with mostly artificial vegetation experimental tests of real vegetation under field conditions are scarce the purpose of this study is to measure the flow resistance of a submerged willow patch where small herbaceous vegetation was allowed to grow in between the willow stems to simulate field conditions detailed flow velocity measurements were performed during an full scale experiment of flow around a submerged patch of willows the parameter values of the willow vegetation model as well as the friction coefficients of the vegetated banks and unvegetated channel bed were computed simultaneously using bayesian inference using a 2d hydrodynamic model results show that the presence of understory growth greatly affects flow patterns and the value of the effective vegetation density parameter measured flow velocities in the patch with understory growth were very low and the patch has relatively high deflection after removal of this undergrowth flow velocities in the patch increased and deflection of the vegetation canopy decreased we show that estimating vegetation density using an often used rigid cylinder estimator based on vegetation sampling underestimated the effective value by more than an order of magnitude we argue that proposed extensions to existing vegetation models which can take into account understory growth and reconfiguration could be tested under field conditions using the approach followed in this paper keywords natural vegetation flow experiment vegetation roughness bayesian inference hydraulic modelling 1 introduction vegetation in rivers and streams is one of the largest sources of flow resistance luhar and nepf 2013 and an important source of uncertainty in hydrodynamic models used for flood management warmink et al 2013 and river engineering applications berends et al 2018 the presence of vegetation affects the morphological evolution of rivers van oorschot et al 2015 biodiversity straatsma et al 2017 and water quality dosskey et al 2010 therefore a good representation of vegetation in computer models is important advances in remote sensing and classification algorithms enable detailed maps of the spatial distribution of vegetation geerling et al 2009 forzieri et al 2011 hodges 2015 the local effect of vegetation on flow is commonly modelled through a friction term in the momentum equation this term also known as roughness or flow resistance often represents various ways of energy losses that are not explicitly modelled we can broadly distinguish two ways to determine vegetative friction in field situations the first approach to determine vegetative friction is based on look up tables or vegetation models the table of typical manning type friction values by chow 1959 are still used to characterise the roughness of streams based on images or descriptions of the vegetation e g light brush and trees in winter look up tables are a convenient way to couple remote sensing to hydrodynamic modelling e g forzieri et al 2011 however despite the widespread use of the manning coefficient as a lumped parameter to characterise describe roughness the implicit assumption of the manning equation for a logarithmic velocity profile does not hold for vegetation naden et al 2006 ferguson 2007 for this reason various semi empirical models have been developed that compute the contribution for vegetation on flow resistance based on vegetation parameters such as stem count and plant morphology klopstra et al 1997 baptist et al 2007 huthoff et al 2007 yang and choi 2010 li et al 2015 it has been shown that such models perform well against a large body of data collected from laboratory experiments vargas luna et al 2015 further studies have been carried out to better capture real world vegetation dynamics such as reconfiguration järvelä 2004 dijkstra and uittenbogaard 2010 verschoren et al 2016 and contribution of foliage bal et al 2011 västilä and järvelä 2014 in practice the vegetation parameters necessary to use these formulas should be either directly measured or taken from a look up table successful application of this approach to field conditions depends on the validity of the vegetation model and the availability of data on vegetation the second approach to determine vegetative friction is through inverse modelling which involves fitting the model in practice usually a subset of model parameters to measurements it is called inverse modelling because the quantity of interest is not model output but model input such as parameters in practice inverse modelling is used to find the values for look up tables because the lumped roughness can be straightforwardly computed by inverting the manning equation under the assumption of steady uniform flow a logarithmic flow profile and a known energy slope marjoribanks et al 2014 in the case of non uniform flow the inverted bélanger equation can be used instead errico et al 2018 roughness values estimated through inverse modelling are often used as the measured roughness to provide the experimental basis for vegetation models however estimating the friction in heterogeneous real world situations based on the energy slope e g a single vegetation patch which does not cover the entire channel is complicated estimating the friction of multiple sources based on a single observation the energy slope may result in non unique solutions to parameter values beven 2006 this problem is analogous to that of underdetermination in regression problems which allows an infinite number of acceptable combinations of unknown friction factors if the parameters exceed the available observations this severely limits our ability to estimate the friction values and validate vegetation models in real world situations in cases where non uniqueness is an issue probabilistic parameter estimation is preferred to deterministic optimisation matott et al 2009 guillaume et al 2019 formally this is achieved through bayesian inference which produces probability distributions of parameter values given the subjective likelihood that the model given these values confirms experimental observation a well known bayesian inference methodology is glue generalised likelihood uncertainty estimation originally developed for non uniqueness problems in hydrology beven and binley 1992 fonseca et al 2014 mayotte et al 2017 however previous work has shown that estimating more than one friction source based on water levels can result in unidentifiable parameter distributions which are characterised by wide and unconstrained shapes werner et al 2005 to increase the identifiability of parameter distributions other observational data can be used such as inundation patterns pappenberger et al 2005 hypothetically detailed velocity measurements around vegetation patches can serve a similar function to estimate flow resistance using glue however the literature on flow data around real world vegetation patches are scarce naden et al 2006 marjoribanks et al 2017 and comparisons between laboratory and field in general are rare huthoff et al 2013 groom and friedrich 2018 our aim is to investigate the flow resistance of vegetation under natural conditions specifically we are interested in the effect of secondary vegetation growing in between and under the branches of the dominant vegetation species such secondary vegetation is hard to detect remotely from satellite or drone imagery and may therefore lead to an underestimation of biomass in large scale river models that rely on ecotope maps to estimate floodplain friction straatsma and huthoff 2011 forzieri et al 2011 warmink et al 2013 van oorschot et al 2015 berends et al 2019 it is expected that understory growth increases the flow resistance but it is unknown by how much in this study we perform a stream scale experiment with real vegetation where secondary vegetation was allowed to develop naturally this physical experiment is coupled to a digital twin numerical model to compute friction parameters by bayesian inference 2 methods 2 1 physical experiment setup large scale experiments were performed at the site of the kict rec korea institute of civil engineering and building technology river experiment center which is located in the city of andong south korea this facility is designed for full scale physical experiments and consists of three separate channels of various slope and sinuosity large capacity pumps can generate the maximum flow rate up to 10 m 3 s 1 the length of each channel is approximately 600 m the experiments for vegetated flow were performed in the downstream section of the a1 channel which has a trapezoidal cross section with a bottom width of 3 m top width of 11 m bed slope of 0 001 m m and bank side slope of roughly 1 1 5 v h as shown in fig 1 the bankside slopes of the channel are vegetated with grasses and small annuals native to the region the channel bed consists of sandy materials with a mean particle size of 0 8 mm seven alternating willow patches each with a length of 4 m and width of 1 5 m were planted in the 52 m section of a1 channel which was located about 125 m upstream from the downstream weir and 400 m downstream from the upstream weir the patches were planted in two configurations the four most upstream patches in a dense configuration of 22 trees per m2 and the three downstream patches in a sparse configuration of 7 3 trees per m2 the willow saplings were allowed to grow at the site for 10 months before the experiments started in august 2015 the average height of rooted willows was 0 4 m when they were planted with an initial trunk diameter of 1 cm in addition indigenous small scale rough herbaceous vegetation and grasses were allowed to grow on the bank side slopes of the channels the bed itself was relatively unvegetated and mobile herbaceous vegetation had also spontaneously developed in between the willows in the vegetation patches although the flume was mostly dry during this in which the vegetation grew although periods of prolonged flow occurred when the experiments took place in some other part of the flume this regime of mostly dry and occasional flooding is typical for many korean streams which are ephemeral in nature after the flow experiment vegetation samples were taken to measure the height diameter and morphology of the willows the bed level in the experimental area was measured using riegl lms z390i terrestrial laser scanner the flow measurements were taken after uniform steady flow was achieved the water depth in the channel downstream was fixed to approximately 1 1 m detailed three dimensional velocity measurements with an sontek 16 mhz adv acoustic doppler velocimeter were performed in the middle and 4 m upstream of the first dense patch at d3 fig 2 the adv devices were positioned pointing downward from an overhanging bridge to get a signal within the vegetation patch the devices were slightly moved to the left or right when needed during trial convergence testing a required measurement time of 100 s was found both the discharge and the depth averaged flow velocity profile are derived from the adv measurements we gathered data from three cases i flow measurements at the upstream cross section d0 ii flow measurements in the centre of the first willow patch d3 iii and finally flow measurements at d3 but with the undergrowth removed fig 2 we will refer to these cases as d0 d3a willows and undergrowth and d3b only willows respectively for case d0 and d3a adv measurements were carried out every 30 cm for a total of 23 locations in y direction for d3b a lower resolution was used of 60 cm for a total of 12 locations in y direction in the vertical direction a measurement was carried out every 5 cm for d3a a vertical resolution of 2 5 cm was used in the vegetation patch 2 2 numerical experiment setup a digital twin of the experimental flume was constructed with the open source delft3d 4 modelling system lesser et al 2004 1 1 the source code of delft3d 4 as well as its validation documents can be retrieved from https oss deltares nl using a two dimensional regular numerical computational grid of 15 m 171 m the size of individual grid cells was 0 5 m in flow direction by 0 375 m with bed levels defined in the centre of each cell a constant discharge determined from the flume adv measurements was imposed at the upstream boundary while the downstream boundary condition was defined as a constant water level to ensure a downstream water depth of approximately 1 1 m the model was initialised with a constant water level each simulation ran for 30 min of model time with a time step of 0 3 s the run time was chosen such that the entire model achieved equilibrium condition by the last time step the scale of the model given the relatively short run time small water depths fine grid and small time step required changing some default numerical and physical parameters we used a smoothing time of 2 min and a threshold depth for flooding drying of 1 cm the horizontal eddy viscosity was held constant throughout the model at 10 2 m 2 s 1 to be able to capture the sharp velocity gradients around the patches low viscosity values were used vionnet et al 2004 verschoren et al 2016 in the experimental area the geometric data was based on the bed level measurements of the experimental setup for regions not covered by the bed level measurements a synthetic cross section was defined from the design specifications of the channel bed friction was defined using four distinct roughness classes two for the willow patches dense and sparse one for the vegetated channel slopes and one for the mobile sand channel bed each individual class is assigned its own roughness formula and friction parameters every grid cell boundary was assigned one of these four classes all friction formulas are expressed in terms of the chézy friction coefficient c m 1 2 s 1 for the channel bed we use the manning friction formula 1 c h 1 6 n b 1 with water depth h m and manning coefficient nb sm 1 3 the keulegan equation also known as the colebrook white equation was found to be best suited to reproduce the depth averaged velocity on the channel slopes 2 c α 1 log 10 α 2 h k s with nikuradse roughness height ks m and parameters α 1 18 m1 2 s 1 α 2 12 the values of both nb and ks were determined with glue there are various models available for resolving vegetation friction here we used the two layer approach of baptist et al 2007 which performs favourably against laboratory experiments vargas luna et al 2015 and is generalised as follows 3 c c b 2 ϕ 2 g 1 2 α g κ ln h h d where cb is the chézy friction coefficient of the bed without vegetation ϕ the vegetation parameter g ms 2 the gravitational acceleration κ the von karman constant hd m the deflected vegetation canopy height and α an indicator such that α 0 for emergent conditions h hd and α 1 for submerged conditions h hd the dimensionless vegetation term ϕ models the contribution of vegetation to the total friction of the vegetated part of the cross section in the original formulation of baptist et al 2007 it is computed as ϕ c d m d h v with drag coefficient cd stems per square meter m m 2 stem diameter d m and vegetation height hv m this stick model for ϕ is ideally suited for vegetation that may be approximated as rigid cylinders a fixed value for hd was used which was estimated from measurements furthermore we assumed that the friction in the patch was dominated by the vegetation and that the bed term had a neglible impact therefore we chose a constant value of cb at 60 m1 2 s 1 which led to negligible addition to the total roughness the vegetation parameter ϕ was considered unknown and estimated with glue 2 3 glue method the set of unknown parameters in the numerical experiment was given by θ n b k s ϕ i e the friction coefficients of the channel bed and slope and the vegetation parameter the problem considered here was how to choose the values for parameters in such a way that the measured flow velocities were reproduced by the flow model while an optimal set of values may be found using one of various optimisation strategies available in literature beven 2006 argued that multiple parameter sets may well be found that all produce acceptable results this creates uncertainty regarding the parameter values thus obtained through inverse modelling the added benefit of glue above regular calibration procedures is that this uncertainty is formally taken into account to achieve this glue uses bayesian inference in which usually very limited prior knowledge about the parameter values is updated given the probability that those parameter values resulted in model output that compared favourably with measurements this is technically achieved through bayes theorem 4 p θ u l u θ ϵ p θ where ϵ is the error between modelled and measured flow velocities and u measured flow velocities the prior distribution p θ that encodes our prior assumptions of probable parameter values for parameter set θ the posterior distribution p θ u is our goal because it expresses the probability of parameter values in θ after having seen the measured flow velocities u the likelihood l u θ ϵ is a function of the model θ ϵ and observations u and represents the probability of u given θ and ϵ to determine the functional form of the likelihood we needed to assume a statistical relationship between observed and modelled flow velocities here we related the measured flow velocity vector uc at cross section c to the simulated flow velocity u c y θ as follows 5 u c y u c y θ ϵ c where ϵ c is a normal and independently distributed error term with variance σ ϵ 2 i e ϵ c n 0 σ ϵ 2 the assumption of independent errors and zero bias expresses our assumption that the hydraulic model should be able to reproduce observe flow velocity profiles such that any remaining discrepancy is adequately described by a normal distribution as is commonly done in glue applications we adopted a uniform prior distribution for each parameter the upper and lower limits of each distribution were determined by exploratory computations with the model for our adopted error model 5 the corresponding likelihood function is 6 l θ u 2 π σ ϵ 2 1 2 exp u c y u c y θ 2 2 σ ϵ 2 the unknowns in the inverse problem are both the model parameters θ and the variance of the residual errors σ ϵ 2 to estimate σ ϵ 2 we used the maximum likelihood estimate σ ϵ m l e 2 n 1 i 1 n u i u i m l e 2 with n the number of observations and u i m l e the model results for the best performing parameter set stedinger et al 2008 since the likelihood function returns the probability of the observed flow velocities given the model results evaluated with a given parameter set θ model results that deviate significantly from observations will return likelihoods approaching zero the likelihood function used here was formally derived from the adopted statistical model 5 we note that glue allows for great flexibility in choosing from a variety of informal likelihood functions as well which need an additional behavioural threshold to differentiate between behavioural and non behavioural parameter sets since 6 tends to zero for very improbable parameter sets such a behavioural threshold was not needed here the posterior parameter distributions were obtained through monte carlo simulation using the sobol low discrepancy sequence sobol 1967 we sampled from the prior distributions to create a large number of possible parameter sets here we used a sample size of 5000 model evaluations with each evaluation using a different randomly sampled set of parameter values the same ensemble was used for cases d3a and d3b using the value for hd derived from case d3a afterwards the ensemble values of ϕ for d3b were corrected to account for a different height of the deflected vegetation canopy the probability distribution for the parameter vector θ was obtained by application of bayes theorem this procedure gave us two valuable sources of information on uncertainty related to model and observation first the estimate of σ ϵ 2 or the predictive uncertainty this is the residual variance between model and measurement which cannot be explained by choosing different parameter settings the second is p θ u i e the posterior parameter distribution or model uncertainty this expresses the uncertainty in the parameter values given the measurements 3 results 3 1 flow measurements based on the adv measurements the flow fields covering the vertical y z plane were constructed to compute discharge from velocity measurements it is usually necessary to assume a velocity profile e g logarithmic see boiten 2000 however due to the irregular flow profiles expected in the flow through vegetation and the density of velocity measurements we instead linearly interpolated points in between the adv support points to a regular cross section covering grid while assuming zero flow velocity at the bed fig 4b d f the total discharge was then computed by summing the product of the grid cell area we found discharges of 2 91 m3 s 1 d0 2 66 m3 s 1 d3a and 2 54 m3 s 1 d3b the differences are attributed to uncertainty in flow velocity measurements and interpolation inaccuracies the measurements at the unvegetated cross section d0 show that the depth averaged flow is low on the slopes and increases toward the centre of the channel reaching about 0 5 ms 1 fig 4a at the right hand side of the channel y 6 m a local increase in flow velocity is measured given that the high velocities were consistently measured by different adv devices we assume they are not due to instrument problems therefore we did not reject these measurements but left it to the numerical model to explain whether these measurements are expected we used the discharge at d3a as the upstream boundary condition in the numerical model two series of measurements were carried out for the cross section in the willow patch d3 one for the natural situation including organic debris and undergrowth d3a and the other for which the patch was cleaned d3b analysis of measured flow velocities for d3a revealed that one adv measurement device returned near zero results for all flow depths suggesting malfunction measurements from this device at y 6 4 m were discarded from the results similar to d0 a region of higher flow velocities was observed at the right hand side of the patch near the water surface in the patch itself both the flow field fig 4d and the depth averaged results fig 4c show slower velocities however the flow velocities near the water surface are similar to the unvegetated part of the channel suggesting submerged flow this is clear from the vertical velocity profile in the patch fig 3 which shows low flow velocities 0 3 ms 1 and large variation up to z 3 4 m above which the velocities rapidly increase to more than 0 6 ms 1 and variation is significantly reduced from fig 3 we assumed that the vegetation was deflected such that the canopy height was 0 8 m case d3b after removal of all organic debris and undergrowth broadly shows fig 4e f similar patterns to the d3a results with some key differences in the flow through the patch in the vertical profile fig 3 velocities near the bed z 3 6 m are higher compared to d3a which is attributed to removal of undergrowth for higher water depths z 3 6 m velocities show an initial decrease while the variation over the patch increases this is attributed to flow through the branches and leaves which add comparatively more resistance in contrast to d3a clear submergence is not observed therefore we assume that in the case of d3b the vegetation was just submerged meaning that the deflected canopy height was equal to the water depth 3 2 vegetation measurements after the initial flow experiment cases d0 and d3a several samples were taken to determine the height and diameter of the willows table 1 the stems were measured up until the upper knot from which point several branches sprouted we observed that organic debris which had attached itself around the willow stems increased the effective diameter of the plants by 50 on average however the distribution was not uniform some plants experienced a significantly larger increase in diameter while others had little attached debris this was reflected in the increased variance of the observations the undergrowth had an average height of 29 cm and grew in between the willow stems fig 5 and consisted of various ways of herbaceous vegetation it should be expected that both the undergrowth and the diameter increase due to organic debris increase the effective friction of the vegetation patch the expected vegetation parameter ϕ is estimated based on the baptist vegetation model ϕ c d m d h d and the vegetation measurements from table 1 to compute ϕ we assumed the branch height to be equal to the willow height minus the average stem height furthermore we assumed rigid bending at the bed following verschoren et al 2016 such that the deflected stem and branch heights can be computed from the deflected willow height see section 3 1 using standard trigonometry we assumed a case drag coefficient of c d 1 which is a common assumption for submerged flow wunder et al 2011 here we did not account for the effect of foliage the estimated vegetation parameter for case d3a i e including debris undergrowth and a deflected willow height of 0 8 m is approximately ϕ d 3 a 1 33 1 044 the large standard deviation in the expected parameter is mainly due to the variance in the diameter of the herbaceous vegetation in the undergrowth for case d3b i e no debris no undergrowth and a deflected height equal to the willow height is ϕ d 3 b 0 76 0 20 3 3 glue results an important step within glue is to determine the ranges of the uniform prior distributions these ranges should be generous since it is important that these prior distributions cover the range where the a priori unknown posterior distributions will be assuming a trapezoidal channel the lumped manning coefficient for the given discharge and water depth is approximately 0 11 sm 1 3 based on this relatively high friction factor and exploratory computations with the hydrodynamic model we chose suitably large ranges for the prior distributions and a log uniform distribution for ϕ table 2 for all three parameters nb ks ϕ we then computed the joint posterior distributions using glue the posterior distributions give the likelihood that a certain parameter value leads to good model results the width of those distributions is a measure of the parameter uncertainty regarding the possible true value of the parameters given the model and the measurements the marginal posterior distributions of all parameter values are shown in fig 6 the distribution of ϕ for d0 is very wide and does not show a clear peak fig 6a this indicates that the ϕ is an insensitive parameter for cross section d0 therefore ϕ cannot be identified from flow measurements at that cross section this was not unexpected as d0 is located 4 m upstream from the first patch and is therefore not directly influenced by the patch the other two distributions in fig 6a show the values for ϕ that lead to good model results for d3a and d3b d3a which had significant undergrowth and debris shows significantly larger inferred values for ϕ compared to d3b the median value of 101 6 39 8 is an order of magnitude higher than would be expected based plant on parameters using the baptist model for the cleaned willows case d3b the values are much lower with a median value of ϕ 1 56 this is more than twice as high as was expected based on the plant parameters the uncertainty of the estimated parameter values for ϕ is significant as can be observed by the width of the posterior distributions in fig 6a and the standard deviation in table 3 for case d3a especially the values for ϕ vary between 20 and 100 with some of the best simulations found near the upper boundary it is important to note that at such high values the sensitivity of ϕ to the roughness coefficient c is very much reduced due to the inverse square root in 3 which may partly explain why such high uncertainty is found for this case therefore values higher than the upper boundary are not expected to meaningfully improve results estimation of manning coefficient of the channel bed nb and nikuradse roughness height of the slopes ks resulted in well defined posterior distributions for all cases fig 6b c with standard deviations around 0 01 sm 1 3 table 3 interestingly the distributions are not the same for the three cases to measure the roughness of vegetation from water slope measurements the roughness of the bed is commonly assumed to be independent from the roughness of the vegetation e g verschoren et al 2016 however the results shown in fig 6 suggest that the parameter values for both nb and ks are affected by the vegetation patch for both nb and ks the presence of a patch d3a d3b results in higher parameter values therefore the assumption of independence between the parameter of the vegetation patch ϕ and those of the channel bed and slope nb and ks would not have been valid in our case modelled flow velocities given the posterior probability distributions of the parameters are compared with the depth averaged adv measurements fig 7 the depicted uncertainty bands show the range of the model uncertainty variation within the model uncertainty can be explained from uncertainty in the parameter values measurements that fall outside of these bands are explained through the residual error term ϵ in eq 5 from fig 7a we observe that the region of higher flow velocity in cross section d0 cannot be explained by the numerical model this indicates that this must be caused either by an unmodelled process or feature or by measurement error however in general the velocity profiles for all cases are well explained by the numerical model which is reflected in the small standard deviation of the residual error σ ϵ mle for all three cases table 3 finally the uncertainty bands for cases d0 and d3a are comparatively small compared to the other cases both in figs 6 and 7 this is due to a higher density of adv measurements in general a larger number of measurements helps to decrease model uncertainty and increase the identifiability of the individual parameters 4 discussion 4 1 identifiability of friction parameters in this case study we identified three unknown friction parameters related to the channel bed vegetated channel slopes and the willow patch using only water level measurements it is not possible to uniquely determine the values of these parameters our findings show that detailed measurements of the transverse depth averaged velocity profile in combination with a probabilistic inverse modelling approach here glue allows us to estimate the parameter values and quantify the uncertainty of those estimations the inverse modelling approach results in two different types of uncertainty the first is model uncertainty which is the uncertainty of u c y θ in 5 this uncertainty can be decreased by increasing the number of observations i e n in table 3 or alternatively if a higher uncertainty is acceptable the number of observations may be decreased in general data provides information and more data provides more information stedinger et al 2008 the second type of uncertainty is predictive uncertainty given by ϵ c in 5 this uncertainty cannot be decreased without changing models only more precisely estimated in literature it is often assumed that the total roughness is constituted of a linear sum of independent constituent terms e g bed roughness and vegetation roughness the total roughness can be estimated without detailed flow measurements therefore if the bed roughness is known e g from repeating the experiment without vegetation the vegetation roughness can be determined however our results show that the assumption of independence between the terms would not have been valid for our study case for both vegetated cases the estimated friction of the bed and the slope was higher compared to the unvegetated case a potential explanation for this could be that turbulent and shear stresses not sufficiently modelled by the parameterisation of eddy viscosity are compensated by a higher bed roughness the estimated parameter values for ϕ were found to be generally higher than was estimated based on the rigid cylinder estimator this is especially true for case d3a where the presence of undergrowth contributes to the overall failure of the estimator in the cleaned case d3b the values are within the same order of magnitude although still a factor two higher compared to the rigid cylinder estimation a partial explanation could be found in the presence of foliage which in other studies is reported to account for 60 of total drag bal et al 2011 as well as unmodelled stresses in the horizontal exhange layer between vegetated and unvegetated flow truong et al 2019 4 2 uncertainty of vegetation model parameters under field conditions in this study we used the well known two layer model of baptist et al 2007 for submerged flow while this model consistently compares favourably to experimental data application to field conditions is met with several challenges an important limitation of the baptist model is that reconfiguration is not explicitly taken into account reconfiguration can be taken into account in the two layer approach by modelling two different effects the first effect is streamlining of stems and foliage to decrease the total drag of the vegetation this directly effects the vegetation parameter ϕ the model proposed by järvelä 2004 can be used to account for this in this model the vegetation parameter is defined by ϕ c d χ l u v u χ χ where l m is the leaf area index uv ms 1 the flow velocity uχ ms 1 a constant and χ c d χ are species specific parameters the value of the vogel exponent χ is negative and thus decreases ϕ with increasing velocity the second effect of reconfiguration is deflection of the stems which effectively decreases the height of the vegetation canopy height hd this directly modifies the second term in the baptist model which accounts for submerged flow and depending on the chosen model ϕ for submerged flow the estimation of vegetation friction is sensitive to the decrease of the vegetation canopy this is especially so for low submergence ratios 0 75 and high values of ϕ 0 2 for which the contribution of vegetation friction ϕ 2 g 1 2 and contribution of the velocity profile above the vegetation g κ ln h h d to the total friction are of the same order of magnitude under these conditions assuming rigid vegetation may lead to large errors in the estimation of vegetative friction the primary effect of undergrowth is an increased blockage ratio which can be modelled by a higher value for ϕ the inherent weakness of the rigid cylinder estimators for ϕ used by klopstra et al 1997 baptist et al 2007 huthoff et al 2007 yang and choi 2010 is that inclusion of undergrowth is contrived while neglecting undergrowth may lead to significant underestimation of the effective roughness frontal blockage area estimators for ϕ e g västilä and järvelä 2017 do not suffer from this limitation as undergrowth would be one part of the total blockage area comparing the two vegetated cases d3a and d3b we observed lower flow velocities and greater deflection of the stems this may be seen as a secondary effect of undergrowth i e an increase in total drag of the patch and therefore greater reduction in canopy height this suggests that modelling this type of reconfiguration does not only depend on plant specific parameters but on the configuration of the entire patch including secondary vegetation relatively simple predictive models based on single plant behaviour or even lumped models with a velocity dependent relationship with plant specific parameters verschoren et al 2016 are therefore likely to include uncertainty regarding non plant specific drivers future effort may be directed to test an extended baptist or similar two layer model which would include the effects of streamlining and deflection under natural conditions ideally these flume tests should cover a series of different discharges to test under varying water depths and flow velocities to promote identifiability the number of parameters of the extended model should be limited another argument for vegetative friction models with a relatively small number of parameters apart from the issue of identifiability is found in the usually data limited problems in practice vegetation models with few parameters for which the uncertainty can be quantified are perhaps better suited for larger scale field applications than complex models which require intimate knowledge of vegetation configuration 5 conclusions the objective of this study was to investigate flow resistance under natural vegetated conditions in which secondary vegetation grows under and between the dominant species results show that the presence of undergrowth sharply increases the vegetation parameter ϕ as well as increase the deflection of the willows overall the vegetation parameter value was found to be higher than expected based on a priori estimations of a rigid cylinder estimator this is attributed to aspects of real vegetation that deviate from the rigid stick based approach namely foliage undergrowth and reconfiguration this paper shows that velocity measurements in natural channels can be used effectively to estimate the parameter values of multiple sources of uncertainty under natural conditions using probabilistic inverse modelling results show well defined posterior parameter distributions and model results compare favourably to measurements the parameter values found in the non vegetated cross section were found to differ from the vegetated cross section which suggests that for the given model the presence of a vegetation patch requires higher friction values in the surrounding non vegetated part as well an explanation for this has not yet been found future challenges for practical application of vegetation models in large scale 2d applications may require relatively simple models in which the presence of undergrowth and the effects of reconfiguration are taken into account inverse modelling of full scale experiments has been shown to be an appropriate tool for providing the experimental basis for field validation of these models credit authorship contribution statement koen d berends conceptualization methodology software validation formal analysis writing original draft visualization un ji conceptualization methodology investigation resources data curation writing original draft writing review editing project administration funding acquisition w e ellis penning conceptualization resources writing review editing supervision project administration funding acquisition jord j warmink conceptualization methodology writing original draft writing review editing supervision joongu kang validation investigation data curation writing review editing supervision suzanne j m h hulscher writing review editing supervision project administration project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is part of the rivercare research programme supported by the dutch technology foundation ttw project number 13520 which is part of the netherlands organisation for scientic research nwo and which is partly funded by the ministry of economic affairs under grant number p12 14 perspective programme this research was also supported by the international matching joint research project of kict we are very grateful to wijnand ijzermans and the measurement group of kict rec and nnt nature technology of korea for their help during the experiment in 2015 we acknowledge the peer review of two anonymous reviewers whose valuable comments helped improve the style and clarity of this paper 
437,vegetation is a key source of flow resistance in natural channels and floodplains it is therefore important to accurately model the flow resistance to inform decision makers and managers however it is challenging to predict the resistance of real vegetation because vegetation models are based on relatively small scale lab experiments with mostly artificial vegetation experimental tests of real vegetation under field conditions are scarce the purpose of this study is to measure the flow resistance of a submerged willow patch where small herbaceous vegetation was allowed to grow in between the willow stems to simulate field conditions detailed flow velocity measurements were performed during an full scale experiment of flow around a submerged patch of willows the parameter values of the willow vegetation model as well as the friction coefficients of the vegetated banks and unvegetated channel bed were computed simultaneously using bayesian inference using a 2d hydrodynamic model results show that the presence of understory growth greatly affects flow patterns and the value of the effective vegetation density parameter measured flow velocities in the patch with understory growth were very low and the patch has relatively high deflection after removal of this undergrowth flow velocities in the patch increased and deflection of the vegetation canopy decreased we show that estimating vegetation density using an often used rigid cylinder estimator based on vegetation sampling underestimated the effective value by more than an order of magnitude we argue that proposed extensions to existing vegetation models which can take into account understory growth and reconfiguration could be tested under field conditions using the approach followed in this paper keywords natural vegetation flow experiment vegetation roughness bayesian inference hydraulic modelling 1 introduction vegetation in rivers and streams is one of the largest sources of flow resistance luhar and nepf 2013 and an important source of uncertainty in hydrodynamic models used for flood management warmink et al 2013 and river engineering applications berends et al 2018 the presence of vegetation affects the morphological evolution of rivers van oorschot et al 2015 biodiversity straatsma et al 2017 and water quality dosskey et al 2010 therefore a good representation of vegetation in computer models is important advances in remote sensing and classification algorithms enable detailed maps of the spatial distribution of vegetation geerling et al 2009 forzieri et al 2011 hodges 2015 the local effect of vegetation on flow is commonly modelled through a friction term in the momentum equation this term also known as roughness or flow resistance often represents various ways of energy losses that are not explicitly modelled we can broadly distinguish two ways to determine vegetative friction in field situations the first approach to determine vegetative friction is based on look up tables or vegetation models the table of typical manning type friction values by chow 1959 are still used to characterise the roughness of streams based on images or descriptions of the vegetation e g light brush and trees in winter look up tables are a convenient way to couple remote sensing to hydrodynamic modelling e g forzieri et al 2011 however despite the widespread use of the manning coefficient as a lumped parameter to characterise describe roughness the implicit assumption of the manning equation for a logarithmic velocity profile does not hold for vegetation naden et al 2006 ferguson 2007 for this reason various semi empirical models have been developed that compute the contribution for vegetation on flow resistance based on vegetation parameters such as stem count and plant morphology klopstra et al 1997 baptist et al 2007 huthoff et al 2007 yang and choi 2010 li et al 2015 it has been shown that such models perform well against a large body of data collected from laboratory experiments vargas luna et al 2015 further studies have been carried out to better capture real world vegetation dynamics such as reconfiguration järvelä 2004 dijkstra and uittenbogaard 2010 verschoren et al 2016 and contribution of foliage bal et al 2011 västilä and järvelä 2014 in practice the vegetation parameters necessary to use these formulas should be either directly measured or taken from a look up table successful application of this approach to field conditions depends on the validity of the vegetation model and the availability of data on vegetation the second approach to determine vegetative friction is through inverse modelling which involves fitting the model in practice usually a subset of model parameters to measurements it is called inverse modelling because the quantity of interest is not model output but model input such as parameters in practice inverse modelling is used to find the values for look up tables because the lumped roughness can be straightforwardly computed by inverting the manning equation under the assumption of steady uniform flow a logarithmic flow profile and a known energy slope marjoribanks et al 2014 in the case of non uniform flow the inverted bélanger equation can be used instead errico et al 2018 roughness values estimated through inverse modelling are often used as the measured roughness to provide the experimental basis for vegetation models however estimating the friction in heterogeneous real world situations based on the energy slope e g a single vegetation patch which does not cover the entire channel is complicated estimating the friction of multiple sources based on a single observation the energy slope may result in non unique solutions to parameter values beven 2006 this problem is analogous to that of underdetermination in regression problems which allows an infinite number of acceptable combinations of unknown friction factors if the parameters exceed the available observations this severely limits our ability to estimate the friction values and validate vegetation models in real world situations in cases where non uniqueness is an issue probabilistic parameter estimation is preferred to deterministic optimisation matott et al 2009 guillaume et al 2019 formally this is achieved through bayesian inference which produces probability distributions of parameter values given the subjective likelihood that the model given these values confirms experimental observation a well known bayesian inference methodology is glue generalised likelihood uncertainty estimation originally developed for non uniqueness problems in hydrology beven and binley 1992 fonseca et al 2014 mayotte et al 2017 however previous work has shown that estimating more than one friction source based on water levels can result in unidentifiable parameter distributions which are characterised by wide and unconstrained shapes werner et al 2005 to increase the identifiability of parameter distributions other observational data can be used such as inundation patterns pappenberger et al 2005 hypothetically detailed velocity measurements around vegetation patches can serve a similar function to estimate flow resistance using glue however the literature on flow data around real world vegetation patches are scarce naden et al 2006 marjoribanks et al 2017 and comparisons between laboratory and field in general are rare huthoff et al 2013 groom and friedrich 2018 our aim is to investigate the flow resistance of vegetation under natural conditions specifically we are interested in the effect of secondary vegetation growing in between and under the branches of the dominant vegetation species such secondary vegetation is hard to detect remotely from satellite or drone imagery and may therefore lead to an underestimation of biomass in large scale river models that rely on ecotope maps to estimate floodplain friction straatsma and huthoff 2011 forzieri et al 2011 warmink et al 2013 van oorschot et al 2015 berends et al 2019 it is expected that understory growth increases the flow resistance but it is unknown by how much in this study we perform a stream scale experiment with real vegetation where secondary vegetation was allowed to develop naturally this physical experiment is coupled to a digital twin numerical model to compute friction parameters by bayesian inference 2 methods 2 1 physical experiment setup large scale experiments were performed at the site of the kict rec korea institute of civil engineering and building technology river experiment center which is located in the city of andong south korea this facility is designed for full scale physical experiments and consists of three separate channels of various slope and sinuosity large capacity pumps can generate the maximum flow rate up to 10 m 3 s 1 the length of each channel is approximately 600 m the experiments for vegetated flow were performed in the downstream section of the a1 channel which has a trapezoidal cross section with a bottom width of 3 m top width of 11 m bed slope of 0 001 m m and bank side slope of roughly 1 1 5 v h as shown in fig 1 the bankside slopes of the channel are vegetated with grasses and small annuals native to the region the channel bed consists of sandy materials with a mean particle size of 0 8 mm seven alternating willow patches each with a length of 4 m and width of 1 5 m were planted in the 52 m section of a1 channel which was located about 125 m upstream from the downstream weir and 400 m downstream from the upstream weir the patches were planted in two configurations the four most upstream patches in a dense configuration of 22 trees per m2 and the three downstream patches in a sparse configuration of 7 3 trees per m2 the willow saplings were allowed to grow at the site for 10 months before the experiments started in august 2015 the average height of rooted willows was 0 4 m when they were planted with an initial trunk diameter of 1 cm in addition indigenous small scale rough herbaceous vegetation and grasses were allowed to grow on the bank side slopes of the channels the bed itself was relatively unvegetated and mobile herbaceous vegetation had also spontaneously developed in between the willows in the vegetation patches although the flume was mostly dry during this in which the vegetation grew although periods of prolonged flow occurred when the experiments took place in some other part of the flume this regime of mostly dry and occasional flooding is typical for many korean streams which are ephemeral in nature after the flow experiment vegetation samples were taken to measure the height diameter and morphology of the willows the bed level in the experimental area was measured using riegl lms z390i terrestrial laser scanner the flow measurements were taken after uniform steady flow was achieved the water depth in the channel downstream was fixed to approximately 1 1 m detailed three dimensional velocity measurements with an sontek 16 mhz adv acoustic doppler velocimeter were performed in the middle and 4 m upstream of the first dense patch at d3 fig 2 the adv devices were positioned pointing downward from an overhanging bridge to get a signal within the vegetation patch the devices were slightly moved to the left or right when needed during trial convergence testing a required measurement time of 100 s was found both the discharge and the depth averaged flow velocity profile are derived from the adv measurements we gathered data from three cases i flow measurements at the upstream cross section d0 ii flow measurements in the centre of the first willow patch d3 iii and finally flow measurements at d3 but with the undergrowth removed fig 2 we will refer to these cases as d0 d3a willows and undergrowth and d3b only willows respectively for case d0 and d3a adv measurements were carried out every 30 cm for a total of 23 locations in y direction for d3b a lower resolution was used of 60 cm for a total of 12 locations in y direction in the vertical direction a measurement was carried out every 5 cm for d3a a vertical resolution of 2 5 cm was used in the vegetation patch 2 2 numerical experiment setup a digital twin of the experimental flume was constructed with the open source delft3d 4 modelling system lesser et al 2004 1 1 the source code of delft3d 4 as well as its validation documents can be retrieved from https oss deltares nl using a two dimensional regular numerical computational grid of 15 m 171 m the size of individual grid cells was 0 5 m in flow direction by 0 375 m with bed levels defined in the centre of each cell a constant discharge determined from the flume adv measurements was imposed at the upstream boundary while the downstream boundary condition was defined as a constant water level to ensure a downstream water depth of approximately 1 1 m the model was initialised with a constant water level each simulation ran for 30 min of model time with a time step of 0 3 s the run time was chosen such that the entire model achieved equilibrium condition by the last time step the scale of the model given the relatively short run time small water depths fine grid and small time step required changing some default numerical and physical parameters we used a smoothing time of 2 min and a threshold depth for flooding drying of 1 cm the horizontal eddy viscosity was held constant throughout the model at 10 2 m 2 s 1 to be able to capture the sharp velocity gradients around the patches low viscosity values were used vionnet et al 2004 verschoren et al 2016 in the experimental area the geometric data was based on the bed level measurements of the experimental setup for regions not covered by the bed level measurements a synthetic cross section was defined from the design specifications of the channel bed friction was defined using four distinct roughness classes two for the willow patches dense and sparse one for the vegetated channel slopes and one for the mobile sand channel bed each individual class is assigned its own roughness formula and friction parameters every grid cell boundary was assigned one of these four classes all friction formulas are expressed in terms of the chézy friction coefficient c m 1 2 s 1 for the channel bed we use the manning friction formula 1 c h 1 6 n b 1 with water depth h m and manning coefficient nb sm 1 3 the keulegan equation also known as the colebrook white equation was found to be best suited to reproduce the depth averaged velocity on the channel slopes 2 c α 1 log 10 α 2 h k s with nikuradse roughness height ks m and parameters α 1 18 m1 2 s 1 α 2 12 the values of both nb and ks were determined with glue there are various models available for resolving vegetation friction here we used the two layer approach of baptist et al 2007 which performs favourably against laboratory experiments vargas luna et al 2015 and is generalised as follows 3 c c b 2 ϕ 2 g 1 2 α g κ ln h h d where cb is the chézy friction coefficient of the bed without vegetation ϕ the vegetation parameter g ms 2 the gravitational acceleration κ the von karman constant hd m the deflected vegetation canopy height and α an indicator such that α 0 for emergent conditions h hd and α 1 for submerged conditions h hd the dimensionless vegetation term ϕ models the contribution of vegetation to the total friction of the vegetated part of the cross section in the original formulation of baptist et al 2007 it is computed as ϕ c d m d h v with drag coefficient cd stems per square meter m m 2 stem diameter d m and vegetation height hv m this stick model for ϕ is ideally suited for vegetation that may be approximated as rigid cylinders a fixed value for hd was used which was estimated from measurements furthermore we assumed that the friction in the patch was dominated by the vegetation and that the bed term had a neglible impact therefore we chose a constant value of cb at 60 m1 2 s 1 which led to negligible addition to the total roughness the vegetation parameter ϕ was considered unknown and estimated with glue 2 3 glue method the set of unknown parameters in the numerical experiment was given by θ n b k s ϕ i e the friction coefficients of the channel bed and slope and the vegetation parameter the problem considered here was how to choose the values for parameters in such a way that the measured flow velocities were reproduced by the flow model while an optimal set of values may be found using one of various optimisation strategies available in literature beven 2006 argued that multiple parameter sets may well be found that all produce acceptable results this creates uncertainty regarding the parameter values thus obtained through inverse modelling the added benefit of glue above regular calibration procedures is that this uncertainty is formally taken into account to achieve this glue uses bayesian inference in which usually very limited prior knowledge about the parameter values is updated given the probability that those parameter values resulted in model output that compared favourably with measurements this is technically achieved through bayes theorem 4 p θ u l u θ ϵ p θ where ϵ is the error between modelled and measured flow velocities and u measured flow velocities the prior distribution p θ that encodes our prior assumptions of probable parameter values for parameter set θ the posterior distribution p θ u is our goal because it expresses the probability of parameter values in θ after having seen the measured flow velocities u the likelihood l u θ ϵ is a function of the model θ ϵ and observations u and represents the probability of u given θ and ϵ to determine the functional form of the likelihood we needed to assume a statistical relationship between observed and modelled flow velocities here we related the measured flow velocity vector uc at cross section c to the simulated flow velocity u c y θ as follows 5 u c y u c y θ ϵ c where ϵ c is a normal and independently distributed error term with variance σ ϵ 2 i e ϵ c n 0 σ ϵ 2 the assumption of independent errors and zero bias expresses our assumption that the hydraulic model should be able to reproduce observe flow velocity profiles such that any remaining discrepancy is adequately described by a normal distribution as is commonly done in glue applications we adopted a uniform prior distribution for each parameter the upper and lower limits of each distribution were determined by exploratory computations with the model for our adopted error model 5 the corresponding likelihood function is 6 l θ u 2 π σ ϵ 2 1 2 exp u c y u c y θ 2 2 σ ϵ 2 the unknowns in the inverse problem are both the model parameters θ and the variance of the residual errors σ ϵ 2 to estimate σ ϵ 2 we used the maximum likelihood estimate σ ϵ m l e 2 n 1 i 1 n u i u i m l e 2 with n the number of observations and u i m l e the model results for the best performing parameter set stedinger et al 2008 since the likelihood function returns the probability of the observed flow velocities given the model results evaluated with a given parameter set θ model results that deviate significantly from observations will return likelihoods approaching zero the likelihood function used here was formally derived from the adopted statistical model 5 we note that glue allows for great flexibility in choosing from a variety of informal likelihood functions as well which need an additional behavioural threshold to differentiate between behavioural and non behavioural parameter sets since 6 tends to zero for very improbable parameter sets such a behavioural threshold was not needed here the posterior parameter distributions were obtained through monte carlo simulation using the sobol low discrepancy sequence sobol 1967 we sampled from the prior distributions to create a large number of possible parameter sets here we used a sample size of 5000 model evaluations with each evaluation using a different randomly sampled set of parameter values the same ensemble was used for cases d3a and d3b using the value for hd derived from case d3a afterwards the ensemble values of ϕ for d3b were corrected to account for a different height of the deflected vegetation canopy the probability distribution for the parameter vector θ was obtained by application of bayes theorem this procedure gave us two valuable sources of information on uncertainty related to model and observation first the estimate of σ ϵ 2 or the predictive uncertainty this is the residual variance between model and measurement which cannot be explained by choosing different parameter settings the second is p θ u i e the posterior parameter distribution or model uncertainty this expresses the uncertainty in the parameter values given the measurements 3 results 3 1 flow measurements based on the adv measurements the flow fields covering the vertical y z plane were constructed to compute discharge from velocity measurements it is usually necessary to assume a velocity profile e g logarithmic see boiten 2000 however due to the irregular flow profiles expected in the flow through vegetation and the density of velocity measurements we instead linearly interpolated points in between the adv support points to a regular cross section covering grid while assuming zero flow velocity at the bed fig 4b d f the total discharge was then computed by summing the product of the grid cell area we found discharges of 2 91 m3 s 1 d0 2 66 m3 s 1 d3a and 2 54 m3 s 1 d3b the differences are attributed to uncertainty in flow velocity measurements and interpolation inaccuracies the measurements at the unvegetated cross section d0 show that the depth averaged flow is low on the slopes and increases toward the centre of the channel reaching about 0 5 ms 1 fig 4a at the right hand side of the channel y 6 m a local increase in flow velocity is measured given that the high velocities were consistently measured by different adv devices we assume they are not due to instrument problems therefore we did not reject these measurements but left it to the numerical model to explain whether these measurements are expected we used the discharge at d3a as the upstream boundary condition in the numerical model two series of measurements were carried out for the cross section in the willow patch d3 one for the natural situation including organic debris and undergrowth d3a and the other for which the patch was cleaned d3b analysis of measured flow velocities for d3a revealed that one adv measurement device returned near zero results for all flow depths suggesting malfunction measurements from this device at y 6 4 m were discarded from the results similar to d0 a region of higher flow velocities was observed at the right hand side of the patch near the water surface in the patch itself both the flow field fig 4d and the depth averaged results fig 4c show slower velocities however the flow velocities near the water surface are similar to the unvegetated part of the channel suggesting submerged flow this is clear from the vertical velocity profile in the patch fig 3 which shows low flow velocities 0 3 ms 1 and large variation up to z 3 4 m above which the velocities rapidly increase to more than 0 6 ms 1 and variation is significantly reduced from fig 3 we assumed that the vegetation was deflected such that the canopy height was 0 8 m case d3b after removal of all organic debris and undergrowth broadly shows fig 4e f similar patterns to the d3a results with some key differences in the flow through the patch in the vertical profile fig 3 velocities near the bed z 3 6 m are higher compared to d3a which is attributed to removal of undergrowth for higher water depths z 3 6 m velocities show an initial decrease while the variation over the patch increases this is attributed to flow through the branches and leaves which add comparatively more resistance in contrast to d3a clear submergence is not observed therefore we assume that in the case of d3b the vegetation was just submerged meaning that the deflected canopy height was equal to the water depth 3 2 vegetation measurements after the initial flow experiment cases d0 and d3a several samples were taken to determine the height and diameter of the willows table 1 the stems were measured up until the upper knot from which point several branches sprouted we observed that organic debris which had attached itself around the willow stems increased the effective diameter of the plants by 50 on average however the distribution was not uniform some plants experienced a significantly larger increase in diameter while others had little attached debris this was reflected in the increased variance of the observations the undergrowth had an average height of 29 cm and grew in between the willow stems fig 5 and consisted of various ways of herbaceous vegetation it should be expected that both the undergrowth and the diameter increase due to organic debris increase the effective friction of the vegetation patch the expected vegetation parameter ϕ is estimated based on the baptist vegetation model ϕ c d m d h d and the vegetation measurements from table 1 to compute ϕ we assumed the branch height to be equal to the willow height minus the average stem height furthermore we assumed rigid bending at the bed following verschoren et al 2016 such that the deflected stem and branch heights can be computed from the deflected willow height see section 3 1 using standard trigonometry we assumed a case drag coefficient of c d 1 which is a common assumption for submerged flow wunder et al 2011 here we did not account for the effect of foliage the estimated vegetation parameter for case d3a i e including debris undergrowth and a deflected willow height of 0 8 m is approximately ϕ d 3 a 1 33 1 044 the large standard deviation in the expected parameter is mainly due to the variance in the diameter of the herbaceous vegetation in the undergrowth for case d3b i e no debris no undergrowth and a deflected height equal to the willow height is ϕ d 3 b 0 76 0 20 3 3 glue results an important step within glue is to determine the ranges of the uniform prior distributions these ranges should be generous since it is important that these prior distributions cover the range where the a priori unknown posterior distributions will be assuming a trapezoidal channel the lumped manning coefficient for the given discharge and water depth is approximately 0 11 sm 1 3 based on this relatively high friction factor and exploratory computations with the hydrodynamic model we chose suitably large ranges for the prior distributions and a log uniform distribution for ϕ table 2 for all three parameters nb ks ϕ we then computed the joint posterior distributions using glue the posterior distributions give the likelihood that a certain parameter value leads to good model results the width of those distributions is a measure of the parameter uncertainty regarding the possible true value of the parameters given the model and the measurements the marginal posterior distributions of all parameter values are shown in fig 6 the distribution of ϕ for d0 is very wide and does not show a clear peak fig 6a this indicates that the ϕ is an insensitive parameter for cross section d0 therefore ϕ cannot be identified from flow measurements at that cross section this was not unexpected as d0 is located 4 m upstream from the first patch and is therefore not directly influenced by the patch the other two distributions in fig 6a show the values for ϕ that lead to good model results for d3a and d3b d3a which had significant undergrowth and debris shows significantly larger inferred values for ϕ compared to d3b the median value of 101 6 39 8 is an order of magnitude higher than would be expected based plant on parameters using the baptist model for the cleaned willows case d3b the values are much lower with a median value of ϕ 1 56 this is more than twice as high as was expected based on the plant parameters the uncertainty of the estimated parameter values for ϕ is significant as can be observed by the width of the posterior distributions in fig 6a and the standard deviation in table 3 for case d3a especially the values for ϕ vary between 20 and 100 with some of the best simulations found near the upper boundary it is important to note that at such high values the sensitivity of ϕ to the roughness coefficient c is very much reduced due to the inverse square root in 3 which may partly explain why such high uncertainty is found for this case therefore values higher than the upper boundary are not expected to meaningfully improve results estimation of manning coefficient of the channel bed nb and nikuradse roughness height of the slopes ks resulted in well defined posterior distributions for all cases fig 6b c with standard deviations around 0 01 sm 1 3 table 3 interestingly the distributions are not the same for the three cases to measure the roughness of vegetation from water slope measurements the roughness of the bed is commonly assumed to be independent from the roughness of the vegetation e g verschoren et al 2016 however the results shown in fig 6 suggest that the parameter values for both nb and ks are affected by the vegetation patch for both nb and ks the presence of a patch d3a d3b results in higher parameter values therefore the assumption of independence between the parameter of the vegetation patch ϕ and those of the channel bed and slope nb and ks would not have been valid in our case modelled flow velocities given the posterior probability distributions of the parameters are compared with the depth averaged adv measurements fig 7 the depicted uncertainty bands show the range of the model uncertainty variation within the model uncertainty can be explained from uncertainty in the parameter values measurements that fall outside of these bands are explained through the residual error term ϵ in eq 5 from fig 7a we observe that the region of higher flow velocity in cross section d0 cannot be explained by the numerical model this indicates that this must be caused either by an unmodelled process or feature or by measurement error however in general the velocity profiles for all cases are well explained by the numerical model which is reflected in the small standard deviation of the residual error σ ϵ mle for all three cases table 3 finally the uncertainty bands for cases d0 and d3a are comparatively small compared to the other cases both in figs 6 and 7 this is due to a higher density of adv measurements in general a larger number of measurements helps to decrease model uncertainty and increase the identifiability of the individual parameters 4 discussion 4 1 identifiability of friction parameters in this case study we identified three unknown friction parameters related to the channel bed vegetated channel slopes and the willow patch using only water level measurements it is not possible to uniquely determine the values of these parameters our findings show that detailed measurements of the transverse depth averaged velocity profile in combination with a probabilistic inverse modelling approach here glue allows us to estimate the parameter values and quantify the uncertainty of those estimations the inverse modelling approach results in two different types of uncertainty the first is model uncertainty which is the uncertainty of u c y θ in 5 this uncertainty can be decreased by increasing the number of observations i e n in table 3 or alternatively if a higher uncertainty is acceptable the number of observations may be decreased in general data provides information and more data provides more information stedinger et al 2008 the second type of uncertainty is predictive uncertainty given by ϵ c in 5 this uncertainty cannot be decreased without changing models only more precisely estimated in literature it is often assumed that the total roughness is constituted of a linear sum of independent constituent terms e g bed roughness and vegetation roughness the total roughness can be estimated without detailed flow measurements therefore if the bed roughness is known e g from repeating the experiment without vegetation the vegetation roughness can be determined however our results show that the assumption of independence between the terms would not have been valid for our study case for both vegetated cases the estimated friction of the bed and the slope was higher compared to the unvegetated case a potential explanation for this could be that turbulent and shear stresses not sufficiently modelled by the parameterisation of eddy viscosity are compensated by a higher bed roughness the estimated parameter values for ϕ were found to be generally higher than was estimated based on the rigid cylinder estimator this is especially true for case d3a where the presence of undergrowth contributes to the overall failure of the estimator in the cleaned case d3b the values are within the same order of magnitude although still a factor two higher compared to the rigid cylinder estimation a partial explanation could be found in the presence of foliage which in other studies is reported to account for 60 of total drag bal et al 2011 as well as unmodelled stresses in the horizontal exhange layer between vegetated and unvegetated flow truong et al 2019 4 2 uncertainty of vegetation model parameters under field conditions in this study we used the well known two layer model of baptist et al 2007 for submerged flow while this model consistently compares favourably to experimental data application to field conditions is met with several challenges an important limitation of the baptist model is that reconfiguration is not explicitly taken into account reconfiguration can be taken into account in the two layer approach by modelling two different effects the first effect is streamlining of stems and foliage to decrease the total drag of the vegetation this directly effects the vegetation parameter ϕ the model proposed by järvelä 2004 can be used to account for this in this model the vegetation parameter is defined by ϕ c d χ l u v u χ χ where l m is the leaf area index uv ms 1 the flow velocity uχ ms 1 a constant and χ c d χ are species specific parameters the value of the vogel exponent χ is negative and thus decreases ϕ with increasing velocity the second effect of reconfiguration is deflection of the stems which effectively decreases the height of the vegetation canopy height hd this directly modifies the second term in the baptist model which accounts for submerged flow and depending on the chosen model ϕ for submerged flow the estimation of vegetation friction is sensitive to the decrease of the vegetation canopy this is especially so for low submergence ratios 0 75 and high values of ϕ 0 2 for which the contribution of vegetation friction ϕ 2 g 1 2 and contribution of the velocity profile above the vegetation g κ ln h h d to the total friction are of the same order of magnitude under these conditions assuming rigid vegetation may lead to large errors in the estimation of vegetative friction the primary effect of undergrowth is an increased blockage ratio which can be modelled by a higher value for ϕ the inherent weakness of the rigid cylinder estimators for ϕ used by klopstra et al 1997 baptist et al 2007 huthoff et al 2007 yang and choi 2010 is that inclusion of undergrowth is contrived while neglecting undergrowth may lead to significant underestimation of the effective roughness frontal blockage area estimators for ϕ e g västilä and järvelä 2017 do not suffer from this limitation as undergrowth would be one part of the total blockage area comparing the two vegetated cases d3a and d3b we observed lower flow velocities and greater deflection of the stems this may be seen as a secondary effect of undergrowth i e an increase in total drag of the patch and therefore greater reduction in canopy height this suggests that modelling this type of reconfiguration does not only depend on plant specific parameters but on the configuration of the entire patch including secondary vegetation relatively simple predictive models based on single plant behaviour or even lumped models with a velocity dependent relationship with plant specific parameters verschoren et al 2016 are therefore likely to include uncertainty regarding non plant specific drivers future effort may be directed to test an extended baptist or similar two layer model which would include the effects of streamlining and deflection under natural conditions ideally these flume tests should cover a series of different discharges to test under varying water depths and flow velocities to promote identifiability the number of parameters of the extended model should be limited another argument for vegetative friction models with a relatively small number of parameters apart from the issue of identifiability is found in the usually data limited problems in practice vegetation models with few parameters for which the uncertainty can be quantified are perhaps better suited for larger scale field applications than complex models which require intimate knowledge of vegetation configuration 5 conclusions the objective of this study was to investigate flow resistance under natural vegetated conditions in which secondary vegetation grows under and between the dominant species results show that the presence of undergrowth sharply increases the vegetation parameter ϕ as well as increase the deflection of the willows overall the vegetation parameter value was found to be higher than expected based on a priori estimations of a rigid cylinder estimator this is attributed to aspects of real vegetation that deviate from the rigid stick based approach namely foliage undergrowth and reconfiguration this paper shows that velocity measurements in natural channels can be used effectively to estimate the parameter values of multiple sources of uncertainty under natural conditions using probabilistic inverse modelling results show well defined posterior parameter distributions and model results compare favourably to measurements the parameter values found in the non vegetated cross section were found to differ from the vegetated cross section which suggests that for the given model the presence of a vegetation patch requires higher friction values in the surrounding non vegetated part as well an explanation for this has not yet been found future challenges for practical application of vegetation models in large scale 2d applications may require relatively simple models in which the presence of undergrowth and the effects of reconfiguration are taken into account inverse modelling of full scale experiments has been shown to be an appropriate tool for providing the experimental basis for field validation of these models credit authorship contribution statement koen d berends conceptualization methodology software validation formal analysis writing original draft visualization un ji conceptualization methodology investigation resources data curation writing original draft writing review editing project administration funding acquisition w e ellis penning conceptualization resources writing review editing supervision project administration funding acquisition jord j warmink conceptualization methodology writing original draft writing review editing supervision joongu kang validation investigation data curation writing review editing supervision suzanne j m h hulscher writing review editing supervision project administration project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is part of the rivercare research programme supported by the dutch technology foundation ttw project number 13520 which is part of the netherlands organisation for scientic research nwo and which is partly funded by the ministry of economic affairs under grant number p12 14 perspective programme this research was also supported by the international matching joint research project of kict we are very grateful to wijnand ijzermans and the measurement group of kict rec and nnt nature technology of korea for their help during the experiment in 2015 we acknowledge the peer review of two anonymous reviewers whose valuable comments helped improve the style and clarity of this paper 
438,environmental measurements generate great volumes of high dimensional data often noisy and with missing values from which meaningful messages may be extracted through appropriate organisation and summarisation the self organizing map som is an artificial neural network popular for recognizing patterns relationships and clusters in such data through the finetuning of configuration and training parameters the som can be tailored to best suit a specific data set presently most hydrologic applications continue to rely on heuristics software defaults or arbitrarily chosen parameters forfeiting some of the potential benefits of the method and often resulting in an output that does not best represent the actual features of the data this paper guides researchers through the knowledgeable creation and interpretation of an appropriately customised som relevant to their particular high dimensional nonlinear data set an understanding of the effects that parameter selection and training options have on the extracted information is developed practical guidance is given for appropriately modifying parameter sets and comparisons are made with closely related methods data pre processing parameter selection initialisation training visualisation and interpretation of the output map for pattern extraction and clustering are discussed through individually customised soms more meaningful information can be gained from data driven exploratory analysis of the inter component relationships involved in water resource systems keywords self organizing maps parameter selection implementation interpretation tutorial guide error measures 1 introduction water resources measurements often collected over large spatial areas at high temporal resolutions produce great volumes of high dimensional data which must be organized and summarized to extract meaningful messages for use in decision making processes appropriate organisation and summarisation of the data will reduce the available information into a manageable number of characteristic patterns and intervariable relationships the self organizing map som kohonen 1990 is an artificial neural network useful for extracting patterns and finding clusters in large multivariate data sets it is an increasingly popular method used in hydrology water resources research and engineering with approximately 180 articles currently published each year using soms in these fields for clustering spatiotemporal analysis time series analysis infilling of missing data prediction and trend visualisation the popularity of the som method in water resources is due to its intuitive implementation resilience to missing and noisy data common features of measurements collected remotely or in the field ability to integrate real time data and straightforward visual summary of the system and intercomponent relationships relationships of hydrologic variables to physical chemical social or economic systems can also be investigated without requiring expert knowledge from each of the varied disciplines since the som organizes the data with no requirement for an explicit understanding and description of the complex underlying systems that may have produced the data the som extracts the most prevalent patterns in a data set and clusters the data items around these patterns organising the results into an intuitively understandable low dimensional visualisation or map a large number of high dimensional observations become represented with a usually much lower and therefore more manageable number of low dimensional vectors which form centroids for clusters of the original data this facilitates analysis of the properties of the data set through analysis of the ordered low dimensional cluster structure and cluster members the som provides various benefits over other methods for the exploration of environmental data the number of clusters present in the data does not need to be specified before hand the resulting clusters are ordered on the output visualisation indicating similarities and dissimilarities allowing for further analysis if desired creation of a som does not require an explicit understanding of the complex and potentially undefinable processes and relationships within the system that produced the data the analysis is conducted based only on the data presented to it the som can therefore be used to explore complex multi disciplinary or human environmental data sets in which the intervariable relationships are difficult to explicitly quantify in these cases the creation of an elaborate mathematical system model would be complex and time consuming another distinct benefit of the som is the ability to incorporate new data after the map is created providing a tool for online real time data analysis which is useful for the real time processing of sensor data pattern identification clustering and data visualisation together reveal information that may be otherwise unobservable in large data sets other techniques exist for each of these processes separately but the combination leads to the soms unique analysis capabilities for example clustering methods that are not combined with dimension reduction are not easy to visualise and therefore can be less readily interpreted and conveyed though soms are widely used and increasing in popularity for environmental applications uncertainty remains in the soms method due to the two inherent competing goals of approximating the data with the map nodes and preserving the topology of the data set which negate the possibility of specifying a single objective function that the map aims to optimise this gives rise to complications in parameter specification as different choices will improve some aspects of the results and possibly inhibit others parameter choices impact the formation of the output soms with different maps resulting from the use of distinct parameter sets it is therefore important for the analyst to appreciate and understand the parameter options available a sole reliance on software default parameters may result in maps that are not the most suitable size and shape to represent a particular data set or the level of smoothing and generalisation provided on the output map may not suit the specific purposes of the analysis interpretations of the data based on such maps may reveal less information about the data set than there is potential to uncover with respect to the distributions of individual variables and the intervariable relationships analysts must make an informed choice between the options based on the relative benefits of each choice soms literature tends to come in two streams 1 statistical analyses of the som algorithm published in statistics or neural networks journals not easily accessible to researchers from other fields and 2 applications using soms as a black box tool with little or no informed input from the user the literature provides little accessible guidance to lead non statistical researchers through the process of som implementation and interpretation we have found no paper unifying the information needed to understand the best current standard of the som method and in a timely manner to knowledgably create a som to best represent a specific data set through alteration of the som s configuration and training parameters water resources researchers may not have the time and inclination to sift through the technically focused statistical publications in which information required to do this is found instead we have noted that researchers are tending to revert to heuristic or software default parameter sets or are borrowing parameters from som models that have been used in other applications but are not particularly relevant to the specific data set at hand this does not lead to the production of the best maps for representing each unique data set this paper is provided as a guide to aid researchers in understanding current best practices for the informed application of soms within a context of soms background theory the reader is guided through the production and interpretation of a suitable som for the exploratory analysis of their specific multivariate nonlinear data set relevant applications parameter choices code implementation and interpretation of output maps are discussed key references are provided in each section to lead the reader to further resources 2 soms use in water resources with environmental data sets the pattern extraction clustering and visualisation capabilities of the som are often applied to spatiotemporal analysis time series analysis infilling missing data and prediction patterns are extracted as representative system states with each node of the trained map representing a specific pattern these patterns are used as the basis to cluster common system states and determine the nonlinear relationships between variables clustering consists of identifying the number of natural clusters in the data and uniquely assigning each input item to one of the clusters the clusters are based around the most common patterns in the data as identified by the map nodes which form the cluster centroids applications using soms to determine sets of typical system state patterns have included investigations of groundwater types belkhiri et al 2018 coastal water quality li et al 2018 water consumption patterns padulano and giudice 2018 wind patterns duvivier et al 2016 wave climate states barbariol et al 2016 spatial patterns of groundwater properties nguyen et al 2015 choi et al 2014 and soil quality rivera et al 2015 frequency and transition matrices can be created by determining the percentage of data matching each pattern as in falcieri et al 2014 nguyen et al 2015 and swales et al 2016 newton et al 2014 further the use of soms for state frequency analysis by including the persistence of states and synaptic type frequency anomalies for each variable at each node abundant applications use the som for clustering data into similar subsets such as land use and surface water quality gu et al 2019 segments of coastline for ecological classification ramos et al 2016 rainfall and climate regions mannan et al 2018 markonis and strnad 2019 factors affecting cyanobacterial blooms hong 2016 and niveograph patterns wang et al 2013 as well as measurement stations satellite imagery data seasonal patterns sea level pressure and precipitation nonlinear relationships have been investigated using a som between spatiotemporal interactions between groundwater and surface water chen et al 2018 streamflow regimes and fish communities tsai et al 2016 atmospheric circulation and surface climate newton et al 2014 da anunciacao 2014 reservoir water quality in relation to watershed land cover types park et al 2014 relationship of fish larvae with environmental variables jutagate et al 2016 modern and medieval climate circulation patterns edwards et al 2017 spatiotemporal changes in lagoon water quality kim et al 2016 regional precipitation and large scale atmospheric dynamics liu et al 2016 atmospheric circulation and arctic sea ice extent lynch et al 2016 and water vapour transport and mass loss in the greenland ice sheet mattingly et al 2016 maticet al 2017 discovers patterns of salinity and temperature in oscillating adriatic sea regimes rodriguez alarcan lozano 2017 use the som as a decision support system for reservoir regulation vaclavik et al 2013 identify generic global patterns of land pressures and environmental threats by clustering data sets based on intensity of use environmental conditions and socioeconomic indicators vereecken et al 2016 investigate patterns of water mass and energy in soil vegetation atmosphere interactions 2 1 spatiotemporal analysis soms have most often been used to investigate patterns and clusters in the spatial or temporal aspect of the data separately either spatial patterns occurring at different times or temporal patterns occurring in different locations have been investigated and compared on a single map a popular method of spatiotemporal data analysis with soms involves the creation of a series of maps placed next to each other each representing clusters of data at different timesteps requiring the reader to visually extract and analyse the temporally changing spatial patterns another method is to create a single map with all the available data changes over time can then be visualised by plotting batches of consecutive data i e decades onto the trained map or creating trajectories by linking the map nodes that are nearest to each consecutive data point in the sequence either for the entire data set or as separate trajectories for specific data segments however these popular methods may not adequately capture and express the structure of the data if a separate som is produced for each time period these maps may not be directly comparable to each other due to differences in the distributions and correlations in the data at each time step if a single som is created from all data with separate time periods mapped to it the overall map may not accurately describe the finer structure of the data at each time step attempts have been made to apply the som to more innovative visualisations of spatiotemporal data some of these involve using the traditional som in novel ways and some involve extending the som algorithm itself wang et al 2013 creates a single som of twenty years of snow accumulation and melt patterns with high temporal and spatial resolution and then clusters the nodes and plots trajectories for specific spatial locations to reveal cyclical and long term trends wang simultaneously determined spatial differences in time series snow accumulation melt patterns between mountain ranges as well as changes in patterns over time at specific locations using trajectories sarlin 2012 developed the self organising time map sotm to visualise the evolution of the cluster structure across space and time the sotm is made up of a 1d som at each timestep arranged in order of ascending time multivariate temporal and cross sectional aspects are visualised at the same time allowing changing emerging and lost clusters in the data structure to become apparent 2 2 time series analysis time series analysis or trend visualisation using soms entails the identification of temporal patterns and clustering of data items where each data item is an individual time series this produces groups of data items that trend in similar ways soms are practical in time series analysis for outlining the boundaries of existing conditions clustering based on global characteristics extracted from the time series and identifying future directions vantas et al 2019 use a som for classification of rainfall time series 2 3 infilling missing data the som algorithm is quite insensitive to missing values in the data and can be used to infill missing multivariate data data items with missing values for certain variables are matched to their nearest node based only on the values that are present the data vector then adopts the node vector s value for the missing variable soms have been applied to infilling runoff data in inadequately gauged basins based on rainfall measurements nkiaka et al 2016 physiochemical parameters in water samples folguera et al 2015 and estimating water quality in unmonitored streams based on relationships between spatiotemporal watershed attributes and water quality in monitored streams 2 4 prediction patterns in future data may be estimated using the knowledge of inherent states determined to exist in the current data states that occur with certain frequencies and the transitions between states harnessing information from the established nonlinear relationships allows for the prediction of a future variable based on its association with easily predicted variables an extension of the infilling missing data technique this is done through the assumption that for the same conditions combinations of variable values the same patterns will occur the relationships determined by the som may be used to predict what could be expected from new data given the presence of a certain system state by plotting future data onto a map trained with current data the change in cluster membership indicates changing state frequencies predicted for the future chang et al 2016 incorporate the som into a monthly basin wide prediction of groundwater levels to be used in sustainable basin management 3 background developed in finland in 1981 by teuvo kohonen the self organizing map has steadily gained popularity since its introduction with explanatory papers kohonen 1982 1990 kohonen 1998 and kohonen 2013 receiving over 39 000 citations combined the general som method entails arranging a map grid usually consisting of a regular rectangular grid of connected nodes in an initial approximate location over a data set through an iterative training process the map nodes move amongst the data items self organise whilst maintaining their grid structure until their locations provide the best possible coverage of the data set without the map becoming unnecessarily twisted the grid is stretched and bent until the position and orientation best represent the data structure fig 1 shows a synthetic data set black consisting of three gaussian clusters with an initial som grid green placed in a preliminary location and the trained som grid blue better following the structure of the data the som has two main organisational goals pattern identification and topology preservation these are realised through the separation and iteration of two interacting subtasks during the training process matching and updating during the matching stage the closest map node or best matching unit bmu is identified for each item of data the map nodes are then drawn closer to their nearby data items during the updating stage with neighbouring nodes on the map grid moving towards the same data items as the map nodes settle closer to the data items in data space the result is more nodes situated in higher density areas of the data the full algorithm for map training is given in the supplementary materials file the specific location in data space that each map node occupies at the conclusion of training given by the local values of each variable are recorded in the high dimensional vectors of the map nodes this unique combination of variables represents a particular characteristic pattern of the data set following training the individual data items are matched to their closest map nodes on the trained map this produces clusters of data sharing a common nearest node and therefore sharing common key characteristics as identified by the variable values of that node similar data items become matched or mapped to the same or nearby map nodes and more different data mapped to more distant nodes the patterns and clusters extracted from the data during training are visualised on a low dimensional output map organised according to their similarity the degree of similarity between each cluster is proportional to the topological distance between them with closer areas of the map representing similar patterns in the data domain areas of data space with higher densities of data are represented on larger areas of the som when presented in two dimensional space the map is a nonlinear projection of a reduction of the data set as the high number of high dimensional data items become represented in a lower dimension by a usually lower number of map nodes the output map can therefore be more manageably explored than the original data set giving insight into the overall structure and prevalent patterns existing in the data fig 2 depicts the representation of a data set with a som both in data space and map space the trained map is not twisted and data items that are near each other in data space are represented by the same or nearby nodes the nonlinear regression performed by the som is considered nonparametric as it is based on fitting a number of ordered discrete map vectors to describe the distribution of input samples the som is deemed an unsupervised process as it searches for unknown structure in unlabelled data unsupervised learning is closely related to density estimation in that it constructs an estimate of an unobservable probability density function pdf based on the data that is presented if the input samples have a well defined pdf the som map nodes will eventually come to approximate it in an ordered way producing a nonlinear projection of the high dimensional pdf of the input variables onto a low dimensional display kohonen 1995 3 1 comparison with k means clustering and principal component analysis the soms algorithm exists in a theoretical region of knowledge between principal component analysis pca and the k means clustering method with pca providing the most rigid analysis of the three and k means the most flexible pca a type of factor analysis transforms a data set into a small number of linear uncorrelated variables or principal components pcs which become axes of the projection space the set of data points are rotated around their mean to align with the new set of axes which point in the directions of maximum data variance the first principal component is defined by the eigenvector of the data set with the highest eigenvalue and the axis in this direction accounts for as much of the variance in the data as possible pca separates clusters of data items well but is not ideal for representing nonlinear data as the pcs are always linear the som algorithm uses pca during the initialisation stage aligning the initial axes of the map grid along the principal component plane soms are considered a constrained form of the k means clustering algorithm with the nodes constrained to a two dimensional manifold hastie et al 2009 the som method becomes equivalent to the k means algorithm at the end of training when the som neighbourhood kernel is so small it only contains a single node the k means algorithm is proficient at clustering though it does not retain information on cluster ordering and encounters issues in visualisation when the dimensions of input data are high when using the k means algorithm the user is required to specify the number of clusters as a parameter which may be an unknown attribute of the data set the som is beneficial over the k means algorithm for real time analysis of online data and for data sets with high levels of missing data laerhoven 2001 schroeder et al 2009 yin 2008b elaborates on the relationship between soms and pca fig 3 depicts a comparison of k means som and pca results on a single data set source bache and lichman 2013 it can be observed that all the techniques separate the two main clusters in the data k means and soms both place a user specified number of cluster centres amongst the data and the som results are similar to k means except that the grid structure retains an ordering to the clusters the techniques each perform well when compared with certain aspects of the som however if clustering nonlinear projection and an ordering of cluster information is important the som provides these attributes within a unified technique 4 creation of a self organizing map this section provides a guide for the creation of a self organizing map for the representation of a data set an example data set is used for demonstratation from environment canada s hydat database website 3 this data was purposely chosen to have only two dimensions annual minimum and maximum daily streamflow values at a selection of river stations to facilitate visualisations in data space of each step of the process the following steps are followed to create a som 1 construct a matrix of the input data samples 2 preprocess the data matrix standardise normalise transform 3 determine the size and shape of the map grid to best represent the data structure 4 initialise the map over the data drape it over in the principal directions 5 train the som bend and stretch the map to better match the data 6 place data items on the map by matching to their closest map node guidance on the practical implementation of each step in matlab is included in the supplementary materials with segments of basic matlab code for adaptation the code uses the matlab som toolbox vesanto et al 2000 which is available for free download from the website of the laboratory of computer and information science at the helsinki university of technology website 4 an ample supply of open source software is also available for producing self organising maps for example in r wehrens and kruisselbrink 2018 python wittek et al 2017 and java ultsch moerchen 2005 4 1 step 1 consolidate input data 4 1 1 input matrix construction the input data must be in a specific form to be read by a som matrix formation with rows consisting of the data items each separate observation or measurement makes up a row and columns consisting of variables dimensions as in table 1 this format ensures that each of the n items of input data is in vector format xi where i 1 n all vectors are of the same dimension d each dimension is an observation variable such as precipitation temperature ph population level or observations of a certain variable at multiple spatial locations or times in artificial neural network terms the values in the input matrix become numerical weights for the variables with respect to each input sample there is no rule for the amount of training data needed for som creation however due to the stochastic nature of environmental data it can not be assumed that a model made from one set of training data will represent all underlying relationships in the system and results will improve as the quantity of training data increases kingston et al 2005 4 1 2 missing data the som is able to function with a large proportion of missing data for instance if some measurements are missing information for certain variables in this case data is matched to the map nodes based only on the variables for which data is available when calculating the distances between the input item and the map nodes the missing data is excluded from the distance calculations and the data is mapped to the nodes that are closest in euclidean distance based only on the variables that have values present in that data vector as the distance calculations for a data item to all nodes will omit the same variable of data the results are comparable vesanto 2000 4 1 3 categorical data the som is designed to represent data in which the magnitude of the values has meaning categorical data can be incorporated by mapping it to ordinal data if the categorical levels have a natural order or hierarchical structure this is done by labelling each sample with a number ensuring that the numeric labels are in a logical order and not arbitrarily assigned for example categories labelled 2 and 3 must be more similar than categories 2 and 10 for the resulting self organization to have a meaningful interpretation another method for working with categorical data in soms applications is to use one hot encoding to produce dummy variables replacing the categorical variable with binary variables for each category level 4 2 step 2 preprocessing environmental variables naturally consist of different measurement scales and types of data normalising the columns of the input data matrix before map training ensures that variables with greater magnitudes or variances do not overshadow variables that may be less diverse of smaller magnitude or measured in different units this roughly equalises the contribution of each variable to the results kohonen 2001 overlooking the preprocessing step risks causing the main axis of the map to be principally aligned with the variable of largest magnitude thereby producing a map that is mainly representative of this variable as discussed in clark et al 2016 common preprocessing methods are transformations of the input matrix that equate either the variances or minima and maxima of each dimension 1 normalising the columns by scaling the variances to 1 around a mean of 0 or 2 a linear transformation equalising the minima and maxima of each variable another method though far less common is to scale the variance of all variables separately to reflect their perceived relative importance kaski and kohonen 1996 highly skewed variable distributions may benefit from logarithmic transformations agarwal and skupin 2008 4 3 step 3 parameter selection a number of parameters require specification prior to the creation of a som parameter choices include map size the number of nodes map shape the configuration of nodes the initial and final radii of the smoothing kernel and the smoothing kernel shape each application of the som method requires the choice and evaluation of appropriate parameters som parameters cannot be optimised by maximum likelihood estimation as the som training algorithm does not attempt to optimise any particular objective function erwin et al 1992 instead parameter selection requires ample time and experience making this a great disadvantage of som implementation for the non expert gopakumar et al 2007 the final results of the som are influenced by the choice of parameters and arbitrary parameter selection has the potential to lead to maps that fail to reveal portions of the data structure and intervariable relationships flexer 1999 vesanto 2000 kohonen 2001 2013 cereghino and park 2009 liu and weisberg 2011 wang et al 2013 astudillo and oommen 2014 deliberate parameter choices must therefore be based on an understanding of available options and the benefits of each choice common techniques for selection of these parameters range from the application of heuristics the minimization of various error measures or trial and error to produce the most agreeable visualization for the user these decisions are discussed in this section default software parameter values are described as well as more tailored options and the benefits are outlined for choosing values beyond the software defaults objective function before beginning to investigate how parameters are chosen first it is important to realise why parameter selection for the som is particularly challenging in the parameter selection phase of model building objective functions are commonly optimised to aid the choice as they are able to provide a quantifiable assessment of the optimality of a set of parameters however it has been proven that the som training process has no objective function that is optimized exactly erwin et al 1992 pampalk 2001 yin 2008a the som training method cannot be quantified in a single mathematical expression instead it follows the gradient descent of a separate set of energy functions for each node erwin et al 1992 for this reason parameters must be chosen using other methods such as quality measures applied to maps created with different parameter sets as will be described below for more information on the lack of a soms objective function see yin 2008b 4 3 1 map structure 4 3 1 1 grid configuration the grid of map nodes is generally connected in either a hexagonal or rectangular lattice as shown in fig 4 with nodes located at each vertex this configuration determines the number of nearest equally close in map space neighbours for each node the nodes of a rectangular lattice have up to four nearest neighbours in a hexagonal lattice each map node has up to six nearest neighbours this difference influences the results of the som training process in which the locations of the nearest neighbours are updated by the same amount at each iteration a larger number of nearest neighbours leads to greater topological preservation of the input data structure and a more uniform final map and for this reason hexagonal lattices are often considered more effective kalteh et al 2008 rectangular lattices are popular however due to the easy presentation of the final output map on a simple rectangular shape as updating of the map during the training process involves the nearest neighbours surrounding each node nodes along the map edges have different mapping properties compared to those in the center this is known as border effects since there are no neighbouring nodes beyond the map edge these outer nodes have fewer surrounding neighbours and tend to be pulled towards the center the overall topology of the som grid is sometimes set as toroidal with the top and bottom rows and or the left and right columns joined together to avoid the border effects encountered with planar grids by creating finite but boundless spaces alternatively very large emergent soms can be used ultsch and moerchen 2005 4 3 2 map size number of nodes the number of map vectors used to represent the input data and therefore the size of the output som is an important choice to be made by the user the size of the output map will affect the final visualisation of the som including the level of information extracted as each node of the output map represents a characteristic pattern from the input data vesanto 2000 important differences between data items may be missed if the map size is too small and yet distinctions between map vectors may be insignificant if the map size is too large cereghino and park 2009 this is analogous to common model issues of over smoothing and over fitting the number of nodes also influences the applicability of the som for either clustering or visualisation with a smaller number of nodes producing larger clusters and a larger map size leading to a more spread out visualisation of the topological structure of the data flexer 1999 liu et al 2006 evaluated the sensitivity of soms to parameter selection and determined that larger maps lead to more accurate results by virtue of less pattern smoothing the extracted patterns are more similar to the actual patterns in the data the resolution of the map determines which clusters become visible kohonen 2013 a larger map size may produce a finer distinction between structures in the data and more accurate local estimation kohonen 2013 wang et al 2013 yet larger maps may contain nodes with no data matching them indicating that the patterns represented by these node vectors do not actually exist in the data set clark et al 2017 ultsch moerchen 2005 state that large maps few thousand nodes must be used if nontrivial structures in the data are to be extracted and projected onto the low dimensional map they suggest that the use of large maps eg esoms is akin to increasing the resolution of the projection from data space to map space website 5 smaller maps compress the data into a smaller possibly more manageable number of patterns for analysis therefore a trade off exists between the accuracy of representation of the data vectors and generalization of the extracted information when deciding on the number of map nodes to use in fig 5 maps of differing sizes are shown over the same data set with data items matching the same node coloured the same colour this shows the range of data represented by a single node of the smaller maps compared to the larger ones clark et al 2016 further discuss the implications of large ranges of data represented by individual nodes kohonen 2013 states that though the choice of map size is the most common question asked with regards to soms it is not possible to determine it beforehand the choice is often made with the use of quality measures or selecting the map that the user finds most interpretable these methods require estimating parameters through training the map multiple times and comparing the results astudillo and oommen 2014 cereghino and park 2009 som software generally specifies a default value for the number of map nodes based only on the number of input samples available see heuristics below however this method does not consider the possible cluster structure in the specific data set nor any user requirements for visualisation or analysis in many cases the number of nodes is set by estimating the cluster structure of the data set and equating the number of nodes to the number of expected clusters 4 3 2 2 quality measures map size is often determined based on quality assessments on a series of output maps quantifying the accuracy of the maps in describing the input data cereghino and park 2009 this method entails minimising some combination of error measures over the set of maps based on the primary objective or set of objectives of the user the necessity for more than one quality measure arises from the two competing goals of the som algorithm the approximation of the input data by the map vectors and the preservation of the input topography by the interconnected grid of map vectors dimension reduction and visualisation a number of quality measures can be used the most common are the quantization error qe kohonen 1995 a measure of the ability of the som to represent the input data qe quantifies map resolution measuring how closely the map vectors match the data vectors and topographic error te kiviluoto 1996 a measure of the preservation of the topology of the input data structure on the output map a combination of quantization error and topographic error is often used due to the trade off between vector quantisation and topology preservation as the qe decreases te will generally increase though not always so the user must determine the desired balance between them care must be taken with the selection as fyfe 2008 states sometimes the two conflicting criteria produce a visualisation which does not accurately reflect all the features of the data fig 6 shows the use of plots of qe and te vs number of map nodes assuming side ratios as described below under the map shape heading to aid the choice of map size requiring a compromise to be made between the competing processes quantization error the som algorithm chooses the bmu for each input by minimizing squared euclidean distances between the input items and map nodes the qe is the difference between each data point xi and its closest map unit mc averaged over all data points q e 1 n i m c x i 1 n i m c 2 x i 2 2 m c x i the optimal map for representing a data set in terms of vector quantization yields the smallest quantization error qe is useful for comparing the som to other clustering or vector quantization methods though it cannot be used to compare maps of different sizes unequal numbers of map nodes as qe will decrease as map size increases nor for comparing maps with different neighbourhood shapes since it favours maps with specific neighbourhood radii kaski and lagus 1996 topographic error the proportion of nodes for which the first and second best matching map units are not nearest neighbours on the map grid is summed over all inputs for each data point the bmu and second bmu are checked to see if they are adjacent t e 1 n i 1 n u x i where u x i 1 if the first and second bmus of x i are neighbours 0 otherwise one topographic error value represents the entire map a value of zero indicates perfect topology preservation note that the topographic error does not consider diagonal neighbours of the rectangular lattice and so a hexagonal lattice gives a lower te due to having more neighbours for each unit pena et al 2008 the distortion measure dm kohonen 1995 is often encountered in the literature in discussions on parameter selection though is seldom actually used as an error measure it is included here for information purposes distortion measure a measure of the pull that the data items are exerting on the map the dm can be thought of as either the amount that each map unit is pulled towards its influencing data points summed over all the map units or the amount each data point pulls on each of the map nodes combined distortion measure incorporates the neighbourhood function into the calculation of distances between each map unit and each of the data points dm i 1 n j 1 m h ij m j x i 2 where hij is the value of the neighbourhood kernel centred on the bmu of xi at the location of mj the distortion measure differs from the quantization error in that each squared distance is weighted by the value of the neighbourhood function qe and dm are equivalent when the neighbourhood size includes only a single node distortion is larger for larger neighbourhood sizes as the neighbourhood function and the distances from each data item to its bmu decrease with each training iteration the distortion measure decreases as training progresses eventually the plot of distortion flattens out the map is still distorted but no longer updating dm is useful for comparing maps of equal size but not for comparing between differing map sizes 4 3 2 3 heuristics heuristics or rules of thumb are commonly used to chose map size as they provide easy and quick results the most commonly used heuristic for determining the number of map nodes m recommends that it should be approximately 5 n where n is the number of samples in the input data set vesanto 2000 though this method relates map size only to the amount of input data and not to the actual data values or structure it is the default method used in the matlab som toolbox code other heuristic recommendations include the number of neurons nodes should usually be as big as possible website 4 one should try for about 50 hits per node on average kohonen 2013 or if an unlimited number of inputs is available one may try to use as big an array as one is able to compute kohonen 2013 it is noted that each of these methods contrast sometimes greatly with each other however their use remains popular in the literature 4 3 2 4 cluster structure map size can also be specified based on the number of clusters that are determined to exist in the input data attempting to provide one node to represent each estimated cluster an alternative would be to create a series of different sized maps choosing the one that produces the lowest cluster validation measure the existence and number of clusters can be determined through cluster theory in practice the most popular currently used method for map size selection in environmental application papers is the production of a series of maps of different sizes followed by a graphical or visual comparison of the output maps there is no overall preferred method of evaluation shared by all soms users though the choice appears to be most commonly based on the degree of generalisation and number of clusters desired in the output next in popularity is the use of a combination of quality measures then default software heuristics followed by a variety of individual ad hoc methods many papers do not give any information about the rationale of map size selection inclusion of this information would allow the reader to understand if there is a reasonable basis to believe the number of nodes accurately represents the cluster structure of the data set or if other map sizes may reveal a different cluster structure 4 3 2 5 map shape ratio of grid side lengths the best representation of the data will be obtained when the shape of the grid roughly corresponds to the shape of the data structure for example a two dimensional square shaped data set would not be best represented by a rectangular grid with one direction much longer than the other as illustrated in fig 7 establishing the ratio of map side lengths based on the main intervariable relationships in the input data is the method recommended by kohonen 2001 and is the most commonly used procedure this is done by initialising the axes in alignment with the most important linear correlations given by the first and second principal components through training the axes will be bent and stretched eventually coming to follow the most important nonlinear correlations in the data the primary axis will represent the most significant relationship between data dimensions the nonlinear line of best fit and the secondary axis the next most important relationship this method of assigning the side length ratio is performed with the following steps vesanto 2000 1 determine the eigenvectors and eigenvalues in the data from the autocorrelation matrix 2 set the ratio between the two sides of the grid equivalent to the ratio between the two largest eigenvalues and 3 scale the side lengths so that their product l 1 xl 2 is as close as possible to the number of map units determined above 4 3 3 training parameters 4 3 3 6 neighbourhood function the neighbourhood function h h ij is a smoothing kernel applied to the map grid during training the kernel controls the smoothness and generalisation of the mapping by defining its rigidity a matrix item hij is the value at mj of the neighbourhood kernel centred on the bmu of xi for example the value of h 2 5 in table 2 gives the influence on node 5 of data for which the bmu is node 2 each column of h represents the influence on map node mj of the data items matching all map units mi each in a separate row the influence that the data in the voronoi set of mi exerts on other nodes is given in the rows each row consists of values for a surface with a peak at mi mj this indicates that data with bmus closest to node mj will have the most influence on the updating of mj the neighbourhood kernel is generally normalised so that each column sums to 1 equalising the sum of the influence exerted by all data items on each map node at each training iteration the location of each map node mj is updated based on all the data items that are matched to nodes within the specified neighbourhood radius centred at this node the neighbourhood size and therefore the extent of influence of the data items on the map units decreases linearly over the training iterations though the shape of the neighbourhood remains constant throughout training this decrease in kernel size leads to an increased smoothing of the map the size and shape of the neighbourhood kernel must be determined by the user neighbourhood size a large neighbourhood kernel results in a stiff map by overstressing topological ordering and a small kernel results in freer movement of the nodes toward the data vesanto et al 2003 the increased topologocial ordering of maps created with larger neighbourhoods comes at the expense of data quantisation how close the nodes are to the data they represent which improves as neighbourhood size decreases for this reason a compromise is made a large neighbourhood kernel is used at the beginning of training to induce a global ordering of the map nodes and the kernel diminishes in size with each training iteration the node locations are eventually finetuned within a small neighbourhood at the end of training the starting and finishing neighbourhood sizes can be specified by the user the neighbourhood radius is measured in map space not data space fig 8 shows the group of nodes contained within a neighbourhood of radius 0 1 and 2 around a node of a hexagonal and rectangular grid on a hexagonal grid a neighbourhood kernel of radius 2 will incorporate data from 19 nodes whereas on a rectangular grid the same size kernel would incorporate data from 13 nodes the choice of initial neighbourhood size has an impact on the results if it is too small the map may not achieve an appropriate global overall ordering kohonen 1990 hastie et al 2007 kohonen 2001 recommends setting a starting neighbourhood approximately half the largest side length of the map to prevent the risk of ending in a local minimum the final neighbourhood radius usually includes only a single node kohonen 1993 the map loses its spatial interaction at this point and the som becomes equivalent to k means clustering hastie et al 2007 if global ordering has been successful this will still produce the desired results as the grid connections are maintained kohonen 2001 explains that equating the som to the k means algorithm at the end of training through diminishing the neighbourhood to include only a single node guarantees the most accurate approximation of the probability density function of the input and should also eliminate any issues the neighbourhood function may encounter at the borders of the map neighbourhood shape four shapes are commonly used for the nieghbourhood function uniform gaussian cut truncated gaussian and epanechnikov parabolic these shapes are used in the matlab som toolbox with gaussian as the default vesanto et al 2000b table 3 describes the four shapes where σ t is the neighbourhood radius at iteration t dci rc ri is the distance between map units mc and mi on the map grid and 1 x is a step function taking a value of 0 if x 0 or 1 if x 0 updating only the nodes for which the function is nonzero the gaussian kernel is the only one that incorporates the entire input data set to update each of the map nodes and conversely uses each data item in the updating of all of the nodes the other three kernels only update within the specified radius the gaussian neighbourhood therefore produces the smoothest som patterns liu et al 2006 whilst the others have various degrees of smoothing epanechnikov the least for a fixed σ t common to all kernels table 4 gives sample values of neighbourhood kernels for each shape centred around the middle nodes of a 3 3 and 7 7 som higher values indicate which nodes would be updated influenced by the data by a greater amount erwin et al 1992 found that the som s convergence rate is heavily dependent on the shape of the neighbourhood function and the training algorithm is more effective when a convex neighbourhood function is used rather than a concave one ota et al 2011 describe the use of an asymmetric neighbourhood function to remove topological defects which frequently emerge during training and inhibit the global ordering of the map 4 3 3 7 training length the final statistical accuracy of map how well the data is represented depends on the number of iterations since learning is a stochastic process kohonen 1990 however with modern computational resources this should no longer be an issue and software defaults should be adequate there is no upper limit to the number of iterations that can be used in principal the global ordering stage with a large neighbourhood radius can be relatively short compared with the finetuning stage with the smaller neighbourhood radius 4 3 3 8 mask a mask may be applied during the map training process to weight the influence of each variable in the distance calculations for determining bmus the mask is a vector with the same number of dimensions as the input data mask values indicate the relative importance of each variable usually with 0 s and 1 s it can be used to hide certain variables or make others more influential in map training if the user would like to accentuate the significance of certain variables over others note that the mask is only used for finding bmus and is not used in the initialisation stage 4 3 3 9 associated variables introducing a new set of variables onto a trained som that has been created with a different set of variables may enable researchers to discover interesting intervariable relationships cereghino and park 2009 these new variables are known as associated variables as they are linked to the map after training the trained map can be clustered or labelled based on values of the associated variable to visualise the relationships 4 4 step 4 initialise the map the map is generally initialized with a regular linear array set in the directions of highest variance of the input data vectors as determined with principal component analysis the initial values of the map weight vectors are set at uniform intervals along the first and second principal components of the input data set which come to form the axes if the axes lengths are proportional to the two largest eigenvectors of the data this should produce an approximately uniformly spaced lattice this linear form of initialisation is usually used as it ensures the map is already aligned with the most significant linear intervariable relationships before map training begins fig 9 illustrates the alignment of the axes with the principal components as som training is an iterative process of multi dimensional nonlinear optimisation it has the potential to lead to multiple optimal solutions kingston et al 2005 meaning that for the same input data the possible output maps include rotations or inversions of each other the orientation of the final map is dependent on the initial values assigned to the nodes with different sets of initial node locations leading to rotations mirror images or symmetric inversions of the final map kohonen 1990 4 5 step 5 run the training algorithm running the training algorithm through the iterations of the two step matching and updating process re organises the linearly initialised grid of map nodes into a nonlinear arrangement amongst the data items while maintaining the grid connections hastie et al 2009 describe the initialised map nodes as buttons sewn onto the principal component plane in a regular pattern the training process of the som then bends and twists the plane so the buttons best approximate the data the map is smoothed by the updating process in which the new node locations are computed based on the previous locations and the locations of the data items through the maintained grid connections the map nodes organise themselves based on their similarity to each other the trained map will now better approximate the data set than the initialized map with more map units positioned in areas of higher density input space the vectors describing the location of each map node of the same dimensions as the input data vectors have come to represent the most prevalent patterns of unique variable combinations in the data at this stage the algorithm can be run for a number of parameter sets map size and shape neighbourhood kernel size and shape and the results compared through the use of quality measures to choose the map that best preserves the topology quantisation or clustering of the input data or any combination of user objectives 4 6 step 6 place data items on the map after the map has been created each data item finds a place on the map by matching it to its closest most similar map node because the nodes are organized based on their similarity similar input data will become mapped to the same or nearby map nodes the matching is based on a high dimensional similarity measure usually euclidean distance as in the matching stage of map training each data item will have a unique best matching map node but each map node may be matched to more than one data item or none at all placement of the data items on the map leads to the identification of clusters and discovery of relationships between data items 5 interpretation of output map the aim in analysing a som is to identify the key characteristics comprising the predominant patterns in the data set pattern extraction and discover which data items are similar to each other with respect to these characteristics clustering characteristics of the predominant patterns are revealed by the high dimensional vectors associated with each map node as the map becomes organised in data space during training the location of each map node is defined by the combination of dimension values that make up its vector as the data items are matched to their nearest map nodes after training clusters are formed of data items sharing similar characteristics these characteristics are identified by the pattern of the common node map nodes may also be grouped together to form larger clusters of data in their voronoi sets interpretation of a som generally includes a compilation of information through the visual investigation of the labelled map in one or two dimensional output space and the component plane for each variable the visual investigation will reveal the prevalent patterns indicated by the individual node vectors and the clusters of the data nearest to each node investigation of the output map should also disclose a good approximation of the input data distribution including the overall shape and cluster structure in the data characteristics of the clusters and the relationships between variables the results will allow for trend visualisation and infilling of missing data 5 1 visualisation the som is usually interpreted by means of the overall output map as well as individual component planes the output map restricted to show only individual variables 5 2 output map the output map is usually displayed as a regularly spaced grid in one or two dimensional map space labelled with the data items or the main characteristics of the data items that pertain to each node the output map may also be displayed in data space if the dimension of data space is low in map space the distribution of the projected data items across the map is evident whereas in data space the distribution of the map nodes amongst the data items is evident the differences between nodes are often shown by coloured markers or a surface plot an output map of the example data from fig 2 is shown in fig 10 in both data space and map space in data space a the interconnected map grid is shown in black over the coloured data points revealing the placement of the nodes amongst the data in map space the nodes are coloured by similarity with empty nodes remaining white b and data item labels are placed onto the relevant areas of the map c labelling the nodes on the output map as in fig 10c gives an indication of the characteristics of data items represented by each region of the map labelling can involve listing each data item over the node it is assigned to or choosing a representative label for each node based on the group of data it represents in the latter case nodes may be grouped into larger clusters before labelling based on predominant cluster attributes to label the nodes or clusters with representative labels in consideration of the cluster centroid map node variable weights plots of cluster vs dimension and dimension vs cluster are useful to indicate which dimensions are most accounted for in each cluster fig 11 demonstrates how these plots can be used with a 24 dimensional 8 cluster example in the plot on the left a each line represents a separate cluster with the y axis indicating the values weights of each dimension at each cluster centroid it is possible to pick out for example that dimension 19 has low weights in all clusters whereas dimension 4 is prominent in 3 separate clusters on the right b each line represents a dimension with the y axis indicating the dimensions value at the cluster centroid for the 8 clusters of this example it is possible to pick out for example which dimension has by far the largest weight in cluster 6 this method can be used to choose the top weighted dimensions for labelling each cluster another option is to label a map with associated data new data that has not been used in the training as discussed in the section on associated variables this shows where the new data would plot on a map trained with other data defining the relationships between the data sets the axes of the output map establish a meaningful nonlinear coordinate system for the various features of the input data during the self organisation process kohonen 2001 the axes begin the training stage as the first and second linear principal components of the data set and then gain nonlinearity as iterations progress while vesanto 1999 states that the axes of the map grid rarely have any clear interpretation it is possible to form a general perception of their meaning through investigation of the component values of the map vectors along the edges of the map this can be done by careful analysis of the component planes 5 2 1 component planes relationships between the individual variables can be explored with the use of component planes colouring is used to indicate dimension weights values at each node with a separate component plane displaying values of each dimension of the som the axes and grid nodes correspond exactly to those of the som output map inspection of component planes indicates the spread of values in each dimension vesanto 1999 the presence of interesting relationships between variables can be visually determined from the component planes allowing these relationships to be further investigated with scatterplots of the subset of variables of interest plotting component planes of associated variables those not used in map training shows the relationship of new variables to those used to create the map in fig 12 component planes produced from the som in fig 10 indicate that data items located on the lower right of the map have high maximum discharges and medium minimum discharges whereas those located in the lower left have more moderate maximums and relatively high minimums the component planes provide a meaningful interpretation of the axes the axes of the som follow the main nonlinear directions of variance in the data set and by identifying regions of the component planes with high and low values of each variable the general gradient of individual variables along the axes should become evident these gradients will be continuous along the axes though not necessarily monotonic in direction 5 3 pattern extraction the prevalent patterns in the data set are exhibited by the vectors of each map node the unique combination of variables making up each vector is a characteristic pattern these combinations can be analysed via the component planes an analysis of frequencies of occurrence of each pattern is obtained by looking at the percentage of input data assigned to each map node each matching of a data item to a node is known as a hit transitions between patterns can be observed by matching the data items to the map in a sequential order and following the trajectories of the hit locations 5 4 cluster identification clustering is the most frequent reason for implementing a som and a number of methods exist for finding clusters in the data basic or first level clustering involves treating each som node as a cluster centroid as each input data item is uniquely related to one of the nodes clusters are created of data that share similarities based on the features of the extracted patterns each node of the output map comes to represent a cluster to use this method effectively it is best to determine the approximate number of clusters that exist in the input data using a cluster validity measure5 3 4 before setting the number of nodes the number of nodes should be set equal to or greater than this to ensure that each cluster is mapped to a separate node 5 4 1 visualising the space between clusters 5 4 1 10 unified distance matrix u matrix the u matrix ultsch 2003 visualises distances between regions of the data space represented by each node by computing high dimensional similarities between neighbouring nodes how close they are in data space the u matrix determines cluster boundaries based on large dissimilarities ultsch 2003 this is also known as the degree of distortion that is the change in relative distance between the high dimensional locations of the nodes in data space and the low dimensional map representation agarwal and skupin 2008 the distances are indicated on the u matrix map by colour with differing colours indicating the boundaries between clusters it can be seen on the u matrix in fig 13 a that distinct clusters blue exist separated by lighter coloured boundary areas 5 4 1 11 similarity colouring similarity colouring involves spreading a two dimensional colourmap over the principal component projection of the nodes thereby colouring similar nodes similar colours further apart more different nodes become coloured with colours that are perceived as more distinct kaski et al 2000 the similarity colouring and empty map nodes in fig 13 b reveal the same cluster structure as the u matrix on this plot the clusters evident in the lower portion of the map are outlined 5 4 1 12 hits plotting the number of data items matched to each node known as the hits on the output map may allow cluster structure in the data to become visible high intensities of data might become evident on clearly separated regions of the map nodes with zero or relatively low hits delineate the cluster borders using a surface plot to record the hits will produce raised regions of the map where the clusters exist linearly scaling the size of the output map nodes in proportion to the number of hits each receives provides another method of visually indicating cluster structure fig 13 c shows the use of hits to confirm the cluster structure 5 4 2 second level clustering second level clustering is used to find groups of nodes that themselves make up a cluster this is useful when creating a map with a large number of nodes compared to the number of clusters in the data which may be done to gain a good separation between data items on the map it is known as second level clustering since the data has already been clustered with the som first level clustering second level clustering is useful for producing summaries or descriptions of som results second level clustering groups the som nodes either with another som or with a different technique as discussed below the high dimensional node vectors are clustered in data space and the cluster memberships are projected onto the low dimensional map for visualisation with the subsets of nodes grouped together by colour or outlines all the data matched to any of the member nodes are now members of the second level cluster as might be expected the weights of the most influential variables will change rapidly at the borders of the clusters kaski et al 1998 toth 2009 found second level clustering of a larger som to be more suitable for preserving the distinctive features of the classes when compared with using a smaller initial map size some common methods of second level clustering include visually assessing the distance in data space between nodes with the u matrix similarity colouring or the number of hits in each region another som partitive clustering such as k means and hierarchical agglomerative clustering 5 4 2 13 k means clustering k means clustering is a popular method for second level clustering as the topological preservation of the data has already been captured in the first som k means clustering produces good results if the clusters are compact hyper spherical and well separated determining the optimal k value or number of clusters to extract can be done with the cluster validity measures described below 5 4 2 14 som for second level clustering a som can be used for second level clustering in this method the node vectors of the first som become the input vectors for the second som an example is shown in fig 14 in which a 5 2 som is used to cluster the output of an 18 18 som reducing the number of clusters from 324 to 10 results produced are similar to k means clustering with the added benefits of maintaining an order to the clusters and allowing presentation in the familiar som output 5 4 3 hierarchical clustering hierarchical clustering eg ward 1963 can be used to determine many levels of progressively larger clusters on the map an advantage of hierarchical clustering over k means is that many nested levels of clusters can be shown simultaneously on one output map hierarchical clustering can also be performed based on an associated variable another form of agglomerative hierarchical clustering is neuron label clustering used by skupin et al 2013 in which neighbouring clusters are merged if they share the top ranked label term most influential dimension this can be repeated for the second ranked label terms etc to get many separate cluster layers 5 4 4 cluster validity measures if attempting to have each node of the map represent a cluster of the data it is necessary to estimate the number of clusters present in the data set before the soms analysis begins there are various methods available for this which can also be used to determine k in k means clustering the davies bouldin index davies and bouldin 1979 is a popular measure for determining the number of clusters present in a data set and used in the matlab som toolbox vesanto et al 2000 it measures the ratio of within cluster to between cluster distances a small ratio indicates compact well separated clusters the dunn index dunn 1973 and silhouette coefficient rousseeuw 1987 are also used to determine the number of clusters both are also concerned with the ratio of inter cluster to intra cluster distances silhouette clustering determines how appropriately the data has been clustered by ranking data points as well clustered 1 would be better in neighbouring cluster 1 or on the border of two natural clusters 0 and taking the average over the entire dataset clusters with narrower silhouettes than the rest will appear if there are too many or too few clusters the elbow method finds the kink in the curve of cluster dissimilarity as a function of the number of clusters the value on the x axis where the curve begins to decrease less rapidly may indicate the number of natural clusters in the data hastie et al 2009 the gap statistic tibshirani et al 2001 can be used as an automated way of locating this point on the curve these are just a small selection of existing clustering indices charrad et al 2014 provide an automated means to evaluate 30 different indices for determining the number of clusters in the data this returns the optimal clustering scheme for the presented data set out of all of the tested options 5 5 trend visualisation infilling missing data prediction incorporating new data an indication of temporal changes trends in data sets can be gained through soms visualisations using any of the following methods a som is trained with all the available input data and changes over time in the data mapping to each node or second level cluster reveal temporal trends in the data structure all the data matching one map node could be used as input for a local prediction model trajectories or lines connecting the bmus of consecutive data points in a time series may be used to indicate trends consecutive sections of the input data ie years or decades can be plotted on to the map to visualise changes or temporal patterns may be extracted based on trends of the cluster centroids when a separate map is created with data from each time step to use soms for infilling missing data the best matching map node can be found based on the available variables of the data item and then the value of the missing variable adopted from the node vector a similar method of value adoption based on established intervariable relationships is used for prediction the best matching map node can be found based on a set of easily predicted variables and then the value of the unknown variable adopted from the node vector the incorporation of new data onto the trained and clustered map reveals how it relates to the other data items based on the established relationships and clusters new data can be added to the input data set during step 6 placing the data on the map it will be placed into the established clusters and can be compared to the other cluster members 6 conclusion soms have wide applicability and many characteristics that make them particularly well suited to water resources data if they are employed consciously the results will reveal more information about the relationships of the underlying systems water resources engineers and scientists interested in using soms to knowledgeably explore high dimensional nonlinear data sets now have an accessible resource to do so this paper leads researchers through the creation and interpretation of a som relevant to a specific data set providing a guide to understanding meaningful parameter choices and interpreting som results for data driven exploratory analysis of their particular data improving each individual application of the som will lead to greater insight into the inter component relationships involved in water resource systems websites 1 scopus refined by earth and planetary sciences and environmental sciences accessed june 2017 2 google scholar accessed june 2017 3 environment canada s hydat database national water data archive https ec gc ca rhc wsc default asp lang en n 9018b5ec 1 accessed july 2017 4 http databionic esom sourceforge net user html 5 http www cis hut fi somtoolbox accessed july 2017 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103676 appendix supplementary materials image application 1 
438,environmental measurements generate great volumes of high dimensional data often noisy and with missing values from which meaningful messages may be extracted through appropriate organisation and summarisation the self organizing map som is an artificial neural network popular for recognizing patterns relationships and clusters in such data through the finetuning of configuration and training parameters the som can be tailored to best suit a specific data set presently most hydrologic applications continue to rely on heuristics software defaults or arbitrarily chosen parameters forfeiting some of the potential benefits of the method and often resulting in an output that does not best represent the actual features of the data this paper guides researchers through the knowledgeable creation and interpretation of an appropriately customised som relevant to their particular high dimensional nonlinear data set an understanding of the effects that parameter selection and training options have on the extracted information is developed practical guidance is given for appropriately modifying parameter sets and comparisons are made with closely related methods data pre processing parameter selection initialisation training visualisation and interpretation of the output map for pattern extraction and clustering are discussed through individually customised soms more meaningful information can be gained from data driven exploratory analysis of the inter component relationships involved in water resource systems keywords self organizing maps parameter selection implementation interpretation tutorial guide error measures 1 introduction water resources measurements often collected over large spatial areas at high temporal resolutions produce great volumes of high dimensional data which must be organized and summarized to extract meaningful messages for use in decision making processes appropriate organisation and summarisation of the data will reduce the available information into a manageable number of characteristic patterns and intervariable relationships the self organizing map som kohonen 1990 is an artificial neural network useful for extracting patterns and finding clusters in large multivariate data sets it is an increasingly popular method used in hydrology water resources research and engineering with approximately 180 articles currently published each year using soms in these fields for clustering spatiotemporal analysis time series analysis infilling of missing data prediction and trend visualisation the popularity of the som method in water resources is due to its intuitive implementation resilience to missing and noisy data common features of measurements collected remotely or in the field ability to integrate real time data and straightforward visual summary of the system and intercomponent relationships relationships of hydrologic variables to physical chemical social or economic systems can also be investigated without requiring expert knowledge from each of the varied disciplines since the som organizes the data with no requirement for an explicit understanding and description of the complex underlying systems that may have produced the data the som extracts the most prevalent patterns in a data set and clusters the data items around these patterns organising the results into an intuitively understandable low dimensional visualisation or map a large number of high dimensional observations become represented with a usually much lower and therefore more manageable number of low dimensional vectors which form centroids for clusters of the original data this facilitates analysis of the properties of the data set through analysis of the ordered low dimensional cluster structure and cluster members the som provides various benefits over other methods for the exploration of environmental data the number of clusters present in the data does not need to be specified before hand the resulting clusters are ordered on the output visualisation indicating similarities and dissimilarities allowing for further analysis if desired creation of a som does not require an explicit understanding of the complex and potentially undefinable processes and relationships within the system that produced the data the analysis is conducted based only on the data presented to it the som can therefore be used to explore complex multi disciplinary or human environmental data sets in which the intervariable relationships are difficult to explicitly quantify in these cases the creation of an elaborate mathematical system model would be complex and time consuming another distinct benefit of the som is the ability to incorporate new data after the map is created providing a tool for online real time data analysis which is useful for the real time processing of sensor data pattern identification clustering and data visualisation together reveal information that may be otherwise unobservable in large data sets other techniques exist for each of these processes separately but the combination leads to the soms unique analysis capabilities for example clustering methods that are not combined with dimension reduction are not easy to visualise and therefore can be less readily interpreted and conveyed though soms are widely used and increasing in popularity for environmental applications uncertainty remains in the soms method due to the two inherent competing goals of approximating the data with the map nodes and preserving the topology of the data set which negate the possibility of specifying a single objective function that the map aims to optimise this gives rise to complications in parameter specification as different choices will improve some aspects of the results and possibly inhibit others parameter choices impact the formation of the output soms with different maps resulting from the use of distinct parameter sets it is therefore important for the analyst to appreciate and understand the parameter options available a sole reliance on software default parameters may result in maps that are not the most suitable size and shape to represent a particular data set or the level of smoothing and generalisation provided on the output map may not suit the specific purposes of the analysis interpretations of the data based on such maps may reveal less information about the data set than there is potential to uncover with respect to the distributions of individual variables and the intervariable relationships analysts must make an informed choice between the options based on the relative benefits of each choice soms literature tends to come in two streams 1 statistical analyses of the som algorithm published in statistics or neural networks journals not easily accessible to researchers from other fields and 2 applications using soms as a black box tool with little or no informed input from the user the literature provides little accessible guidance to lead non statistical researchers through the process of som implementation and interpretation we have found no paper unifying the information needed to understand the best current standard of the som method and in a timely manner to knowledgably create a som to best represent a specific data set through alteration of the som s configuration and training parameters water resources researchers may not have the time and inclination to sift through the technically focused statistical publications in which information required to do this is found instead we have noted that researchers are tending to revert to heuristic or software default parameter sets or are borrowing parameters from som models that have been used in other applications but are not particularly relevant to the specific data set at hand this does not lead to the production of the best maps for representing each unique data set this paper is provided as a guide to aid researchers in understanding current best practices for the informed application of soms within a context of soms background theory the reader is guided through the production and interpretation of a suitable som for the exploratory analysis of their specific multivariate nonlinear data set relevant applications parameter choices code implementation and interpretation of output maps are discussed key references are provided in each section to lead the reader to further resources 2 soms use in water resources with environmental data sets the pattern extraction clustering and visualisation capabilities of the som are often applied to spatiotemporal analysis time series analysis infilling missing data and prediction patterns are extracted as representative system states with each node of the trained map representing a specific pattern these patterns are used as the basis to cluster common system states and determine the nonlinear relationships between variables clustering consists of identifying the number of natural clusters in the data and uniquely assigning each input item to one of the clusters the clusters are based around the most common patterns in the data as identified by the map nodes which form the cluster centroids applications using soms to determine sets of typical system state patterns have included investigations of groundwater types belkhiri et al 2018 coastal water quality li et al 2018 water consumption patterns padulano and giudice 2018 wind patterns duvivier et al 2016 wave climate states barbariol et al 2016 spatial patterns of groundwater properties nguyen et al 2015 choi et al 2014 and soil quality rivera et al 2015 frequency and transition matrices can be created by determining the percentage of data matching each pattern as in falcieri et al 2014 nguyen et al 2015 and swales et al 2016 newton et al 2014 further the use of soms for state frequency analysis by including the persistence of states and synaptic type frequency anomalies for each variable at each node abundant applications use the som for clustering data into similar subsets such as land use and surface water quality gu et al 2019 segments of coastline for ecological classification ramos et al 2016 rainfall and climate regions mannan et al 2018 markonis and strnad 2019 factors affecting cyanobacterial blooms hong 2016 and niveograph patterns wang et al 2013 as well as measurement stations satellite imagery data seasonal patterns sea level pressure and precipitation nonlinear relationships have been investigated using a som between spatiotemporal interactions between groundwater and surface water chen et al 2018 streamflow regimes and fish communities tsai et al 2016 atmospheric circulation and surface climate newton et al 2014 da anunciacao 2014 reservoir water quality in relation to watershed land cover types park et al 2014 relationship of fish larvae with environmental variables jutagate et al 2016 modern and medieval climate circulation patterns edwards et al 2017 spatiotemporal changes in lagoon water quality kim et al 2016 regional precipitation and large scale atmospheric dynamics liu et al 2016 atmospheric circulation and arctic sea ice extent lynch et al 2016 and water vapour transport and mass loss in the greenland ice sheet mattingly et al 2016 maticet al 2017 discovers patterns of salinity and temperature in oscillating adriatic sea regimes rodriguez alarcan lozano 2017 use the som as a decision support system for reservoir regulation vaclavik et al 2013 identify generic global patterns of land pressures and environmental threats by clustering data sets based on intensity of use environmental conditions and socioeconomic indicators vereecken et al 2016 investigate patterns of water mass and energy in soil vegetation atmosphere interactions 2 1 spatiotemporal analysis soms have most often been used to investigate patterns and clusters in the spatial or temporal aspect of the data separately either spatial patterns occurring at different times or temporal patterns occurring in different locations have been investigated and compared on a single map a popular method of spatiotemporal data analysis with soms involves the creation of a series of maps placed next to each other each representing clusters of data at different timesteps requiring the reader to visually extract and analyse the temporally changing spatial patterns another method is to create a single map with all the available data changes over time can then be visualised by plotting batches of consecutive data i e decades onto the trained map or creating trajectories by linking the map nodes that are nearest to each consecutive data point in the sequence either for the entire data set or as separate trajectories for specific data segments however these popular methods may not adequately capture and express the structure of the data if a separate som is produced for each time period these maps may not be directly comparable to each other due to differences in the distributions and correlations in the data at each time step if a single som is created from all data with separate time periods mapped to it the overall map may not accurately describe the finer structure of the data at each time step attempts have been made to apply the som to more innovative visualisations of spatiotemporal data some of these involve using the traditional som in novel ways and some involve extending the som algorithm itself wang et al 2013 creates a single som of twenty years of snow accumulation and melt patterns with high temporal and spatial resolution and then clusters the nodes and plots trajectories for specific spatial locations to reveal cyclical and long term trends wang simultaneously determined spatial differences in time series snow accumulation melt patterns between mountain ranges as well as changes in patterns over time at specific locations using trajectories sarlin 2012 developed the self organising time map sotm to visualise the evolution of the cluster structure across space and time the sotm is made up of a 1d som at each timestep arranged in order of ascending time multivariate temporal and cross sectional aspects are visualised at the same time allowing changing emerging and lost clusters in the data structure to become apparent 2 2 time series analysis time series analysis or trend visualisation using soms entails the identification of temporal patterns and clustering of data items where each data item is an individual time series this produces groups of data items that trend in similar ways soms are practical in time series analysis for outlining the boundaries of existing conditions clustering based on global characteristics extracted from the time series and identifying future directions vantas et al 2019 use a som for classification of rainfall time series 2 3 infilling missing data the som algorithm is quite insensitive to missing values in the data and can be used to infill missing multivariate data data items with missing values for certain variables are matched to their nearest node based only on the values that are present the data vector then adopts the node vector s value for the missing variable soms have been applied to infilling runoff data in inadequately gauged basins based on rainfall measurements nkiaka et al 2016 physiochemical parameters in water samples folguera et al 2015 and estimating water quality in unmonitored streams based on relationships between spatiotemporal watershed attributes and water quality in monitored streams 2 4 prediction patterns in future data may be estimated using the knowledge of inherent states determined to exist in the current data states that occur with certain frequencies and the transitions between states harnessing information from the established nonlinear relationships allows for the prediction of a future variable based on its association with easily predicted variables an extension of the infilling missing data technique this is done through the assumption that for the same conditions combinations of variable values the same patterns will occur the relationships determined by the som may be used to predict what could be expected from new data given the presence of a certain system state by plotting future data onto a map trained with current data the change in cluster membership indicates changing state frequencies predicted for the future chang et al 2016 incorporate the som into a monthly basin wide prediction of groundwater levels to be used in sustainable basin management 3 background developed in finland in 1981 by teuvo kohonen the self organizing map has steadily gained popularity since its introduction with explanatory papers kohonen 1982 1990 kohonen 1998 and kohonen 2013 receiving over 39 000 citations combined the general som method entails arranging a map grid usually consisting of a regular rectangular grid of connected nodes in an initial approximate location over a data set through an iterative training process the map nodes move amongst the data items self organise whilst maintaining their grid structure until their locations provide the best possible coverage of the data set without the map becoming unnecessarily twisted the grid is stretched and bent until the position and orientation best represent the data structure fig 1 shows a synthetic data set black consisting of three gaussian clusters with an initial som grid green placed in a preliminary location and the trained som grid blue better following the structure of the data the som has two main organisational goals pattern identification and topology preservation these are realised through the separation and iteration of two interacting subtasks during the training process matching and updating during the matching stage the closest map node or best matching unit bmu is identified for each item of data the map nodes are then drawn closer to their nearby data items during the updating stage with neighbouring nodes on the map grid moving towards the same data items as the map nodes settle closer to the data items in data space the result is more nodes situated in higher density areas of the data the full algorithm for map training is given in the supplementary materials file the specific location in data space that each map node occupies at the conclusion of training given by the local values of each variable are recorded in the high dimensional vectors of the map nodes this unique combination of variables represents a particular characteristic pattern of the data set following training the individual data items are matched to their closest map nodes on the trained map this produces clusters of data sharing a common nearest node and therefore sharing common key characteristics as identified by the variable values of that node similar data items become matched or mapped to the same or nearby map nodes and more different data mapped to more distant nodes the patterns and clusters extracted from the data during training are visualised on a low dimensional output map organised according to their similarity the degree of similarity between each cluster is proportional to the topological distance between them with closer areas of the map representing similar patterns in the data domain areas of data space with higher densities of data are represented on larger areas of the som when presented in two dimensional space the map is a nonlinear projection of a reduction of the data set as the high number of high dimensional data items become represented in a lower dimension by a usually lower number of map nodes the output map can therefore be more manageably explored than the original data set giving insight into the overall structure and prevalent patterns existing in the data fig 2 depicts the representation of a data set with a som both in data space and map space the trained map is not twisted and data items that are near each other in data space are represented by the same or nearby nodes the nonlinear regression performed by the som is considered nonparametric as it is based on fitting a number of ordered discrete map vectors to describe the distribution of input samples the som is deemed an unsupervised process as it searches for unknown structure in unlabelled data unsupervised learning is closely related to density estimation in that it constructs an estimate of an unobservable probability density function pdf based on the data that is presented if the input samples have a well defined pdf the som map nodes will eventually come to approximate it in an ordered way producing a nonlinear projection of the high dimensional pdf of the input variables onto a low dimensional display kohonen 1995 3 1 comparison with k means clustering and principal component analysis the soms algorithm exists in a theoretical region of knowledge between principal component analysis pca and the k means clustering method with pca providing the most rigid analysis of the three and k means the most flexible pca a type of factor analysis transforms a data set into a small number of linear uncorrelated variables or principal components pcs which become axes of the projection space the set of data points are rotated around their mean to align with the new set of axes which point in the directions of maximum data variance the first principal component is defined by the eigenvector of the data set with the highest eigenvalue and the axis in this direction accounts for as much of the variance in the data as possible pca separates clusters of data items well but is not ideal for representing nonlinear data as the pcs are always linear the som algorithm uses pca during the initialisation stage aligning the initial axes of the map grid along the principal component plane soms are considered a constrained form of the k means clustering algorithm with the nodes constrained to a two dimensional manifold hastie et al 2009 the som method becomes equivalent to the k means algorithm at the end of training when the som neighbourhood kernel is so small it only contains a single node the k means algorithm is proficient at clustering though it does not retain information on cluster ordering and encounters issues in visualisation when the dimensions of input data are high when using the k means algorithm the user is required to specify the number of clusters as a parameter which may be an unknown attribute of the data set the som is beneficial over the k means algorithm for real time analysis of online data and for data sets with high levels of missing data laerhoven 2001 schroeder et al 2009 yin 2008b elaborates on the relationship between soms and pca fig 3 depicts a comparison of k means som and pca results on a single data set source bache and lichman 2013 it can be observed that all the techniques separate the two main clusters in the data k means and soms both place a user specified number of cluster centres amongst the data and the som results are similar to k means except that the grid structure retains an ordering to the clusters the techniques each perform well when compared with certain aspects of the som however if clustering nonlinear projection and an ordering of cluster information is important the som provides these attributes within a unified technique 4 creation of a self organizing map this section provides a guide for the creation of a self organizing map for the representation of a data set an example data set is used for demonstratation from environment canada s hydat database website 3 this data was purposely chosen to have only two dimensions annual minimum and maximum daily streamflow values at a selection of river stations to facilitate visualisations in data space of each step of the process the following steps are followed to create a som 1 construct a matrix of the input data samples 2 preprocess the data matrix standardise normalise transform 3 determine the size and shape of the map grid to best represent the data structure 4 initialise the map over the data drape it over in the principal directions 5 train the som bend and stretch the map to better match the data 6 place data items on the map by matching to their closest map node guidance on the practical implementation of each step in matlab is included in the supplementary materials with segments of basic matlab code for adaptation the code uses the matlab som toolbox vesanto et al 2000 which is available for free download from the website of the laboratory of computer and information science at the helsinki university of technology website 4 an ample supply of open source software is also available for producing self organising maps for example in r wehrens and kruisselbrink 2018 python wittek et al 2017 and java ultsch moerchen 2005 4 1 step 1 consolidate input data 4 1 1 input matrix construction the input data must be in a specific form to be read by a som matrix formation with rows consisting of the data items each separate observation or measurement makes up a row and columns consisting of variables dimensions as in table 1 this format ensures that each of the n items of input data is in vector format xi where i 1 n all vectors are of the same dimension d each dimension is an observation variable such as precipitation temperature ph population level or observations of a certain variable at multiple spatial locations or times in artificial neural network terms the values in the input matrix become numerical weights for the variables with respect to each input sample there is no rule for the amount of training data needed for som creation however due to the stochastic nature of environmental data it can not be assumed that a model made from one set of training data will represent all underlying relationships in the system and results will improve as the quantity of training data increases kingston et al 2005 4 1 2 missing data the som is able to function with a large proportion of missing data for instance if some measurements are missing information for certain variables in this case data is matched to the map nodes based only on the variables for which data is available when calculating the distances between the input item and the map nodes the missing data is excluded from the distance calculations and the data is mapped to the nodes that are closest in euclidean distance based only on the variables that have values present in that data vector as the distance calculations for a data item to all nodes will omit the same variable of data the results are comparable vesanto 2000 4 1 3 categorical data the som is designed to represent data in which the magnitude of the values has meaning categorical data can be incorporated by mapping it to ordinal data if the categorical levels have a natural order or hierarchical structure this is done by labelling each sample with a number ensuring that the numeric labels are in a logical order and not arbitrarily assigned for example categories labelled 2 and 3 must be more similar than categories 2 and 10 for the resulting self organization to have a meaningful interpretation another method for working with categorical data in soms applications is to use one hot encoding to produce dummy variables replacing the categorical variable with binary variables for each category level 4 2 step 2 preprocessing environmental variables naturally consist of different measurement scales and types of data normalising the columns of the input data matrix before map training ensures that variables with greater magnitudes or variances do not overshadow variables that may be less diverse of smaller magnitude or measured in different units this roughly equalises the contribution of each variable to the results kohonen 2001 overlooking the preprocessing step risks causing the main axis of the map to be principally aligned with the variable of largest magnitude thereby producing a map that is mainly representative of this variable as discussed in clark et al 2016 common preprocessing methods are transformations of the input matrix that equate either the variances or minima and maxima of each dimension 1 normalising the columns by scaling the variances to 1 around a mean of 0 or 2 a linear transformation equalising the minima and maxima of each variable another method though far less common is to scale the variance of all variables separately to reflect their perceived relative importance kaski and kohonen 1996 highly skewed variable distributions may benefit from logarithmic transformations agarwal and skupin 2008 4 3 step 3 parameter selection a number of parameters require specification prior to the creation of a som parameter choices include map size the number of nodes map shape the configuration of nodes the initial and final radii of the smoothing kernel and the smoothing kernel shape each application of the som method requires the choice and evaluation of appropriate parameters som parameters cannot be optimised by maximum likelihood estimation as the som training algorithm does not attempt to optimise any particular objective function erwin et al 1992 instead parameter selection requires ample time and experience making this a great disadvantage of som implementation for the non expert gopakumar et al 2007 the final results of the som are influenced by the choice of parameters and arbitrary parameter selection has the potential to lead to maps that fail to reveal portions of the data structure and intervariable relationships flexer 1999 vesanto 2000 kohonen 2001 2013 cereghino and park 2009 liu and weisberg 2011 wang et al 2013 astudillo and oommen 2014 deliberate parameter choices must therefore be based on an understanding of available options and the benefits of each choice common techniques for selection of these parameters range from the application of heuristics the minimization of various error measures or trial and error to produce the most agreeable visualization for the user these decisions are discussed in this section default software parameter values are described as well as more tailored options and the benefits are outlined for choosing values beyond the software defaults objective function before beginning to investigate how parameters are chosen first it is important to realise why parameter selection for the som is particularly challenging in the parameter selection phase of model building objective functions are commonly optimised to aid the choice as they are able to provide a quantifiable assessment of the optimality of a set of parameters however it has been proven that the som training process has no objective function that is optimized exactly erwin et al 1992 pampalk 2001 yin 2008a the som training method cannot be quantified in a single mathematical expression instead it follows the gradient descent of a separate set of energy functions for each node erwin et al 1992 for this reason parameters must be chosen using other methods such as quality measures applied to maps created with different parameter sets as will be described below for more information on the lack of a soms objective function see yin 2008b 4 3 1 map structure 4 3 1 1 grid configuration the grid of map nodes is generally connected in either a hexagonal or rectangular lattice as shown in fig 4 with nodes located at each vertex this configuration determines the number of nearest equally close in map space neighbours for each node the nodes of a rectangular lattice have up to four nearest neighbours in a hexagonal lattice each map node has up to six nearest neighbours this difference influences the results of the som training process in which the locations of the nearest neighbours are updated by the same amount at each iteration a larger number of nearest neighbours leads to greater topological preservation of the input data structure and a more uniform final map and for this reason hexagonal lattices are often considered more effective kalteh et al 2008 rectangular lattices are popular however due to the easy presentation of the final output map on a simple rectangular shape as updating of the map during the training process involves the nearest neighbours surrounding each node nodes along the map edges have different mapping properties compared to those in the center this is known as border effects since there are no neighbouring nodes beyond the map edge these outer nodes have fewer surrounding neighbours and tend to be pulled towards the center the overall topology of the som grid is sometimes set as toroidal with the top and bottom rows and or the left and right columns joined together to avoid the border effects encountered with planar grids by creating finite but boundless spaces alternatively very large emergent soms can be used ultsch and moerchen 2005 4 3 2 map size number of nodes the number of map vectors used to represent the input data and therefore the size of the output som is an important choice to be made by the user the size of the output map will affect the final visualisation of the som including the level of information extracted as each node of the output map represents a characteristic pattern from the input data vesanto 2000 important differences between data items may be missed if the map size is too small and yet distinctions between map vectors may be insignificant if the map size is too large cereghino and park 2009 this is analogous to common model issues of over smoothing and over fitting the number of nodes also influences the applicability of the som for either clustering or visualisation with a smaller number of nodes producing larger clusters and a larger map size leading to a more spread out visualisation of the topological structure of the data flexer 1999 liu et al 2006 evaluated the sensitivity of soms to parameter selection and determined that larger maps lead to more accurate results by virtue of less pattern smoothing the extracted patterns are more similar to the actual patterns in the data the resolution of the map determines which clusters become visible kohonen 2013 a larger map size may produce a finer distinction between structures in the data and more accurate local estimation kohonen 2013 wang et al 2013 yet larger maps may contain nodes with no data matching them indicating that the patterns represented by these node vectors do not actually exist in the data set clark et al 2017 ultsch moerchen 2005 state that large maps few thousand nodes must be used if nontrivial structures in the data are to be extracted and projected onto the low dimensional map they suggest that the use of large maps eg esoms is akin to increasing the resolution of the projection from data space to map space website 5 smaller maps compress the data into a smaller possibly more manageable number of patterns for analysis therefore a trade off exists between the accuracy of representation of the data vectors and generalization of the extracted information when deciding on the number of map nodes to use in fig 5 maps of differing sizes are shown over the same data set with data items matching the same node coloured the same colour this shows the range of data represented by a single node of the smaller maps compared to the larger ones clark et al 2016 further discuss the implications of large ranges of data represented by individual nodes kohonen 2013 states that though the choice of map size is the most common question asked with regards to soms it is not possible to determine it beforehand the choice is often made with the use of quality measures or selecting the map that the user finds most interpretable these methods require estimating parameters through training the map multiple times and comparing the results astudillo and oommen 2014 cereghino and park 2009 som software generally specifies a default value for the number of map nodes based only on the number of input samples available see heuristics below however this method does not consider the possible cluster structure in the specific data set nor any user requirements for visualisation or analysis in many cases the number of nodes is set by estimating the cluster structure of the data set and equating the number of nodes to the number of expected clusters 4 3 2 2 quality measures map size is often determined based on quality assessments on a series of output maps quantifying the accuracy of the maps in describing the input data cereghino and park 2009 this method entails minimising some combination of error measures over the set of maps based on the primary objective or set of objectives of the user the necessity for more than one quality measure arises from the two competing goals of the som algorithm the approximation of the input data by the map vectors and the preservation of the input topography by the interconnected grid of map vectors dimension reduction and visualisation a number of quality measures can be used the most common are the quantization error qe kohonen 1995 a measure of the ability of the som to represent the input data qe quantifies map resolution measuring how closely the map vectors match the data vectors and topographic error te kiviluoto 1996 a measure of the preservation of the topology of the input data structure on the output map a combination of quantization error and topographic error is often used due to the trade off between vector quantisation and topology preservation as the qe decreases te will generally increase though not always so the user must determine the desired balance between them care must be taken with the selection as fyfe 2008 states sometimes the two conflicting criteria produce a visualisation which does not accurately reflect all the features of the data fig 6 shows the use of plots of qe and te vs number of map nodes assuming side ratios as described below under the map shape heading to aid the choice of map size requiring a compromise to be made between the competing processes quantization error the som algorithm chooses the bmu for each input by minimizing squared euclidean distances between the input items and map nodes the qe is the difference between each data point xi and its closest map unit mc averaged over all data points q e 1 n i m c x i 1 n i m c 2 x i 2 2 m c x i the optimal map for representing a data set in terms of vector quantization yields the smallest quantization error qe is useful for comparing the som to other clustering or vector quantization methods though it cannot be used to compare maps of different sizes unequal numbers of map nodes as qe will decrease as map size increases nor for comparing maps with different neighbourhood shapes since it favours maps with specific neighbourhood radii kaski and lagus 1996 topographic error the proportion of nodes for which the first and second best matching map units are not nearest neighbours on the map grid is summed over all inputs for each data point the bmu and second bmu are checked to see if they are adjacent t e 1 n i 1 n u x i where u x i 1 if the first and second bmus of x i are neighbours 0 otherwise one topographic error value represents the entire map a value of zero indicates perfect topology preservation note that the topographic error does not consider diagonal neighbours of the rectangular lattice and so a hexagonal lattice gives a lower te due to having more neighbours for each unit pena et al 2008 the distortion measure dm kohonen 1995 is often encountered in the literature in discussions on parameter selection though is seldom actually used as an error measure it is included here for information purposes distortion measure a measure of the pull that the data items are exerting on the map the dm can be thought of as either the amount that each map unit is pulled towards its influencing data points summed over all the map units or the amount each data point pulls on each of the map nodes combined distortion measure incorporates the neighbourhood function into the calculation of distances between each map unit and each of the data points dm i 1 n j 1 m h ij m j x i 2 where hij is the value of the neighbourhood kernel centred on the bmu of xi at the location of mj the distortion measure differs from the quantization error in that each squared distance is weighted by the value of the neighbourhood function qe and dm are equivalent when the neighbourhood size includes only a single node distortion is larger for larger neighbourhood sizes as the neighbourhood function and the distances from each data item to its bmu decrease with each training iteration the distortion measure decreases as training progresses eventually the plot of distortion flattens out the map is still distorted but no longer updating dm is useful for comparing maps of equal size but not for comparing between differing map sizes 4 3 2 3 heuristics heuristics or rules of thumb are commonly used to chose map size as they provide easy and quick results the most commonly used heuristic for determining the number of map nodes m recommends that it should be approximately 5 n where n is the number of samples in the input data set vesanto 2000 though this method relates map size only to the amount of input data and not to the actual data values or structure it is the default method used in the matlab som toolbox code other heuristic recommendations include the number of neurons nodes should usually be as big as possible website 4 one should try for about 50 hits per node on average kohonen 2013 or if an unlimited number of inputs is available one may try to use as big an array as one is able to compute kohonen 2013 it is noted that each of these methods contrast sometimes greatly with each other however their use remains popular in the literature 4 3 2 4 cluster structure map size can also be specified based on the number of clusters that are determined to exist in the input data attempting to provide one node to represent each estimated cluster an alternative would be to create a series of different sized maps choosing the one that produces the lowest cluster validation measure the existence and number of clusters can be determined through cluster theory in practice the most popular currently used method for map size selection in environmental application papers is the production of a series of maps of different sizes followed by a graphical or visual comparison of the output maps there is no overall preferred method of evaluation shared by all soms users though the choice appears to be most commonly based on the degree of generalisation and number of clusters desired in the output next in popularity is the use of a combination of quality measures then default software heuristics followed by a variety of individual ad hoc methods many papers do not give any information about the rationale of map size selection inclusion of this information would allow the reader to understand if there is a reasonable basis to believe the number of nodes accurately represents the cluster structure of the data set or if other map sizes may reveal a different cluster structure 4 3 2 5 map shape ratio of grid side lengths the best representation of the data will be obtained when the shape of the grid roughly corresponds to the shape of the data structure for example a two dimensional square shaped data set would not be best represented by a rectangular grid with one direction much longer than the other as illustrated in fig 7 establishing the ratio of map side lengths based on the main intervariable relationships in the input data is the method recommended by kohonen 2001 and is the most commonly used procedure this is done by initialising the axes in alignment with the most important linear correlations given by the first and second principal components through training the axes will be bent and stretched eventually coming to follow the most important nonlinear correlations in the data the primary axis will represent the most significant relationship between data dimensions the nonlinear line of best fit and the secondary axis the next most important relationship this method of assigning the side length ratio is performed with the following steps vesanto 2000 1 determine the eigenvectors and eigenvalues in the data from the autocorrelation matrix 2 set the ratio between the two sides of the grid equivalent to the ratio between the two largest eigenvalues and 3 scale the side lengths so that their product l 1 xl 2 is as close as possible to the number of map units determined above 4 3 3 training parameters 4 3 3 6 neighbourhood function the neighbourhood function h h ij is a smoothing kernel applied to the map grid during training the kernel controls the smoothness and generalisation of the mapping by defining its rigidity a matrix item hij is the value at mj of the neighbourhood kernel centred on the bmu of xi for example the value of h 2 5 in table 2 gives the influence on node 5 of data for which the bmu is node 2 each column of h represents the influence on map node mj of the data items matching all map units mi each in a separate row the influence that the data in the voronoi set of mi exerts on other nodes is given in the rows each row consists of values for a surface with a peak at mi mj this indicates that data with bmus closest to node mj will have the most influence on the updating of mj the neighbourhood kernel is generally normalised so that each column sums to 1 equalising the sum of the influence exerted by all data items on each map node at each training iteration the location of each map node mj is updated based on all the data items that are matched to nodes within the specified neighbourhood radius centred at this node the neighbourhood size and therefore the extent of influence of the data items on the map units decreases linearly over the training iterations though the shape of the neighbourhood remains constant throughout training this decrease in kernel size leads to an increased smoothing of the map the size and shape of the neighbourhood kernel must be determined by the user neighbourhood size a large neighbourhood kernel results in a stiff map by overstressing topological ordering and a small kernel results in freer movement of the nodes toward the data vesanto et al 2003 the increased topologocial ordering of maps created with larger neighbourhoods comes at the expense of data quantisation how close the nodes are to the data they represent which improves as neighbourhood size decreases for this reason a compromise is made a large neighbourhood kernel is used at the beginning of training to induce a global ordering of the map nodes and the kernel diminishes in size with each training iteration the node locations are eventually finetuned within a small neighbourhood at the end of training the starting and finishing neighbourhood sizes can be specified by the user the neighbourhood radius is measured in map space not data space fig 8 shows the group of nodes contained within a neighbourhood of radius 0 1 and 2 around a node of a hexagonal and rectangular grid on a hexagonal grid a neighbourhood kernel of radius 2 will incorporate data from 19 nodes whereas on a rectangular grid the same size kernel would incorporate data from 13 nodes the choice of initial neighbourhood size has an impact on the results if it is too small the map may not achieve an appropriate global overall ordering kohonen 1990 hastie et al 2007 kohonen 2001 recommends setting a starting neighbourhood approximately half the largest side length of the map to prevent the risk of ending in a local minimum the final neighbourhood radius usually includes only a single node kohonen 1993 the map loses its spatial interaction at this point and the som becomes equivalent to k means clustering hastie et al 2007 if global ordering has been successful this will still produce the desired results as the grid connections are maintained kohonen 2001 explains that equating the som to the k means algorithm at the end of training through diminishing the neighbourhood to include only a single node guarantees the most accurate approximation of the probability density function of the input and should also eliminate any issues the neighbourhood function may encounter at the borders of the map neighbourhood shape four shapes are commonly used for the nieghbourhood function uniform gaussian cut truncated gaussian and epanechnikov parabolic these shapes are used in the matlab som toolbox with gaussian as the default vesanto et al 2000b table 3 describes the four shapes where σ t is the neighbourhood radius at iteration t dci rc ri is the distance between map units mc and mi on the map grid and 1 x is a step function taking a value of 0 if x 0 or 1 if x 0 updating only the nodes for which the function is nonzero the gaussian kernel is the only one that incorporates the entire input data set to update each of the map nodes and conversely uses each data item in the updating of all of the nodes the other three kernels only update within the specified radius the gaussian neighbourhood therefore produces the smoothest som patterns liu et al 2006 whilst the others have various degrees of smoothing epanechnikov the least for a fixed σ t common to all kernels table 4 gives sample values of neighbourhood kernels for each shape centred around the middle nodes of a 3 3 and 7 7 som higher values indicate which nodes would be updated influenced by the data by a greater amount erwin et al 1992 found that the som s convergence rate is heavily dependent on the shape of the neighbourhood function and the training algorithm is more effective when a convex neighbourhood function is used rather than a concave one ota et al 2011 describe the use of an asymmetric neighbourhood function to remove topological defects which frequently emerge during training and inhibit the global ordering of the map 4 3 3 7 training length the final statistical accuracy of map how well the data is represented depends on the number of iterations since learning is a stochastic process kohonen 1990 however with modern computational resources this should no longer be an issue and software defaults should be adequate there is no upper limit to the number of iterations that can be used in principal the global ordering stage with a large neighbourhood radius can be relatively short compared with the finetuning stage with the smaller neighbourhood radius 4 3 3 8 mask a mask may be applied during the map training process to weight the influence of each variable in the distance calculations for determining bmus the mask is a vector with the same number of dimensions as the input data mask values indicate the relative importance of each variable usually with 0 s and 1 s it can be used to hide certain variables or make others more influential in map training if the user would like to accentuate the significance of certain variables over others note that the mask is only used for finding bmus and is not used in the initialisation stage 4 3 3 9 associated variables introducing a new set of variables onto a trained som that has been created with a different set of variables may enable researchers to discover interesting intervariable relationships cereghino and park 2009 these new variables are known as associated variables as they are linked to the map after training the trained map can be clustered or labelled based on values of the associated variable to visualise the relationships 4 4 step 4 initialise the map the map is generally initialized with a regular linear array set in the directions of highest variance of the input data vectors as determined with principal component analysis the initial values of the map weight vectors are set at uniform intervals along the first and second principal components of the input data set which come to form the axes if the axes lengths are proportional to the two largest eigenvectors of the data this should produce an approximately uniformly spaced lattice this linear form of initialisation is usually used as it ensures the map is already aligned with the most significant linear intervariable relationships before map training begins fig 9 illustrates the alignment of the axes with the principal components as som training is an iterative process of multi dimensional nonlinear optimisation it has the potential to lead to multiple optimal solutions kingston et al 2005 meaning that for the same input data the possible output maps include rotations or inversions of each other the orientation of the final map is dependent on the initial values assigned to the nodes with different sets of initial node locations leading to rotations mirror images or symmetric inversions of the final map kohonen 1990 4 5 step 5 run the training algorithm running the training algorithm through the iterations of the two step matching and updating process re organises the linearly initialised grid of map nodes into a nonlinear arrangement amongst the data items while maintaining the grid connections hastie et al 2009 describe the initialised map nodes as buttons sewn onto the principal component plane in a regular pattern the training process of the som then bends and twists the plane so the buttons best approximate the data the map is smoothed by the updating process in which the new node locations are computed based on the previous locations and the locations of the data items through the maintained grid connections the map nodes organise themselves based on their similarity to each other the trained map will now better approximate the data set than the initialized map with more map units positioned in areas of higher density input space the vectors describing the location of each map node of the same dimensions as the input data vectors have come to represent the most prevalent patterns of unique variable combinations in the data at this stage the algorithm can be run for a number of parameter sets map size and shape neighbourhood kernel size and shape and the results compared through the use of quality measures to choose the map that best preserves the topology quantisation or clustering of the input data or any combination of user objectives 4 6 step 6 place data items on the map after the map has been created each data item finds a place on the map by matching it to its closest most similar map node because the nodes are organized based on their similarity similar input data will become mapped to the same or nearby map nodes the matching is based on a high dimensional similarity measure usually euclidean distance as in the matching stage of map training each data item will have a unique best matching map node but each map node may be matched to more than one data item or none at all placement of the data items on the map leads to the identification of clusters and discovery of relationships between data items 5 interpretation of output map the aim in analysing a som is to identify the key characteristics comprising the predominant patterns in the data set pattern extraction and discover which data items are similar to each other with respect to these characteristics clustering characteristics of the predominant patterns are revealed by the high dimensional vectors associated with each map node as the map becomes organised in data space during training the location of each map node is defined by the combination of dimension values that make up its vector as the data items are matched to their nearest map nodes after training clusters are formed of data items sharing similar characteristics these characteristics are identified by the pattern of the common node map nodes may also be grouped together to form larger clusters of data in their voronoi sets interpretation of a som generally includes a compilation of information through the visual investigation of the labelled map in one or two dimensional output space and the component plane for each variable the visual investigation will reveal the prevalent patterns indicated by the individual node vectors and the clusters of the data nearest to each node investigation of the output map should also disclose a good approximation of the input data distribution including the overall shape and cluster structure in the data characteristics of the clusters and the relationships between variables the results will allow for trend visualisation and infilling of missing data 5 1 visualisation the som is usually interpreted by means of the overall output map as well as individual component planes the output map restricted to show only individual variables 5 2 output map the output map is usually displayed as a regularly spaced grid in one or two dimensional map space labelled with the data items or the main characteristics of the data items that pertain to each node the output map may also be displayed in data space if the dimension of data space is low in map space the distribution of the projected data items across the map is evident whereas in data space the distribution of the map nodes amongst the data items is evident the differences between nodes are often shown by coloured markers or a surface plot an output map of the example data from fig 2 is shown in fig 10 in both data space and map space in data space a the interconnected map grid is shown in black over the coloured data points revealing the placement of the nodes amongst the data in map space the nodes are coloured by similarity with empty nodes remaining white b and data item labels are placed onto the relevant areas of the map c labelling the nodes on the output map as in fig 10c gives an indication of the characteristics of data items represented by each region of the map labelling can involve listing each data item over the node it is assigned to or choosing a representative label for each node based on the group of data it represents in the latter case nodes may be grouped into larger clusters before labelling based on predominant cluster attributes to label the nodes or clusters with representative labels in consideration of the cluster centroid map node variable weights plots of cluster vs dimension and dimension vs cluster are useful to indicate which dimensions are most accounted for in each cluster fig 11 demonstrates how these plots can be used with a 24 dimensional 8 cluster example in the plot on the left a each line represents a separate cluster with the y axis indicating the values weights of each dimension at each cluster centroid it is possible to pick out for example that dimension 19 has low weights in all clusters whereas dimension 4 is prominent in 3 separate clusters on the right b each line represents a dimension with the y axis indicating the dimensions value at the cluster centroid for the 8 clusters of this example it is possible to pick out for example which dimension has by far the largest weight in cluster 6 this method can be used to choose the top weighted dimensions for labelling each cluster another option is to label a map with associated data new data that has not been used in the training as discussed in the section on associated variables this shows where the new data would plot on a map trained with other data defining the relationships between the data sets the axes of the output map establish a meaningful nonlinear coordinate system for the various features of the input data during the self organisation process kohonen 2001 the axes begin the training stage as the first and second linear principal components of the data set and then gain nonlinearity as iterations progress while vesanto 1999 states that the axes of the map grid rarely have any clear interpretation it is possible to form a general perception of their meaning through investigation of the component values of the map vectors along the edges of the map this can be done by careful analysis of the component planes 5 2 1 component planes relationships between the individual variables can be explored with the use of component planes colouring is used to indicate dimension weights values at each node with a separate component plane displaying values of each dimension of the som the axes and grid nodes correspond exactly to those of the som output map inspection of component planes indicates the spread of values in each dimension vesanto 1999 the presence of interesting relationships between variables can be visually determined from the component planes allowing these relationships to be further investigated with scatterplots of the subset of variables of interest plotting component planes of associated variables those not used in map training shows the relationship of new variables to those used to create the map in fig 12 component planes produced from the som in fig 10 indicate that data items located on the lower right of the map have high maximum discharges and medium minimum discharges whereas those located in the lower left have more moderate maximums and relatively high minimums the component planes provide a meaningful interpretation of the axes the axes of the som follow the main nonlinear directions of variance in the data set and by identifying regions of the component planes with high and low values of each variable the general gradient of individual variables along the axes should become evident these gradients will be continuous along the axes though not necessarily monotonic in direction 5 3 pattern extraction the prevalent patterns in the data set are exhibited by the vectors of each map node the unique combination of variables making up each vector is a characteristic pattern these combinations can be analysed via the component planes an analysis of frequencies of occurrence of each pattern is obtained by looking at the percentage of input data assigned to each map node each matching of a data item to a node is known as a hit transitions between patterns can be observed by matching the data items to the map in a sequential order and following the trajectories of the hit locations 5 4 cluster identification clustering is the most frequent reason for implementing a som and a number of methods exist for finding clusters in the data basic or first level clustering involves treating each som node as a cluster centroid as each input data item is uniquely related to one of the nodes clusters are created of data that share similarities based on the features of the extracted patterns each node of the output map comes to represent a cluster to use this method effectively it is best to determine the approximate number of clusters that exist in the input data using a cluster validity measure5 3 4 before setting the number of nodes the number of nodes should be set equal to or greater than this to ensure that each cluster is mapped to a separate node 5 4 1 visualising the space between clusters 5 4 1 10 unified distance matrix u matrix the u matrix ultsch 2003 visualises distances between regions of the data space represented by each node by computing high dimensional similarities between neighbouring nodes how close they are in data space the u matrix determines cluster boundaries based on large dissimilarities ultsch 2003 this is also known as the degree of distortion that is the change in relative distance between the high dimensional locations of the nodes in data space and the low dimensional map representation agarwal and skupin 2008 the distances are indicated on the u matrix map by colour with differing colours indicating the boundaries between clusters it can be seen on the u matrix in fig 13 a that distinct clusters blue exist separated by lighter coloured boundary areas 5 4 1 11 similarity colouring similarity colouring involves spreading a two dimensional colourmap over the principal component projection of the nodes thereby colouring similar nodes similar colours further apart more different nodes become coloured with colours that are perceived as more distinct kaski et al 2000 the similarity colouring and empty map nodes in fig 13 b reveal the same cluster structure as the u matrix on this plot the clusters evident in the lower portion of the map are outlined 5 4 1 12 hits plotting the number of data items matched to each node known as the hits on the output map may allow cluster structure in the data to become visible high intensities of data might become evident on clearly separated regions of the map nodes with zero or relatively low hits delineate the cluster borders using a surface plot to record the hits will produce raised regions of the map where the clusters exist linearly scaling the size of the output map nodes in proportion to the number of hits each receives provides another method of visually indicating cluster structure fig 13 c shows the use of hits to confirm the cluster structure 5 4 2 second level clustering second level clustering is used to find groups of nodes that themselves make up a cluster this is useful when creating a map with a large number of nodes compared to the number of clusters in the data which may be done to gain a good separation between data items on the map it is known as second level clustering since the data has already been clustered with the som first level clustering second level clustering is useful for producing summaries or descriptions of som results second level clustering groups the som nodes either with another som or with a different technique as discussed below the high dimensional node vectors are clustered in data space and the cluster memberships are projected onto the low dimensional map for visualisation with the subsets of nodes grouped together by colour or outlines all the data matched to any of the member nodes are now members of the second level cluster as might be expected the weights of the most influential variables will change rapidly at the borders of the clusters kaski et al 1998 toth 2009 found second level clustering of a larger som to be more suitable for preserving the distinctive features of the classes when compared with using a smaller initial map size some common methods of second level clustering include visually assessing the distance in data space between nodes with the u matrix similarity colouring or the number of hits in each region another som partitive clustering such as k means and hierarchical agglomerative clustering 5 4 2 13 k means clustering k means clustering is a popular method for second level clustering as the topological preservation of the data has already been captured in the first som k means clustering produces good results if the clusters are compact hyper spherical and well separated determining the optimal k value or number of clusters to extract can be done with the cluster validity measures described below 5 4 2 14 som for second level clustering a som can be used for second level clustering in this method the node vectors of the first som become the input vectors for the second som an example is shown in fig 14 in which a 5 2 som is used to cluster the output of an 18 18 som reducing the number of clusters from 324 to 10 results produced are similar to k means clustering with the added benefits of maintaining an order to the clusters and allowing presentation in the familiar som output 5 4 3 hierarchical clustering hierarchical clustering eg ward 1963 can be used to determine many levels of progressively larger clusters on the map an advantage of hierarchical clustering over k means is that many nested levels of clusters can be shown simultaneously on one output map hierarchical clustering can also be performed based on an associated variable another form of agglomerative hierarchical clustering is neuron label clustering used by skupin et al 2013 in which neighbouring clusters are merged if they share the top ranked label term most influential dimension this can be repeated for the second ranked label terms etc to get many separate cluster layers 5 4 4 cluster validity measures if attempting to have each node of the map represent a cluster of the data it is necessary to estimate the number of clusters present in the data set before the soms analysis begins there are various methods available for this which can also be used to determine k in k means clustering the davies bouldin index davies and bouldin 1979 is a popular measure for determining the number of clusters present in a data set and used in the matlab som toolbox vesanto et al 2000 it measures the ratio of within cluster to between cluster distances a small ratio indicates compact well separated clusters the dunn index dunn 1973 and silhouette coefficient rousseeuw 1987 are also used to determine the number of clusters both are also concerned with the ratio of inter cluster to intra cluster distances silhouette clustering determines how appropriately the data has been clustered by ranking data points as well clustered 1 would be better in neighbouring cluster 1 or on the border of two natural clusters 0 and taking the average over the entire dataset clusters with narrower silhouettes than the rest will appear if there are too many or too few clusters the elbow method finds the kink in the curve of cluster dissimilarity as a function of the number of clusters the value on the x axis where the curve begins to decrease less rapidly may indicate the number of natural clusters in the data hastie et al 2009 the gap statistic tibshirani et al 2001 can be used as an automated way of locating this point on the curve these are just a small selection of existing clustering indices charrad et al 2014 provide an automated means to evaluate 30 different indices for determining the number of clusters in the data this returns the optimal clustering scheme for the presented data set out of all of the tested options 5 5 trend visualisation infilling missing data prediction incorporating new data an indication of temporal changes trends in data sets can be gained through soms visualisations using any of the following methods a som is trained with all the available input data and changes over time in the data mapping to each node or second level cluster reveal temporal trends in the data structure all the data matching one map node could be used as input for a local prediction model trajectories or lines connecting the bmus of consecutive data points in a time series may be used to indicate trends consecutive sections of the input data ie years or decades can be plotted on to the map to visualise changes or temporal patterns may be extracted based on trends of the cluster centroids when a separate map is created with data from each time step to use soms for infilling missing data the best matching map node can be found based on the available variables of the data item and then the value of the missing variable adopted from the node vector a similar method of value adoption based on established intervariable relationships is used for prediction the best matching map node can be found based on a set of easily predicted variables and then the value of the unknown variable adopted from the node vector the incorporation of new data onto the trained and clustered map reveals how it relates to the other data items based on the established relationships and clusters new data can be added to the input data set during step 6 placing the data on the map it will be placed into the established clusters and can be compared to the other cluster members 6 conclusion soms have wide applicability and many characteristics that make them particularly well suited to water resources data if they are employed consciously the results will reveal more information about the relationships of the underlying systems water resources engineers and scientists interested in using soms to knowledgeably explore high dimensional nonlinear data sets now have an accessible resource to do so this paper leads researchers through the creation and interpretation of a som relevant to a specific data set providing a guide to understanding meaningful parameter choices and interpreting som results for data driven exploratory analysis of their particular data improving each individual application of the som will lead to greater insight into the inter component relationships involved in water resource systems websites 1 scopus refined by earth and planetary sciences and environmental sciences accessed june 2017 2 google scholar accessed june 2017 3 environment canada s hydat database national water data archive https ec gc ca rhc wsc default asp lang en n 9018b5ec 1 accessed july 2017 4 http databionic esom sourceforge net user html 5 http www cis hut fi somtoolbox accessed july 2017 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103676 appendix supplementary materials image application 1 
439,the study of fluid flow in non darcy porous medium subjected to hall current is very important in scientific and engineering applications such as water filters groundwater discharge in aquifers petroleum engineering mhd generators and chemical engineering forchheimer model is needed at high flow rate where flow will exhibit non linearity with respect to velocity which make darcy law inapplicable at these conditions in this case the momentum equations become non linear the classical differential transformation method transforms the non linear differential equations to a non linear algebraic system which gives more than one solution and may be unstable that leads to divergence of the required solution the novelty of present method that it is a power series solution avoiding solution multiplicity and divergence by a linearization technique that is applied on non linear governing equations to obtain the unique and convergent solution the uniqueness convergence and stability of the new technique are tested by comparisons with previously available works and it is also verified by a fourth order accurate finite difference fofdm solution the effects flow parameters on the velocity and friction factor are illustrated the values of parameters in the present work are chosen according to the available previous results the power of the persent method to compute over a lagre range of parameters and the distinctiveness between curves in figures keywords new approach lmdtm mhd flow fofdm porous plates hall current forchheimer medium friction factor 1 introduction the flow of fluids through porous medium under influence of magnetic field and suction injection at plates has many engineering applications such as mhd generators water filters petroleum engineering and chemical engineering verma and gupta 2018 non darcy forchheimer flow has many engineering applications where it describes fluid flow in consolidated or unconsolidated porous media when abrupt changes in velocity dominates sharma et al 2017 a criterion or a generalized equation is required to understand this ow behavior in the isotropic anisotropic carbonate and sandstone reservoirs the relationship between the pressure drop and flow rate in problems of fluid flow through porous media is known to be affected by the nature of flow through the porous media it has been observed by darcy that the pressure drop remains proportional to the rate of flow at low reynolds number sharma et al 2017 but at high flow rate flow will exhibit non linearity with respect to velocity which make darcy law inapplicable at these conditions in the non darcy flow the inertial forces and kinetic energy changes signifcantly due to the expansion and contraction of gas in the porous medium to account this non linearity between pressure gradient and velocity a non darcy term was frst introduced by forchheimer el dabe et al 2017 have been investigated the non darcian mhd flow of a casson nanofluid through parallel plates channel porous plates with uniform suction and injection are considered they used the finite difference method fdm of second order accuracy to solve the nonlinear system of differential equations they obseved that the velocity reached the steady state faster than temperature and nanparticles concentration attia et al 2015 studied the effects of the non drcian forchheimer and hall current resistances on the unsteady flow and heat transfer between two porous plates they solved the governing partial differential equations numerically by the finite difference method fdm joule and viscous dissipations are considered in the energy equation ewis 2019 used a second order accurate finite difference method to solve the governing equations of natural convection of non newtonian rivlin ericksen fluid flow and heat transfer under the influences of non darcy resistance force constant pressure gradient dissipation and radiation the novelty his is to solve this problem between parallel plates channel instead of one plate the fluid flows between two heated parallel plates that are kept at constant temperatures finite difference schemes transform the coupled non linear differential momentum and energy equations to linearized system of algebraic equations some comparisons are made to study the convergence and stability of the solution sulochana et al 2018 studied the effects of nanofluids and magnetic field on flow temperature and concentration in a porous medium past an inclined plate in the presence of radiation they used the perturbation method to solve the differential equations govern the problem thier results show that thermal radiation and chemical reaction restrictions have a trend to enhance thermal and mass transport rates respectively and also water based tio 2 nanofluid possess higher velocity compared with water based cuo nanofluids applications of analytical methods on non linear differential equations are preferred if it is possible to introduce exact approximate solutions of mathematical and engineering problems the differential transform method dtm obtains an analytical solution in the form of a polynomial it is different from the traditional high order taylor s series method which requires symbolic representation of the necessary derivatives of the data functions it has been used to solve effectively easily and accurately a large class of linear and nonlinear differential equations with approximations ewis 2013 inestigated the effect of darcy model on velocity and temperature in the presence of joule and viscous dissipations he used the tradional dtm to solve the differential equations that govern the mhd couette flow and heat transfer in porous medium between porous parallel plates the hall term affects the main velocity component u in the x direction and gives rise to another velocity component w in the z direction the convergence and stability of the dtm is tested using a high accurate fdm his study shows the power of dtm to obtain power series solution of coupled linear differential equations the multi step differential transformation method mdtm has been used for solving linear nonlinear differential equations instead of the traditional differential transformation method dtm to avoid divergence in the resulted polynomial keimanesh et al 2011 used the mdtm to study a third grade non newtonian fluid flow between two parallel plates they studied the reliability and performance of the mdtm in comparison with the numerical method fourth order runge kutta and other analytical methods such as hpm ham and dtm in solving this problem rashidi et al 2011 applied the mdtm on flow of a second grade fluid over a stretching or shrinking sheet their dtm solutions are only valid for small values of the independent variable gkdoğan et al 2012 proposed a fast and effective adaptive mdtm to solve nonlinear differential equations at the same time a comparison between the msdtm and the adaptive mdtm reveals that the proposed approach is an efficiency tool for solving the considered equations using fewer time steps gusynin et al 2017 studied estimating of accuracy and justification the application effectiveness of the multi step differential transform method for solving non linear boundary value problems they presented results of numerical solution of a non linear boundary value problem and shown the efficiency of application of the multi step differential transform method compared with traditional differential transform method the unsteady magnetohydrodynamic flow of an electrically conducting viscous incompressible non newtonian casson fluid bounded by two parallel non conducting porous plates has been studied with heat transfer considering the hall effect sayed ahmed et al 2011 numerical solutions are obtained for the governing momentum and energy equations taking the joule and viscous dissipations into consideration the effect of unsteady pressure gradient the hall term the parameter describing the non newtonian behavior on both the velocities and temperature distributions have been studied the transient mhd couette flow and heat transfer of dusty fluid between two parallel plates and the effect of the temperature dependent properties has been investigated mosayebidorcheh et al 2015 a hybrid treatment based on finite difference method fdm and differential transform method dtm is used to solve the coupled flow and heat transfer equations reddy et al 2017 used the adomian decomposition method for hall andion slip effects on mixed convection flow of a chemically reacting newtonian fluid between parallel plates with heat generation absorption in addition the numerical data for skin friction heat and mass transfer rates have been shown in tabular form the novelty of present method is to avoid solution multiplicity and divergence by a linearization technique that is applied on non linear governing equations to obtain the unique and convergent solution in the present paper a new approach to multi step differential transformation method called lmdtm is introduced and applied on an engineering application the dtm and msdtm transform the non linear differential equations to non linear algebraic equations they introduce recurrence relations between coefficients using boundary conditions which lead to other non linear algebraic equations the solution of last equations gives more than real and complex roots the new approach lmdtm transforms the resulted system of non linear algebraic equations to a linearized one the linearized equations are enhanced by iterations up to achieving the required accuracy the convergence and stability of the new technique are tested by comparison with previously available works and an accurate numerical solution called fourth order finite difference method fofdm the effects flow on the field unknowns are studied it is observed that the new approach is simple with high accuracies and it gives the required unique solution the new approach represents a benchmark for various numerical and analytical solutions which have more than one solution the new approach lmdtm is applied on mhd flow in non darcy medium between porous parallel plates considering hall current the linearization technique is applied on the non linear forchiemer term the present results show the high efficiency and powerful of the present approach 2 governing equations the fluid is assumed to be steady laminar incompressible and flows between two infinite horizontal plates the plates are located at y h planes and extend from x to as shown in fig 1 the two plates are stationary and permeable with suction injection at one plate and injection suction at the other with velocity v 0 the porous medium forchiemer model is considered and leads to non linear term in momentum equations a constant pressure gradient and uniform magnetic field b 0 is applied in the positive y direction and is assumed disturbed as the induced magnetic field is considered by assuming a large magnetic reynolds number thus the hall effect is taken into consideration and consequently a z component for the velocity is expected to arise the momentum equation in vector form is given by el dabe et al 2017 and reddy et al 2017 1 ρ v v p τ j b 0 f b where ρ is the density of the fluid v is the velocity vector is the gradient derivative p is the preesure τ is the shear stress tensor b 0 is the vector magnetic field f b is the body force the current density j is given by 2 j σ v b 0 β j b 0 where σ is the eletric conductivity and β is the hall factor the induced magnetic field results a normal velocity component in z direction thus the fluid velocity vector is given by 3 v u ı v 0 j w k where u is the axial velocity w is velocity component results by hall current and v 0 is the transverse velocity at plates an order of magniude analysis is applied on the momentum equations considering the ratios of dimensions of the plates spacing between them mala et al 1997 and breugem et al 2005 according to this concept the orders of magniudes of flow quantities may be written as x z u w and p x are o l where l is a refernce length such that l h also y v p y and p z are o h under the assumptions mentioned above the governing equations are written as el dabe et al 2017 and reddy et al 2017 4 ρ v 0 u d p d x μ u σ b 0 2 1 m h 2 u m h w μ k p u ρ b k p u 2 5 ρ v 0 w μ w σ b 0 2 1 m h 2 w m h u μ k p w ρ b k p w 2 where where m h σ β b 0 is the hall parameter and μ is the fluid viscosity the boundary conditions of momentum equations are shown in fig 1 and they are given by 6 u h w h w h u h 0 it is expedient to write the governing equations in the non dimensional form to do this the following non dimensional quantities are introduced y y h 2 h u u u 0 w w u 0 p p ρ u 0 2 f s h b k p forchheimer parameter h a 2 σ b 0 2 h 2 μ h a hartmann number m p h μ ρ u 0 k p porosity parameter r e ρ u 0 h μ reynolds number s ρ v 0 h μ suction parameter thus eqs 4 6 are rewritten as hats are dropped for simplicity 7 s 2 u α u 4 h a 2 1 m h 2 u m h w m p u f s u 2 8 s 2 u w 4 h a 2 1 m h 2 w m h u m p w f s w 2 9 u 0 w 0 w 1 u 1 0 3 the linearized multi step differential transformation method lmdtm the finite domain of solution 0 y 1 is divided into m subintervals such that the mesh size is δ 1 m with counter m 1 2 3 m 1 the kth order differential transformation f k m of a the function fm y in the mth sub interval y m y m 1 is defined as ewis 2013 10 f k m f m y y y m k 1 k 1 where k 1 2 3 and m 1 2 3 m thus the inverse differential transformation dit of the coefficient f k m is defined as 11 f m y i 1 f k m y y m k 1 according these eqs 10 11 the required transformations are obtained by the following theorems taking y 0 0 theorem 1 if f m y α is a constant then its differential transformation is given by f k m α δ k 1 where k 1 2 theorem 2 if f m y u y then its differential transformation is given by f k m u k m where k 1 2 theorem 3 if f m y u y then its differential transformation is given by f k m k u k 1 m where k 1 2 theorem 4 if f m y u y then its differential transformation is given by f k m k k 1 u k 2 m where k 1 2 theorem 5 if f m y u 2 y then its differential transformation is given by f k m r 0 k 1 u r 1 m u k r m where k 1 2 by using the basic properties of differential transform method fromabove theorems and taking the transform of eqs 7 9 one can obtain that 12 s 2 k u k 1 m α δ k 1 k k 1 4 u k 2 m h a 2 1 m h 2 u k m m h w k m m p u k m f s r 0 k 1 u r 1 m u k r m 13 s 2 k w k 1 m k k 1 4 w k 2 m h a 2 1 m h 2 w k m m h u k m m p w k m f s r 0 k 1 w r 1 m w k r m 14 u 1 1 w 1 1 k 1 u k m k 1 w k m 0 where δ k 1 1 k 1 0 o w is the kronecker delta u k 2 m and w k 2 m is are differential transformations of velocities u and w respectively in the mth sub interval eqs 12 14 represent non linear system algebraic equations because of multiplications in summations the basic idea is to transform these terms to linearized ones to avoid more than one solution that arise when the non linear algebraic equations are solved numerically and they are subjected to divergence thus system 12 14 is rewritten in the linearized form where the bar symbol represent the linearized terms 15 s 2 k u k 1 m α δ k 1 k k 1 4 u k 2 m h a 2 1 m h 2 u k m m h w k m m p u k m f s r 0 k 1 u r 1 m u k r m 16 s 2 k w k 1 m k k 1 4 w k 2 m h a 2 1 m h 2 w k m m h u k m m p w k m f s r 0 k 1 w r 1 m w k r m where bar notation refes to the linearized term continuity and differentiability of the analytic function f x at nodes m 1 2 m are written as 17 f m y m 1 f m 1 y m 1 18 f m y m 1 f m 1 y m 1 the mdtm of at nodes are written as 19a k 1 u k m m k 1 u 1 m 1 19b k 1 w k m m k 1 w 1 m 1 20a k 2 k 1 u k m m k 2 u 2 m 1 20b k 2 k 1 w k m m k 2 w 2 m 1 with the new technique lmdtm the given differential equations and related boundary conditions continuity and differentiability are transformed into a linearized system instead of non linear algebraic equations which are resulted from dtm or mdtm finally 2 n 1 m coefficients u k m w k m of power series of degree n are computed by iterative technique these coefficients are used to compute the basic unknowns in the governing equations as 21 u m y i 1 n u k m y y m k 1 22 w m y i 1 n w k m y y m k 1 where y m y y m 1 and m 1 2 m the skin friction factor is an important fluid flow parameter because of its very importance in the engineering applications these quantities are computed after solution the governing equations the skin friction factor is defined as ewis 2018 23 c f 2 τ w ρ u 0 2 the dimensionless forms of skin friction factors for axial velocity and transverse velocity are written respectively as 24 c f u r e c f u u 0 at lower plate u 1 at upper plate 25 c f w r e c f w w 0 at lower plate w 1 at upper plate using the mdtm we can write 26 c f u u 1 1 at lower plate k 2 k 1 u k m m k 2 at upper plate 27 c f w w 1 1 at lower plate k 2 k 1 w k m m k 2 at upper plate 4 the fourth order finite difference method fofdm the finite domain of solution 0 y 1 is divided into n 1 subintervals such that the mesh size is δ 1 n 1 with counter i 1 2 3 n the linearized system of coupled non linear ordinary differential eqs 7 9 is transformed to a system algebraic equations using the fourth order difference schemes the following fourth order schemes are obtained by taylor s expansions of the variable f x represents u or w about y i i 1 δ ewis 2018 28 f y y 1 1 12 δ 25 f 1 48 f 2 36 f 3 16 f 4 3 f 5 29 f y y 2 1 12 δ 3 f 1 10 f 2 18 f 3 6 f 4 f 5 30 f y y i 1 12 δ f i 2 8 f i 1 8 f i 1 f i 2 31 f y y n 1 1 12 δ f n 4 6 f n 3 18 f n 2 10 f n 1 3 f n 32 f y y n 1 12 δ 25 f n 48 f n 1 36 f n 2 16 f n 3 3 f n 4 33 f y y 2 1 12 δ 2 10 f 1 15 f 2 4 f 3 14 f 4 6 f 5 f 6 34 f y y i 1 12 δ 2 f i 2 16 f i 1 30 f i 16 f i 1 f i 2 35 f y y n 1 1 12 δ 2 f n 5 6 f n 4 14 f n 3 4 f n 2 15 f n 1 10 f n another linearized form is used for the non linear terms in the governing equations so one can write 36 s 2 u α u 4 h a 2 1 m h 2 u m h w m p u f s u u 37 s 2 u w 4 h a 2 1 m h 2 w m h u m p w f s w w where bar notation refers to the linearized variable where it must be modified by iterated techniques replacing the continuous terms in eqs 36 and 37 by corresponding discrete formulas 28 35 the resulted linearized system with boundary conditions are solved for the two components of the flow velocity u w 5 error analysis an error analysis is introduced to verify the power of the persent method comparing with the available previous results and the fourth order finite difference method tables 1 2 3 4 5 6 7 illustrate the comparison of present analytical results with previously available exact and numerical results the present analytical results are listed for different degrees of polynomials n different number of multi step intervals m and numerical sub intervals n where the order of trucated series in velocity is 1 n 1 m n 1 in addition to other errors such as the rounoff error the given comparisons show the power of the new method where it is convergent stable and high accurate for a wide range of problem parameters table 1 shows the comparison of the new approach with the exact solution of verma and gupta 2018 the axial velocity profile is computed for s 0 5 f s m h 0 m p 4 h a 1 α 5 4 it is observed that the maximum absolute error a e 1 of axial velocity between lmdtm with second degree polynomials n 2 and the available exact solution verma and gupta 2018 is 7 35 10 3 but a d 2 between lmdtm with fifteenth degree polynomials n 15 and the available exact solution verma and gupta 2018 is 5 10 16 this comparison shows the power of the new method such that it is flexible to obtain the exact solution with increasing the degree of polynomials n and number of multi step intervals m tables 2 and 3 show the velocity profiles and for s f s m h m p h a 1 α 5 n 10 and m 40 it is observed that the maximum absolute difference a d 1 of axial velocity between lmdtm and fofdm is 1 5 10 9 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 1 2 10 4 it is also observed that the maximum absolute difference a d 1 of transverse velocity between lmdtm and fofdm is 2 9 10 10 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 8 9 10 6 tables 4 and 5 show the velocity profiles and for s f s m p h a 1 mh 3 α 5 n 10 and m 40 it is observed that the maximum absolute difference a d 1 of axial velocity between lmdtm and fofdm is 10 14 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 1 2 10 4 it is also observed that the maximum absolute difference a d 1 of transverse velocity between lmdtm and fofdm is 5 10 15 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 5 7 10 6 tables 6 and 7 show the velocity profiles and for s 0 fs 2 and mh 1 it is observed that the maximum absolute difference a d 1 of axial velocity between lmdtm and fofdm is 1 5 10 9 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 8 10 5 it is also observed that the maximum absolute difference a d 1 of transverse velocity between lmdtm and fofdm is 2 9 10 10 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 8 9 10 6 figs 2 3 4 5 show the rates of convergence and stability of axial velocity u and transverse velocity w the degree of polynomial n and number of multi step intervals m takes values of 2 3 and 10 to show generations of convergence and stability of the lmdtm in comparison with the stable and the fourth order accurate fofdm with number of points n 241 so lmdtm is powerful and effective for solving non linear problems by series it is observed that increasing n decreases the absolute relative error in computations with highest rate for axial velocity u and lowest rate for transverse velocity w it is also observed that the degree of polynomial n 10 in lmdtm satisfies convergence and stability in comparison with the stable and fourth order accurate fofdm 6 results and discussion the present results are listed intales and figures to show the trend of profiles and the accuracy of the new approach the values of parameters in the present work are choosen according to the available previous results the power of the persent method to compute over a lagre range of parameters and the distinctiveness among curves in figures tables 8 9 10 11 show the uniqueness convergence and accuracy of skin friction factors u 0 u 1 w 0 and w 1 for various values of forchhiemer parameter fs 0 5 10 20 and 30 and two values of hall parameter mh 0 5 and 1 and suction injection parameter s 0 and 5 all tables are listed for tenth degree polynomials n 10 number of multi step intervals m 60 numerical sub intervals n 501 α 5 m p 1 and h a 2 it is observed that absolute difference between lmdtm and fofdm ranges from 7 2 10 12 to 6 9 10 7 based on flow parameters behaviors of velocities and their derivatives it is also observed that lmdtm satisfies the required uniqueness accuracy convergence and stability in comparison with the stable and accurate fofdm table 8 shows that the friction factors of main velocity decreases in magnitude as fs decreases but they are increases with increasing hall effect it is observed that the friction factor at left plate is less than in magnitude friction factor at right plate because s 0 table 9 shows that the friction factors of transverse velocity decrease in magnitude as fs decreases but they are increase with increasing hall effect it is observed that the friction factor at left plate is less than in magnitude friction factor at right plate because s 0 table 10 shows that the friction factors of main velocity decrease in magnitude as fs decreases but they are increase with increasing hall effect it is observed that the friction factor at left plate is equal in magnitude to friction factor at right plate because s 0 table 11 shows that the friction factors of transverse velocity decrease in magnitude as fs decreases but they are increase with increasing hall effect it is observed that the friction factor at left plate is less than in magnitude friction factor at right plate because s 0 it is observed that the friction factor at left plate is equal in magnitude to friction factor at right plate because s 0 variations of velocity profiles u y and w y with fs mh and s are shown in figs 6 7 8 at n 101 m 40 n 10 mp 1 ha 2 α 5 fig 6 shows the variations of velocities with forchheimer parameter fs and impermeable plates s 0 it is clear that increasing fs decreases u and w because forchhiemer parameter fs resists motion it is also observed that the profiles are symmetric because of absence of porosity of plates the shown curves show excellent agreement between lmdtm and fofdm it is observed that the profile of main velocity u is higher than transverse velocity w for the same paramerters it is also observed that the reduction in these profiles is proportioinal to fs these results show the uniqueness convergence and stability of the new technique by comparisons with the fourth order accurate finite difference solution fofdm fig 7 shows the variations of velocities with hall parameter mh with permeable plates s 5 and fs 1 it is clear that increasing mh increases u and w because mh generates velocity it is also observed that the profiles are skewed to left lower plate because s 5 represent injection and suction at lower and upper plates respectively this represents a lateral force which deviates the veolcities u and w the shown curves show excellent agreement between lmdtm and fofdm these curves show that the transverse velocity w is affected by the hall current mh more than the main velocity u because mh generates w it is observed that the profile of main velocity u is higher than transverse velocity w for the same paramerters these results show the uniqueness convergence and stability of the new technique by comparisons with the fourth order accurate finite difference solution fofdm fig 8 shows the variations of velocities with suction injection parameter s 5 0 and 5 at fs 1 and mh 0 5 it is clear that increasing mh increases u and w because mh generates velocity it is also observed that the profiles are skewed to left for s 5 and skewed to right for s 5 and symmetric at s 0 the shown curves show excellent agreement between lmdtm and fofdm these results show the uniqueness convergence and stability of the new technique by comparisons with the fourth order accurate finite difference fofdm solution 7 conclusions forchheimer model is needed at high flow rate where flow will exhibit non linearity with respect to velocity which make darcy law inapplicable at these conditions in this case the momentum equations become non linear a new approach to multi step differential transformation method called lmdtm is studied and applied on a system of non linear differential equations the dtm and mdtm transform the non linear differential equations to non linear algebraic equations they introduce recurrence relations between coefficients using boundary conditions which lead to other non linear algebraic equations the solution of last equations gives more than real and complex roots the new approachis a power series solution avoiding solution multiplicity and divergence by a linearization technique that is applied on non linear governing equations to obtain the unique and convergent solution the new approach lmdtm transforms the resulted system of non linear algebraic equations to linear ones the linearized equations are enhanced by iterations up to achieving the required accuracy the convergence and stability of the new technique are tested by comparison with previously available works and an accurate numerical solution called fourth order finite difference method fofdm the effects of hall parameter forchhiemer parameter and suction injection on the velocities and friction factors are tabulated and plotted it is observed that the new approach is simple with high accuracies and it gives the required unique solution the new approach lmdtm is applied on mhd flow in non darcy medium between porous parallel plates considering hall current the linearization technique is applied on the non linear forchiemer term the present results show the high efficiency and powerful of the present approach the prsent results show the uniqueness of the new technique by comparisons with the fourth order accurate finite difference solution fofdm credit authorship contribution statement karem m ewis writing original draft declaration of competing interest none supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103677 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
439,the study of fluid flow in non darcy porous medium subjected to hall current is very important in scientific and engineering applications such as water filters groundwater discharge in aquifers petroleum engineering mhd generators and chemical engineering forchheimer model is needed at high flow rate where flow will exhibit non linearity with respect to velocity which make darcy law inapplicable at these conditions in this case the momentum equations become non linear the classical differential transformation method transforms the non linear differential equations to a non linear algebraic system which gives more than one solution and may be unstable that leads to divergence of the required solution the novelty of present method that it is a power series solution avoiding solution multiplicity and divergence by a linearization technique that is applied on non linear governing equations to obtain the unique and convergent solution the uniqueness convergence and stability of the new technique are tested by comparisons with previously available works and it is also verified by a fourth order accurate finite difference fofdm solution the effects flow parameters on the velocity and friction factor are illustrated the values of parameters in the present work are chosen according to the available previous results the power of the persent method to compute over a lagre range of parameters and the distinctiveness between curves in figures keywords new approach lmdtm mhd flow fofdm porous plates hall current forchheimer medium friction factor 1 introduction the flow of fluids through porous medium under influence of magnetic field and suction injection at plates has many engineering applications such as mhd generators water filters petroleum engineering and chemical engineering verma and gupta 2018 non darcy forchheimer flow has many engineering applications where it describes fluid flow in consolidated or unconsolidated porous media when abrupt changes in velocity dominates sharma et al 2017 a criterion or a generalized equation is required to understand this ow behavior in the isotropic anisotropic carbonate and sandstone reservoirs the relationship between the pressure drop and flow rate in problems of fluid flow through porous media is known to be affected by the nature of flow through the porous media it has been observed by darcy that the pressure drop remains proportional to the rate of flow at low reynolds number sharma et al 2017 but at high flow rate flow will exhibit non linearity with respect to velocity which make darcy law inapplicable at these conditions in the non darcy flow the inertial forces and kinetic energy changes signifcantly due to the expansion and contraction of gas in the porous medium to account this non linearity between pressure gradient and velocity a non darcy term was frst introduced by forchheimer el dabe et al 2017 have been investigated the non darcian mhd flow of a casson nanofluid through parallel plates channel porous plates with uniform suction and injection are considered they used the finite difference method fdm of second order accuracy to solve the nonlinear system of differential equations they obseved that the velocity reached the steady state faster than temperature and nanparticles concentration attia et al 2015 studied the effects of the non drcian forchheimer and hall current resistances on the unsteady flow and heat transfer between two porous plates they solved the governing partial differential equations numerically by the finite difference method fdm joule and viscous dissipations are considered in the energy equation ewis 2019 used a second order accurate finite difference method to solve the governing equations of natural convection of non newtonian rivlin ericksen fluid flow and heat transfer under the influences of non darcy resistance force constant pressure gradient dissipation and radiation the novelty his is to solve this problem between parallel plates channel instead of one plate the fluid flows between two heated parallel plates that are kept at constant temperatures finite difference schemes transform the coupled non linear differential momentum and energy equations to linearized system of algebraic equations some comparisons are made to study the convergence and stability of the solution sulochana et al 2018 studied the effects of nanofluids and magnetic field on flow temperature and concentration in a porous medium past an inclined plate in the presence of radiation they used the perturbation method to solve the differential equations govern the problem thier results show that thermal radiation and chemical reaction restrictions have a trend to enhance thermal and mass transport rates respectively and also water based tio 2 nanofluid possess higher velocity compared with water based cuo nanofluids applications of analytical methods on non linear differential equations are preferred if it is possible to introduce exact approximate solutions of mathematical and engineering problems the differential transform method dtm obtains an analytical solution in the form of a polynomial it is different from the traditional high order taylor s series method which requires symbolic representation of the necessary derivatives of the data functions it has been used to solve effectively easily and accurately a large class of linear and nonlinear differential equations with approximations ewis 2013 inestigated the effect of darcy model on velocity and temperature in the presence of joule and viscous dissipations he used the tradional dtm to solve the differential equations that govern the mhd couette flow and heat transfer in porous medium between porous parallel plates the hall term affects the main velocity component u in the x direction and gives rise to another velocity component w in the z direction the convergence and stability of the dtm is tested using a high accurate fdm his study shows the power of dtm to obtain power series solution of coupled linear differential equations the multi step differential transformation method mdtm has been used for solving linear nonlinear differential equations instead of the traditional differential transformation method dtm to avoid divergence in the resulted polynomial keimanesh et al 2011 used the mdtm to study a third grade non newtonian fluid flow between two parallel plates they studied the reliability and performance of the mdtm in comparison with the numerical method fourth order runge kutta and other analytical methods such as hpm ham and dtm in solving this problem rashidi et al 2011 applied the mdtm on flow of a second grade fluid over a stretching or shrinking sheet their dtm solutions are only valid for small values of the independent variable gkdoğan et al 2012 proposed a fast and effective adaptive mdtm to solve nonlinear differential equations at the same time a comparison between the msdtm and the adaptive mdtm reveals that the proposed approach is an efficiency tool for solving the considered equations using fewer time steps gusynin et al 2017 studied estimating of accuracy and justification the application effectiveness of the multi step differential transform method for solving non linear boundary value problems they presented results of numerical solution of a non linear boundary value problem and shown the efficiency of application of the multi step differential transform method compared with traditional differential transform method the unsteady magnetohydrodynamic flow of an electrically conducting viscous incompressible non newtonian casson fluid bounded by two parallel non conducting porous plates has been studied with heat transfer considering the hall effect sayed ahmed et al 2011 numerical solutions are obtained for the governing momentum and energy equations taking the joule and viscous dissipations into consideration the effect of unsteady pressure gradient the hall term the parameter describing the non newtonian behavior on both the velocities and temperature distributions have been studied the transient mhd couette flow and heat transfer of dusty fluid between two parallel plates and the effect of the temperature dependent properties has been investigated mosayebidorcheh et al 2015 a hybrid treatment based on finite difference method fdm and differential transform method dtm is used to solve the coupled flow and heat transfer equations reddy et al 2017 used the adomian decomposition method for hall andion slip effects on mixed convection flow of a chemically reacting newtonian fluid between parallel plates with heat generation absorption in addition the numerical data for skin friction heat and mass transfer rates have been shown in tabular form the novelty of present method is to avoid solution multiplicity and divergence by a linearization technique that is applied on non linear governing equations to obtain the unique and convergent solution in the present paper a new approach to multi step differential transformation method called lmdtm is introduced and applied on an engineering application the dtm and msdtm transform the non linear differential equations to non linear algebraic equations they introduce recurrence relations between coefficients using boundary conditions which lead to other non linear algebraic equations the solution of last equations gives more than real and complex roots the new approach lmdtm transforms the resulted system of non linear algebraic equations to a linearized one the linearized equations are enhanced by iterations up to achieving the required accuracy the convergence and stability of the new technique are tested by comparison with previously available works and an accurate numerical solution called fourth order finite difference method fofdm the effects flow on the field unknowns are studied it is observed that the new approach is simple with high accuracies and it gives the required unique solution the new approach represents a benchmark for various numerical and analytical solutions which have more than one solution the new approach lmdtm is applied on mhd flow in non darcy medium between porous parallel plates considering hall current the linearization technique is applied on the non linear forchiemer term the present results show the high efficiency and powerful of the present approach 2 governing equations the fluid is assumed to be steady laminar incompressible and flows between two infinite horizontal plates the plates are located at y h planes and extend from x to as shown in fig 1 the two plates are stationary and permeable with suction injection at one plate and injection suction at the other with velocity v 0 the porous medium forchiemer model is considered and leads to non linear term in momentum equations a constant pressure gradient and uniform magnetic field b 0 is applied in the positive y direction and is assumed disturbed as the induced magnetic field is considered by assuming a large magnetic reynolds number thus the hall effect is taken into consideration and consequently a z component for the velocity is expected to arise the momentum equation in vector form is given by el dabe et al 2017 and reddy et al 2017 1 ρ v v p τ j b 0 f b where ρ is the density of the fluid v is the velocity vector is the gradient derivative p is the preesure τ is the shear stress tensor b 0 is the vector magnetic field f b is the body force the current density j is given by 2 j σ v b 0 β j b 0 where σ is the eletric conductivity and β is the hall factor the induced magnetic field results a normal velocity component in z direction thus the fluid velocity vector is given by 3 v u ı v 0 j w k where u is the axial velocity w is velocity component results by hall current and v 0 is the transverse velocity at plates an order of magniude analysis is applied on the momentum equations considering the ratios of dimensions of the plates spacing between them mala et al 1997 and breugem et al 2005 according to this concept the orders of magniudes of flow quantities may be written as x z u w and p x are o l where l is a refernce length such that l h also y v p y and p z are o h under the assumptions mentioned above the governing equations are written as el dabe et al 2017 and reddy et al 2017 4 ρ v 0 u d p d x μ u σ b 0 2 1 m h 2 u m h w μ k p u ρ b k p u 2 5 ρ v 0 w μ w σ b 0 2 1 m h 2 w m h u μ k p w ρ b k p w 2 where where m h σ β b 0 is the hall parameter and μ is the fluid viscosity the boundary conditions of momentum equations are shown in fig 1 and they are given by 6 u h w h w h u h 0 it is expedient to write the governing equations in the non dimensional form to do this the following non dimensional quantities are introduced y y h 2 h u u u 0 w w u 0 p p ρ u 0 2 f s h b k p forchheimer parameter h a 2 σ b 0 2 h 2 μ h a hartmann number m p h μ ρ u 0 k p porosity parameter r e ρ u 0 h μ reynolds number s ρ v 0 h μ suction parameter thus eqs 4 6 are rewritten as hats are dropped for simplicity 7 s 2 u α u 4 h a 2 1 m h 2 u m h w m p u f s u 2 8 s 2 u w 4 h a 2 1 m h 2 w m h u m p w f s w 2 9 u 0 w 0 w 1 u 1 0 3 the linearized multi step differential transformation method lmdtm the finite domain of solution 0 y 1 is divided into m subintervals such that the mesh size is δ 1 m with counter m 1 2 3 m 1 the kth order differential transformation f k m of a the function fm y in the mth sub interval y m y m 1 is defined as ewis 2013 10 f k m f m y y y m k 1 k 1 where k 1 2 3 and m 1 2 3 m thus the inverse differential transformation dit of the coefficient f k m is defined as 11 f m y i 1 f k m y y m k 1 according these eqs 10 11 the required transformations are obtained by the following theorems taking y 0 0 theorem 1 if f m y α is a constant then its differential transformation is given by f k m α δ k 1 where k 1 2 theorem 2 if f m y u y then its differential transformation is given by f k m u k m where k 1 2 theorem 3 if f m y u y then its differential transformation is given by f k m k u k 1 m where k 1 2 theorem 4 if f m y u y then its differential transformation is given by f k m k k 1 u k 2 m where k 1 2 theorem 5 if f m y u 2 y then its differential transformation is given by f k m r 0 k 1 u r 1 m u k r m where k 1 2 by using the basic properties of differential transform method fromabove theorems and taking the transform of eqs 7 9 one can obtain that 12 s 2 k u k 1 m α δ k 1 k k 1 4 u k 2 m h a 2 1 m h 2 u k m m h w k m m p u k m f s r 0 k 1 u r 1 m u k r m 13 s 2 k w k 1 m k k 1 4 w k 2 m h a 2 1 m h 2 w k m m h u k m m p w k m f s r 0 k 1 w r 1 m w k r m 14 u 1 1 w 1 1 k 1 u k m k 1 w k m 0 where δ k 1 1 k 1 0 o w is the kronecker delta u k 2 m and w k 2 m is are differential transformations of velocities u and w respectively in the mth sub interval eqs 12 14 represent non linear system algebraic equations because of multiplications in summations the basic idea is to transform these terms to linearized ones to avoid more than one solution that arise when the non linear algebraic equations are solved numerically and they are subjected to divergence thus system 12 14 is rewritten in the linearized form where the bar symbol represent the linearized terms 15 s 2 k u k 1 m α δ k 1 k k 1 4 u k 2 m h a 2 1 m h 2 u k m m h w k m m p u k m f s r 0 k 1 u r 1 m u k r m 16 s 2 k w k 1 m k k 1 4 w k 2 m h a 2 1 m h 2 w k m m h u k m m p w k m f s r 0 k 1 w r 1 m w k r m where bar notation refes to the linearized term continuity and differentiability of the analytic function f x at nodes m 1 2 m are written as 17 f m y m 1 f m 1 y m 1 18 f m y m 1 f m 1 y m 1 the mdtm of at nodes are written as 19a k 1 u k m m k 1 u 1 m 1 19b k 1 w k m m k 1 w 1 m 1 20a k 2 k 1 u k m m k 2 u 2 m 1 20b k 2 k 1 w k m m k 2 w 2 m 1 with the new technique lmdtm the given differential equations and related boundary conditions continuity and differentiability are transformed into a linearized system instead of non linear algebraic equations which are resulted from dtm or mdtm finally 2 n 1 m coefficients u k m w k m of power series of degree n are computed by iterative technique these coefficients are used to compute the basic unknowns in the governing equations as 21 u m y i 1 n u k m y y m k 1 22 w m y i 1 n w k m y y m k 1 where y m y y m 1 and m 1 2 m the skin friction factor is an important fluid flow parameter because of its very importance in the engineering applications these quantities are computed after solution the governing equations the skin friction factor is defined as ewis 2018 23 c f 2 τ w ρ u 0 2 the dimensionless forms of skin friction factors for axial velocity and transverse velocity are written respectively as 24 c f u r e c f u u 0 at lower plate u 1 at upper plate 25 c f w r e c f w w 0 at lower plate w 1 at upper plate using the mdtm we can write 26 c f u u 1 1 at lower plate k 2 k 1 u k m m k 2 at upper plate 27 c f w w 1 1 at lower plate k 2 k 1 w k m m k 2 at upper plate 4 the fourth order finite difference method fofdm the finite domain of solution 0 y 1 is divided into n 1 subintervals such that the mesh size is δ 1 n 1 with counter i 1 2 3 n the linearized system of coupled non linear ordinary differential eqs 7 9 is transformed to a system algebraic equations using the fourth order difference schemes the following fourth order schemes are obtained by taylor s expansions of the variable f x represents u or w about y i i 1 δ ewis 2018 28 f y y 1 1 12 δ 25 f 1 48 f 2 36 f 3 16 f 4 3 f 5 29 f y y 2 1 12 δ 3 f 1 10 f 2 18 f 3 6 f 4 f 5 30 f y y i 1 12 δ f i 2 8 f i 1 8 f i 1 f i 2 31 f y y n 1 1 12 δ f n 4 6 f n 3 18 f n 2 10 f n 1 3 f n 32 f y y n 1 12 δ 25 f n 48 f n 1 36 f n 2 16 f n 3 3 f n 4 33 f y y 2 1 12 δ 2 10 f 1 15 f 2 4 f 3 14 f 4 6 f 5 f 6 34 f y y i 1 12 δ 2 f i 2 16 f i 1 30 f i 16 f i 1 f i 2 35 f y y n 1 1 12 δ 2 f n 5 6 f n 4 14 f n 3 4 f n 2 15 f n 1 10 f n another linearized form is used for the non linear terms in the governing equations so one can write 36 s 2 u α u 4 h a 2 1 m h 2 u m h w m p u f s u u 37 s 2 u w 4 h a 2 1 m h 2 w m h u m p w f s w w where bar notation refers to the linearized variable where it must be modified by iterated techniques replacing the continuous terms in eqs 36 and 37 by corresponding discrete formulas 28 35 the resulted linearized system with boundary conditions are solved for the two components of the flow velocity u w 5 error analysis an error analysis is introduced to verify the power of the persent method comparing with the available previous results and the fourth order finite difference method tables 1 2 3 4 5 6 7 illustrate the comparison of present analytical results with previously available exact and numerical results the present analytical results are listed for different degrees of polynomials n different number of multi step intervals m and numerical sub intervals n where the order of trucated series in velocity is 1 n 1 m n 1 in addition to other errors such as the rounoff error the given comparisons show the power of the new method where it is convergent stable and high accurate for a wide range of problem parameters table 1 shows the comparison of the new approach with the exact solution of verma and gupta 2018 the axial velocity profile is computed for s 0 5 f s m h 0 m p 4 h a 1 α 5 4 it is observed that the maximum absolute error a e 1 of axial velocity between lmdtm with second degree polynomials n 2 and the available exact solution verma and gupta 2018 is 7 35 10 3 but a d 2 between lmdtm with fifteenth degree polynomials n 15 and the available exact solution verma and gupta 2018 is 5 10 16 this comparison shows the power of the new method such that it is flexible to obtain the exact solution with increasing the degree of polynomials n and number of multi step intervals m tables 2 and 3 show the velocity profiles and for s f s m h m p h a 1 α 5 n 10 and m 40 it is observed that the maximum absolute difference a d 1 of axial velocity between lmdtm and fofdm is 1 5 10 9 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 1 2 10 4 it is also observed that the maximum absolute difference a d 1 of transverse velocity between lmdtm and fofdm is 2 9 10 10 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 8 9 10 6 tables 4 and 5 show the velocity profiles and for s f s m p h a 1 mh 3 α 5 n 10 and m 40 it is observed that the maximum absolute difference a d 1 of axial velocity between lmdtm and fofdm is 10 14 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 1 2 10 4 it is also observed that the maximum absolute difference a d 1 of transverse velocity between lmdtm and fofdm is 5 10 15 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 5 7 10 6 tables 6 and 7 show the velocity profiles and for s 0 fs 2 and mh 1 it is observed that the maximum absolute difference a d 1 of axial velocity between lmdtm and fofdm is 1 5 10 9 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 8 10 5 it is also observed that the maximum absolute difference a d 1 of transverse velocity between lmdtm and fofdm is 2 9 10 10 but a d 2 between lmdtm and the second order accurate fdm attia et al 2015 is 8 9 10 6 figs 2 3 4 5 show the rates of convergence and stability of axial velocity u and transverse velocity w the degree of polynomial n and number of multi step intervals m takes values of 2 3 and 10 to show generations of convergence and stability of the lmdtm in comparison with the stable and the fourth order accurate fofdm with number of points n 241 so lmdtm is powerful and effective for solving non linear problems by series it is observed that increasing n decreases the absolute relative error in computations with highest rate for axial velocity u and lowest rate for transverse velocity w it is also observed that the degree of polynomial n 10 in lmdtm satisfies convergence and stability in comparison with the stable and fourth order accurate fofdm 6 results and discussion the present results are listed intales and figures to show the trend of profiles and the accuracy of the new approach the values of parameters in the present work are choosen according to the available previous results the power of the persent method to compute over a lagre range of parameters and the distinctiveness among curves in figures tables 8 9 10 11 show the uniqueness convergence and accuracy of skin friction factors u 0 u 1 w 0 and w 1 for various values of forchhiemer parameter fs 0 5 10 20 and 30 and two values of hall parameter mh 0 5 and 1 and suction injection parameter s 0 and 5 all tables are listed for tenth degree polynomials n 10 number of multi step intervals m 60 numerical sub intervals n 501 α 5 m p 1 and h a 2 it is observed that absolute difference between lmdtm and fofdm ranges from 7 2 10 12 to 6 9 10 7 based on flow parameters behaviors of velocities and their derivatives it is also observed that lmdtm satisfies the required uniqueness accuracy convergence and stability in comparison with the stable and accurate fofdm table 8 shows that the friction factors of main velocity decreases in magnitude as fs decreases but they are increases with increasing hall effect it is observed that the friction factor at left plate is less than in magnitude friction factor at right plate because s 0 table 9 shows that the friction factors of transverse velocity decrease in magnitude as fs decreases but they are increase with increasing hall effect it is observed that the friction factor at left plate is less than in magnitude friction factor at right plate because s 0 table 10 shows that the friction factors of main velocity decrease in magnitude as fs decreases but they are increase with increasing hall effect it is observed that the friction factor at left plate is equal in magnitude to friction factor at right plate because s 0 table 11 shows that the friction factors of transverse velocity decrease in magnitude as fs decreases but they are increase with increasing hall effect it is observed that the friction factor at left plate is less than in magnitude friction factor at right plate because s 0 it is observed that the friction factor at left plate is equal in magnitude to friction factor at right plate because s 0 variations of velocity profiles u y and w y with fs mh and s are shown in figs 6 7 8 at n 101 m 40 n 10 mp 1 ha 2 α 5 fig 6 shows the variations of velocities with forchheimer parameter fs and impermeable plates s 0 it is clear that increasing fs decreases u and w because forchhiemer parameter fs resists motion it is also observed that the profiles are symmetric because of absence of porosity of plates the shown curves show excellent agreement between lmdtm and fofdm it is observed that the profile of main velocity u is higher than transverse velocity w for the same paramerters it is also observed that the reduction in these profiles is proportioinal to fs these results show the uniqueness convergence and stability of the new technique by comparisons with the fourth order accurate finite difference solution fofdm fig 7 shows the variations of velocities with hall parameter mh with permeable plates s 5 and fs 1 it is clear that increasing mh increases u and w because mh generates velocity it is also observed that the profiles are skewed to left lower plate because s 5 represent injection and suction at lower and upper plates respectively this represents a lateral force which deviates the veolcities u and w the shown curves show excellent agreement between lmdtm and fofdm these curves show that the transverse velocity w is affected by the hall current mh more than the main velocity u because mh generates w it is observed that the profile of main velocity u is higher than transverse velocity w for the same paramerters these results show the uniqueness convergence and stability of the new technique by comparisons with the fourth order accurate finite difference solution fofdm fig 8 shows the variations of velocities with suction injection parameter s 5 0 and 5 at fs 1 and mh 0 5 it is clear that increasing mh increases u and w because mh generates velocity it is also observed that the profiles are skewed to left for s 5 and skewed to right for s 5 and symmetric at s 0 the shown curves show excellent agreement between lmdtm and fofdm these results show the uniqueness convergence and stability of the new technique by comparisons with the fourth order accurate finite difference fofdm solution 7 conclusions forchheimer model is needed at high flow rate where flow will exhibit non linearity with respect to velocity which make darcy law inapplicable at these conditions in this case the momentum equations become non linear a new approach to multi step differential transformation method called lmdtm is studied and applied on a system of non linear differential equations the dtm and mdtm transform the non linear differential equations to non linear algebraic equations they introduce recurrence relations between coefficients using boundary conditions which lead to other non linear algebraic equations the solution of last equations gives more than real and complex roots the new approachis a power series solution avoiding solution multiplicity and divergence by a linearization technique that is applied on non linear governing equations to obtain the unique and convergent solution the new approach lmdtm transforms the resulted system of non linear algebraic equations to linear ones the linearized equations are enhanced by iterations up to achieving the required accuracy the convergence and stability of the new technique are tested by comparison with previously available works and an accurate numerical solution called fourth order finite difference method fofdm the effects of hall parameter forchhiemer parameter and suction injection on the velocities and friction factors are tabulated and plotted it is observed that the new approach is simple with high accuracies and it gives the required unique solution the new approach lmdtm is applied on mhd flow in non darcy medium between porous parallel plates considering hall current the linearization technique is applied on the non linear forchiemer term the present results show the high efficiency and powerful of the present approach the prsent results show the uniqueness of the new technique by comparisons with the fourth order accurate finite difference solution fofdm credit authorship contribution statement karem m ewis writing original draft declaration of competing interest none supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103677 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
