index,text
3195,it is important to study the source sink relationship of pollution however research on the groundwater pollution sources and pollution sinks has often been conducted separately in the past which makes it difficult to study the source sink relationship of pollutants in this study the source and sink of pollution in a site contaminated by a light non aqueous phase liquid lnapl is considered to study the relationship between them the simulation model needs to be called repeatedly in the research process to save time and reduce the calculation load the resnet 18 network structure is first utilized to create a substitute of simulation model however the performance of resnet 18 surrogate model largely depends on the choice of hyperparameters the variable density grid search method is thus utilized to optimize the hyperparameters in addition the quality of training samples is an important factor to determine the accuracy of surrogate models and the accuracy of the surrogate model will affect the effect of groundwater pollution identification therefore the resnet 18 surrogate model and the ensemble smoother algorithm are combined to construct an adaptive cyclic improved iterative process through the adaptive cyclic improved iterative process the training samples surrogate model and identification results of variables to be solved can be improved and updated together with the iterative process to improve the solution effect of groundwater lnapls pollution source identification problem biodegradation is an important sink of lnapl pollutants therefore the biodegradation on the lnapl pollutants in groundwater is considered by adding a biodegradation model to the original model of multi phase flow finally based on the numerical simulation models of multiphase flow and biodegradation the monte carlo stochastic simulation are used to study the source sink relationship of lnapl pollutants a hypothetical case based on real conditions was used to illustrate the validity of the methods the results implied that the proposed methods can not only improve the effect of lnapl pollution identification but also study the source sink relationship of lnapl pollutant keywords lnapls contamination source sink relationship deep learning numerical simulation model of biodegradation adaptive cyclic improved iterative process data availability data will be made available on request 1 introduction organic petroleum pollutants are a major problem in the context of groundwater pollution owing to their low solubility after entering groundwater they mostly flow in a separate phase and are called non aqueous phase liquids napls light non aqueous phase liquids lnapls have a density lower than that of water and dense non aqueous phase liquids have a higher density than water napls are often characterized by high toxicity if not treated in time they can cause significant pollution of the groundwater aksoy 1985 lari et al 2019 tomlinson et al 2017 napl induced groundwater pollution can be reduced by accurately obtaining information on the sources of pollution research on groundwater contamination source identification gcsi is thus important there are some important reviews about gcsi atmadja and bagtzoglou 2001 barati moghaddam et al 2021 gómez hernández and xu 2022 research on gcsi began in the 1980s dokou and pinder 2009 gorelick et al 1983 and the methods in this field can be divided into methods of direct and indirect inversion atmadja and bagtzoglou 2001 methods of direct inversion refer to the deformation processing of the equations of groundwater flow and solute transport without using a forward model in the process of inversion to achieve a direct reverse solution mathematically it is a solution of inverse time and traceability methods of direct inversion include tikhonov regularization neupauer et al 2000 quasi reversibility bagtzoglou and atmadja 2003 adjoint state method michalak and kitanidis 2004 and marching jury backward beam equation bagtzoglou and atmadja 2003 the application conditions of direct inversion methods are relatively harsh they are generally used to study ideal examples and have significant limitations in application to complex practical problems the method of indirect inversion is opposite to that of direct inversion in that the forward model needs to be called multiple times in the inversion process this method originates from the trial and error method the core idea is that when solving an inverse problem an expert first gives a trial solution substitutes this solution into the forward model to obtain the calculated value and compares the calculated value with the observed data if the gap between them is within an acceptable range this trial solution is the solution of the inverse problem otherwise the expert generates a new trial point and repeats the above process until the solution is acceptable methods of indirect inversion can be further classified into two types simulation optimization and stochastic inversion methods hwang et al 2020 the aim of the former approach is to minimize the gap between observation values and outputs of the simulation model to obtain a group of optimal solutions particle swarm optimization zhao et al 2020 simulated annealing yeh et al 2007 and genetic algorithms ayvaz 2016 guo et al 2019 are optimization algorithms often used in gcsi however simulation optimization cannot consider uncertainty where this reduces the reliability of the results in the stochastic inverse approach the goal is to maximize the posterior probability of unknown variables with the help of observed data it can consider uncertainty to compensate for the deficiency of the simulation optimization approach chang et al 2021a the geostatistical approach butera and tanda 2003 generalized likelihood uncertainty estimation vrugt et al 2008 minimum relative entropy ulrych and woodbury 2003 and markov chain monte carlo yan et al 2019 zhang et al 2016 are examples compared with the direct inversion method the method of indirect inversion has greater potential for application and is more mature and flexible for solving practical problems in addition to the above methods the ensemble kalman filter enkf is a data assimilation method that has been widely used for gcsi problems in the last few years xu and jaime gomez hernandez 2018 it is a kind of stochastic inversion method that combines the kf method with the monte carlo sampling strategy sun et al 2020 the ensemble smoother es is an alternative that can reduce the burden of calculation imposed by the enkf xu et al 2020 it avoids the need for simulation restarts at each time step which renders the algorithm more efficient than the enkf emerick and reynolds 2013 the es here was adjusted with reference to the iterative ensemble smoother ju et al 2018 to expedite calculation the idea is to incorporate all the data at all times for several iterations and gradually update the unknown variables after each iteration until the convergence condition has been satisfied in past research groundwater pollution sources and sinks have often been separately considered research on the pollution sink mainly explains the fate and destination of pollutants in groundwater the relevant studies have focused on the physical chemical and biological effects of pollutants in groundwater chaudhary and singh 2022 singh et al 2021 zhai et al 2022 which are collectively referred to as the fate process of pollutants bedient et al 1999 in this study both the source and the sink of pollutants in an lnapl contaminated site were considered the information on groundwater pollution source due to lnapls was identified by the es algorithm moreover the effects of biodegradation were considered in the forward simulation of lnapls migration a monte carlo stochastic simulation was used to examine the effects of the pollution source on the allocation of pollutants at each destination in the aquifer this helps clarify the relationship between the source and sink of pollutants in groundwater the indirect method and the stochastic simulation involve a large number of calls to the simulation model which is time consuming the surrogate model is a good choice for alleviating the computational burden caused by this it can be classified into two types shallow and deep learning methods unlike shallow learning deep learning can simulate more complex mapping relationships and is widely used in groundwater research li et al 2021 pan et al 2021 however the performance of deep learning largely depends on the choice of hyperparameters jin et al 2021 li et al 2018 most of them find suitable hyperparameter by manually adjusting parameters yu et al 2020 zhang et al 2020 where this depends on the experience of the scientist therefore the variable density grid search vdgs method was utilized to optimize the hyperparameters of the resnet 18 network structure and apply this to replace the simulation models of multiphase flow and biodegradation to verify its effectiveness in addition the quality of training samples is an important factor to determine the accuracy of surrogate models in the past the training samples of the surrogate model are often generated at one time which makes the quality of the training samples difficult to be continuously improved with the iteration therefore it is difficult to establish a high precision surrogate model which will affect the solution accuracy of groundwater lnapls pollution source identification problem one approach to solve this problem is to build an adaptive cyclic improved iterative process that combines the surrogate model with the process of solution of inverse problems zhang et al 2016 the results of recognition are constantly added to the training data to update the surrogate model which is then used in the subsequent inversion however surrogate models used in the adaptive cyclic improved iterative process are shallow chang et al 2021a gong and duan 2017 zhang et al 2016 we thus combine a resnet 18 surrogate model a deep learning method and the es algorithm to construct an adaptive cyclic improved iterative process based on deep learning through the adaptive cyclic improved iterative process based on deep learning the training samples resnet 18 surrogate models and identification results of variables to be solved can be improved and updated together with the iterative process to improve the solution effect of groundwater lnapls pollution source identification problem the aim here is to explore the source sink relationship of the pollutant at an lnapl contaminated site the contributions of this paper are as follows the deep residual network method based on resnet 18 structure was applied to construct the surrogate model of multiphase flow and biodegradation numerical simulation models which effectively improved the approximation accuracy of the surrogate model to the simulation models moreover the variable density grid search method was also used to optimize the hyperparameters in the surrogate model which further improved the approximation accuracy of the surrogate model to the simulation model an adaptive cyclic improved iterative process based on deep learning is constructed to improve the quality of training samples and the solution effect of groundwater lnapls pollution source identification problem in this way the training samples resnet 18 surrogate models and identification results of variables to be solved can be improved and updated together with the iterative process based on the numerical simulation models of multiphase flow and biodegradation the stochastic simulation of groundwater pollution source sink relationship is studied by using monte carlo method 2 methodology 2 1 system model 2 1 1 multi phase flow model there are some of the relevant papers related to numerical simulations by different methods kumar et al 2020 rajput and singh 2021 according to the principle of mass conservation the laws of flow of different components can be summarized into an equation called the equation for the mass conservation chu and lu 2015 delshad et al 1996 wang et al 2021 1 ϕ c k ρ k t l 1 n p ρ k c kl v l ϕ s l k kl c kl r k f o r k 1 n c where k is the component index representing different components nc is the number of components l is the index of the phases and can include aqueous oil microemulsion phase and air phases np is the number of phases involved in the calculation ϕ is the effective porosity l 3 l 3 c k is the total concentration of the k th component l 3 l 3 which is expressed in the form of a volume fraction ρ k is the density of the k th component m l 3 t is the simulation time t c kl is the concentration of the k th component in the l th phase l 3 l 3 which is expressed in the form of a volume fraction v l is the velocity of fluid in the l th phase l t 1 which can be calculated by darcy s law s l is the saturation of the l th phase l 3 l 3 k kl is the tensor of the dispersion coefficient for the k th component in the l th phase l 2 t 1 and r k is the overall source or sink term for the k th component m l 3 t 1 2 1 2 biodegradation model biodegradation is an important process in sites contaminated with lnapl if it is not considered the law of lnapls migration cannot be accurately described the fundamental architecture of equations of the model for a single substrate electron acceptor and biological species are as follows hosseininoosheri et al 2016 2 ds dt β κ x m c s s μ max x y s k s s a k a a k abio s 3 d s dt β κ v c s s μ max ρ x y s k s s a k a a k abio s 4 da dt β κ x m c a a μ max x e y s k s s a k a a 5 d a dt β κ v c a a μ max ρ x e y s k s s a k a a 6 dx dt μ max x s k s s a k a a b x 7 d x dt μ max x s k s s a k a a b x where s represents the concentration of the substrate in the water m l 3 β represents the surface area of a single microcolony l 2 x represents the concentration of the attached biomass m l 3 m c represents the cell mass of a single microcolony m c ρ x v c m κ represents the coefficient of mass transfer l t 1 s represents the concentration of the substrate in the attached biomass m l 3 μ max represents the maximum specific growth rate t 1 y represents the yield coefficient which denotes the cell mass produced per substrate mass biodegraded m m 1 x represents the concentration of biomass in the water m l 3 k s represents the half saturation coefficient of the substrate m l 3 a represents the concentration of electron acceptors in the water m l 3 k a represents their half saturation coefficient m l 3 v c represents the volume of a single microcolony l 3 k abio represents the coefficient of the first order reaction rate t 1 ρ x represents the density of the biomass m l 3 a represents the concentration of the electron acceptor in the attached biomass m l 3 e represents the mass of the electron acceptors consumed per substrate of mass biodegraded m m 1 and b represents the endogenous decay coefficient t 1 the combined multi phase flow and biodegradation model is solved by utchem chu and lu 2015 hou and lu 2018 through operator splitting where the solution of the multiphase flow equation is used as an initial condition for the biodegradation equation in each calculated time step operator splitting is appealing from the view point of the calculation efficiency valocchi and malmstead 1992 2 2 resnet surrogate system 2 2 1 model architecture for the general surrogate model of deep learning the deeper it is the stronger its capability to fit a non linear expression is and theoretically the higher is the accuracy of fitting however in practice the deeper the neural network is the more difficult it is to train and the more easily does the gradient vanish resulting in a reduction in overall accuracy the residual network resnet proposed by he et al 2016 introduces a residual block to solve this problem the core of the resnet is the design of the residual block we use two residual block structures denoted by res block 1 and res block 2 as shown in fig 1 x denotes the input to the network h x is the target function and g x is the function of the shortcut path instead of directly learning h x in the residual module we learn h x x or h x g x and record it as the function f x in this way the output of the neural network consists of two parts the residual mapping f x and the direct mapping x or g x the introduction of the residual block greatly alleviates the vanishing gradient problem due to an increase in the number of the neural network layers the residual mapping can be expressed as follows he et al 2016 8 y l x l f x l ω l 9 x l 1 f y l where x l is the input to the l t h residual block ω l is its weight and f is the residual function in general f is set to a relu function for the sake of discussion the function f is set as an identity map x l 1 y l the relationship between the l t h residual block of a deeper layer and the l t h residual block can be expressed as 10 x l x l i l l 1 f x i ω i the above equation shows that the input signal can be propagated directly from the low to the high layer during forward propagation because it contains a natural identity map in this way the problem of network degradation can be solved according to the chain rule of derivatives the gradient of the loss function ε with respect to x l can be expressed as 11 ε x l ε x l x l x l ε x l 1 x l i l l 1 f x i ω i the above formula reflects two properties of the residual network in the training process x l i l l 1 f x i ω i cannot always be 1 that is the gradient does not disappear in the residual network ε x l indicates that the gradient of the l t h residual block can be directly transferred to any layer shallower than it the classical deep resnet structures are resnet 18 resnet 34 resnet 50 resnet 101 and resnet 152 we use resnet 18 as model architecture here it has 18 wt layers starting with a convolution layer followed by eight residual blocks and finally a fully connected layer the main architecture of resnet 18 is displayed in fig 2 2 2 2 hyperparameter optimization some parameters of the deep learning surrogate model need to be set before the learning process rather than obtained through training they are called hyperparameters examples include the learning rate number of layers batch size number of neurons and regularization coefficient of each layer the selection of the hyperparameters greatly affects the efficiency of training and the performance of the surrogate model for deep learning hyperparameter optimization is the process of finding the optimal combination of hyperparameter examples of methods to this end include manual adjustment grid search random search bayesian optimization and evolutionary algorithm based optimization each method has its own advantages and disadvantages we use the grid search and improve it to obtain the vdgs let z be a set of n hyperparameters z z 1 z 2 z n z z the score function s c o r e z is used to evaluate the performance of the model the aim of hyperparameter optimization is to find the set of hyperparameter z that yields the best performance 12 z arg max z z s c o r e z 13 s c o r e z j 1 d scor e j z where scor e j z is the score obtained according to the j th accuracy evaluation index when the set of hyperparameter is set to z d is the number of indicators participating in accuracy evaluation steps of the vdgs method are as follows 1 according to the model architecture determine the hyperparameters to be optimized z z 1 z 2 z n 2 select appropriate candidate values for each hyperparameter where the candidate value ensemble of the i t h hyperparameter is z i z i valu e i 1 v a l u e i 2 v a l u e i n i 3 traverse all hyperparameter combinations for grid search 4 select the hyperparameter combination with the highest scoring function s c o r e z and determine whether the model meets the accuracy related requirements if so get the combination of hyperparameters z with the highest score output the model and terminate iterative process if not reduce the scale and judge whether the scale reaches the minimum limit if so use the minimum of scale and the combination of hyperparameters with the highest score to update the candidate value ensemble z i if not conduct fine search according to the transformed scale and the combination of hyperparameters with the highest score and update the candidate value ensemble z i then continue to repeat step 3 the flow chart to present applied vdgs method was shown in fig 3 in this paper resnet 18 surrogate model and the vdgs method were realized on matlab platform 2 3 fast es algorithm both the es and enkf are stochastic methods based on the kalman filter evensen and van leeuwen 2000 these methods were initially used for data assimilation and in recent years have been introduced to research on water resources for parameter estimation and identifying the sources of pollution dokou and pinder 2011 li et al 2019 in the inversion process these methods are based on the cross correlation between model parameters and states and use the observed values to update the model parameters and states this updating process does not require calling the simulation model of the system frequently the cost of this method is thus generally lower than other algorithms chang et al 2021b however both the es and enkf update unknown variables successively with the observed data at different times to speed up the algorithm the es here was adjusted with reference to the iterative ensemble smoother ju et al 2018 directly use the observational data at all times and determine whether convergence is achieved through the conditions for iterative convergence the operational process of the adjusted ensemble smoother is as follows generate the initial ensemble x 0 x 1 0 x ne 0 according to the prior distribution which includes ne unknown variable vectors then update each unknown variable vector in the ensemble x j 1 x 1 j 1 x ne j 1 using the observed data vector y 14 x k j x k j 1 c xy c yy r 1 y ε k f x k j 1 k 1 n e 15 c xy 1 ne k 1 ne x k j 1 x j 1 f x k j 1 f j 1 t 16 c yy 1 ne k 1 ne f x k j 1 f j 1 f x k j 1 f j 1 t where x k j is the k t h unknown variable vector after the j t h iteration c xy is the mutual covariance matrix of the ensemble in the j 1 t h iteration x j 1 and the corresponding output ensemble y j 1 f x 1 j 1 f x ne j 1 c yy is the self covariance matrix of y j 1 r is the observed error covariance matrix f x k j 1 is the output of the simulation model surrogate model with parameters x k j 1 ε k represents random samples of the observed error f j 1 represents the mean vector of the output of the model in the j 1 t h iteration and x j 1 is the mean unknown variable vector in the j 1 t h iteration the update steps are repeated until one of the following conditions is satisfied 1 s x j 1 s x j s x j ξ 2 the given number of iterations times j is larger than the maximum permitted number of iterations i max in the above ξ is a predefined parameter set on the given requirement and s x j is the degree of data fitting expressed as follows 17 s x j k 1 ne y k f x k j r 1 y k f x k j t 2 4 adaptive cyclic improved iterative process to gradually improve the accuracy of approximation of the model in the region of interest the adaptive cyclic improved iterative process for gcsi problems is constructed and the results of identification are updated using it hou et al 2021 ju et al 2018 we propose a novel adaptive cyclic improved iterative process by combining the deep residual network with the fast es algorithm information on the sources of pollution and sensitive parameters of the model are identified by using the resnet 18 surrogate model and the es algorithm this is then used to calculate the standardized euclidean distance between each sample point in the training sample and the given results of identification the results of identification are substituted into the simulation model to obtain a set of input output samples new samples are added to the original training samples the sample that is furthest from the given result of recognition is eliminated and the training samples are updated the euclidean distance can be used to measure the distance between points however the scale of each dimensional component in the data is often different the normalized euclidean distance has been proposed to solve this problem the normalized euclidean distance between a x 11 x 12 x 1 n and b x 21 x 22 x 2 n can be expressed as follows 18 d i 1 n x 1 i x 2 i 2 s i 2 where s i 2 is the variance of the i t h dimension by reconstructing the resnet 18 surrogate model by using updated training samples the precision of the surrogate model in the vicinity of the results of recognition can be enhanced the updated surrogate model and es algorithm are used for inversion recognition to obtain the new results of identification this process is repeated until the convergence condition is satisfied the updated training samples can have a greater coverage near the results of identification such that the surrogate model has a sufficiently high accuracy of approximation in their vicinity through this adaptive cyclic improved iterative process the training samples can be updated gradually the quality of the training data and the performance of the surrogate model can be improved therefore the groundwater contamination identification results can be modified and updated gradually the flow chart to present the adaptive cyclic improved iterative process was shown in fig 4 3 case study 3 1 problem description we used a site contaminated by lnapls as the study area the groundwater in a pore confined aquifer in the study area had been polluted by benzene which is an lnapl the groundwater flowed from northeast to southwest its hydraulic gradient was small about 2 and the aquifer was 10 m thick the boundary conditions of the study area are shown in fig 5 there were 11 observation wells and a pollution source in this aquifer the location of the wells and the potential range of the sources of pollution are shown in fig 6 the simulation time was 7 300 days and data on pollutant concentrations were collected on days 6 720 7 080 and 7 230 the observed data in table 1 was used to identify the unknown variables the pollution due to lnapls in this area might have come from an upstream factory after entering the aquifer over a long period of time lnapls underwent natural dissolution and dispersion the effect of biodegradation was considered when simulating the law of transport of lnapls the variables to be identified during inversion included 1 the initial and termination release times of the pollutant 2 the location of the pollution source including longitudinal and horizontal coordinates 3 the intensity of release and 4 the model parameters selected based on sensitivity analysis the non sensitive parameters were fixed as constants the parameters of the multi phase flow model of the site are listed in table 2 and those of the biodegradation model are shown in table 3 3 2 sensitivity analysis of model parameters sensitivity analysis can be used to determine the influence of parameters on the output of simulation models methods for both local and global sensitivity analysis are commonly used for this we used local sensitivity analysis to examine the impact of parameters of the simulation model on the output the aim was to screen the parameters that had a great impact on the output of the simulation model as the variables to be identified and to set the other parameters as constants the sensitivity of each parameter was calculated using the following formula 19 s i f x i f x i x i f x i f x i x i x i where s i is the sensitivity of the i t h parameter of the simulation model f is the output of the simulation model x i is the i t h parameter of the simulation model and δ x i is the change in x i when the rate of change of the parameters to be analyzed is the same the greater s i is the greater is the sensitivity of the parameters this shows the parameters that need to be analyzed we conducted a sensitivity analysis of the following 11 parameters permeability k porosity ϕ longitudinal aqueous dispersivity α wl transverse aqueous dispersivity α wt longitudinal dispersivity of the oil phase α ol transverse dispersivity of the oil phase α ot the biomass density ρ x colony radius r c colony thickness τ number of cells per microcolony n cell and number of cells per mass of soil c c the results indicate that the four most sensitive parameters were permeability porosity colony radius and transverse aqueous dispersivity fig 7 we thus set the four sensitive model parameters and pollution source information horizontal coordinates and longitudinal coordinates of pollution source location initial release time of pollutants termination release time of pollutants and release intensity of pollution sources as the variables to be identified in the inversion process 3 3 resnet surrogate model we used resnet 18 to establish the surrogate model the steps for constructing the model are as follows a use the latin hypercube sampling approach to sample in the feasible region of the variables to be identified these were the initial release time and termination release time of the pollutants the pollution source location including longitudinal and horizontal coordinates the release intensity permeability porosity colony radius and transverse aqueous dispersivity a total of 330 groups of variables to be identified were extracted according to their prior distributions as shown in table 4 and input to the simulation model the models were run to obtain their concentrations at observation wells corresponding to 330 groups of inputs b two important hyperparameters of resnet 18 the learning rate and batch size were optimized by the vdgs method the appropriate candidate values for the two hyperparameters were determined and combined the resnet 18 model with different combinations of hyperparameter was trained by using the 300 groups of input and output samples and the accuracy of the model was evaluated by using the scoring function the best combination of hyperparameters was checked to determine whether it met the accuracy requirements if it did the corresponding model was output if the accuracy requirements were not satisfied the candidate values of the hyperparameters were determined again according to their given optimal combination and the above steps were repeated c the remaining 30 groups of samples were used to validate the accuracy of the trained resnet 18 surrogate model they were input to the trained model and the outputs of the surrogate model were compared with those of the simulation model to calculate the root mean squared error rmse and deterministic coefficient r2 they were used in turn to assess the approximation of the resnet 18 model and the simulation model 3 4 adaptive cyclic improved iterative process the trained resnet 18 surrogate model and fast es algorithm were used to simultaneously identify information on the sources of pollution and the sensitive parameters ne ξ and i max were set to 600 0 001 and 1000 respectively the results of identification were substituted into the simulation model to obtain an input output sample and the new sample were added to the original 300 training samples the sample with the longest distance from the identification result was eliminated thus the number of updated training samples was still 300 the updated training samples were then used to rebuild the resnet 18 surrogate model for the inverse process this process was repeated until the convergence condition had been satisfied the convergence condition was that the normalized euclidean distance between the results of recognition in two adjacent iterations of the process must be less than 0 03 the training samples surrogate model and identification result were gradually improved through this adaptive cyclic improved iterative process this process was applied to identify the pollution source in a hypothetical example based on real conditions to analyze and illustrate the effectiveness of the adaptive cyclic improved iterative process 3 5 relationship between source and sink according to the result of groundwater pollution source identification the relationship between the sources and sinks of lnapls is further studied because the result was a set of values the laws that could be obtained from them by using deterministic methods are limited based on the numerical simulation models of multiphase flow and biodegradation the monte carlo stochastic simulation was used to study the source sink relationship of lnapl pollutant take 80 to 120 of the identification result about the pollution source if it exceeds the prior range take the upper and lower limits of the prior range as the reasonable range it is assumed that the information on the pollution source is uniformly distributed within the reasonable range latin hypercube sampling was used to sample 600 groups of information on the pollution source in the reasonable range as the input samples of the stochastic simulation based on the numerical simulation models of multiphase flow and biodegradation the monte carlo simulation was used to calculate the allocation of pollutants at each destination multiple groups of results were obtained through the stochastic simulation through statistical analysis of multiple groups of results the probability distribution of allocation of pollutants at each destination and pollutant release is obtained and the uncertain impact of the random change of pollution source information on the distribution of pollutants at each destination is analyzed 4 results and discussion 4 1 comparison of resnet 18 models in different iterations to assess the performance of the surrogate model 30 testing samples were used to compute the degree of approximation of the resnet 18 surrogate model and the simulation model fig 8 shows the graph of fitting between the outputs of the simulation and those of the optimal resnet 18 surrogate model in different iterations the closer the scatter point was to the red slash the closer the outputs of the surrogate model were to those of the simulation model and the higher was the precision the blue scatter was closest to the line followed by the red and green scatters this shows that when the vdgs method was used to optimize the hyperparameters the performance of the resnet 18 surrogate model significantly improved the same conclusion can be drawn from table 5 which lists the coefficient of determination r2 and the rmse of the optimal surrogate model in different iterations the r2 of the optimal surrogate model in the first iteration was only 0 9200 and its rmse was 113 122 μg l while the r2 of the optimal surrogate model in the third iteration was 0 9788 and its rmse was 55 044 μg l therefore the optimal resnet 18 in the third iteration was considered to be accurate enough to perform the subsequent calculation moreover the introduction of the surrogate model significantly reduced the computational load in the inversion process the adaptive cyclic improved iterative process was carried out for ten rounds in each round the es algorithm needed about 200 updates on average to reach the convergence condition because the capacity of the ensemble was set to 600 the model needed to be called 600 times in each update to get the results therefore the model needed to be called about 1 200 000 times in the entire inversion process each run of the multi phase flow simulation and the biodegradation simulation using utchem took about 40 min of cpu time on a workstation with a dual processor and 64 gb of ram hence the inversion process required about 800 000 h 33 333 days such a long time is unacceptable however when the inversion process used a surrogate model instead of simulation models the process required only 10 min in addition the acquisition of training samples took about 220 h and hyperparameter optimization took 26 h the time cost of using the surrogate model was 246 h a large amount of time and computational load was thus saved 4 2 results of adaptive cyclic improved iterative process the introduction of the surrogate model significantly improved the efficiency of inversion but also introduced error to reduce it an adaptive cyclic improved iterative process was constructed the adaptive cyclic improved iterative process ended after ten rounds at this time the normalized euclidean distance between the ninth and 10th results of identification was 0 026 lower than the threshold value of 0 03 a comparison between the initial and the final results of recognition is shown in table 6 further the maximum relative error of the initial results reached 41 37 and after 10 rounds of adaptive cyclic improved iterative process was only 14 36 this shows that the adaptive cyclic improved iterative process improved the inversion based accuracy of identification fig 9 shows the convergence curve of the average relative error in the adaptive cyclic improved iterative process with increasing number of iterations the average relative error decreased continuously which shows that the results of recognition gradually improved fig 9 shows that the change in the curve was steep at the beginning which indicates that the effect of improvement in the recognition result was prominent and then began to gradually flatten which indicates that the iterative process began to converge in practice the recognition results cannot be verified directly because the true value is not known therefore this study substitutes the final results into the simulation model to obtain the calculated output value of concentration and evaluates the effect of the recognition results by calculating the similarity between the observed data and the simulated data the comparison results of observed data and simulated data are shown in fig 10 the closer the marker is to the line y x the closer the simulated data are to the observed data it can be seen from the figure that the observed data and simulated data are very close which shows that the recognition result is accurate in addition the method proposed by todaro et al 2021 was used to verify whether the results of our study are equifinality the test results denied the equifinality in the final recognition result 4 3 relationship between source and sink to study the source sink relationship of lnapl pollutant we set all parameters except those representing information on the pollution source to fixed values given changing pollution sources the changes in the allocation of pollutants at each destination were studied fig 11 shows the histogram of the frequency distributions of biodegradation aquifer pollutant residual quantity boundary pollutant outflow and aquifer pollutant release it is clear that the amount of biodegradation aquifer pollutant residual quantity and aquifer pollutant release obeyed the normal distribution and the boundary pollutant outflow did not to further verify this law the kolmogorov smirnov k s test was carried out by using the statistical product and service solutions spss software version 23 0 assuming that biodegradation aquifer pollutant residual quantity boundary pollutant outflow and aquifer pollutant release obeyed the normal distribution they had significance values of 0 07 0 246 0 and 0 164 respectively when the significance is higher than 0 05 the relevant hypothesis is considered true that is it obeys the normal distribution the results of the k s test illustrate the correctness of the observed law relevant statistical indicators are shown in table 7 because the mean values were different the ratio of the standard deviation to the mean value that is the variation coefficient was calculated to compare the degrees of dispersion of different variables and is shown in table 7 the variation coefficient of the boundary pollutant outflow was the highest indicating that the variation and the uncetainty of this variable were the largest to determine the distribution of the pollutants after entering the aquifer the ratios of biodegradation aquifer pollutant residual quantity boundary pollutant outflow to aquifer pollutant release are calculated and are plotted as histograms of frequency distribution in fig 12 it shows that the ratio of biodegradation to aquifer pollutant release was in the range of 1 25 to 1 50 the aquifer pollutant residual quantity was concentrated in the range of 97 50 to 99 00 and the boundary pollutant outflow was concentrated in 0 00 0 50 this shows that most of the pollutants remained in the aquifer some were decomposed by microorganisms and a small part flowed out at the boundary to explore the uncertain impact of the random change of pollution source information on the allocation of pollutants at each destination a scatter diagram was drawn and subjected to linear fitting analysis as shown in fig 13 the different rows represent the allocation of pollutants at each destination and aquifer pollutant release and the different columns represent related variables about pollution source information the first column shows the influence of the horizontal coordinate of the pollution source location the results show it have little impact on the aquifer pollutant residual quantity and aquifer pollutant release with a correlation coefficient r2 lower than 0 1 but had a significant impact on biodegradation and the boundary pollutant outflow with values of r2 of 0 5798 and 0 7777 respectively the relation between the horizontal coordinate of the pollution source location and the boundary pollutant outflow was interesting the former was in the range of 680 m to 740 m and the latter tended to be close to zero in the range of the horizontal coordinate from 740 m to 820 m as the value of the horizontal coordinate increased the boundary pollutant outflow increased when it was in the range from 680 m to 740 m the pollution source location was far from the upstream and downstream boundaries and thus a small amount flowed out when the horizontal coordinate of the pollution source location was in the range of 740 m to 820 m the pollution source location was close to the upstream boundary therefore more pollutants flowed out of the aquifer through dispersion at the upstream boundary the larger the horizontal coordinate of the pollution source location was the closer it was to the upstream boundary and the greater the quantity of pollutant that flowed out of the aquifer through dispersion therefore there was a positive correlation within this range as the horizontal coordinate increased so did the outflow of pollutants the amount of pollutants remaining in the aquifer decreased as did biodegradation therefore the horizontal coordinate of the pollution source location was negatively correlated with the amount of biodegradation the second column shows the influence of the longitudinal coordinate of the pollution source location it had little impact on the allocation of pollutants at each destination and aquifer pollutant release the third and fourth columns show the influence of the initial and the termination release times of the pollution source each had a certain impact on the allocation of pollutants at each destination and aquifer pollutant release except the boundary pollutant outflow this is because the initial and termination release times determined the continuous release time of pollutants when the initial release time was longer the time of continuous release was smaller and the total amount of pollutant discharged into the aquifer decreased thus the initial release time was negatively correlated with the allocation of pollutants at each destination and aquifer pollutant release however the longer the termination release time was the longer was the continuous release time and the greater was the total amount of pollutant discharged into the aquifer thus the termination release time was positively correlated with the allocation of pollutants at each destination and aquifer pollutant release the fifth column shows the influence of the release intensity of the pollution source on the allocation of pollutants at each destination and aquifer pollutant release the release intensity had a significant impact on the allocation of pollutants at each destination and aquifer pollutant release except the boundary pollutant outflow this is because the release intensity of the pollution source directly controlled the total amount of pollutant discharged into the aquifer therefore it was positively correlated with the allocation of pollutants at each destination and aquifer pollutant release 4 4 discussion the deep learning based surrogate model has received extensive attention in research on water resources li et al 2021 pan et al 2021 yu et al 2020 zhang et al 2020 compared with shallow learning deep learning has a stronger learning ability and can learn more complex relationships of non linear mapping we used resnet 18 the classical structure of a deep residual network to build surrogate model for simulation models of multi phase flow and biodegradation however the performance of the deep learning based surrogate model greatly depends on the choice of hyperparameters in previous studies hyperparameters are often adjusted manually mo et al 2019 where this is limited by the experience of the researcher therefore the grid search a hyperparameter optimization method is introduced and improved to obtain the vdgs method this yielded satisfactory results however many other strategies can be used for hyperparameter optimization such as random search bayesian optimization and evolutionary algorithm based optimization further research in the area can improve hyperparameter optimization method which will improve the performance of deep learning in research on water resources although surrogate models can save considerable time and reduce the computational load there is an inevitable gap between them and simulation models that may lead to inaccurate results of identification many studies have attempted to solve this problem one approach involves building an adaptive cyclic improved iterative process that combines the establishment of surrogate models with the solution of inverse problems zhang et al 2016 this is done by constantly adding the given results of recognition to the training data to update the surrogate model which is then used in the subsequent inversion process however the surrogate models used in the adaptive cyclic improved iterative process are based on shallow learning chang et al 2021a gong and duan 2017 zhang et al 2016 we combined the adaptive cyclic improved iterative process with the deep learning based surrogate model and applied it to the simulation model featuring complex non linear mapping relationship satisfactory results were achieved different deep learning based models and adaptive cyclic improved iterative process can be combined to improve their use in future the groundwater pollution source forms the source of groundwater pollutants while the sink of groundwater pollution is their destination past research has often separately considered the sources and sinks of pollution when seek to identify sources of groundwater pollution the impact of the pollution sink on pollutant transport has rarely been considered in the studies of pollution sinks little consideration is given to how information on the pollution sources is obtained and whether it is accurate however the transfer of pollutants from source to sink is a complete process if the two are studied separately the internal relationship between remains unclear we thus studied the sources and sinks of lnapls in groundwater at the same time here on the one hand the groundwater pollution source information is regarded as unknown and its identification is carried out on the other hand the biodegradation of lnapls during groundwater transport was considered based on the numerical simulation models of multiphase flow and biodegradation the monte carlo simulation was used to calculate the allocation of pollutants at each destination with changing pollution source information however it is assumed that the parameters about the pollution source information had a uniform distribution in the stochastic simulation in future research we can obtain their posterior distribution through the mcmc algorithm and then conduct stochastic simulations the results thus obtained will better reflect the practical situation 5 conclusions this study combined the resnet 18 surrogate model and the es algorithm to construct an adaptive cyclic improved iterative process and used it to simultaneously identify information on the sources of pollution and sensitive parameters of the model used this adaptive cyclic improved iterative process was used to gradually improve the training samples surrogate models and results of identification based on the identification results and the numerical simulation models of multiphase flow and biodegradation the relationship between sources and sinks of lnapls is studied by monte carlo stochastic simulation the vdgs method was used for hyperparameter optimization to improve the performance of the resnet 18 surrogate model these methods were applied to a site contaminated by lnapl where the effects of biodegradation were considered in the simulation of pollutant transport the conclusions are as follows 1 the resnet 18 architecture was used to construct the surrogate model of simulation models of multi phase flow and biodegradation this saved a considerable amount of time and reduced the computational burden caused by repeated calls of simulation models moreover the use of the vdgs to optimize the hyperparameters greatly improved the performance of the resnet 18 surrogate model 2 the results of identification were gradually enhanced by using the adaptive cyclic improved iterative process after 10 rounds of updates the average relative error in identifying the source of pollution ranged from 13 11 to 5 54 this shows that the adaptive cyclic improved iterative process improved the accuracy of pollution source identification results 3 through stochastic simulation it can be found that the amount of biodegradation aquifer pollutant residual quantity and aquifer pollutant release obeyed the normal distribution in case of changing sources of pollution moreover the correlation between the pollution source information and the allocation of pollutant at each destination can also be obtained credit authorship contribution statement zhenbo chang conceptualization methodology software writing original draft validation formal analysis wenxi lu conceptualization resources writing review editing supervision project administration zibo wang methodology software validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national key research and development program of china no 2018yfc1800405 and the national natural science foundation of china no 41972252 
3195,it is important to study the source sink relationship of pollution however research on the groundwater pollution sources and pollution sinks has often been conducted separately in the past which makes it difficult to study the source sink relationship of pollutants in this study the source and sink of pollution in a site contaminated by a light non aqueous phase liquid lnapl is considered to study the relationship between them the simulation model needs to be called repeatedly in the research process to save time and reduce the calculation load the resnet 18 network structure is first utilized to create a substitute of simulation model however the performance of resnet 18 surrogate model largely depends on the choice of hyperparameters the variable density grid search method is thus utilized to optimize the hyperparameters in addition the quality of training samples is an important factor to determine the accuracy of surrogate models and the accuracy of the surrogate model will affect the effect of groundwater pollution identification therefore the resnet 18 surrogate model and the ensemble smoother algorithm are combined to construct an adaptive cyclic improved iterative process through the adaptive cyclic improved iterative process the training samples surrogate model and identification results of variables to be solved can be improved and updated together with the iterative process to improve the solution effect of groundwater lnapls pollution source identification problem biodegradation is an important sink of lnapl pollutants therefore the biodegradation on the lnapl pollutants in groundwater is considered by adding a biodegradation model to the original model of multi phase flow finally based on the numerical simulation models of multiphase flow and biodegradation the monte carlo stochastic simulation are used to study the source sink relationship of lnapl pollutants a hypothetical case based on real conditions was used to illustrate the validity of the methods the results implied that the proposed methods can not only improve the effect of lnapl pollution identification but also study the source sink relationship of lnapl pollutant keywords lnapls contamination source sink relationship deep learning numerical simulation model of biodegradation adaptive cyclic improved iterative process data availability data will be made available on request 1 introduction organic petroleum pollutants are a major problem in the context of groundwater pollution owing to their low solubility after entering groundwater they mostly flow in a separate phase and are called non aqueous phase liquids napls light non aqueous phase liquids lnapls have a density lower than that of water and dense non aqueous phase liquids have a higher density than water napls are often characterized by high toxicity if not treated in time they can cause significant pollution of the groundwater aksoy 1985 lari et al 2019 tomlinson et al 2017 napl induced groundwater pollution can be reduced by accurately obtaining information on the sources of pollution research on groundwater contamination source identification gcsi is thus important there are some important reviews about gcsi atmadja and bagtzoglou 2001 barati moghaddam et al 2021 gómez hernández and xu 2022 research on gcsi began in the 1980s dokou and pinder 2009 gorelick et al 1983 and the methods in this field can be divided into methods of direct and indirect inversion atmadja and bagtzoglou 2001 methods of direct inversion refer to the deformation processing of the equations of groundwater flow and solute transport without using a forward model in the process of inversion to achieve a direct reverse solution mathematically it is a solution of inverse time and traceability methods of direct inversion include tikhonov regularization neupauer et al 2000 quasi reversibility bagtzoglou and atmadja 2003 adjoint state method michalak and kitanidis 2004 and marching jury backward beam equation bagtzoglou and atmadja 2003 the application conditions of direct inversion methods are relatively harsh they are generally used to study ideal examples and have significant limitations in application to complex practical problems the method of indirect inversion is opposite to that of direct inversion in that the forward model needs to be called multiple times in the inversion process this method originates from the trial and error method the core idea is that when solving an inverse problem an expert first gives a trial solution substitutes this solution into the forward model to obtain the calculated value and compares the calculated value with the observed data if the gap between them is within an acceptable range this trial solution is the solution of the inverse problem otherwise the expert generates a new trial point and repeats the above process until the solution is acceptable methods of indirect inversion can be further classified into two types simulation optimization and stochastic inversion methods hwang et al 2020 the aim of the former approach is to minimize the gap between observation values and outputs of the simulation model to obtain a group of optimal solutions particle swarm optimization zhao et al 2020 simulated annealing yeh et al 2007 and genetic algorithms ayvaz 2016 guo et al 2019 are optimization algorithms often used in gcsi however simulation optimization cannot consider uncertainty where this reduces the reliability of the results in the stochastic inverse approach the goal is to maximize the posterior probability of unknown variables with the help of observed data it can consider uncertainty to compensate for the deficiency of the simulation optimization approach chang et al 2021a the geostatistical approach butera and tanda 2003 generalized likelihood uncertainty estimation vrugt et al 2008 minimum relative entropy ulrych and woodbury 2003 and markov chain monte carlo yan et al 2019 zhang et al 2016 are examples compared with the direct inversion method the method of indirect inversion has greater potential for application and is more mature and flexible for solving practical problems in addition to the above methods the ensemble kalman filter enkf is a data assimilation method that has been widely used for gcsi problems in the last few years xu and jaime gomez hernandez 2018 it is a kind of stochastic inversion method that combines the kf method with the monte carlo sampling strategy sun et al 2020 the ensemble smoother es is an alternative that can reduce the burden of calculation imposed by the enkf xu et al 2020 it avoids the need for simulation restarts at each time step which renders the algorithm more efficient than the enkf emerick and reynolds 2013 the es here was adjusted with reference to the iterative ensemble smoother ju et al 2018 to expedite calculation the idea is to incorporate all the data at all times for several iterations and gradually update the unknown variables after each iteration until the convergence condition has been satisfied in past research groundwater pollution sources and sinks have often been separately considered research on the pollution sink mainly explains the fate and destination of pollutants in groundwater the relevant studies have focused on the physical chemical and biological effects of pollutants in groundwater chaudhary and singh 2022 singh et al 2021 zhai et al 2022 which are collectively referred to as the fate process of pollutants bedient et al 1999 in this study both the source and the sink of pollutants in an lnapl contaminated site were considered the information on groundwater pollution source due to lnapls was identified by the es algorithm moreover the effects of biodegradation were considered in the forward simulation of lnapls migration a monte carlo stochastic simulation was used to examine the effects of the pollution source on the allocation of pollutants at each destination in the aquifer this helps clarify the relationship between the source and sink of pollutants in groundwater the indirect method and the stochastic simulation involve a large number of calls to the simulation model which is time consuming the surrogate model is a good choice for alleviating the computational burden caused by this it can be classified into two types shallow and deep learning methods unlike shallow learning deep learning can simulate more complex mapping relationships and is widely used in groundwater research li et al 2021 pan et al 2021 however the performance of deep learning largely depends on the choice of hyperparameters jin et al 2021 li et al 2018 most of them find suitable hyperparameter by manually adjusting parameters yu et al 2020 zhang et al 2020 where this depends on the experience of the scientist therefore the variable density grid search vdgs method was utilized to optimize the hyperparameters of the resnet 18 network structure and apply this to replace the simulation models of multiphase flow and biodegradation to verify its effectiveness in addition the quality of training samples is an important factor to determine the accuracy of surrogate models in the past the training samples of the surrogate model are often generated at one time which makes the quality of the training samples difficult to be continuously improved with the iteration therefore it is difficult to establish a high precision surrogate model which will affect the solution accuracy of groundwater lnapls pollution source identification problem one approach to solve this problem is to build an adaptive cyclic improved iterative process that combines the surrogate model with the process of solution of inverse problems zhang et al 2016 the results of recognition are constantly added to the training data to update the surrogate model which is then used in the subsequent inversion however surrogate models used in the adaptive cyclic improved iterative process are shallow chang et al 2021a gong and duan 2017 zhang et al 2016 we thus combine a resnet 18 surrogate model a deep learning method and the es algorithm to construct an adaptive cyclic improved iterative process based on deep learning through the adaptive cyclic improved iterative process based on deep learning the training samples resnet 18 surrogate models and identification results of variables to be solved can be improved and updated together with the iterative process to improve the solution effect of groundwater lnapls pollution source identification problem the aim here is to explore the source sink relationship of the pollutant at an lnapl contaminated site the contributions of this paper are as follows the deep residual network method based on resnet 18 structure was applied to construct the surrogate model of multiphase flow and biodegradation numerical simulation models which effectively improved the approximation accuracy of the surrogate model to the simulation models moreover the variable density grid search method was also used to optimize the hyperparameters in the surrogate model which further improved the approximation accuracy of the surrogate model to the simulation model an adaptive cyclic improved iterative process based on deep learning is constructed to improve the quality of training samples and the solution effect of groundwater lnapls pollution source identification problem in this way the training samples resnet 18 surrogate models and identification results of variables to be solved can be improved and updated together with the iterative process based on the numerical simulation models of multiphase flow and biodegradation the stochastic simulation of groundwater pollution source sink relationship is studied by using monte carlo method 2 methodology 2 1 system model 2 1 1 multi phase flow model there are some of the relevant papers related to numerical simulations by different methods kumar et al 2020 rajput and singh 2021 according to the principle of mass conservation the laws of flow of different components can be summarized into an equation called the equation for the mass conservation chu and lu 2015 delshad et al 1996 wang et al 2021 1 ϕ c k ρ k t l 1 n p ρ k c kl v l ϕ s l k kl c kl r k f o r k 1 n c where k is the component index representing different components nc is the number of components l is the index of the phases and can include aqueous oil microemulsion phase and air phases np is the number of phases involved in the calculation ϕ is the effective porosity l 3 l 3 c k is the total concentration of the k th component l 3 l 3 which is expressed in the form of a volume fraction ρ k is the density of the k th component m l 3 t is the simulation time t c kl is the concentration of the k th component in the l th phase l 3 l 3 which is expressed in the form of a volume fraction v l is the velocity of fluid in the l th phase l t 1 which can be calculated by darcy s law s l is the saturation of the l th phase l 3 l 3 k kl is the tensor of the dispersion coefficient for the k th component in the l th phase l 2 t 1 and r k is the overall source or sink term for the k th component m l 3 t 1 2 1 2 biodegradation model biodegradation is an important process in sites contaminated with lnapl if it is not considered the law of lnapls migration cannot be accurately described the fundamental architecture of equations of the model for a single substrate electron acceptor and biological species are as follows hosseininoosheri et al 2016 2 ds dt β κ x m c s s μ max x y s k s s a k a a k abio s 3 d s dt β κ v c s s μ max ρ x y s k s s a k a a k abio s 4 da dt β κ x m c a a μ max x e y s k s s a k a a 5 d a dt β κ v c a a μ max ρ x e y s k s s a k a a 6 dx dt μ max x s k s s a k a a b x 7 d x dt μ max x s k s s a k a a b x where s represents the concentration of the substrate in the water m l 3 β represents the surface area of a single microcolony l 2 x represents the concentration of the attached biomass m l 3 m c represents the cell mass of a single microcolony m c ρ x v c m κ represents the coefficient of mass transfer l t 1 s represents the concentration of the substrate in the attached biomass m l 3 μ max represents the maximum specific growth rate t 1 y represents the yield coefficient which denotes the cell mass produced per substrate mass biodegraded m m 1 x represents the concentration of biomass in the water m l 3 k s represents the half saturation coefficient of the substrate m l 3 a represents the concentration of electron acceptors in the water m l 3 k a represents their half saturation coefficient m l 3 v c represents the volume of a single microcolony l 3 k abio represents the coefficient of the first order reaction rate t 1 ρ x represents the density of the biomass m l 3 a represents the concentration of the electron acceptor in the attached biomass m l 3 e represents the mass of the electron acceptors consumed per substrate of mass biodegraded m m 1 and b represents the endogenous decay coefficient t 1 the combined multi phase flow and biodegradation model is solved by utchem chu and lu 2015 hou and lu 2018 through operator splitting where the solution of the multiphase flow equation is used as an initial condition for the biodegradation equation in each calculated time step operator splitting is appealing from the view point of the calculation efficiency valocchi and malmstead 1992 2 2 resnet surrogate system 2 2 1 model architecture for the general surrogate model of deep learning the deeper it is the stronger its capability to fit a non linear expression is and theoretically the higher is the accuracy of fitting however in practice the deeper the neural network is the more difficult it is to train and the more easily does the gradient vanish resulting in a reduction in overall accuracy the residual network resnet proposed by he et al 2016 introduces a residual block to solve this problem the core of the resnet is the design of the residual block we use two residual block structures denoted by res block 1 and res block 2 as shown in fig 1 x denotes the input to the network h x is the target function and g x is the function of the shortcut path instead of directly learning h x in the residual module we learn h x x or h x g x and record it as the function f x in this way the output of the neural network consists of two parts the residual mapping f x and the direct mapping x or g x the introduction of the residual block greatly alleviates the vanishing gradient problem due to an increase in the number of the neural network layers the residual mapping can be expressed as follows he et al 2016 8 y l x l f x l ω l 9 x l 1 f y l where x l is the input to the l t h residual block ω l is its weight and f is the residual function in general f is set to a relu function for the sake of discussion the function f is set as an identity map x l 1 y l the relationship between the l t h residual block of a deeper layer and the l t h residual block can be expressed as 10 x l x l i l l 1 f x i ω i the above equation shows that the input signal can be propagated directly from the low to the high layer during forward propagation because it contains a natural identity map in this way the problem of network degradation can be solved according to the chain rule of derivatives the gradient of the loss function ε with respect to x l can be expressed as 11 ε x l ε x l x l x l ε x l 1 x l i l l 1 f x i ω i the above formula reflects two properties of the residual network in the training process x l i l l 1 f x i ω i cannot always be 1 that is the gradient does not disappear in the residual network ε x l indicates that the gradient of the l t h residual block can be directly transferred to any layer shallower than it the classical deep resnet structures are resnet 18 resnet 34 resnet 50 resnet 101 and resnet 152 we use resnet 18 as model architecture here it has 18 wt layers starting with a convolution layer followed by eight residual blocks and finally a fully connected layer the main architecture of resnet 18 is displayed in fig 2 2 2 2 hyperparameter optimization some parameters of the deep learning surrogate model need to be set before the learning process rather than obtained through training they are called hyperparameters examples include the learning rate number of layers batch size number of neurons and regularization coefficient of each layer the selection of the hyperparameters greatly affects the efficiency of training and the performance of the surrogate model for deep learning hyperparameter optimization is the process of finding the optimal combination of hyperparameter examples of methods to this end include manual adjustment grid search random search bayesian optimization and evolutionary algorithm based optimization each method has its own advantages and disadvantages we use the grid search and improve it to obtain the vdgs let z be a set of n hyperparameters z z 1 z 2 z n z z the score function s c o r e z is used to evaluate the performance of the model the aim of hyperparameter optimization is to find the set of hyperparameter z that yields the best performance 12 z arg max z z s c o r e z 13 s c o r e z j 1 d scor e j z where scor e j z is the score obtained according to the j th accuracy evaluation index when the set of hyperparameter is set to z d is the number of indicators participating in accuracy evaluation steps of the vdgs method are as follows 1 according to the model architecture determine the hyperparameters to be optimized z z 1 z 2 z n 2 select appropriate candidate values for each hyperparameter where the candidate value ensemble of the i t h hyperparameter is z i z i valu e i 1 v a l u e i 2 v a l u e i n i 3 traverse all hyperparameter combinations for grid search 4 select the hyperparameter combination with the highest scoring function s c o r e z and determine whether the model meets the accuracy related requirements if so get the combination of hyperparameters z with the highest score output the model and terminate iterative process if not reduce the scale and judge whether the scale reaches the minimum limit if so use the minimum of scale and the combination of hyperparameters with the highest score to update the candidate value ensemble z i if not conduct fine search according to the transformed scale and the combination of hyperparameters with the highest score and update the candidate value ensemble z i then continue to repeat step 3 the flow chart to present applied vdgs method was shown in fig 3 in this paper resnet 18 surrogate model and the vdgs method were realized on matlab platform 2 3 fast es algorithm both the es and enkf are stochastic methods based on the kalman filter evensen and van leeuwen 2000 these methods were initially used for data assimilation and in recent years have been introduced to research on water resources for parameter estimation and identifying the sources of pollution dokou and pinder 2011 li et al 2019 in the inversion process these methods are based on the cross correlation between model parameters and states and use the observed values to update the model parameters and states this updating process does not require calling the simulation model of the system frequently the cost of this method is thus generally lower than other algorithms chang et al 2021b however both the es and enkf update unknown variables successively with the observed data at different times to speed up the algorithm the es here was adjusted with reference to the iterative ensemble smoother ju et al 2018 directly use the observational data at all times and determine whether convergence is achieved through the conditions for iterative convergence the operational process of the adjusted ensemble smoother is as follows generate the initial ensemble x 0 x 1 0 x ne 0 according to the prior distribution which includes ne unknown variable vectors then update each unknown variable vector in the ensemble x j 1 x 1 j 1 x ne j 1 using the observed data vector y 14 x k j x k j 1 c xy c yy r 1 y ε k f x k j 1 k 1 n e 15 c xy 1 ne k 1 ne x k j 1 x j 1 f x k j 1 f j 1 t 16 c yy 1 ne k 1 ne f x k j 1 f j 1 f x k j 1 f j 1 t where x k j is the k t h unknown variable vector after the j t h iteration c xy is the mutual covariance matrix of the ensemble in the j 1 t h iteration x j 1 and the corresponding output ensemble y j 1 f x 1 j 1 f x ne j 1 c yy is the self covariance matrix of y j 1 r is the observed error covariance matrix f x k j 1 is the output of the simulation model surrogate model with parameters x k j 1 ε k represents random samples of the observed error f j 1 represents the mean vector of the output of the model in the j 1 t h iteration and x j 1 is the mean unknown variable vector in the j 1 t h iteration the update steps are repeated until one of the following conditions is satisfied 1 s x j 1 s x j s x j ξ 2 the given number of iterations times j is larger than the maximum permitted number of iterations i max in the above ξ is a predefined parameter set on the given requirement and s x j is the degree of data fitting expressed as follows 17 s x j k 1 ne y k f x k j r 1 y k f x k j t 2 4 adaptive cyclic improved iterative process to gradually improve the accuracy of approximation of the model in the region of interest the adaptive cyclic improved iterative process for gcsi problems is constructed and the results of identification are updated using it hou et al 2021 ju et al 2018 we propose a novel adaptive cyclic improved iterative process by combining the deep residual network with the fast es algorithm information on the sources of pollution and sensitive parameters of the model are identified by using the resnet 18 surrogate model and the es algorithm this is then used to calculate the standardized euclidean distance between each sample point in the training sample and the given results of identification the results of identification are substituted into the simulation model to obtain a set of input output samples new samples are added to the original training samples the sample that is furthest from the given result of recognition is eliminated and the training samples are updated the euclidean distance can be used to measure the distance between points however the scale of each dimensional component in the data is often different the normalized euclidean distance has been proposed to solve this problem the normalized euclidean distance between a x 11 x 12 x 1 n and b x 21 x 22 x 2 n can be expressed as follows 18 d i 1 n x 1 i x 2 i 2 s i 2 where s i 2 is the variance of the i t h dimension by reconstructing the resnet 18 surrogate model by using updated training samples the precision of the surrogate model in the vicinity of the results of recognition can be enhanced the updated surrogate model and es algorithm are used for inversion recognition to obtain the new results of identification this process is repeated until the convergence condition is satisfied the updated training samples can have a greater coverage near the results of identification such that the surrogate model has a sufficiently high accuracy of approximation in their vicinity through this adaptive cyclic improved iterative process the training samples can be updated gradually the quality of the training data and the performance of the surrogate model can be improved therefore the groundwater contamination identification results can be modified and updated gradually the flow chart to present the adaptive cyclic improved iterative process was shown in fig 4 3 case study 3 1 problem description we used a site contaminated by lnapls as the study area the groundwater in a pore confined aquifer in the study area had been polluted by benzene which is an lnapl the groundwater flowed from northeast to southwest its hydraulic gradient was small about 2 and the aquifer was 10 m thick the boundary conditions of the study area are shown in fig 5 there were 11 observation wells and a pollution source in this aquifer the location of the wells and the potential range of the sources of pollution are shown in fig 6 the simulation time was 7 300 days and data on pollutant concentrations were collected on days 6 720 7 080 and 7 230 the observed data in table 1 was used to identify the unknown variables the pollution due to lnapls in this area might have come from an upstream factory after entering the aquifer over a long period of time lnapls underwent natural dissolution and dispersion the effect of biodegradation was considered when simulating the law of transport of lnapls the variables to be identified during inversion included 1 the initial and termination release times of the pollutant 2 the location of the pollution source including longitudinal and horizontal coordinates 3 the intensity of release and 4 the model parameters selected based on sensitivity analysis the non sensitive parameters were fixed as constants the parameters of the multi phase flow model of the site are listed in table 2 and those of the biodegradation model are shown in table 3 3 2 sensitivity analysis of model parameters sensitivity analysis can be used to determine the influence of parameters on the output of simulation models methods for both local and global sensitivity analysis are commonly used for this we used local sensitivity analysis to examine the impact of parameters of the simulation model on the output the aim was to screen the parameters that had a great impact on the output of the simulation model as the variables to be identified and to set the other parameters as constants the sensitivity of each parameter was calculated using the following formula 19 s i f x i f x i x i f x i f x i x i x i where s i is the sensitivity of the i t h parameter of the simulation model f is the output of the simulation model x i is the i t h parameter of the simulation model and δ x i is the change in x i when the rate of change of the parameters to be analyzed is the same the greater s i is the greater is the sensitivity of the parameters this shows the parameters that need to be analyzed we conducted a sensitivity analysis of the following 11 parameters permeability k porosity ϕ longitudinal aqueous dispersivity α wl transverse aqueous dispersivity α wt longitudinal dispersivity of the oil phase α ol transverse dispersivity of the oil phase α ot the biomass density ρ x colony radius r c colony thickness τ number of cells per microcolony n cell and number of cells per mass of soil c c the results indicate that the four most sensitive parameters were permeability porosity colony radius and transverse aqueous dispersivity fig 7 we thus set the four sensitive model parameters and pollution source information horizontal coordinates and longitudinal coordinates of pollution source location initial release time of pollutants termination release time of pollutants and release intensity of pollution sources as the variables to be identified in the inversion process 3 3 resnet surrogate model we used resnet 18 to establish the surrogate model the steps for constructing the model are as follows a use the latin hypercube sampling approach to sample in the feasible region of the variables to be identified these were the initial release time and termination release time of the pollutants the pollution source location including longitudinal and horizontal coordinates the release intensity permeability porosity colony radius and transverse aqueous dispersivity a total of 330 groups of variables to be identified were extracted according to their prior distributions as shown in table 4 and input to the simulation model the models were run to obtain their concentrations at observation wells corresponding to 330 groups of inputs b two important hyperparameters of resnet 18 the learning rate and batch size were optimized by the vdgs method the appropriate candidate values for the two hyperparameters were determined and combined the resnet 18 model with different combinations of hyperparameter was trained by using the 300 groups of input and output samples and the accuracy of the model was evaluated by using the scoring function the best combination of hyperparameters was checked to determine whether it met the accuracy requirements if it did the corresponding model was output if the accuracy requirements were not satisfied the candidate values of the hyperparameters were determined again according to their given optimal combination and the above steps were repeated c the remaining 30 groups of samples were used to validate the accuracy of the trained resnet 18 surrogate model they were input to the trained model and the outputs of the surrogate model were compared with those of the simulation model to calculate the root mean squared error rmse and deterministic coefficient r2 they were used in turn to assess the approximation of the resnet 18 model and the simulation model 3 4 adaptive cyclic improved iterative process the trained resnet 18 surrogate model and fast es algorithm were used to simultaneously identify information on the sources of pollution and the sensitive parameters ne ξ and i max were set to 600 0 001 and 1000 respectively the results of identification were substituted into the simulation model to obtain an input output sample and the new sample were added to the original 300 training samples the sample with the longest distance from the identification result was eliminated thus the number of updated training samples was still 300 the updated training samples were then used to rebuild the resnet 18 surrogate model for the inverse process this process was repeated until the convergence condition had been satisfied the convergence condition was that the normalized euclidean distance between the results of recognition in two adjacent iterations of the process must be less than 0 03 the training samples surrogate model and identification result were gradually improved through this adaptive cyclic improved iterative process this process was applied to identify the pollution source in a hypothetical example based on real conditions to analyze and illustrate the effectiveness of the adaptive cyclic improved iterative process 3 5 relationship between source and sink according to the result of groundwater pollution source identification the relationship between the sources and sinks of lnapls is further studied because the result was a set of values the laws that could be obtained from them by using deterministic methods are limited based on the numerical simulation models of multiphase flow and biodegradation the monte carlo stochastic simulation was used to study the source sink relationship of lnapl pollutant take 80 to 120 of the identification result about the pollution source if it exceeds the prior range take the upper and lower limits of the prior range as the reasonable range it is assumed that the information on the pollution source is uniformly distributed within the reasonable range latin hypercube sampling was used to sample 600 groups of information on the pollution source in the reasonable range as the input samples of the stochastic simulation based on the numerical simulation models of multiphase flow and biodegradation the monte carlo simulation was used to calculate the allocation of pollutants at each destination multiple groups of results were obtained through the stochastic simulation through statistical analysis of multiple groups of results the probability distribution of allocation of pollutants at each destination and pollutant release is obtained and the uncertain impact of the random change of pollution source information on the distribution of pollutants at each destination is analyzed 4 results and discussion 4 1 comparison of resnet 18 models in different iterations to assess the performance of the surrogate model 30 testing samples were used to compute the degree of approximation of the resnet 18 surrogate model and the simulation model fig 8 shows the graph of fitting between the outputs of the simulation and those of the optimal resnet 18 surrogate model in different iterations the closer the scatter point was to the red slash the closer the outputs of the surrogate model were to those of the simulation model and the higher was the precision the blue scatter was closest to the line followed by the red and green scatters this shows that when the vdgs method was used to optimize the hyperparameters the performance of the resnet 18 surrogate model significantly improved the same conclusion can be drawn from table 5 which lists the coefficient of determination r2 and the rmse of the optimal surrogate model in different iterations the r2 of the optimal surrogate model in the first iteration was only 0 9200 and its rmse was 113 122 μg l while the r2 of the optimal surrogate model in the third iteration was 0 9788 and its rmse was 55 044 μg l therefore the optimal resnet 18 in the third iteration was considered to be accurate enough to perform the subsequent calculation moreover the introduction of the surrogate model significantly reduced the computational load in the inversion process the adaptive cyclic improved iterative process was carried out for ten rounds in each round the es algorithm needed about 200 updates on average to reach the convergence condition because the capacity of the ensemble was set to 600 the model needed to be called 600 times in each update to get the results therefore the model needed to be called about 1 200 000 times in the entire inversion process each run of the multi phase flow simulation and the biodegradation simulation using utchem took about 40 min of cpu time on a workstation with a dual processor and 64 gb of ram hence the inversion process required about 800 000 h 33 333 days such a long time is unacceptable however when the inversion process used a surrogate model instead of simulation models the process required only 10 min in addition the acquisition of training samples took about 220 h and hyperparameter optimization took 26 h the time cost of using the surrogate model was 246 h a large amount of time and computational load was thus saved 4 2 results of adaptive cyclic improved iterative process the introduction of the surrogate model significantly improved the efficiency of inversion but also introduced error to reduce it an adaptive cyclic improved iterative process was constructed the adaptive cyclic improved iterative process ended after ten rounds at this time the normalized euclidean distance between the ninth and 10th results of identification was 0 026 lower than the threshold value of 0 03 a comparison between the initial and the final results of recognition is shown in table 6 further the maximum relative error of the initial results reached 41 37 and after 10 rounds of adaptive cyclic improved iterative process was only 14 36 this shows that the adaptive cyclic improved iterative process improved the inversion based accuracy of identification fig 9 shows the convergence curve of the average relative error in the adaptive cyclic improved iterative process with increasing number of iterations the average relative error decreased continuously which shows that the results of recognition gradually improved fig 9 shows that the change in the curve was steep at the beginning which indicates that the effect of improvement in the recognition result was prominent and then began to gradually flatten which indicates that the iterative process began to converge in practice the recognition results cannot be verified directly because the true value is not known therefore this study substitutes the final results into the simulation model to obtain the calculated output value of concentration and evaluates the effect of the recognition results by calculating the similarity between the observed data and the simulated data the comparison results of observed data and simulated data are shown in fig 10 the closer the marker is to the line y x the closer the simulated data are to the observed data it can be seen from the figure that the observed data and simulated data are very close which shows that the recognition result is accurate in addition the method proposed by todaro et al 2021 was used to verify whether the results of our study are equifinality the test results denied the equifinality in the final recognition result 4 3 relationship between source and sink to study the source sink relationship of lnapl pollutant we set all parameters except those representing information on the pollution source to fixed values given changing pollution sources the changes in the allocation of pollutants at each destination were studied fig 11 shows the histogram of the frequency distributions of biodegradation aquifer pollutant residual quantity boundary pollutant outflow and aquifer pollutant release it is clear that the amount of biodegradation aquifer pollutant residual quantity and aquifer pollutant release obeyed the normal distribution and the boundary pollutant outflow did not to further verify this law the kolmogorov smirnov k s test was carried out by using the statistical product and service solutions spss software version 23 0 assuming that biodegradation aquifer pollutant residual quantity boundary pollutant outflow and aquifer pollutant release obeyed the normal distribution they had significance values of 0 07 0 246 0 and 0 164 respectively when the significance is higher than 0 05 the relevant hypothesis is considered true that is it obeys the normal distribution the results of the k s test illustrate the correctness of the observed law relevant statistical indicators are shown in table 7 because the mean values were different the ratio of the standard deviation to the mean value that is the variation coefficient was calculated to compare the degrees of dispersion of different variables and is shown in table 7 the variation coefficient of the boundary pollutant outflow was the highest indicating that the variation and the uncetainty of this variable were the largest to determine the distribution of the pollutants after entering the aquifer the ratios of biodegradation aquifer pollutant residual quantity boundary pollutant outflow to aquifer pollutant release are calculated and are plotted as histograms of frequency distribution in fig 12 it shows that the ratio of biodegradation to aquifer pollutant release was in the range of 1 25 to 1 50 the aquifer pollutant residual quantity was concentrated in the range of 97 50 to 99 00 and the boundary pollutant outflow was concentrated in 0 00 0 50 this shows that most of the pollutants remained in the aquifer some were decomposed by microorganisms and a small part flowed out at the boundary to explore the uncertain impact of the random change of pollution source information on the allocation of pollutants at each destination a scatter diagram was drawn and subjected to linear fitting analysis as shown in fig 13 the different rows represent the allocation of pollutants at each destination and aquifer pollutant release and the different columns represent related variables about pollution source information the first column shows the influence of the horizontal coordinate of the pollution source location the results show it have little impact on the aquifer pollutant residual quantity and aquifer pollutant release with a correlation coefficient r2 lower than 0 1 but had a significant impact on biodegradation and the boundary pollutant outflow with values of r2 of 0 5798 and 0 7777 respectively the relation between the horizontal coordinate of the pollution source location and the boundary pollutant outflow was interesting the former was in the range of 680 m to 740 m and the latter tended to be close to zero in the range of the horizontal coordinate from 740 m to 820 m as the value of the horizontal coordinate increased the boundary pollutant outflow increased when it was in the range from 680 m to 740 m the pollution source location was far from the upstream and downstream boundaries and thus a small amount flowed out when the horizontal coordinate of the pollution source location was in the range of 740 m to 820 m the pollution source location was close to the upstream boundary therefore more pollutants flowed out of the aquifer through dispersion at the upstream boundary the larger the horizontal coordinate of the pollution source location was the closer it was to the upstream boundary and the greater the quantity of pollutant that flowed out of the aquifer through dispersion therefore there was a positive correlation within this range as the horizontal coordinate increased so did the outflow of pollutants the amount of pollutants remaining in the aquifer decreased as did biodegradation therefore the horizontal coordinate of the pollution source location was negatively correlated with the amount of biodegradation the second column shows the influence of the longitudinal coordinate of the pollution source location it had little impact on the allocation of pollutants at each destination and aquifer pollutant release the third and fourth columns show the influence of the initial and the termination release times of the pollution source each had a certain impact on the allocation of pollutants at each destination and aquifer pollutant release except the boundary pollutant outflow this is because the initial and termination release times determined the continuous release time of pollutants when the initial release time was longer the time of continuous release was smaller and the total amount of pollutant discharged into the aquifer decreased thus the initial release time was negatively correlated with the allocation of pollutants at each destination and aquifer pollutant release however the longer the termination release time was the longer was the continuous release time and the greater was the total amount of pollutant discharged into the aquifer thus the termination release time was positively correlated with the allocation of pollutants at each destination and aquifer pollutant release the fifth column shows the influence of the release intensity of the pollution source on the allocation of pollutants at each destination and aquifer pollutant release the release intensity had a significant impact on the allocation of pollutants at each destination and aquifer pollutant release except the boundary pollutant outflow this is because the release intensity of the pollution source directly controlled the total amount of pollutant discharged into the aquifer therefore it was positively correlated with the allocation of pollutants at each destination and aquifer pollutant release 4 4 discussion the deep learning based surrogate model has received extensive attention in research on water resources li et al 2021 pan et al 2021 yu et al 2020 zhang et al 2020 compared with shallow learning deep learning has a stronger learning ability and can learn more complex relationships of non linear mapping we used resnet 18 the classical structure of a deep residual network to build surrogate model for simulation models of multi phase flow and biodegradation however the performance of the deep learning based surrogate model greatly depends on the choice of hyperparameters in previous studies hyperparameters are often adjusted manually mo et al 2019 where this is limited by the experience of the researcher therefore the grid search a hyperparameter optimization method is introduced and improved to obtain the vdgs method this yielded satisfactory results however many other strategies can be used for hyperparameter optimization such as random search bayesian optimization and evolutionary algorithm based optimization further research in the area can improve hyperparameter optimization method which will improve the performance of deep learning in research on water resources although surrogate models can save considerable time and reduce the computational load there is an inevitable gap between them and simulation models that may lead to inaccurate results of identification many studies have attempted to solve this problem one approach involves building an adaptive cyclic improved iterative process that combines the establishment of surrogate models with the solution of inverse problems zhang et al 2016 this is done by constantly adding the given results of recognition to the training data to update the surrogate model which is then used in the subsequent inversion process however the surrogate models used in the adaptive cyclic improved iterative process are based on shallow learning chang et al 2021a gong and duan 2017 zhang et al 2016 we combined the adaptive cyclic improved iterative process with the deep learning based surrogate model and applied it to the simulation model featuring complex non linear mapping relationship satisfactory results were achieved different deep learning based models and adaptive cyclic improved iterative process can be combined to improve their use in future the groundwater pollution source forms the source of groundwater pollutants while the sink of groundwater pollution is their destination past research has often separately considered the sources and sinks of pollution when seek to identify sources of groundwater pollution the impact of the pollution sink on pollutant transport has rarely been considered in the studies of pollution sinks little consideration is given to how information on the pollution sources is obtained and whether it is accurate however the transfer of pollutants from source to sink is a complete process if the two are studied separately the internal relationship between remains unclear we thus studied the sources and sinks of lnapls in groundwater at the same time here on the one hand the groundwater pollution source information is regarded as unknown and its identification is carried out on the other hand the biodegradation of lnapls during groundwater transport was considered based on the numerical simulation models of multiphase flow and biodegradation the monte carlo simulation was used to calculate the allocation of pollutants at each destination with changing pollution source information however it is assumed that the parameters about the pollution source information had a uniform distribution in the stochastic simulation in future research we can obtain their posterior distribution through the mcmc algorithm and then conduct stochastic simulations the results thus obtained will better reflect the practical situation 5 conclusions this study combined the resnet 18 surrogate model and the es algorithm to construct an adaptive cyclic improved iterative process and used it to simultaneously identify information on the sources of pollution and sensitive parameters of the model used this adaptive cyclic improved iterative process was used to gradually improve the training samples surrogate models and results of identification based on the identification results and the numerical simulation models of multiphase flow and biodegradation the relationship between sources and sinks of lnapls is studied by monte carlo stochastic simulation the vdgs method was used for hyperparameter optimization to improve the performance of the resnet 18 surrogate model these methods were applied to a site contaminated by lnapl where the effects of biodegradation were considered in the simulation of pollutant transport the conclusions are as follows 1 the resnet 18 architecture was used to construct the surrogate model of simulation models of multi phase flow and biodegradation this saved a considerable amount of time and reduced the computational burden caused by repeated calls of simulation models moreover the use of the vdgs to optimize the hyperparameters greatly improved the performance of the resnet 18 surrogate model 2 the results of identification were gradually enhanced by using the adaptive cyclic improved iterative process after 10 rounds of updates the average relative error in identifying the source of pollution ranged from 13 11 to 5 54 this shows that the adaptive cyclic improved iterative process improved the accuracy of pollution source identification results 3 through stochastic simulation it can be found that the amount of biodegradation aquifer pollutant residual quantity and aquifer pollutant release obeyed the normal distribution in case of changing sources of pollution moreover the correlation between the pollution source information and the allocation of pollutant at each destination can also be obtained credit authorship contribution statement zhenbo chang conceptualization methodology software writing original draft validation formal analysis wenxi lu conceptualization resources writing review editing supervision project administration zibo wang methodology software validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the national key research and development program of china no 2018yfc1800405 and the national natural science foundation of china no 41972252 
3196,artificial neural network ann models have been used for hydrological and water resources modelling for several decades where their calibration training has received much attention the aim of automated calibration processes is to identify values of the model parameters that minimize an error measure between model outputs and corresponding measured values the degree of difficulty associated with this process is a function of the nature of the relationship between changes in values of model parameters and corresponding changes in the error measure i e the features of the error surface and the automated method used to identify the combination of model parameters that minimizes the error measure i e the optimization method used to find the global optimum in the error surface while the impact of different optimization methods has been studied extensively how the features of the error surface change for different model structures has not in this paper it is demonstrated that exploratory landscape analysis ela metrics can be used successfully to characterize the features of the error surfaces of ann models thereby helping to explain the reasons for an increase or decrease in calibration difficulty and in doing so shedding new light on findings in existing literature results show that the error surfaces of single layer multi layer perceptrons mlps with fewer hidden nodes have a more well defined overall shape and have fewer local optima while the error surfaces of single layer mlps with a larger numbers of hidden nodes are flatter and have many distributed deep local optima consequently single layer mlps with a smaller number of hidden nodes can be calibrated successfully using gradient based methods such as the back propagation algorithm whereas single layer mlps with a relatively large number of hidden nodes are best calibrated using hybrid approaches that go beyond basic gradient based methods for example combination of global based evolutionary algorithms coupled with a local search keywords artificial neural networks anns multi layer perceptrons mlps calibration training optimization fitness landscape error surface exploratory landscape analysis ela data availability data will be made available on request 1 introduction artificial neural networks anns have been used extensively for hydrological modelling over the last several decades including water quality forecasting maier and dandy 1999 lischeid 2001 bowden et al 2005 cigizoglu and kisi 2006 kisi 2010 bayram et al 2012 lafdani et al 2013 olyaie et al 2015 zounemat kermani et al 2016 amamra et al 2018 meral et al 2018 banadkooki et al 2020 kim et al 2021 water quantity forecasting sajikumar and thandaveswara 1999 zealand et al 1999 gautam et al 2000 kim and barros 2001 sivakumar et al 2002 zhang and govindaraju 2003 rajurkar et al 2004 coulibaly and baldwin 2005 kingston et al 2005 wang et al 2006 yu and liong 2007 chua et al 2008 wu et al 2009 adamowski and sun 2010 khatibi et al 2011 jothiprakash and magar 2012 piotrowski and napiorkowski 2013 he et al 2015 humphrey et al 2016 tan et al 2018 fathian et al 2019 cheng et al 2020 water level forecasting see and openshaw 1999 phien and kha 2003 pereira filho and dos santos 2006 chau 2006 2007 leahy et al 2008 tiwari and chatterjee 2010 hajji et al 2012 pan et al 2013 nourani and mousavi 2016 mukherjee and ramachandran 2018 kurian et al 2020 xie et al 2021 evaporation modelling kişi 2006 2013 cobaner 2011 chaudhari et al 2012 kişi and tombul 2013 feng et al 2018 ferreira et al 2019 maroufpoor et al 2020 nourani et al 2020a nourani et al 2020b the prediction of soil properties elshorbagy and parasuraman 2008 parchami araghi et al 2013 trenouth and gharabaghi 2015 zanetti et al 2015 zhuo et al 2016 rahmati 2017 patrignani and ochsner 2018 li et al 2020 jian et al 2021 water temperature forecasting sahoo et al 2009 sabouri et al 2013 sabouri et al 2016 cole et al 2014 deweber and wagner 2014 graf et al 2019 and some other applications xu et al 2017 nourani et al 2017 zubaidi et al 2018 nguyen ky et al 2018 rezaali et al 2021 as with all models calibration is a critical component of the development of ann models termed training in the ann literature however for ann models calibration takes on additional importance firstly compared with process driven models see mount et al 2016 the structure of ann models is generally unknown and is often determined via a trial and error process models with different structures are calibrated and the model structure that performs best on the calibration data or test data if cross validation is used is selected maier et al 2010 wu et al 2014 consequently the selected model structure is a function of the success of the calibration process in that if the calibration does not identify the combination of model parameter values that corresponds to the lowest error for a given model structure the conclusions about which model structure is most appropriate can be incorrect secondly given the black box nature of anns the only way to extract meaningful information about the system being modelled is via analysis of the calibrated model structure and parameters dimopoulos et al 1995 lek et al 1996 maier and dandy 1997 kingston et al 2006 mount et al 2013 humphrey et al 2017 consequently the model information obtained is a function of the success of the calibration process and for reasons outlined above a poorly performing calibration process can lead to inaccurate information given the importance of the calibration of ann models it is not surprising that this issue has received significant attention in literature an area of particular focus has been the comparison of optimization algorithms for calibrating ann models kingston et al 2005 wu et al 2013 with many studies reaching contradictory conclusions about which optimization approaches perform better while providing limited insight into why this might be the case for example piotrowski and napiokowski 2011 compared the performance of differential evolutionary algorithm de particle swarm optimization pso and differential evolution with that of global and local neighborhoods degl and levenberg marquardt lm algorithms for calibrating ann models where they found that de had very poor performance and lm a second order gradient algorithm outperformed all metaheuristics in contrast maroufpoor et al 2020 found that lm often prematurely converged to local optima and grey wolf optimization gwo a metaheuristic could obtain better solutions than lm second order gradient algorithms were also found to perform worse than first order gradient methods by maier and dandy 1999 in order to better understand potential reasons for the contrasting findings in the studies outlined above it is important to recognize that the success of a particular calibration approach is not only a function which algorithm is used but also a function of the difficulty of the calibration problem itself maier et al 2014 calibration problems can be represented geometrically by the error surface otherwise known as the error function fitness function fitness landscape or response surface which consists of the relationship between different values of model parameters and the corresponding calibration errors maier et al 2019 as the purpose of calibration is to identify the set of model parameters that result in the smallest calibration error akin to finding the lowest point in the error function calibration difficulty is a function of the features of the error surface for example if the error surface is smooth with a single well defined minimum this optimal point is relatively easy to find conversely if the error surface is rough that is it possesses many minima of similar or equal value i e local minima the overall global minimum is difficult to find guillaume et al 2019 similarly the presence of flat regions or plateaus in the error function generally make it less computationally efficient to guide the search towards regions with lower error given that many ann models contain a relatively large number of parameters that need to be calibrated it is difficult to visualize their error surfaces kingston et al 2005 an alternative approach to gaining insight into the features of the error surfaces of anns with different model structures is with the aid of exploratory landscape analysis ela metrics mersmann et al 2010 while these metrics have been shown to be able to identify a range of fitness landscape error surface features on a number of mathematical benchmark problems mersmann et al 2011 munoz et al 2015b munoz and smith miles 2017 their application to high dimensional real world problems has been limited as their computational requirements are generally considered prohibitive this is because these metrics are calculated based on samples from the parameter space and the number of samples required to obtain meaningful metric values has been shown to increase exponentially with problem dimensionality at least for some metrics munoz et al 2015a however recently zhu et al 2022 identified a number of ela metrics that have low dependence on problem dimensionality and sample size making them computationally tractable for application to higher dimensional problems this opens the door to using these metrics to gain a better understanding of the features of the error surface and hence how the calibration difficulty of anns is likely to change with model structure consequently the overall aim of this paper is to assess whether ela metrics that have low dependence on problem dimensionality and sample size are able to provide meaningful information on the potential calibration difficulty of ann models with different structures in terms of the number of hidden nodes and numbers of model parameters the remainder of this paper is organized as follows details of the proposed ela metrics and the fitness landscape features they provide information on are given in section 2 while details of the case studies and computational experiments used to illustrate the ability of these metrics to provide meaningful information about the fitness landscapes of anns with different structures and numbers of model parameters are given in section 3 results and discussion in section 4 with a summary and conclusions provided in section 5 2 fitness landscape features and exploratory fitness landscape metrics details of the proposed ela metrics and the error surface features they provide information on are given in following subsections including information on the global structure of the error surface by using the mean pairwise convexity deviation metric section 2 1 information on the multimodality of the error surface by using the maximum entropy of information content metric section 2 2 information on plateaus in the error surface by using the epsilon of information content metric section 2 3 and information on basin size homogeneity by using the median basin centroidal distance and median search function evaluations metrics section 2 4 these metrics are selected as i they have been found to have low dependence on problem dimensionality and sample size zhu et al 2022 thereby enabling them to be applied to real world problems and ii because they are able to identify a wide range of features of an error surface enabling a comprehensive assessment of the impact of model structure on calibration difficulty to be performed details of how these metrics are calculated are given in appendix a 2 1 global structure and the mean pairwise convexity deviation the global structure refers to the general shape of the error surface global optima of error surfaces with a more well defined global structure e g a big bowl shape are easier to find as such error surfaces are able to guide optimization algorithms into promising regions of the search space in contrast the global optimum is more difficult to find for error surfaces with a less well defined global structure e g a flat landscape or a landscape with slopes that change direction frequently or lead to different regions as there is little consistent information to guide optimization algorithms towards this optimum the mean pairwise convexity deviation metric mersmann et al 2010 calculates the convexity of the error surface based on pairs of samples where convexity is measured with respect to the line between the two points convexity is related to the global shape of the error surface typically a surface with positive convexity refers to a well defined global structure which makes it easier to find global optima as outlined above non positive convexity on the other hand can make the search process more difficult as in this case the gradient information provides little guidance to the search 2 2 multimodality and the maximum entropy of information content the landscape feature multimodality refers to the number of local optima on the error surface which is also highly correlated with the degree of roughness of the error surface error surfaces with higher degrees of multimodality have a higher density of local optima making it more difficult to find the global optimum in contrast error surfaces with a lower multimodality have a lower density of local optima making it easier to identify the global optimum multi modality can be measured using the maximum entropy of information content metric hmax munoz et al 2015a this metric builds a ternary sequence based on the fitness values of a sequence of samples where values of 1 1 and 0 are used in the sequence to refer to fitness values of a sample that is bigger smaller and equal to that of the following sample the sequence of a rough surface a high multi modal surface will involve frequent changes in number in contrast a smooth surface an error surface with low multimodality will have a relatively consistent sequence the maximum entropy of the sequence is calculated to characterize the frequency of change in the sequences 2 3 plateaus and the epsilon of information content plateaus refer to regions of flatness in an error surface searching on error surfaces with more plateaus is generally less computationally efficient as such error surfaces contain regions where there are minimal differences in function values providing less distinct information to guide optimization algorithms into promising areas in contrast error surfaces with fewer plateaus generally make searching more computationally efficient as they provide useful information more consistently throughout the landscape the epsilon of information content metric munoz et al 2015a utilizes the same sample sequence as hmax to characterize the plateaus a tolerance value ε is assigned for comparison of whether the fitness values of two neighboring samples are to be considered as equal the corresponding ternary sequence is generated as for hmax but where the strict equality for label 0 is replaced by the ε interval about the given sample value the epsilon of information content metric value is the value of ε that returns a sequence completely of the label 0 a relatively flat surface will return a very small ε value whereas a highly variable surface will return a large ε value the logarithm of the ε values is used for result presentation 2 4 basin size homogeneity and associated metrics basin size homogeneity is associated with the properties of local optima on the error surface this feature refers to both the distribution of local optima and the difficulty in finding local optima firstly with regard to the optima distribution if local optima are contained within a small sub region of the entire parameter space it is easier for an algorithm to find the global optimum or the near global optimum region if local optima are spread throughout the error surface it can be more difficult for an algorithm to find the global optimal region secondly difficulty in finding local optima refers to the number evaluations required by a gradient based heuristic to find the local optima from random initial start points this is related to the efficiency of finding local optima error surfaces that require a large number of evaluations to find the local optima are considered hard to calibrate the median basin centroidal distance is a metric that assesses the distribution of local basins containing local optima on the error surface the metric finds a large pre specified number of local optima using a gradient algorithm and uses hierarchical clustering to collate local optima within a very small distance in the same local basin it calculates the pairwise distance between the identified local basins and uses the median to summarize the average distance the second metric to characterize basin size homogeneity is the median search function evaluations metric which assesses the difficulty in finding local optima this metric refers to the median number of function evaluations required to identify each local optimum it can indicate how extensive and complex the basin is for a given local optimum 3 case studies and computational experiments 3 1 overview in order to illustrate the type of information that can be obtained about the features of the error surfaces of single layer mlp models with the aid of the ela metrics introduced in section 2 and how these are affected by anns with different model structures and numbers of parameters the ela metrics are applied to a number of case studies and model structures fig 1 as can be seen four case studies are used including kentucky river catchment rainfall runoff data usa hereon refer to as the kentucky runoff case murray river salinity data australia murray salinity case myponga water distribution system chlorine data australia myponga chlorine case and south australian surface water turbidity data australia sa turbidity case see section 3 2 for details these are selected as they represent a variety of hydrological modelling problems that have different numbers of inputs and data series lengths therefore representing a diversity of error surfaces these case studies have also been used as benchmark problems in a number of previous studies investigating the impact of different ann model development practices e g wu et al 2013 humphrey et al 2017 mlps are used as the ann model architecture as this has been by far the dominant architecture used in previous ann applications in hydrology and water resources see maier et al 2010 wu et al 2014 it should be noted that although there has been a significant increase in the use of deep neural networks in recent years e g see razavi 2021 their use for hydrological modelling has been largely restricted to the use of ltsms e g gu et al 2020 gao et al 2020 yin et al 2021 apaydin et al 2021 with many studies still using mlps with a single layer of hidden nodes e g maroufpoor et al 2020 feng et al 2018 mukherjee and ramachandran 2018 or a hybrid approach combing a single hidden layer mlp with another machine learning approach e g tan et al 2018 nourani 2017 graf et al 2019 fathian et al 2019 this is likely because the majority of applications in hydrological modelling are concerned with prediction and forecasting of low dimensional time series whereas the majority of applications of deep anns are in pattern recognition and image processing related applications however it should be noted that the ela metrics suggested in section 2 are able to provide information on the features of the error surface for any type of ann including deep anns the primary purpose of the particular case studies and types of anns used in this study is to illustrate the potential of using ela metrics for obtaining information on the features of the error surfaces for anns for each case study an mlp with a single layer of hidden nodes is used as this has been shown to provide universal function approximation capability de villiers and barnard 1993 in each case the number of hidden nodes is varied between 0 and 10 resulting in numbers of model parameters connection and bias weights ranging from 1 to 120 ten stochastic replicates of each mlp model with a given number of hidden nodes 0 1 2 10 are generated for each of the four case studies resulting in a total of 440 error surfaces fig 1 further details of the mlp models are given in section 3 3 in order to enable the ela metrics to be calculated 2 000 samples of the error surfaces are generated for each of the 11 model structures and four case studies mersmann et al 2010 this is considered adequate based on the findings of zhu et al 2022 these sets of samples for each of the 440 error surfaces are used to calculate a corresponding set of values for the 5 ela metrics considered see section 2 the above analyses were conducted using the university of adelaide s supercomputing facilities which consist of 48 skylake nodes with 80 cpus and 377 gb of memory per node the r package validann humphrey et al 2017 is used for mlp development latin hypercube sampling generated using the lhs package in r is used for sample generation and the r package flacoo kerschke and trautmann 2016 is used for the calculation of the 5 fitness landscape metrics 3 2 case studies as outlined above the case studies used in this paper are the kentucky runoff murray salinity myponga chlorine and sa turbidity cases the objective of the kentucky runoff case study is to forecast runoff in the kentucky river catchment one day in advance based on previous daily observations of effective rainfall and runoff the objective of the murray salinity case study is to forecast salinity in the river murray at murray bridge 14 days in advance to enable the salinity of the water that is pumped from this location to adelaide about 100kms away to be minimized as salt is a conservative substance and transported from upstream daily salinity data from two upstream locations mannum and waikerie are considered as potential inputs the objective of the myponga chlorine case study is to forecast free chlorine levels in a water transmission pipeline 24 h in advance to assist with upstream chlorine dosage control for water disinfection purposes potential inputs include upstream chlorine dosage levels past values of free chlorine at the location of interest and past values of water temperature as these affect the rate of chlorine decay the purpose of the sa turbidity case study is to predict the treated water quality filtered water turbidity in response to raw water quality based on five quality parameters and applied treatment level alum dose it should be noted that as this is a prediction case study rather than a forecasting case study there are no lagged values further information on the data sets used for these case studies is provided in table 1 the input selection and data splitting undertaken by wu et al 2013 is used with the calibration datasets used as the basis for ela metric calculation 3 3 mlp models and data transformations the inputs and outputs of all cases used in this paper are summarized in table 2 inputs x are transformed to the standard normal distribution x n 0 1 and outputs are scaled linearly between 0 1 and 0 9 in order to be aligned with humphrey et al 2017 the hyperbolic tangent function is used as the mlp transfer function between input nodes and hidden nodes and a linear function is used between hidden nodes and output nodes 4 results and discussion the results for each of the 5 ela metrics are given in sections 4 1 to 4 5 followed by a summary and discussion of the results in section 4 6 4 1 mean pairwise convexity deviation the results of the mean pairwise convexity deviation are shown in fig 2 the results represent the deviation between the fitness value on the error surface from that of a linear regression line so that negative results indicate that the error surface is positively convex the results show that the error surfaces of the mlp models become more convex as the number of hidden nodes increases except for the zero hidden node cases where mlps have a linear structure i e the hyperbolic tangent transformation from the hidden layer is not utilized this trend is likely attributed to the increased ability of the higher dimensional anns to fit the data more accurately i e the increase in the degrees of freedom lead to better defined regions of high performing parameter values with the zero hidden node case being an anomaly resulting from the linear model having a well defined optimum despite being a poor performing model structure as a result the rmse error metric used in this paper transferred the linear model to a quadratic error surface with a well defined optimum giving the error surface a level of convexity which is better than the corresponding 1 hidden node mlps this increase in convexity indicates a change of error surface structure as illustrated in fig 2 c this is a key result as it shows that models that have more hidden nodes have an advantage over models with fewer hidden nodes as a more convex structure can provide clearer gradient information to guide the search through the calibration process however the decreased calibration difficulty of models with more hidden nodes due to this increase in convexity is potentially counteracted by the increased calibration difficulty resulting from an increase in the dimension of the error surface nevertheless the increase in convexity with an increase in the number of hidden nodes could explain why optimization algorithms can still find good solutions for models with a large number of hidden nodes and consequently parameters even though the problems are more highly dimensional as the number of hidden nodes increases so does the number of parameters and so a similar pattern is observed in fig 2 b as in fig 2 a in interpreting this figure it is important to note that the kentucky runoff and murray salinity cases represent smaller mlps with only two inputs in comparison to the larger sa turbidity case 5 inputs and the largest myponga chlorine case 10 inputs it is seen in fig 2 b that for a given number of parameters i e error surface dimension the smaller mlps are more convex than the larger ones implying that increasing the number of inputs can serve to reduce the convexity of an mlp s error surface making it more difficult to calibrate this highlights the potential importance of using formal input variable selection ivs algorithms for identifying the smallest number of inputs that have a significant impact on model performance e g galelli et al 2014 4 2 maximum entropy of information content hmax hmax represents how rough or how multi modal the error surface is where a higher hmax refers to a rougher error surface and vice versa fig 3 presents the results of hmax versus the number of hidden nodes fig 3 a and the associated number of parameters fig 3 b for all case studies as shown fig 3 a hmax grows dramatically as the number of hidden nodes increases to larger than 3 and the number of parameters is greater than 15 this increase in hmax indicates an increase in the number of oscillations in and hence the roughness of the error surface as the number of hidden nodes increases as illustrated in fig 3 c however after this initial increase hmax reaches a plateau for numbers of hidden nodes ranging from 3 to 10 this is because the error surfaces of these anns is already extremely rough considering that hmax can reach the theoretical maximum of 1 0 only when all pairs of neighboring symbols consist of differing symbols i e pairs of symbols such as 1 0 1 1 or 1 1 the maximum of hmax is very hard to reach the symbol sequence of practical samples is likely to contain pairs of neighboring symbols with the same symbol which can significantly limit the maximum of the hmax value in practice 4 3 epsilon of information content the epsilon of information content represents the range of the fitness values of the error surface where a big ε indicates a big range in fitness values and a small ε refers to a flatter error surface fig 4 shows the result of the epsilon of information content for all case studies these cases show a clear trend of an increase in surface flatness with an increasing number of hidden nodes evidenced in fig 4 a as illustrated in fig 4 c in this figure it is also seen that models with more inputs have a flatter structure than models with fewer inputs that is compare the high input case of myponga chlorine with the low input cases of kentucky runoff and murray salinity even when the number of parameters is the same for models with a different number of inputs i e consider points intersecting a vertical line in fig 4 b models with more inputs still show a flatter surface than those with a fewer number of inputs this again highlights the importance of the use of formal ivs algorithms as discussed in section 4 1 it should be noticed that theoretically single layer mlps with more hidden nodes should have at least one not worse global optimum than those with fewer hidden nodes however these regions with optimal solutions are considered to be very small compared to the overall error surface as shown in fig 4 c although regions with optimal solutions shown in the red oval in fig 4 c have potentially much lower error than the general regions shown in the black oval in fig 4 c as the size of these optimal regions are small they have limited impact on the overall flatness of the error surface additionally for an increasing number of hidden nodes the highly localized nature of the global optima within these error surfaces means that they become increasingly hard to find 4 4 median basin centroidal distance median basin centroidal distance characterizes the distribution of local basins and associated local optima across the error surface a bigger distance refers to a greater spread of basins and a small distance refers to basins clustered in a small region of the error surface fig 5 presents the results of this metric for all cases the plots for all cases start at a 0 distance as only one optimum can be found for anns with no hidden nodes however for one or more hidden nodes the distance increases almost linearly with an increase in the number of hidden nodes as seen in fig 5 b there is a strong consistency across the cases of basin distance for a given number of parameters indicating the distance is more dependent on the number of parameters regardless of the number of hidden nodes this aids in the interpretation of fig 5 a where it is seen that for a given number of hidden nodes the larger cases myponga chlorine and sa turbidity possess a greater basin distance i e more inputs increase the basin distance an increase in the distance between local optima is considered a disadvantage for optimization as this means algorithms have to explore a larger area on the error surface in order to identify the global optimum from the distributed local optima compared with the cases where the local optima are clustered in a relatively small region see fig 5 c therefore models with many parameters require the use of optimization algorithms with a strong exploration capacity in order to be able to search through the entire space without missing any local optima or pre maturely converging to sub optimal sub regions see maier et al 2019 4 5 median search function evaluations the median search function evaluations metric measures the difficulty of identifying local optima through a gradient based local search a larger number of evaluations indicates that the error surface is more difficult to optimize as shown in fig 6 a as with the median basin centroidal distance the number of evaluations increases near linearly for an increasing number of hidden nodes with the larger cases requiring more evaluations this expected result can be explained by considering fig 6 b where the function evaluations increase near linearly with the mlp parameter number where the increase can be directly attributed to the increases in error surface dimension i e higher dimensional surfaces require more evaluations to find the local and global optima this is illustrated in fig 6 c where basins with local optima of mlps with a smaller number of parameters are relatively small and easy to search whereas those of mlps with larger numbers of parameters are relatively large and spreading resulting in difficulty in exploiting the local optima in the basin 4 6 discussion based on the results presented in sections 4 1 to 4 5 the error surfaces of mlp models become more complex as the number of hidden nodes increases irrespective of case study as illustrated conceptually in fig 7 the key change is that as the number of hidden nodes increases the general shape of the error surface structure becomes flatter overall whilst also becoming rougher with an increasing number of local optima with smaller and deeper basins of attraction dispersed across the entire surface overall the global structure of the error surface was found to be more related to the number of hidden nodes while the features related to multimodality and local basin distribution were found to be related to the number of parameters potential reasons for the above observations could be related to the increase in the degree of freedom associated with mlps with a larger number of hidden nodes which in turn is likely to increase the ability of an ann to fit the data and therefore reduce the model error however further increases in the number of hidden nodes can conversely increase the potential for overfitting which will introduce many local minima and consequently result in a higher degree of roughness additionally an increasing number of hidden nodes may also cause an unnecessarily high degree of freedom within the model structure i e insensitive parameters that have little influence on the error surface which may result in flat regions in the error surface other factors that are likely to affect the observed results include the data and error metric used to construct the error surface as the features of the error surface are a function of the characteristics of the data and error metric for example the length of input data and its statistical characteristics such as the information content or degree of noise of the data are expected to affect the quality of the mlp calibration data with a higher information content i e unique patterns in the data and broader coverage across the input space are expected to result in a more robust connection between inputs and outputs consequently contributing to a less flat and easier to calibrate error surface additionally the selection of error metrics should provide a sensitivity of the features of the error surface as error metrics that are insensitive to the model fitting characteristics are likely to provide a poor quality surface such as a rough error surface with irregular oscillations a flat surface with very little information or a non convex surface in order to test the above hypotheses additional computational experiments would be required including using synthetic datasets with varying lengths and statistical characteristics such as different number of inputs and different level of input interaction coupled with using different error metrics the above findings about the impact of the number of hidden nodes on the features of the error surface are in agreement with the conclusions of maier and dandy 1998a b who found that based on the results of extensive calibration trials this work used the murray salinity case study using mlps with different numbers of inputs and hidden nodes the error surfaces of mlps with more parameters have large plateaus with many local optima that are deep and have steep slopes the existence of many local optima for mlps with a larger number of parameters was also demonstrated by kingston et al 2005 who produced 3 d error surface plots of an ann rainfall runoff model by fixing all connection weights to their optimal values while altering two weights at a time between a set of pre determined limits the above results suggest that it is more difficult to calibrate ann models with more parameters when calibration methods are used that do not have the ability to explore different regions of the error surface widely this is because more models with more parameters have error surfaces with a larger number of local optima in which calibration approaches with low exploratory capability such as gradient based optimization algorithms can become trapped see maier et al 2019 this is why the commonly used back propagation bp and levenberg marquardt lm algorithms have been found to perform relatively poorly on mlp anns with a larger number of parameters in previous studies for example maier and dandy 1996 found that model performance decreased with an increase in the number of model parameters when the bp algorithm was used for calibrating models with different numbers of inputs and hidden nodes for the murray salinity case study similarly piotrowski and napiorkowski 2011 found that the variability in calibration performance increased for calibration trials from different starting positions in model parameter space when a lm algorithm was applied to ann models with a larger number of parameters suggesting increased difficulty in finding better solutions for more models with increased parameters conversely the above results also explain why the relative performance of calibration approaches with a greater ability to explore the search space has been shown to increase for ann models with a larger number of parameters as they have a greater ability to escape local optima and find better regions in the error surface for example for the murray salinity case study maier and dandy 1998a found that the performance of the bp algorithm increased for models with a larger number of parameters when the exploratory ability of the algorithm was improved by increasing the step size used to explore the error surface in addition a number of studies have found that metaheuristics which are known for their increased exploration ability see maier et al 2019 are able to achieve better calibration performance than gradient based methods for anns with a larger number of parameters for example kingston et al 2005 found that a genetic algorithm ga and the complex shuffled complex evolution algorithm outperformed the bp algorithm for a rainfall runoff mlp with a large number of parameters and maroufpoor et al 2020 found that a grey wolf optimization algorithm outperformed the lm algorithm for calibrating mlps with a large number of parameters anns for estimating reference evapotranspiration however although the greater exploratory ability of metaheuristics enables them to find better regions in complex error surfaces because of their decreased exploitative capability they generally have difficulty in finding the bottom of the deep narrow local optima that are a feature of the error surfaces of mlp models with a larger number of parameters this explains why piotrowski and napiorkowski 2011 found that while the average performance of evolutionary algorithms over a number of calibration trials from different starting positions in model parameter space was better than that of the lm algorithm the lm algorithm was able to find the best solutions in individual trials provided the number of starting positions was sufficiently large this also explains why a number of studies have found that a hybrid approach as part of which a metaheuristic is used to find good regions in the error surface that are then used as starting positions for gradient based approaches have been found to result in improved calibration performance of anns with a larger number of parameters for example alavi and gandomi 2011 used simulated annealing sa coupled with a lm algorithm bahrami et al 2016 coupled sa and gas with a lm algorithm and chau 2007 used a split step particle swarm optimization pso algorithm which coupled standard pso and lm algorithms in addition recent advances in calibration of high dimensional anns have yielded a new generation of gradient based algorithms razavi 2021 which arguably make use of their own meta heuristics and can therefore also be considered hybrid algorithms consequently the use of such hybrid algorithms is recommended for the calibration of mlp ann models with a larger number of parameters this highlights the value of approaches that assist with identifying more parsimonious ann models such as input variable selection techniques see galelli et al 2014 model structure selection metrics e g kingston et al 2008 and approaches that help improve the fitness landscape properties of highly parameterized models e g regularization see razavi 2021 guillaume et al 2019 5 summary and conclusions calibration is an important component of the development of any model but is especially critical for anns as the quality of the calibration not only determines values of the unknown model parameters but also the structure of the model and the degree to which underlying system knowledge can be elicited from the calibrated model the success of model calibration is a function of how well suited the optimization algorithm used is to explore the error surface under consideration while there have been many studies comparing the performance of different optimization algorithms existing literature has been largely silent on the properties of the error surface of anns with different structures making it difficult to understand and explain why certain optimization algorithms perform better than others and which optimization approaches are preferred under particular circumstances this paper has addressed this shortcoming by demonstrating that five exploratory landscape analysis ela metrics that have been shown to have low dependence on problem dimensionality and sample size in previous studies can be used to better understand the features of the error surfaces of anns of varying complexity based on the results of four water quantity and quality case studies from the literature kentucky runoff murray salinity myponga chlorine sa turbidity it has been demonstrated that mlps with a smaller number of hidden nodes and parameters are easier to calibrate as they have a more well defined overall shape that is able to guide optimization algorithms to better regions in the error surface more easily additionally the error surface of smaller mlps is smoother so that it is harder for algorithms to be trapped in local optima in contrast the generally flatter error surface of mlps with more parameters and hidden nodes provides limited information to guide the search to better regions in the error surface in addition the higher level of multimodality roughness of larger mlps can also make it more difficult to identify the global optimum especially for optimization algorithms with limited exploration capacity such as gradient based methods on the other hand as error surfaces of larger mlps are more convex than those of smaller mlps which results in better defined gradient information in local regions it should be easier to converge to the local optima of larger mlps however this is also likely to lead to premature convergence to local optima rather than the identification of the global optimum in addition the presence of these widely distributed narrow and deep local optima in the error surfaces of more complex mlps means that hybrid approaches to calibration are likely to result in better performance this is because such approaches use algorithms with higher degrees of exploration such as metaheuristics in the initial stages of the calibration to find good regions in the error surface followed by algorithms with a higher degree of exploitation such as gradient methods in the latter stages of calibration to enable good locally optimal or globally optimal solutions to be identified while the findings of this study highlight the potential of using ela metrics for better understanding the error surfaces of mlps of different complexity for a range of case studies thereby enabling light to be shed on the findings of previous studies further analysis is needed to better understand the reasons for the observed changes in the features of error surfaces of single layer mlp models with changes in the number of hidden nodes and with changes in the information content of the data this could be achieved with the aid of additional computational experiments using synthetically generated data with different attributes tests including varying model parameter interactions and sensitivity ranges and tests on the impact of error metrics formal analyses of the convergence of optimization algorithms could also provide a theoretical foundation for interpretation especially in relation to the effect of degrees of freedom further analysis is also required to generalize the results of this study to a broader array of machine learning strategies this would include application of the metrics to anns with different architectures support vector machines generalized regression neural networks recurrent neural networks and different types of deep neural networks in addition the findings of this research open the door to developing evidence based approaches to tailoring optimization methods and parameterizations see wang et al 2020 zheng et al 2017 for calibrating ann models of different types and complexity based on the knowledge of error surface features rather than relying on a brute force approach to using a range of optimization approaches and picking the one that performs best for the problem at hand similarly future work could focus on the determination of relationships between fitness landscape features and methods used for other aspects of ann model development such as input variable selection and the determination of the most appropriate model structure declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first author is supported by an adelaide graduate research scholarship which is gratefully acknowledged the authors would also like to thank joseph guillaume and saman razavi whose comments have helped to improve the quality of this paper significantly appendix a ela metric calculation a1 mean pairwise convexity deviation as discussed in section 2 4 the mean pairwise convexity deviation describes the general global structure of error surface their calculation requires implementation of the following general steps i select random pairs of points x i x j from the total number of samples of the fitness landscape considered to ensure most of the samples are included in the calculation we use n random pairs of points in this study where n is the number of initial samples ii calculate a linear combination of x i x j to select a new point x n between the two points where a1 x n w x i 1 w x j where w is a random number between 0 and 1 calculate the fitness value y n of x n based on the corresponding test function iii calculate the fitness values y i y j of x i x j based on the corresponding test function use linear regression to calculate the approximated linear fitness value y n at x n a2 y n w y i 1 w y j where w is the same w as in ii iv calculate the difference δ between y n and y n by δ y n y n v if δ is negative the landscape between the selected two points is convex providing good gradient information to guide the search in this region of the fitness landscape vi if δ is positive the landscape between the selected two points is not convex providing poor gradient information to guide the search in this region of the fitness landscape vii the average of the calculated n pairwise divisions is used to summarize the general convexity of the error surface a2 maximum entropy and epsilon of information content as discussed in section 2 4 maximum entropy and epsilon of information content describes the roughness and degree of plateau of error surface respectively their calculation requires implementation of the following general steps i sort all samples into a sequence nearest neighboring as part of which the following sample x i 1 of one sample x i is the closest sample to the corresponding by euclidean distance is used as sample sorting method in this paper ii build a symbol sequence by using the following rule a3 i 1 i f y i 1 y i 0 i f y i 1 y i 1 i f y i 1 y i where 0 is the accuracy parameter of the symbol sequence and y i is the fitness value of sample x i it can be seen that is controlled by the value of if is small can be quite sensitive and contain frequent symbol changes in the sequence for example sequence 1 1 1 1 1 if is big on the other hand can be insensitive and contain many 0 values in the sequence for example sequence 0 0 0 0 0 iii calculate the information content h of the sequence based on the definition a4 h a b p ab log 6 p ab where a b 1 0 1 and p ab is the probability that two neighbored symbols a b are different iv build a new sequence by removing all 0 values in and calculate the partial information content m which is defined as a5 m n 1 where n is the length of sequence v the entropy and epsilon are estimated based on h and m respectively the two curves against are shown in fig a1 munoz et al 2015b describes the way to calculate them is shown as following equations a6 h max max h a7 ratio log 10 max m m 0 2 w h e r e m 0 m 0 a3 median basin centroidal distance and median search function evaluations as discussed in section 2 4 median basin centroidal distance and median search function evaluations are able to describe the degree of basin size homogeneity of local optima of error surface their calculation process is presented as below i use a gradient algorithm to find local optima starting from initial samples x s in this study the l bfgs b algorithm zhu et al 1997 was used due to its capacity to setup the range of calculation to avoid the identified local optima being beyond the range of the fitness landscape ii use of hierarchical clustering to cluster identified local optima in i local optima within a given euclidean distance e are included in the same cluster which refers to a corresponding local basin in this study e is 5 of total euclidean distance length of the whole fitness landscape as this distance performs well in distinguishing different clusters without resulting in a computational burden that results in intractability if e is too small a larger number of clusters is likely to be generated by hierarchical clustering which increases complexity and the computational requirements of subsequent calculations iii calculate the centroid x c of each local basin identified in ii based on the local optima in the corresponding basin iv calculate the pairwise distance between x c the median value of all pairwise distances is returned as the result of median basin centroidal distance v when running l bfgs b algorithm for all samples in i record the number of evaluations for each sample to converge to the corresponding local optimum the median number of evaluations for all samples is returned as the result of median search function evaluations 
3196,artificial neural network ann models have been used for hydrological and water resources modelling for several decades where their calibration training has received much attention the aim of automated calibration processes is to identify values of the model parameters that minimize an error measure between model outputs and corresponding measured values the degree of difficulty associated with this process is a function of the nature of the relationship between changes in values of model parameters and corresponding changes in the error measure i e the features of the error surface and the automated method used to identify the combination of model parameters that minimizes the error measure i e the optimization method used to find the global optimum in the error surface while the impact of different optimization methods has been studied extensively how the features of the error surface change for different model structures has not in this paper it is demonstrated that exploratory landscape analysis ela metrics can be used successfully to characterize the features of the error surfaces of ann models thereby helping to explain the reasons for an increase or decrease in calibration difficulty and in doing so shedding new light on findings in existing literature results show that the error surfaces of single layer multi layer perceptrons mlps with fewer hidden nodes have a more well defined overall shape and have fewer local optima while the error surfaces of single layer mlps with a larger numbers of hidden nodes are flatter and have many distributed deep local optima consequently single layer mlps with a smaller number of hidden nodes can be calibrated successfully using gradient based methods such as the back propagation algorithm whereas single layer mlps with a relatively large number of hidden nodes are best calibrated using hybrid approaches that go beyond basic gradient based methods for example combination of global based evolutionary algorithms coupled with a local search keywords artificial neural networks anns multi layer perceptrons mlps calibration training optimization fitness landscape error surface exploratory landscape analysis ela data availability data will be made available on request 1 introduction artificial neural networks anns have been used extensively for hydrological modelling over the last several decades including water quality forecasting maier and dandy 1999 lischeid 2001 bowden et al 2005 cigizoglu and kisi 2006 kisi 2010 bayram et al 2012 lafdani et al 2013 olyaie et al 2015 zounemat kermani et al 2016 amamra et al 2018 meral et al 2018 banadkooki et al 2020 kim et al 2021 water quantity forecasting sajikumar and thandaveswara 1999 zealand et al 1999 gautam et al 2000 kim and barros 2001 sivakumar et al 2002 zhang and govindaraju 2003 rajurkar et al 2004 coulibaly and baldwin 2005 kingston et al 2005 wang et al 2006 yu and liong 2007 chua et al 2008 wu et al 2009 adamowski and sun 2010 khatibi et al 2011 jothiprakash and magar 2012 piotrowski and napiorkowski 2013 he et al 2015 humphrey et al 2016 tan et al 2018 fathian et al 2019 cheng et al 2020 water level forecasting see and openshaw 1999 phien and kha 2003 pereira filho and dos santos 2006 chau 2006 2007 leahy et al 2008 tiwari and chatterjee 2010 hajji et al 2012 pan et al 2013 nourani and mousavi 2016 mukherjee and ramachandran 2018 kurian et al 2020 xie et al 2021 evaporation modelling kişi 2006 2013 cobaner 2011 chaudhari et al 2012 kişi and tombul 2013 feng et al 2018 ferreira et al 2019 maroufpoor et al 2020 nourani et al 2020a nourani et al 2020b the prediction of soil properties elshorbagy and parasuraman 2008 parchami araghi et al 2013 trenouth and gharabaghi 2015 zanetti et al 2015 zhuo et al 2016 rahmati 2017 patrignani and ochsner 2018 li et al 2020 jian et al 2021 water temperature forecasting sahoo et al 2009 sabouri et al 2013 sabouri et al 2016 cole et al 2014 deweber and wagner 2014 graf et al 2019 and some other applications xu et al 2017 nourani et al 2017 zubaidi et al 2018 nguyen ky et al 2018 rezaali et al 2021 as with all models calibration is a critical component of the development of ann models termed training in the ann literature however for ann models calibration takes on additional importance firstly compared with process driven models see mount et al 2016 the structure of ann models is generally unknown and is often determined via a trial and error process models with different structures are calibrated and the model structure that performs best on the calibration data or test data if cross validation is used is selected maier et al 2010 wu et al 2014 consequently the selected model structure is a function of the success of the calibration process in that if the calibration does not identify the combination of model parameter values that corresponds to the lowest error for a given model structure the conclusions about which model structure is most appropriate can be incorrect secondly given the black box nature of anns the only way to extract meaningful information about the system being modelled is via analysis of the calibrated model structure and parameters dimopoulos et al 1995 lek et al 1996 maier and dandy 1997 kingston et al 2006 mount et al 2013 humphrey et al 2017 consequently the model information obtained is a function of the success of the calibration process and for reasons outlined above a poorly performing calibration process can lead to inaccurate information given the importance of the calibration of ann models it is not surprising that this issue has received significant attention in literature an area of particular focus has been the comparison of optimization algorithms for calibrating ann models kingston et al 2005 wu et al 2013 with many studies reaching contradictory conclusions about which optimization approaches perform better while providing limited insight into why this might be the case for example piotrowski and napiokowski 2011 compared the performance of differential evolutionary algorithm de particle swarm optimization pso and differential evolution with that of global and local neighborhoods degl and levenberg marquardt lm algorithms for calibrating ann models where they found that de had very poor performance and lm a second order gradient algorithm outperformed all metaheuristics in contrast maroufpoor et al 2020 found that lm often prematurely converged to local optima and grey wolf optimization gwo a metaheuristic could obtain better solutions than lm second order gradient algorithms were also found to perform worse than first order gradient methods by maier and dandy 1999 in order to better understand potential reasons for the contrasting findings in the studies outlined above it is important to recognize that the success of a particular calibration approach is not only a function which algorithm is used but also a function of the difficulty of the calibration problem itself maier et al 2014 calibration problems can be represented geometrically by the error surface otherwise known as the error function fitness function fitness landscape or response surface which consists of the relationship between different values of model parameters and the corresponding calibration errors maier et al 2019 as the purpose of calibration is to identify the set of model parameters that result in the smallest calibration error akin to finding the lowest point in the error function calibration difficulty is a function of the features of the error surface for example if the error surface is smooth with a single well defined minimum this optimal point is relatively easy to find conversely if the error surface is rough that is it possesses many minima of similar or equal value i e local minima the overall global minimum is difficult to find guillaume et al 2019 similarly the presence of flat regions or plateaus in the error function generally make it less computationally efficient to guide the search towards regions with lower error given that many ann models contain a relatively large number of parameters that need to be calibrated it is difficult to visualize their error surfaces kingston et al 2005 an alternative approach to gaining insight into the features of the error surfaces of anns with different model structures is with the aid of exploratory landscape analysis ela metrics mersmann et al 2010 while these metrics have been shown to be able to identify a range of fitness landscape error surface features on a number of mathematical benchmark problems mersmann et al 2011 munoz et al 2015b munoz and smith miles 2017 their application to high dimensional real world problems has been limited as their computational requirements are generally considered prohibitive this is because these metrics are calculated based on samples from the parameter space and the number of samples required to obtain meaningful metric values has been shown to increase exponentially with problem dimensionality at least for some metrics munoz et al 2015a however recently zhu et al 2022 identified a number of ela metrics that have low dependence on problem dimensionality and sample size making them computationally tractable for application to higher dimensional problems this opens the door to using these metrics to gain a better understanding of the features of the error surface and hence how the calibration difficulty of anns is likely to change with model structure consequently the overall aim of this paper is to assess whether ela metrics that have low dependence on problem dimensionality and sample size are able to provide meaningful information on the potential calibration difficulty of ann models with different structures in terms of the number of hidden nodes and numbers of model parameters the remainder of this paper is organized as follows details of the proposed ela metrics and the fitness landscape features they provide information on are given in section 2 while details of the case studies and computational experiments used to illustrate the ability of these metrics to provide meaningful information about the fitness landscapes of anns with different structures and numbers of model parameters are given in section 3 results and discussion in section 4 with a summary and conclusions provided in section 5 2 fitness landscape features and exploratory fitness landscape metrics details of the proposed ela metrics and the error surface features they provide information on are given in following subsections including information on the global structure of the error surface by using the mean pairwise convexity deviation metric section 2 1 information on the multimodality of the error surface by using the maximum entropy of information content metric section 2 2 information on plateaus in the error surface by using the epsilon of information content metric section 2 3 and information on basin size homogeneity by using the median basin centroidal distance and median search function evaluations metrics section 2 4 these metrics are selected as i they have been found to have low dependence on problem dimensionality and sample size zhu et al 2022 thereby enabling them to be applied to real world problems and ii because they are able to identify a wide range of features of an error surface enabling a comprehensive assessment of the impact of model structure on calibration difficulty to be performed details of how these metrics are calculated are given in appendix a 2 1 global structure and the mean pairwise convexity deviation the global structure refers to the general shape of the error surface global optima of error surfaces with a more well defined global structure e g a big bowl shape are easier to find as such error surfaces are able to guide optimization algorithms into promising regions of the search space in contrast the global optimum is more difficult to find for error surfaces with a less well defined global structure e g a flat landscape or a landscape with slopes that change direction frequently or lead to different regions as there is little consistent information to guide optimization algorithms towards this optimum the mean pairwise convexity deviation metric mersmann et al 2010 calculates the convexity of the error surface based on pairs of samples where convexity is measured with respect to the line between the two points convexity is related to the global shape of the error surface typically a surface with positive convexity refers to a well defined global structure which makes it easier to find global optima as outlined above non positive convexity on the other hand can make the search process more difficult as in this case the gradient information provides little guidance to the search 2 2 multimodality and the maximum entropy of information content the landscape feature multimodality refers to the number of local optima on the error surface which is also highly correlated with the degree of roughness of the error surface error surfaces with higher degrees of multimodality have a higher density of local optima making it more difficult to find the global optimum in contrast error surfaces with a lower multimodality have a lower density of local optima making it easier to identify the global optimum multi modality can be measured using the maximum entropy of information content metric hmax munoz et al 2015a this metric builds a ternary sequence based on the fitness values of a sequence of samples where values of 1 1 and 0 are used in the sequence to refer to fitness values of a sample that is bigger smaller and equal to that of the following sample the sequence of a rough surface a high multi modal surface will involve frequent changes in number in contrast a smooth surface an error surface with low multimodality will have a relatively consistent sequence the maximum entropy of the sequence is calculated to characterize the frequency of change in the sequences 2 3 plateaus and the epsilon of information content plateaus refer to regions of flatness in an error surface searching on error surfaces with more plateaus is generally less computationally efficient as such error surfaces contain regions where there are minimal differences in function values providing less distinct information to guide optimization algorithms into promising areas in contrast error surfaces with fewer plateaus generally make searching more computationally efficient as they provide useful information more consistently throughout the landscape the epsilon of information content metric munoz et al 2015a utilizes the same sample sequence as hmax to characterize the plateaus a tolerance value ε is assigned for comparison of whether the fitness values of two neighboring samples are to be considered as equal the corresponding ternary sequence is generated as for hmax but where the strict equality for label 0 is replaced by the ε interval about the given sample value the epsilon of information content metric value is the value of ε that returns a sequence completely of the label 0 a relatively flat surface will return a very small ε value whereas a highly variable surface will return a large ε value the logarithm of the ε values is used for result presentation 2 4 basin size homogeneity and associated metrics basin size homogeneity is associated with the properties of local optima on the error surface this feature refers to both the distribution of local optima and the difficulty in finding local optima firstly with regard to the optima distribution if local optima are contained within a small sub region of the entire parameter space it is easier for an algorithm to find the global optimum or the near global optimum region if local optima are spread throughout the error surface it can be more difficult for an algorithm to find the global optimal region secondly difficulty in finding local optima refers to the number evaluations required by a gradient based heuristic to find the local optima from random initial start points this is related to the efficiency of finding local optima error surfaces that require a large number of evaluations to find the local optima are considered hard to calibrate the median basin centroidal distance is a metric that assesses the distribution of local basins containing local optima on the error surface the metric finds a large pre specified number of local optima using a gradient algorithm and uses hierarchical clustering to collate local optima within a very small distance in the same local basin it calculates the pairwise distance between the identified local basins and uses the median to summarize the average distance the second metric to characterize basin size homogeneity is the median search function evaluations metric which assesses the difficulty in finding local optima this metric refers to the median number of function evaluations required to identify each local optimum it can indicate how extensive and complex the basin is for a given local optimum 3 case studies and computational experiments 3 1 overview in order to illustrate the type of information that can be obtained about the features of the error surfaces of single layer mlp models with the aid of the ela metrics introduced in section 2 and how these are affected by anns with different model structures and numbers of parameters the ela metrics are applied to a number of case studies and model structures fig 1 as can be seen four case studies are used including kentucky river catchment rainfall runoff data usa hereon refer to as the kentucky runoff case murray river salinity data australia murray salinity case myponga water distribution system chlorine data australia myponga chlorine case and south australian surface water turbidity data australia sa turbidity case see section 3 2 for details these are selected as they represent a variety of hydrological modelling problems that have different numbers of inputs and data series lengths therefore representing a diversity of error surfaces these case studies have also been used as benchmark problems in a number of previous studies investigating the impact of different ann model development practices e g wu et al 2013 humphrey et al 2017 mlps are used as the ann model architecture as this has been by far the dominant architecture used in previous ann applications in hydrology and water resources see maier et al 2010 wu et al 2014 it should be noted that although there has been a significant increase in the use of deep neural networks in recent years e g see razavi 2021 their use for hydrological modelling has been largely restricted to the use of ltsms e g gu et al 2020 gao et al 2020 yin et al 2021 apaydin et al 2021 with many studies still using mlps with a single layer of hidden nodes e g maroufpoor et al 2020 feng et al 2018 mukherjee and ramachandran 2018 or a hybrid approach combing a single hidden layer mlp with another machine learning approach e g tan et al 2018 nourani 2017 graf et al 2019 fathian et al 2019 this is likely because the majority of applications in hydrological modelling are concerned with prediction and forecasting of low dimensional time series whereas the majority of applications of deep anns are in pattern recognition and image processing related applications however it should be noted that the ela metrics suggested in section 2 are able to provide information on the features of the error surface for any type of ann including deep anns the primary purpose of the particular case studies and types of anns used in this study is to illustrate the potential of using ela metrics for obtaining information on the features of the error surfaces for anns for each case study an mlp with a single layer of hidden nodes is used as this has been shown to provide universal function approximation capability de villiers and barnard 1993 in each case the number of hidden nodes is varied between 0 and 10 resulting in numbers of model parameters connection and bias weights ranging from 1 to 120 ten stochastic replicates of each mlp model with a given number of hidden nodes 0 1 2 10 are generated for each of the four case studies resulting in a total of 440 error surfaces fig 1 further details of the mlp models are given in section 3 3 in order to enable the ela metrics to be calculated 2 000 samples of the error surfaces are generated for each of the 11 model structures and four case studies mersmann et al 2010 this is considered adequate based on the findings of zhu et al 2022 these sets of samples for each of the 440 error surfaces are used to calculate a corresponding set of values for the 5 ela metrics considered see section 2 the above analyses were conducted using the university of adelaide s supercomputing facilities which consist of 48 skylake nodes with 80 cpus and 377 gb of memory per node the r package validann humphrey et al 2017 is used for mlp development latin hypercube sampling generated using the lhs package in r is used for sample generation and the r package flacoo kerschke and trautmann 2016 is used for the calculation of the 5 fitness landscape metrics 3 2 case studies as outlined above the case studies used in this paper are the kentucky runoff murray salinity myponga chlorine and sa turbidity cases the objective of the kentucky runoff case study is to forecast runoff in the kentucky river catchment one day in advance based on previous daily observations of effective rainfall and runoff the objective of the murray salinity case study is to forecast salinity in the river murray at murray bridge 14 days in advance to enable the salinity of the water that is pumped from this location to adelaide about 100kms away to be minimized as salt is a conservative substance and transported from upstream daily salinity data from two upstream locations mannum and waikerie are considered as potential inputs the objective of the myponga chlorine case study is to forecast free chlorine levels in a water transmission pipeline 24 h in advance to assist with upstream chlorine dosage control for water disinfection purposes potential inputs include upstream chlorine dosage levels past values of free chlorine at the location of interest and past values of water temperature as these affect the rate of chlorine decay the purpose of the sa turbidity case study is to predict the treated water quality filtered water turbidity in response to raw water quality based on five quality parameters and applied treatment level alum dose it should be noted that as this is a prediction case study rather than a forecasting case study there are no lagged values further information on the data sets used for these case studies is provided in table 1 the input selection and data splitting undertaken by wu et al 2013 is used with the calibration datasets used as the basis for ela metric calculation 3 3 mlp models and data transformations the inputs and outputs of all cases used in this paper are summarized in table 2 inputs x are transformed to the standard normal distribution x n 0 1 and outputs are scaled linearly between 0 1 and 0 9 in order to be aligned with humphrey et al 2017 the hyperbolic tangent function is used as the mlp transfer function between input nodes and hidden nodes and a linear function is used between hidden nodes and output nodes 4 results and discussion the results for each of the 5 ela metrics are given in sections 4 1 to 4 5 followed by a summary and discussion of the results in section 4 6 4 1 mean pairwise convexity deviation the results of the mean pairwise convexity deviation are shown in fig 2 the results represent the deviation between the fitness value on the error surface from that of a linear regression line so that negative results indicate that the error surface is positively convex the results show that the error surfaces of the mlp models become more convex as the number of hidden nodes increases except for the zero hidden node cases where mlps have a linear structure i e the hyperbolic tangent transformation from the hidden layer is not utilized this trend is likely attributed to the increased ability of the higher dimensional anns to fit the data more accurately i e the increase in the degrees of freedom lead to better defined regions of high performing parameter values with the zero hidden node case being an anomaly resulting from the linear model having a well defined optimum despite being a poor performing model structure as a result the rmse error metric used in this paper transferred the linear model to a quadratic error surface with a well defined optimum giving the error surface a level of convexity which is better than the corresponding 1 hidden node mlps this increase in convexity indicates a change of error surface structure as illustrated in fig 2 c this is a key result as it shows that models that have more hidden nodes have an advantage over models with fewer hidden nodes as a more convex structure can provide clearer gradient information to guide the search through the calibration process however the decreased calibration difficulty of models with more hidden nodes due to this increase in convexity is potentially counteracted by the increased calibration difficulty resulting from an increase in the dimension of the error surface nevertheless the increase in convexity with an increase in the number of hidden nodes could explain why optimization algorithms can still find good solutions for models with a large number of hidden nodes and consequently parameters even though the problems are more highly dimensional as the number of hidden nodes increases so does the number of parameters and so a similar pattern is observed in fig 2 b as in fig 2 a in interpreting this figure it is important to note that the kentucky runoff and murray salinity cases represent smaller mlps with only two inputs in comparison to the larger sa turbidity case 5 inputs and the largest myponga chlorine case 10 inputs it is seen in fig 2 b that for a given number of parameters i e error surface dimension the smaller mlps are more convex than the larger ones implying that increasing the number of inputs can serve to reduce the convexity of an mlp s error surface making it more difficult to calibrate this highlights the potential importance of using formal input variable selection ivs algorithms for identifying the smallest number of inputs that have a significant impact on model performance e g galelli et al 2014 4 2 maximum entropy of information content hmax hmax represents how rough or how multi modal the error surface is where a higher hmax refers to a rougher error surface and vice versa fig 3 presents the results of hmax versus the number of hidden nodes fig 3 a and the associated number of parameters fig 3 b for all case studies as shown fig 3 a hmax grows dramatically as the number of hidden nodes increases to larger than 3 and the number of parameters is greater than 15 this increase in hmax indicates an increase in the number of oscillations in and hence the roughness of the error surface as the number of hidden nodes increases as illustrated in fig 3 c however after this initial increase hmax reaches a plateau for numbers of hidden nodes ranging from 3 to 10 this is because the error surfaces of these anns is already extremely rough considering that hmax can reach the theoretical maximum of 1 0 only when all pairs of neighboring symbols consist of differing symbols i e pairs of symbols such as 1 0 1 1 or 1 1 the maximum of hmax is very hard to reach the symbol sequence of practical samples is likely to contain pairs of neighboring symbols with the same symbol which can significantly limit the maximum of the hmax value in practice 4 3 epsilon of information content the epsilon of information content represents the range of the fitness values of the error surface where a big ε indicates a big range in fitness values and a small ε refers to a flatter error surface fig 4 shows the result of the epsilon of information content for all case studies these cases show a clear trend of an increase in surface flatness with an increasing number of hidden nodes evidenced in fig 4 a as illustrated in fig 4 c in this figure it is also seen that models with more inputs have a flatter structure than models with fewer inputs that is compare the high input case of myponga chlorine with the low input cases of kentucky runoff and murray salinity even when the number of parameters is the same for models with a different number of inputs i e consider points intersecting a vertical line in fig 4 b models with more inputs still show a flatter surface than those with a fewer number of inputs this again highlights the importance of the use of formal ivs algorithms as discussed in section 4 1 it should be noticed that theoretically single layer mlps with more hidden nodes should have at least one not worse global optimum than those with fewer hidden nodes however these regions with optimal solutions are considered to be very small compared to the overall error surface as shown in fig 4 c although regions with optimal solutions shown in the red oval in fig 4 c have potentially much lower error than the general regions shown in the black oval in fig 4 c as the size of these optimal regions are small they have limited impact on the overall flatness of the error surface additionally for an increasing number of hidden nodes the highly localized nature of the global optima within these error surfaces means that they become increasingly hard to find 4 4 median basin centroidal distance median basin centroidal distance characterizes the distribution of local basins and associated local optima across the error surface a bigger distance refers to a greater spread of basins and a small distance refers to basins clustered in a small region of the error surface fig 5 presents the results of this metric for all cases the plots for all cases start at a 0 distance as only one optimum can be found for anns with no hidden nodes however for one or more hidden nodes the distance increases almost linearly with an increase in the number of hidden nodes as seen in fig 5 b there is a strong consistency across the cases of basin distance for a given number of parameters indicating the distance is more dependent on the number of parameters regardless of the number of hidden nodes this aids in the interpretation of fig 5 a where it is seen that for a given number of hidden nodes the larger cases myponga chlorine and sa turbidity possess a greater basin distance i e more inputs increase the basin distance an increase in the distance between local optima is considered a disadvantage for optimization as this means algorithms have to explore a larger area on the error surface in order to identify the global optimum from the distributed local optima compared with the cases where the local optima are clustered in a relatively small region see fig 5 c therefore models with many parameters require the use of optimization algorithms with a strong exploration capacity in order to be able to search through the entire space without missing any local optima or pre maturely converging to sub optimal sub regions see maier et al 2019 4 5 median search function evaluations the median search function evaluations metric measures the difficulty of identifying local optima through a gradient based local search a larger number of evaluations indicates that the error surface is more difficult to optimize as shown in fig 6 a as with the median basin centroidal distance the number of evaluations increases near linearly for an increasing number of hidden nodes with the larger cases requiring more evaluations this expected result can be explained by considering fig 6 b where the function evaluations increase near linearly with the mlp parameter number where the increase can be directly attributed to the increases in error surface dimension i e higher dimensional surfaces require more evaluations to find the local and global optima this is illustrated in fig 6 c where basins with local optima of mlps with a smaller number of parameters are relatively small and easy to search whereas those of mlps with larger numbers of parameters are relatively large and spreading resulting in difficulty in exploiting the local optima in the basin 4 6 discussion based on the results presented in sections 4 1 to 4 5 the error surfaces of mlp models become more complex as the number of hidden nodes increases irrespective of case study as illustrated conceptually in fig 7 the key change is that as the number of hidden nodes increases the general shape of the error surface structure becomes flatter overall whilst also becoming rougher with an increasing number of local optima with smaller and deeper basins of attraction dispersed across the entire surface overall the global structure of the error surface was found to be more related to the number of hidden nodes while the features related to multimodality and local basin distribution were found to be related to the number of parameters potential reasons for the above observations could be related to the increase in the degree of freedom associated with mlps with a larger number of hidden nodes which in turn is likely to increase the ability of an ann to fit the data and therefore reduce the model error however further increases in the number of hidden nodes can conversely increase the potential for overfitting which will introduce many local minima and consequently result in a higher degree of roughness additionally an increasing number of hidden nodes may also cause an unnecessarily high degree of freedom within the model structure i e insensitive parameters that have little influence on the error surface which may result in flat regions in the error surface other factors that are likely to affect the observed results include the data and error metric used to construct the error surface as the features of the error surface are a function of the characteristics of the data and error metric for example the length of input data and its statistical characteristics such as the information content or degree of noise of the data are expected to affect the quality of the mlp calibration data with a higher information content i e unique patterns in the data and broader coverage across the input space are expected to result in a more robust connection between inputs and outputs consequently contributing to a less flat and easier to calibrate error surface additionally the selection of error metrics should provide a sensitivity of the features of the error surface as error metrics that are insensitive to the model fitting characteristics are likely to provide a poor quality surface such as a rough error surface with irregular oscillations a flat surface with very little information or a non convex surface in order to test the above hypotheses additional computational experiments would be required including using synthetic datasets with varying lengths and statistical characteristics such as different number of inputs and different level of input interaction coupled with using different error metrics the above findings about the impact of the number of hidden nodes on the features of the error surface are in agreement with the conclusions of maier and dandy 1998a b who found that based on the results of extensive calibration trials this work used the murray salinity case study using mlps with different numbers of inputs and hidden nodes the error surfaces of mlps with more parameters have large plateaus with many local optima that are deep and have steep slopes the existence of many local optima for mlps with a larger number of parameters was also demonstrated by kingston et al 2005 who produced 3 d error surface plots of an ann rainfall runoff model by fixing all connection weights to their optimal values while altering two weights at a time between a set of pre determined limits the above results suggest that it is more difficult to calibrate ann models with more parameters when calibration methods are used that do not have the ability to explore different regions of the error surface widely this is because more models with more parameters have error surfaces with a larger number of local optima in which calibration approaches with low exploratory capability such as gradient based optimization algorithms can become trapped see maier et al 2019 this is why the commonly used back propagation bp and levenberg marquardt lm algorithms have been found to perform relatively poorly on mlp anns with a larger number of parameters in previous studies for example maier and dandy 1996 found that model performance decreased with an increase in the number of model parameters when the bp algorithm was used for calibrating models with different numbers of inputs and hidden nodes for the murray salinity case study similarly piotrowski and napiorkowski 2011 found that the variability in calibration performance increased for calibration trials from different starting positions in model parameter space when a lm algorithm was applied to ann models with a larger number of parameters suggesting increased difficulty in finding better solutions for more models with increased parameters conversely the above results also explain why the relative performance of calibration approaches with a greater ability to explore the search space has been shown to increase for ann models with a larger number of parameters as they have a greater ability to escape local optima and find better regions in the error surface for example for the murray salinity case study maier and dandy 1998a found that the performance of the bp algorithm increased for models with a larger number of parameters when the exploratory ability of the algorithm was improved by increasing the step size used to explore the error surface in addition a number of studies have found that metaheuristics which are known for their increased exploration ability see maier et al 2019 are able to achieve better calibration performance than gradient based methods for anns with a larger number of parameters for example kingston et al 2005 found that a genetic algorithm ga and the complex shuffled complex evolution algorithm outperformed the bp algorithm for a rainfall runoff mlp with a large number of parameters and maroufpoor et al 2020 found that a grey wolf optimization algorithm outperformed the lm algorithm for calibrating mlps with a large number of parameters anns for estimating reference evapotranspiration however although the greater exploratory ability of metaheuristics enables them to find better regions in complex error surfaces because of their decreased exploitative capability they generally have difficulty in finding the bottom of the deep narrow local optima that are a feature of the error surfaces of mlp models with a larger number of parameters this explains why piotrowski and napiorkowski 2011 found that while the average performance of evolutionary algorithms over a number of calibration trials from different starting positions in model parameter space was better than that of the lm algorithm the lm algorithm was able to find the best solutions in individual trials provided the number of starting positions was sufficiently large this also explains why a number of studies have found that a hybrid approach as part of which a metaheuristic is used to find good regions in the error surface that are then used as starting positions for gradient based approaches have been found to result in improved calibration performance of anns with a larger number of parameters for example alavi and gandomi 2011 used simulated annealing sa coupled with a lm algorithm bahrami et al 2016 coupled sa and gas with a lm algorithm and chau 2007 used a split step particle swarm optimization pso algorithm which coupled standard pso and lm algorithms in addition recent advances in calibration of high dimensional anns have yielded a new generation of gradient based algorithms razavi 2021 which arguably make use of their own meta heuristics and can therefore also be considered hybrid algorithms consequently the use of such hybrid algorithms is recommended for the calibration of mlp ann models with a larger number of parameters this highlights the value of approaches that assist with identifying more parsimonious ann models such as input variable selection techniques see galelli et al 2014 model structure selection metrics e g kingston et al 2008 and approaches that help improve the fitness landscape properties of highly parameterized models e g regularization see razavi 2021 guillaume et al 2019 5 summary and conclusions calibration is an important component of the development of any model but is especially critical for anns as the quality of the calibration not only determines values of the unknown model parameters but also the structure of the model and the degree to which underlying system knowledge can be elicited from the calibrated model the success of model calibration is a function of how well suited the optimization algorithm used is to explore the error surface under consideration while there have been many studies comparing the performance of different optimization algorithms existing literature has been largely silent on the properties of the error surface of anns with different structures making it difficult to understand and explain why certain optimization algorithms perform better than others and which optimization approaches are preferred under particular circumstances this paper has addressed this shortcoming by demonstrating that five exploratory landscape analysis ela metrics that have been shown to have low dependence on problem dimensionality and sample size in previous studies can be used to better understand the features of the error surfaces of anns of varying complexity based on the results of four water quantity and quality case studies from the literature kentucky runoff murray salinity myponga chlorine sa turbidity it has been demonstrated that mlps with a smaller number of hidden nodes and parameters are easier to calibrate as they have a more well defined overall shape that is able to guide optimization algorithms to better regions in the error surface more easily additionally the error surface of smaller mlps is smoother so that it is harder for algorithms to be trapped in local optima in contrast the generally flatter error surface of mlps with more parameters and hidden nodes provides limited information to guide the search to better regions in the error surface in addition the higher level of multimodality roughness of larger mlps can also make it more difficult to identify the global optimum especially for optimization algorithms with limited exploration capacity such as gradient based methods on the other hand as error surfaces of larger mlps are more convex than those of smaller mlps which results in better defined gradient information in local regions it should be easier to converge to the local optima of larger mlps however this is also likely to lead to premature convergence to local optima rather than the identification of the global optimum in addition the presence of these widely distributed narrow and deep local optima in the error surfaces of more complex mlps means that hybrid approaches to calibration are likely to result in better performance this is because such approaches use algorithms with higher degrees of exploration such as metaheuristics in the initial stages of the calibration to find good regions in the error surface followed by algorithms with a higher degree of exploitation such as gradient methods in the latter stages of calibration to enable good locally optimal or globally optimal solutions to be identified while the findings of this study highlight the potential of using ela metrics for better understanding the error surfaces of mlps of different complexity for a range of case studies thereby enabling light to be shed on the findings of previous studies further analysis is needed to better understand the reasons for the observed changes in the features of error surfaces of single layer mlp models with changes in the number of hidden nodes and with changes in the information content of the data this could be achieved with the aid of additional computational experiments using synthetically generated data with different attributes tests including varying model parameter interactions and sensitivity ranges and tests on the impact of error metrics formal analyses of the convergence of optimization algorithms could also provide a theoretical foundation for interpretation especially in relation to the effect of degrees of freedom further analysis is also required to generalize the results of this study to a broader array of machine learning strategies this would include application of the metrics to anns with different architectures support vector machines generalized regression neural networks recurrent neural networks and different types of deep neural networks in addition the findings of this research open the door to developing evidence based approaches to tailoring optimization methods and parameterizations see wang et al 2020 zheng et al 2017 for calibrating ann models of different types and complexity based on the knowledge of error surface features rather than relying on a brute force approach to using a range of optimization approaches and picking the one that performs best for the problem at hand similarly future work could focus on the determination of relationships between fitness landscape features and methods used for other aspects of ann model development such as input variable selection and the determination of the most appropriate model structure declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first author is supported by an adelaide graduate research scholarship which is gratefully acknowledged the authors would also like to thank joseph guillaume and saman razavi whose comments have helped to improve the quality of this paper significantly appendix a ela metric calculation a1 mean pairwise convexity deviation as discussed in section 2 4 the mean pairwise convexity deviation describes the general global structure of error surface their calculation requires implementation of the following general steps i select random pairs of points x i x j from the total number of samples of the fitness landscape considered to ensure most of the samples are included in the calculation we use n random pairs of points in this study where n is the number of initial samples ii calculate a linear combination of x i x j to select a new point x n between the two points where a1 x n w x i 1 w x j where w is a random number between 0 and 1 calculate the fitness value y n of x n based on the corresponding test function iii calculate the fitness values y i y j of x i x j based on the corresponding test function use linear regression to calculate the approximated linear fitness value y n at x n a2 y n w y i 1 w y j where w is the same w as in ii iv calculate the difference δ between y n and y n by δ y n y n v if δ is negative the landscape between the selected two points is convex providing good gradient information to guide the search in this region of the fitness landscape vi if δ is positive the landscape between the selected two points is not convex providing poor gradient information to guide the search in this region of the fitness landscape vii the average of the calculated n pairwise divisions is used to summarize the general convexity of the error surface a2 maximum entropy and epsilon of information content as discussed in section 2 4 maximum entropy and epsilon of information content describes the roughness and degree of plateau of error surface respectively their calculation requires implementation of the following general steps i sort all samples into a sequence nearest neighboring as part of which the following sample x i 1 of one sample x i is the closest sample to the corresponding by euclidean distance is used as sample sorting method in this paper ii build a symbol sequence by using the following rule a3 i 1 i f y i 1 y i 0 i f y i 1 y i 1 i f y i 1 y i where 0 is the accuracy parameter of the symbol sequence and y i is the fitness value of sample x i it can be seen that is controlled by the value of if is small can be quite sensitive and contain frequent symbol changes in the sequence for example sequence 1 1 1 1 1 if is big on the other hand can be insensitive and contain many 0 values in the sequence for example sequence 0 0 0 0 0 iii calculate the information content h of the sequence based on the definition a4 h a b p ab log 6 p ab where a b 1 0 1 and p ab is the probability that two neighbored symbols a b are different iv build a new sequence by removing all 0 values in and calculate the partial information content m which is defined as a5 m n 1 where n is the length of sequence v the entropy and epsilon are estimated based on h and m respectively the two curves against are shown in fig a1 munoz et al 2015b describes the way to calculate them is shown as following equations a6 h max max h a7 ratio log 10 max m m 0 2 w h e r e m 0 m 0 a3 median basin centroidal distance and median search function evaluations as discussed in section 2 4 median basin centroidal distance and median search function evaluations are able to describe the degree of basin size homogeneity of local optima of error surface their calculation process is presented as below i use a gradient algorithm to find local optima starting from initial samples x s in this study the l bfgs b algorithm zhu et al 1997 was used due to its capacity to setup the range of calculation to avoid the identified local optima being beyond the range of the fitness landscape ii use of hierarchical clustering to cluster identified local optima in i local optima within a given euclidean distance e are included in the same cluster which refers to a corresponding local basin in this study e is 5 of total euclidean distance length of the whole fitness landscape as this distance performs well in distinguishing different clusters without resulting in a computational burden that results in intractability if e is too small a larger number of clusters is likely to be generated by hierarchical clustering which increases complexity and the computational requirements of subsequent calculations iii calculate the centroid x c of each local basin identified in ii based on the local optima in the corresponding basin iv calculate the pairwise distance between x c the median value of all pairwise distances is returned as the result of median basin centroidal distance v when running l bfgs b algorithm for all samples in i record the number of evaluations for each sample to converge to the corresponding local optimum the median number of evaluations for all samples is returned as the result of median search function evaluations 
3197,accurate estimation of water storage estimates is important for the reliable assessment of regional water resources and climate variability the purpose of this study is to generate long term and high resolution water storage estimates based on the world wide water w3 model ten river basins over china are chosen as the study area and the performance of w3 based simulations is evaluated against gldas and grace solutions more importantly w3 based simulations are employed to investigate the occurrence and spatiotemporal evolution of drought events in the yangtze river basin yzrb to evaluate the superiority of the w3 model results suggest that larger amplitude is mainly distributed in southern china for soil moisture which is 3 times larger than that of snow water equivalent the deficiencies in groundwater storage anomalies are highlighted for clsm and w3 models although the correlation is at a high level 0 54 on average between these two simulations additionally the w3 based terrestrial water storage anomalies twsa are superior to other models in the yellow river basin yrb and yzrb against grace observations with the correlation coefficient cc and root mean squared error rmse of 0 51 40 24 mm and 0 81 27 08 mm respectively however all models present poor performances in the haihe river basin hrb with the smallest cc 0 27 on average and largest rmse 113 29 mm on average with respect to characterizing extreme events the w3 model can accurately reflect the spatiotemporal evolution of the 2011 drought event against scpdsi and spei metrics specifically twsa experience severe deficit in the southern yzrb based on the w3 model which is most consistent with two drought metrics and bulletin records the three cornered hat method reveals that larger uncertainties in twsa can be observed in the southeast river basin serb 63 93 mm yzrb 67 29 mm pearl river basin prb 73 52 mm southwest river basin swrb 68 62 mm and the dominant uncertainty resulting from the soil moisture component the water storage estimates generated in this study can lead to better understanding on regional water resources and provide reliable information to elaborate depictions of climate changes keywords grace observations the world wide water model water storage estimates drought event ten river basins 1 introduction terrestrial water storage tws is a critical state variable of the global hydrological cycle affecting regional water balance and energy budgets rodell and famiglietti 2001 hu et al 2022 vertically tws changes represent an integrated change in various water storage components e g soil moisture storage snow water equivalent and groundwater storage gws in recent years the intensive climate changes and anthropogenic activities have caused a tremendous influence on the tws as well as the hydrological cycle globally long et al 2017 zhou et al 2022 therefore it is necessary to master the dynamic characteristics of water storage estimates for human living and ecological protection the launch of grace satellites can provide monthly terrestrial water storage anomalies twsa with unprecedented accuracy by integrating the auxiliary information from hydrological models groundwater storage anomalies gwsa can be further separated from grace observations recent studies have successfully employed grace data to estimate gws changes especially its depletion in many regions e g in india rodell et al 2009 long et al 2016 high plains strassberg et al 2007 longuevergne et al 2010 and north china plain feng et al 2013 yin et al 2020 although reasonable achievements have been made some shortcomings still exist with respect to grace solutions specifically the observations from the grace satellites have data available only since march 2002 which does not meet the requirement for producing baseline tws information for at least 30 years arguez et al 2019 approximately 12 month data gaps exist between grace and its following grace fo missions due to the battery management instrument issues and calibration campaigns sun et al 2020 yang et al 2021 additionally it is impossible to quantify the contribution of individual hydrological components to grace derived twsa without auxiliary information tangdamrongsub et al 2017 hu et al 2019a more importantly the grace footprint is estimated to be 200 000 km2 based on the inherent resolution as a result of the satellite elevation long et al 2015 the coarser spatial resolution greatly restricts the extensive applications of grace solutions in local scale water resources management an alternative measure to investigate tws changes is based on global hydrological models ghms sutanudjaja et al 2018 müller schmied et al 2021 and land surface models lsms kumar et al 2017 hu et al 2021 which integrate some atmospheric and surface properties with gauged measurements tourian et al 2018 particularly the global land data assimilation system gldas can generate multiple optimal fields of land surface states and fluxes which can be publicly available for long term periods thus gldas based simulations have become the most popular hydrological products widely used for some researches related to runoff peng et al 2016 soil moisture spennemann et al 2015 and precipitation chen et al 2020 however these kinds of models belong to lsms thus the groundwater storage component is not included additionally these models are developed on a global scale so it may remain a challenge when applied to local scale regions several newly hydrological models e g pcr globwb sutanudjaja et al 2018 wghm müller schmied et al 2021 have been developed to generate high resolution water storage estimates and the significant improvement lies in that anthropogenic activities are also taken into account within these models unfortunately the water consumption data are provided by official agencies at annual and provincial scales which are required to be interpolated into daily and gridded datasets for the model operation wang et al 2021 large uncertainties may occur during the process of hydrological simulations especially in areas with intensive human activities also these outputs are updated after a long time which cannot meet the requirements of real time research with the development of machine learning it proves to be an effective measure to overcome these above limitations of grace data by combing hydrological models and deep learning algorithms previous studies have demonstrated that the modeled water storage estimates play the dominant influencing factor on the variations in tws with respect to the short lifetime sun et al 2020 employed three kinds of machine learning models that is deep neural network multiple linear regression and seasonal autoregressive integrated moving average with exogenous variable to reconstruct the missing monthly data at a grid cell scale by coming three forcing data namely precipitation temperature and noah simulated tws li et al 2021 present a new global reconstruction of long term 1979 2020 twsa fields by combing machine learning meteorological variables and the highly correlated hydrological variables e g soil moisture with regard to the coarse spatial resolution wan et al 2015 developed a land surface model based downscaling approach to disaggregate the monthly grace equivalent water thickness data to daily 0 125 values zhong et al 2021 present a two step method for downscaling grace solutions from their original spatiotemporal resolution to a high resolution daily 5 km by combining lsm simulated high spatiotemporal resolution twsa estimates particularly the dynamical downscaling da approaches can downscale grace based estimates by combing hydrological models and data assimilation methods many studies tangdamrongsub et al 2018 tian et al 2019 have certificated that the da methods have the benefit of downscaling the resolution of grace products as the model scale as well as improving the accuracy of water storage estimates therefore it is great necessary to achieve various water storage estimates with long term period and higher spatial resolution employed as the predictor of deep learning models for the first time this study utilizes the w3 model to produce daily hydrological simulations at a 0 25 0 25 resolution giving credit to the unique advantages of the w3 model specifically the groundwater storage and surface water storage components are considered within the w3 model van dijk et al 2010 moreover the spatial resolution of the w3 model is higher than that of most lsms which is more applicable for research in local scale regions compared to ghms based simulations the w3 model code is publicly available and researchers can extend the period of products as long as they need also the accuracy of w3 simulations is satisfactory and its forcing data are relatively easy to obtain khaki et al 2018 2020 some studies long et al 2017 mehrnegar et al 2020 pointed out that the ensemble of multiple models has the potential to improve the accuracy of water storage estimates because no single model is consistently superior to other global models therefore the w3 model can provide alternative hydrological simulations to better understand regional water resources and enrich auxiliary datasets for grace improvement the purpose of this study is not only to generate various water storage estimates based on the w3 model but also to apply these simulations to investigate the extreme climate event so as to evaluate the superior of the w3 model relative to lsms to achieve the goal this paper is organized as follows section 2 describes the detailed information of the study area section 3 presents a brief description of datasets and different kinds of physical models and methods section 4 presents the spatiotemporal characteristics of various water storage estimates from hydrological models and grace observations section 5 investigates the potential of w3 simulations in detecting extreme events 2 study area china is a vast territory with different terrain elevations and exhibits a wide range of climates and topography generally northeastern china belongs to the monsoon climate and a continental climate governs northwestern china while a plateau alpine climate in the qinghai tibet plateau yin et al 2021 the precipitation in the northern basins is less with the annual amount below 600 mm except for the liaohe river basin moreover water resources are insufficient and significantly unstable as for southern basins the precipitation is abundant greater than800 mm with small variability and water resources are relatively sufficient and stable in southern basins chen et al 2005 with the reduction of altitude the mean annual temperature increases from 4 to 20 and evapotranspiration varies from a maximum in southeast china to a minimum in the northwest ranging from 18 to 1400 mm gao et al 2019 huang et al 2019 to better study the spatiotemporal characteristics of water storage estimates fig 1 shows the distribution of 10 river basins rbs within the whole china namely songhua river basin srb liaohe river basin lrb haihe river basin hrb yellow river basin yrb huaihe river basin hhrb southeast river basin serb yangtze river basin yzrb pearl river basin prb southwest river basin swrb and northwest river basin nwrb respectively 3 data and methods 3 1 datasets 3 1 1 grace solutions in this study monthly rl06 grace mascon mass concentration products were used to characterize the spatiotemporal variations of twsa which are provided by the centre for space research csr m https www csr utexas edu grace and the jet propulsion laboratory jpl m https grace jpl nasa gov notably the grace mission ended its operation in october 2017 after more than 15 years in orbit and its successor the grace follow on grace fo mission was launched in may 2018 although the mascon solutions greatly reduced leakage errors from land to ocean the gain factors provided by jpl were applied to the jpl m data to account for leakage errors the uncertainties were officially provided along with the twsa data and the uncertainty in csr m solutions was assumed to be 2 cm as suggested by csr zheng et al 2020 the cubic spline interpolation method was used to fill the data gaps during the grace satellites period 3 1 2 gldas products gldas drives multiple offline land surface models integrates a huge quantity of observation based data and executes globally at high resolutions enabled by the land information system lis zhong et al 2020 the gldas version 2 1 models are utilized in this study which drive four lsms including noah catchment clsm the community land model clm and the variable infiltration capacity vic it can produce various water storage estimates covering the period from 2000 to the present and the monthly products are generated through temporal averaging of the 3 hourly products rui 2014 2018 currently the noah model can provide hydrological outputs with the spatial resolution of 0 25 and 1 while only 1 resolution products are available with regard to other models as indicated in table 1 3 1 3 drought index the palmer drought severity index pdsi belongs to a standardized drought index that utilizes readily available precipitation and temperature data to evaluate relative dryness the original pdsi is restricted by fixed climate weighting factors and duration factors which makes it not comparable under different climate regions yang et al 2018 correspondingly the self calibrated palmer drought severity index scpdsi automatically calibrates the behavior of the index at any location by replacing empirical constants in the index computation with dynamically calculated values so it can realize the comparison of the drought assessment results in different regions the scpdsi datasets provided by the cru https crudata uea ac uk cru data drought were employed in this study with a spatial resolution of 0 5 like the pdsi index the standardized precipitation evapotranspiration index spei also considers the influence of potential evapotranspiration on drought severity moreover the inclusion of temperature along with precipitation data allows spei to account for the impact of temperature on a drought situation in this study the meteorological data precipitation and potential evapotranspiration provided by the cru ts 4 05 https crudata uea ac uk cru data hrg were used to calculate the spei at a 0 5 resolution during the period from 1901 to 2020 the calculation of spei can be summarized as three steps 1 calculating the difference d of precipitation and potential evapotranspiration at different timescales 2 calculating the cumulative distribution function cdf of log logistic for d series 3 transforming cdf to standard normal variables vicente serrano et al 2010 the spei at 6 month timescale stands for the hydrological drought which is closely related to human living and employed in this study the development of a unified drought classification standard is convenient for researchers to compare the performance of different drought indexes based on previous studies sun et al 2018 wu et al 2021 the severity of drought can be divided into five levels as shown in table 2 3 2 methods the world wide water w3 model was developed in this study to generate multiple water storage components with higher resolution and longer period grace derived twsa were utilized to evaluate the performance of modeled estimates over 10 rbs and the uncertainties in twsa and its contained components were quantified based on the three cornered hat tch method more importantly the w3 based simulations were further employed to characterize the spatiotemporal evolution of the drought event in the yzrb where better performance was achieved by the w3 model two different kinds of drought indices were used as the reference to investigate the accuracy of the w3 model gldas simulations and grace observations 3 2 1 world wide water model the w3 model is a global water balance model which can estimate daily water balance dynamics and water related vegetation characteristics van dijk et al 2013 vertically w3 simulated twsa is the integration of surface water soil moisture groundwater snow and vegetation water storage specifically the soil water storage is partitioned into three layers individual stores which are top layer so shallow root layer ss and deep root layer sd in equivalent water height respectively thickness can be estimated based on the proportion of field capacity water storage to the available water content thus the thickness of the top layer soil in w3 is 5 10 cm and the shallow and deep root soil layers have an estimated thickness between 10 and 21 cm and 3 6 m respectively tian et al 2017 the groundwater storage sg is considered based on a simple model with recharge from deep drainage capillary rise evaporation from groundwater saturated areas and discharge into streams the groundwater withdrawal module is currently not available in the w3 model the w3 model is developed on the basis of the awra l model and the detailed descriptions can be found in van dijk 2010 it should be noticeable that the w3 groundwater component is likely inaccurate due to the simplified generalization of the groundwater i e the lack of a groundwater consumption module and the simplified generalization of the groundwater component in the w3 model in this study seven kinds of variables were downloaded from era interim to run the w3 model at a daily scale generating the 1979 2019 water storage estimates with a spatial resolution of 0 25 specifically these meteorological variables included precipitation maximum temperature minimum temperature surface pressure surface solar radiation downwards wind speed and albedo respectively the w3 parameters used in this study were obtained from a default configuration of the awra l version 0 5 van dijk 2010 3 2 2 three cornered hat method the tch method premoli and tavella 1993 has been widely used to estimate the relative uncertainties of grace observations without any prior knowledge at basin and grid scale thus employed in this study assuming that a given set of observations obsi consist of two components the true value xtrue and an associated measurement error ε i 1 o b s i x true ε i i 1 2 n practically the true estimates of xtrue are unavailable so any observation sequences are chosen arbitrarily as the reference to calculate the difference between n 1 processing center and the reference values 2 y in o b s i o b s n ε i ε i i 1 2 n 1 the discrepancies among different n 1 solutions are concatenated in the following matrix with the dimension of m n 1 3 y m n 1 y m 1 y m 2 y m n 1 y 11 y 12 y 1 n 1 y 21 y 22 y 2 n 1 y m 1 y m 2 y m n 1 where m stands for the number of months the covariance matrix of the difference sequences is expressed as follows 4 s cov y s 11 s 12 s 1 n 1 s 21 s 22 s 2 n 1 s n 1 1 s n 1 2 s n 1 n 1 where cov represents the covariance operator sij stands for either a variance estimate i j or a covariance estimate i j an unknown noise covariance matrix r n n is introduced and the relationship between r and s is expressed as galindo and palacio 2003 5 s j t r j i u r q q t r nn i u t where i is the identity matrix with the dimension of n 1 n 1 and u is the n 1 vector 1 1 1 t this study utilizes the objection function g provided by galindo and palacio 1999 to obtain a unique solution based on eq 6 and the constrain function is shown in eq 7 6 g q r nn i j n r ij 2 k 2 7 h q r nn r nn q r nn u t s 1 q r nn u k 0 in order to assure that the initial values fulfill the constraints the initial conditions are set as eq 8 which are proposed by torcaso et al 1998 8 r in 0 0 i n r nn 0 2 u t s 1 u 1 after obtaining the free parameters by minimizing eq 6 the remaining unknown parameters of r can also be determined 3 2 3 decomposition of time series data normally water storage changes exhibit seasonal and secular signals a multiple linear regression analysis can be applied to examine the temporal variability of tws yang et al 2017 the regression model used can be expressed as follows 9 s t β 1 β 2 t β 3 sin π t 6 β 4 cos π t 6 β 5 sin π t 3 β 6 cos π t 3 ε where s t is the monthly twsa at month t and β 1 β 2 β 3 β 4 β 5 and β 6 represent the constant offset linear trend annual β 3 and β 4 and semi annual β 5 and β 6 components respectively the model error is expressed by ε the annual amplitude of the time series datasets can be calculated as follows 10 annual a m p l i t u d e β 3 2 β 4 2 3 2 4 evaluation index three indices were employed to evaluate the performance of different models against grace observations including pearson correlation coefficient cc root mean squared error rmse and mean error me respectively specifically the index cc can quantify how model simulations and in situ measurements are temporally correlated while rmse measures how closely the amplitude of predicted values matches the observed values the me index is used to evaluate the underestimates and overestimates of hydrological models relative to in situ measurements the equations to calculate these three metrics are expressed as follows 11 cc i 1 n y i y y i y i 1 n y i y 2 i 1 n o i o 2 12 rmse i 1 n y i o i 2 n 13 me 1 n i 1 n y i o i where o and y represent observations and predicted values respectively n is the length of time series y and o are the mean values of y and o respectively the best fit between simulated results and in situ measurements under ideal conditions would yield rmse 0 mm me 0 mm and cc 1 the overall flowchart of this study is presented in fig 2 4 results 4 1 spatiotemporal characteristics of soil moisture the temporal features of soil moisture storage anomalies smsa are shown in fig 3 over 10 rbs which are provided by clsm noah vic and w3 models respectively similar characteristics are observed in terms of seasonal variations normally peak values emerge between july and september while the lowest values appear in november january however the peak month of the vic model is different from that of other three models in some rbs for instance the vic model reaches the peak value in may for the srb fig 3a while occurring in august for other three models with regard to the variation amplitude the vic model generally shows the largest amplitude over the whole china with the mean value of 29 71 3 69 mm correspondingly the smsa generated by the clsm model are obviously underestimated in all rbs varying from 0 72 0 31 mm nwrb fig 3j to 23 39 1 53 mm swrb fig 3i respectively obvious downtrends are observed in the hhrb fig 3e and swrb with the change rate of 0 94 0 62 mm yr and 2 04 0 27 mm yr respectively for other rbs upward trends are detected by these models during the period from 2000 to 2020 fig 4 shows the spatial distribution of annual amplitude and long term trend of smsa simulated by four different hydrological models with respect to the annual amplitude the clsm model is obviously underestimated relative to other three models with the mean value of 9 09 1 98 mm the spatial distribution of w3 based results is similar to that of noah and vic models smaller amplitude is observed in northern china including the lrb rb 2 hrb rb 3 yrb rb 4 and nwrb rb 10 correspondingly the larger amplitude is mainly distributed in southwest china while some discrepancies exist in terms of the locations where the maximum amplitude emerges specifically larger values are observed between the yzrb rb 7 and prb rb 8 for the noah model while between the yzrb rb 7 and swrb rb 9 for the vic model for the w3 model the largest amplitude is mainly distributed in the swrb which is mainly caused by the differences in the meteorological forcing data in terms of the long term trend the nwrb experiences a significant downtrend for three lsms however the vic model reveals a more serious decreasing trend than the clsm model in the southern nwrb although its spatial distribution is very similar an extra downtrend is detected around the hhrb rb 5 which is also detected in the w3 model with smaller decreasing trends additionally there is a significant surplus in the srb rb 1 which is observed by noah vic and w3 models with the rates of 3 69 0 43 2 38 0 38 and 0 88 0 26 mm yr respectively 4 2 spatiotemporal characteristics of snow water equivalent the temporal variations of snow water equivalent anomalies swea are shown in fig 5 over 10 rbs which are provided by clsm noah vic and w3 models respectively it can be observed that the variation amplitude of swea is much smaller than that of smsa component which is consistent with the conclusions from previous studies yin et al 2017 generally the larger amplitude is observed in the srb fig 5a and swrb fig 5i with the value above 60 mm the peak values are observed in the vic model except for the swrb as shown in fig 5i though these four models have different variation amplitudes the temporal correlations of swea are basically consistent during the study period with the cc above 0 70 on average the amplitude of the w3 model is comparable to that of the vic model and larger than that of the noah and clsm models fig 6 shows the spatial distributions of annual amplitude and long term trend of swea over 10 rbs it can be found that these models experience different spatial patterns larger amplitude in the swrb rb 9 is observed by vic and w3 models while this signal is not detected by other two lsms moreover the peak amplitude of the w3 model is much larger than that of the noah model with regard to the rbs located in mainland china the amplitude is very small with the value of approximately 1 25 0 22 mm overall the annual amplitude of swea is 3 times smaller than that of smsa indicating that the variations in swea are very small significant downtrends in the swea are also distributed in the swrb rb 9 simulated by noah and w3 models which is consistent with the characteristics of annual amplitude the long term trends simulated by three lsms are positive values in the srb rb 1 as shown in fig 6e 6 g in contrast the intensity of the signal is very weak although it is also detected by the w3 model for other rbs the long term trends of swea basically remain zero during the study period 4 3 spatiotemporal characteristics of groundwater storage for the models used in this study the groundwater storage component is only considered in clsm and w3 models notably gwsa estimates are obtained by separating other components including root zone soil moisture snow water equivalent and canopy interception from the clsm simulated twsa it can be concluded from fig 7 that w3 based gwsa are generally smaller than that of the clsm model in northern china e g srb fig 7a and hhrb fig 7e nevertheless the gwsa agreement i e cc between the two results is at a high level 0 54 on average varying from 0 41 yrb fig 7d to 0 68 prb fig 7h as shown in fig 8 the smallest rmse is observed in the nwrb fig 7j with the value of 5 77 mm and the reason is that the variation amplitude is smaller rather than the best consistency in the time series the groundwater component is considered in hydrological models e g w3 mainly from the perspective of water balance as such it is concluded that the accuracy of gwsa estimates is unsatisfactory due to the simplified generalization of the groundwater module particularly previous studies feng et al 2013 huang et al 2015 have identified the gws shrinkage rates based on in situ level measurements particularly in the hrb and hhrb however this signal is not reflected by these two models fig 7c and 7e highlighting the deficiencies of simulated gwsa from clsm and w3 models from the perspective of spatial distribution the clsm simulations indicate the largest annual changes of gwsa in the southern swrb rb 9 which is basically consistent with that of the w3 model however the areas with maximum amplitude in the yzrb rb 7 are different between these two models and the spatial distribution of w3 simulations is more concentrated relative to that of the clsm model moreover the annual amplitude of w3 simulations is generally smaller than that of the clsm model by comparing fig 9 a and 9b with respect to the long term trend the clsm simulations reveal a decreasing trend from northeast to southwest china specifically the gwsa trends simulated by the clsm model are 3 62 0 74 mm yr in the srb rb 1 and the downtrend equals 5 12 0 52 mm yr in the swrb rb 9 in addition there are no significant differences between w3 simulated gwsa values among all rbs as shown in fig 9d more importantly the clsm model detects several areas with obvious decreasing trends e g hhrb rb 5 and swrb rb 9 however this signal is not detected by the w3 model implying the limitation with respect to the w3 simulated gwsa component 4 4 spatiotemporal characteristics of tws anomalies previous studies scanlon et al 2018 yang et al 2020 have evaluated the performance of hydrological models based on grace observations and achieved reasonable performance the comparison of simulated and grace derived twsa series is shown in fig 10 and table 3 the bold numbers denote that the best performance is achieved by corresponding models in terms of cc and rmse generally better consistency can be detected between different results in terms of seasonal variations however there are obvious differences in the long term trend for some rbs for instance grace observations experience sharply decreasing trends especially during the period between 2015 and 2020 in the hrb fig 10c yrb fig 10d and hhrb fig 10e while the trends in simulated twsa remain stable or even positive values the primary reason is that anthropogenic activities are not considered in the lsms and w3 model tangdamrongsub et al 2018 wang et al 2021 according to the statistics in table 3 different models feature marked discrepancies against grace observations throughout the whole china specially the noah model presents the best fit with grace observations in the lrb fig 10b and hhrb fig 10e with the largest pr and smallest rmse correspondingly the w3 model is superior to other models in the yrb fig 10d and yzrb fig 10g against grace observations it is worth noting that all models present poor performance in the hrb with the smallest cc 0 27 on average and largest rmse 113 29 on average additionally the best performance is not obtained by the clsm model for any rbs indicating that the parameters and structure of the model are not applicable to these rbs within china it can be observed that the best metrics cannot be achieved at the same time in some rbs e g srb serb and prb making it difficult to determine the model with the best performance so the index distance between indices of simulation and observation diso proposed by hu et al 2019b is utilized to comprehensively evaluate the superior of different hydrological models it can effectively overcome the limitation in distinguishing performance based on a single index when some models achieve higher accuracy from fixed aspects while poor agreement with respect to other aspects detailed algorithms about the diso index can refer to zhou et al 2021 according to the definition the model with the smallest diso value is considered to perform the best among all models it is calculated that the noah model is superior to other three models in the srb and prb with the smallest diso of 3 02 and 1 82 respectively similarly the w3 model performs the best in the serb with the smallest diso of 1 81 as indicated in fig 11 with regard to the annual amplitude larger values are mainly distributed around the southern rbs for hydrological simulations and grace observations the largest amplitude emerges in the swrb rb 9 particularly the best consistency is observed between w3 simulations and grace observations we can also find that the spatial distribution of lsms and grace observations is more blocky correspondingly w3 based results look more continuous which can better reflect the evolution and distribution of the annual amplitude in different areas fig 12 in terms of the long term trend grace observations feature obvious decreasing trends in the hrb rb 3 swrb rb 9 and nwrb rb 10 the downtrend in the hrb can also be observed in clsm noah and w3 models while obviously underestimated relative to grace observations the main reason for this phenomenon is that anthropogenic activities are not considered in these models and the model parameters are also not calibrated in china grace observations reveal significant positive trends in the middle reaches of the yzrb rb 7 and the signal is only reflected by the clsm model the increasing trends distributed in the nwrb rb 10 are only observed by grace observations particularly the noah model shows the opposite trend in the same area 5 discussions 5 1 detection of extreme events in the yzrb it is acknowledged that several extreme events have occurred in the yzrb during the past decade based on the analysis in section 4 4 the w3 model performs the best in the yzrb against grace observations relative to other lsms hence we intend to further investigate whether the w3 model is superior to other lsms in detecting extreme events previous studies sun et al 2018 wang et al 2020 identified 11 drought events during the period from 2003 to 2013 in this study the latest and serious event emerging between april 2011 and november 2011 is chosen as an example moreover the spatial distribution of scpdsi and spei metrics are applied as the criterion to validate the accuracy of different models and grace observations fig 13 shows the average values of grace observations hydrological simulations and drought index during the drought period obvious differences can be observed in terms of the spatial distribution although hydrological simulations match well with grace observations in time series fig 10 two kinds of drought metrics indicate that the severe water deficit occurs in the southern yzrb which is most consistent with the signals of the w3 model the worst performance can be found in the vic model which is far different from the spatial distribution of two drought metrics fig 13e and 13f concerning grace observations only small scale areas of the southern yrzb feature a weak downtrend and clear discrepancies exist in the central part of the yzrb relative to that of drought metrics for the noah model negative values occur in the upper and lower reaches of the yzrb while no drought event is detected based on the drought metrics generally the water deficit distribution based on the w3 model is consistent with the result from drought indexes moreover the spatial distribution of w3 based simulations looks more smooth and continuous and we can better understand the evolution of water storage derived from the w3 model compared to other hydrological models and grace observations to verify the evolution of the drought event more intuitively we plot the distribution of twsa and different drought degrees from april 2011 to november 2011 the sub figures are calculated based on every two months for instance fig 14 a1 d1 represent the mean values of grace observations w3 simulations scpdsi and spei between april 2011 and may 2011 similarly the following subfigures are obtained based on the arithmetic average for the subsequent months it can be seen that there are large differences between grace observations and drought indexes specifically grace observations a2 detect the severe deficit in the northeast yzrb during july september and an obvious surplus is distributed along the southeast yzrb however two drought metrics only indicate the drought event in the northeast yzrb while the southeast yzrb suffers from different degrees of drought particularly the extreme drought degree detected by the spei index in contrast the w3 based simulations b2 are basically consistent with the distribution of two drought metrics for instance the middle reaches of the yzrb are covered by severe and extreme drought meanwhile a significant deficit is detected by the w3 model within these areas this is consistent with the actual drought events in the middle and lower reaches of the yzrb recorded in the bulletin of flood and drought disasters in china https zwgk cma gov cn zfxxgk gknr qxbg the spatial distribution of the w3 model matches well with the bulletin records highlighting the ability of the w3 model to monitor extreme hydrological events it should be noted that the spatial distribution of the w3 based twsa is most consistent with that of grace observations while different from that of two drought indexes between october and december a4 d4 the dominant reason is that the drought indices mainly reflect water deficit levels from the perspective of meteorology while grace based twsa deficiency is mainly focused on the runoff soil moisture and groundwater which are related to hydrology and agriculture 5 2 uncertainty analysis the spatial distribution of twsa uncertainties is shown in fig 15 which is calculated based on the tch method overall grace observations show the smallest uncertainties over the whole china fig 15e likewise the clsm suggests smaller uncertainties relative to other hydrological models which may be caused by the smaller amplitude of the clsm based water storage estimates the largest uncertainties can be observed in the noah model as shown in fig 15b moreover larger uncertainties are mainly distributed in southern china especially the middle reaches of the yzrb although the w3 model show larger uncertainties the values are much smaller than that of the noah model larger uncertainties are mainly distributed in the prb and swrb where larger annual amplitude in twsa emerges the uncertainties in twsa and smsa calculated based on the tch method are plotted in fig 16 over 10 rbs the histograms marked with different colors represent the uncertainties in twsa estimates provided by different models and grace observations in 10 rbs the shaded areas stand for the uncertainties in smsa which are the dominant component of the corresponding twsa it can be seen that the noah model shows the largest uncertainties compared to other three models followed by the w3 model as for different rbs larger uncertainties can be observed in the serb yzrb prb and swrb with the value of 63 93 mm 67 29 mm 73 52 mm and 68 62 mm respectively additionally we also calculate the uncertainties of different water storage components which comprise the twsa generally the uncertainties in smsa dominate the larger ratio of the total twsa uncertainties especially for the inland rbs it should be noticeable that the smsa uncertainties are lower in some rbs e g the yzrb the reason for this phenomenon is that the river basin is crowded with water and the frequently extreme events happened in this area so the influence of soil moisture is relatively smaller than that of surface water additionally the nwrb experiences the smallest uncertainties for different water storage estimates as well as different hydrological models 5 3 limitations and future study with regard to the w3 model there are also some limitations required to be improved in the following study specifically restricted by the spatiotemporal resolution of era interim forcing data the generated water storage estimates only cover the period from 1980 to 2019 with the spatial resolution of 0 25 alternative forcing data will be required to extend the time periods for the purpose of real time research additionally the spatial resolution also cannot meet the requirements of local scale water resource management and we will try to improve the resolution from 0 25 to higher resolution e g 0 05 currently the w3 model shows great potential in many regions of china e g the yzrb in order to further improve the accuracy of model prediction the model parameters are required to be calibrated against some in situ measurements what we want to express is that every model has its advantages and disadvantages and no model can be suitable for all regions and cases the generated products based on the w3 model are designed to provide more reliable and alternative data sources for some research related to climate change water resources management and ecological protection 6 conclusions this study aims to produce various water storage estimates with long term period and higher resolution using the w3 model these products are validated against gldas simulations and grace observations in terms of time series annual amplitude and long term trends the tch method is employed to evaluate the uncertainties in various components moreover the produced twsa are also used to analyze the occurrence and evolution of drought events in the yrzb to demonstrate the superiority of the w3 model the main conclusions are as follows 1 similar characteristics are observed with respect to seasonal variations of smsa peak values occur between july and september while the lowest values appear in november january in contrast the smsa generated by the clsm model are obviously underestimated in all rbs varying from 0 72 0 31 mm nwrb to 23 39 1 53 mm swrb respectively the annual amplitude of swea is much smaller than that of smsa which is also applicable to gwsa moreover the temporal correlation shows good consistency although the annual amplitude of gwsa and swea components varies greatly among different hydrological models 2 statistically 4 out of 10 rbs achieve the best performance by the noah model followed by the w3 model and vic model specifically w3 based twsa match well with grace observation in the yrb and yzrb with the cc metrics of 0 51 and 0 81 respectively moreover the significant advantages lie in that the spatial changes of w3 based simulation look more smooth while the grace observations and lsm simulations emerge with abrupt changes between adjacent pixels 3 w3 based simulations can match the drought index better than grace observations and lsms model in both occurrence and evolution of drought events within the yzrb specifically the most severe deficit happens in the southern yrzb for other models it cannot capture the signals large uncertainties can be observed in the swrb yzrb prb and swrb mainly distributed in southern china with values of 63 93 mm 67 29 mm 73 52 mm and 68 62 mm respectively credit authorship contribution statement wenjie yin conceptualization methodology investigation writing original draft shuai yang methodology software formal analysis writing review editing litang hu supervision funding acquisition writing review editing siyuan tian methodology writing review editing xuelei wang formal analysis writing review editing ruxin zhao methodology writing review editing peijun li writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the key r d program of hebei province 21374201d jiangxi provincial key laboratory of water resources and environment of poyang lake jiangxi academy of water science and engineering nanchang 330 029 china grant no 2020gpsys02 state key laboratory of geodesy and earth s dynamics innovation academy for precision measurement science and technology cas wuhan 430077 china grant no sklged2021 1 3 the national natural science foundation of china grant no 41877173 41831283 
3197,accurate estimation of water storage estimates is important for the reliable assessment of regional water resources and climate variability the purpose of this study is to generate long term and high resolution water storage estimates based on the world wide water w3 model ten river basins over china are chosen as the study area and the performance of w3 based simulations is evaluated against gldas and grace solutions more importantly w3 based simulations are employed to investigate the occurrence and spatiotemporal evolution of drought events in the yangtze river basin yzrb to evaluate the superiority of the w3 model results suggest that larger amplitude is mainly distributed in southern china for soil moisture which is 3 times larger than that of snow water equivalent the deficiencies in groundwater storage anomalies are highlighted for clsm and w3 models although the correlation is at a high level 0 54 on average between these two simulations additionally the w3 based terrestrial water storage anomalies twsa are superior to other models in the yellow river basin yrb and yzrb against grace observations with the correlation coefficient cc and root mean squared error rmse of 0 51 40 24 mm and 0 81 27 08 mm respectively however all models present poor performances in the haihe river basin hrb with the smallest cc 0 27 on average and largest rmse 113 29 mm on average with respect to characterizing extreme events the w3 model can accurately reflect the spatiotemporal evolution of the 2011 drought event against scpdsi and spei metrics specifically twsa experience severe deficit in the southern yzrb based on the w3 model which is most consistent with two drought metrics and bulletin records the three cornered hat method reveals that larger uncertainties in twsa can be observed in the southeast river basin serb 63 93 mm yzrb 67 29 mm pearl river basin prb 73 52 mm southwest river basin swrb 68 62 mm and the dominant uncertainty resulting from the soil moisture component the water storage estimates generated in this study can lead to better understanding on regional water resources and provide reliable information to elaborate depictions of climate changes keywords grace observations the world wide water model water storage estimates drought event ten river basins 1 introduction terrestrial water storage tws is a critical state variable of the global hydrological cycle affecting regional water balance and energy budgets rodell and famiglietti 2001 hu et al 2022 vertically tws changes represent an integrated change in various water storage components e g soil moisture storage snow water equivalent and groundwater storage gws in recent years the intensive climate changes and anthropogenic activities have caused a tremendous influence on the tws as well as the hydrological cycle globally long et al 2017 zhou et al 2022 therefore it is necessary to master the dynamic characteristics of water storage estimates for human living and ecological protection the launch of grace satellites can provide monthly terrestrial water storage anomalies twsa with unprecedented accuracy by integrating the auxiliary information from hydrological models groundwater storage anomalies gwsa can be further separated from grace observations recent studies have successfully employed grace data to estimate gws changes especially its depletion in many regions e g in india rodell et al 2009 long et al 2016 high plains strassberg et al 2007 longuevergne et al 2010 and north china plain feng et al 2013 yin et al 2020 although reasonable achievements have been made some shortcomings still exist with respect to grace solutions specifically the observations from the grace satellites have data available only since march 2002 which does not meet the requirement for producing baseline tws information for at least 30 years arguez et al 2019 approximately 12 month data gaps exist between grace and its following grace fo missions due to the battery management instrument issues and calibration campaigns sun et al 2020 yang et al 2021 additionally it is impossible to quantify the contribution of individual hydrological components to grace derived twsa without auxiliary information tangdamrongsub et al 2017 hu et al 2019a more importantly the grace footprint is estimated to be 200 000 km2 based on the inherent resolution as a result of the satellite elevation long et al 2015 the coarser spatial resolution greatly restricts the extensive applications of grace solutions in local scale water resources management an alternative measure to investigate tws changes is based on global hydrological models ghms sutanudjaja et al 2018 müller schmied et al 2021 and land surface models lsms kumar et al 2017 hu et al 2021 which integrate some atmospheric and surface properties with gauged measurements tourian et al 2018 particularly the global land data assimilation system gldas can generate multiple optimal fields of land surface states and fluxes which can be publicly available for long term periods thus gldas based simulations have become the most popular hydrological products widely used for some researches related to runoff peng et al 2016 soil moisture spennemann et al 2015 and precipitation chen et al 2020 however these kinds of models belong to lsms thus the groundwater storage component is not included additionally these models are developed on a global scale so it may remain a challenge when applied to local scale regions several newly hydrological models e g pcr globwb sutanudjaja et al 2018 wghm müller schmied et al 2021 have been developed to generate high resolution water storage estimates and the significant improvement lies in that anthropogenic activities are also taken into account within these models unfortunately the water consumption data are provided by official agencies at annual and provincial scales which are required to be interpolated into daily and gridded datasets for the model operation wang et al 2021 large uncertainties may occur during the process of hydrological simulations especially in areas with intensive human activities also these outputs are updated after a long time which cannot meet the requirements of real time research with the development of machine learning it proves to be an effective measure to overcome these above limitations of grace data by combing hydrological models and deep learning algorithms previous studies have demonstrated that the modeled water storage estimates play the dominant influencing factor on the variations in tws with respect to the short lifetime sun et al 2020 employed three kinds of machine learning models that is deep neural network multiple linear regression and seasonal autoregressive integrated moving average with exogenous variable to reconstruct the missing monthly data at a grid cell scale by coming three forcing data namely precipitation temperature and noah simulated tws li et al 2021 present a new global reconstruction of long term 1979 2020 twsa fields by combing machine learning meteorological variables and the highly correlated hydrological variables e g soil moisture with regard to the coarse spatial resolution wan et al 2015 developed a land surface model based downscaling approach to disaggregate the monthly grace equivalent water thickness data to daily 0 125 values zhong et al 2021 present a two step method for downscaling grace solutions from their original spatiotemporal resolution to a high resolution daily 5 km by combining lsm simulated high spatiotemporal resolution twsa estimates particularly the dynamical downscaling da approaches can downscale grace based estimates by combing hydrological models and data assimilation methods many studies tangdamrongsub et al 2018 tian et al 2019 have certificated that the da methods have the benefit of downscaling the resolution of grace products as the model scale as well as improving the accuracy of water storage estimates therefore it is great necessary to achieve various water storage estimates with long term period and higher spatial resolution employed as the predictor of deep learning models for the first time this study utilizes the w3 model to produce daily hydrological simulations at a 0 25 0 25 resolution giving credit to the unique advantages of the w3 model specifically the groundwater storage and surface water storage components are considered within the w3 model van dijk et al 2010 moreover the spatial resolution of the w3 model is higher than that of most lsms which is more applicable for research in local scale regions compared to ghms based simulations the w3 model code is publicly available and researchers can extend the period of products as long as they need also the accuracy of w3 simulations is satisfactory and its forcing data are relatively easy to obtain khaki et al 2018 2020 some studies long et al 2017 mehrnegar et al 2020 pointed out that the ensemble of multiple models has the potential to improve the accuracy of water storage estimates because no single model is consistently superior to other global models therefore the w3 model can provide alternative hydrological simulations to better understand regional water resources and enrich auxiliary datasets for grace improvement the purpose of this study is not only to generate various water storage estimates based on the w3 model but also to apply these simulations to investigate the extreme climate event so as to evaluate the superior of the w3 model relative to lsms to achieve the goal this paper is organized as follows section 2 describes the detailed information of the study area section 3 presents a brief description of datasets and different kinds of physical models and methods section 4 presents the spatiotemporal characteristics of various water storage estimates from hydrological models and grace observations section 5 investigates the potential of w3 simulations in detecting extreme events 2 study area china is a vast territory with different terrain elevations and exhibits a wide range of climates and topography generally northeastern china belongs to the monsoon climate and a continental climate governs northwestern china while a plateau alpine climate in the qinghai tibet plateau yin et al 2021 the precipitation in the northern basins is less with the annual amount below 600 mm except for the liaohe river basin moreover water resources are insufficient and significantly unstable as for southern basins the precipitation is abundant greater than800 mm with small variability and water resources are relatively sufficient and stable in southern basins chen et al 2005 with the reduction of altitude the mean annual temperature increases from 4 to 20 and evapotranspiration varies from a maximum in southeast china to a minimum in the northwest ranging from 18 to 1400 mm gao et al 2019 huang et al 2019 to better study the spatiotemporal characteristics of water storage estimates fig 1 shows the distribution of 10 river basins rbs within the whole china namely songhua river basin srb liaohe river basin lrb haihe river basin hrb yellow river basin yrb huaihe river basin hhrb southeast river basin serb yangtze river basin yzrb pearl river basin prb southwest river basin swrb and northwest river basin nwrb respectively 3 data and methods 3 1 datasets 3 1 1 grace solutions in this study monthly rl06 grace mascon mass concentration products were used to characterize the spatiotemporal variations of twsa which are provided by the centre for space research csr m https www csr utexas edu grace and the jet propulsion laboratory jpl m https grace jpl nasa gov notably the grace mission ended its operation in october 2017 after more than 15 years in orbit and its successor the grace follow on grace fo mission was launched in may 2018 although the mascon solutions greatly reduced leakage errors from land to ocean the gain factors provided by jpl were applied to the jpl m data to account for leakage errors the uncertainties were officially provided along with the twsa data and the uncertainty in csr m solutions was assumed to be 2 cm as suggested by csr zheng et al 2020 the cubic spline interpolation method was used to fill the data gaps during the grace satellites period 3 1 2 gldas products gldas drives multiple offline land surface models integrates a huge quantity of observation based data and executes globally at high resolutions enabled by the land information system lis zhong et al 2020 the gldas version 2 1 models are utilized in this study which drive four lsms including noah catchment clsm the community land model clm and the variable infiltration capacity vic it can produce various water storage estimates covering the period from 2000 to the present and the monthly products are generated through temporal averaging of the 3 hourly products rui 2014 2018 currently the noah model can provide hydrological outputs with the spatial resolution of 0 25 and 1 while only 1 resolution products are available with regard to other models as indicated in table 1 3 1 3 drought index the palmer drought severity index pdsi belongs to a standardized drought index that utilizes readily available precipitation and temperature data to evaluate relative dryness the original pdsi is restricted by fixed climate weighting factors and duration factors which makes it not comparable under different climate regions yang et al 2018 correspondingly the self calibrated palmer drought severity index scpdsi automatically calibrates the behavior of the index at any location by replacing empirical constants in the index computation with dynamically calculated values so it can realize the comparison of the drought assessment results in different regions the scpdsi datasets provided by the cru https crudata uea ac uk cru data drought were employed in this study with a spatial resolution of 0 5 like the pdsi index the standardized precipitation evapotranspiration index spei also considers the influence of potential evapotranspiration on drought severity moreover the inclusion of temperature along with precipitation data allows spei to account for the impact of temperature on a drought situation in this study the meteorological data precipitation and potential evapotranspiration provided by the cru ts 4 05 https crudata uea ac uk cru data hrg were used to calculate the spei at a 0 5 resolution during the period from 1901 to 2020 the calculation of spei can be summarized as three steps 1 calculating the difference d of precipitation and potential evapotranspiration at different timescales 2 calculating the cumulative distribution function cdf of log logistic for d series 3 transforming cdf to standard normal variables vicente serrano et al 2010 the spei at 6 month timescale stands for the hydrological drought which is closely related to human living and employed in this study the development of a unified drought classification standard is convenient for researchers to compare the performance of different drought indexes based on previous studies sun et al 2018 wu et al 2021 the severity of drought can be divided into five levels as shown in table 2 3 2 methods the world wide water w3 model was developed in this study to generate multiple water storage components with higher resolution and longer period grace derived twsa were utilized to evaluate the performance of modeled estimates over 10 rbs and the uncertainties in twsa and its contained components were quantified based on the three cornered hat tch method more importantly the w3 based simulations were further employed to characterize the spatiotemporal evolution of the drought event in the yzrb where better performance was achieved by the w3 model two different kinds of drought indices were used as the reference to investigate the accuracy of the w3 model gldas simulations and grace observations 3 2 1 world wide water model the w3 model is a global water balance model which can estimate daily water balance dynamics and water related vegetation characteristics van dijk et al 2013 vertically w3 simulated twsa is the integration of surface water soil moisture groundwater snow and vegetation water storage specifically the soil water storage is partitioned into three layers individual stores which are top layer so shallow root layer ss and deep root layer sd in equivalent water height respectively thickness can be estimated based on the proportion of field capacity water storage to the available water content thus the thickness of the top layer soil in w3 is 5 10 cm and the shallow and deep root soil layers have an estimated thickness between 10 and 21 cm and 3 6 m respectively tian et al 2017 the groundwater storage sg is considered based on a simple model with recharge from deep drainage capillary rise evaporation from groundwater saturated areas and discharge into streams the groundwater withdrawal module is currently not available in the w3 model the w3 model is developed on the basis of the awra l model and the detailed descriptions can be found in van dijk 2010 it should be noticeable that the w3 groundwater component is likely inaccurate due to the simplified generalization of the groundwater i e the lack of a groundwater consumption module and the simplified generalization of the groundwater component in the w3 model in this study seven kinds of variables were downloaded from era interim to run the w3 model at a daily scale generating the 1979 2019 water storage estimates with a spatial resolution of 0 25 specifically these meteorological variables included precipitation maximum temperature minimum temperature surface pressure surface solar radiation downwards wind speed and albedo respectively the w3 parameters used in this study were obtained from a default configuration of the awra l version 0 5 van dijk 2010 3 2 2 three cornered hat method the tch method premoli and tavella 1993 has been widely used to estimate the relative uncertainties of grace observations without any prior knowledge at basin and grid scale thus employed in this study assuming that a given set of observations obsi consist of two components the true value xtrue and an associated measurement error ε i 1 o b s i x true ε i i 1 2 n practically the true estimates of xtrue are unavailable so any observation sequences are chosen arbitrarily as the reference to calculate the difference between n 1 processing center and the reference values 2 y in o b s i o b s n ε i ε i i 1 2 n 1 the discrepancies among different n 1 solutions are concatenated in the following matrix with the dimension of m n 1 3 y m n 1 y m 1 y m 2 y m n 1 y 11 y 12 y 1 n 1 y 21 y 22 y 2 n 1 y m 1 y m 2 y m n 1 where m stands for the number of months the covariance matrix of the difference sequences is expressed as follows 4 s cov y s 11 s 12 s 1 n 1 s 21 s 22 s 2 n 1 s n 1 1 s n 1 2 s n 1 n 1 where cov represents the covariance operator sij stands for either a variance estimate i j or a covariance estimate i j an unknown noise covariance matrix r n n is introduced and the relationship between r and s is expressed as galindo and palacio 2003 5 s j t r j i u r q q t r nn i u t where i is the identity matrix with the dimension of n 1 n 1 and u is the n 1 vector 1 1 1 t this study utilizes the objection function g provided by galindo and palacio 1999 to obtain a unique solution based on eq 6 and the constrain function is shown in eq 7 6 g q r nn i j n r ij 2 k 2 7 h q r nn r nn q r nn u t s 1 q r nn u k 0 in order to assure that the initial values fulfill the constraints the initial conditions are set as eq 8 which are proposed by torcaso et al 1998 8 r in 0 0 i n r nn 0 2 u t s 1 u 1 after obtaining the free parameters by minimizing eq 6 the remaining unknown parameters of r can also be determined 3 2 3 decomposition of time series data normally water storage changes exhibit seasonal and secular signals a multiple linear regression analysis can be applied to examine the temporal variability of tws yang et al 2017 the regression model used can be expressed as follows 9 s t β 1 β 2 t β 3 sin π t 6 β 4 cos π t 6 β 5 sin π t 3 β 6 cos π t 3 ε where s t is the monthly twsa at month t and β 1 β 2 β 3 β 4 β 5 and β 6 represent the constant offset linear trend annual β 3 and β 4 and semi annual β 5 and β 6 components respectively the model error is expressed by ε the annual amplitude of the time series datasets can be calculated as follows 10 annual a m p l i t u d e β 3 2 β 4 2 3 2 4 evaluation index three indices were employed to evaluate the performance of different models against grace observations including pearson correlation coefficient cc root mean squared error rmse and mean error me respectively specifically the index cc can quantify how model simulations and in situ measurements are temporally correlated while rmse measures how closely the amplitude of predicted values matches the observed values the me index is used to evaluate the underestimates and overestimates of hydrological models relative to in situ measurements the equations to calculate these three metrics are expressed as follows 11 cc i 1 n y i y y i y i 1 n y i y 2 i 1 n o i o 2 12 rmse i 1 n y i o i 2 n 13 me 1 n i 1 n y i o i where o and y represent observations and predicted values respectively n is the length of time series y and o are the mean values of y and o respectively the best fit between simulated results and in situ measurements under ideal conditions would yield rmse 0 mm me 0 mm and cc 1 the overall flowchart of this study is presented in fig 2 4 results 4 1 spatiotemporal characteristics of soil moisture the temporal features of soil moisture storage anomalies smsa are shown in fig 3 over 10 rbs which are provided by clsm noah vic and w3 models respectively similar characteristics are observed in terms of seasonal variations normally peak values emerge between july and september while the lowest values appear in november january however the peak month of the vic model is different from that of other three models in some rbs for instance the vic model reaches the peak value in may for the srb fig 3a while occurring in august for other three models with regard to the variation amplitude the vic model generally shows the largest amplitude over the whole china with the mean value of 29 71 3 69 mm correspondingly the smsa generated by the clsm model are obviously underestimated in all rbs varying from 0 72 0 31 mm nwrb fig 3j to 23 39 1 53 mm swrb fig 3i respectively obvious downtrends are observed in the hhrb fig 3e and swrb with the change rate of 0 94 0 62 mm yr and 2 04 0 27 mm yr respectively for other rbs upward trends are detected by these models during the period from 2000 to 2020 fig 4 shows the spatial distribution of annual amplitude and long term trend of smsa simulated by four different hydrological models with respect to the annual amplitude the clsm model is obviously underestimated relative to other three models with the mean value of 9 09 1 98 mm the spatial distribution of w3 based results is similar to that of noah and vic models smaller amplitude is observed in northern china including the lrb rb 2 hrb rb 3 yrb rb 4 and nwrb rb 10 correspondingly the larger amplitude is mainly distributed in southwest china while some discrepancies exist in terms of the locations where the maximum amplitude emerges specifically larger values are observed between the yzrb rb 7 and prb rb 8 for the noah model while between the yzrb rb 7 and swrb rb 9 for the vic model for the w3 model the largest amplitude is mainly distributed in the swrb which is mainly caused by the differences in the meteorological forcing data in terms of the long term trend the nwrb experiences a significant downtrend for three lsms however the vic model reveals a more serious decreasing trend than the clsm model in the southern nwrb although its spatial distribution is very similar an extra downtrend is detected around the hhrb rb 5 which is also detected in the w3 model with smaller decreasing trends additionally there is a significant surplus in the srb rb 1 which is observed by noah vic and w3 models with the rates of 3 69 0 43 2 38 0 38 and 0 88 0 26 mm yr respectively 4 2 spatiotemporal characteristics of snow water equivalent the temporal variations of snow water equivalent anomalies swea are shown in fig 5 over 10 rbs which are provided by clsm noah vic and w3 models respectively it can be observed that the variation amplitude of swea is much smaller than that of smsa component which is consistent with the conclusions from previous studies yin et al 2017 generally the larger amplitude is observed in the srb fig 5a and swrb fig 5i with the value above 60 mm the peak values are observed in the vic model except for the swrb as shown in fig 5i though these four models have different variation amplitudes the temporal correlations of swea are basically consistent during the study period with the cc above 0 70 on average the amplitude of the w3 model is comparable to that of the vic model and larger than that of the noah and clsm models fig 6 shows the spatial distributions of annual amplitude and long term trend of swea over 10 rbs it can be found that these models experience different spatial patterns larger amplitude in the swrb rb 9 is observed by vic and w3 models while this signal is not detected by other two lsms moreover the peak amplitude of the w3 model is much larger than that of the noah model with regard to the rbs located in mainland china the amplitude is very small with the value of approximately 1 25 0 22 mm overall the annual amplitude of swea is 3 times smaller than that of smsa indicating that the variations in swea are very small significant downtrends in the swea are also distributed in the swrb rb 9 simulated by noah and w3 models which is consistent with the characteristics of annual amplitude the long term trends simulated by three lsms are positive values in the srb rb 1 as shown in fig 6e 6 g in contrast the intensity of the signal is very weak although it is also detected by the w3 model for other rbs the long term trends of swea basically remain zero during the study period 4 3 spatiotemporal characteristics of groundwater storage for the models used in this study the groundwater storage component is only considered in clsm and w3 models notably gwsa estimates are obtained by separating other components including root zone soil moisture snow water equivalent and canopy interception from the clsm simulated twsa it can be concluded from fig 7 that w3 based gwsa are generally smaller than that of the clsm model in northern china e g srb fig 7a and hhrb fig 7e nevertheless the gwsa agreement i e cc between the two results is at a high level 0 54 on average varying from 0 41 yrb fig 7d to 0 68 prb fig 7h as shown in fig 8 the smallest rmse is observed in the nwrb fig 7j with the value of 5 77 mm and the reason is that the variation amplitude is smaller rather than the best consistency in the time series the groundwater component is considered in hydrological models e g w3 mainly from the perspective of water balance as such it is concluded that the accuracy of gwsa estimates is unsatisfactory due to the simplified generalization of the groundwater module particularly previous studies feng et al 2013 huang et al 2015 have identified the gws shrinkage rates based on in situ level measurements particularly in the hrb and hhrb however this signal is not reflected by these two models fig 7c and 7e highlighting the deficiencies of simulated gwsa from clsm and w3 models from the perspective of spatial distribution the clsm simulations indicate the largest annual changes of gwsa in the southern swrb rb 9 which is basically consistent with that of the w3 model however the areas with maximum amplitude in the yzrb rb 7 are different between these two models and the spatial distribution of w3 simulations is more concentrated relative to that of the clsm model moreover the annual amplitude of w3 simulations is generally smaller than that of the clsm model by comparing fig 9 a and 9b with respect to the long term trend the clsm simulations reveal a decreasing trend from northeast to southwest china specifically the gwsa trends simulated by the clsm model are 3 62 0 74 mm yr in the srb rb 1 and the downtrend equals 5 12 0 52 mm yr in the swrb rb 9 in addition there are no significant differences between w3 simulated gwsa values among all rbs as shown in fig 9d more importantly the clsm model detects several areas with obvious decreasing trends e g hhrb rb 5 and swrb rb 9 however this signal is not detected by the w3 model implying the limitation with respect to the w3 simulated gwsa component 4 4 spatiotemporal characteristics of tws anomalies previous studies scanlon et al 2018 yang et al 2020 have evaluated the performance of hydrological models based on grace observations and achieved reasonable performance the comparison of simulated and grace derived twsa series is shown in fig 10 and table 3 the bold numbers denote that the best performance is achieved by corresponding models in terms of cc and rmse generally better consistency can be detected between different results in terms of seasonal variations however there are obvious differences in the long term trend for some rbs for instance grace observations experience sharply decreasing trends especially during the period between 2015 and 2020 in the hrb fig 10c yrb fig 10d and hhrb fig 10e while the trends in simulated twsa remain stable or even positive values the primary reason is that anthropogenic activities are not considered in the lsms and w3 model tangdamrongsub et al 2018 wang et al 2021 according to the statistics in table 3 different models feature marked discrepancies against grace observations throughout the whole china specially the noah model presents the best fit with grace observations in the lrb fig 10b and hhrb fig 10e with the largest pr and smallest rmse correspondingly the w3 model is superior to other models in the yrb fig 10d and yzrb fig 10g against grace observations it is worth noting that all models present poor performance in the hrb with the smallest cc 0 27 on average and largest rmse 113 29 on average additionally the best performance is not obtained by the clsm model for any rbs indicating that the parameters and structure of the model are not applicable to these rbs within china it can be observed that the best metrics cannot be achieved at the same time in some rbs e g srb serb and prb making it difficult to determine the model with the best performance so the index distance between indices of simulation and observation diso proposed by hu et al 2019b is utilized to comprehensively evaluate the superior of different hydrological models it can effectively overcome the limitation in distinguishing performance based on a single index when some models achieve higher accuracy from fixed aspects while poor agreement with respect to other aspects detailed algorithms about the diso index can refer to zhou et al 2021 according to the definition the model with the smallest diso value is considered to perform the best among all models it is calculated that the noah model is superior to other three models in the srb and prb with the smallest diso of 3 02 and 1 82 respectively similarly the w3 model performs the best in the serb with the smallest diso of 1 81 as indicated in fig 11 with regard to the annual amplitude larger values are mainly distributed around the southern rbs for hydrological simulations and grace observations the largest amplitude emerges in the swrb rb 9 particularly the best consistency is observed between w3 simulations and grace observations we can also find that the spatial distribution of lsms and grace observations is more blocky correspondingly w3 based results look more continuous which can better reflect the evolution and distribution of the annual amplitude in different areas fig 12 in terms of the long term trend grace observations feature obvious decreasing trends in the hrb rb 3 swrb rb 9 and nwrb rb 10 the downtrend in the hrb can also be observed in clsm noah and w3 models while obviously underestimated relative to grace observations the main reason for this phenomenon is that anthropogenic activities are not considered in these models and the model parameters are also not calibrated in china grace observations reveal significant positive trends in the middle reaches of the yzrb rb 7 and the signal is only reflected by the clsm model the increasing trends distributed in the nwrb rb 10 are only observed by grace observations particularly the noah model shows the opposite trend in the same area 5 discussions 5 1 detection of extreme events in the yzrb it is acknowledged that several extreme events have occurred in the yzrb during the past decade based on the analysis in section 4 4 the w3 model performs the best in the yzrb against grace observations relative to other lsms hence we intend to further investigate whether the w3 model is superior to other lsms in detecting extreme events previous studies sun et al 2018 wang et al 2020 identified 11 drought events during the period from 2003 to 2013 in this study the latest and serious event emerging between april 2011 and november 2011 is chosen as an example moreover the spatial distribution of scpdsi and spei metrics are applied as the criterion to validate the accuracy of different models and grace observations fig 13 shows the average values of grace observations hydrological simulations and drought index during the drought period obvious differences can be observed in terms of the spatial distribution although hydrological simulations match well with grace observations in time series fig 10 two kinds of drought metrics indicate that the severe water deficit occurs in the southern yzrb which is most consistent with the signals of the w3 model the worst performance can be found in the vic model which is far different from the spatial distribution of two drought metrics fig 13e and 13f concerning grace observations only small scale areas of the southern yrzb feature a weak downtrend and clear discrepancies exist in the central part of the yzrb relative to that of drought metrics for the noah model negative values occur in the upper and lower reaches of the yzrb while no drought event is detected based on the drought metrics generally the water deficit distribution based on the w3 model is consistent with the result from drought indexes moreover the spatial distribution of w3 based simulations looks more smooth and continuous and we can better understand the evolution of water storage derived from the w3 model compared to other hydrological models and grace observations to verify the evolution of the drought event more intuitively we plot the distribution of twsa and different drought degrees from april 2011 to november 2011 the sub figures are calculated based on every two months for instance fig 14 a1 d1 represent the mean values of grace observations w3 simulations scpdsi and spei between april 2011 and may 2011 similarly the following subfigures are obtained based on the arithmetic average for the subsequent months it can be seen that there are large differences between grace observations and drought indexes specifically grace observations a2 detect the severe deficit in the northeast yzrb during july september and an obvious surplus is distributed along the southeast yzrb however two drought metrics only indicate the drought event in the northeast yzrb while the southeast yzrb suffers from different degrees of drought particularly the extreme drought degree detected by the spei index in contrast the w3 based simulations b2 are basically consistent with the distribution of two drought metrics for instance the middle reaches of the yzrb are covered by severe and extreme drought meanwhile a significant deficit is detected by the w3 model within these areas this is consistent with the actual drought events in the middle and lower reaches of the yzrb recorded in the bulletin of flood and drought disasters in china https zwgk cma gov cn zfxxgk gknr qxbg the spatial distribution of the w3 model matches well with the bulletin records highlighting the ability of the w3 model to monitor extreme hydrological events it should be noted that the spatial distribution of the w3 based twsa is most consistent with that of grace observations while different from that of two drought indexes between october and december a4 d4 the dominant reason is that the drought indices mainly reflect water deficit levels from the perspective of meteorology while grace based twsa deficiency is mainly focused on the runoff soil moisture and groundwater which are related to hydrology and agriculture 5 2 uncertainty analysis the spatial distribution of twsa uncertainties is shown in fig 15 which is calculated based on the tch method overall grace observations show the smallest uncertainties over the whole china fig 15e likewise the clsm suggests smaller uncertainties relative to other hydrological models which may be caused by the smaller amplitude of the clsm based water storage estimates the largest uncertainties can be observed in the noah model as shown in fig 15b moreover larger uncertainties are mainly distributed in southern china especially the middle reaches of the yzrb although the w3 model show larger uncertainties the values are much smaller than that of the noah model larger uncertainties are mainly distributed in the prb and swrb where larger annual amplitude in twsa emerges the uncertainties in twsa and smsa calculated based on the tch method are plotted in fig 16 over 10 rbs the histograms marked with different colors represent the uncertainties in twsa estimates provided by different models and grace observations in 10 rbs the shaded areas stand for the uncertainties in smsa which are the dominant component of the corresponding twsa it can be seen that the noah model shows the largest uncertainties compared to other three models followed by the w3 model as for different rbs larger uncertainties can be observed in the serb yzrb prb and swrb with the value of 63 93 mm 67 29 mm 73 52 mm and 68 62 mm respectively additionally we also calculate the uncertainties of different water storage components which comprise the twsa generally the uncertainties in smsa dominate the larger ratio of the total twsa uncertainties especially for the inland rbs it should be noticeable that the smsa uncertainties are lower in some rbs e g the yzrb the reason for this phenomenon is that the river basin is crowded with water and the frequently extreme events happened in this area so the influence of soil moisture is relatively smaller than that of surface water additionally the nwrb experiences the smallest uncertainties for different water storage estimates as well as different hydrological models 5 3 limitations and future study with regard to the w3 model there are also some limitations required to be improved in the following study specifically restricted by the spatiotemporal resolution of era interim forcing data the generated water storage estimates only cover the period from 1980 to 2019 with the spatial resolution of 0 25 alternative forcing data will be required to extend the time periods for the purpose of real time research additionally the spatial resolution also cannot meet the requirements of local scale water resource management and we will try to improve the resolution from 0 25 to higher resolution e g 0 05 currently the w3 model shows great potential in many regions of china e g the yzrb in order to further improve the accuracy of model prediction the model parameters are required to be calibrated against some in situ measurements what we want to express is that every model has its advantages and disadvantages and no model can be suitable for all regions and cases the generated products based on the w3 model are designed to provide more reliable and alternative data sources for some research related to climate change water resources management and ecological protection 6 conclusions this study aims to produce various water storage estimates with long term period and higher resolution using the w3 model these products are validated against gldas simulations and grace observations in terms of time series annual amplitude and long term trends the tch method is employed to evaluate the uncertainties in various components moreover the produced twsa are also used to analyze the occurrence and evolution of drought events in the yrzb to demonstrate the superiority of the w3 model the main conclusions are as follows 1 similar characteristics are observed with respect to seasonal variations of smsa peak values occur between july and september while the lowest values appear in november january in contrast the smsa generated by the clsm model are obviously underestimated in all rbs varying from 0 72 0 31 mm nwrb to 23 39 1 53 mm swrb respectively the annual amplitude of swea is much smaller than that of smsa which is also applicable to gwsa moreover the temporal correlation shows good consistency although the annual amplitude of gwsa and swea components varies greatly among different hydrological models 2 statistically 4 out of 10 rbs achieve the best performance by the noah model followed by the w3 model and vic model specifically w3 based twsa match well with grace observation in the yrb and yzrb with the cc metrics of 0 51 and 0 81 respectively moreover the significant advantages lie in that the spatial changes of w3 based simulation look more smooth while the grace observations and lsm simulations emerge with abrupt changes between adjacent pixels 3 w3 based simulations can match the drought index better than grace observations and lsms model in both occurrence and evolution of drought events within the yzrb specifically the most severe deficit happens in the southern yrzb for other models it cannot capture the signals large uncertainties can be observed in the swrb yzrb prb and swrb mainly distributed in southern china with values of 63 93 mm 67 29 mm 73 52 mm and 68 62 mm respectively credit authorship contribution statement wenjie yin conceptualization methodology investigation writing original draft shuai yang methodology software formal analysis writing review editing litang hu supervision funding acquisition writing review editing siyuan tian methodology writing review editing xuelei wang formal analysis writing review editing ruxin zhao methodology writing review editing peijun li writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the key r d program of hebei province 21374201d jiangxi provincial key laboratory of water resources and environment of poyang lake jiangxi academy of water science and engineering nanchang 330 029 china grant no 2020gpsys02 state key laboratory of geodesy and earth s dynamics innovation academy for precision measurement science and technology cas wuhan 430077 china grant no sklged2021 1 3 the national natural science foundation of china grant no 41877173 41831283 
3198,conceptual rainfall runoff models have been used extensively to simulate streamflow in urban catchments however spatiotemporal changes in the impervious surface area as an important land use indicator for urban environment has not been used explicitly as input into conceptual rainfall runoff models in this study spatiotemporal impervious surface fraction isf data were incorporated into two lumped and distributed 1 km models namely simhyd and awbm in an urbanized catchment in southeast queensland australia a sub pixel classification technique was used to derive isf from 6 landsat images between 1988 and 2015 daily rainfall runoff models were calibrated with isf as an input variable to evaluate model performance for different simulation periods four model runs including m1 lumped models with constant isf m2 lumped models with temporally varying isf m3 distributed models with fixed spatial isf and m4 distributed models with spatiotemporal isf and climate data results showed that 1 the sub pixel classification algorithm can be used to accurately derive isf data with a mean absolute error mae ranging from 0 07 to 0 12 for 6 images 2 isf in the catchment has increased gradually from 12 2 to 34 6 from 1988 to 2015 3 the performance of simhyd and awbm were not improved noticeably when distributed climate and isf data were used 4 simulation capability of lumped and distributed simhyd and awbm were improved e g increase in nse from 0 68 to 0 73 and decrease in pbias from 30 2 to 1 2 for lumped simhyd when temporal m2 and or spatiotemporal m4 isf data 5 year were used 5 difference in model performance is small and insignificantly whether annual or 5 yearly isf data were used and 6 annual streamflow would increase by 6 9 7 3 mm equal to 2 9 3 in response to every 1 increase in isf for the catchment tested the methodology developed in this study can be applied for assessment of the effect of urbanisation on regional water balance keywords rainfall runoff models urbanization impervious surface land use landsat images sub pixel classification 1 introduction increase in the impervious surface where water cannot infiltrate into the soil is a major environmental issue of concern due to urban development increased impervious surfaces affect catchment hydrology especially water balance components by modifying the natural vegetation cover zhou et al 2014 impervious surfaces increase can lead to a significant increase in flood peak and catchment discharge and reduction in infiltration and groundwater recharge li et al 2018 miller and hess 2017 wang et al 2020 zhang et al 2018b therefore evaluating the magnitude of the impact of urban development on rainfall runoff relationship in urban catchments will provide critical and valuable information in water resource management activities as an effective tool hydrological models have been used for simulating hydrological processes worldwide sood and smakhtin 2015 amongst several hydrological models conceptual rainfall runoff models have been widely used for water balance modelling hadour et al 2020 zhang et al 2019 as they only use rainfall and potential evapotranspiration pet data and a few parameters to simulate streamflow in the catchment vaze and teng 2011 zhang et al 2016 conceptual rainfall runoff models showed good capability to simulate streamflow in both natural yu and zhu 2015 zhang et al 2013 zhang et al 2009 and urbanized catchments perrin and bouvier 2004 saadi et al 2021 for example saadi et al 2020 demonstrated that conceptual rainfall runoff models e g gr4h and ihacres can reproduce streamflow with acceptable accuracy for urbanized catchments in france and usa conceptual rainfall runoff models have two main limitations first conceptual rainfall runoff models are lumped models so uncertainties associated with the spatial variability of climate and landuse data across a catchment poses major error in simulated streamflow to address this problem spatially averaged climate data representing more accurate rainfall amount over the catchment were used as the climate data input to model segond et al 2007 or the conceptual rainfall runoff model was applied at finer spatial resolution distributed gridded with different climate datasets but with the same parameters for each grid cell vaze et al 2011 however spatially variable climate and landuse impervious surface data over an urban catchment has not been represented to a distributed conceptual rainfall runoff models to examine any possible improvement second most of conceptual rainfall runoff models could not consider explicitly landuse characteristics e g spatiotemporal landuse maps in hydrological relationships used for streamflow simulation chiew et al 2008 saadi et al 2021 a few attempts have been made to incorporate landuse characteristics into conceptual rainfall runoff models saadi et al 2021 has modified a lumped conceptual rainfall runoff model gr4h to incorporate urbanization features impervious surfaces likewise simhyd model a lumped conceptual rainfall runoff model commonly used in australia was also modified so as to consider impervious surface data chiew et al 2002 these studies aimed to examine possible improvement in gr4h and simhyd accuracy by adding urban feature impervious surface data into hydrological processes however the effect of incorporating spatiotemporal impervious surface data on the performance of the conceptual rainfall runoff models has not been examined yet previous studies mostly attempted to show how incorporation of remotely sensed vegetation data into physically based hydrological models can improve their performance donohue et al 2010 wagner et al 2019 zhou et al 2013 however only a few studies attempted to include impervious surface data into hydrological models these studies could be categorized in two groups based on their purposes first remotely sensed impervious surface data are incorporated with physically based hydrological models to assess the impact of urban development on hydrological processes of urbanized catchments liu et al 2021 for example dams et al 2013 investigated changes in water balance components e g surface runoff and infiltration of a catchment in response to increased impervious surfaces using a distributed physically based hydrological wetspa second the performance of distributed physically based hydrological models e g wetspa was evaluated by incorporating spatially remotely sensed impervious surface data berezowski et al 2012 demarchi et al 2012 demonstrated that a physically based distributed model wetspa produces substantially better estimates of peak discharges when landuse characteristics derived from satellite images rather than local land use maps were integrated into the model zhou et al 2010 showed improvement in the performance of a distributed physically based hydrological model dors with incorporation of high resolution remotely sensed isa data in order to provide temporally varying and spatially explicit land use data remotely sensed data derived from high resolution hr and moderate resolution mr satellite images have been used in hydrological studies du et al 2012 yan et al 2016 zhang et al 2018a in comparison with hr images mr images such as landsat and modis have been widely used in environmental studies particularly land use change studies tasumi 2019 yang et al 2020 zheng et al 2020 this is mainly because they provide long term images and are freely available https earthexplorer usgs gov impervious surface data have been extracted mainly from landsat images because they are available for longer period since 1972 than modis images since 2000 zhang and weng 2016 also landsat images provide better spatial resolution 30 m than modis images 500 m to extract impervious surface data from landsat images per pixel e g maximum likelihood support vector machine and decision tree classifier lee and lathrop 2006 and sub pixel artificial neural network and spectral mixing analysis classification approaches were used jia et al 2017 xu et al 2019 in per pixel method each pixel is assigned to only one land cover type while sub pixel methods assign each pixel to one or more land cover types previous studies demonstrated that sub pixel techniques outperform traditional per pixel approaches for estimating impervious surface in urban area in terms of accuracy lu et al 2014 weng 2012 previous studies demonstrated the ability of sub pixel classification algorithms for quantifying the spatiotemporal changes of impervious surface occurred in urban areas sexton et al 2013 for example zhang and weng 2016 showed urban development in the pearl river delta china from 1988 to 2013 using timeseries of landsat images in this study time series of impervious surface fraction isf representing the impervious surface area per unit area derived from landsat images of an urbanized catchment were incorporated into two widely used conceptual rainfall runoff models namely simhyd and awbmu to 1 test whether or not applying conceptual rainfall runoff models at finer spatial resolution using spatial isf and climate data can lead to improvement in the simulation capability at daily time step lumped versus distributed hydrological modelling 2 test whether or not incorporating temporal isf into lumped and spatiotemporal temporal 5 years isf into distributed conceptual rainfall runoff models can lead to improvement in the simulation capability at the daily time step temporal versus constant landuse input 3 test whether or not our findings about model performance improvement are sensitive to rainfall runoff model selection 4 develop a method to assess the effect of future urban expansion on catchment water balance especially streamflow 2 materials and methods 2 1 case study bulimba catchment covers an area of 128 km2 and is located in southeast queensland seq australia fig 1 a between 153 048 to 153 158 longitude and 27 428 to 27 622 latitude bulimba creek is a major tributary of the lower brisbane river and flows from south to north into the brisbane river it flows through some of the suburbs in brisbane city in queensland qld state of australia before discharging into the brisbane river near to moreton bay in this study the upper bulimba catchment 58 km2 where streamflow data were collected were used for hydrological simulation and analysis fig 1b therefore all hydrological and remote sensing analysis was conducted over the upper bulimba catchment fig 1c shows the land use map 2013 of the upper bulimba catchment https data qld gov au accordingly the upper bulimba catchment is mainly covered by intensive land uses e g residential and service areas and a few patches of land as nature reserves or agriculture fig 1c 2 2 extraction of impervious surface 2 2 1 remote sensing data for this research six landsat level 2 l2 and tier 1 t1 images path row 89 79 as medium resolution mr images 30 m were selected to extract impervious surface five landsat 5 tm images recorded in june 1988 june 1993 july 1998 july 2004 and june 2009 and one landsat 8 oli tirs image recorded in june 2015 were used landsat level 2 is radiometrically calibrated and provides the most accurate surface reflectance data as the main input for classification algorithms also landsat images categorized into tier 1 represent lowest geometric errors thus they have been used mainly by previous studies conducting time series analysis at landsat pixel level sexton et al 2013 zhang and weng 2016 to avoid errors related to inter annual landcover change particularly vegetation condition on classification results images with almost similar acquisition date june july were selected 2 2 2 sub pixel classification of landsat images fig 2 shows the overall schematic of the methodology used to retrieve impervious surface data in each landsat pixel in this study impervious surface data are represented by impervious surface fraction isf which shows impervious surface area data per unit area first water pixels related to small water bodies in the case study were removed using modified normalized difference water index mndwi xu 2006 as a good indicator of liquid water which is less sensitive to atmosphere condition bie et al 2020 ndehedehe et al 2020 xie et al 2020 eq 1 1 mndwi r green r swir1 r green r swir1 where rgreen and rswir1 are surface reflectance values in band green band 3 in landsat 8 and band 2 in landsat 5 and shortwave infrared 1 band 6 in landsat 8 and band 5 in landsat 5 of landsat images in this study a threshold value of 0 were represented to define as water pixels second vegetation impervious and soil end members landsat pixels representing a pure landcover type were selected third linear spectral mixture analysis lsma adams et al 1986 algorithm was applied to extract the fraction of vegetation impervious surface and soil in each landsat pixel here lsma algorithm is briefly explained in the lsma it is assumed that the surface reflectance in each band is derived from a linear combination of the surface reflectance of the three end members thus the proportion of the surface reflectance for each end member represents proportions of the area covered by distinct features on the ground eq 2 shows the lsma model 2 r j i 1 n f i r ij e j 3 i 1 n f i 1 where rj fi rij ej are the surface reflectance for band j the fraction of end member i the surface reflectance by the end member i in band j and the unmodeled residual respectively in this study six bands j of landsat including blue green red near infrared niir and two shortwave infrared swir 1 2 and four endmembers i of vegetation low albedo e g dark impervious surface materials high albedo e g commercial industrial transportation areas and soil were used to develop fraction images by the lsma it has been demonstrated that it is quite likely that the endmembers for impervious surfaces are similar to some of the high albedo endmembers such as buildings and roads and low albedo endmembers such as dark impervious surfaces details about high and low albedo spectral behavior can be found in previous studies such as wu and murray 2003 and lu and weng 2006 isf was retrieved from the sum of high albedo and low albedo fraction images in this study since high and low albedo endmembers solely represented the respective impervious surfaces lu et al 2011 ramezani et al 2021 moreover constrained condition is imposed based on eq 3 which means the sum of all fractions must be equal to 1 previous literatures completely explained the lsma method and how to select the landsat bands and end members lu and weng 2004 wu and murray 2003 the steps mentioned above were implemented for six landsat images used in this study like previous studies of analyzing landsat timeseries dams et al 2013 we assumed that the isf in each landsat pixel in the case study area did not decrease during the considered time frame according to this assumption if a pixel is non urban isf 0 in 2015 the corresponding pixel in previous years e g 2010 is non urban as well applying this assumption on six landsat images excludes the risk of introducing errors in the increase in isf between the six images dams et al 2013 2 2 3 accuracy assessment of impervious surface fraction data in this study freely available google earth images in 2015 was used to assess the accuracy of the landsat derived isf data in 200 samples jia et al 2017 joseph et al 2012 each sample contains 9 landsat pixels there was one limitation for validating landsat derived isf data in this study google earth images provided quality images especially after 2000 whereas three of landsat images recorded in 1988 1993 and 1998 to address this problem the samples were selected amongst those landsat pixels which had variation less than 5 in all images pixels with approximately same isf over the time frame thus google earth derived isf data in 2015 in samples were used to validate the corresponding data in mr images in 2015 2009 2004 1998 1993 1988 isf data from google earth images was calculated in each sample by drawing polygons manually in google earth software google earth image acquisition date june 2015 was almost similar to that for landsat images table 1 ultimately isf data derived from landsat in the samples were evaluated by commonly used statistics such as the coefficient of determination r2 and the mean absolute error mae 2 3 runoff modelling 2 3 1 simhyd model simhyd is a conceptual and lumped rainfall runoff model chiew et al 2002 simhyd model has been used in hydrological studies in australia for runoff simulation chiew et al 2008 jones et al 2006 zeng et al 2014 fig 3 shows the current structure of the simhyd model described in rainfall runoff library rrl user guide podger 2004 simhyd estimates daily runoff from rainfall and potential evapotranspiration data it uses nine parameters to adjust generated runoff table 1 detailed description of hydrological processes used in simhyd model could be found in rrl user guide podger 2004 as shown in fig 3 simhyd simulates runoff for pervious surface infiltration excess runoff saturation excess runoff and base flow and impervious surface impervious runoff in simhyd model pervious surface fraction psf parameter is used to separate runoff generated from pervious and impervious surfaces eq 4 4 runoff pervious r u n o f f p s f im p e r v i o u s r u n o f f 1 p s f in the current version of simhyd psf as a parameter is assumed constant over calibration or simulation periods podger 2004 in this study however simhyd was recoded to incorporate psf data as a daily input thus psf data could be represented as a daily in our modified simhyd it enabled us to enter psf changes during the calibration or simulation periods psf data were derived from landsat derived isf data using the following simple equation 5 psf 1 isf 2 3 2 awbmu model australian water balance model awbm is one of the most common rainfall runoff models used in australia boughton 2004 its simulation capability has also been demonstrated in other parts of the world jaiswal et al 2020 kunnath poovakka and eldho 2019 awbm produces daily runoff from daily rainfall and pet three surface storages are used for computation of partial runoff from a catchment podger 2004 these surface storages are mostly considered for pervious surfaces boughton 2004 more details about awbm model can be found in rrl user guide podger 2004 in this study awbm was recoded and modified to consider runoff generated from impervious surfaces a new surface storage was added to the model fig 4 then a parameter cu called impervious threshold representing the capacity of impervious surface storage was added to previous eight parameters of awbm table 2 the parameter au as the impervious surface fraction isf derived from landsat images was allowed to vary over time this modified version of awbm is called awbmu in this study 2 3 3 input data for rainfall runoff models daily rainfall and potential evapotranspiration pet data for the rainfall runoff modeling in the upper bulimba catchment were obtained from silo data source https www longpaddock qld gov au silo silo provides gridded 5 km daily timeseries of several climate variables across australia a sophisticated thin spline interpolation technique which considers rainfall variations with elevation was used to derive rainfall and pet data silo data is the commonly used and most reliable climate data for environmental studies in australia daily streamflow data for the upper bulimba catchment was also obtained from water monitoring information portal wmip of queensland https water monitoring information qld gov au and brisbane city council bcc regarding the availability of isf data derived from landsat images and observed daily runoff data in the case study 33 year period from october 1 1985 to september 30 2018 water year of 1986 2018 was considered for calibration and simulation of simhyd and awbmu models in this study in order to investigate the improvement in simhyd model performance with improved spatial representation of climate inputs and isf data lumped and distributed simhyd and awbmu models were used for the lumped modeling approach daily rainfall and pet data available at 5 km resolution from silo database were arithmetically averaged over the catchment area also overall isf for the upper bulimba catchment derived from landsat images was also used in the lumped models for the distributed modeling approach simhyd and awbmu models were applied to 1 km grid cells fig 5 here daily rainfall and pet data from silo which are available at 5 km were downscaled to 1 km using inverse distance weighting idw interpolation method to input to the distributed models detailed description about idw method can be found in ware et al 1991 isf derived from landsat 30 30 m image was also aggregated over each grid cell 1 1 km and introduced to the distributed models distributed simhyd and awbmu simulated daily runoffs from all the grid cells then they were aggregated without any spatial routing to obtain the modeled daily streamflow for the catchment 2 3 4 calibration of rainfall runoff models lumped and distributed simhyd and awbmu models were calibrated during period 1986 1990 at daily time step note that isf derived in 1988 was assumed to be constant during the calibration period 1986 1990 in this study pest doherty 2005 a model independent parameter estimation package was used for parameter estimation detailed information on structure of pest can be found in the pest user guide https dev sspa pest pantheonsite io documentation as discussed previously impervious surface fraction for simhyd and awbmu model was obtained from landsat image 1988 and incorporated to the models thus the other 8 parameters table 1 for simhyd and the other 9 parameters table 2 were calibrated simhyd and awbmu parameters were estimated by minimizing the sum of squared errors sse 6 sse i 1 n y obs y sim 2 where y obs and y sim represents the daily observed and simulated runoff respectively and n is the total number of days during the calibration period 1986 1990 two more calibration runs were attempted for this study first lumped and distributed simhyd and awbmu models were calibrated using annual isf values to test the effect of a higher temporal resolution of isf on daily simulated flows for this purpose five yearly isf values were linearly interpolated at annual intervals over the period 1986 1990 second the models were calibrated for the period 2013 2018 with the highest measured isf value in 2015 for this calibration n is the total number of days during 2013 2018 the reason for this second calibration is explained in next section 2 3 5 simulation of daily flows daily runoff was simulated by lumped and distributed simhyd and awbmu over the period 1991 2018 to test the impact of spatiotemporal climate and isf data on simulation capability of the lumped and distributed simhyd and awbmu models four model runs m1 m4 were considered table 3 in m1 isf value at catchment used in the calibration periods considered constant and incorporated into lumped simhyd and awbmu models in m2 temporal 5 year isf data incorporated into lumped simhyd and awbmu models in m3 isf data at 1 km spatial resolution used in calibration period considered constant and incorporated into distributed simhyd and awbmu models in m4 spatiotemporal 1 km and 5 year isf data incorporated into distributed simhyd and awbmu models note that calibrated parameter values for lumped and distributed simhyd and awbmu models were used without adjustment to corresponding model runs in simulation periods it is worth noting that isf data retrieved in 1988 1993 1998 2004 2009 and 2015 were assumed to be constant over the period of 1986 1990 1991 1995 1996 2001 2002 2006 2007 2012 2013 2018 respectively two more simulations were also undertaken first the four model runs were carried out over the simulation period 1991 2018 using annual isf values to test the effect of higher temporal resolution annual rather than 5 year of isf data on simulated flows isf values were linearly interpolated at annual intervals over the period 1991 2018 second the four model runs were carried out for the period 1986 2012 with low isf values using parameter values estimated through calibration for the period 2013 2018 these simulations were carried out to further assess the effect of using temporal than constant isf on simulated flows in the catchment 2 3 6 performance evaluation of rainfall runoff models to assess the performance of simhyd and awbmu simulation capability at daily time steps during the calibration and simulation periods performance criteria of the nash sutcliffe coefficient of efficiency nse and percent bias pbias were used in this study the nse ranges from to 1 indicates how well the simulated and observed runoff data fit the 1 1 line eq 7 nse values of 0 75 1 0 65 0 75 0 5 0 65 and 0 5 represent very good good satisfactory and unsatisfactory model performance respectively moriasi et al 2007 the pbias eq 8 measures the average tendency in simulated runoff data in fact pbias demonstrates that the simulated values are lower or higher than the observed runoff data according to eq 8 positive and negative value of pbias indicates overestimation and underestimation of the model bias respectively also zero value of pbias shows the best model simulation performance without bias in brief the model performance is very good good satisfactory and unsatisfactory when the value of pbias is 10 10 15 15 25 and 25 respectively moriasi et al 2007 7 nse 1 i 1 n y sim y obs 2 i 1 n y obs y 2 8 pbias i 1 n y sim y obs i 1 n y obs 100 where y obs y sim are the observed and the simulated daily runoff data respectively y is the mean of the observed runoff data and n is the total number of days over the simulation periods 1991 2018 and 1986 2012 2 3 7 urbanization impact on streamflow in order to assess the impact of isf increase on streamflow two simulation scenarios were considered both scenarios were implemented at the daily timestep during 1991 2018 in both scenarios daily timeseries of observed climate data rainfall and pet over 1991 2018 were used to run simhyd and awbmu models isf derived in 1990 was held constant for the whole period 1991 2018 in the first scenario whereas time varying annual isf over 1991 2018 was used in the second scenarios thus the difference between daily simulated streamflows in two scenarios only resulted from changes in isf that varied annually a regression model was developed between the increase in annual isf and increase in annual streamflow mm simulated by lumped simhyd and awbmu over the simulation period 1991 2018 increase in annual isf disf and increase in annual streamflow dq resulted from two scenarios were calculated using the following equations 9 disf isf isf o 10 dq q m2 q m1 where isfo is the isf value for 1990 and qm2 and qm1 are annual streamflow simulated by lumped rainfall runoff models over the simulation period 1991 2018 using annual isf and constant isf 1990 respectively the difference dq between qm2 and qm1 can therefore be used to indicate the amount of increase in the annual flow due to increases in the isf 3 results and discussion 3 1 accuracy of extracted impervious surface fraction data impervious surface fraction isf values in 200 samples derived from landsat and google earth were assessed in terms of mae and r2 for 6 landsat images recorded in 1988 1993 1998 2004 2009 and 2015 good agreement between observed google earth and modeled landsat isf was observed with r2 ranging from 0 93 to 0 97 table 4 the highest and lowest correlation values were in 2015 and 1998 respectively also the mean absolute error demonstrated good accuracy for the linear spectral mixing analysis lsma method used in this study the lowest and highest accuracy were found for 1988 mae 0 12 and 2015 mae 0 07 respectively in comparison to previous studies using lsma our results demonstrated acceptable accuracy for all years li and lu 2016 for example lsma method used by van de voorde et al 2009 had a mae of 0 13 for a landsat etm however all mae values found in this study were equal or less than 0 12 accuracy of extracted isf data could be affected by several factors selected end members for each image might be the most important factor impacting landsat derived isf data zhang and weng 2016 another reason for lack of accuracy might be related to radiometric and geometric errors in each landsat images lu et al 2011 3 2 spatiotemporal patterns of impervious surface fraction fig 6 shows the trend in overall impervious surface fraction isf over 1988 2015 in the upper bulimba catchment essentially this catchment for this case study experienced a gradual and nearly linear increase in isf over the period isf increased from 0 12 in 1988 to 0 34 in 2015 given the gauged area of 58 km2 the impervious surface area isa has increased from 6 96 km2 to 19 72 km2 over the period 1988 2015 figs 7 and 8 show isf maps in the upper bulimba catchment from 1988 to 2015 these isf maps show significant urban development spatially and temporally in the upper bulimba catchment from 1988 to 2015 two types of urban development i e infill and greenfield development occurred in the upper bulimba catchment some non urban isf 0 areas in 1988 has changed to urban areas indicating that greenfield development occurred fig 7 also it is obvious that isf has increased significantly within the boundary of the urban areas in 1988 infill development approximately 3 1 and 9 7 km2 of total impervious surface area increase 12 8 km2 were related to infill and greenfield development respectively in the case study fig 8 represents aggregated landsat isf data at 1 km grids used for distributed hydrologic simulations 3 3 calibration results calibration was implemented for lumped and distributed 1 km simhyd and awbmu models at daily time step using pest for the period of 1986 1990 isf data derived at the catchment and 1 km grids in 1988 were assumed to be constant 5 year over the calibration period fig 9 shows good match between observed and simulated streamflow values over the calibration model performance during calibration for lumped and distributed simhyd and awbmu over the calibration period is shown in table 5 lumped and distributed simhyd and awbmu models performed very well with nse of 0 75 moriasi et al 2007 low pbias values 10 indicate a performance rating of very good for the calibration period moriasi et al 2007 positive pbias values indicate small overestimation of daily runoff for lumped and distributed simhyd and awbmu models overall in terms of nse values lumped and distributed awbmu showed slightly better performance than lumped and distributed simhyd to estimate daily runoff over the calibration period streamflows less than 7 mm day were overestimated pbias ranges from 19 9 to 34 by lumped and distributed simhyd and awbmu while greater streamflows 7mm day were underestimated pbias ranges from 17 6 to 28 5 in terms of nse simhyd and awbmu performed the same when 1 km distributed isf was used for the calibration period pbias showed slight improvement in performance of simhyd when distributed isf and climate data than lumped inputs were used however pbias increased slightly when distributed inputs were used in awbmu in brief no significant improvement in simhyd and awbmu was noted when distributed isf and climate data were used for the calibration period spatial resolution of rainfall data was always a major problem in hydrological studies affecting hydrological model performance particularly rainfall runoff models vaze et al 2011 vaze and teng 2011 zhang et al 2009 vaze et al 2011 assessed the improvement in australian conceptual rainfall runoff models when they were applied at a higher spatial resolution 0 05 than for the whole catchment lumped spatially distributed rainfall sourced from silo were used to run rainfall runoff models for catchments ranging in size from 50 to 2000 km2 in australia they showed that application of gridded rainfall data led to improve in the performance of australian rainfall runoff models e g simhyd for large catchments 500 km2 only however rainfall runoff models rarely improved for small catchments particularly with catchment areas less than 100 km2 silo climate data derived at 5 km spatial resolution could not represent high rainfall variation across the upper bulimba catchment which has an area of 58 km2 we used idw interpolation method to downscale silo rainfall data into 1 km resolution indicating that the final rainfall data used in distributed simhyd and awbmu models do not show significant variation across the upper bulimba catchment for the upper bulimba catchment the mean annual and maximum daily rainfall during the 1986 2018 period varied between 995 and 1050 mm and 186 and 200 mm respectively over the 58 cells these values indicate low spatial variation in rainfall amounts therefore the observed low rainfall variation might be the main reason why no significant improvement was observed when a spatial varying rainfall data was incorporated into distributed simhyd or awbmu for the upper bulimba catchment with the area of 57 km2 which is consistent with vaze et al 2011 models were also calibrated using annual isf table 5 using annual isf rather than 5 year isf only led to slight overestimation in daily flows for the calibration period while nse values remained unchanged this shows that using higher temporal resolution annual isf rather than 5 year had no strong effects on simulation accuracy of the models for the catchment tested 3 4 simulation results daily runoff was simulated by lumped and distributed simhyd and awbmu models using constant and temporal isf data over the period 1991 2018 summary of performance results for four model runs m1 m4 are presented in table 6 simhyd and awbmu with fixed isf m1 and m3 showed good accuracy in simulating daily runoff based on nse values range from 0 65 to 0 68 moriasi et al 2007 however large values of pbias 25 for m1 and m3 represented noticeable underestimation of streamflow for both models comparison of results derived from model runs m1 and m3 showed that the performance of simhyd and awbmu only improved slightly 1 3 in terms of pbias while nse values decreased 0 02 0 03 this result indicated that incorporating only spatial isf and climate data would not lead to improvement in both model s performance as discussed in section 3 3 low variation in climate data could be the reason for low improvement between m1 and m3 model runs compared to m1 and m3 simulation capability of lumped and distributed simhyd and awbmu models improved significantly when spatiotemporal isf m2 and m4 were applied nse values were increased by 0 05 for simhyd and 0 08 for awbmu in m2 and m4 than m1 and m3 also pbias for simhyd model decreased dramatically from 30 2 in m1 to 1 2 in m3 and 26 8 in m2 to 6 1 in m4 likewise pbias decreased from 27 8 in m1 to 8 7 in m3 and 25 8 in m2 to 8 2 in m4 for awbmu simhyd and awbmu lumped and distributed incorporated by spatiotemporal isf slightly overestimated daily runoff 1 2 to 8 7 for the upper bulimba catchment over the simulation period in terms of nse and pbias values lumped awbmu with temporal isf m2 and lumped simhyd with temporal isf m2 showed the best performance in simulating daily runoff at study catchment for more investigation about the effect of using temporal isf on simulation accuracy of the two rainfall runoff models daily streamflow was also simulated over the period 1986 2012 this time isf derived in 2015 was considered constant over the simulation period 1986 2012 results indicated considerable overestimation pbias 20 of daily streamflow using lumped and distributed simhyd and awbmu when isf in 2015 considered constant over the period 1986 2012 however when temporal decrease in isf from 2015 to 1988 was considered during 1986 2012 pbais decreased from 23 to 2 4 lumped simhyd 20 to 7 lumped awbmu 22 to 1 distributed simhyd and 20 to 6 distributed awbmu these results were in agreement with previous studies incorporated temporal land use data with hydrological models wagner et al 2019 zhou et al 2013 indicated that incorporating temporal leaf area index lai leads to a slight increase 0 01 0 07 in the nse of daily runoff and significant decrease 3 11 in pbias also a slight increase 0 02 0 04 in the nse was indicated by teklay et al 2019 due to use of temporal landuse data such as overland manning s and canopy storage in swat model moreover results in table 6 demonstrated that improvement in simulation capability due to using temporal isf data in an urbanized catchment was not sensitive to model selection as shown in fig 9 most of streamflow values 30 mm day were underestimated by simhyd and awbmu under all model runs m1 m4 having said that streamflow 30 mm day were less underestimated when spatiotemporal isf m2 and m4 than constant isf m1 and m3 were used by both models over simulation period also pbias values for streamflow values 30 mm day derived from simhyd and awbmu in m1 53 and 55 m2 31 and 37 m3 57 and 58 and m4 34 and 41 indicated less underestimation of streamflow values 30 mm day when spatiotemporal isf were used these results showed that using temporal isf than constant isf would lead to improvement in simhyd and awbmu simulation capability for reproducing high streamflow values 30 mm day in the case study fig 10 shows accumulated daily observed and simulated runoff data by simhyd and awbmu model with constant and spatiotemporal isf data m1 m4 over the simulation period results reveal a considerable improvement in accumulated streamflow simulated with both simhyd and awbmu models when spatiotemporal isf data m2 and m4 were applied instead of constant isf data m1 and m3 in other words it is demonstrated that considering temporal isf data in lumped and distributed conceptual rainfall runoff modelling could lead to better estimation of accumulated runoff from a catchment experienced significant urban development it is worth noting that the dramatic increase in observed and simulated runoff in 1996 is due to noticeable increase in observed rainfall in the case study catchment the performance of the lumped and gridded rainfall runoff models incorporated with 5 year and annual isf were also compared nse values for lumped and distributed simhyd and awbm with 5 year isf 0 73 0 73 0 75 and 0 74 and annual isf 0 73 0 73 0 75 and 0 74 showed that nse remained unchanged same as calibration results table 5 when annual isf compared to 5 year isf were used in the models pbias however showed slight increase same as calibration results table 5 when annual isf 1 9 9 2 6 8 and 8 6 compared to 5 year isf 1 2 8 7 6 2 and 8 2 were used the models these results indicated no improvement in terms of nse and slight decrease in simulation accuracy in terms of pbias in the performance of models when annual isf than 5 year isf were considered in rainfall runoff models in this study 3 5 relationship between increases in isf and increase in streamflow fig 11 shows increase in annual isf versus increase in annual streamflow mm respectively simulated by lumped simhyd and awbmu over the simulation period 1991 2018 in order to find the relationship between increase in annual isf and increase in annual streamflow mm year simple linear regression models were developed for simhyd eq 11 and awbmu eq 12 11 dq 6 9 disf r 2 0 95 12 dq 7 3 disf r 2 0 91 according to eq 11 and 12 1 increase in isf would lead to an increase in annual streamflow by 6 9 and 7 3 mm using simhyd and awbmu respectively for the study catchment if we take the mean annual flow of 234 mm using simhyd qm1 and 246 mm using awbmu qm1 as the baseline with a constant isf of 14 4 for 1990 equations 11 and 12 would imply a 2 9 3 increase in streamflow from the baseline for 1 increase in isf for the catchment tested it is worth noting that the empirical relationship between changes in isf and changes in flow applies to the catchment over the period over the period from 1991 to 2018 when the increase in the impervious area was the primary cause for the increase in streamflow overall results mentioned above would help predict increases in streamflow due to urbanization in the case study for future periods 4 conclusion in this study the impact of incorporating spatiotemporal impervious surface fraction isf and climate data on the performance of two conceptual rainfall runoff models simhyd and awbmu in an urbanized catchment was examined the performance of simhyd and awbmu were not improved when they are applied at higher spatial resolution distributed 1 km with spatially varying isf and climate data during calibration and simulation runs for the study catchment tested this is largely due to low spatial variability of climate rainfall and pet data over the relatively small catchment on the other hand integrating spatiotemporal isf into lumped gridded simhyd and awbmu over the simulation period led considerable improvement in their performance for estimating daily streamflow results indicated improvement in nse and pbias values for example pbias decreased noticeably e g from 30 2 to 1 2 and 26 8 to 6 1 for lumped simhyd and awbmu respectively by considering spatiotemporal 5 year or annual isf rather than constant isf over the simulation period our analysis showed that using constant isf measured in 1988 lowest isf for the simulation period 1991 2018 led to considerable underestimation pbias 26 of daily simulated streamflows by the lumped and gridded rainfall runoff models conversely daily simulated streamflows were overestimated pbias 20 when constant isf measured in 2015 highest isf were considered for the simulation period 1986 2012 improvement in flow simulation as a result of using spatiotemporal varying isf in lumped and distributed models were not sensitive to model selection as both simhyd and awbmu showed almost identical performance in terms of nse and pbias in addition regarding the observed time series of rainfall during 1991 2018 it is found that urbanization has increased streamflow in the study catchment every 1 increase in isf would lead to 6 9 7 3 mm or approximately 3 increase in annual streamflow overall our key finding is that incorporating spatiotemporal 5 year or annual isf data into a lumped and distributed rainfall runoff model can significantly improve its model performance in urban catchment tested conceptual models when coupled with time varying data on isf can be used to estimate streamflow accurately and reliably in response to urbanization and to make informed decisions in relation to sustainable land and water planning and management for future periods it is highly recommended that future studies should incorporate temporal annual or 5 yearly changes in landuse especially the impervious fraction which can be readily derived from remote sensing images into hydrological models conceptual or physically based for predicting streamflow for urban catchments credit authorship contribution statement mohammad reza ramezani conceptualization data curation methodology software visualization writing original draft writing review editing bofu yu conceptualization methodology project administration supervision writing original draft writing review editing niloofar tarakemehzadeh data curation methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge griffith university queensland australia for providing financial support through guiprs and guprs scholarships 
3198,conceptual rainfall runoff models have been used extensively to simulate streamflow in urban catchments however spatiotemporal changes in the impervious surface area as an important land use indicator for urban environment has not been used explicitly as input into conceptual rainfall runoff models in this study spatiotemporal impervious surface fraction isf data were incorporated into two lumped and distributed 1 km models namely simhyd and awbm in an urbanized catchment in southeast queensland australia a sub pixel classification technique was used to derive isf from 6 landsat images between 1988 and 2015 daily rainfall runoff models were calibrated with isf as an input variable to evaluate model performance for different simulation periods four model runs including m1 lumped models with constant isf m2 lumped models with temporally varying isf m3 distributed models with fixed spatial isf and m4 distributed models with spatiotemporal isf and climate data results showed that 1 the sub pixel classification algorithm can be used to accurately derive isf data with a mean absolute error mae ranging from 0 07 to 0 12 for 6 images 2 isf in the catchment has increased gradually from 12 2 to 34 6 from 1988 to 2015 3 the performance of simhyd and awbm were not improved noticeably when distributed climate and isf data were used 4 simulation capability of lumped and distributed simhyd and awbm were improved e g increase in nse from 0 68 to 0 73 and decrease in pbias from 30 2 to 1 2 for lumped simhyd when temporal m2 and or spatiotemporal m4 isf data 5 year were used 5 difference in model performance is small and insignificantly whether annual or 5 yearly isf data were used and 6 annual streamflow would increase by 6 9 7 3 mm equal to 2 9 3 in response to every 1 increase in isf for the catchment tested the methodology developed in this study can be applied for assessment of the effect of urbanisation on regional water balance keywords rainfall runoff models urbanization impervious surface land use landsat images sub pixel classification 1 introduction increase in the impervious surface where water cannot infiltrate into the soil is a major environmental issue of concern due to urban development increased impervious surfaces affect catchment hydrology especially water balance components by modifying the natural vegetation cover zhou et al 2014 impervious surfaces increase can lead to a significant increase in flood peak and catchment discharge and reduction in infiltration and groundwater recharge li et al 2018 miller and hess 2017 wang et al 2020 zhang et al 2018b therefore evaluating the magnitude of the impact of urban development on rainfall runoff relationship in urban catchments will provide critical and valuable information in water resource management activities as an effective tool hydrological models have been used for simulating hydrological processes worldwide sood and smakhtin 2015 amongst several hydrological models conceptual rainfall runoff models have been widely used for water balance modelling hadour et al 2020 zhang et al 2019 as they only use rainfall and potential evapotranspiration pet data and a few parameters to simulate streamflow in the catchment vaze and teng 2011 zhang et al 2016 conceptual rainfall runoff models showed good capability to simulate streamflow in both natural yu and zhu 2015 zhang et al 2013 zhang et al 2009 and urbanized catchments perrin and bouvier 2004 saadi et al 2021 for example saadi et al 2020 demonstrated that conceptual rainfall runoff models e g gr4h and ihacres can reproduce streamflow with acceptable accuracy for urbanized catchments in france and usa conceptual rainfall runoff models have two main limitations first conceptual rainfall runoff models are lumped models so uncertainties associated with the spatial variability of climate and landuse data across a catchment poses major error in simulated streamflow to address this problem spatially averaged climate data representing more accurate rainfall amount over the catchment were used as the climate data input to model segond et al 2007 or the conceptual rainfall runoff model was applied at finer spatial resolution distributed gridded with different climate datasets but with the same parameters for each grid cell vaze et al 2011 however spatially variable climate and landuse impervious surface data over an urban catchment has not been represented to a distributed conceptual rainfall runoff models to examine any possible improvement second most of conceptual rainfall runoff models could not consider explicitly landuse characteristics e g spatiotemporal landuse maps in hydrological relationships used for streamflow simulation chiew et al 2008 saadi et al 2021 a few attempts have been made to incorporate landuse characteristics into conceptual rainfall runoff models saadi et al 2021 has modified a lumped conceptual rainfall runoff model gr4h to incorporate urbanization features impervious surfaces likewise simhyd model a lumped conceptual rainfall runoff model commonly used in australia was also modified so as to consider impervious surface data chiew et al 2002 these studies aimed to examine possible improvement in gr4h and simhyd accuracy by adding urban feature impervious surface data into hydrological processes however the effect of incorporating spatiotemporal impervious surface data on the performance of the conceptual rainfall runoff models has not been examined yet previous studies mostly attempted to show how incorporation of remotely sensed vegetation data into physically based hydrological models can improve their performance donohue et al 2010 wagner et al 2019 zhou et al 2013 however only a few studies attempted to include impervious surface data into hydrological models these studies could be categorized in two groups based on their purposes first remotely sensed impervious surface data are incorporated with physically based hydrological models to assess the impact of urban development on hydrological processes of urbanized catchments liu et al 2021 for example dams et al 2013 investigated changes in water balance components e g surface runoff and infiltration of a catchment in response to increased impervious surfaces using a distributed physically based hydrological wetspa second the performance of distributed physically based hydrological models e g wetspa was evaluated by incorporating spatially remotely sensed impervious surface data berezowski et al 2012 demarchi et al 2012 demonstrated that a physically based distributed model wetspa produces substantially better estimates of peak discharges when landuse characteristics derived from satellite images rather than local land use maps were integrated into the model zhou et al 2010 showed improvement in the performance of a distributed physically based hydrological model dors with incorporation of high resolution remotely sensed isa data in order to provide temporally varying and spatially explicit land use data remotely sensed data derived from high resolution hr and moderate resolution mr satellite images have been used in hydrological studies du et al 2012 yan et al 2016 zhang et al 2018a in comparison with hr images mr images such as landsat and modis have been widely used in environmental studies particularly land use change studies tasumi 2019 yang et al 2020 zheng et al 2020 this is mainly because they provide long term images and are freely available https earthexplorer usgs gov impervious surface data have been extracted mainly from landsat images because they are available for longer period since 1972 than modis images since 2000 zhang and weng 2016 also landsat images provide better spatial resolution 30 m than modis images 500 m to extract impervious surface data from landsat images per pixel e g maximum likelihood support vector machine and decision tree classifier lee and lathrop 2006 and sub pixel artificial neural network and spectral mixing analysis classification approaches were used jia et al 2017 xu et al 2019 in per pixel method each pixel is assigned to only one land cover type while sub pixel methods assign each pixel to one or more land cover types previous studies demonstrated that sub pixel techniques outperform traditional per pixel approaches for estimating impervious surface in urban area in terms of accuracy lu et al 2014 weng 2012 previous studies demonstrated the ability of sub pixel classification algorithms for quantifying the spatiotemporal changes of impervious surface occurred in urban areas sexton et al 2013 for example zhang and weng 2016 showed urban development in the pearl river delta china from 1988 to 2013 using timeseries of landsat images in this study time series of impervious surface fraction isf representing the impervious surface area per unit area derived from landsat images of an urbanized catchment were incorporated into two widely used conceptual rainfall runoff models namely simhyd and awbmu to 1 test whether or not applying conceptual rainfall runoff models at finer spatial resolution using spatial isf and climate data can lead to improvement in the simulation capability at daily time step lumped versus distributed hydrological modelling 2 test whether or not incorporating temporal isf into lumped and spatiotemporal temporal 5 years isf into distributed conceptual rainfall runoff models can lead to improvement in the simulation capability at the daily time step temporal versus constant landuse input 3 test whether or not our findings about model performance improvement are sensitive to rainfall runoff model selection 4 develop a method to assess the effect of future urban expansion on catchment water balance especially streamflow 2 materials and methods 2 1 case study bulimba catchment covers an area of 128 km2 and is located in southeast queensland seq australia fig 1 a between 153 048 to 153 158 longitude and 27 428 to 27 622 latitude bulimba creek is a major tributary of the lower brisbane river and flows from south to north into the brisbane river it flows through some of the suburbs in brisbane city in queensland qld state of australia before discharging into the brisbane river near to moreton bay in this study the upper bulimba catchment 58 km2 where streamflow data were collected were used for hydrological simulation and analysis fig 1b therefore all hydrological and remote sensing analysis was conducted over the upper bulimba catchment fig 1c shows the land use map 2013 of the upper bulimba catchment https data qld gov au accordingly the upper bulimba catchment is mainly covered by intensive land uses e g residential and service areas and a few patches of land as nature reserves or agriculture fig 1c 2 2 extraction of impervious surface 2 2 1 remote sensing data for this research six landsat level 2 l2 and tier 1 t1 images path row 89 79 as medium resolution mr images 30 m were selected to extract impervious surface five landsat 5 tm images recorded in june 1988 june 1993 july 1998 july 2004 and june 2009 and one landsat 8 oli tirs image recorded in june 2015 were used landsat level 2 is radiometrically calibrated and provides the most accurate surface reflectance data as the main input for classification algorithms also landsat images categorized into tier 1 represent lowest geometric errors thus they have been used mainly by previous studies conducting time series analysis at landsat pixel level sexton et al 2013 zhang and weng 2016 to avoid errors related to inter annual landcover change particularly vegetation condition on classification results images with almost similar acquisition date june july were selected 2 2 2 sub pixel classification of landsat images fig 2 shows the overall schematic of the methodology used to retrieve impervious surface data in each landsat pixel in this study impervious surface data are represented by impervious surface fraction isf which shows impervious surface area data per unit area first water pixels related to small water bodies in the case study were removed using modified normalized difference water index mndwi xu 2006 as a good indicator of liquid water which is less sensitive to atmosphere condition bie et al 2020 ndehedehe et al 2020 xie et al 2020 eq 1 1 mndwi r green r swir1 r green r swir1 where rgreen and rswir1 are surface reflectance values in band green band 3 in landsat 8 and band 2 in landsat 5 and shortwave infrared 1 band 6 in landsat 8 and band 5 in landsat 5 of landsat images in this study a threshold value of 0 were represented to define as water pixels second vegetation impervious and soil end members landsat pixels representing a pure landcover type were selected third linear spectral mixture analysis lsma adams et al 1986 algorithm was applied to extract the fraction of vegetation impervious surface and soil in each landsat pixel here lsma algorithm is briefly explained in the lsma it is assumed that the surface reflectance in each band is derived from a linear combination of the surface reflectance of the three end members thus the proportion of the surface reflectance for each end member represents proportions of the area covered by distinct features on the ground eq 2 shows the lsma model 2 r j i 1 n f i r ij e j 3 i 1 n f i 1 where rj fi rij ej are the surface reflectance for band j the fraction of end member i the surface reflectance by the end member i in band j and the unmodeled residual respectively in this study six bands j of landsat including blue green red near infrared niir and two shortwave infrared swir 1 2 and four endmembers i of vegetation low albedo e g dark impervious surface materials high albedo e g commercial industrial transportation areas and soil were used to develop fraction images by the lsma it has been demonstrated that it is quite likely that the endmembers for impervious surfaces are similar to some of the high albedo endmembers such as buildings and roads and low albedo endmembers such as dark impervious surfaces details about high and low albedo spectral behavior can be found in previous studies such as wu and murray 2003 and lu and weng 2006 isf was retrieved from the sum of high albedo and low albedo fraction images in this study since high and low albedo endmembers solely represented the respective impervious surfaces lu et al 2011 ramezani et al 2021 moreover constrained condition is imposed based on eq 3 which means the sum of all fractions must be equal to 1 previous literatures completely explained the lsma method and how to select the landsat bands and end members lu and weng 2004 wu and murray 2003 the steps mentioned above were implemented for six landsat images used in this study like previous studies of analyzing landsat timeseries dams et al 2013 we assumed that the isf in each landsat pixel in the case study area did not decrease during the considered time frame according to this assumption if a pixel is non urban isf 0 in 2015 the corresponding pixel in previous years e g 2010 is non urban as well applying this assumption on six landsat images excludes the risk of introducing errors in the increase in isf between the six images dams et al 2013 2 2 3 accuracy assessment of impervious surface fraction data in this study freely available google earth images in 2015 was used to assess the accuracy of the landsat derived isf data in 200 samples jia et al 2017 joseph et al 2012 each sample contains 9 landsat pixels there was one limitation for validating landsat derived isf data in this study google earth images provided quality images especially after 2000 whereas three of landsat images recorded in 1988 1993 and 1998 to address this problem the samples were selected amongst those landsat pixels which had variation less than 5 in all images pixels with approximately same isf over the time frame thus google earth derived isf data in 2015 in samples were used to validate the corresponding data in mr images in 2015 2009 2004 1998 1993 1988 isf data from google earth images was calculated in each sample by drawing polygons manually in google earth software google earth image acquisition date june 2015 was almost similar to that for landsat images table 1 ultimately isf data derived from landsat in the samples were evaluated by commonly used statistics such as the coefficient of determination r2 and the mean absolute error mae 2 3 runoff modelling 2 3 1 simhyd model simhyd is a conceptual and lumped rainfall runoff model chiew et al 2002 simhyd model has been used in hydrological studies in australia for runoff simulation chiew et al 2008 jones et al 2006 zeng et al 2014 fig 3 shows the current structure of the simhyd model described in rainfall runoff library rrl user guide podger 2004 simhyd estimates daily runoff from rainfall and potential evapotranspiration data it uses nine parameters to adjust generated runoff table 1 detailed description of hydrological processes used in simhyd model could be found in rrl user guide podger 2004 as shown in fig 3 simhyd simulates runoff for pervious surface infiltration excess runoff saturation excess runoff and base flow and impervious surface impervious runoff in simhyd model pervious surface fraction psf parameter is used to separate runoff generated from pervious and impervious surfaces eq 4 4 runoff pervious r u n o f f p s f im p e r v i o u s r u n o f f 1 p s f in the current version of simhyd psf as a parameter is assumed constant over calibration or simulation periods podger 2004 in this study however simhyd was recoded to incorporate psf data as a daily input thus psf data could be represented as a daily in our modified simhyd it enabled us to enter psf changes during the calibration or simulation periods psf data were derived from landsat derived isf data using the following simple equation 5 psf 1 isf 2 3 2 awbmu model australian water balance model awbm is one of the most common rainfall runoff models used in australia boughton 2004 its simulation capability has also been demonstrated in other parts of the world jaiswal et al 2020 kunnath poovakka and eldho 2019 awbm produces daily runoff from daily rainfall and pet three surface storages are used for computation of partial runoff from a catchment podger 2004 these surface storages are mostly considered for pervious surfaces boughton 2004 more details about awbm model can be found in rrl user guide podger 2004 in this study awbm was recoded and modified to consider runoff generated from impervious surfaces a new surface storage was added to the model fig 4 then a parameter cu called impervious threshold representing the capacity of impervious surface storage was added to previous eight parameters of awbm table 2 the parameter au as the impervious surface fraction isf derived from landsat images was allowed to vary over time this modified version of awbm is called awbmu in this study 2 3 3 input data for rainfall runoff models daily rainfall and potential evapotranspiration pet data for the rainfall runoff modeling in the upper bulimba catchment were obtained from silo data source https www longpaddock qld gov au silo silo provides gridded 5 km daily timeseries of several climate variables across australia a sophisticated thin spline interpolation technique which considers rainfall variations with elevation was used to derive rainfall and pet data silo data is the commonly used and most reliable climate data for environmental studies in australia daily streamflow data for the upper bulimba catchment was also obtained from water monitoring information portal wmip of queensland https water monitoring information qld gov au and brisbane city council bcc regarding the availability of isf data derived from landsat images and observed daily runoff data in the case study 33 year period from october 1 1985 to september 30 2018 water year of 1986 2018 was considered for calibration and simulation of simhyd and awbmu models in this study in order to investigate the improvement in simhyd model performance with improved spatial representation of climate inputs and isf data lumped and distributed simhyd and awbmu models were used for the lumped modeling approach daily rainfall and pet data available at 5 km resolution from silo database were arithmetically averaged over the catchment area also overall isf for the upper bulimba catchment derived from landsat images was also used in the lumped models for the distributed modeling approach simhyd and awbmu models were applied to 1 km grid cells fig 5 here daily rainfall and pet data from silo which are available at 5 km were downscaled to 1 km using inverse distance weighting idw interpolation method to input to the distributed models detailed description about idw method can be found in ware et al 1991 isf derived from landsat 30 30 m image was also aggregated over each grid cell 1 1 km and introduced to the distributed models distributed simhyd and awbmu simulated daily runoffs from all the grid cells then they were aggregated without any spatial routing to obtain the modeled daily streamflow for the catchment 2 3 4 calibration of rainfall runoff models lumped and distributed simhyd and awbmu models were calibrated during period 1986 1990 at daily time step note that isf derived in 1988 was assumed to be constant during the calibration period 1986 1990 in this study pest doherty 2005 a model independent parameter estimation package was used for parameter estimation detailed information on structure of pest can be found in the pest user guide https dev sspa pest pantheonsite io documentation as discussed previously impervious surface fraction for simhyd and awbmu model was obtained from landsat image 1988 and incorporated to the models thus the other 8 parameters table 1 for simhyd and the other 9 parameters table 2 were calibrated simhyd and awbmu parameters were estimated by minimizing the sum of squared errors sse 6 sse i 1 n y obs y sim 2 where y obs and y sim represents the daily observed and simulated runoff respectively and n is the total number of days during the calibration period 1986 1990 two more calibration runs were attempted for this study first lumped and distributed simhyd and awbmu models were calibrated using annual isf values to test the effect of a higher temporal resolution of isf on daily simulated flows for this purpose five yearly isf values were linearly interpolated at annual intervals over the period 1986 1990 second the models were calibrated for the period 2013 2018 with the highest measured isf value in 2015 for this calibration n is the total number of days during 2013 2018 the reason for this second calibration is explained in next section 2 3 5 simulation of daily flows daily runoff was simulated by lumped and distributed simhyd and awbmu over the period 1991 2018 to test the impact of spatiotemporal climate and isf data on simulation capability of the lumped and distributed simhyd and awbmu models four model runs m1 m4 were considered table 3 in m1 isf value at catchment used in the calibration periods considered constant and incorporated into lumped simhyd and awbmu models in m2 temporal 5 year isf data incorporated into lumped simhyd and awbmu models in m3 isf data at 1 km spatial resolution used in calibration period considered constant and incorporated into distributed simhyd and awbmu models in m4 spatiotemporal 1 km and 5 year isf data incorporated into distributed simhyd and awbmu models note that calibrated parameter values for lumped and distributed simhyd and awbmu models were used without adjustment to corresponding model runs in simulation periods it is worth noting that isf data retrieved in 1988 1993 1998 2004 2009 and 2015 were assumed to be constant over the period of 1986 1990 1991 1995 1996 2001 2002 2006 2007 2012 2013 2018 respectively two more simulations were also undertaken first the four model runs were carried out over the simulation period 1991 2018 using annual isf values to test the effect of higher temporal resolution annual rather than 5 year of isf data on simulated flows isf values were linearly interpolated at annual intervals over the period 1991 2018 second the four model runs were carried out for the period 1986 2012 with low isf values using parameter values estimated through calibration for the period 2013 2018 these simulations were carried out to further assess the effect of using temporal than constant isf on simulated flows in the catchment 2 3 6 performance evaluation of rainfall runoff models to assess the performance of simhyd and awbmu simulation capability at daily time steps during the calibration and simulation periods performance criteria of the nash sutcliffe coefficient of efficiency nse and percent bias pbias were used in this study the nse ranges from to 1 indicates how well the simulated and observed runoff data fit the 1 1 line eq 7 nse values of 0 75 1 0 65 0 75 0 5 0 65 and 0 5 represent very good good satisfactory and unsatisfactory model performance respectively moriasi et al 2007 the pbias eq 8 measures the average tendency in simulated runoff data in fact pbias demonstrates that the simulated values are lower or higher than the observed runoff data according to eq 8 positive and negative value of pbias indicates overestimation and underestimation of the model bias respectively also zero value of pbias shows the best model simulation performance without bias in brief the model performance is very good good satisfactory and unsatisfactory when the value of pbias is 10 10 15 15 25 and 25 respectively moriasi et al 2007 7 nse 1 i 1 n y sim y obs 2 i 1 n y obs y 2 8 pbias i 1 n y sim y obs i 1 n y obs 100 where y obs y sim are the observed and the simulated daily runoff data respectively y is the mean of the observed runoff data and n is the total number of days over the simulation periods 1991 2018 and 1986 2012 2 3 7 urbanization impact on streamflow in order to assess the impact of isf increase on streamflow two simulation scenarios were considered both scenarios were implemented at the daily timestep during 1991 2018 in both scenarios daily timeseries of observed climate data rainfall and pet over 1991 2018 were used to run simhyd and awbmu models isf derived in 1990 was held constant for the whole period 1991 2018 in the first scenario whereas time varying annual isf over 1991 2018 was used in the second scenarios thus the difference between daily simulated streamflows in two scenarios only resulted from changes in isf that varied annually a regression model was developed between the increase in annual isf and increase in annual streamflow mm simulated by lumped simhyd and awbmu over the simulation period 1991 2018 increase in annual isf disf and increase in annual streamflow dq resulted from two scenarios were calculated using the following equations 9 disf isf isf o 10 dq q m2 q m1 where isfo is the isf value for 1990 and qm2 and qm1 are annual streamflow simulated by lumped rainfall runoff models over the simulation period 1991 2018 using annual isf and constant isf 1990 respectively the difference dq between qm2 and qm1 can therefore be used to indicate the amount of increase in the annual flow due to increases in the isf 3 results and discussion 3 1 accuracy of extracted impervious surface fraction data impervious surface fraction isf values in 200 samples derived from landsat and google earth were assessed in terms of mae and r2 for 6 landsat images recorded in 1988 1993 1998 2004 2009 and 2015 good agreement between observed google earth and modeled landsat isf was observed with r2 ranging from 0 93 to 0 97 table 4 the highest and lowest correlation values were in 2015 and 1998 respectively also the mean absolute error demonstrated good accuracy for the linear spectral mixing analysis lsma method used in this study the lowest and highest accuracy were found for 1988 mae 0 12 and 2015 mae 0 07 respectively in comparison to previous studies using lsma our results demonstrated acceptable accuracy for all years li and lu 2016 for example lsma method used by van de voorde et al 2009 had a mae of 0 13 for a landsat etm however all mae values found in this study were equal or less than 0 12 accuracy of extracted isf data could be affected by several factors selected end members for each image might be the most important factor impacting landsat derived isf data zhang and weng 2016 another reason for lack of accuracy might be related to radiometric and geometric errors in each landsat images lu et al 2011 3 2 spatiotemporal patterns of impervious surface fraction fig 6 shows the trend in overall impervious surface fraction isf over 1988 2015 in the upper bulimba catchment essentially this catchment for this case study experienced a gradual and nearly linear increase in isf over the period isf increased from 0 12 in 1988 to 0 34 in 2015 given the gauged area of 58 km2 the impervious surface area isa has increased from 6 96 km2 to 19 72 km2 over the period 1988 2015 figs 7 and 8 show isf maps in the upper bulimba catchment from 1988 to 2015 these isf maps show significant urban development spatially and temporally in the upper bulimba catchment from 1988 to 2015 two types of urban development i e infill and greenfield development occurred in the upper bulimba catchment some non urban isf 0 areas in 1988 has changed to urban areas indicating that greenfield development occurred fig 7 also it is obvious that isf has increased significantly within the boundary of the urban areas in 1988 infill development approximately 3 1 and 9 7 km2 of total impervious surface area increase 12 8 km2 were related to infill and greenfield development respectively in the case study fig 8 represents aggregated landsat isf data at 1 km grids used for distributed hydrologic simulations 3 3 calibration results calibration was implemented for lumped and distributed 1 km simhyd and awbmu models at daily time step using pest for the period of 1986 1990 isf data derived at the catchment and 1 km grids in 1988 were assumed to be constant 5 year over the calibration period fig 9 shows good match between observed and simulated streamflow values over the calibration model performance during calibration for lumped and distributed simhyd and awbmu over the calibration period is shown in table 5 lumped and distributed simhyd and awbmu models performed very well with nse of 0 75 moriasi et al 2007 low pbias values 10 indicate a performance rating of very good for the calibration period moriasi et al 2007 positive pbias values indicate small overestimation of daily runoff for lumped and distributed simhyd and awbmu models overall in terms of nse values lumped and distributed awbmu showed slightly better performance than lumped and distributed simhyd to estimate daily runoff over the calibration period streamflows less than 7 mm day were overestimated pbias ranges from 19 9 to 34 by lumped and distributed simhyd and awbmu while greater streamflows 7mm day were underestimated pbias ranges from 17 6 to 28 5 in terms of nse simhyd and awbmu performed the same when 1 km distributed isf was used for the calibration period pbias showed slight improvement in performance of simhyd when distributed isf and climate data than lumped inputs were used however pbias increased slightly when distributed inputs were used in awbmu in brief no significant improvement in simhyd and awbmu was noted when distributed isf and climate data were used for the calibration period spatial resolution of rainfall data was always a major problem in hydrological studies affecting hydrological model performance particularly rainfall runoff models vaze et al 2011 vaze and teng 2011 zhang et al 2009 vaze et al 2011 assessed the improvement in australian conceptual rainfall runoff models when they were applied at a higher spatial resolution 0 05 than for the whole catchment lumped spatially distributed rainfall sourced from silo were used to run rainfall runoff models for catchments ranging in size from 50 to 2000 km2 in australia they showed that application of gridded rainfall data led to improve in the performance of australian rainfall runoff models e g simhyd for large catchments 500 km2 only however rainfall runoff models rarely improved for small catchments particularly with catchment areas less than 100 km2 silo climate data derived at 5 km spatial resolution could not represent high rainfall variation across the upper bulimba catchment which has an area of 58 km2 we used idw interpolation method to downscale silo rainfall data into 1 km resolution indicating that the final rainfall data used in distributed simhyd and awbmu models do not show significant variation across the upper bulimba catchment for the upper bulimba catchment the mean annual and maximum daily rainfall during the 1986 2018 period varied between 995 and 1050 mm and 186 and 200 mm respectively over the 58 cells these values indicate low spatial variation in rainfall amounts therefore the observed low rainfall variation might be the main reason why no significant improvement was observed when a spatial varying rainfall data was incorporated into distributed simhyd or awbmu for the upper bulimba catchment with the area of 57 km2 which is consistent with vaze et al 2011 models were also calibrated using annual isf table 5 using annual isf rather than 5 year isf only led to slight overestimation in daily flows for the calibration period while nse values remained unchanged this shows that using higher temporal resolution annual isf rather than 5 year had no strong effects on simulation accuracy of the models for the catchment tested 3 4 simulation results daily runoff was simulated by lumped and distributed simhyd and awbmu models using constant and temporal isf data over the period 1991 2018 summary of performance results for four model runs m1 m4 are presented in table 6 simhyd and awbmu with fixed isf m1 and m3 showed good accuracy in simulating daily runoff based on nse values range from 0 65 to 0 68 moriasi et al 2007 however large values of pbias 25 for m1 and m3 represented noticeable underestimation of streamflow for both models comparison of results derived from model runs m1 and m3 showed that the performance of simhyd and awbmu only improved slightly 1 3 in terms of pbias while nse values decreased 0 02 0 03 this result indicated that incorporating only spatial isf and climate data would not lead to improvement in both model s performance as discussed in section 3 3 low variation in climate data could be the reason for low improvement between m1 and m3 model runs compared to m1 and m3 simulation capability of lumped and distributed simhyd and awbmu models improved significantly when spatiotemporal isf m2 and m4 were applied nse values were increased by 0 05 for simhyd and 0 08 for awbmu in m2 and m4 than m1 and m3 also pbias for simhyd model decreased dramatically from 30 2 in m1 to 1 2 in m3 and 26 8 in m2 to 6 1 in m4 likewise pbias decreased from 27 8 in m1 to 8 7 in m3 and 25 8 in m2 to 8 2 in m4 for awbmu simhyd and awbmu lumped and distributed incorporated by spatiotemporal isf slightly overestimated daily runoff 1 2 to 8 7 for the upper bulimba catchment over the simulation period in terms of nse and pbias values lumped awbmu with temporal isf m2 and lumped simhyd with temporal isf m2 showed the best performance in simulating daily runoff at study catchment for more investigation about the effect of using temporal isf on simulation accuracy of the two rainfall runoff models daily streamflow was also simulated over the period 1986 2012 this time isf derived in 2015 was considered constant over the simulation period 1986 2012 results indicated considerable overestimation pbias 20 of daily streamflow using lumped and distributed simhyd and awbmu when isf in 2015 considered constant over the period 1986 2012 however when temporal decrease in isf from 2015 to 1988 was considered during 1986 2012 pbais decreased from 23 to 2 4 lumped simhyd 20 to 7 lumped awbmu 22 to 1 distributed simhyd and 20 to 6 distributed awbmu these results were in agreement with previous studies incorporated temporal land use data with hydrological models wagner et al 2019 zhou et al 2013 indicated that incorporating temporal leaf area index lai leads to a slight increase 0 01 0 07 in the nse of daily runoff and significant decrease 3 11 in pbias also a slight increase 0 02 0 04 in the nse was indicated by teklay et al 2019 due to use of temporal landuse data such as overland manning s and canopy storage in swat model moreover results in table 6 demonstrated that improvement in simulation capability due to using temporal isf data in an urbanized catchment was not sensitive to model selection as shown in fig 9 most of streamflow values 30 mm day were underestimated by simhyd and awbmu under all model runs m1 m4 having said that streamflow 30 mm day were less underestimated when spatiotemporal isf m2 and m4 than constant isf m1 and m3 were used by both models over simulation period also pbias values for streamflow values 30 mm day derived from simhyd and awbmu in m1 53 and 55 m2 31 and 37 m3 57 and 58 and m4 34 and 41 indicated less underestimation of streamflow values 30 mm day when spatiotemporal isf were used these results showed that using temporal isf than constant isf would lead to improvement in simhyd and awbmu simulation capability for reproducing high streamflow values 30 mm day in the case study fig 10 shows accumulated daily observed and simulated runoff data by simhyd and awbmu model with constant and spatiotemporal isf data m1 m4 over the simulation period results reveal a considerable improvement in accumulated streamflow simulated with both simhyd and awbmu models when spatiotemporal isf data m2 and m4 were applied instead of constant isf data m1 and m3 in other words it is demonstrated that considering temporal isf data in lumped and distributed conceptual rainfall runoff modelling could lead to better estimation of accumulated runoff from a catchment experienced significant urban development it is worth noting that the dramatic increase in observed and simulated runoff in 1996 is due to noticeable increase in observed rainfall in the case study catchment the performance of the lumped and gridded rainfall runoff models incorporated with 5 year and annual isf were also compared nse values for lumped and distributed simhyd and awbm with 5 year isf 0 73 0 73 0 75 and 0 74 and annual isf 0 73 0 73 0 75 and 0 74 showed that nse remained unchanged same as calibration results table 5 when annual isf compared to 5 year isf were used in the models pbias however showed slight increase same as calibration results table 5 when annual isf 1 9 9 2 6 8 and 8 6 compared to 5 year isf 1 2 8 7 6 2 and 8 2 were used the models these results indicated no improvement in terms of nse and slight decrease in simulation accuracy in terms of pbias in the performance of models when annual isf than 5 year isf were considered in rainfall runoff models in this study 3 5 relationship between increases in isf and increase in streamflow fig 11 shows increase in annual isf versus increase in annual streamflow mm respectively simulated by lumped simhyd and awbmu over the simulation period 1991 2018 in order to find the relationship between increase in annual isf and increase in annual streamflow mm year simple linear regression models were developed for simhyd eq 11 and awbmu eq 12 11 dq 6 9 disf r 2 0 95 12 dq 7 3 disf r 2 0 91 according to eq 11 and 12 1 increase in isf would lead to an increase in annual streamflow by 6 9 and 7 3 mm using simhyd and awbmu respectively for the study catchment if we take the mean annual flow of 234 mm using simhyd qm1 and 246 mm using awbmu qm1 as the baseline with a constant isf of 14 4 for 1990 equations 11 and 12 would imply a 2 9 3 increase in streamflow from the baseline for 1 increase in isf for the catchment tested it is worth noting that the empirical relationship between changes in isf and changes in flow applies to the catchment over the period over the period from 1991 to 2018 when the increase in the impervious area was the primary cause for the increase in streamflow overall results mentioned above would help predict increases in streamflow due to urbanization in the case study for future periods 4 conclusion in this study the impact of incorporating spatiotemporal impervious surface fraction isf and climate data on the performance of two conceptual rainfall runoff models simhyd and awbmu in an urbanized catchment was examined the performance of simhyd and awbmu were not improved when they are applied at higher spatial resolution distributed 1 km with spatially varying isf and climate data during calibration and simulation runs for the study catchment tested this is largely due to low spatial variability of climate rainfall and pet data over the relatively small catchment on the other hand integrating spatiotemporal isf into lumped gridded simhyd and awbmu over the simulation period led considerable improvement in their performance for estimating daily streamflow results indicated improvement in nse and pbias values for example pbias decreased noticeably e g from 30 2 to 1 2 and 26 8 to 6 1 for lumped simhyd and awbmu respectively by considering spatiotemporal 5 year or annual isf rather than constant isf over the simulation period our analysis showed that using constant isf measured in 1988 lowest isf for the simulation period 1991 2018 led to considerable underestimation pbias 26 of daily simulated streamflows by the lumped and gridded rainfall runoff models conversely daily simulated streamflows were overestimated pbias 20 when constant isf measured in 2015 highest isf were considered for the simulation period 1986 2012 improvement in flow simulation as a result of using spatiotemporal varying isf in lumped and distributed models were not sensitive to model selection as both simhyd and awbmu showed almost identical performance in terms of nse and pbias in addition regarding the observed time series of rainfall during 1991 2018 it is found that urbanization has increased streamflow in the study catchment every 1 increase in isf would lead to 6 9 7 3 mm or approximately 3 increase in annual streamflow overall our key finding is that incorporating spatiotemporal 5 year or annual isf data into a lumped and distributed rainfall runoff model can significantly improve its model performance in urban catchment tested conceptual models when coupled with time varying data on isf can be used to estimate streamflow accurately and reliably in response to urbanization and to make informed decisions in relation to sustainable land and water planning and management for future periods it is highly recommended that future studies should incorporate temporal annual or 5 yearly changes in landuse especially the impervious fraction which can be readily derived from remote sensing images into hydrological models conceptual or physically based for predicting streamflow for urban catchments credit authorship contribution statement mohammad reza ramezani conceptualization data curation methodology software visualization writing original draft writing review editing bofu yu conceptualization methodology project administration supervision writing original draft writing review editing niloofar tarakemehzadeh data curation methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors acknowledge griffith university queensland australia for providing financial support through guiprs and guprs scholarships 
3199,choosing a suitable process oriented eco hydrological model is essential for obtaining reliable simulations of hydrological processes determining soil hydraulic and solute transport parameters is another fundamental prerequisite research discussing the impact of considering evaporation fractionation on parameter estimation and practical applications of isotope transport models is limited in this study we analyzed parameter estimation results for two datasets for humid and arid conditions using the isotope transport model in hydrus 1d in which we either did or did not consider fractionation the global sensitivity analysis using the morris and sobol methods and the parameter estimation using the particle swarm optimization algorithm highlight the significant impact of considering evaporation fractionation on inverse modeling the kling gupta efficiency kge index for isotope data can increase by 0 09 and 1 49 for the humid and arid datasets respectively when selecting suitable fractionation scenarios differences in estimated parameters propagate into the results of two practical applications of stable isotope tracing i the assessment of root water uptake rwu and drainage travel times i e the time elapsed between water entering the soil profile as precipitation and leaving it as transpiration or drainage in the lysimeter humid conditions and ii evaporation estimation in a controlled experimental soil column arid conditions the peak displacement method with optimized longitudinal dispersivity provides much lower travel times than those obtained using the particle tracking algorithm in hydrus 1d considering evaporation fractionation using the craig gordon cg and gonfiantini models is likely to result in estimates of older water ages for rwu than the no fractionation scenario the isotope mass balance method that uses the isotopic composition profile simulated by hydrus 1d while considering fractionation using the cg and gonfiantini models or the measured evaporation isotope flux provides comparable results in evaporation estimation as the hydrus 1d water mass balance method and direct laboratory measurements in contrast the no fractionation scenario reasonably estimates evaporation only when using the hydrus 1d water mass balance method the direct use of simulated isotopic compositions in the no fractionation scenario may result in large biases in practical applications in the arid zone where evaporation fractionation is more extensive than in humid areas keywords hydrus 1d global sensitivity analysis particle swarm optimization water travel time temporal origin evaporation estimation data availability data will be made available on request 1 introduction reliable water balance simulations in the vadose zone are important to understand and forecast the impact of anthropogenic disturbances such as global warming and land use change on soil water storage groundwater recharge and evapotranspiration a detailed mechanistic understanding of water fluxes in the vadose zone could support optimal and efficient management strategies for promoting the long term sustainability of water resources and associated ecosystem functions penna et al 2018 for example the exact quantification of evaporation affects water availability for plants nelson et al 2020 and constrains groundwater recharge condon et al 2020 however the conventional methods e g pan experiments for estimating evaporation fluxes often require extensive field monitoring of water flow which is often time consuming expensive labor demanding and affected by considerable uncertainty skrzypek et al 2015 stable isotopes of hydrogen 2h and oxygen 18o are widely used to trace water fluxes across the critical zone and can be expressed as isotopic ratios 2h 1h and 18o 16o by using the δ notation i e δ2h and δ18o the isotopic composition of shallow soil water provides insights into evaporation fractionation characteristics this information can be easily used to calculate corresponding evaporation fluxes for example skrzypek et al 2015 combined the equations for evaporation estimation based on the revised craig gordon model craig and gordon 1965 and developed a software hydrocalculator using this software they estimated evaporation losses and validated its results using pan measurements this method has been extended to soil evaporation estimation for example sprenger et al 2017 estimated that evaporation was about 5 and 10 of infiltrating water in the heath and scots pine soils respectively while the spatial origin of the water plants use has been widely studied e g allen et al 2019 very little is known about its temporal origin brinkmann et al 2018 miguez macho and fan 2021 to track water across the critical zone we need to assess how fast water moves down to the soil profile bottom and when and how much water returns to the atmosphere through root water uptake rwu the premise is to accurately estimate travel times tt of irrigation precipitation water i e the time between water entering the soil profile as irrigation precipitation and leaving it back to the atmosphere as transpiration or at the soil profile bottom as drainage the peak displacement method represents the most widespread technique to estimate travel time from the time difference between signals in soil water stable isotope time series directly measured at specific soil depths chesnaux and stumpp 2018 koeniger et al 2016 stumpp et al 2012 however this method is unfeasible when there is no pronounced peak correspondence between isotopic compositions of precipitation and drainage water samples another widely used isotope transport based method is to inversely estimate the parameters for time invariant tt distributions ttds e g timbe et al 2014 or time variant storage selection sas functions benettin and bertuzzo 2018 harman 2015 rinaldo et al 2015 implemented in lumped hydrological models such oversimplified models are based on few soil and vegetation parameters but have limitations in describing transient conditions or simulating isotope transport sprenger et al 2016b in contrast isotope transport can be reliably simulated using the richards equation based hydrological models with appropriate soil and vegetation parameters and known boundary and initial conditions however direct measurements of soil hydraulic and transport parameters required by such models are time consuming and labor demanding therefore such parameters are commonly obtained using inverse modeling by minimizing the errors between easily measured state variables and fluxes e g soil water contents and pressure heads at different soil depths or leachate water volumes and corresponding model simulations hopmans et al 2002 mertens et al 2006 vrugt et al 2008 wollschlager et al 2009 wohling and vrugt 2011 nevertheless it is not always necessary to account for all model parameters in parameter optimization since some can be fixed as they can be either determined experimentally or have a minor impact on the model output the latter can be determined using the global sensitivity analysis gsa the sobol and morris methods are among the two most widespread gsa methods liu et al 2020 the sobol method provides the most accurate sensitivity indices but it requires several model runs and is thus computationally intensive gatel et al 2019 in contrast the morris method cannot yield the order of the most sensitive parameters as accurately as the sobol method but its computational cost is much lower and it can still pinpoint the most influential parameters campolongo et al 2007 herman et al 2013 many inverse modeling algorithms can be used for parameter estimation for example the levenberg marquardt optimization lmo proved to be very efficient and was therefore implemented in hydrus šimůnek et al 2008 however the lmo is sensitive to the initial parameter values provided by the user and often falls into local instead of global minimum brunetti et al 2016 thus global optimization algorithms such as particle swarm optimization pso have become more widespread over the last decades e g vrugt and robinson 2007 when optimizing isotope transport parameters via inverse modeling isotopic compositions from multiple soil depths must be included in the objective function and combined with other state variables and fluxes for example research shows that the model calibration can be improved by simultaneously considering stable isotopes and soil moisture information sprenger et al 2015 groh et al 2018 mattei et al 2020 however the correct model structure is a fundamental prerequisite to obtaining successful simulations in particular research discussing the impact of considering evaporation fractionation on parameter estimation and practical applications of isotope transport models is limited penna et al 2018 therefore we pose two scientific questions first how will the consideration of evaporation fractionation affect the parameter estimation results of the isotope transport model second how will this effect propagate into practical applications such as water travel times and evaporation estimation to answer these questions we compare the parameter estimation results obtained using the isotope transport model in hydrus 1d zhou et al 2021 that does or does not consider evaporation fractionation for two available datasets 1 a 150 cm thick layered soil profile in a lysimeter under humid climate where evaporation fractionation is negligible 2 a 35 cm thick soil column subject to evaporation where evaporation fractionation process is dominant the accuracy of the parameterization obtained by the pso algorithm is assessed based on its ability to reproduce measured water fluxes and isotope transport data the parameters estimated while considering or not evaporation fractionation are then used to calculate travel times and evaporation 2 materials and methods two experimental datasets are considered in this study the first dataset is collected using a field lysimeter 150 cm thick layered soil profile located in austria under humid climate conditions stumpp et al 2012 section 2 1 1 the second dataset is collected using a 35 cm thick soil column in france subject to evaporation to mimic arid climate conditions braud et al 2009a section 2 1 2 numerical simulations of water flow and isotope transport with and without evaporation fractionation are implemented in hydrus 1d the modeling setup is briefly described in section 2 2 and method s1 in the supplementary material the sensitivity analysis based on the sobol and morris methods is performed to evaluate the interactions between soil hydraulic and solute transport parameters and the impact of multiple measured data types section 2 3 method s2 and results s1 s2 the accuracy of the parameterization obtained by the pso algorithm is assessed based on its ability to reproduce the observed data sections 2 4 3 1 1 and 3 2 1 the parameters estimated while considering or not considering evaporation fractionation are then used to calculate travel times and evaporation and quantify the impact of their different estimates sections 2 5 2 6 3 1 2 and 3 2 2 the effects of varying climate conditions and estimation methods are then compared and illuminated section 4 the schematic outline of the different methods used is shown in fig 1 2 1 site description and data availability 2 1 1 stumpp et al 2012 dataset the first dataset is taken from the lysimeter 3 of stumpp et al 2012 available at https www pc progress com en default aspx h1d lib isotope the field experiment was conducted in a humid region located at the research area of the hblfa höhere bundeslehr und forschungsanstalt für landwirtschaft raumberg gumpenstein in gumpenstein austria this area has a mean annual temperature of 6 9 c and average annual precipitation p of 1035 mm the annual potential evapotranspiration et 0 for grass reference during the experiment period may 2002 to february 2007 calculated by the penman monteith equation is about 557 mm and the corresponding aridity index p et 0 is about 1 86 corresponding to a humid climate class liang 1982 the cylindrical lysimeter with a depth of 150 cm and a surface area of 10000 cm2 was embedded in a rainfed agricultural field cambisol planted with winter rye and fertilized with liquid cattle slurry the observation period was from may 2002 to february 2007 1736 days table s1 shows the summary of the observed data the temporal distribution of p et 0 soil surface temperature t s air relative humidity rh and leaf area index lai during the simulation period are shown in fig 2 more details about data acquisition including meteorological parameters and root water uptake information can be found in stumpp et al 2012 2 1 2 braud et al 2009a dataset braud et al 2009a designed a rubic iv experiment that started on april 11 2005 corresponding to day of the year doy 101 and lasted 338 days the experiment consisted of 6 columns 12 cm in diameter and 35 cm in height the soil columns were filled with a silt loam collected at the field station of lusignan france and wetted using demineralized water of the known isotopic composition the bottom was closed by clay marbles the soil was initially saturated and subject to evaporation only dry air was simultaneously injected over all six columns the isotopic composition of the air changed due to water vapor released by evaporation from soil columns the air was finally trapped in a cryoscopic device which allowed the determination of evaporation fluxes from bare soil columns and the corresponding isotopic composition of the water vapor under non steady state conditions more details about the experimental setup can be found in figs 1 2 of braud et al 2009a the data collected in column 2 ending at doy 264 were analyzed in this study thirteen variables were measured continuously at a frequency of about 15 min to assess the water balance of the soil column these variables included the room temperature the atmospheric pressure the absolute pressure of the dry air before it entered the soil column air mass flow for the humidity control above the soil column the mass of the soil column air temperature and humidity at the outlet of the soil column the temperatures of the cryoscopic trapping downstream and upstream of the columns and the air temperature and residual air humidity at the outlets of two cold traps the vapor was trapped twice a day during the first three months and only once a day after that once evaporation decreased soil column 2 was dismantled on september 21 2005 doy 264 to sample liquid water and measure the gravimetric soil water content more details about data acquisition can be found in braud et al 2009a the temporal distributions of the evaporation flux e the isotopic composition of the evaporation flux δ e outlet air temperature t air and outlet air relative humidity rh during the simulation period are shown in fig 3 2 2 model setup the hydrus 1d model modified by zhou et al 2021 to simulate the transport of soil water isotopes while considering evaporation fractionation was used in this study a brief summary of the model setup including the governing equations without and with vapor flow for the stumpp et al 2012 and braud et al 2009a datasets respectively boundary conditions bcs and model inputs is shown in figs 4 5 more details can be found in zhou et al 2021 2 2 1 stumpp et al 2012 dataset the soil profile was 150 cm deep and was discretized into 151 nodes it consisted of three different soil horizons 0 29 cm 30 89 cm 90 150 cm the initial pressure head profile was assumed to be at hydrostatic equilibrium with the pressure head h 150 cm at the soil surface the weighted average δ18o of precipitation 9 5 and estimated temperature 20 were used as initial conditions the atmospheric with a surface layer and seepage face boundary conditions bc were used for water flow at the upper and lower boundaries respectively the temperature bc was used for heat transport at both boundaries in this humid condition example evaporation fractionation was limited to the soil surface due to the lack of the vapor phase within the soil the solute flux and zero concentration gradient bcs were used for isotope transport at the upper and lower boundaries respectively the isotope flux associated with evaporation was calculated either assuming no fractionation or using the craig gordon or gonfiantini fractionation models hereafter referred to as non frac cg frac and gon frac respectively the non frac scenario calculated the isotope flux of evaporation by assuming that the isotopic composition of the evaporation flux was the same as that of surface soil water the isotopic composition of the atmospheric water vapor δ a in the cg frac scenario was estimated based on its equilibrium relationship with the isotopic composition of rainfall skrzypek et al 2015 the gon frac scenario was simplified without the need for the isotopic composition of the atmospheric water vapor to consider fractionation zhou et al 2021 a detailed description of the cg and gonfiantini models can be found in method s1 for simplification only equilibrium fractionation was considered at the soil surface since kinetic fractionation could be neglected in this example zhou et al 2021 in other words the kinetic fractionation coefficient n k in eq 11 of zhou et al 2021 was set to 0 and thus the kinetic fractionation factor at the soil surface α i k in the cg frac and gon frac scenarios eqs s2 s3 was equal to 1 2 2 2 braud et al 2009a dataset the simulated soil profile was 35 cm deep and was discretized into 132 nodes following braud et al 2009a the soil column was initially almost fully saturated with the measured initial pressure head increased linearly from 1 cm at the soil surface to 35 cm at the soil profile bottom the observed initial soil temperature and δ18o were 24 25 and 6 34 respectively the temperature bc was used for heat transport at both surface and bottom boundaries using temperatures measured at 2 5 and 24 cm depths respectively the atmospheric and zero flux bcs were used for water flow at the upper and lower boundaries respectively the measured evaporation flux e was used as the upper bc for water flow in this arid condition example evaporation fractionation occurred both at the soil surface and within the soil due to the existence of the vapor phase the stagnant air layer bc which had been modified to account for evaporation fractionation and zero flux bc were used for isotope transport at the upper and lower boundaries respectively the surface isotope flux associated with evaporation was calculated either assuming no fractionation using the craig gordon or gonfiantini fractionation models or using the measured values hereafter referred to as non frac cg frac gon frac and meas frac respectively the non frac scenario calculated the isotope flux of evaporation by assuming that its isotopic composition was the same as that of surface soil water i e no fractionation at the soil surface and equilibrium and kinetic fractionation factors within the soil α α i d were equal to 1 i e no fractionation within the soil the theory of cg frac and gon frac scenarios was explained in method s1 for simplification the kinetic fractionation coefficient n k in eq 11 of zhou et al 2021 was set to 1 and thus the kinetic fractionation factor at the soil surface α i k in the cg frac and gon frac scenarios eqs s2 s3 was equal to 1 0324 the measured isotopic composition of the outlet water vapor δ e was used in the meas frac scenario to calculate the surface isotope flux e i corresponding to the evaporation flux e more details about how upper boundary fluxes were calculated can be found in braud et al 2009a 2 3 global sensitivity analysis five soil hydraulic parameters i e θ r θ s n α and k s need to be optimized for each layer of the soil profile to simulate water flow using the hydrus 1d model the residual water content θ r was set to zero to reduce the number of fitting parameters to simulate isotope transport in the soil the longitudinal dispersivity λ also needs to be optimized since only the isotopic composition of the lysimeter discharge was measured in the stumpp et al 2012 dataset the dispersivity of three individual soil layers cannot be estimated therefore only one longitudinal dispersivity for the entire lysimeter was estimated therefore the total number of parameters p was 13 and 5 for the stumpp et al 2012 and braud et al 2009a datasets respectively the global sensitivity analysis gsa using both morris and sobol methods was conducted in this study to determine the most influential parameters and their interactions the detailed description of these two methods is shown in method s2 in the supplementary material the sensitivity analysis was conducted using python s sensitivity analysis library salib herman and usher 2017 the script produces the input parameter space overwrites the input parameters file and runs the executable module of hydrus 1d for each simulation of the stumpp et al 2012 dataset five kling gupta efficiency kge indices for different evaluation indicators were calculated including for the time series of the bottom water flux kge bf the soil water content at different depths kge wc the bottom water isotopic composition kge wi the water retention curves kge rc and the average of the four kge values kge avg for each simulation of the braud et al 2009a dataset three kling gupta efficiency kge indices for different evaluation indicators were calculated including the final soil water content profile kge wc the final water isotopic composition profile kge wi and the average of the two kge values kge avg the kge index compares the correlation coefficient r the ratio of mean values β and the ratio of variances γ between simulated and observed data the value of the kge index is always smaller or equal to 1 the higher the kge value the better fit between the simulated and observed values the positive and negative kge values are often considered good and bad solutions knoben et al 2019 1 k g e 1 1 r 2 1 β 2 1 γ 2 0 5 if a hydrus 1d run was not finished within a prescribed time i e 30 s and 60 s for the stumpp et al 2012 and braud et al 2009a datasets respectively or the length of the modeled hydrograph was shorter than the total simulation period 1736 and 163 days for the stumpp et al 2012 and braud et al 2009a datasets respectively it was considered non convergent the run was then terminated and a large negative value 1e 7 was prescribed to the objective function non convergent runs in gsa are a frequent problem when using nonlinear environmental hydrological models and there are no clear indications on how to handle these unfeasible points razavi et al 2021 removing or skipping them alters the sampling trajectory and can result in biased conclusions especially if non convergent runs lie in informative regions of the parameter space recently sheikholeslami et al 2019 compared strategies such as median substitution single nearest neighbor or response surface modeling brunetti et al 2017 to fill in for model crashes their results show that interpolating non convergent runs with a radial basis function trained in the vicinity of that point leads to reliable results and outperforms other strategies we implemented a similar approach in the present work but with important differences in particular 1 for each non convergent point we calculated its euclidean distance from all other convergent points in the gsa sample 2 convergent points were ordered in ascending order i e from the closest to the farthest 3 the 100 closest convergent points were used to train a response surface surrogate based on the kriging partial least squares method kpls bouhlel et al 2016 which outperforms traditional kriging on high dimensional problems 4 the trained kpls surrogate was finally used to interpolate non convergent runs in the original gsa sample the use of multiple localized surrogates allowed for better reconstruction of the topological features of the response surface in the vicinity of the non convergent points in this study the global sensitivity analysis was combined with the monte carlo filtering to identify reduced ranges of parameters with good solutions for subsequent parameter optimization potential solutions were filtered into good solutions with kge 0 0 and bad solutions with kge 0 0 kernel density estimation kde plots were then used to identify areas with high density good solutions while the correlation analysis was conducted to determine interactions between parameters and may help reduce the input factor space more details can be found in brunetti et al 2016 this type of procedure shares multiple similarities with the generalized likelihood uncertainty estimation glue proposed by beven and freer 2001 the joint use of the gsa sample with the glue approach i e gsa glue ratto et al 2001 allows for obtaining a rough assessment of the parameters uncertainty and successful estimates of soil hydraulic parameters e g brunetti et al 2018 2 4 parameter optimization the particle swarm optimization pso algorithm was used in this study for parameter optimization in the pso a swarm of candidate solutions is moved around in the search space according to a few equations the movement of the particles is guided by the optimal position of themselves and the whole swarm once improved positions are discovered they are used to guide the swarm s movement this process is repeated until the global optimal position that all particles tend to follow is found shi and eberhart 1998 the pso parameters cognitive parameter c 1 0 267 social parameter c 2 3 395 inertia weight w 0 444 from brunetti et al 2016 were used in this study the number of particle swarm and iterations are 40 and 200 respectively the pyswarm library in python was used for the pso the process was similar to the gsa except that reduced ranges of parameters were used in this way the number of potential local minima is reduced and the convergence improves only the set of parameters leading to the maximum kge avg i e minimum 1 kge avg as the objective function was retained as optimized parameters 2 5 first practical application calculation of drainage and rwu travel times 2 5 1 the peak displacement isotope transport based method the peak displacement method estimates travel times from the time lag between signals in the measured input rainfall isotopic composition and output drainage isotopic composition isotope time series in the stumpp et al 2012 dataset a pronounced correspondence was observed between the depleted precipitation peak in the winter november 18 2005 to april 14 2006 and the lysimeter discharge the mean drainage travel time t o t accounting for dispersion effects can be calculated by the mean peak isotopic composition lag time t m t using eq 2 2 t o t m 1 3 λ l 2 3 λ l where l is the lysimeter length l more details can be found in stumpp et al 2012 in this study t m from stumpp et al 2012 and dispersivities λ optimized using hydrus 1d assuming different fractionation scenarios were used 2 5 2 the particle tracking water flow based method the particle tracking algorithm is based on the water mass balance calculation the initial position of the particles is defined using the initial water content distribution depending on the precipitation irrigation inputs the particles may be released at the soil surface and leave at the soil profile bottom in this study the input parameters wstand the initial distribution and wprec the upper bc distribution for the particle tracking algorithm were set to 10 cm and a negative number which triggers the option of releasing particles with each rain event respectively more details about the particle tracking algorithm can be found in šimůnek 1991 or zhou et al 2021 when knowing the positions of the particles at different times the residence time rt and locations of water from all precipitation irrigation events can be obtained i e the residence time distribution rtd note that the particle travel time tt is the sum of the particle age i e residence time and life expectancy i e time to reach the destination the former is the time elapsed since the particle release while the latter is the remaining time before the particle reaches the outlet benettin et al 2015 therefore when the particles leave the lysimeter bottom or as root water uptake rwu their residence times can be called drainage or rwu travel times respectively the particle tracking module additionally assesses rwu between two neighboring particles as a function of time when particles are released for each precipitation event we can precisely evaluate the contribution of each precipitation event to rwu at different times we can then infer the temporal origin of rwu by synthesizing this information different fractionation scenarios with the soil hydraulic parameters optimized using hydrus 1d were used to run the particle tracking module to calculate drainage and rwu travel times 2 6 second practical application calculation of evaporation flux 2 6 1 the water flow based method braud et al 2009a calculated evaporation using three methods the first method determines the evaporation rate by continuously measuring the vapor flux and humidity at the outlet of the soil column the second method obtains the evaporation rate by repeatedly weighing the soil column finally the third method determines the evaporation rate by weighting the mass of the frozen water trapped at the outlet of the soil column these three methods are hereafter referred to as direct measurement column weighting and trapped volume respectively this study presents these results also as the reference for other methods more details can be found in braud et al 2009a another water flow based method used in this study to calculate water flux components was to analyze the water mass balance simulated in hydrus 1d e g sutanto et al 2012 2 6 2 the isotope transport based method for an isolated water volume with an initial isotopic composition δ 0 evaporating into the atmosphere the isotopic composition of the residual liquid water δ s can be calculated as benettin et al 2018 3 δ s δ 0 δ 1 f e xm δ where δ is the limiting isotopic composition that would be approached when water is drying up xm is the temporal enrichment slope and fe is described below eq 3 is based on the isotope mass balance equations of gonfiantini 1986 and the isotopic composition of the evaporation flux estimated by the craig gordon model craig and gordon 1965 more details about the derivations can be found in gonfiantini 1986 this equation implies that the isotopic composition of soil water only changes due to evaporation fractionation the ratio of the evaporation loss to the initial water storage f e can be then estimated as sprenger et al 2017 4 f e 1 δ s δ δ 0 δ 1 xm the two variables δ and xm can be calculated as benettin et al 2018 5 δ r h δ a ε k ε α r h 1 0 3 ε k ε α 6 xm r h 1 0 3 ε k ε α 1 r h 1 0 3 ε k where δ a is the isotopic composition of the atmospheric water vapor rh is the air relative humidity α is the dimensionless equilibrium fractionation factor while ε and ε k are equilibrium and kinetic fractionation enrichments respectively details about the calculation procedure for these parameters α ε ε k can be found in benettin et al 2018 or zhou et al 2021 the equivalent kinetic fractionation factor within the soil α i d used to calculate ε k was optimized manually to get the best match of f e with those from water flow based methods in section 2 6 1 the fraction of water that evaporated before the end of the braud et al 2009a experiment was calculated in this study average measured values of rh t air t s and δ 0 during the experiment and the final isotope profile simulated using hydrus 1d were used in the above equations 3 results 3 1 stumpp et al 2012 dataset analysis 3 1 1 parameter optimization and model performance the global sensitivity analysis and monte carlo filtering results for the stumpp et al 2012 dataset are shown in the results s1 section of the supplementary material overall soil hydraulic parameters of different layers had comparable impacts on the model outputs the order of sensitive parameters is shape parameters of the water retention function namely n and α saturated water content θ s saturated hydraulic conductivity k s and dispersivitie λ the final optimized soil hydraulic and solute transport parameters and corresponding kges are shown in table 1 considering evaporation fractionation impacted parameter estimation significantly especially in the optimization of the soil saturated hydraulic conductivity k s and shape parameter α overall the water retention and soil hydraulic conductivity curves fig s8 differed greatly between different fractionation scenarios in the third layer but were relatively similar in the first and second layers the water retention curve in the gon frac scenario best matched the measured one but did not outperform those from the cg frac and non frac scenarios as seen from the kge rc values in table 1 compared with the cg frac and gon frac scenarios the water retention curve in the non frac scenario had a steeper decline and a lower saturated water content in the third layer while it became more gradual with higher saturated water contents in the first and second layers however the non frac scenario always produced higher hydraulic conductivities than the cg frac and gon frac scenarios note that the non frac scenario also had higher hydraulic conductivities in the third layer because of relatively higher matric potentials the fits for different fractionation scenarios are shown in fig 6 the isotopic composition of the lysimeter discharge remained the same for different fractionation scenarios during about the first 150 days and started deviating after this time but the trends were still similar except for some vertical shifts different fractionation scenarios resulted in a similar average fitting performance kge avg within 0 03 the non frac scenario had the highest kge wi i e for water isotopic composition followed by the cg frac scenario while the gon frac scenario performed the worst the difference between kge wi indices for different fractionation scenarios was within 0 09 3 1 2 first practical application drainage travel times and rwu temporal origin the mean travel times mtts of drainage i e from the surface to the bottom estimated by the peak displacement method are shown in table 2 the mtts were 251 9 251 9 and 257 1 days for the non frac cg frac and gon frac scenarios respectively the consideration of fractionation using the gonfiantini model slightly overestimated the travel times compared to the non frac scenario however the difference was not very evident within 6 days for different fractionation scenarios fig s9 shows the spatial temporal distribution of particles simulated using the soil hydraulic parameters estimated considering different fractionation scenarios the residence time distribution rtd of soil water is displayed in fig 7 the mean residence time mrt the mean of rts averaged over the entire simulation duration increased with soil depth in all scenarios due to a time lag involved in water transfer the mrts for the non frac scenario for depths of 30 70 and 110 cm were 82 1 138 2 and 203 6 days respectively the mrts for the cg frac scenario for 30 70 and 110 cm depths were 69 9 170 0 and 258 5 days respectively finally the mrts for the gon frac scenario for 30 70 and 110 cm depths were 80 6 174 3 and 270 6 days respectively in terms of temporal distribution rts showed five distinct seasonal cycles specifically they had a trough after every rainy season and a peak after every dry season showing a pronounced lag effect in other words rts were determined by the trade off between precipitation input and evapotranspiration removal corresponding travel times of drainage are shown as probability density distribution histograms in fig s10 and summarized in table 2 the means and standard deviations of travel times were 297 5 79 96 356 8 104 29 and 369 9 101 24 days for the non frac cg frac and gon frac scenarios respectively the particle tracking method produced significantly higher travel times by about 89 days than the peak displacement method similarly considering fractionation using the cg frac and gon frac scenarios led to longer travel times tts than the non frac scenario in addition the difference was very evident reached 72 days for different scenarios to further explore and quantify the rtd differences when considering different fractionation models the temporal origin of rwu is plotted in fig 8 fig 8 shows the monthly transpiration sums in the upper panels and fractional contributions of water of a certain age origin to these monthly transpiration sums in the lower panels note that the amount and temporal distribution of transpiration were similar under different fractionation scenarios 54 95 53 91 and 54 03 cm for non frac cg frac and gon frac respectively therefore only the temporal distribution of transpiration in the non frac scenario is displayed as for the age distribution of rwu for example in the non frac scenario the yellow line in 2002 indicates that about 29 of the water taken up by roots in august was older than may while the remaining 71 was from may august of 2002 5 from june 16 from july and 50 from august more details about how to read the age distribution of rwu can be found in fig 5 of brinkmann et al 2018 the maximum water age for rwu for different fractionation scenarios was almost the same about 300 d in october 2002 330 d in september 2003 270 d in november 2004 and 180 d in february 2006 except for 240 d in december 2003 and 180 d in february of 2005 for the non frac scenario these results were consistent with water residence times at the maximum rooting depths in fig 7 however different fractionation scenarios had relatively large impacts up to three months on the minimum water age for rwu the most obvious example was the 2003 growing season a relatively dry year with less precipitation as shown in fig 2 the minimum water age for rwu in 2003 was within about a month for the gon frac scenario and 120 d february for the non frac and cg frac scenarios in addition the dynamics of fractional monthly contributions to rwu also varied between different scenarios in general the water age for rwu was far longer in dry years 2003 2004 than in wet years 2005 2006 suggesting that drought can promote crop uptake of old water in the same growing season the water age for rwu was consistently lower in may and june than in july and august which reflected an increase in the rooting depth 3 2 braud et al 2009a dataset analysis 3 2 1 parameter optimization and model performance the global sensitivity analysis and monte carlo filtering results for the braud et al 2009a dataset are shown in the results s2 section of the supplementary material the most sensitive parameters were shape parameters n and saturated water contents θ s the final optimized soil hydraulic and solute transport parameters and corresponding kges are shown in table 3 considering or not evaporation fractionation also impacted parameter estimation significantly the most significant impacts were on dispersivity λ and the shape parameter α table 3 the soil water retention curves fig s12 showed that the wilting points were almost identical for the non frac and fractionation cg frac gon frac meas frac scenarios however the saturated water contents were higher and water contents started to drop later in the fractionation scenarios than those in the non frac scenario the soil hydraulic conductivity curves fig s12 showed that the saturated hydraulic conductivities were very similar but the hydraulic conductivities in the fractionation scenarios were a little higher than those in the non frac scenario the fits of soil profile isotopic compositions for different fractionation scenarios are shown in fig 9 the non frac scenario had an almost uniform isotopic composition profile in this case the parameter optimization depended mainly on the measured soil water content profile in fractionation scenarios the peak value of the isotopic composition profile in the meas frac scenario was smaller than those in the gon frac and cg frac scenarios while the value of dispersivities was the opposite different fractionation scenarios resulted in significantly different average fitting performances kge avg reached 0 72 the meas frac scenario had the highest kge wi i e for soil water isotopic composition followed by gon frac and cg frac scenarios while the non frac scenario performed the worst the difference between kge wi indices for different fractionation scenarios reached 1 49 3 2 2 second practical application estimation of evaporation flux table 4 shows cumulative evaporation obtained using different measurements and simulated considering different fractionation scenarios the average isotopic composition of the whole profile was calculated using soil water contents and the column depth as weights cumulative evaporation was estimated to account for about 64 4 63 1 and 65 6 of the initial soil water storage in the cg frac gon frac and meas frac scenarios respectively these values for the cg frac gon frac and meas frac scenarios were slightly lower than but comparable to laboratory measurements and the hydrus 1d water balance slight differences may have been caused by uncontrollable measurement errors in the isotopic composition of the atmospheric water vapor δ a in eq 5 which is the most sensitive parameter in the isotope mass balance method skrzypek et al 2015 cumulative evaporation cannot be estimated using this method in the non frac scenario since no isotopic enrichment occurred i e δ s δ 0 in eq 4 4 discussion 4 1 impacts of evaporation fractionation on parameter estimation and model performance for the stumpp et al 2012 dataset as indicated in section 3 1 1 the fractionation scenarios cg frac and gon frac had lower hydraulic conductivities than the non frac scenario this is because fractionation decreases the isotope flux by evaporation compared with a no fractionation scenario the isotopic composition of the evaporation flux cannot be greater than that of surface soil water and thus increases the isotope flux by net infiltration to get a good fit between simulated and observed isotopic compositions of discharge water the inverse modeling yields a larger longitudinal dispersivity to increase the dispersion of isotopes table 1 or lower hydraulic conductivities to decrease downward convection of isotopes fig s8 the simulated isotopic composition of the lysimeter discharge remained the same for different fractionation scenarios during about the first 150 d and started deviating after this time fig 6 this suggests that it takes about 150 d before the impact of different treatments of the upper bc for isotope transport propagates to the soil profile bottom and affects the isotopic composition in drainage water zhou et al 2021 this time interval i e about 150 d is much smaller than the travel time of the first particle released at the soil surface as calculated by the particle tracking method fig s9 this is because the particle tracking algorithm considers only piston flow while dispersion accelerates the arrival of isotopes to the soil profile bottom however the trends are still similar except for some vertical shifts since kge wi values did not differ much for different fractionation scenarios within 0 09 fig 6 and table 1 considering or not evaporation fractionation does not significantly impact the isotopic composition in discharge water in this example humid conditions the non frac scenario had a slightly higher kge wi indicating that it can fit isotopic data better followed by cg frac while gon frac performed the worst this is understandable since evaporation fractionation could be neglected in this example as seen from the dual isotope plots fig 5 of stumpp et al 2012 for the braud et al 2009a dataset as indicated in section 3 2 1 the hydraulic conductivities in the fractionation cg frac gon frac meas frac scenarios were a little higher than those in the non frac scenario this is because fractionation decreases the isotope flux by evaporation compared with a no fractionation scenario a higher hydraulic conductivity in the fractionation scenarios promotes upward evaporation and fractionation this increases the isotopic composition of remaining soil water and thus produces a better fit between simulated and observed isotope profiles when evaporation fractionation was not considered the isotopic composition of evaporation remained the same as the initial isotopic composition this resulted in a uniform isotopic composition equal to the initial value distribution of soil water throughout the profile in the non frac scenario fig 9 in fractionation scenarios the peak value of the isotopic composition profile was inversely proportional to the dispersivity value fig 9 and table 3 which is consistent with the conclusions from braud et al 2009b the isotopic composition profiles and the kge wi values differed dramatically reached 1 48 between different fractionation scenarios fig 9 and table 3 this implies that considering evaporation fractionation significantly impacts the isotopic composition profile in this example arid conditions the meas frac scenario had the highest kge wi i e for the water isotopic composition followed by the gon frac and then cg frac while the non frac scenario performed the worst this is understandable since evaporation fractionation could not be neglected and the measured evaporation isotope flux is the most accurate for this example braud et al 2009b 4 2 impacts of evaporation fractionation on practical applications 4 2 1 estimation of drainage and rwu travel times differences in water travel times were not evident among different fractionation scenarios table 4 since the numerator in eq 2 is much larger than the denominator in the peak displacement method as a result water travel times were similar for different fractionation scenarios despite a very different dispersivity however for the particle tracking method based on water flow calculations differences in water travel times were evident among different fractionation scenarios table 2 despite their similar kge values table 1 in addition differences in estimated soil hydraulic parameters may also cause discrepancies in tts of individual precipitation events and the temporal origin of water for rwu figs s8 and 7 8 overall the particle tracking method gave much higher travel times than the peak displacement method table 2 different results by these two methods may be associated with different rainfall events selected for these calculations the peak displacement method calculates the travel times during frequent and heavy precipitation events precipitation events from 2005 2006 while particle tracking assesses the travel times over longer periods zhou et al 2021 notably water travel times in the non frac scenario obtained by the particle tracking method are most consistent with the approximate estimate of 41 weeks provided by previous studies with similar crops and areas stumpp et al 2009 it is worth mentioning that asadollahi et al 2020 pointed out that the sas approach was a good alternative for estimating water travel times when the system was too complicated to be fully described by the hydrus 1d model our study demonstrates that the water flow based particle tracking module in hydrus 1d is another promising way of constraining estimation errors in water travel times especially when there is not enough isotope data to calibrate the lumped or physically based isotope transport models in contrast considering fractionation using either the cg or gonfiantini models will likely lead to larger water travel time estimates than in the non frac scenario table 2 this is because fractionation scenarios result in a larger dispersivity to increase the dispersion of isotopes or lower hydraulic conductivities to decrease convection of isotopes as discussed in section 4 1 4 2 2 estimation of the evaporation flux for evaporation estimation the isotope transport based methods for different fractionation cg frac gon frac and meas frac scenarios can give comparable results to the water flow based methods including laboratory measurements and the hydrus 1d water balance in contrast the non frac scenario can produce similar results only when using the water flow based method hydrus 1d water balance however since the measured evaporation flux was used as the upper boundary condition in this arid conditions example it is not clear whether the similarity between estimated evaporation amounts using the hydrus 1d water balance method in the non frac and fractionation cg frac gon frac meas frac scenarios was due to this boundary condition or because actual soil hydraulic conductivities and water contents were continuously adjusted to actual soil fluxes without ever reaching full saturation however it is clear that evaporation fractionation has a significant impact on the isotope transport and isotopic compositions in arid conditions as shown in fig 9 therefore the direct use of simulated isotopic compositions in the non frac scenario may result in large biases in practical applications in arid conditions as seen from the evaporation estimation results in table 4 4 3 comparison of different climate conditions and implications for future studies the soil saturated hydraulic conductivities k s and the retention curve shape parameter α were the parameters most affected by the consideration of evaporation fractionation for the humid condition dataset table 1 for the arid condition dataset these were the dispersivity λ and the retention curve shape parameter α table 3 this is likely associated with the effects of soil texture on retention curves and soil moisture conditions in different climate zones radcliffe and šimůnek 2018 overall soil water retention and hydraulic conductivity curves fig s12 in different fractionation scenarios were more similar for the braud et al 2009a dataset than the stumpp et al 2012 dataset fig s8 one reason is that the measured evaporation flux was used as the upper bc in the former which constrains the model flexibility another reason is that there was only one soil layer in the braud et al 2009a dataset while there were three soil layers in the stumpp et al 2012 dataset there is likely a compensation effect between the parameters of different layers and thus the parameter values can vary more in the stumpp et al 2012 dataset while evaporation fractionation plays an essential role in parameter estimation in both cases its impact on model performance is relatively small in the example for humid conditions but more significant in the example for arid conditions as discussed in sections 4 1 and 4 2 this is expected since evaporation plays a more important role in the water balance of the arid dataset table 4 than in the humid dataset fig s13 these conclusions also indirectly validate the common assumption that evaporation fractionation may be neglected in some humid regions but not in arid areas sprenger et al 2016a however parameter sensitivities and optimization results reflect complex combined effects of climate soil and vegetation characteristics the isotopic composition of soil water is not only affected by evaporation fractionation but also by the mixing of rainfall with soil water and different flow paths in the soil leading to its variations with depths and time the insufficient knowledge of the spatiotemporal isotope distribution e g in shallow and deep depths or during different stages of evaporation and the lack of such information in the objective function may bias the parameter estimation results for example not including isotopes from different soil depths within the soil profile might lead to an underestimation of evaporation fractionation in general biased estimation of water mixing within the profile and a similar isotopic signal in the discharge in this study we considered either the time series of the isotopic composition of the bottom flux in the stumpp et al 2012 dataset or the final isotopic composition profile in the braud et al 2009a dataset in addition observation data types and spatiotemporal distributions are different for these two datasets and this difference may affect the comparison of parameter estimation results between different climate conditions the gsa was carried out for the non frac scenario for the stumpp et al 2012 dataset and the meas frac scenario for the braud et al 2009a dataset because they were closest to the experimental conditions this implicitly assumes that sensitivity remains the same for different model structures however different model structures may affect gsa and pso results which should be further explored last but not least the impacts of possible transpiration fractionation as observed in multiple studies should also be included in future analyses e g barbeta et al 2019 therefore it is difficult to generalize the results of this study or apply them to other specific conditions 5 summary and conclusions in this study we analyzed parameter estimation results for two datasets collected under humid and arid climate conditions using the isotope transport model in which we either did or did not consider evaporation fractionation the global sensitivity analysis using the morris and sobol methods and the parameter estimation using the particle swarm optimization algorithm highlight the significant impacts of considering evaporation fractionation on parameter estimation and model performance the kge index for isotope data can increase by 0 09 and 1 49 for the humid and arid datasets respectively when selecting suitable fractionation scenarios the impact of different parameter values estimated when considering or not evaporation fractionation propagates into practical applications of isotope transport modeling the isotope transport based method peak displacement gave much lower water travel times than the water flow based method particle tracking for humid conditions considering fractionation using the cg and gonfiantini models will likely lead to larger water travel time estimates and ages for rwu for arid conditions example the isotope transport based method isotope mass balance can provide comparable evaporation estimates for different fractionation cg frac gon frac meas frac scenarios as the water flow based methods hydrus 1d water balance and laboratory measurements in contrast the non frac scenario can produce reasonable evaporation estimation only when using the water flow based method the direct use of simulated isotopic compositions in the no fractionation scenario may result in large biases in practical applications in arid regions where evaporation fractionation is more extensive than in humid areas integrated use of water flow and isotope transport based methods may provide mutual validation and be an important way to avoid this problem this research may shed some light on future laboratory and field experimental designs regarding the practical applications of the isotope transport modeling in different climate zones credit authorship contribution statement tiantian zhou conceptualization methodology software writing original draft jirka šimůnek conceptualization funding acquisition methodology software writing review editing isabelle braud methodology writing review editing paolo nasta conceptualization writing review editing giuseppe brunetti methodology software writing review editing yi liu software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was financially supported by the multistate w4188 program funded by nifa grant no ca r ens 5047 rr we acknowledge paolo benettin from the laboratory of ecohydrology enac iie echo école polytechnique fédérale de lausanne epfl lausanne switzerland for his useful suggestions we also appreciate the editors and reviewers for their constructive comments on this manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128100 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3199,choosing a suitable process oriented eco hydrological model is essential for obtaining reliable simulations of hydrological processes determining soil hydraulic and solute transport parameters is another fundamental prerequisite research discussing the impact of considering evaporation fractionation on parameter estimation and practical applications of isotope transport models is limited in this study we analyzed parameter estimation results for two datasets for humid and arid conditions using the isotope transport model in hydrus 1d in which we either did or did not consider fractionation the global sensitivity analysis using the morris and sobol methods and the parameter estimation using the particle swarm optimization algorithm highlight the significant impact of considering evaporation fractionation on inverse modeling the kling gupta efficiency kge index for isotope data can increase by 0 09 and 1 49 for the humid and arid datasets respectively when selecting suitable fractionation scenarios differences in estimated parameters propagate into the results of two practical applications of stable isotope tracing i the assessment of root water uptake rwu and drainage travel times i e the time elapsed between water entering the soil profile as precipitation and leaving it as transpiration or drainage in the lysimeter humid conditions and ii evaporation estimation in a controlled experimental soil column arid conditions the peak displacement method with optimized longitudinal dispersivity provides much lower travel times than those obtained using the particle tracking algorithm in hydrus 1d considering evaporation fractionation using the craig gordon cg and gonfiantini models is likely to result in estimates of older water ages for rwu than the no fractionation scenario the isotope mass balance method that uses the isotopic composition profile simulated by hydrus 1d while considering fractionation using the cg and gonfiantini models or the measured evaporation isotope flux provides comparable results in evaporation estimation as the hydrus 1d water mass balance method and direct laboratory measurements in contrast the no fractionation scenario reasonably estimates evaporation only when using the hydrus 1d water mass balance method the direct use of simulated isotopic compositions in the no fractionation scenario may result in large biases in practical applications in the arid zone where evaporation fractionation is more extensive than in humid areas keywords hydrus 1d global sensitivity analysis particle swarm optimization water travel time temporal origin evaporation estimation data availability data will be made available on request 1 introduction reliable water balance simulations in the vadose zone are important to understand and forecast the impact of anthropogenic disturbances such as global warming and land use change on soil water storage groundwater recharge and evapotranspiration a detailed mechanistic understanding of water fluxes in the vadose zone could support optimal and efficient management strategies for promoting the long term sustainability of water resources and associated ecosystem functions penna et al 2018 for example the exact quantification of evaporation affects water availability for plants nelson et al 2020 and constrains groundwater recharge condon et al 2020 however the conventional methods e g pan experiments for estimating evaporation fluxes often require extensive field monitoring of water flow which is often time consuming expensive labor demanding and affected by considerable uncertainty skrzypek et al 2015 stable isotopes of hydrogen 2h and oxygen 18o are widely used to trace water fluxes across the critical zone and can be expressed as isotopic ratios 2h 1h and 18o 16o by using the δ notation i e δ2h and δ18o the isotopic composition of shallow soil water provides insights into evaporation fractionation characteristics this information can be easily used to calculate corresponding evaporation fluxes for example skrzypek et al 2015 combined the equations for evaporation estimation based on the revised craig gordon model craig and gordon 1965 and developed a software hydrocalculator using this software they estimated evaporation losses and validated its results using pan measurements this method has been extended to soil evaporation estimation for example sprenger et al 2017 estimated that evaporation was about 5 and 10 of infiltrating water in the heath and scots pine soils respectively while the spatial origin of the water plants use has been widely studied e g allen et al 2019 very little is known about its temporal origin brinkmann et al 2018 miguez macho and fan 2021 to track water across the critical zone we need to assess how fast water moves down to the soil profile bottom and when and how much water returns to the atmosphere through root water uptake rwu the premise is to accurately estimate travel times tt of irrigation precipitation water i e the time between water entering the soil profile as irrigation precipitation and leaving it back to the atmosphere as transpiration or at the soil profile bottom as drainage the peak displacement method represents the most widespread technique to estimate travel time from the time difference between signals in soil water stable isotope time series directly measured at specific soil depths chesnaux and stumpp 2018 koeniger et al 2016 stumpp et al 2012 however this method is unfeasible when there is no pronounced peak correspondence between isotopic compositions of precipitation and drainage water samples another widely used isotope transport based method is to inversely estimate the parameters for time invariant tt distributions ttds e g timbe et al 2014 or time variant storage selection sas functions benettin and bertuzzo 2018 harman 2015 rinaldo et al 2015 implemented in lumped hydrological models such oversimplified models are based on few soil and vegetation parameters but have limitations in describing transient conditions or simulating isotope transport sprenger et al 2016b in contrast isotope transport can be reliably simulated using the richards equation based hydrological models with appropriate soil and vegetation parameters and known boundary and initial conditions however direct measurements of soil hydraulic and transport parameters required by such models are time consuming and labor demanding therefore such parameters are commonly obtained using inverse modeling by minimizing the errors between easily measured state variables and fluxes e g soil water contents and pressure heads at different soil depths or leachate water volumes and corresponding model simulations hopmans et al 2002 mertens et al 2006 vrugt et al 2008 wollschlager et al 2009 wohling and vrugt 2011 nevertheless it is not always necessary to account for all model parameters in parameter optimization since some can be fixed as they can be either determined experimentally or have a minor impact on the model output the latter can be determined using the global sensitivity analysis gsa the sobol and morris methods are among the two most widespread gsa methods liu et al 2020 the sobol method provides the most accurate sensitivity indices but it requires several model runs and is thus computationally intensive gatel et al 2019 in contrast the morris method cannot yield the order of the most sensitive parameters as accurately as the sobol method but its computational cost is much lower and it can still pinpoint the most influential parameters campolongo et al 2007 herman et al 2013 many inverse modeling algorithms can be used for parameter estimation for example the levenberg marquardt optimization lmo proved to be very efficient and was therefore implemented in hydrus šimůnek et al 2008 however the lmo is sensitive to the initial parameter values provided by the user and often falls into local instead of global minimum brunetti et al 2016 thus global optimization algorithms such as particle swarm optimization pso have become more widespread over the last decades e g vrugt and robinson 2007 when optimizing isotope transport parameters via inverse modeling isotopic compositions from multiple soil depths must be included in the objective function and combined with other state variables and fluxes for example research shows that the model calibration can be improved by simultaneously considering stable isotopes and soil moisture information sprenger et al 2015 groh et al 2018 mattei et al 2020 however the correct model structure is a fundamental prerequisite to obtaining successful simulations in particular research discussing the impact of considering evaporation fractionation on parameter estimation and practical applications of isotope transport models is limited penna et al 2018 therefore we pose two scientific questions first how will the consideration of evaporation fractionation affect the parameter estimation results of the isotope transport model second how will this effect propagate into practical applications such as water travel times and evaporation estimation to answer these questions we compare the parameter estimation results obtained using the isotope transport model in hydrus 1d zhou et al 2021 that does or does not consider evaporation fractionation for two available datasets 1 a 150 cm thick layered soil profile in a lysimeter under humid climate where evaporation fractionation is negligible 2 a 35 cm thick soil column subject to evaporation where evaporation fractionation process is dominant the accuracy of the parameterization obtained by the pso algorithm is assessed based on its ability to reproduce measured water fluxes and isotope transport data the parameters estimated while considering or not evaporation fractionation are then used to calculate travel times and evaporation 2 materials and methods two experimental datasets are considered in this study the first dataset is collected using a field lysimeter 150 cm thick layered soil profile located in austria under humid climate conditions stumpp et al 2012 section 2 1 1 the second dataset is collected using a 35 cm thick soil column in france subject to evaporation to mimic arid climate conditions braud et al 2009a section 2 1 2 numerical simulations of water flow and isotope transport with and without evaporation fractionation are implemented in hydrus 1d the modeling setup is briefly described in section 2 2 and method s1 in the supplementary material the sensitivity analysis based on the sobol and morris methods is performed to evaluate the interactions between soil hydraulic and solute transport parameters and the impact of multiple measured data types section 2 3 method s2 and results s1 s2 the accuracy of the parameterization obtained by the pso algorithm is assessed based on its ability to reproduce the observed data sections 2 4 3 1 1 and 3 2 1 the parameters estimated while considering or not considering evaporation fractionation are then used to calculate travel times and evaporation and quantify the impact of their different estimates sections 2 5 2 6 3 1 2 and 3 2 2 the effects of varying climate conditions and estimation methods are then compared and illuminated section 4 the schematic outline of the different methods used is shown in fig 1 2 1 site description and data availability 2 1 1 stumpp et al 2012 dataset the first dataset is taken from the lysimeter 3 of stumpp et al 2012 available at https www pc progress com en default aspx h1d lib isotope the field experiment was conducted in a humid region located at the research area of the hblfa höhere bundeslehr und forschungsanstalt für landwirtschaft raumberg gumpenstein in gumpenstein austria this area has a mean annual temperature of 6 9 c and average annual precipitation p of 1035 mm the annual potential evapotranspiration et 0 for grass reference during the experiment period may 2002 to february 2007 calculated by the penman monteith equation is about 557 mm and the corresponding aridity index p et 0 is about 1 86 corresponding to a humid climate class liang 1982 the cylindrical lysimeter with a depth of 150 cm and a surface area of 10000 cm2 was embedded in a rainfed agricultural field cambisol planted with winter rye and fertilized with liquid cattle slurry the observation period was from may 2002 to february 2007 1736 days table s1 shows the summary of the observed data the temporal distribution of p et 0 soil surface temperature t s air relative humidity rh and leaf area index lai during the simulation period are shown in fig 2 more details about data acquisition including meteorological parameters and root water uptake information can be found in stumpp et al 2012 2 1 2 braud et al 2009a dataset braud et al 2009a designed a rubic iv experiment that started on april 11 2005 corresponding to day of the year doy 101 and lasted 338 days the experiment consisted of 6 columns 12 cm in diameter and 35 cm in height the soil columns were filled with a silt loam collected at the field station of lusignan france and wetted using demineralized water of the known isotopic composition the bottom was closed by clay marbles the soil was initially saturated and subject to evaporation only dry air was simultaneously injected over all six columns the isotopic composition of the air changed due to water vapor released by evaporation from soil columns the air was finally trapped in a cryoscopic device which allowed the determination of evaporation fluxes from bare soil columns and the corresponding isotopic composition of the water vapor under non steady state conditions more details about the experimental setup can be found in figs 1 2 of braud et al 2009a the data collected in column 2 ending at doy 264 were analyzed in this study thirteen variables were measured continuously at a frequency of about 15 min to assess the water balance of the soil column these variables included the room temperature the atmospheric pressure the absolute pressure of the dry air before it entered the soil column air mass flow for the humidity control above the soil column the mass of the soil column air temperature and humidity at the outlet of the soil column the temperatures of the cryoscopic trapping downstream and upstream of the columns and the air temperature and residual air humidity at the outlets of two cold traps the vapor was trapped twice a day during the first three months and only once a day after that once evaporation decreased soil column 2 was dismantled on september 21 2005 doy 264 to sample liquid water and measure the gravimetric soil water content more details about data acquisition can be found in braud et al 2009a the temporal distributions of the evaporation flux e the isotopic composition of the evaporation flux δ e outlet air temperature t air and outlet air relative humidity rh during the simulation period are shown in fig 3 2 2 model setup the hydrus 1d model modified by zhou et al 2021 to simulate the transport of soil water isotopes while considering evaporation fractionation was used in this study a brief summary of the model setup including the governing equations without and with vapor flow for the stumpp et al 2012 and braud et al 2009a datasets respectively boundary conditions bcs and model inputs is shown in figs 4 5 more details can be found in zhou et al 2021 2 2 1 stumpp et al 2012 dataset the soil profile was 150 cm deep and was discretized into 151 nodes it consisted of three different soil horizons 0 29 cm 30 89 cm 90 150 cm the initial pressure head profile was assumed to be at hydrostatic equilibrium with the pressure head h 150 cm at the soil surface the weighted average δ18o of precipitation 9 5 and estimated temperature 20 were used as initial conditions the atmospheric with a surface layer and seepage face boundary conditions bc were used for water flow at the upper and lower boundaries respectively the temperature bc was used for heat transport at both boundaries in this humid condition example evaporation fractionation was limited to the soil surface due to the lack of the vapor phase within the soil the solute flux and zero concentration gradient bcs were used for isotope transport at the upper and lower boundaries respectively the isotope flux associated with evaporation was calculated either assuming no fractionation or using the craig gordon or gonfiantini fractionation models hereafter referred to as non frac cg frac and gon frac respectively the non frac scenario calculated the isotope flux of evaporation by assuming that the isotopic composition of the evaporation flux was the same as that of surface soil water the isotopic composition of the atmospheric water vapor δ a in the cg frac scenario was estimated based on its equilibrium relationship with the isotopic composition of rainfall skrzypek et al 2015 the gon frac scenario was simplified without the need for the isotopic composition of the atmospheric water vapor to consider fractionation zhou et al 2021 a detailed description of the cg and gonfiantini models can be found in method s1 for simplification only equilibrium fractionation was considered at the soil surface since kinetic fractionation could be neglected in this example zhou et al 2021 in other words the kinetic fractionation coefficient n k in eq 11 of zhou et al 2021 was set to 0 and thus the kinetic fractionation factor at the soil surface α i k in the cg frac and gon frac scenarios eqs s2 s3 was equal to 1 2 2 2 braud et al 2009a dataset the simulated soil profile was 35 cm deep and was discretized into 132 nodes following braud et al 2009a the soil column was initially almost fully saturated with the measured initial pressure head increased linearly from 1 cm at the soil surface to 35 cm at the soil profile bottom the observed initial soil temperature and δ18o were 24 25 and 6 34 respectively the temperature bc was used for heat transport at both surface and bottom boundaries using temperatures measured at 2 5 and 24 cm depths respectively the atmospheric and zero flux bcs were used for water flow at the upper and lower boundaries respectively the measured evaporation flux e was used as the upper bc for water flow in this arid condition example evaporation fractionation occurred both at the soil surface and within the soil due to the existence of the vapor phase the stagnant air layer bc which had been modified to account for evaporation fractionation and zero flux bc were used for isotope transport at the upper and lower boundaries respectively the surface isotope flux associated with evaporation was calculated either assuming no fractionation using the craig gordon or gonfiantini fractionation models or using the measured values hereafter referred to as non frac cg frac gon frac and meas frac respectively the non frac scenario calculated the isotope flux of evaporation by assuming that its isotopic composition was the same as that of surface soil water i e no fractionation at the soil surface and equilibrium and kinetic fractionation factors within the soil α α i d were equal to 1 i e no fractionation within the soil the theory of cg frac and gon frac scenarios was explained in method s1 for simplification the kinetic fractionation coefficient n k in eq 11 of zhou et al 2021 was set to 1 and thus the kinetic fractionation factor at the soil surface α i k in the cg frac and gon frac scenarios eqs s2 s3 was equal to 1 0324 the measured isotopic composition of the outlet water vapor δ e was used in the meas frac scenario to calculate the surface isotope flux e i corresponding to the evaporation flux e more details about how upper boundary fluxes were calculated can be found in braud et al 2009a 2 3 global sensitivity analysis five soil hydraulic parameters i e θ r θ s n α and k s need to be optimized for each layer of the soil profile to simulate water flow using the hydrus 1d model the residual water content θ r was set to zero to reduce the number of fitting parameters to simulate isotope transport in the soil the longitudinal dispersivity λ also needs to be optimized since only the isotopic composition of the lysimeter discharge was measured in the stumpp et al 2012 dataset the dispersivity of three individual soil layers cannot be estimated therefore only one longitudinal dispersivity for the entire lysimeter was estimated therefore the total number of parameters p was 13 and 5 for the stumpp et al 2012 and braud et al 2009a datasets respectively the global sensitivity analysis gsa using both morris and sobol methods was conducted in this study to determine the most influential parameters and their interactions the detailed description of these two methods is shown in method s2 in the supplementary material the sensitivity analysis was conducted using python s sensitivity analysis library salib herman and usher 2017 the script produces the input parameter space overwrites the input parameters file and runs the executable module of hydrus 1d for each simulation of the stumpp et al 2012 dataset five kling gupta efficiency kge indices for different evaluation indicators were calculated including for the time series of the bottom water flux kge bf the soil water content at different depths kge wc the bottom water isotopic composition kge wi the water retention curves kge rc and the average of the four kge values kge avg for each simulation of the braud et al 2009a dataset three kling gupta efficiency kge indices for different evaluation indicators were calculated including the final soil water content profile kge wc the final water isotopic composition profile kge wi and the average of the two kge values kge avg the kge index compares the correlation coefficient r the ratio of mean values β and the ratio of variances γ between simulated and observed data the value of the kge index is always smaller or equal to 1 the higher the kge value the better fit between the simulated and observed values the positive and negative kge values are often considered good and bad solutions knoben et al 2019 1 k g e 1 1 r 2 1 β 2 1 γ 2 0 5 if a hydrus 1d run was not finished within a prescribed time i e 30 s and 60 s for the stumpp et al 2012 and braud et al 2009a datasets respectively or the length of the modeled hydrograph was shorter than the total simulation period 1736 and 163 days for the stumpp et al 2012 and braud et al 2009a datasets respectively it was considered non convergent the run was then terminated and a large negative value 1e 7 was prescribed to the objective function non convergent runs in gsa are a frequent problem when using nonlinear environmental hydrological models and there are no clear indications on how to handle these unfeasible points razavi et al 2021 removing or skipping them alters the sampling trajectory and can result in biased conclusions especially if non convergent runs lie in informative regions of the parameter space recently sheikholeslami et al 2019 compared strategies such as median substitution single nearest neighbor or response surface modeling brunetti et al 2017 to fill in for model crashes their results show that interpolating non convergent runs with a radial basis function trained in the vicinity of that point leads to reliable results and outperforms other strategies we implemented a similar approach in the present work but with important differences in particular 1 for each non convergent point we calculated its euclidean distance from all other convergent points in the gsa sample 2 convergent points were ordered in ascending order i e from the closest to the farthest 3 the 100 closest convergent points were used to train a response surface surrogate based on the kriging partial least squares method kpls bouhlel et al 2016 which outperforms traditional kriging on high dimensional problems 4 the trained kpls surrogate was finally used to interpolate non convergent runs in the original gsa sample the use of multiple localized surrogates allowed for better reconstruction of the topological features of the response surface in the vicinity of the non convergent points in this study the global sensitivity analysis was combined with the monte carlo filtering to identify reduced ranges of parameters with good solutions for subsequent parameter optimization potential solutions were filtered into good solutions with kge 0 0 and bad solutions with kge 0 0 kernel density estimation kde plots were then used to identify areas with high density good solutions while the correlation analysis was conducted to determine interactions between parameters and may help reduce the input factor space more details can be found in brunetti et al 2016 this type of procedure shares multiple similarities with the generalized likelihood uncertainty estimation glue proposed by beven and freer 2001 the joint use of the gsa sample with the glue approach i e gsa glue ratto et al 2001 allows for obtaining a rough assessment of the parameters uncertainty and successful estimates of soil hydraulic parameters e g brunetti et al 2018 2 4 parameter optimization the particle swarm optimization pso algorithm was used in this study for parameter optimization in the pso a swarm of candidate solutions is moved around in the search space according to a few equations the movement of the particles is guided by the optimal position of themselves and the whole swarm once improved positions are discovered they are used to guide the swarm s movement this process is repeated until the global optimal position that all particles tend to follow is found shi and eberhart 1998 the pso parameters cognitive parameter c 1 0 267 social parameter c 2 3 395 inertia weight w 0 444 from brunetti et al 2016 were used in this study the number of particle swarm and iterations are 40 and 200 respectively the pyswarm library in python was used for the pso the process was similar to the gsa except that reduced ranges of parameters were used in this way the number of potential local minima is reduced and the convergence improves only the set of parameters leading to the maximum kge avg i e minimum 1 kge avg as the objective function was retained as optimized parameters 2 5 first practical application calculation of drainage and rwu travel times 2 5 1 the peak displacement isotope transport based method the peak displacement method estimates travel times from the time lag between signals in the measured input rainfall isotopic composition and output drainage isotopic composition isotope time series in the stumpp et al 2012 dataset a pronounced correspondence was observed between the depleted precipitation peak in the winter november 18 2005 to april 14 2006 and the lysimeter discharge the mean drainage travel time t o t accounting for dispersion effects can be calculated by the mean peak isotopic composition lag time t m t using eq 2 2 t o t m 1 3 λ l 2 3 λ l where l is the lysimeter length l more details can be found in stumpp et al 2012 in this study t m from stumpp et al 2012 and dispersivities λ optimized using hydrus 1d assuming different fractionation scenarios were used 2 5 2 the particle tracking water flow based method the particle tracking algorithm is based on the water mass balance calculation the initial position of the particles is defined using the initial water content distribution depending on the precipitation irrigation inputs the particles may be released at the soil surface and leave at the soil profile bottom in this study the input parameters wstand the initial distribution and wprec the upper bc distribution for the particle tracking algorithm were set to 10 cm and a negative number which triggers the option of releasing particles with each rain event respectively more details about the particle tracking algorithm can be found in šimůnek 1991 or zhou et al 2021 when knowing the positions of the particles at different times the residence time rt and locations of water from all precipitation irrigation events can be obtained i e the residence time distribution rtd note that the particle travel time tt is the sum of the particle age i e residence time and life expectancy i e time to reach the destination the former is the time elapsed since the particle release while the latter is the remaining time before the particle reaches the outlet benettin et al 2015 therefore when the particles leave the lysimeter bottom or as root water uptake rwu their residence times can be called drainage or rwu travel times respectively the particle tracking module additionally assesses rwu between two neighboring particles as a function of time when particles are released for each precipitation event we can precisely evaluate the contribution of each precipitation event to rwu at different times we can then infer the temporal origin of rwu by synthesizing this information different fractionation scenarios with the soil hydraulic parameters optimized using hydrus 1d were used to run the particle tracking module to calculate drainage and rwu travel times 2 6 second practical application calculation of evaporation flux 2 6 1 the water flow based method braud et al 2009a calculated evaporation using three methods the first method determines the evaporation rate by continuously measuring the vapor flux and humidity at the outlet of the soil column the second method obtains the evaporation rate by repeatedly weighing the soil column finally the third method determines the evaporation rate by weighting the mass of the frozen water trapped at the outlet of the soil column these three methods are hereafter referred to as direct measurement column weighting and trapped volume respectively this study presents these results also as the reference for other methods more details can be found in braud et al 2009a another water flow based method used in this study to calculate water flux components was to analyze the water mass balance simulated in hydrus 1d e g sutanto et al 2012 2 6 2 the isotope transport based method for an isolated water volume with an initial isotopic composition δ 0 evaporating into the atmosphere the isotopic composition of the residual liquid water δ s can be calculated as benettin et al 2018 3 δ s δ 0 δ 1 f e xm δ where δ is the limiting isotopic composition that would be approached when water is drying up xm is the temporal enrichment slope and fe is described below eq 3 is based on the isotope mass balance equations of gonfiantini 1986 and the isotopic composition of the evaporation flux estimated by the craig gordon model craig and gordon 1965 more details about the derivations can be found in gonfiantini 1986 this equation implies that the isotopic composition of soil water only changes due to evaporation fractionation the ratio of the evaporation loss to the initial water storage f e can be then estimated as sprenger et al 2017 4 f e 1 δ s δ δ 0 δ 1 xm the two variables δ and xm can be calculated as benettin et al 2018 5 δ r h δ a ε k ε α r h 1 0 3 ε k ε α 6 xm r h 1 0 3 ε k ε α 1 r h 1 0 3 ε k where δ a is the isotopic composition of the atmospheric water vapor rh is the air relative humidity α is the dimensionless equilibrium fractionation factor while ε and ε k are equilibrium and kinetic fractionation enrichments respectively details about the calculation procedure for these parameters α ε ε k can be found in benettin et al 2018 or zhou et al 2021 the equivalent kinetic fractionation factor within the soil α i d used to calculate ε k was optimized manually to get the best match of f e with those from water flow based methods in section 2 6 1 the fraction of water that evaporated before the end of the braud et al 2009a experiment was calculated in this study average measured values of rh t air t s and δ 0 during the experiment and the final isotope profile simulated using hydrus 1d were used in the above equations 3 results 3 1 stumpp et al 2012 dataset analysis 3 1 1 parameter optimization and model performance the global sensitivity analysis and monte carlo filtering results for the stumpp et al 2012 dataset are shown in the results s1 section of the supplementary material overall soil hydraulic parameters of different layers had comparable impacts on the model outputs the order of sensitive parameters is shape parameters of the water retention function namely n and α saturated water content θ s saturated hydraulic conductivity k s and dispersivitie λ the final optimized soil hydraulic and solute transport parameters and corresponding kges are shown in table 1 considering evaporation fractionation impacted parameter estimation significantly especially in the optimization of the soil saturated hydraulic conductivity k s and shape parameter α overall the water retention and soil hydraulic conductivity curves fig s8 differed greatly between different fractionation scenarios in the third layer but were relatively similar in the first and second layers the water retention curve in the gon frac scenario best matched the measured one but did not outperform those from the cg frac and non frac scenarios as seen from the kge rc values in table 1 compared with the cg frac and gon frac scenarios the water retention curve in the non frac scenario had a steeper decline and a lower saturated water content in the third layer while it became more gradual with higher saturated water contents in the first and second layers however the non frac scenario always produced higher hydraulic conductivities than the cg frac and gon frac scenarios note that the non frac scenario also had higher hydraulic conductivities in the third layer because of relatively higher matric potentials the fits for different fractionation scenarios are shown in fig 6 the isotopic composition of the lysimeter discharge remained the same for different fractionation scenarios during about the first 150 days and started deviating after this time but the trends were still similar except for some vertical shifts different fractionation scenarios resulted in a similar average fitting performance kge avg within 0 03 the non frac scenario had the highest kge wi i e for water isotopic composition followed by the cg frac scenario while the gon frac scenario performed the worst the difference between kge wi indices for different fractionation scenarios was within 0 09 3 1 2 first practical application drainage travel times and rwu temporal origin the mean travel times mtts of drainage i e from the surface to the bottom estimated by the peak displacement method are shown in table 2 the mtts were 251 9 251 9 and 257 1 days for the non frac cg frac and gon frac scenarios respectively the consideration of fractionation using the gonfiantini model slightly overestimated the travel times compared to the non frac scenario however the difference was not very evident within 6 days for different fractionation scenarios fig s9 shows the spatial temporal distribution of particles simulated using the soil hydraulic parameters estimated considering different fractionation scenarios the residence time distribution rtd of soil water is displayed in fig 7 the mean residence time mrt the mean of rts averaged over the entire simulation duration increased with soil depth in all scenarios due to a time lag involved in water transfer the mrts for the non frac scenario for depths of 30 70 and 110 cm were 82 1 138 2 and 203 6 days respectively the mrts for the cg frac scenario for 30 70 and 110 cm depths were 69 9 170 0 and 258 5 days respectively finally the mrts for the gon frac scenario for 30 70 and 110 cm depths were 80 6 174 3 and 270 6 days respectively in terms of temporal distribution rts showed five distinct seasonal cycles specifically they had a trough after every rainy season and a peak after every dry season showing a pronounced lag effect in other words rts were determined by the trade off between precipitation input and evapotranspiration removal corresponding travel times of drainage are shown as probability density distribution histograms in fig s10 and summarized in table 2 the means and standard deviations of travel times were 297 5 79 96 356 8 104 29 and 369 9 101 24 days for the non frac cg frac and gon frac scenarios respectively the particle tracking method produced significantly higher travel times by about 89 days than the peak displacement method similarly considering fractionation using the cg frac and gon frac scenarios led to longer travel times tts than the non frac scenario in addition the difference was very evident reached 72 days for different scenarios to further explore and quantify the rtd differences when considering different fractionation models the temporal origin of rwu is plotted in fig 8 fig 8 shows the monthly transpiration sums in the upper panels and fractional contributions of water of a certain age origin to these monthly transpiration sums in the lower panels note that the amount and temporal distribution of transpiration were similar under different fractionation scenarios 54 95 53 91 and 54 03 cm for non frac cg frac and gon frac respectively therefore only the temporal distribution of transpiration in the non frac scenario is displayed as for the age distribution of rwu for example in the non frac scenario the yellow line in 2002 indicates that about 29 of the water taken up by roots in august was older than may while the remaining 71 was from may august of 2002 5 from june 16 from july and 50 from august more details about how to read the age distribution of rwu can be found in fig 5 of brinkmann et al 2018 the maximum water age for rwu for different fractionation scenarios was almost the same about 300 d in october 2002 330 d in september 2003 270 d in november 2004 and 180 d in february 2006 except for 240 d in december 2003 and 180 d in february of 2005 for the non frac scenario these results were consistent with water residence times at the maximum rooting depths in fig 7 however different fractionation scenarios had relatively large impacts up to three months on the minimum water age for rwu the most obvious example was the 2003 growing season a relatively dry year with less precipitation as shown in fig 2 the minimum water age for rwu in 2003 was within about a month for the gon frac scenario and 120 d february for the non frac and cg frac scenarios in addition the dynamics of fractional monthly contributions to rwu also varied between different scenarios in general the water age for rwu was far longer in dry years 2003 2004 than in wet years 2005 2006 suggesting that drought can promote crop uptake of old water in the same growing season the water age for rwu was consistently lower in may and june than in july and august which reflected an increase in the rooting depth 3 2 braud et al 2009a dataset analysis 3 2 1 parameter optimization and model performance the global sensitivity analysis and monte carlo filtering results for the braud et al 2009a dataset are shown in the results s2 section of the supplementary material the most sensitive parameters were shape parameters n and saturated water contents θ s the final optimized soil hydraulic and solute transport parameters and corresponding kges are shown in table 3 considering or not evaporation fractionation also impacted parameter estimation significantly the most significant impacts were on dispersivity λ and the shape parameter α table 3 the soil water retention curves fig s12 showed that the wilting points were almost identical for the non frac and fractionation cg frac gon frac meas frac scenarios however the saturated water contents were higher and water contents started to drop later in the fractionation scenarios than those in the non frac scenario the soil hydraulic conductivity curves fig s12 showed that the saturated hydraulic conductivities were very similar but the hydraulic conductivities in the fractionation scenarios were a little higher than those in the non frac scenario the fits of soil profile isotopic compositions for different fractionation scenarios are shown in fig 9 the non frac scenario had an almost uniform isotopic composition profile in this case the parameter optimization depended mainly on the measured soil water content profile in fractionation scenarios the peak value of the isotopic composition profile in the meas frac scenario was smaller than those in the gon frac and cg frac scenarios while the value of dispersivities was the opposite different fractionation scenarios resulted in significantly different average fitting performances kge avg reached 0 72 the meas frac scenario had the highest kge wi i e for soil water isotopic composition followed by gon frac and cg frac scenarios while the non frac scenario performed the worst the difference between kge wi indices for different fractionation scenarios reached 1 49 3 2 2 second practical application estimation of evaporation flux table 4 shows cumulative evaporation obtained using different measurements and simulated considering different fractionation scenarios the average isotopic composition of the whole profile was calculated using soil water contents and the column depth as weights cumulative evaporation was estimated to account for about 64 4 63 1 and 65 6 of the initial soil water storage in the cg frac gon frac and meas frac scenarios respectively these values for the cg frac gon frac and meas frac scenarios were slightly lower than but comparable to laboratory measurements and the hydrus 1d water balance slight differences may have been caused by uncontrollable measurement errors in the isotopic composition of the atmospheric water vapor δ a in eq 5 which is the most sensitive parameter in the isotope mass balance method skrzypek et al 2015 cumulative evaporation cannot be estimated using this method in the non frac scenario since no isotopic enrichment occurred i e δ s δ 0 in eq 4 4 discussion 4 1 impacts of evaporation fractionation on parameter estimation and model performance for the stumpp et al 2012 dataset as indicated in section 3 1 1 the fractionation scenarios cg frac and gon frac had lower hydraulic conductivities than the non frac scenario this is because fractionation decreases the isotope flux by evaporation compared with a no fractionation scenario the isotopic composition of the evaporation flux cannot be greater than that of surface soil water and thus increases the isotope flux by net infiltration to get a good fit between simulated and observed isotopic compositions of discharge water the inverse modeling yields a larger longitudinal dispersivity to increase the dispersion of isotopes table 1 or lower hydraulic conductivities to decrease downward convection of isotopes fig s8 the simulated isotopic composition of the lysimeter discharge remained the same for different fractionation scenarios during about the first 150 d and started deviating after this time fig 6 this suggests that it takes about 150 d before the impact of different treatments of the upper bc for isotope transport propagates to the soil profile bottom and affects the isotopic composition in drainage water zhou et al 2021 this time interval i e about 150 d is much smaller than the travel time of the first particle released at the soil surface as calculated by the particle tracking method fig s9 this is because the particle tracking algorithm considers only piston flow while dispersion accelerates the arrival of isotopes to the soil profile bottom however the trends are still similar except for some vertical shifts since kge wi values did not differ much for different fractionation scenarios within 0 09 fig 6 and table 1 considering or not evaporation fractionation does not significantly impact the isotopic composition in discharge water in this example humid conditions the non frac scenario had a slightly higher kge wi indicating that it can fit isotopic data better followed by cg frac while gon frac performed the worst this is understandable since evaporation fractionation could be neglected in this example as seen from the dual isotope plots fig 5 of stumpp et al 2012 for the braud et al 2009a dataset as indicated in section 3 2 1 the hydraulic conductivities in the fractionation cg frac gon frac meas frac scenarios were a little higher than those in the non frac scenario this is because fractionation decreases the isotope flux by evaporation compared with a no fractionation scenario a higher hydraulic conductivity in the fractionation scenarios promotes upward evaporation and fractionation this increases the isotopic composition of remaining soil water and thus produces a better fit between simulated and observed isotope profiles when evaporation fractionation was not considered the isotopic composition of evaporation remained the same as the initial isotopic composition this resulted in a uniform isotopic composition equal to the initial value distribution of soil water throughout the profile in the non frac scenario fig 9 in fractionation scenarios the peak value of the isotopic composition profile was inversely proportional to the dispersivity value fig 9 and table 3 which is consistent with the conclusions from braud et al 2009b the isotopic composition profiles and the kge wi values differed dramatically reached 1 48 between different fractionation scenarios fig 9 and table 3 this implies that considering evaporation fractionation significantly impacts the isotopic composition profile in this example arid conditions the meas frac scenario had the highest kge wi i e for the water isotopic composition followed by the gon frac and then cg frac while the non frac scenario performed the worst this is understandable since evaporation fractionation could not be neglected and the measured evaporation isotope flux is the most accurate for this example braud et al 2009b 4 2 impacts of evaporation fractionation on practical applications 4 2 1 estimation of drainage and rwu travel times differences in water travel times were not evident among different fractionation scenarios table 4 since the numerator in eq 2 is much larger than the denominator in the peak displacement method as a result water travel times were similar for different fractionation scenarios despite a very different dispersivity however for the particle tracking method based on water flow calculations differences in water travel times were evident among different fractionation scenarios table 2 despite their similar kge values table 1 in addition differences in estimated soil hydraulic parameters may also cause discrepancies in tts of individual precipitation events and the temporal origin of water for rwu figs s8 and 7 8 overall the particle tracking method gave much higher travel times than the peak displacement method table 2 different results by these two methods may be associated with different rainfall events selected for these calculations the peak displacement method calculates the travel times during frequent and heavy precipitation events precipitation events from 2005 2006 while particle tracking assesses the travel times over longer periods zhou et al 2021 notably water travel times in the non frac scenario obtained by the particle tracking method are most consistent with the approximate estimate of 41 weeks provided by previous studies with similar crops and areas stumpp et al 2009 it is worth mentioning that asadollahi et al 2020 pointed out that the sas approach was a good alternative for estimating water travel times when the system was too complicated to be fully described by the hydrus 1d model our study demonstrates that the water flow based particle tracking module in hydrus 1d is another promising way of constraining estimation errors in water travel times especially when there is not enough isotope data to calibrate the lumped or physically based isotope transport models in contrast considering fractionation using either the cg or gonfiantini models will likely lead to larger water travel time estimates than in the non frac scenario table 2 this is because fractionation scenarios result in a larger dispersivity to increase the dispersion of isotopes or lower hydraulic conductivities to decrease convection of isotopes as discussed in section 4 1 4 2 2 estimation of the evaporation flux for evaporation estimation the isotope transport based methods for different fractionation cg frac gon frac and meas frac scenarios can give comparable results to the water flow based methods including laboratory measurements and the hydrus 1d water balance in contrast the non frac scenario can produce similar results only when using the water flow based method hydrus 1d water balance however since the measured evaporation flux was used as the upper boundary condition in this arid conditions example it is not clear whether the similarity between estimated evaporation amounts using the hydrus 1d water balance method in the non frac and fractionation cg frac gon frac meas frac scenarios was due to this boundary condition or because actual soil hydraulic conductivities and water contents were continuously adjusted to actual soil fluxes without ever reaching full saturation however it is clear that evaporation fractionation has a significant impact on the isotope transport and isotopic compositions in arid conditions as shown in fig 9 therefore the direct use of simulated isotopic compositions in the non frac scenario may result in large biases in practical applications in arid conditions as seen from the evaporation estimation results in table 4 4 3 comparison of different climate conditions and implications for future studies the soil saturated hydraulic conductivities k s and the retention curve shape parameter α were the parameters most affected by the consideration of evaporation fractionation for the humid condition dataset table 1 for the arid condition dataset these were the dispersivity λ and the retention curve shape parameter α table 3 this is likely associated with the effects of soil texture on retention curves and soil moisture conditions in different climate zones radcliffe and šimůnek 2018 overall soil water retention and hydraulic conductivity curves fig s12 in different fractionation scenarios were more similar for the braud et al 2009a dataset than the stumpp et al 2012 dataset fig s8 one reason is that the measured evaporation flux was used as the upper bc in the former which constrains the model flexibility another reason is that there was only one soil layer in the braud et al 2009a dataset while there were three soil layers in the stumpp et al 2012 dataset there is likely a compensation effect between the parameters of different layers and thus the parameter values can vary more in the stumpp et al 2012 dataset while evaporation fractionation plays an essential role in parameter estimation in both cases its impact on model performance is relatively small in the example for humid conditions but more significant in the example for arid conditions as discussed in sections 4 1 and 4 2 this is expected since evaporation plays a more important role in the water balance of the arid dataset table 4 than in the humid dataset fig s13 these conclusions also indirectly validate the common assumption that evaporation fractionation may be neglected in some humid regions but not in arid areas sprenger et al 2016a however parameter sensitivities and optimization results reflect complex combined effects of climate soil and vegetation characteristics the isotopic composition of soil water is not only affected by evaporation fractionation but also by the mixing of rainfall with soil water and different flow paths in the soil leading to its variations with depths and time the insufficient knowledge of the spatiotemporal isotope distribution e g in shallow and deep depths or during different stages of evaporation and the lack of such information in the objective function may bias the parameter estimation results for example not including isotopes from different soil depths within the soil profile might lead to an underestimation of evaporation fractionation in general biased estimation of water mixing within the profile and a similar isotopic signal in the discharge in this study we considered either the time series of the isotopic composition of the bottom flux in the stumpp et al 2012 dataset or the final isotopic composition profile in the braud et al 2009a dataset in addition observation data types and spatiotemporal distributions are different for these two datasets and this difference may affect the comparison of parameter estimation results between different climate conditions the gsa was carried out for the non frac scenario for the stumpp et al 2012 dataset and the meas frac scenario for the braud et al 2009a dataset because they were closest to the experimental conditions this implicitly assumes that sensitivity remains the same for different model structures however different model structures may affect gsa and pso results which should be further explored last but not least the impacts of possible transpiration fractionation as observed in multiple studies should also be included in future analyses e g barbeta et al 2019 therefore it is difficult to generalize the results of this study or apply them to other specific conditions 5 summary and conclusions in this study we analyzed parameter estimation results for two datasets collected under humid and arid climate conditions using the isotope transport model in which we either did or did not consider evaporation fractionation the global sensitivity analysis using the morris and sobol methods and the parameter estimation using the particle swarm optimization algorithm highlight the significant impacts of considering evaporation fractionation on parameter estimation and model performance the kge index for isotope data can increase by 0 09 and 1 49 for the humid and arid datasets respectively when selecting suitable fractionation scenarios the impact of different parameter values estimated when considering or not evaporation fractionation propagates into practical applications of isotope transport modeling the isotope transport based method peak displacement gave much lower water travel times than the water flow based method particle tracking for humid conditions considering fractionation using the cg and gonfiantini models will likely lead to larger water travel time estimates and ages for rwu for arid conditions example the isotope transport based method isotope mass balance can provide comparable evaporation estimates for different fractionation cg frac gon frac meas frac scenarios as the water flow based methods hydrus 1d water balance and laboratory measurements in contrast the non frac scenario can produce reasonable evaporation estimation only when using the water flow based method the direct use of simulated isotopic compositions in the no fractionation scenario may result in large biases in practical applications in arid regions where evaporation fractionation is more extensive than in humid areas integrated use of water flow and isotope transport based methods may provide mutual validation and be an important way to avoid this problem this research may shed some light on future laboratory and field experimental designs regarding the practical applications of the isotope transport modeling in different climate zones credit authorship contribution statement tiantian zhou conceptualization methodology software writing original draft jirka šimůnek conceptualization funding acquisition methodology software writing review editing isabelle braud methodology writing review editing paolo nasta conceptualization writing review editing giuseppe brunetti methodology software writing review editing yi liu software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was financially supported by the multistate w4188 program funded by nifa grant no ca r ens 5047 rr we acknowledge paolo benettin from the laboratory of ecohydrology enac iie echo école polytechnique fédérale de lausanne epfl lausanne switzerland for his useful suggestions we also appreciate the editors and reviewers for their constructive comments on this manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128100 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
