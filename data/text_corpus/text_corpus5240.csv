index,text
26200,spatial knowledge of the climatic or environmental variables associated with the most frequent circulation types is essential with regard to developing strategies to address the risk of avalanches floods soil erosion air pollution or other natural hazards in order to derive an environmental regionalization we present an open source r package known as synoptreg which combines the spatialization of environmental variables based on the atmospheric circulation types the synoptreg package contains a set of functions which we will employ 1 to perform a pca based synoptic classification using an atmospheric variable 2 to map the spatial distribution of the selected environmental variable based upon the circulation types 3 to develop a spatial environmental regionalization based on the previous results we illustrate the usefulness of the package for a case study in the alps area highlights we present an open source r package to regionalize environmental gridded data the package allows to compute a weather type classification an environmental variable is mapped based on the circulation types we conducted a spatial regionalization based on the previous environmental maps keywords alps environmental regionalization r package synoptreg synoptic classification 1 introduction the use of a synoptic classification a branch of climatology focusing on the spatial and temporal characterization of large weather systems in combination with environmental spatial variables could help us to understand the role played by the spatial variability of environmental elements such models would enable us to establish fine scale continuous fields as inputs to for instance distributed hydrological models allowing us to investigate the impacts of climate change upon ecological and social systems tveito and ustrnul 2003 yalew et al 2018 several studies have attempted to determine the climatic or environmental impacts of atmospheric circulation for example bissolli and westermeier 2005 performed simple linear regression models of precipitation using a digital elevation model dem as the independent variable to obtain daily mean precipitation maps for each circulation type these were based on the objective synoptic classification conducted by the german weather service fernández montes et al 2013 analyzed the relationship between weather types and days with extreme temperatures during the spring and summer seasons garrido perez et al 2017 determined the impact of high latitude blocks and subtropical ridges on daily pm10 particulate matter 10 μm observations finally duane et al 2016 suggested that associating synoptic patterns with observed fire spread patterns may help to understand the factors causing fires to spread over the landscape one way to summarize environmental spatial grids involves the use of multivariate statistical methods such as the principal component analysis pca or the cluster analysis numerous studies attempt to stratify environmental variables in a given number of regions for instance zscheischler et al 2012 employ a k means method to perform a global climatic classification combining weather data and remote sensing derived data another similar study is presented by carro calvo et al 2017 the main goal of which involves regionalization of summertime tropospheric ozone o3 in europe they applied the k means clustering technique to the near surface o3 average over europe at a 1 1 resolution for the summer season during the 1998 2012 period the results reveal a spatial division of nine regions in which tropospheric o3 displays coherent spatio temporal patterns as for computer software for performing synoptic classifications the most important development in this field involves the software package cost733class for development comparison and evaluation of classifications philipp et al 2016 created by the cost733 action within the framework of the european cooperation in the field of scientific and technical research cost this software developed on unix linux operating systems enables computation of up to 27 basic automatic classification methods describing atmospheric circulation apart from the cost action the authors are only aware of the existence of the r package jcext otero 2018b which enables the circulation types of jenkinson collison to be computed one of its applications can be found in otero et al 2018a however no r package has been found to compute a synoptic classification based upon a pca approach a fact that gives greater relevance to the package presented herein our paper presents the implementation of a new methodology for regionalizing environmental variables through thesynoptreg r package https cran r project org package synoptreg users will be able to obtain a synoptic classification of any atmospheric variable mean sea level pressure geopotential height etc a spatialization of the environmental variable of interest based on the weather types obtained from the synoptic classification and finally a regionalization of this environmental variable this package constitutes a step forward in the regionalization of climatic and environmental variables because it links the synoptic atmospheric scale with the regional environmental one thus providing a categorical regionalization of the environmental variable of interest considering characteristics other than the statistical ones the present study is structured as follows in section 2 the methods of the entire procedure are described in section 3 we provide user details referring to the internal features of the functions and their sequence of use for achieving results finally in section 4 we provide a short summary of the main results in addition we describe some potential improvements of the package as well as projected future developments the code of the figures produced in this article can be found in appendix a 2 methodological basis the methodology applied in this package mainly involves establishing an objective weather type classification which assigns each of the days to a specific circulation type ct by means of multivariate statistical techniques furthermore once the spatialization of the environmental or climatic variable has been conducted a spatial regionalization is executed by means of a cluster analysis therefore the procedure for establishing regionalization is as follows 1 execution of an atmospheric synoptic classification 2 spatialization of the mean daily amounts of gridded environmental or climatic variable based on the various cts 3 synthesis of the different spatial patterns of the environmental variable by means of the pca technique as well as mean filters and resampling of cell sizes 4 clustering of the selected principal components pc to obtain an environmental spatial stratification 2 1 synoptic classification method we applied a pca to a spatial mode s mode matrix to reduce the dimension of the variables in this matrix the grid points are the variables and the days are the observations the s mode pca enables a climate regionalization richman 1986 lopez bustins et al 2015 because it shows the representative degree of each day in each component nonetheless an s mode pca does not provide a direct synoptic classification because a classification of circulation types has not been achieved hence our cases need to be allocated to a specific group and as a result a clustering method must be performed in order to categorize the atmospheric circulation types as for the variables used we standardized the matrix to maintain one single scale when using different variables thus avoiding overrepresentation of any one of them we employed the scree test cattell 1966 to retain the components explaining a significant portion of the total variance these principal components were subsequently rotated by means of a varimax rotation esteban et al 2006 along with the rotated components we made use of the scores to apply the extreme scores method esteban et al 2005 the scores show the degree of representativeness associated with the variation modes of each principal component i e classification of each day to its most representative centroid the extreme scores method uses the scores 2 and 2 thus establishing a positive and negative phase for each principal component the extreme scores establish the number of groups and their centroids for the k means method esteban et al 2006 k means is applied without iterations because the centroids are well established by the method of extreme scores 2 2 regionalization the regionalization process consists of the following four steps 1 establishing the spatialization of the gridded environmental variable based on the weather types 2 applying a pca to n environmental maps in raster format and selecting the ones that present a greater variance this process enabled us to reduce redundant information on the models and to extract only the most significant information from the set of maps 3 other options involve resampling of the previous environmental rasters to a coarse spatial resolution and use of a filter based upon a weight matrix for the neighboring cells of the focal cell this step is important with regard to reducing local effects such as the valley bottom effect in addition applying the spatial filter avoids notable discontinuities in the final regionalization 4 finally the k means algorithm is applied to the rasters obtained in step 3 this final step must be applied iteratively in order to decide the correct number of regions to facilitate this decision a pseudo mae index is proposed 1 1 mae n 1 i 1 n d i c i where di is the mean grid cell resulting from the average of the pc selected and ci is the mean of the respective centroids of the selected pc as mentioned in carro calvo et al 2017 the increase in the explained variance by means of clustering can be explained by the increase in the number of centroids however this increase does not always imply greater variance since the n clusters explain practically the same variance as the n 1 clusters this graphically represented pseudo mae index enables us to visualize which cluster shows no decrease in the error or increase in the variance this measure is proposed as a complement to the decision based upon visual inspection the purpose of the clustering process is to provide a small but sufficient number of clusters that capture the diversity of the environmental variable with no loss of generalization 3 the synoptreg package the use of the package functions provides 1 an objective synoptic classification that determines the most frequent weather types for a given study area 2 a spatialization of the gridded environmental or climatic study variable based on the ct 3 a discrete regionalization of the target variable to this end the package includes 8 functions that must be applied sequentially although some of them can be applied separately 3 1 input data requirements the synoptreg package contains a function for reading input data in netcdf format which is the most widespread format for storing large volumes of climate information containing at least the spatial and temporal dimensions the read nc function enables us to read these files as long as they are continuous and present no internal gaps this function extracts the variable data the latitude and the longitude vector and the date sequence these are stored in a list object which is subsequently used to conduct the rest of the methodological procedure although read nc is used to read the file in netcdf format the tidy cuttime nc function must be used to format the data and transform them into an s mode data frame table 1 in which the variables columns are the grid points lat lon and the observations rows represent the dates of the dataset furthermore we can select the range of years to be used in the synoptic classification year range thus deciding whether we want to work with a concrete climatological season winter spring summer fall using the season parameter importantly the read nc and tidy cuttime nc functions need to be used both for the synoptic classification to be computed by means of the reanalysis data and for the environmental variable to be regionalized the package includes a dataset of mean sea level pressure mslp from the era inerim project dee et al 2011 at a spatial resolution of 2 5 centered on western europe 60 n 30 n 30 w 15 e and used to perform the synoptic classification and another dataset of daily precipitation for the balearic islands spain provided by the spread project serrano notivoli et al 2017 at a spatial resolution of 5 km herein however the era interim grid from the european centre for medium range weather forecasts ecmwf for mslp was focused upon central europe 60 n 30 n 10 w 30 e the spatial resolution of the dataset was 0 75 with a daily temporal resolution 12 h ut between 1979 and 2017 moreover we chose the alps mountain range fig 1 for the climatic study 49 n 43 n 4 e 16 5 e for this purpose we employed the e obs grid of daily precipitation haylock et al 2008 at a spatial resolution of 0 25 this gridded dataset can be downloaded from the european climate assessment dataset website https www ecad eu 3 2 computing the synoptic classification as was pointed out in the methodology section the synoptic classification is computed by means of a pca based approach for this reason prior to the classification process it is advisable to use the function pca decision which extracts a report on the proportion of variance explained by each of the pcs as well as a plot of the scree test catell 1966 fig s1 based upon the scree test information we can decide the number of pcs to be retained which was 6 in the example once the number of components to be retained in the synoptic classification has been decided the synoptclas function can be used this function requires as input data the data frame resulting from the use of the function tidy cuttime nc smode data and the number of components ncomp we can then perform the classification with these two parameters consequently the following results are obtained a data frame containing the mslp data grouped by ct statistics on the number of days assigned to each ct for the whole period we also obtain the frequency of each one of the cts per month and per year all these statistics enable us to perform a trend analysis for different circulation types fig s2 or simply to calculate the frequency of each of the sects per month fig s3 among other possibilities in addition the rotated loadings the scores and the multivariate coordinates of these scores can be consulted the latter ones are used to perform the classification with the k means algorithm in order to obtain the maps of the synoptic classification we will first use the function raster clas which only requires the vector of longitudes and latitudes and the data frame result of the synoptclas application by applying this function we will obtain a rasterstack object that can be easily plotted using any of the existing visualization libraries fig 2 3 3 spatial distribution of precipitation and regionalization to obtain the final regionalization we first mapped the mean daily precipitation over the study area based on each of the previous cts fig 3 to this end we used the function raster ct2env obtaining raster maps of mean daily precipitation mdp for each weather type with the mdp maps different spatial distributions of precipitation linked to the cts can be observed but with the use of theraster pca function we obtained the most frequent spatial patterns of precipitation in the study area this function applies an r mode pca to the precipitation rasterstack although in this demonstration a coarse resolution grid is used the application of a mean filter to the raster cells of the pca result can favour greater continuity of the spatial patterns thus facilitating the final regionalization fig s4 application of a 5 5 cell filter fig s4 reveals the raster pca result as can be seen in table 2 pc1 obtains a higher percentage of explained variance than pc2 pc2 is higher than pc 3 in this sense and so forth as in the case of the scree test the most relevant components expressing the main spatial patterns must be selected this selection can be made through visual inspection of fig s4 and the summary returned by the raster pca function where the percentages of variance explained by each of the pcs are noted table 2 we used the above data to select the first 4 pcs in order to perform the cluster analysis using the regionalization function which requires a rasterstack object and the number of cluster centers we desire i e the number of regions to use the regionalization function performing an iterative process is recommended which involves dividing the study area into k clusters for example fig 4 shows the result of regionalizing the alps area from k 2 to k 10 thus it is possible to visually inspect the regionalization that appears to be the most appropriate however the pseudo mae value returned by the function which measures the intracluster error can be used to take the final decision to comment briefly upon the regionalization results for k 2 the two basic precipitation regions in the alps area are displayed these are based on maximums in the cold half of the year blue west and south and maximums in the warm half of the year red centre and north for k 3 the three basic climates are shown temperate green maritime influence west mediterranean red south and continental blue north for k 5 the spatial complexity of precipitation in a widely contrasted alpine region is shown graphing of the pseudo mae error reveals that adding a region does not always cause a clear decrease in the error and it might therefore not be worth adding a new region if this does not cause a decrease in the pseudo mae value fig 5 shows that in the transition from k 3 to k 4 there is hardly any variation in the error consequently adding another region does not provide too much information and the regionalization k 3 could therefore be retained the same occurs with an increase from k 8 to k 9 and k 8 would therefore be the recommended option in any case the final decision depends upon the research objectives and on the geographical scale of the study area 4 further developments limitations and conclusions the synoptreg package contains a set of functions that provide an automatic synoptic classification and a regionalization of the climatic or environmental variable of interest the functions integrated in this package have been tested in the present research and have provided highly satisfactory results nonetheless the package presents some limitations for instance it is not adapted to working with temporarily discontinuous data or with series containing few observations days because the application of the pca in s mode means that the number of observations must be greater than the number of variables this requires an extensive temporal series of observations daily time scale for example for this reason we are working on a new function that applies the temporal mode t mode matrix which allows a synoptic classification when using short series of data and the t mode unlike the s mode does not require a clustering process furthermore we are also working to introduce a second variable in the synoptclas function i e that the pca can be applied to a data grid of two variables e g mslp and geopotential height at 500 hpa all these improvements will be implemented in future versions of the synoptreg package acknowledgments the present study was funded by the wemotor project cso2014 55799 c2 1 r and the clices project cgl2017 83866 c3 2 r of the spanish ministry of science innovation and universities this research was conducted within the framework of the climatology group of the university of barcelona 2017 sgr 1362 catalonia regional govt and the water research institute of the university of barcelona we also wish to acknowledge the e obs dataset from the eu fp6 project ensembles http ensembles eu metoffice com last accessed 21 december 2018 and the data providers in the eca d project http www ecad eu last accessed 21 december 2018 m l c has been granted a pre doctoral fpu scholarship spanish ministry of education culture and sports appendix a supplementary data the following are the supplementary data to this article data profile data profile supplement synoptreg v2 supplement synoptreg v2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 006 
26200,spatial knowledge of the climatic or environmental variables associated with the most frequent circulation types is essential with regard to developing strategies to address the risk of avalanches floods soil erosion air pollution or other natural hazards in order to derive an environmental regionalization we present an open source r package known as synoptreg which combines the spatialization of environmental variables based on the atmospheric circulation types the synoptreg package contains a set of functions which we will employ 1 to perform a pca based synoptic classification using an atmospheric variable 2 to map the spatial distribution of the selected environmental variable based upon the circulation types 3 to develop a spatial environmental regionalization based on the previous results we illustrate the usefulness of the package for a case study in the alps area highlights we present an open source r package to regionalize environmental gridded data the package allows to compute a weather type classification an environmental variable is mapped based on the circulation types we conducted a spatial regionalization based on the previous environmental maps keywords alps environmental regionalization r package synoptreg synoptic classification 1 introduction the use of a synoptic classification a branch of climatology focusing on the spatial and temporal characterization of large weather systems in combination with environmental spatial variables could help us to understand the role played by the spatial variability of environmental elements such models would enable us to establish fine scale continuous fields as inputs to for instance distributed hydrological models allowing us to investigate the impacts of climate change upon ecological and social systems tveito and ustrnul 2003 yalew et al 2018 several studies have attempted to determine the climatic or environmental impacts of atmospheric circulation for example bissolli and westermeier 2005 performed simple linear regression models of precipitation using a digital elevation model dem as the independent variable to obtain daily mean precipitation maps for each circulation type these were based on the objective synoptic classification conducted by the german weather service fernández montes et al 2013 analyzed the relationship between weather types and days with extreme temperatures during the spring and summer seasons garrido perez et al 2017 determined the impact of high latitude blocks and subtropical ridges on daily pm10 particulate matter 10 μm observations finally duane et al 2016 suggested that associating synoptic patterns with observed fire spread patterns may help to understand the factors causing fires to spread over the landscape one way to summarize environmental spatial grids involves the use of multivariate statistical methods such as the principal component analysis pca or the cluster analysis numerous studies attempt to stratify environmental variables in a given number of regions for instance zscheischler et al 2012 employ a k means method to perform a global climatic classification combining weather data and remote sensing derived data another similar study is presented by carro calvo et al 2017 the main goal of which involves regionalization of summertime tropospheric ozone o3 in europe they applied the k means clustering technique to the near surface o3 average over europe at a 1 1 resolution for the summer season during the 1998 2012 period the results reveal a spatial division of nine regions in which tropospheric o3 displays coherent spatio temporal patterns as for computer software for performing synoptic classifications the most important development in this field involves the software package cost733class for development comparison and evaluation of classifications philipp et al 2016 created by the cost733 action within the framework of the european cooperation in the field of scientific and technical research cost this software developed on unix linux operating systems enables computation of up to 27 basic automatic classification methods describing atmospheric circulation apart from the cost action the authors are only aware of the existence of the r package jcext otero 2018b which enables the circulation types of jenkinson collison to be computed one of its applications can be found in otero et al 2018a however no r package has been found to compute a synoptic classification based upon a pca approach a fact that gives greater relevance to the package presented herein our paper presents the implementation of a new methodology for regionalizing environmental variables through thesynoptreg r package https cran r project org package synoptreg users will be able to obtain a synoptic classification of any atmospheric variable mean sea level pressure geopotential height etc a spatialization of the environmental variable of interest based on the weather types obtained from the synoptic classification and finally a regionalization of this environmental variable this package constitutes a step forward in the regionalization of climatic and environmental variables because it links the synoptic atmospheric scale with the regional environmental one thus providing a categorical regionalization of the environmental variable of interest considering characteristics other than the statistical ones the present study is structured as follows in section 2 the methods of the entire procedure are described in section 3 we provide user details referring to the internal features of the functions and their sequence of use for achieving results finally in section 4 we provide a short summary of the main results in addition we describe some potential improvements of the package as well as projected future developments the code of the figures produced in this article can be found in appendix a 2 methodological basis the methodology applied in this package mainly involves establishing an objective weather type classification which assigns each of the days to a specific circulation type ct by means of multivariate statistical techniques furthermore once the spatialization of the environmental or climatic variable has been conducted a spatial regionalization is executed by means of a cluster analysis therefore the procedure for establishing regionalization is as follows 1 execution of an atmospheric synoptic classification 2 spatialization of the mean daily amounts of gridded environmental or climatic variable based on the various cts 3 synthesis of the different spatial patterns of the environmental variable by means of the pca technique as well as mean filters and resampling of cell sizes 4 clustering of the selected principal components pc to obtain an environmental spatial stratification 2 1 synoptic classification method we applied a pca to a spatial mode s mode matrix to reduce the dimension of the variables in this matrix the grid points are the variables and the days are the observations the s mode pca enables a climate regionalization richman 1986 lopez bustins et al 2015 because it shows the representative degree of each day in each component nonetheless an s mode pca does not provide a direct synoptic classification because a classification of circulation types has not been achieved hence our cases need to be allocated to a specific group and as a result a clustering method must be performed in order to categorize the atmospheric circulation types as for the variables used we standardized the matrix to maintain one single scale when using different variables thus avoiding overrepresentation of any one of them we employed the scree test cattell 1966 to retain the components explaining a significant portion of the total variance these principal components were subsequently rotated by means of a varimax rotation esteban et al 2006 along with the rotated components we made use of the scores to apply the extreme scores method esteban et al 2005 the scores show the degree of representativeness associated with the variation modes of each principal component i e classification of each day to its most representative centroid the extreme scores method uses the scores 2 and 2 thus establishing a positive and negative phase for each principal component the extreme scores establish the number of groups and their centroids for the k means method esteban et al 2006 k means is applied without iterations because the centroids are well established by the method of extreme scores 2 2 regionalization the regionalization process consists of the following four steps 1 establishing the spatialization of the gridded environmental variable based on the weather types 2 applying a pca to n environmental maps in raster format and selecting the ones that present a greater variance this process enabled us to reduce redundant information on the models and to extract only the most significant information from the set of maps 3 other options involve resampling of the previous environmental rasters to a coarse spatial resolution and use of a filter based upon a weight matrix for the neighboring cells of the focal cell this step is important with regard to reducing local effects such as the valley bottom effect in addition applying the spatial filter avoids notable discontinuities in the final regionalization 4 finally the k means algorithm is applied to the rasters obtained in step 3 this final step must be applied iteratively in order to decide the correct number of regions to facilitate this decision a pseudo mae index is proposed 1 1 mae n 1 i 1 n d i c i where di is the mean grid cell resulting from the average of the pc selected and ci is the mean of the respective centroids of the selected pc as mentioned in carro calvo et al 2017 the increase in the explained variance by means of clustering can be explained by the increase in the number of centroids however this increase does not always imply greater variance since the n clusters explain practically the same variance as the n 1 clusters this graphically represented pseudo mae index enables us to visualize which cluster shows no decrease in the error or increase in the variance this measure is proposed as a complement to the decision based upon visual inspection the purpose of the clustering process is to provide a small but sufficient number of clusters that capture the diversity of the environmental variable with no loss of generalization 3 the synoptreg package the use of the package functions provides 1 an objective synoptic classification that determines the most frequent weather types for a given study area 2 a spatialization of the gridded environmental or climatic study variable based on the ct 3 a discrete regionalization of the target variable to this end the package includes 8 functions that must be applied sequentially although some of them can be applied separately 3 1 input data requirements the synoptreg package contains a function for reading input data in netcdf format which is the most widespread format for storing large volumes of climate information containing at least the spatial and temporal dimensions the read nc function enables us to read these files as long as they are continuous and present no internal gaps this function extracts the variable data the latitude and the longitude vector and the date sequence these are stored in a list object which is subsequently used to conduct the rest of the methodological procedure although read nc is used to read the file in netcdf format the tidy cuttime nc function must be used to format the data and transform them into an s mode data frame table 1 in which the variables columns are the grid points lat lon and the observations rows represent the dates of the dataset furthermore we can select the range of years to be used in the synoptic classification year range thus deciding whether we want to work with a concrete climatological season winter spring summer fall using the season parameter importantly the read nc and tidy cuttime nc functions need to be used both for the synoptic classification to be computed by means of the reanalysis data and for the environmental variable to be regionalized the package includes a dataset of mean sea level pressure mslp from the era inerim project dee et al 2011 at a spatial resolution of 2 5 centered on western europe 60 n 30 n 30 w 15 e and used to perform the synoptic classification and another dataset of daily precipitation for the balearic islands spain provided by the spread project serrano notivoli et al 2017 at a spatial resolution of 5 km herein however the era interim grid from the european centre for medium range weather forecasts ecmwf for mslp was focused upon central europe 60 n 30 n 10 w 30 e the spatial resolution of the dataset was 0 75 with a daily temporal resolution 12 h ut between 1979 and 2017 moreover we chose the alps mountain range fig 1 for the climatic study 49 n 43 n 4 e 16 5 e for this purpose we employed the e obs grid of daily precipitation haylock et al 2008 at a spatial resolution of 0 25 this gridded dataset can be downloaded from the european climate assessment dataset website https www ecad eu 3 2 computing the synoptic classification as was pointed out in the methodology section the synoptic classification is computed by means of a pca based approach for this reason prior to the classification process it is advisable to use the function pca decision which extracts a report on the proportion of variance explained by each of the pcs as well as a plot of the scree test catell 1966 fig s1 based upon the scree test information we can decide the number of pcs to be retained which was 6 in the example once the number of components to be retained in the synoptic classification has been decided the synoptclas function can be used this function requires as input data the data frame resulting from the use of the function tidy cuttime nc smode data and the number of components ncomp we can then perform the classification with these two parameters consequently the following results are obtained a data frame containing the mslp data grouped by ct statistics on the number of days assigned to each ct for the whole period we also obtain the frequency of each one of the cts per month and per year all these statistics enable us to perform a trend analysis for different circulation types fig s2 or simply to calculate the frequency of each of the sects per month fig s3 among other possibilities in addition the rotated loadings the scores and the multivariate coordinates of these scores can be consulted the latter ones are used to perform the classification with the k means algorithm in order to obtain the maps of the synoptic classification we will first use the function raster clas which only requires the vector of longitudes and latitudes and the data frame result of the synoptclas application by applying this function we will obtain a rasterstack object that can be easily plotted using any of the existing visualization libraries fig 2 3 3 spatial distribution of precipitation and regionalization to obtain the final regionalization we first mapped the mean daily precipitation over the study area based on each of the previous cts fig 3 to this end we used the function raster ct2env obtaining raster maps of mean daily precipitation mdp for each weather type with the mdp maps different spatial distributions of precipitation linked to the cts can be observed but with the use of theraster pca function we obtained the most frequent spatial patterns of precipitation in the study area this function applies an r mode pca to the precipitation rasterstack although in this demonstration a coarse resolution grid is used the application of a mean filter to the raster cells of the pca result can favour greater continuity of the spatial patterns thus facilitating the final regionalization fig s4 application of a 5 5 cell filter fig s4 reveals the raster pca result as can be seen in table 2 pc1 obtains a higher percentage of explained variance than pc2 pc2 is higher than pc 3 in this sense and so forth as in the case of the scree test the most relevant components expressing the main spatial patterns must be selected this selection can be made through visual inspection of fig s4 and the summary returned by the raster pca function where the percentages of variance explained by each of the pcs are noted table 2 we used the above data to select the first 4 pcs in order to perform the cluster analysis using the regionalization function which requires a rasterstack object and the number of cluster centers we desire i e the number of regions to use the regionalization function performing an iterative process is recommended which involves dividing the study area into k clusters for example fig 4 shows the result of regionalizing the alps area from k 2 to k 10 thus it is possible to visually inspect the regionalization that appears to be the most appropriate however the pseudo mae value returned by the function which measures the intracluster error can be used to take the final decision to comment briefly upon the regionalization results for k 2 the two basic precipitation regions in the alps area are displayed these are based on maximums in the cold half of the year blue west and south and maximums in the warm half of the year red centre and north for k 3 the three basic climates are shown temperate green maritime influence west mediterranean red south and continental blue north for k 5 the spatial complexity of precipitation in a widely contrasted alpine region is shown graphing of the pseudo mae error reveals that adding a region does not always cause a clear decrease in the error and it might therefore not be worth adding a new region if this does not cause a decrease in the pseudo mae value fig 5 shows that in the transition from k 3 to k 4 there is hardly any variation in the error consequently adding another region does not provide too much information and the regionalization k 3 could therefore be retained the same occurs with an increase from k 8 to k 9 and k 8 would therefore be the recommended option in any case the final decision depends upon the research objectives and on the geographical scale of the study area 4 further developments limitations and conclusions the synoptreg package contains a set of functions that provide an automatic synoptic classification and a regionalization of the climatic or environmental variable of interest the functions integrated in this package have been tested in the present research and have provided highly satisfactory results nonetheless the package presents some limitations for instance it is not adapted to working with temporarily discontinuous data or with series containing few observations days because the application of the pca in s mode means that the number of observations must be greater than the number of variables this requires an extensive temporal series of observations daily time scale for example for this reason we are working on a new function that applies the temporal mode t mode matrix which allows a synoptic classification when using short series of data and the t mode unlike the s mode does not require a clustering process furthermore we are also working to introduce a second variable in the synoptclas function i e that the pca can be applied to a data grid of two variables e g mslp and geopotential height at 500 hpa all these improvements will be implemented in future versions of the synoptreg package acknowledgments the present study was funded by the wemotor project cso2014 55799 c2 1 r and the clices project cgl2017 83866 c3 2 r of the spanish ministry of science innovation and universities this research was conducted within the framework of the climatology group of the university of barcelona 2017 sgr 1362 catalonia regional govt and the water research institute of the university of barcelona we also wish to acknowledge the e obs dataset from the eu fp6 project ensembles http ensembles eu metoffice com last accessed 21 december 2018 and the data providers in the eca d project http www ecad eu last accessed 21 december 2018 m l c has been granted a pre doctoral fpu scholarship spanish ministry of education culture and sports appendix a supplementary data the following are the supplementary data to this article data profile data profile supplement synoptreg v2 supplement synoptreg v2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 006 
26201,the deteriorating environmental system and increasing number of time varying datasets have generated a significant demand for new techniques to support online high efficiency climate analysis in response to this demand this paper introduces an innovative multilayer architecture driven by a tile service in a virtual globe environment this architecture is supported by the open source and highly efficient osgearth virtual globe in addition we develop a gsoap based tile service to address the challenges of visualizing and analysing massive data under hardware restrictions conditions the service contains a lossless pyramid tile set that can automatically provide tiles with proper accuracy according to the demands of users under different conditions furthermore we add a hybrid database file system to the service for high availability this service is then applied for estimation of ocean carbon flux and a proof of concept prototype satco2 has been developed to demonstrate the feasibility and performance of the proposed architecture keywords tile service virtual globe high efficiency online analysis climate change software availability name satellite based marine carbon monitoring and analysis system satco2 developer state key laboratory of satellite ocean environment dynamics zhejiang provincial key laboratory of geographic information system contact address duzhenhong zju edu cn year first available 2016 hardware required discrete graphics recommend nvidia program languages c availability http www satco2 com cost free to researchers 1 introduction since the late 19th century the planet s average surface temperature has risen by approximately 0 9 degrees celsius a change driven largely by increased carbon dioxide and other human made emissions into the atmosphere nasa 2018 although the warming of the planet will be gradual the increasing frequency and severity of extreme weather events like intense storms heat waves droughts and floods will be abrupt and acutely felt organization et al 2008 these extreme events are interacting to generate emerging public health threats that endanger the health and well being of hundreds of millions of people myers and patz 2009 thus understanding the basic structure of the climate system and seeking new techniques for diagnostic insight into climate changes has become an urgent research topic for climate scientists the virtual globe technique which is a new data processing and analysis tool that can integrate heterogeneous geospatial data from real time in situ measurements remote sensing rs and geographic information systems giss at the global scale has been utilized to support spatial analysis and decision making in climate change research yu and gong 2012 liang et al 2014 chen and van genderen 2008 liu et al 2015a since the world we are facing is three dimensional the spatial dynamic changes in climatic phenomena cannot be shown if we use traditional two dimensional methods for visual display and simulation as a tool to connect people and big data the virtual globe technique addresses visible information directly and provides a real time ability for interaction simulations as an example for the past few years geo browsers e g google earth microsoft virtual earth nasa world wind and esri arcgis explorer have brought significant benefit to studies in earth system science especially for climate change research gould et al 2008 gong et al 2011 in the meantime massive amounts of data have been accumulated under long term observations as satellite remote sensing has been widely used with its large scale and real time advantages in addition petabyte scale archives have become freely available from multiple u s government agencies including nasa noaa and the u s geological survey woodcock et al 2008 vasco et al 2010 loveland and dwyer 2012 nemani et al 2011 that is people can communicate the data downloaded from the abovementioned agencies and their research findings in an intuitive three dimensional global perspective yu and gong 2012 however the download action is essentially creating a copy from the hard disks of the server to the users and is sure to encounter bottlenecks for example it greatly raises the requirements for both the hardware and the network conditions of the users when the data are truly immense moreover in most cases users may have to download the desired data and acquire specific visualization tools which require specialized expertise and training zhang et al 2016 as a result for the convenience of climate researchers the need to establish a method by taking full advantage of these resources is becoming more pressing in this paper we introduce a tile service driven architecture for high efficiency visual simulation and analysis through the internet this study has two main objectives 1 to realize the rapid display of massive data on a three dimensional virtual globe under limited network bandwidth and 2 to provide efficient statistical analysis for high concurrency requests under restricted hardware conditions the remainder of this paper is structured as follows section 2 provides an overview of the works related to this paper section 3 introduces the architecture design and section 4 describes the methodology in detail section 5 evaluates the feasibility and efficiency of the architecture using a proof of concept prototype and section 6 concludes the paper and discusses future research directions 2 related works in recent years data visualization based on the virtual globe has played an increasingly important role in representing and analyzing scientific data standart et al 2011 as an illustration li et al 2011 adopted a systematic framework for visualizing dynamic geoscience phenomena in virtual globe environments liu et al 2015b proposed a systematic meteorological data visualization framework in world wind to visualize and analyse meteorological data li and wang 2017 introduced a new web based platform for visualizing multidimensional time varying climate data on a virtual globe although these approaches work well for rendering large scale geoscience phenomena challenges still exist that require further research as an example the cases above are visualization systems that lack underlying support for statistical analysis applications moreover as remote sensing data are strong in real time and have a complex structure the increasing sizes of datasets often exceed the computational capabilities of local processors thus data computation problems need to be considered emphatically in another dimension however google released earth engine for planetary scale geospatial analysis the images ingested into earth engine are pre processed to facilitate fast and efficient data access for users additionally to enable fast visualization during algorithm development a pyramid of reduced resolution tiles was created for each image and stored in the tile database gorelick et al 2017 the image pyramid method was first introduced by adelson et al 1984 to improve the speed of real time display and zoom of original images an image pyramid is a collection of images all arising from a single original image that uses a quadtree structure to represent an image in which successively deeper levels represent successively finer subdivisions of the image area open c v 2013 hunter and steiglitz 1979 similar to earth engine the image pyramid concept was also applied to the visualization of large mars powell et al 2010 and lunar images chang et al 2011 the main idea was to break images into workable sized tiles process each tile in parallel and finally merge the processed tiles to produce the requested result for visualization soille et al 2018 specifically rather than processing the whole dataset the process only calls the parts of images that users desire to participate in the computation greatly enhancing the calculation efficiency although earth engine has enormous computing power and can visualize very large datasets the display is two dimensional and not vivid enough to effectively represent the dynamic change of three dimensional or four dimensional phenomena cheng et al 2019 to address these issues we propose a virtual globe based architecture driven by a tile service for rapid transmission high efficiency geospatial analysis and vivid visualization of vast amounts of data through the internet the proposed architecture allows investigators to explore digital information intuitively in a true three dimensional environment the next section describes the architecture design and key techniques 3 architecture design virtual globes are highly capable of supporting image vector and terrain data but have a limited capability for the online visualization and analysis of massive data liang et al 2014 as shown in fig 1 an innovative multilayer architecture is proposed to visualize vast amounts of data for analysis through the internet working from the bottom up the architecture includes a tile service layer a 3d engine layer and a data engine layer the tile service layer the core of the architecture is the tile data source interface that is composed of a tile set a hybrid database file system and a web service this tile set is organized by an image pyramid structure in a lossless way the tiles are then compressed and stored in hadoops distributed file systems hdfs fortner 1998 during data processing the key statistics and the metadata are inserted into a postgresql database for faster future retrieval and calculation additionally the database is compatible with postgis for heterogeneous data applications such as in situ data the above components make up a hybrid database file system a detailed description of this method can be found in section 4 1 above this layer is the 3d engine layer which provides a platform for high performance three dimensional visualization the 3d engine layer is implemented on top of osgearth which is an open source 3d gis system that enables access to local and internet based raster and vector data sources liang et al 2015 as shown in fig 2 the framework of osgearth contains a core module globe engine which implements the data loading by a paged level of detail lod quadtree structure pagedlod is derived from lod and adds the ability to defer the loading of child nodes until they are actually required for rapid response to the activation of the globe engine the manipulator is composed of several handlers as soon as the globe engine is activated it will call the render engine to modify the scene graph by cull or draw traversals notably the renderer engine is able to link and execute external gpu code such as compute unified device architecture cuda kernels and custom shaders extending the osgearth virtual globe we integrate a plugin for receiving and processing compressed tiles the plugin is available through curl and gdal geospatial data abstraction library as an open source file transmission tool curl follows url syntax and works under the command line mode the client sends data requests to the server and adopts curl to decompress the data and then shows the data in real time on the osgearth virtual globe after the dataset is read by gdal in addition the cross platform qt provides a set of ui elements for classic desktop style user interfaces these libraries together with a gpu accelerated library for image parallel computing form the data engine layer all of these layers work together to provide online large volume data visualization and analysis applications in climate change research section 4 describes these mechanisms 4 methodology as the multi layers of images and geo referenced visualization over an entire globe require a great amount of well processed data which would be very difficult to create and manage in a stand alone system wang et al 2013 we develop a client server like tile service for massive data analysis and visualization under online hardware restrictions and high concurrency request conditions 4 1 tile service for high efficiency analysis 4 1 1 data preprocessing and organization workflow to realize online high efficiency visualization and analysis we adopt a pre processing method for raw data to build a tile set based on the image pyramid structure for each image according to its spatial extent and spatial reference then the client requests corresponding level tiles in accordance with the distance from the viewpoint for display in the case of a statistical calculation the system calls the highest level tiles lossless level tiles to perform the computation this method focuses on resolving three problems 1 tile indexing problem to reduce the tile retrieval time the tiles need to be organized in an efficient and generic manner 2 tile accuracy problem ensure that the highest level tiles are a full backup for the original image and that the backup procedure should be reversible 3 data redundancy problem in situations of limited disk space and constantly updating data the tile should be set to minimize the data size to reduce the pressure on the server s hard drive and to facilitate rapid transmission through the internet we designed the following solution for accelerating the calculating and loading speeds of images without any loss in data accuracy to ensure real time internet based analysis and visualization first the tile indexing problem is inspired and addressed by the data loading method of osgearth as shown in fig 3 based on a pyramid model osgearth selects tiles to perform dynamic loading of large datasets in other words by recording the spatial reference and spatial extent information of the original image osgearth adopts a real time image decomposition process by a tilekey technology that is the index of each tile and calculated by the distance from the viewpoint to conduct data management tilekey is in a structure of level x y at level 0 the whole globe is divided into two hemisphere tiles and each tile is managed by tilekey 0 0 0 and 0 1 0 then as the distance from the viewpoint becomes closer osgearth continues to index the tiles with a tilekey from the lower levels to the higher levels according to a quadtree data structure for every two adjacent levels each tile in the upper level is equally divided into four lower level tiles as the levels go deeper the tiles decrease until the spatial resolution is finer than that of the original image with the refinement of the tiles the range of each tile decreases gradually such that when doing data loading the corresponding tile will be loaded only if there are valid data from the original image range this data loading method can improve the rendering efficiency by avoiding unnecessary data loading but data processing also creates a certain efficiency loss thus the original image are preprocessed to tiles to avoid such efficiency loss similarly the tiles are organized by the rule of tilekey which means that the retrieving path of each tile file can be expressed by the folder level subfolder x and tile file name y we also regard the image file name as a unique identifier to reduce the tile retrieval time meanwhile in the process of data preprocessing the maximum pyramid level projection coordinates row number column number and spatial extent of the original image are recorded as metadata for further usage according to the built in tilekey the osgearth virtual globe can therefore perform direct rendering by resolving the corresponding tile paths to conduct real time requests which can be expressed as 1 f level x y http serverip address filename level x y second there is a tile accuracy problem to retain the pixel value of the original image for any original image pixel m n with a value α we must find one pixel p q among the tiles that has the same value α and such mapping should be a one to one correspondence since osgearth continues to decompose tiles from up to down until the spatial resolution of the tiles is finer than that of the original image the pixel number of the maximum level tiles is larger than that of the original image we set the maximum level that the original image can achieve as max and regard all of the pixels among the tiles at level max as set o all the pixels of the maximum level tiles in the intersection of the same spatial extent of the original image are contained in set m which is a subset of set o with a pixel value set n similarly all of the pixels of the original image are set p with a pixel value set q since set m should be larger than set p we define a pixel set r with a pixel value set s that is a subset of set m and the mapping between set p and set r is a one to one correspondence fig 4 shows the relationship description of all these sets thus the main task would be the construction of a one to one correspondence mapping from set p to set r once this mapping relationship is generated it is further processed to obtain the rest of the pixel values in set o r stand for set o minus set r the method we adopt here is the nearest neighbour search method because it can retain all pixel values of the original image and is more efficient without losing accuracy extending the nearest neighbour search method we integrate some modifications to further enhance the analysis efficiency the details of this method are as follows we first illustrate the mapping of points from set p to set r fig 5 a to maintain a one to one correspondence in the mapping between set p and set r we define the pixel in set r as having a coordinate a m a x i j with a pixel value α m a x i j similarly a pixel of set p can be denoted as b l m with a pixel value β l m the mapping between these two sets can be represented as 2 f b l m a m a x i j where i θl j γm and χ indicates the round of χ the two parameters γ and θ are the ratio of the row column number from set m to set p with this mapping function we can find for each pixel in set p a unique pixel in set r and we define this pixel value as α m a x i j β l m however there are still some pixels in set m that have not been valued that we call set m r similarly we construct a mapping from set m r to set p 3 g a m a x i j b l m where l i θ m j γ we add one more digit for all of the elements from set m r to facilitate the restoration of the original image from the tiles e g we assign 0 34321 to the pixel from set m r when the corresponding original pixel value is 0 3432 furthermore by assigning nodata to set o m we then obtain a 1 1 tile backup for the original image quickly and easily next we adopt the quadtree data structure to obtain a pyramid tile set fig 5 b finally we address the problem of data redundancy as the tile set is already a 1 1 backup of the original image we only need to save this tile set without the original image considering the data volume as well as its rapid growth we designed the following compression strategy to reduce the consumption of disk space fig 5 c because some parts of the original image are useless for our research e g the parts blocked by overcast conditions we set the values of these parts as nodata all of the nodata values are rendered transparent on the virtual globe apart from nodata there are pixels with a float format that have poor predictability furthermore although the compression strategy can reduce the disk footprint and improve the online file transmission speed it can also result in decompression problems in this situation we adopt the deflate deutsch 1996 encoder as compression encoding and the curl module as decompression engine for data requests deflate is a popular dictionary based compression method that was originally used in the well known zip and gzip software and has since been adopted by many applications the most important is the http protocol david 2004 it operates by constructing a dictionary to register a string that appears repeatedly in the original data flow as a shorter string meanwhile the curl module supports gzip and inflates content encoding and performs automatic decompression as the tile set evolves the requested tiles are transmitted from the server to the client upon decompression the highest level tiles go through a data restoration phase to reconstruct the original image part next either the lower level tiles or the restored image part are pushed to the gpu for rendering 4 1 2 hybrid database file system for heterogeneous data organization on the server side a large volume tile set with its metadata are constructed in addition in situ data are also needed in climate change research all of these heterogeneous data should be unified into a data platform for organization purposes to shift the burden from the client to the server to accomplish this a hybrid database file system is adopted for the unified organization of multi source data and fig 6 shows the schema design first we adopt the hdfs to manage the tile set the hdfs cluster consists of a single namenode a master server that manages the file system namespace and regulates access to files by clients in addition there are a number of datanodes which connect to the namenode and respond to requests from the namenode for filesystem operations borthakur 2008 hadoop due to its scalability in storage and computing power is a suitable platform for increasing volumes of remote sensing data lee and lee 2013 however the hdfs has some deficiencies in the management of multisource data for example the smallest unit of vector data is a record that is specific time and space parameter information and multiple records constitute a vector data this structured way of organizing data can be disastrous for hdfs to remedy this in our hybrid database file system the in situ data and the metadata of original images are managed by postgresql while the spatial objects are stored in postgis the postgresql postgis strategy not only reduces the retrieval pressure of the data node but also improves the query efficiency thus by taking advantage of the spatial information service of postgresql postgis the proposed hybrid database file system solves the problem of heterogeneous data organization and provides efficient data storage and access services 4 1 3 gsoap based web service for the purpose of simplifying client to server interactions we construct a web service based on gsoap tools as the heterogeneous data have been stored in the hybrid database file system we need only to interact with the virtual globe send the request to the server through the soap protocol and wait for the response after it finishes all of the computations based on the lossless tile set soap foo and lee 2002 is a lightweight protocol for exchanging structured information in a decentralized and distributed environment the majority of c web service toolkit libraries provide a set of api functions to handle specific soap data models however users have to change their program structures to accommodate the relevant libraries in contrast gsoap provides a kind of transparent soap api function by compiler technology and the detailed contents of soap that are not related to the development are hidden to users thus we need only to define a series of relevant apis both in the client and in the server thereby organizing the relevant parameters into xml messages for multi request cases such as time series queries we combine a plurality of xml messages into one xml message based on the xml structural features upon receiving the message the server parses the message and returns the result to the client similarly the server also combines multiple results into one taking temporal average along sections for example we organize the xml keys by region of interest roi file of interest foi and maxlevel roi contains the latitude and longitude information of each sections foi comprises the dataset type and start end date of the composition period of satellite data information and maxlevel is the maximum pyramid level reached by the original image all of this information can be easily accessed from the hybrid database as an example table 1 is an xml encoding request of temporal average along sections query 4 2 tile service based ocean carbon flux estimation for analysis purposes with the proposed tile service we can perform online high efficiency ocean carbon flux estimation by large sized remote sensing images in this study we adopt a commonly used method to calculate the air sea co2 flux for ocean carbon flux estimation which is based on the multiplication of the co2 partial pressure difference between the surface seawater and the atmosphere and the co2 gas transfer velocity the equation is as follows 4 flux δ p co 2 e wcp acp k h co 2 ρ c 2 k 24 10 2 where wcp is the partial pressure of co2 in seawater acp is the partial pressure of co2 in atmosphere k h co 2 is the dissolution efficient of co2 ρ is the sea water density c 2 is the wind speed coefficient and k is the gas transfer velocity all of these unknown variables can be calculated from sea surface temperature sst sea surface salinity sss and sea surface wind speed ssw the details are attached in appendix a these five remote sensing image data wcp acp sst ssw and sss have been pre processed and constantly updated in the hybrid database file system for high efficiency analysis through the tile service as the original image has been broken into lightweight workable tiles the entire calculation process is applied to each tile in parallel fig 7 and the calculation results can be loaded onto the virtual globe directly tile by tile without any merging similarly the calculated flux tile data is stored in the hybrid database file system and can be used for external carbon budget estimation through the tile service as mentioned above users interact with the virtual globe send the request to the server and wait for a response moreover the estimation process only calls the parts of image that users desire to participate in the computation greatly enhancing the calculation efficiency 4 3 gpu based volume rendering of remote tile sets for visualization purposes based on the tile service a direct volume rendering method du et al 2015 is adopted to realize the spatiotemporal visualization of remote time varying air sea co2 flux tile sets volume rendering is a technique for rendering images of volumes containing a mixture of materials and can represent both the interiors and the boundaries between different regions drebin et al 1988 laur and hanrahan 1991 this technology has been widely used for the simulation of climate phenomena and the visualization of massive multidimensional datasets arthur et al 2010 darles et al 2011 song et al 2006 furthermore with hardware advances graphics processing units gpus have been widely used to accelerate volumetric rendering for interactive applications monte et al 2013 specifically in this section volume data reconstruction and updating are realized by cuda parallel computing with the tile set on the server side and a half angle slice rendering technique green 2008 is adopted for the dynamic visualization of climate data with a shadow effect cuda is a parallel computing platform and programming model developed by nvidia for general computing on graphical processing units gpus nvidia 2010 with cuda we are able to dramatically speed up the half angle slice computing by harnessing the power of gpus the entire rendering process can be summarized as follows 1 generate or update the location and colour of the flux particles 2 send visualization results over the network from servers 3 calculate the half angle array and order the particles along the array 4 for each tile select a set of particles for projection and render them from the viewpoint of both the viewer and the light 5 compose tiles to form the final image and repeat the rendering process fig 8 shows the effect of this half angle volume rendering approach for air sea co2 flux dynamic simulation 5 demonstration and evaluation 5 1 system demonstration integrating the architecture we proposed above the satellite based marine carbon monitoring and analysis system satco2 is developed for ocean carbon flux estimations and its database provides relevant remote sensing data by providing information on the influence of climatic variability and change on carbon stocks and fluxes kang et al 2014 fig 9 shows some screenshots of the satco2 interface fig 9 a shows a view of monthly averaged sea surface wind data for may 2016 where blue represents a low wind speed red represents a high wind speed and the black vectors represent wind directions as shown in fig 9 b satco2 adopts the method mentioned in section 4 2 to calculate the air sea co2 flux for ocean carbon flux estimation fig 9 c shows the carbon budget calculation result of the east china sea during april 2000 the air sea carbon budget during april 2000 is 12 62 tc vr 5 2 experiment evaluation the data we use in this section for the experiments are ten day average chlorophyll α concentration data from 1 january 2010 to 31 december 2015 in the south china sea area 98 127 e 0 25 n with a horizontal resolution of 2 km section 5 2 1 compares the loading times of lossless tiles and the original image under different network bandwidths we then compare the calculation times of time series data in different time spans under conditions of whether we adopt a lossless tile set in section 5 2 2 and section 5 2 3 shows the rendering performance evaluation by the spatiotemporal visualization process 5 2 1 enhanced efficiency of the data loading the image animation module of satco2 i e query and select the time series data from the database for animation after which the selected data will be loaded to the virtual globe as layers is applied to compare the loading times between the lossless tiles and the original image to eliminate the uncertainty caused by data compression in our experiments the original image is pre processed with the same compression method mentioned in section 4 1 we also repeat the experiment five times the cache is cleared after each experiment and choose the average result to avoid uncertainty factors such as bandwidth fluctuations as much as possible apart from this uncertainty because of the inconsistent amount of tiles among different levels in the pyramid structure only tiles from the maximum level are loaded to ensure that the detail of lossless tiles is the same as the original image the experiment consists of five different bandwidths 4 mbps 10 mbps 20 mbps 50 mbps and 100 mbps and ten variable data counts the loading time costs and comparison results are shown in fig 10 fig 10 a shows a comparison of the loading times among the original compressed images in different network bandwidths while fig 10 b compares the lossless compressed tiles with the same parameter used in fig 10 a fig 10 c shows the ratio of loading times between the original image in fig 10 a and the lossless compressed tiles in fig 10 b that are under the same bandwidth and data count as we can see from fig 10 a the relation between the loading time and the number of loading images is almost linear at a lower bandwidth e g 10 mbps and 4 mbps which means that the network bandwidth is the dominant factor in fig 10 b almost all of the loading times fluctuate between 4 and 5 s whenever the bandwidth changes we can conclude that hardware conditions such as fluctuations in the memory could dominate in this case it can be determined from fig 10 c that the advantage of the tiling strategy in data loading is quite obvious at low bandwidths compared with the time cost of the original image however this gap is narrowed as the bandwidth increases and the loading time is almost the same when the bandwidth is broad enough e g 100 mbps in this experiment we only simulate in the way of point to point transmission if we are facing real world applications insomuch that the upload bandwidths of servers and the download bandwidths of clients are both limited it will challenge the transmission of original images substantially in contrast the tile strategy could behave better at speeding loading times up for the transmission of massive data from the server to the client under a limited network bandwidth this phenomenon is mainly due to the characteristics of the architecture it is able to do real time operations and realize the block by block loading of pyramid tiles on demand while the default method needs to wait for the entire original dataset to finish transmission 5 2 2 efficiency and accuracy promotion in geospatial analysis in traditional euclidean geometry both lines and polygons are composed of infinite points but remote sensing images are made up of only finite pixels because of the concept of resolution based on the point level the following experiment is carried out to compare the calculation efficiency and accuracy between the lossless tiles and the original image moreover as multiple requests are combined into an entire xml message when performing a time series query the independent variable is the data source this experiment is performed on the server side to eliminate any consideration of network conditions and to better demonstrate the results all data are prepared and hosted in the server s file system by generating a random coordinate set in the original image we select time series of different time spans and conduct a point time series query then the query result is saved to compare the accuracy the time cost comparison is shown in fig 11 as we can see from fig 11 the lossless tiling strategy can obviously improve the efficiency of the statistical analysis and this enhancement is more remarkable as the analysis becomes more complex the loss due to mapping from the original coordinates to tilekey is negligible as the time complexity of the nearest neighbour search algorithm is o n additionally calling the tile of an image can significantly reduce the memory consumption and disk read time thus we pay more attention to the accuracy problem theoretically the result of these two methods should be the same as the tile set is a full backup of the original data and we can prove this to be true in our experiment by introducing the average error of the result as 5 rae a i b i b i where b i is the original value and a i stands for the tile value as shown in table 2 the rae from january 2010 to december 2010 is 0 based on these results we can conclude that the tiling strategy can be much more efficient at statistical analysis than the original image with no loss of accuracy 5 2 3 rendering performance evaluation in this section the experiment is performed on a pc with an nvidia geforce gtx 660 2 gb graphics card and a 10 mbps network fig 12 shows the fps with a spatiotemporal visualization of the different numbers of carbon flux tile data in a local area network the minimum maximum baseline is the rendering efficiency of du s experiment 2015 in a standalone pc with the same graphics card the result shows that the average fps of the tile volume rendering exceeds the baseline this is because the tiles can be rendered as they reach the client rather than having to wait for the original image to be read from the disk into memory before the data reconstruction and update processing can begin thus we can conclude that tile service technology improves online rendering performance dramatically 6 conclusions and discussion the objective of this paper is to propose a systematic architecture for online high efficiency visualization and geospatial analysis in climate change research according to the special features of climate change research e g large volumes of data and heavy computing burden a tile service driven architecture is implemented in this architecture the original grid dataset is organized into a compressed pyramid tile set so that both the access efficiency and the loading speed for voluminous datasets are improved exploiting the tile set and organization of these data models a gsoap based web service and a hybrid database file system are configured to maintain a high efficiency calculation and analysis within the constraints of the hardware resources we also introduce the open source virtual globe osgearth that can generate spatiotemporal dynamics quickly and has high efficiency as well as greater realism in this way the proposed architecture can not only provide an intuitive interaction friendly data information user interface on the virtual globe but also greatly improve the efficiency of data transmission three dimensional visualization and geospatial analysis without a loss in accuracy which has been proven in our experiments above in addition we combine this architecture to construct a satco2 system by taking full advantage of the architecture satco2 helps researchers to study the ocean carbon cycle and climate change in a virtual globe environment through the internet even when the volume of data is truly enormous compared to earth engine satco2 is based on an effective open source virtual globe and supports high quality volume rendering which is suitable for enhancing visualizations moreover since satco2 focuses on carbon cycle research it has a more professional dataset than earth engine however earth engine is web based and is more convenient for researchers to visualize very large datasets within browsers thus we consider porting the proposed architecture to a web platform for further research although the results presented in this paper demonstrate the successful application of the architecture we proposed this architecture also has some shortcomings that need to be improved first because the original remote sensing image is organized into a pyramid tile set and then stored in hdfs the tile is broken into smaller sized tiles once again and finally stored in different data nodes which causes a secondary index when performing data loading undoubtedly reducing efficiency in future research we consider combining these two methods and directly processing the data to lossless tiles during the storing procedure second the proposed lossless tile set strategy is mainly effective for the osgearth virtual globe and has certain limitations this strategy will be transferred to other virtual globe platforms for further evaluation acknowledgements the remote sensing datasets for the long term marine environment and carbon cycle were provided by the second institute of oceanography sio state oceanic administration of china soa appendix a parameter equations of air sea co2 flux calculation 1 equation of the dissolution efficiency of co2 mol kg atm millero 1995 a1 ln k h co 2 60 2409 93 4517 100 t 23 3585 ln t 100 sss 0 023517 0 023656 t 100 0 0047036 t 100 2 a2 t sst c 273 15 2 sea water density can be calculated by the function of surface water temperature and salinity millero and poisson 1981 a3 ρ ρ w a sss b sss 3 2 c sss 2 10 3 where a4 a 0 824493 4 0899 10 3 sst 7 6438 10 5 sst 2 8 2467 10 7 sst 3 5 3875 10 9 sst 4 a5 b 5 72466 10 3 1 0227 10 4 sst 1 6546 10 6 sst 2 a6 c 4 8314 10 4 a7 ρ w 999 842594 6 793952 10 2 sst 9 09529 10 3 sst 2 1 001685 10 4 sst 3 1 120083 10 6 sst 4 6 536332 10 9 sst 5 3 to calculate the monthly average flux it is often necessary to consider the influence of high frequency wind speed change e g daily on the monthly average wind speed using a coefficient of c2 wanninkhof et al 2002 the c2 coefficient is not needed when calculating daily flux a8 c 2 u j 2 m e a n u m e a n 2 u j is high frequency satellite derived wind speed e g daily and u m e a n is satellite derived monthly average wind speed both with units of m s 4 based on the relationship between the gas transfer velocity k and the wind speed at 10 m above sea level u 10 the commonly used equations for calculating k are shown in the table below table a1 commonly used equations for calculating gas transfer velocity table a1 equation 1 k 660 0 31 u 10 2 instantaneous wind speed k 660 0 39 u 10 2 long term average wind speed wanninkhof 1992 2 k 660 0 27 u 10 2 sweeney et al 2007 3 k 600 0 266 u 10 2 ho et al 2006 4 k 660 0 0283 u 10 3 wanninkhof and mcgillis 1999 k 660 and k 600 indicate the k with schmidt numbers sc of 660 and 600 respectively a9 k k 600 s c 600 0 5 a10 k k 660 s c 660 0 5 when the sea surface temperature ranges from 0 to 30 c the sc can be calculated with the following equation a11 sc 2073 1 125 62 sst 3 6276 sst 2 0 043219 sst 3 for the calculation of the satellite derived air sea co2 flux in the open ocean the k u 10 equations of 1 long term wind speed and 2 in the above table are commonly used appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 005 funding this research was supported by the national key research and development program of china 2018yfb0505000 2016yfc1400903 the national natural science foundation of china 41871287 41671391 and the public science and technology research funds projects for ocean research 201505003 
26201,the deteriorating environmental system and increasing number of time varying datasets have generated a significant demand for new techniques to support online high efficiency climate analysis in response to this demand this paper introduces an innovative multilayer architecture driven by a tile service in a virtual globe environment this architecture is supported by the open source and highly efficient osgearth virtual globe in addition we develop a gsoap based tile service to address the challenges of visualizing and analysing massive data under hardware restrictions conditions the service contains a lossless pyramid tile set that can automatically provide tiles with proper accuracy according to the demands of users under different conditions furthermore we add a hybrid database file system to the service for high availability this service is then applied for estimation of ocean carbon flux and a proof of concept prototype satco2 has been developed to demonstrate the feasibility and performance of the proposed architecture keywords tile service virtual globe high efficiency online analysis climate change software availability name satellite based marine carbon monitoring and analysis system satco2 developer state key laboratory of satellite ocean environment dynamics zhejiang provincial key laboratory of geographic information system contact address duzhenhong zju edu cn year first available 2016 hardware required discrete graphics recommend nvidia program languages c availability http www satco2 com cost free to researchers 1 introduction since the late 19th century the planet s average surface temperature has risen by approximately 0 9 degrees celsius a change driven largely by increased carbon dioxide and other human made emissions into the atmosphere nasa 2018 although the warming of the planet will be gradual the increasing frequency and severity of extreme weather events like intense storms heat waves droughts and floods will be abrupt and acutely felt organization et al 2008 these extreme events are interacting to generate emerging public health threats that endanger the health and well being of hundreds of millions of people myers and patz 2009 thus understanding the basic structure of the climate system and seeking new techniques for diagnostic insight into climate changes has become an urgent research topic for climate scientists the virtual globe technique which is a new data processing and analysis tool that can integrate heterogeneous geospatial data from real time in situ measurements remote sensing rs and geographic information systems giss at the global scale has been utilized to support spatial analysis and decision making in climate change research yu and gong 2012 liang et al 2014 chen and van genderen 2008 liu et al 2015a since the world we are facing is three dimensional the spatial dynamic changes in climatic phenomena cannot be shown if we use traditional two dimensional methods for visual display and simulation as a tool to connect people and big data the virtual globe technique addresses visible information directly and provides a real time ability for interaction simulations as an example for the past few years geo browsers e g google earth microsoft virtual earth nasa world wind and esri arcgis explorer have brought significant benefit to studies in earth system science especially for climate change research gould et al 2008 gong et al 2011 in the meantime massive amounts of data have been accumulated under long term observations as satellite remote sensing has been widely used with its large scale and real time advantages in addition petabyte scale archives have become freely available from multiple u s government agencies including nasa noaa and the u s geological survey woodcock et al 2008 vasco et al 2010 loveland and dwyer 2012 nemani et al 2011 that is people can communicate the data downloaded from the abovementioned agencies and their research findings in an intuitive three dimensional global perspective yu and gong 2012 however the download action is essentially creating a copy from the hard disks of the server to the users and is sure to encounter bottlenecks for example it greatly raises the requirements for both the hardware and the network conditions of the users when the data are truly immense moreover in most cases users may have to download the desired data and acquire specific visualization tools which require specialized expertise and training zhang et al 2016 as a result for the convenience of climate researchers the need to establish a method by taking full advantage of these resources is becoming more pressing in this paper we introduce a tile service driven architecture for high efficiency visual simulation and analysis through the internet this study has two main objectives 1 to realize the rapid display of massive data on a three dimensional virtual globe under limited network bandwidth and 2 to provide efficient statistical analysis for high concurrency requests under restricted hardware conditions the remainder of this paper is structured as follows section 2 provides an overview of the works related to this paper section 3 introduces the architecture design and section 4 describes the methodology in detail section 5 evaluates the feasibility and efficiency of the architecture using a proof of concept prototype and section 6 concludes the paper and discusses future research directions 2 related works in recent years data visualization based on the virtual globe has played an increasingly important role in representing and analyzing scientific data standart et al 2011 as an illustration li et al 2011 adopted a systematic framework for visualizing dynamic geoscience phenomena in virtual globe environments liu et al 2015b proposed a systematic meteorological data visualization framework in world wind to visualize and analyse meteorological data li and wang 2017 introduced a new web based platform for visualizing multidimensional time varying climate data on a virtual globe although these approaches work well for rendering large scale geoscience phenomena challenges still exist that require further research as an example the cases above are visualization systems that lack underlying support for statistical analysis applications moreover as remote sensing data are strong in real time and have a complex structure the increasing sizes of datasets often exceed the computational capabilities of local processors thus data computation problems need to be considered emphatically in another dimension however google released earth engine for planetary scale geospatial analysis the images ingested into earth engine are pre processed to facilitate fast and efficient data access for users additionally to enable fast visualization during algorithm development a pyramid of reduced resolution tiles was created for each image and stored in the tile database gorelick et al 2017 the image pyramid method was first introduced by adelson et al 1984 to improve the speed of real time display and zoom of original images an image pyramid is a collection of images all arising from a single original image that uses a quadtree structure to represent an image in which successively deeper levels represent successively finer subdivisions of the image area open c v 2013 hunter and steiglitz 1979 similar to earth engine the image pyramid concept was also applied to the visualization of large mars powell et al 2010 and lunar images chang et al 2011 the main idea was to break images into workable sized tiles process each tile in parallel and finally merge the processed tiles to produce the requested result for visualization soille et al 2018 specifically rather than processing the whole dataset the process only calls the parts of images that users desire to participate in the computation greatly enhancing the calculation efficiency although earth engine has enormous computing power and can visualize very large datasets the display is two dimensional and not vivid enough to effectively represent the dynamic change of three dimensional or four dimensional phenomena cheng et al 2019 to address these issues we propose a virtual globe based architecture driven by a tile service for rapid transmission high efficiency geospatial analysis and vivid visualization of vast amounts of data through the internet the proposed architecture allows investigators to explore digital information intuitively in a true three dimensional environment the next section describes the architecture design and key techniques 3 architecture design virtual globes are highly capable of supporting image vector and terrain data but have a limited capability for the online visualization and analysis of massive data liang et al 2014 as shown in fig 1 an innovative multilayer architecture is proposed to visualize vast amounts of data for analysis through the internet working from the bottom up the architecture includes a tile service layer a 3d engine layer and a data engine layer the tile service layer the core of the architecture is the tile data source interface that is composed of a tile set a hybrid database file system and a web service this tile set is organized by an image pyramid structure in a lossless way the tiles are then compressed and stored in hadoops distributed file systems hdfs fortner 1998 during data processing the key statistics and the metadata are inserted into a postgresql database for faster future retrieval and calculation additionally the database is compatible with postgis for heterogeneous data applications such as in situ data the above components make up a hybrid database file system a detailed description of this method can be found in section 4 1 above this layer is the 3d engine layer which provides a platform for high performance three dimensional visualization the 3d engine layer is implemented on top of osgearth which is an open source 3d gis system that enables access to local and internet based raster and vector data sources liang et al 2015 as shown in fig 2 the framework of osgearth contains a core module globe engine which implements the data loading by a paged level of detail lod quadtree structure pagedlod is derived from lod and adds the ability to defer the loading of child nodes until they are actually required for rapid response to the activation of the globe engine the manipulator is composed of several handlers as soon as the globe engine is activated it will call the render engine to modify the scene graph by cull or draw traversals notably the renderer engine is able to link and execute external gpu code such as compute unified device architecture cuda kernels and custom shaders extending the osgearth virtual globe we integrate a plugin for receiving and processing compressed tiles the plugin is available through curl and gdal geospatial data abstraction library as an open source file transmission tool curl follows url syntax and works under the command line mode the client sends data requests to the server and adopts curl to decompress the data and then shows the data in real time on the osgearth virtual globe after the dataset is read by gdal in addition the cross platform qt provides a set of ui elements for classic desktop style user interfaces these libraries together with a gpu accelerated library for image parallel computing form the data engine layer all of these layers work together to provide online large volume data visualization and analysis applications in climate change research section 4 describes these mechanisms 4 methodology as the multi layers of images and geo referenced visualization over an entire globe require a great amount of well processed data which would be very difficult to create and manage in a stand alone system wang et al 2013 we develop a client server like tile service for massive data analysis and visualization under online hardware restrictions and high concurrency request conditions 4 1 tile service for high efficiency analysis 4 1 1 data preprocessing and organization workflow to realize online high efficiency visualization and analysis we adopt a pre processing method for raw data to build a tile set based on the image pyramid structure for each image according to its spatial extent and spatial reference then the client requests corresponding level tiles in accordance with the distance from the viewpoint for display in the case of a statistical calculation the system calls the highest level tiles lossless level tiles to perform the computation this method focuses on resolving three problems 1 tile indexing problem to reduce the tile retrieval time the tiles need to be organized in an efficient and generic manner 2 tile accuracy problem ensure that the highest level tiles are a full backup for the original image and that the backup procedure should be reversible 3 data redundancy problem in situations of limited disk space and constantly updating data the tile should be set to minimize the data size to reduce the pressure on the server s hard drive and to facilitate rapid transmission through the internet we designed the following solution for accelerating the calculating and loading speeds of images without any loss in data accuracy to ensure real time internet based analysis and visualization first the tile indexing problem is inspired and addressed by the data loading method of osgearth as shown in fig 3 based on a pyramid model osgearth selects tiles to perform dynamic loading of large datasets in other words by recording the spatial reference and spatial extent information of the original image osgearth adopts a real time image decomposition process by a tilekey technology that is the index of each tile and calculated by the distance from the viewpoint to conduct data management tilekey is in a structure of level x y at level 0 the whole globe is divided into two hemisphere tiles and each tile is managed by tilekey 0 0 0 and 0 1 0 then as the distance from the viewpoint becomes closer osgearth continues to index the tiles with a tilekey from the lower levels to the higher levels according to a quadtree data structure for every two adjacent levels each tile in the upper level is equally divided into four lower level tiles as the levels go deeper the tiles decrease until the spatial resolution is finer than that of the original image with the refinement of the tiles the range of each tile decreases gradually such that when doing data loading the corresponding tile will be loaded only if there are valid data from the original image range this data loading method can improve the rendering efficiency by avoiding unnecessary data loading but data processing also creates a certain efficiency loss thus the original image are preprocessed to tiles to avoid such efficiency loss similarly the tiles are organized by the rule of tilekey which means that the retrieving path of each tile file can be expressed by the folder level subfolder x and tile file name y we also regard the image file name as a unique identifier to reduce the tile retrieval time meanwhile in the process of data preprocessing the maximum pyramid level projection coordinates row number column number and spatial extent of the original image are recorded as metadata for further usage according to the built in tilekey the osgearth virtual globe can therefore perform direct rendering by resolving the corresponding tile paths to conduct real time requests which can be expressed as 1 f level x y http serverip address filename level x y second there is a tile accuracy problem to retain the pixel value of the original image for any original image pixel m n with a value α we must find one pixel p q among the tiles that has the same value α and such mapping should be a one to one correspondence since osgearth continues to decompose tiles from up to down until the spatial resolution of the tiles is finer than that of the original image the pixel number of the maximum level tiles is larger than that of the original image we set the maximum level that the original image can achieve as max and regard all of the pixels among the tiles at level max as set o all the pixels of the maximum level tiles in the intersection of the same spatial extent of the original image are contained in set m which is a subset of set o with a pixel value set n similarly all of the pixels of the original image are set p with a pixel value set q since set m should be larger than set p we define a pixel set r with a pixel value set s that is a subset of set m and the mapping between set p and set r is a one to one correspondence fig 4 shows the relationship description of all these sets thus the main task would be the construction of a one to one correspondence mapping from set p to set r once this mapping relationship is generated it is further processed to obtain the rest of the pixel values in set o r stand for set o minus set r the method we adopt here is the nearest neighbour search method because it can retain all pixel values of the original image and is more efficient without losing accuracy extending the nearest neighbour search method we integrate some modifications to further enhance the analysis efficiency the details of this method are as follows we first illustrate the mapping of points from set p to set r fig 5 a to maintain a one to one correspondence in the mapping between set p and set r we define the pixel in set r as having a coordinate a m a x i j with a pixel value α m a x i j similarly a pixel of set p can be denoted as b l m with a pixel value β l m the mapping between these two sets can be represented as 2 f b l m a m a x i j where i θl j γm and χ indicates the round of χ the two parameters γ and θ are the ratio of the row column number from set m to set p with this mapping function we can find for each pixel in set p a unique pixel in set r and we define this pixel value as α m a x i j β l m however there are still some pixels in set m that have not been valued that we call set m r similarly we construct a mapping from set m r to set p 3 g a m a x i j b l m where l i θ m j γ we add one more digit for all of the elements from set m r to facilitate the restoration of the original image from the tiles e g we assign 0 34321 to the pixel from set m r when the corresponding original pixel value is 0 3432 furthermore by assigning nodata to set o m we then obtain a 1 1 tile backup for the original image quickly and easily next we adopt the quadtree data structure to obtain a pyramid tile set fig 5 b finally we address the problem of data redundancy as the tile set is already a 1 1 backup of the original image we only need to save this tile set without the original image considering the data volume as well as its rapid growth we designed the following compression strategy to reduce the consumption of disk space fig 5 c because some parts of the original image are useless for our research e g the parts blocked by overcast conditions we set the values of these parts as nodata all of the nodata values are rendered transparent on the virtual globe apart from nodata there are pixels with a float format that have poor predictability furthermore although the compression strategy can reduce the disk footprint and improve the online file transmission speed it can also result in decompression problems in this situation we adopt the deflate deutsch 1996 encoder as compression encoding and the curl module as decompression engine for data requests deflate is a popular dictionary based compression method that was originally used in the well known zip and gzip software and has since been adopted by many applications the most important is the http protocol david 2004 it operates by constructing a dictionary to register a string that appears repeatedly in the original data flow as a shorter string meanwhile the curl module supports gzip and inflates content encoding and performs automatic decompression as the tile set evolves the requested tiles are transmitted from the server to the client upon decompression the highest level tiles go through a data restoration phase to reconstruct the original image part next either the lower level tiles or the restored image part are pushed to the gpu for rendering 4 1 2 hybrid database file system for heterogeneous data organization on the server side a large volume tile set with its metadata are constructed in addition in situ data are also needed in climate change research all of these heterogeneous data should be unified into a data platform for organization purposes to shift the burden from the client to the server to accomplish this a hybrid database file system is adopted for the unified organization of multi source data and fig 6 shows the schema design first we adopt the hdfs to manage the tile set the hdfs cluster consists of a single namenode a master server that manages the file system namespace and regulates access to files by clients in addition there are a number of datanodes which connect to the namenode and respond to requests from the namenode for filesystem operations borthakur 2008 hadoop due to its scalability in storage and computing power is a suitable platform for increasing volumes of remote sensing data lee and lee 2013 however the hdfs has some deficiencies in the management of multisource data for example the smallest unit of vector data is a record that is specific time and space parameter information and multiple records constitute a vector data this structured way of organizing data can be disastrous for hdfs to remedy this in our hybrid database file system the in situ data and the metadata of original images are managed by postgresql while the spatial objects are stored in postgis the postgresql postgis strategy not only reduces the retrieval pressure of the data node but also improves the query efficiency thus by taking advantage of the spatial information service of postgresql postgis the proposed hybrid database file system solves the problem of heterogeneous data organization and provides efficient data storage and access services 4 1 3 gsoap based web service for the purpose of simplifying client to server interactions we construct a web service based on gsoap tools as the heterogeneous data have been stored in the hybrid database file system we need only to interact with the virtual globe send the request to the server through the soap protocol and wait for the response after it finishes all of the computations based on the lossless tile set soap foo and lee 2002 is a lightweight protocol for exchanging structured information in a decentralized and distributed environment the majority of c web service toolkit libraries provide a set of api functions to handle specific soap data models however users have to change their program structures to accommodate the relevant libraries in contrast gsoap provides a kind of transparent soap api function by compiler technology and the detailed contents of soap that are not related to the development are hidden to users thus we need only to define a series of relevant apis both in the client and in the server thereby organizing the relevant parameters into xml messages for multi request cases such as time series queries we combine a plurality of xml messages into one xml message based on the xml structural features upon receiving the message the server parses the message and returns the result to the client similarly the server also combines multiple results into one taking temporal average along sections for example we organize the xml keys by region of interest roi file of interest foi and maxlevel roi contains the latitude and longitude information of each sections foi comprises the dataset type and start end date of the composition period of satellite data information and maxlevel is the maximum pyramid level reached by the original image all of this information can be easily accessed from the hybrid database as an example table 1 is an xml encoding request of temporal average along sections query 4 2 tile service based ocean carbon flux estimation for analysis purposes with the proposed tile service we can perform online high efficiency ocean carbon flux estimation by large sized remote sensing images in this study we adopt a commonly used method to calculate the air sea co2 flux for ocean carbon flux estimation which is based on the multiplication of the co2 partial pressure difference between the surface seawater and the atmosphere and the co2 gas transfer velocity the equation is as follows 4 flux δ p co 2 e wcp acp k h co 2 ρ c 2 k 24 10 2 where wcp is the partial pressure of co2 in seawater acp is the partial pressure of co2 in atmosphere k h co 2 is the dissolution efficient of co2 ρ is the sea water density c 2 is the wind speed coefficient and k is the gas transfer velocity all of these unknown variables can be calculated from sea surface temperature sst sea surface salinity sss and sea surface wind speed ssw the details are attached in appendix a these five remote sensing image data wcp acp sst ssw and sss have been pre processed and constantly updated in the hybrid database file system for high efficiency analysis through the tile service as the original image has been broken into lightweight workable tiles the entire calculation process is applied to each tile in parallel fig 7 and the calculation results can be loaded onto the virtual globe directly tile by tile without any merging similarly the calculated flux tile data is stored in the hybrid database file system and can be used for external carbon budget estimation through the tile service as mentioned above users interact with the virtual globe send the request to the server and wait for a response moreover the estimation process only calls the parts of image that users desire to participate in the computation greatly enhancing the calculation efficiency 4 3 gpu based volume rendering of remote tile sets for visualization purposes based on the tile service a direct volume rendering method du et al 2015 is adopted to realize the spatiotemporal visualization of remote time varying air sea co2 flux tile sets volume rendering is a technique for rendering images of volumes containing a mixture of materials and can represent both the interiors and the boundaries between different regions drebin et al 1988 laur and hanrahan 1991 this technology has been widely used for the simulation of climate phenomena and the visualization of massive multidimensional datasets arthur et al 2010 darles et al 2011 song et al 2006 furthermore with hardware advances graphics processing units gpus have been widely used to accelerate volumetric rendering for interactive applications monte et al 2013 specifically in this section volume data reconstruction and updating are realized by cuda parallel computing with the tile set on the server side and a half angle slice rendering technique green 2008 is adopted for the dynamic visualization of climate data with a shadow effect cuda is a parallel computing platform and programming model developed by nvidia for general computing on graphical processing units gpus nvidia 2010 with cuda we are able to dramatically speed up the half angle slice computing by harnessing the power of gpus the entire rendering process can be summarized as follows 1 generate or update the location and colour of the flux particles 2 send visualization results over the network from servers 3 calculate the half angle array and order the particles along the array 4 for each tile select a set of particles for projection and render them from the viewpoint of both the viewer and the light 5 compose tiles to form the final image and repeat the rendering process fig 8 shows the effect of this half angle volume rendering approach for air sea co2 flux dynamic simulation 5 demonstration and evaluation 5 1 system demonstration integrating the architecture we proposed above the satellite based marine carbon monitoring and analysis system satco2 is developed for ocean carbon flux estimations and its database provides relevant remote sensing data by providing information on the influence of climatic variability and change on carbon stocks and fluxes kang et al 2014 fig 9 shows some screenshots of the satco2 interface fig 9 a shows a view of monthly averaged sea surface wind data for may 2016 where blue represents a low wind speed red represents a high wind speed and the black vectors represent wind directions as shown in fig 9 b satco2 adopts the method mentioned in section 4 2 to calculate the air sea co2 flux for ocean carbon flux estimation fig 9 c shows the carbon budget calculation result of the east china sea during april 2000 the air sea carbon budget during april 2000 is 12 62 tc vr 5 2 experiment evaluation the data we use in this section for the experiments are ten day average chlorophyll α concentration data from 1 january 2010 to 31 december 2015 in the south china sea area 98 127 e 0 25 n with a horizontal resolution of 2 km section 5 2 1 compares the loading times of lossless tiles and the original image under different network bandwidths we then compare the calculation times of time series data in different time spans under conditions of whether we adopt a lossless tile set in section 5 2 2 and section 5 2 3 shows the rendering performance evaluation by the spatiotemporal visualization process 5 2 1 enhanced efficiency of the data loading the image animation module of satco2 i e query and select the time series data from the database for animation after which the selected data will be loaded to the virtual globe as layers is applied to compare the loading times between the lossless tiles and the original image to eliminate the uncertainty caused by data compression in our experiments the original image is pre processed with the same compression method mentioned in section 4 1 we also repeat the experiment five times the cache is cleared after each experiment and choose the average result to avoid uncertainty factors such as bandwidth fluctuations as much as possible apart from this uncertainty because of the inconsistent amount of tiles among different levels in the pyramid structure only tiles from the maximum level are loaded to ensure that the detail of lossless tiles is the same as the original image the experiment consists of five different bandwidths 4 mbps 10 mbps 20 mbps 50 mbps and 100 mbps and ten variable data counts the loading time costs and comparison results are shown in fig 10 fig 10 a shows a comparison of the loading times among the original compressed images in different network bandwidths while fig 10 b compares the lossless compressed tiles with the same parameter used in fig 10 a fig 10 c shows the ratio of loading times between the original image in fig 10 a and the lossless compressed tiles in fig 10 b that are under the same bandwidth and data count as we can see from fig 10 a the relation between the loading time and the number of loading images is almost linear at a lower bandwidth e g 10 mbps and 4 mbps which means that the network bandwidth is the dominant factor in fig 10 b almost all of the loading times fluctuate between 4 and 5 s whenever the bandwidth changes we can conclude that hardware conditions such as fluctuations in the memory could dominate in this case it can be determined from fig 10 c that the advantage of the tiling strategy in data loading is quite obvious at low bandwidths compared with the time cost of the original image however this gap is narrowed as the bandwidth increases and the loading time is almost the same when the bandwidth is broad enough e g 100 mbps in this experiment we only simulate in the way of point to point transmission if we are facing real world applications insomuch that the upload bandwidths of servers and the download bandwidths of clients are both limited it will challenge the transmission of original images substantially in contrast the tile strategy could behave better at speeding loading times up for the transmission of massive data from the server to the client under a limited network bandwidth this phenomenon is mainly due to the characteristics of the architecture it is able to do real time operations and realize the block by block loading of pyramid tiles on demand while the default method needs to wait for the entire original dataset to finish transmission 5 2 2 efficiency and accuracy promotion in geospatial analysis in traditional euclidean geometry both lines and polygons are composed of infinite points but remote sensing images are made up of only finite pixels because of the concept of resolution based on the point level the following experiment is carried out to compare the calculation efficiency and accuracy between the lossless tiles and the original image moreover as multiple requests are combined into an entire xml message when performing a time series query the independent variable is the data source this experiment is performed on the server side to eliminate any consideration of network conditions and to better demonstrate the results all data are prepared and hosted in the server s file system by generating a random coordinate set in the original image we select time series of different time spans and conduct a point time series query then the query result is saved to compare the accuracy the time cost comparison is shown in fig 11 as we can see from fig 11 the lossless tiling strategy can obviously improve the efficiency of the statistical analysis and this enhancement is more remarkable as the analysis becomes more complex the loss due to mapping from the original coordinates to tilekey is negligible as the time complexity of the nearest neighbour search algorithm is o n additionally calling the tile of an image can significantly reduce the memory consumption and disk read time thus we pay more attention to the accuracy problem theoretically the result of these two methods should be the same as the tile set is a full backup of the original data and we can prove this to be true in our experiment by introducing the average error of the result as 5 rae a i b i b i where b i is the original value and a i stands for the tile value as shown in table 2 the rae from january 2010 to december 2010 is 0 based on these results we can conclude that the tiling strategy can be much more efficient at statistical analysis than the original image with no loss of accuracy 5 2 3 rendering performance evaluation in this section the experiment is performed on a pc with an nvidia geforce gtx 660 2 gb graphics card and a 10 mbps network fig 12 shows the fps with a spatiotemporal visualization of the different numbers of carbon flux tile data in a local area network the minimum maximum baseline is the rendering efficiency of du s experiment 2015 in a standalone pc with the same graphics card the result shows that the average fps of the tile volume rendering exceeds the baseline this is because the tiles can be rendered as they reach the client rather than having to wait for the original image to be read from the disk into memory before the data reconstruction and update processing can begin thus we can conclude that tile service technology improves online rendering performance dramatically 6 conclusions and discussion the objective of this paper is to propose a systematic architecture for online high efficiency visualization and geospatial analysis in climate change research according to the special features of climate change research e g large volumes of data and heavy computing burden a tile service driven architecture is implemented in this architecture the original grid dataset is organized into a compressed pyramid tile set so that both the access efficiency and the loading speed for voluminous datasets are improved exploiting the tile set and organization of these data models a gsoap based web service and a hybrid database file system are configured to maintain a high efficiency calculation and analysis within the constraints of the hardware resources we also introduce the open source virtual globe osgearth that can generate spatiotemporal dynamics quickly and has high efficiency as well as greater realism in this way the proposed architecture can not only provide an intuitive interaction friendly data information user interface on the virtual globe but also greatly improve the efficiency of data transmission three dimensional visualization and geospatial analysis without a loss in accuracy which has been proven in our experiments above in addition we combine this architecture to construct a satco2 system by taking full advantage of the architecture satco2 helps researchers to study the ocean carbon cycle and climate change in a virtual globe environment through the internet even when the volume of data is truly enormous compared to earth engine satco2 is based on an effective open source virtual globe and supports high quality volume rendering which is suitable for enhancing visualizations moreover since satco2 focuses on carbon cycle research it has a more professional dataset than earth engine however earth engine is web based and is more convenient for researchers to visualize very large datasets within browsers thus we consider porting the proposed architecture to a web platform for further research although the results presented in this paper demonstrate the successful application of the architecture we proposed this architecture also has some shortcomings that need to be improved first because the original remote sensing image is organized into a pyramid tile set and then stored in hdfs the tile is broken into smaller sized tiles once again and finally stored in different data nodes which causes a secondary index when performing data loading undoubtedly reducing efficiency in future research we consider combining these two methods and directly processing the data to lossless tiles during the storing procedure second the proposed lossless tile set strategy is mainly effective for the osgearth virtual globe and has certain limitations this strategy will be transferred to other virtual globe platforms for further evaluation acknowledgements the remote sensing datasets for the long term marine environment and carbon cycle were provided by the second institute of oceanography sio state oceanic administration of china soa appendix a parameter equations of air sea co2 flux calculation 1 equation of the dissolution efficiency of co2 mol kg atm millero 1995 a1 ln k h co 2 60 2409 93 4517 100 t 23 3585 ln t 100 sss 0 023517 0 023656 t 100 0 0047036 t 100 2 a2 t sst c 273 15 2 sea water density can be calculated by the function of surface water temperature and salinity millero and poisson 1981 a3 ρ ρ w a sss b sss 3 2 c sss 2 10 3 where a4 a 0 824493 4 0899 10 3 sst 7 6438 10 5 sst 2 8 2467 10 7 sst 3 5 3875 10 9 sst 4 a5 b 5 72466 10 3 1 0227 10 4 sst 1 6546 10 6 sst 2 a6 c 4 8314 10 4 a7 ρ w 999 842594 6 793952 10 2 sst 9 09529 10 3 sst 2 1 001685 10 4 sst 3 1 120083 10 6 sst 4 6 536332 10 9 sst 5 3 to calculate the monthly average flux it is often necessary to consider the influence of high frequency wind speed change e g daily on the monthly average wind speed using a coefficient of c2 wanninkhof et al 2002 the c2 coefficient is not needed when calculating daily flux a8 c 2 u j 2 m e a n u m e a n 2 u j is high frequency satellite derived wind speed e g daily and u m e a n is satellite derived monthly average wind speed both with units of m s 4 based on the relationship between the gas transfer velocity k and the wind speed at 10 m above sea level u 10 the commonly used equations for calculating k are shown in the table below table a1 commonly used equations for calculating gas transfer velocity table a1 equation 1 k 660 0 31 u 10 2 instantaneous wind speed k 660 0 39 u 10 2 long term average wind speed wanninkhof 1992 2 k 660 0 27 u 10 2 sweeney et al 2007 3 k 600 0 266 u 10 2 ho et al 2006 4 k 660 0 0283 u 10 3 wanninkhof and mcgillis 1999 k 660 and k 600 indicate the k with schmidt numbers sc of 660 and 600 respectively a9 k k 600 s c 600 0 5 a10 k k 660 s c 660 0 5 when the sea surface temperature ranges from 0 to 30 c the sc can be calculated with the following equation a11 sc 2073 1 125 62 sst 3 6276 sst 2 0 043219 sst 3 for the calculation of the satellite derived air sea co2 flux in the open ocean the k u 10 equations of 1 long term wind speed and 2 in the above table are commonly used appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 005 funding this research was supported by the national key research and development program of china 2018yfb0505000 2016yfc1400903 the national natural science foundation of china 41871287 41671391 and the public science and technology research funds projects for ocean research 201505003 
26202,demand is increasing for greater transparency of the science underpinning decision making processes in land resource management to illustrate how the application of fine grained data provenance can increase the credibility and transparency of scientific methods and outputs we implement provenance tracking for two different modelling frameworks pyluc and lumass and present results from example models pyluc is a python based framework for generating spatial land use classification data with automatically generated technical documentation lumass is a spatial modelling and optimisation framework within which new zealand s sediment budget model sednetnz is implemented in both cases detailed provenance tracking resulted in a complexity of information which necessitated the development of an interactive data provenance visualization tool to help science producers and users explore verify and understand model outputs we argue that best data management and sharing practice should include fine grained data provenance to meet demands for the quality and integrity of science based data and information keywords fine grained data provenance provenance visualization transparent science environmental modelling lumass pyluc software availability python land use classification framework pyluc software pyluc developer en jolly jollyb landcareresearch co nz year first available 2019 hardware pc mac hpc 4 gb ram 10 gb disk space software required python 3 5 with gdal environment program language python program size 1 mb licence gnu general public licence v3 gpl availability source code download https bitbucket org landcareresearch pyluc land use management support system lumass software lumass developer alexander herzig herziga landcareresearch co nz year first available 2012 hardware pc notebook 4 gb ram 1 gb disk space software required windows 64bit or linux program language c program size 30 mb source 312 mb windows 64bit installed licence gnu general public licence v3 gpl availability source code windows 64 bit binaries download https bitbucket org landcareresearch lumass provenance visualization tool provis d3 software provis d3 developer tomas burleigh burleight landcareresearch co nz year first available 2019 hardware pc mac software required modern web browser edge chrome firefox program language javascript licence gnu general public licence v3 gpl availability web site access http vizdemo landcareresearch co nz provis d3 index html 1 introduction 1 1 background the size of environmental datasets is continuously increasing in parallel with technologies that allow data to be collected at greater resolutions in time and space as a result the ability to manage these data and perform advanced scientific analysis is becoming more complex simmhan et al 2005 moreover data sharing among and across scientific communities is becoming the norm nested within this context environmental models are developed and used to help inform decision making in policy however it has been suggested that science based knowledge can be flawed by the user being unaware of the source process and context behind the information being used kattge et al 2014 mittelstrass 2010 spiekermann et al 2015 weichselgartner obersteiner 2002 furthermore organisations are increasingly required to justify why they made certain decisions or provided certain advice thus the demand for more transparent science has increased duarte et al 2015 browman 2016 özkundakci et al 2018 brinckman et al 2019 indeed larcombe and ridd 2018 proposed governments establish independent organisations to undertake quality reviews and audits of important scientific results that underpin major government spending decisions which were to include a system of guaranteed and organised technical debate to rigorously test for scientific deficiencies therefore tools need to be developed that can help improve understanding on the source process and context behind the information within this context data provenance is aimed at improving the process of knowledge generation and sharing data provenance tracks and documents information about entities activities and people involved in producing a piece of data belhajjame et al 2013 and thus has the potential to increase the saliency credibility legitimacy and reproducibility of scientific methods and outputs a clear advantage of data provenance and increasingly a requirement for datasets within the scientific community that inform policy is the increased transparency it provides the australian national data service ands for example understands data provenance as the instrument to ensure that the data that leads to a scientific finding or publication can be trusted and verified and that data can be re purposed and reused trustfully in more research treloar and wu 2016 p 184 a growing body of literature exists on data provenance and various proposals of definitions terminologies and ontologies can be found e g simmhan et al 2005 moreau 2010 some growing out of an interest in data lineage e g lanter 1991 due to the lack of consensus in the scientific community on the meaning of provenance and its implementation most researchers have limited data provenance to only a few aspects of provenance generally encompassing the origin lineage or source of the data buneman et al 2001 ram and liu 2008 2009 liu and ram 2011 and recorded as part of the metadata about a dataset provenance can be captured manually by editing associated metadata after some process has been executed automated capture of provenance information helps address some of the problems associated with manual capture e g incomplete records though the applicability ease of implementation and usefulness of this method depends substantially on how a dataset is created in the context of their work on provenance systems the world wide web consortium w3c provenance incubator group broadened the scope of data provenance defining provenance of a resource as a record that describes entities and processes involved in producing and delivering or otherwise influencing that resource provenance provides a critical foundation for assessing authenticity enabling trust and allowing reproducibility provenance assertions are a form of contextual metadata and can themselves become important records with their own provenance w3c 2010 this definition clearly includes more than just the lineage or source of data it also encompasses contextual information of the processes that generated that data to have value provenance thus must describe who or what was involved in the creation where existing data were sourced and how the data were processed simmhan et al 2005 provide a comprehensive account of how provenance can be used to support a range of applications e g audit trail attribution w3c 2010 describe key dimensions of data provenance grouped according to the requirements for content management and use the requirements were used to inform the design of the data provenance system which can be defined as an environment that supports a computer based representation of provenance that can be queried and reasoned over using specified technologies moreau et al 2008 provenance systems encompass the capture storage management and dissemination of provenance information provenance needs to be readily available for consumers of a given dataset and it is paramount that those consumers can trust the authenticity and completeness of the information two major approaches to representing provenance according to simmhan et al 2005 are annotations and inversion the annotation approach collects metadata about input data sources processes and any additional relevant information at each step in the process of creating and editing a data file the inversion method instead inverts the functions and or queries used to generate a dataset to identify the source data a primary advantage of the inversion method is efficient storage whereas the annotation approach is of far greater flexibility in the level of provenance information that may be captured a differentiation in terms of granularity can be helpful to elucidate what is captured by data provenance tracking course grained data provenance also known as workflow provenance is defined at a higher granularity level as it captures the relationships between different activities and items in a model but excludes the complete derivation of an item so that reproducibility cannot be achieved sheikh et al 2018 fine grained data provenance is applied at the level of the value and documents the association between inputs outputs and affiliated processes sheikh et al 2018 fine grained data provenance can increase the transparency in modelling as it is intrinsically tied to each model run and provides a comprehensive account of what was done and how it was done for a specific modelling case to facilitate cross platform sharing of provenance information it is important to use an appropriate standardised data model a number of standardised models exist some more domain specific than others the standard lineage model in the iso 19115 geographic information metadata as specified by the technical committee 211 of the international organisation for standardisation iso can be used to model geospatial data provenance di et al 2013 however this model is quite constrained even within the geospatial community closa et al 2017 the w3c produced the prov family of documents w3c 2013a outlining a generic model for representing provenance information using common encoding formats such as xml w3c 2013b rdf and json jiang et al 2018 compared the iso 19115 to w3c prov and evaluated their compatibility concluding that the main benefit of w3c prov is its causality based design which allows rich semantics to be asserted on the relations between things though w3c prov currently needs more domain enrichment jiang et al 2018 it is an effective way forward for implementing provenance tracking tools already exist for creating editing and visualising prov information rdatatracker lerner and boose 2014 outputs prov json files and sumatra davison 2012 has the outputting of prov xml in its development roadmap prov data can be visualised in a visualization tool called prov svg car 2017 1 2 provenance environmental modelling and land resource data land resource data provides scientists policy makers regulators and land owners with the basis to assess the state and trend of the land resource monitor the impact of policies and regulation and respond effectively through land management decisions environmental modelling is often undertaken to investigate changing resources e g land use change and its environmental impacts information resulting from environmental models thus underpins decision making processes by both business and government agencies it is in the interest of both environmental modellers and decision makers in policy and practice to increase the transparency surrounding these processes given that transparency is the primary quality of data provenance it can serve to meet the demand for credible science and sustained legitimacy in policy when deciding which information is to be captured it is essential to consider the possible uses and users of data provenance from the policy perspective best practice requires policy documents to i provide evidence to support their conclusions and recommendations ii ensure there is an easy audit trail to allow decision makers to understand the data the model used to generate the data and assumptions underlying conclusions and recommendations and iii deliver sufficient information to support any later re evaluation or challenge supplying provenance information with land resource data that underpins policy therefore contributes to best practice from an environmental modelling standpoint modellers need to ensure that i the purpose of the model is understood ii the modelling process is transparent and iii best practice guidelines for modelling are followed özkundakci et al 2018 jakeman et al 2006 transparency in modelling encourages use of best practice guidelines to strengthen reliability and increase the credibility of the science until now recommendations for improving reproducible science have been made for social nosek et al 2015 and environmental science croucher et al 2017 tonitto et al 2018 these generally include version control of code and associated data and storing data in a repository which can be made publicly available for citation while these measures support reproducible science the provision of code cannot guarantee a replication of results as parameters and data inputs can change from one model run to the next the demands of transparent environmental modelling require more details related to the individual elements within the model which is more likely guaranteed by application of fine grained data provenance within modelling frameworks users often need to determine whether the results are correct which requires full transparency of the model the resulting documentation of fine grained data provenance therefore also serves model verification 1 3 fine grained provenance tracking to improve transparent modelling to improve reproducibility in environmental science as well as its credibility it is important that the precise conditions and context of models are retained and that transparent science is not constrained to the mere long term preservation of data in an accessible form as horsburgh athanasiadis 2018 correctly recognized reproducibility includes tracking and documentation of the algorithms used the input parameters software versions involved the intermediate data pre processing and transformation steps and the provision of precise information to guarantee reproduction of exact results using the same data the recorded details of a complete workflow are frequently inadequate for example some random and temporarily generated elements can be involved within a dynamic model making reproducibility difficult horsburgh athanasiadis 2018 conclude that since it is unlikely that all scientific disciplines will settle on standards for data collection management and analysis better methods are needed for capturing scientific workflows to enhance the reproducibility of data intensive analyses and modelling we argue that explicit fine grained provenance tracking is a method that can help meet the demands of transparent and reproducible science until now data provenance applications in the context of environmental modelling are limited in both frequency and granularity zhang et al 2017 demonstrated provenance tracking for a relatively simple simulation of watershed runoff iturbide et al 2019 include a provenance metadata model metaclip to describe the data workflow of climate products essawy et al 2018 developed a workflow documentation including retrospective provenance to improve the reproducibility of computational environmental models however these studies are limited to documenting workflow provenance which captures association among different processes but excludes the intricacies of algorithms used the key components which determine the output sheikh et al 2018 we implemented fine grained data provenance using two different modelling frameworks each with a different application here we define a framework as the underlying software and architecture as well as the module and model system argent et al 2006 development was carried out independently using the prov model w3c prov is a generic data model that is designed to support application specific extensions jiang et al 2018 the first spatial modelling framework pyluc was developed to help generate land use classifications lucs with self documenting definitions that could be processed to create both the spatial dataset the luc and supporting documentation including fine grained data provenance the requirement was for clients to have more transparency with respect to how the classified land use data was generated the primary purpose of implementing fine grained provenance tracking within the second spatial modelling framework the land use management support system lumass was to support model development through improved documentation and model verification we compare the different implementations and discuss the achievements and challenges of each to date this is the first attempt at fine grained explicit data provenance tracking in environmental modelling following implementation we examine the feasibility of the implementations cohen boulakia et al 2017 recognized the need for interactive systems and tools able to visualize query or mine provenance information represented using prov standards given the enormity of the provenance information generated macko and seltzer 2011 designing such tools is very challenging we present preliminary results of two interactive visualization tools that aim to deal with the complexity of the graphic representation of data provenance using optimized algorithms and improved interactivity achieved through a secondary neighbourhood visualization the tools were tested using the provenance data generated by pyluc and lumass we then evaluate the utility of the provenance information produced by the two frameworks and discuss the usability of their representation using the interactive visualization systems 2 methods 2 1 the pyluc land use classification tool provenance by design pyluc was developed to support the development of spatial land use classifications luc land use can be defined as the activities or socio economic functions for which land is used the allocation and use of land affect all aspects of a society s overall well being economic environmental social and cultural and quality of life it differs from land cover which describes the physical state of the land leslie 2004 land use data provide information on the function and purpose for which land is currently used and when tracked over time how land use changes young 1998 knowledge of land use supports analysis and management of land vegetation water resources and quality and the maintenance of biodiversity the objective of a land use classification is to provide a framework to guide the collection of data and the creation of effective databases to ensure comparability and compatibility gong et al 2009 pyluc is a geospatial data processing framework written using the python language aimed at complementing standard geographic information system gis tools to generate spatial land use classifications lucs but with increased levels of transparency and repeatability it was developed to fill a void where researchers developing land use classifications needed to be able to provide better provenance and technical documentation with the classification output but struggled to find the right tools to do so a typical user case for scientists is to develop a luc using familiar gis tools then transfer the finalised logic to a pyluc definition script to produce the definitive dataset ready for publication or delivery to a client often local or central government to help inform policy an essential output of pyluc is the automated documentation and provenance information generated during a classification run the intention for the provenance information here is to facilitate the process of understanding what steps a classification algorithm took exactly what data it ingested where who produced that data who wrote each part of the algorithm what their affiliations were and how everything ties together in order to increase the transparency of the luc product data provenance is not primarily aimed at reproducibility though it does support reproducible science by increasing confidence in datasets in the initial version of this application we restricted input data sources to those hosted on instances of the koordinates 1 1 https koordinates com geospatial data warehouse platform this was done for three main reasons 1 many of new zealand s authoritative geospatial data sources exist on this platform 2 to encourage the use of original primary sources with any modifications explicitly baked into the luc definition and 3 because data layers stored on this platform are immutable which ensures all referenced datasets will exist in their original form at any point in the future the last two reasons significantly contribute to the transparency and repeatability of the luc process thus it is possible for a user to recreate a luc with essential technical documentation and provenance data given a pyluc definition script single text file and a koordinates user account with appropriate permissions to access protected data sources this avoids passing on a copy of the source dataset itself which can easily be copied modified and shared a process which could create problems with data licensing and over time can lead to issues with reproducibility pyluc was created from scratch with data provenance in mind as such its internal data model is structured to follow the w3c prov data model prov dm w3c 2013c which revolves around agents activities entities and associated relations as illustrated in fig 1 the general concept is that people and organisations are agents where most people will be affiliated with one or more organisations luc algorithms are defined by a series of classification rules or steps that are represented using prov activities while all data input intermediate output from luc rules and final output are represented by entities pyluc distinguishes variables from datasets in this model using hierarchical entities to denote association this is done to help understand exactly which variables from an input dataset contributed to which rules the model in fig 1 is arranged to show the flow of data from the top down however it may be more intuitive to read from the bottom up by following the arrows starting at the final output entity this wasgeneratedby a rule that used outputs generated by multiple rules that in turn used multiple input vars each of which wasderivedfrom an input dataset each input dataset wasattributedto an organisation each of which had one or more persons who actedonbehalf of them each person may also have actedonbehalf of another person normally used for delegation where the person who runs pyluc may not be the author of a rule each rule wasassociatedwith the author person responsible for writing it with the corresponding output also being attributed to them provenance is delivered by pyluc as a prov n file standard notation for w3c prov model that while not easily human readable can be ingested by other tools for analysis and visualization for example provstore 2017 is an all in one website 2 2 https openprovenance org store for prov information that currently serves as the primary distribution mechanism for pyluc provenance records it provides for uploading read only prov documents as well as exposing a rest api that can be used to directly access the remotely hosted provenance records via third party tools provstore allows the export of provenance records in various formats json turtle xml etc contains several visualization tools and can be treated as a trusted third party source provenance records uploaded to this site are read only and cannot be edited thus given the upload process is an automated component of an automated system viewers can be reasonably confident it has not been manually tampered with to demonstrate the utility of the provenance tracking implemented in the pyluc framework we present the output of a relatively simple luc algorithm called lunz land use new zealand in section 3 this algorithm combines several land cover and land use datasets of varying levels of utility dependent on geographic location or type of information using a set of rules essentially merging several geospatial input layers into one single output layer 2 2 integration of fine grained model provenance documentation into the lumass modelling framework 2 2 1 the lumass spatial modelling and optimisation framework the land use management support system lumass is a free and open source spatial modelling and optimisation framework it aims to support the understanding of landscape ecosystems by environmental modelling jørgensen 1994 powers et al 2011 ausseil et al 2013 and to aide land management decision making by spatial optimisation herzig et al 2013 2016 2018 lumass provides a visual programming environment to enable scientists and gis professionals with little to no programming skills to develop complex dynamic geospatial raster based models operating on large datasets herzig and rutledge 2013 users combine atomic processing components e g for map algebra spatial optimisation or the processing of tabular data to processing pipelines and group these into higher level aggregate components which can be nested to build hierarchies of processing steps the execution order of individual processing steps starts at the highest level of aggregation and is recursively propagated to lower aggregation levels at each aggregation level the order of execution is determined by time levels associated with each component and progresses from higher time levels to lower time levels to enable dynamics and feedbacks individual processing components pipelines and aggregate components can be executed iteratively processing different datasets and or applying different processing parameters at each iteration step herzig 2017 to debug and test a model users can execute individual model components at selected iteration steps and inspect the parameters submitted to individual processing components at a given time step in the user interface however there has been no documentation of this information that is there has been no record of which datasets a model used modified or generated nor which processing components with which parameters were used at which time step to compute a particular result dataset unless special care and discipline is exercised by the modeller to manually record provenance information which is very time consuming at this level a non automated recording is particularly error prone especially in multi scenario modelling exercises the automated documentation of fine grained model provenance not only supports the modeller in these multi scenario modelling exercises but also serves as a reliable record to demonstrate to end users which algorithms were applied with which parameters to which dataset in the following sub sections we present the conceptual application of the w3c prov dm w3c 2013c to track fine grained provenance information for models implemented in the lumass modelling framework we then describe how we retro fitted fine grained model provenance tracking into the lumass source code to produce prov n w3c 2013d encoded provenance records finally we describe our use case to test the automated fine grained tracking of model provenance information implemented in lumass using the sednetnz model implementation 2 2 2 application of the w3c prov data model to track fine grained model provenance in lumass each model component of a lumass model is represented as an agent prov n softwareagent fig 2 and the hierarchical structure of model components is captured by the concept of delegation prov n actedonbehalfof the execution of a lumass model component is represented as a prov n activity that is associated with prov n wasassociatedwith the respective agent properties describing the model structure are assigned to the agents while runtime specific processing parameters equations etc are attributes assigned to the respective activity an agent is responsible for lumass processing pipelines including the flow of data between them is captured as information exchange between activities prov n wasinformedby input and output data of a model are represented as prov n entities that are used prov n generated prov n wasgeneratedby and revised prov n wasderivedfrom by activities start and end times of processing pipelines and individual activities as well as the time of usage generation and revision of data is recorded 2 2 3 implementation of fine grained model provenance tracking the execution of lumass models is internally governed by a modelcontroller object it manages the register of model components and model wide settings such as whether to track provenance information or not by default lumass records model provenance each time a model is executed and saves the information using prov n w3c 2013d notation in a text file provn in the user specified lumass working directory files are named after the model name and the date and time the model run was started e g hillslopeerosion 20180331t123039 provn to ensure cross platform usability dashes and colons are removed from the iso 8601 time stamp notation at the source code level provenance tracking was retro fitted to the modelling framework by the following adaptations the modelcontroller class was extended to i create a new model provenance file each time its associated model is executed ii receive provenance records from the framework objects while the model is running and append them to the provenance file iii collect optional prov n attribute value pairs for a given component or process object iv keep track of the number of revisions of each entity and v finalise the provenance file and close it since individual model component and process objects are unaware of the structure of the model and which dataset has been processed by which other processes before writing a prov n expression out the model controller filters potentially duplicate entries and adjusts entity identifiers to reflect the correct number of revisions a dataset has undergone the lumass logging framework was extended by the logprovn method to communicate provenance information among the framework objects and to facilitate the generation of prov n conformant provenance records nmlogger logprovn const nmprovconcept concept const qstringlist args qstringlist attr thereby concept is an enumeration type and denotes one of the prov dm data types or relations used by lumass i e entity activity generation usage communication start end revision agent attribution association or delegation the argument args contains the list of required model component or process object identifiers representing the required elements of the particular prov n expression for the given data type or relation e g entity e1 or wasderivedfrom e2 e1 the argument attr contains a list of optional attribute value pairs that are used to further specify the prov n data type or relation and to provide additional information to describe the data type or relation for example the data type agent exclusively refers to softwareagents hence the logprovn method always inserts the attribute prov type prov softwareagent at the front of the list of attributes when the data type agent is specified similarly the relation wasassocitedwith is further specified by the attribute prov role operator to indicate that a process component is executing the underlying process and the relation actedonbehalfof is further specified by the attribute prov type delegation to indicate that an aggregate component delegates the execution of enclosed processes to the respective process components based on these three blocks of information the method then builds a prov n conformant expression including the removal of any new line and carriage return characters and sends this record to the modelcontroller who writes into the provn file the base class for aggregate and process components was extended to capture provenance information on the hierarchical structure of the model delegation and association as well as on activities including start and end the base class of process objects was extended to capture relational information such as communication generation usage and derivation individual process classes were extended to capture process specific runtime parameters testing automated fine grained model provenance tracking by the lumass framework the sednetnz model use case sednetnz is an erosion model that estimates the generation and transport of sediment through river networks based on a simplified physical representation of hillslope and channel processes at small sub catchment scale the model provides estimates of long term annual average sediment load and yield sednetnz spatially distributes budgets of fine sediment in the landscape it incorporates landslide gully earthflow surficial and bank erosion as well as flood plain deposition the important forms of soil erosion in new zealand these estimates enable improved targeting of erosion mitigation to the key contributing processes and analysis of the linkages between upstream sediment generation and downstream sediment loading for further information regarding details of the sednetnz model we refer to dymond et al 2016 although sednetnz is a model that continues to be developed and improved it is simultaneously being applied to provide clients e g local government with estimates of sediment yields for catchments throughout new zealand within this dynamic context strategies must be in place to eliminate potential inconsistencies and confusion these include i version control and archiving of the input data sub models and results and ii documentation of the workflow and changes thereof to prevent loss of organisational memory a further challenge relates to the clarification of licencing and copyright restrictions i e the implications of licences associated with input data on derived products these challenges are more likely to be overcome through a comprehensive approach to modelling which needs to i adhere to best practice guidelines for modelling and ii incorporate data provenance in the model development and application sednetnz draws on existing datasets as inputs for various components of the model which are sourced from different organisations within new zealand these include datasets such as the river environment classification the land cover data base nz erosion terrains 15 m digital elevation model such input data can often follow different standards with respect to metadata documentation and can also lack detailed provenance information for the sednetnz application the first step was to include the standardisation and updating of provenance information to incorporate all metadata relevant to further processing and dissemination of results this provenance information was recorded in an xml format which in the case of spatial data follows the schema defined by the attached metadata or a modified version thereof this provenance information can be made available independently of the fine grained model provenance recorded by the lumass modelling framework to test provenance tracking for the sednetnz model we implemented it using the lumass modelling framework and provided provenance information for individual major components of the model i e hillslope erosion bank erosion floodplain deposition and mean erosion per catchment as well as for the entire model itself lumass tracks the hierarchical model structure as well as runtime iteration specific model parameters as used in equations sql expressions and other processing parameters e g filenames the tracked information provides a complete account of what operations were applied to a specific file when and in which order 2 3 visualization of data provenance with proviz given the scale of provenance data produced by pyluc and lumass presented in section 3 and its complex nature a visualization tool was required in order to make this data more meaningful to end users the interactive provstore provenance visualization tools can be useful for helping to understand the structure of the logic behind a model or classification algorithm huynh and moreau 2014 2017 currently four layouts are available sankey wheel hive and gantt diagrams however the utility of the provstore visualization tools diminishes quickly with even moderately complicated fine grained provenance data other efforts to create improved visualization tools can be attributed to macko and seltzer 2011 boyd 2012 borkin et al 2013 karsai et al 2016 kohwalter et al 2016 stitz et al 2016 and oliveira et al 2017 oliveira et al 2017 reviewed the most prominent visualization tools that are compatible with the prov model these include the inprov tool which is a radial format offering a range of navigation features boyd 2012 however it does not appear to have been further developed and is not readily accessible prov o viz hoekstra and growth 2014 uses a sankey layout much like that available on provstore karsai et al 2016 uses a node layout that provides a clustering function however the grouping of nodes is currently a manual process as is the creation of meaningful names more recent efforts include avocado stitz et al 2016 and prov viewer kohwalter et al 2016 avocado was designed for data driven biomedical research and caters in particular for workflow derived data provenance of strong hierarchical nature prov viewer has useful functionality such as meaningful shapes and colours of vertices and edges a vertex collapse feature a temporal vertex filter and an automatic collapse feature that summarizes the provenance information through reversible deduplication the visionary framework oliveira et al 2017 builds on prov viewer suggesting that analysis of provenance graphs using complex networks techniques can generate metrics to further understanding and use of data provenance for the purposes of environmental models it is questionable whether such analysis of provenance data can help the model user develop a greater appreciation of the intricacies of the model our focus is therefore on creating an optimized interactive visualization of fine grained data provenance for environmental modelling to this end we created provis a web based tool utilising the d3 javascript library that is capable of visualising very large provenance files in a way that makes them easier for modellers and researchers to understand the intention is for the tool to be extended in the future to increasing its utility for visualising provenance data of environmental models we placed considerable emphasis on improving the user experience and developed a tool that supports two visualizations of prov n files two important requirements were providing i choice of visualization and ii interactive features to improve scalability for large and complex prov n files this is achieved by representing the prov n document as a directed graph and integrating interactive features activities agents entities form the nodes of the graphs the relations between nodes actedonbehalfof wasgeneratedby wasassociatedwith form the edges the increased scalability achieved in these visualizations allows the user to explore the provenance information in an interactive manner and thus reduces the perceived complexity of the data representation 1 standard interactive graph the first of the visualizations uses the same style of visualization as the standard export graphic option on provstore but is interactively filterable an index of nodes and edges grouped by type allow items in the index to be selected in the same way nodes on the graph are selectable selecting an edge will select the output of the edge on selection the item is displayed along with its neighbours within a secondary visualization in the primary visualization as is the case in the graphics on provstore nodes that are neighbours can be drawn far apart from one another which is an impediment to viewing and understanding the relationships between elements of the model therefore and unlike provstore a secondary neighbourhood visualization renders the selected nodes and its neighbours in a local visualization to improve understanding of relationships between the activities agents and entities selecting elements in the secondary graphic or from the index will cause the primary graphic to zoom in on the selected thing nodes in the graph can also be filtered by name 2 the force directed graph visualization the second of the visualizations employs a physics based analogy whereby nodes e g activities and agents repel each other like charged particles but are attracted via their edges i e relationships and a central force ultimately attracts all nodes to the centre classes of nodes and edges can be excluded from the visualization removing nodes or edges can increase the number of components connected sets of nodes in the graph components of the graph are rendered in separate force simulation layouts each component is drawn inside a circle the visualization places a single entity that is considered the end product of a component into the centre of the circle the end product is estimated by calculating a topological graph if the graph is not acyclic then the topological sort of the subgraph containing only entities and activities is used each node n is positioned by its index in the topological sort t n the final product is the node in the component with lowest t n i e the first node in the sorted list of nodes this physics like visualization allows a large amount of data to be represented on screen moreover nodes agents activities entities can be selected to show a secondary node neighbourhood visualization of the selected element and its neighbours the secondary visualization places the selected node in the centre with inputs to the left and outputs to the right as for the first visualization there is an index of nodes grouped by type which are selectable and nodes can also be filtered by name 3 results 3 1 data provenance of lunz as produced by pyluc to increase science transparency effectively through application of data provenance the representation of the generated provenance information must be both user friendly and promote comprehension of the model itself land use classifications can be conceptually complicated but the mechanics of accomplishing this are relatively simple when compared to models typically implemented in lumass additionally the provenance information for a land use classification is intended for users of the generated dataset central and local government as well as technical researchers from a range of disciplines lumass provenance information however is intended to be used by modellers seeking to replicate or debug a complex model or part thereof and as such there is much more information to capture as a result the level of complexity of provenance information generated by pyluc shown in this subsection differs to that of lumass shown in subsection 3 2 as prov n notation is not easily human readable the provenance generated by pyluc for the lunz luc has been visualised by the proviz tool for this section following on from fig 1 which illustrates the conceptual framework for the pyluc application fig 3 is a standard representation of the provenance information resulting from a run of the lunz land use classification model the primary upper pane shows a graph of the entire provenance file the secondary lower pane is a gateway to explore the model s provenance by selecting parts of the upper pane in doing so the user i obtains in depth understanding of the model s processes ii verifies attribution of sources and entities and iii views specifics related to the generation of the model s outputs the lower panel in fig 3 shows the neighbourhood of one of the final activities in the pyluc model rules lunz final the algorithm used to define the final land use classes for new zealand two agents wasassociatedwith the activity lr jollyb and lr mandersona lr landcare research by selecting the agent lr jollyb the neighbourhood view shifts to show i the associations of the agent to all activities ii the attributes defining the agent which list the foaf name as ben jolly and prov type as prov person and iii that ben jolly actedonbehalfof an organisation named landcare research returning to the activity rules lunz final the seven entities used to generate resultslunz final can be viewed selecting the first of these provides the local neighbourhood of the entity results final orch which is attributed to the prov person lr mandersona and generated by the activity rules n5 orchard types by navigating to this next activity the 13 entities used within the algorithm are viewable in contrast to the lumass implementation pyluc provenance information does not contain the actual algorithm code the intent is for the provenance data to be consumed alongside the technical documentation automatically generated at the same time which includes the code and is perhaps a more suitable format in the context of pyluc to ensure complete reproducibility is met the pyluc definition file can be packaged alongside the provenance information and technical documentation 3 2 data provenance of sednetnz as produced by lumass fig 4 which illustrates the force based visualization tool developed specifically for large complex models is a representation of the components of the sednetnz model dymond et al 2016 in graph theory a component is a set of nodes which are linked together in some way this visualization draws each component of a prov n json document inside separate circles each activity entity or agent contained in sednetnz is represented by a small coloured node prov n defines different relations between these types objects relations are represented by the lines connecting the things in the relation in general the arrows point from the future to the past for example a grey arrow represents the relation wasderivedfrom in prov n meaning that entity a2 wasderivedfrom entity a1 sometimes in a prov n document a relation may refer to an activity entity or agent that has not been defined in the document in this visualization these things which have been referred to but not defined are drawn as a white circle with a coloured outline the panel on the right of fig 4 displays an activity nm mapalgebra9 update 1 of the sednetnz model in its immediate neighbourhood the attributes of the activity are listed in the associated table this activity was informed by four other activities imagereader this simply means that the activity is a function that reads images hence the activity nm mapalgebra9 update 1 reads the four images listed in its attribute table each image representing the sediment yield of each of the four erosion processes landslide surficial earthflow and gully erosion the mapexpression in the attribute table specifies the function which is the sum of the sediment yields of the four erosion processes an agent prov softwareagent wasassociatedwith the activity nm mapalgebra9 which specifies the number of iterations for the activity and the timelevel which defines the model hierarchy in lumass the generation of the new dataset resulting from this activity occurs in a second activity i e nm imagewriter11 upate 1 which wasinformedby nm mapalgebra9 update 1 by selecting this next activity the user can see that the entity img totalhillerosion img wasgeneratedby the image writer the visualization thus enables the user to navigate through a specific model run by viewing the relations between components and the respective attributes the data provenance provides detailed information of what was done how by which in this sednetnz example the agent is a prov n softwareagent which in contrast to pyluc where agents are persons and organisations who though the start and end times of processing pipelines and individual activities as well as the time of usage generation and revision of data are recorded in the prov n documents these attributes are excluded from representation to create a more user friendly experience 4 discussion transparency openness and reproducibility are readily recognized as vital features of science mcnutt 2014 miguel et al 2014 cohen boulakia et al 2017 croucher et al 2017 horsburgh athanasiadis 2018 brinckman et al 2019 it is critical that these features become an integral component of our science so that the credibility and legitimacy of science is upheld this requires that all data methods and results have a documented audit trail that tracks their origin and provenance the essence of data provenance application is to facilitate the process of understanding what a model does i e to increase transparency in science modelling data provenance is not primarily aimed at reproducibility though it does support reproducible science by increasing confidence in datasets we applied data provenance tracking to two different modelling frameworks each with a varied interpretation of the w3c prov standard new zealand s sediment budget model sednetnz and in a new tool pyluc developed to help generate land use classifications in both cases these frameworks are used to create data used by other scientists and policy makers where the provision of provenance information would be of great value once data provenance is systematically and comprehensively recorded the requirements of both users and owners of the data need to be considered in terms of the level of granularity of provenance information to be provided to different users for both sednetnz and pyluc the client is likely to be interested in the metadata describing the various model outputs the lineage or source of data inputs ownership and copyright licencing terms and conditions fine grain data provenance relating to the detailed processing of the data is likely to be more relevant to the creator and owner of the data and to other scientists to allow the replication of the workflow mechanisms that allow the extraction and integration of provenance information at different levels of granularity workflow and value level to support the information needs of different users is a desirable goal integration of data provenance tracking into the pyluc framework was relatively straightforward compared to retrospective implementation in lumass this is primarily due to design decisions made very early in the project with provenance tracking being part of the design pyluc follows an object oriented software paradigm where the overall operation input datasets rules and processing engine are represented by classes it was possible to add additional properties and methods to these classes to cater for provenance tracking and generation as well as a provenance class to collate the information in this case the value gained from implementing data provenance greatly surpassed the effort required to integrate it in lumass the provenance tracking was retro fitted to an existing framework this required significant resourcing and involved mapping the prov dm to the existing hierarchy and structure of the modelling framework because of the fine grained tracking of provenance information changes were required at many different levels of the code for comprehensive implementation the work required to retrofit provenance tracking to existing environmental modelling frameworks may be a disincentive to researchers however once provenance tracking is integrated into a framework users of that framework can apply it to any existing or new model different users will require the provenance information in different formats not all users will be familiar with working with xml rdf or json files for example provenance information needs to be provided in human readable forms but also increasingly in a form that supports machine to machine sharing to facilitate cross platform sharing of provenance information it is important to use an appropriate standardised model a number of models have been suggested with the prov family of standards w3c 2013a having the greatest uptake to date in both pyluc and lumass we were able to successfully gather provenance information create a prov n file and use the same data structure and encoding in our own interactive visualization tool however as mentioned above development of provenance tracking in each modelling framework was done independently w3c prov dm is a generic data model that is designed to support application specific extensions jiang et al 2018 and it transpired that the two frameworks interpreted the w3c standard slightly differently according to the purpose of the framework in part this reflects the fact that prov and more specifically prov dm is a generic data model able to document the provenance of a piece of data or a thing i e any piece of data or thing a generic provenance ontology needs to be interpreted differently to capture the diverse complex provenance requirements in specific domains for example a domain independent provenance question might ask for all the steps taken in a workflow that produced a named dataset a domain specific provenance question might ask for all the values of calibration parameters that have been used by a land use optimisation model applied over the upper sub catchments of a specific geographic area as a result we see the emergence of domain specific provenance applications that employ prov dm and use prov o as an upper level reference ontology ma et al 2014 stocker 2015 cuevas vicenttín 2015 closa et al 2017 valdez et al 2018 there is also scope within prov dm for far more granular and explicit representations using the various subclasses qualified properties and expanded terms as required the two frameworks introduced here adopted a different interpretation of prov dm to suit their respective purpose for example lumass used the software agent sub class to represent the hierarchical structure of lumass models whereas pyluc used agents to represent human agents and organisations even though both presented prov dm applications are compatible and could be extended to use the same conceptualisation for example lumass could use a higher level of agents to represent human agents and organisations to which data can be attributed in terms of activities processes in lumass are equivalent to rules in pyluc however compatibility of fine grained model provenance in the context of an environmental modelling workflow may not be given for a different set of models this raises the more general question of how incompatible model provenance could be integrated into an overarching workflow provenance of environmental modelling that can still be visualised and queried in a meaningful way this research question was initially addressed by chirigati and freire 2012 and in the context of workflow management systems by missier et al 2010 and liew et al 2016 while further research must focus on compatibility of different granularities of provenance for environmental modelling a way forward may be given through provenance bundles w3c 2013c which can support provenance of provenance and provenance aggregators are capable of merging bundles to maximise the value of provenance users need access to be able to comprehend and understand provenance data we found that the provenance data that are produced can be large and complex this is in line with the findings of other researchers e g macko and seltzer 2011 borkin et al 2013 and curcin 2017 as mentioned above it seems likely that provenance data need to be filtered and presented to users in different ways depending on their information needs as we demonstrated users can be assisted by tools that support querying browsing filtering and viewing of the provenance data the visualization tools created and presented here demonstrate a useable format for provenance representation as the user can interactively determine the level of detail viewed research into the visualization of linked data ontologies suggest well designed visualizations combined with appropriate interaction techniques enable users to navigate through and make sense of large and complex datasets schneiderman 1996 valsecchi et al 2015 ivanova et al 2016 visual exploration of provenance is likely to equally benefit users helping them understand both how data have been created and the interconnectivity of data most of the previously developed visualization tools are directed at workflow provenance which is generally not as rich in information as fine grained data provenance further interdisciplinary research and development is required to improve the usability of the tools for different types of users rigorous usability testing was beyond the scope of this study to determine how usability can best be achieved will require a deeper exploration of the nature of information needed by the users further development of the tools is also needed to create meaningful labelling of items represented in the provenance visualizations this could be achieved by using userids as names in the provenance log rather than the unique identifiers from the model run that are currently used additionally if a model such as sednetnz has multiple sub models the utility of the visualizations would be improved by having labelled clusters in the visualization each representing a sub model the user would thus be able to navigate to and explore a specific area of interest within the provenance graph currently the filtering option allows a user to search for items within the model meaningful labelling is however a pre requisite to assure functionality though increasing transparency improves the credibility of science it also brings potential risks for the researcher and other agents involved in the modelling e g the providers of the input data those involved in some way are made vulnerable to criticism when errors are exposed even if there was no ill intent at the core of environmental research is the endeavour to increase understanding of environmental systems and processes therefore greater openness at the risk of criticism is better for science and the scientist should embrace the opportunity for checks and balances however opening the science to allow competitors to view details of a model can be detrimental in what is a highly competitive science market mellor et al 2018 though provenance tracking can include attribution intellectual property rights associated with the development and maintenance of models must also be protected 5 conclusions provenance information is increasingly required in a world where the number and size of land resource datasets are continuously increasing provenance information greatly helps the management of land resource data it can be used to discover data based on how they were created by whom and why it helps understand the sources and transformations that were applied to data it can also be used for monitoring the usage and production of data by workflows to replay a workflow in order to regenerate the data from its original source and to provide a context for reusing data published to the community in addition we have found provenance information to be useful for model verification and debugging purposes the application of data provenance was showcased for two different frameworks to illustrate its value and usability for land resource related environmental modelling scenarios the first was a user led generation of a land use classification with pyluc the second demonstrated application within the existing framework of lumass using new zealand s sediment budget model sednetnz in both cases data provenance was generated and made available for archiving querying and visualization by different users of equal importance to the generation of data provenance is a suitable visualization of the provenance information to allow it to be explored and understood by users detailed provenance tracking in the two different applications resulted in a complexity of information which poses a challenge for the user who wants to engage with it the visualization tools developed to this end provide an interactive user friendly means to explore and understand the provenance information of the model outputs notwithstanding some of the issues raised in the discussion the collection and provision of provenance information are increasingly recognized as necessary we argue that best land resource data management and sharing practice should include data provenance only then can the increasing demand for the quality and integrity of environmental model outputs be met and evidenced based decision making in policy and practice be truly informed acknowledgements the research presented in this contribution was supported by the new zealand ministry of business innovation and employment mbie research project innovative data analysis contract number prop 38356 etr lcr we thank dr tom etherington for commenting on an earlier draft of this manuscript 
26202,demand is increasing for greater transparency of the science underpinning decision making processes in land resource management to illustrate how the application of fine grained data provenance can increase the credibility and transparency of scientific methods and outputs we implement provenance tracking for two different modelling frameworks pyluc and lumass and present results from example models pyluc is a python based framework for generating spatial land use classification data with automatically generated technical documentation lumass is a spatial modelling and optimisation framework within which new zealand s sediment budget model sednetnz is implemented in both cases detailed provenance tracking resulted in a complexity of information which necessitated the development of an interactive data provenance visualization tool to help science producers and users explore verify and understand model outputs we argue that best data management and sharing practice should include fine grained data provenance to meet demands for the quality and integrity of science based data and information keywords fine grained data provenance provenance visualization transparent science environmental modelling lumass pyluc software availability python land use classification framework pyluc software pyluc developer en jolly jollyb landcareresearch co nz year first available 2019 hardware pc mac hpc 4 gb ram 10 gb disk space software required python 3 5 with gdal environment program language python program size 1 mb licence gnu general public licence v3 gpl availability source code download https bitbucket org landcareresearch pyluc land use management support system lumass software lumass developer alexander herzig herziga landcareresearch co nz year first available 2012 hardware pc notebook 4 gb ram 1 gb disk space software required windows 64bit or linux program language c program size 30 mb source 312 mb windows 64bit installed licence gnu general public licence v3 gpl availability source code windows 64 bit binaries download https bitbucket org landcareresearch lumass provenance visualization tool provis d3 software provis d3 developer tomas burleigh burleight landcareresearch co nz year first available 2019 hardware pc mac software required modern web browser edge chrome firefox program language javascript licence gnu general public licence v3 gpl availability web site access http vizdemo landcareresearch co nz provis d3 index html 1 introduction 1 1 background the size of environmental datasets is continuously increasing in parallel with technologies that allow data to be collected at greater resolutions in time and space as a result the ability to manage these data and perform advanced scientific analysis is becoming more complex simmhan et al 2005 moreover data sharing among and across scientific communities is becoming the norm nested within this context environmental models are developed and used to help inform decision making in policy however it has been suggested that science based knowledge can be flawed by the user being unaware of the source process and context behind the information being used kattge et al 2014 mittelstrass 2010 spiekermann et al 2015 weichselgartner obersteiner 2002 furthermore organisations are increasingly required to justify why they made certain decisions or provided certain advice thus the demand for more transparent science has increased duarte et al 2015 browman 2016 özkundakci et al 2018 brinckman et al 2019 indeed larcombe and ridd 2018 proposed governments establish independent organisations to undertake quality reviews and audits of important scientific results that underpin major government spending decisions which were to include a system of guaranteed and organised technical debate to rigorously test for scientific deficiencies therefore tools need to be developed that can help improve understanding on the source process and context behind the information within this context data provenance is aimed at improving the process of knowledge generation and sharing data provenance tracks and documents information about entities activities and people involved in producing a piece of data belhajjame et al 2013 and thus has the potential to increase the saliency credibility legitimacy and reproducibility of scientific methods and outputs a clear advantage of data provenance and increasingly a requirement for datasets within the scientific community that inform policy is the increased transparency it provides the australian national data service ands for example understands data provenance as the instrument to ensure that the data that leads to a scientific finding or publication can be trusted and verified and that data can be re purposed and reused trustfully in more research treloar and wu 2016 p 184 a growing body of literature exists on data provenance and various proposals of definitions terminologies and ontologies can be found e g simmhan et al 2005 moreau 2010 some growing out of an interest in data lineage e g lanter 1991 due to the lack of consensus in the scientific community on the meaning of provenance and its implementation most researchers have limited data provenance to only a few aspects of provenance generally encompassing the origin lineage or source of the data buneman et al 2001 ram and liu 2008 2009 liu and ram 2011 and recorded as part of the metadata about a dataset provenance can be captured manually by editing associated metadata after some process has been executed automated capture of provenance information helps address some of the problems associated with manual capture e g incomplete records though the applicability ease of implementation and usefulness of this method depends substantially on how a dataset is created in the context of their work on provenance systems the world wide web consortium w3c provenance incubator group broadened the scope of data provenance defining provenance of a resource as a record that describes entities and processes involved in producing and delivering or otherwise influencing that resource provenance provides a critical foundation for assessing authenticity enabling trust and allowing reproducibility provenance assertions are a form of contextual metadata and can themselves become important records with their own provenance w3c 2010 this definition clearly includes more than just the lineage or source of data it also encompasses contextual information of the processes that generated that data to have value provenance thus must describe who or what was involved in the creation where existing data were sourced and how the data were processed simmhan et al 2005 provide a comprehensive account of how provenance can be used to support a range of applications e g audit trail attribution w3c 2010 describe key dimensions of data provenance grouped according to the requirements for content management and use the requirements were used to inform the design of the data provenance system which can be defined as an environment that supports a computer based representation of provenance that can be queried and reasoned over using specified technologies moreau et al 2008 provenance systems encompass the capture storage management and dissemination of provenance information provenance needs to be readily available for consumers of a given dataset and it is paramount that those consumers can trust the authenticity and completeness of the information two major approaches to representing provenance according to simmhan et al 2005 are annotations and inversion the annotation approach collects metadata about input data sources processes and any additional relevant information at each step in the process of creating and editing a data file the inversion method instead inverts the functions and or queries used to generate a dataset to identify the source data a primary advantage of the inversion method is efficient storage whereas the annotation approach is of far greater flexibility in the level of provenance information that may be captured a differentiation in terms of granularity can be helpful to elucidate what is captured by data provenance tracking course grained data provenance also known as workflow provenance is defined at a higher granularity level as it captures the relationships between different activities and items in a model but excludes the complete derivation of an item so that reproducibility cannot be achieved sheikh et al 2018 fine grained data provenance is applied at the level of the value and documents the association between inputs outputs and affiliated processes sheikh et al 2018 fine grained data provenance can increase the transparency in modelling as it is intrinsically tied to each model run and provides a comprehensive account of what was done and how it was done for a specific modelling case to facilitate cross platform sharing of provenance information it is important to use an appropriate standardised data model a number of standardised models exist some more domain specific than others the standard lineage model in the iso 19115 geographic information metadata as specified by the technical committee 211 of the international organisation for standardisation iso can be used to model geospatial data provenance di et al 2013 however this model is quite constrained even within the geospatial community closa et al 2017 the w3c produced the prov family of documents w3c 2013a outlining a generic model for representing provenance information using common encoding formats such as xml w3c 2013b rdf and json jiang et al 2018 compared the iso 19115 to w3c prov and evaluated their compatibility concluding that the main benefit of w3c prov is its causality based design which allows rich semantics to be asserted on the relations between things though w3c prov currently needs more domain enrichment jiang et al 2018 it is an effective way forward for implementing provenance tracking tools already exist for creating editing and visualising prov information rdatatracker lerner and boose 2014 outputs prov json files and sumatra davison 2012 has the outputting of prov xml in its development roadmap prov data can be visualised in a visualization tool called prov svg car 2017 1 2 provenance environmental modelling and land resource data land resource data provides scientists policy makers regulators and land owners with the basis to assess the state and trend of the land resource monitor the impact of policies and regulation and respond effectively through land management decisions environmental modelling is often undertaken to investigate changing resources e g land use change and its environmental impacts information resulting from environmental models thus underpins decision making processes by both business and government agencies it is in the interest of both environmental modellers and decision makers in policy and practice to increase the transparency surrounding these processes given that transparency is the primary quality of data provenance it can serve to meet the demand for credible science and sustained legitimacy in policy when deciding which information is to be captured it is essential to consider the possible uses and users of data provenance from the policy perspective best practice requires policy documents to i provide evidence to support their conclusions and recommendations ii ensure there is an easy audit trail to allow decision makers to understand the data the model used to generate the data and assumptions underlying conclusions and recommendations and iii deliver sufficient information to support any later re evaluation or challenge supplying provenance information with land resource data that underpins policy therefore contributes to best practice from an environmental modelling standpoint modellers need to ensure that i the purpose of the model is understood ii the modelling process is transparent and iii best practice guidelines for modelling are followed özkundakci et al 2018 jakeman et al 2006 transparency in modelling encourages use of best practice guidelines to strengthen reliability and increase the credibility of the science until now recommendations for improving reproducible science have been made for social nosek et al 2015 and environmental science croucher et al 2017 tonitto et al 2018 these generally include version control of code and associated data and storing data in a repository which can be made publicly available for citation while these measures support reproducible science the provision of code cannot guarantee a replication of results as parameters and data inputs can change from one model run to the next the demands of transparent environmental modelling require more details related to the individual elements within the model which is more likely guaranteed by application of fine grained data provenance within modelling frameworks users often need to determine whether the results are correct which requires full transparency of the model the resulting documentation of fine grained data provenance therefore also serves model verification 1 3 fine grained provenance tracking to improve transparent modelling to improve reproducibility in environmental science as well as its credibility it is important that the precise conditions and context of models are retained and that transparent science is not constrained to the mere long term preservation of data in an accessible form as horsburgh athanasiadis 2018 correctly recognized reproducibility includes tracking and documentation of the algorithms used the input parameters software versions involved the intermediate data pre processing and transformation steps and the provision of precise information to guarantee reproduction of exact results using the same data the recorded details of a complete workflow are frequently inadequate for example some random and temporarily generated elements can be involved within a dynamic model making reproducibility difficult horsburgh athanasiadis 2018 conclude that since it is unlikely that all scientific disciplines will settle on standards for data collection management and analysis better methods are needed for capturing scientific workflows to enhance the reproducibility of data intensive analyses and modelling we argue that explicit fine grained provenance tracking is a method that can help meet the demands of transparent and reproducible science until now data provenance applications in the context of environmental modelling are limited in both frequency and granularity zhang et al 2017 demonstrated provenance tracking for a relatively simple simulation of watershed runoff iturbide et al 2019 include a provenance metadata model metaclip to describe the data workflow of climate products essawy et al 2018 developed a workflow documentation including retrospective provenance to improve the reproducibility of computational environmental models however these studies are limited to documenting workflow provenance which captures association among different processes but excludes the intricacies of algorithms used the key components which determine the output sheikh et al 2018 we implemented fine grained data provenance using two different modelling frameworks each with a different application here we define a framework as the underlying software and architecture as well as the module and model system argent et al 2006 development was carried out independently using the prov model w3c prov is a generic data model that is designed to support application specific extensions jiang et al 2018 the first spatial modelling framework pyluc was developed to help generate land use classifications lucs with self documenting definitions that could be processed to create both the spatial dataset the luc and supporting documentation including fine grained data provenance the requirement was for clients to have more transparency with respect to how the classified land use data was generated the primary purpose of implementing fine grained provenance tracking within the second spatial modelling framework the land use management support system lumass was to support model development through improved documentation and model verification we compare the different implementations and discuss the achievements and challenges of each to date this is the first attempt at fine grained explicit data provenance tracking in environmental modelling following implementation we examine the feasibility of the implementations cohen boulakia et al 2017 recognized the need for interactive systems and tools able to visualize query or mine provenance information represented using prov standards given the enormity of the provenance information generated macko and seltzer 2011 designing such tools is very challenging we present preliminary results of two interactive visualization tools that aim to deal with the complexity of the graphic representation of data provenance using optimized algorithms and improved interactivity achieved through a secondary neighbourhood visualization the tools were tested using the provenance data generated by pyluc and lumass we then evaluate the utility of the provenance information produced by the two frameworks and discuss the usability of their representation using the interactive visualization systems 2 methods 2 1 the pyluc land use classification tool provenance by design pyluc was developed to support the development of spatial land use classifications luc land use can be defined as the activities or socio economic functions for which land is used the allocation and use of land affect all aspects of a society s overall well being economic environmental social and cultural and quality of life it differs from land cover which describes the physical state of the land leslie 2004 land use data provide information on the function and purpose for which land is currently used and when tracked over time how land use changes young 1998 knowledge of land use supports analysis and management of land vegetation water resources and quality and the maintenance of biodiversity the objective of a land use classification is to provide a framework to guide the collection of data and the creation of effective databases to ensure comparability and compatibility gong et al 2009 pyluc is a geospatial data processing framework written using the python language aimed at complementing standard geographic information system gis tools to generate spatial land use classifications lucs but with increased levels of transparency and repeatability it was developed to fill a void where researchers developing land use classifications needed to be able to provide better provenance and technical documentation with the classification output but struggled to find the right tools to do so a typical user case for scientists is to develop a luc using familiar gis tools then transfer the finalised logic to a pyluc definition script to produce the definitive dataset ready for publication or delivery to a client often local or central government to help inform policy an essential output of pyluc is the automated documentation and provenance information generated during a classification run the intention for the provenance information here is to facilitate the process of understanding what steps a classification algorithm took exactly what data it ingested where who produced that data who wrote each part of the algorithm what their affiliations were and how everything ties together in order to increase the transparency of the luc product data provenance is not primarily aimed at reproducibility though it does support reproducible science by increasing confidence in datasets in the initial version of this application we restricted input data sources to those hosted on instances of the koordinates 1 1 https koordinates com geospatial data warehouse platform this was done for three main reasons 1 many of new zealand s authoritative geospatial data sources exist on this platform 2 to encourage the use of original primary sources with any modifications explicitly baked into the luc definition and 3 because data layers stored on this platform are immutable which ensures all referenced datasets will exist in their original form at any point in the future the last two reasons significantly contribute to the transparency and repeatability of the luc process thus it is possible for a user to recreate a luc with essential technical documentation and provenance data given a pyluc definition script single text file and a koordinates user account with appropriate permissions to access protected data sources this avoids passing on a copy of the source dataset itself which can easily be copied modified and shared a process which could create problems with data licensing and over time can lead to issues with reproducibility pyluc was created from scratch with data provenance in mind as such its internal data model is structured to follow the w3c prov data model prov dm w3c 2013c which revolves around agents activities entities and associated relations as illustrated in fig 1 the general concept is that people and organisations are agents where most people will be affiliated with one or more organisations luc algorithms are defined by a series of classification rules or steps that are represented using prov activities while all data input intermediate output from luc rules and final output are represented by entities pyluc distinguishes variables from datasets in this model using hierarchical entities to denote association this is done to help understand exactly which variables from an input dataset contributed to which rules the model in fig 1 is arranged to show the flow of data from the top down however it may be more intuitive to read from the bottom up by following the arrows starting at the final output entity this wasgeneratedby a rule that used outputs generated by multiple rules that in turn used multiple input vars each of which wasderivedfrom an input dataset each input dataset wasattributedto an organisation each of which had one or more persons who actedonbehalf of them each person may also have actedonbehalf of another person normally used for delegation where the person who runs pyluc may not be the author of a rule each rule wasassociatedwith the author person responsible for writing it with the corresponding output also being attributed to them provenance is delivered by pyluc as a prov n file standard notation for w3c prov model that while not easily human readable can be ingested by other tools for analysis and visualization for example provstore 2017 is an all in one website 2 2 https openprovenance org store for prov information that currently serves as the primary distribution mechanism for pyluc provenance records it provides for uploading read only prov documents as well as exposing a rest api that can be used to directly access the remotely hosted provenance records via third party tools provstore allows the export of provenance records in various formats json turtle xml etc contains several visualization tools and can be treated as a trusted third party source provenance records uploaded to this site are read only and cannot be edited thus given the upload process is an automated component of an automated system viewers can be reasonably confident it has not been manually tampered with to demonstrate the utility of the provenance tracking implemented in the pyluc framework we present the output of a relatively simple luc algorithm called lunz land use new zealand in section 3 this algorithm combines several land cover and land use datasets of varying levels of utility dependent on geographic location or type of information using a set of rules essentially merging several geospatial input layers into one single output layer 2 2 integration of fine grained model provenance documentation into the lumass modelling framework 2 2 1 the lumass spatial modelling and optimisation framework the land use management support system lumass is a free and open source spatial modelling and optimisation framework it aims to support the understanding of landscape ecosystems by environmental modelling jørgensen 1994 powers et al 2011 ausseil et al 2013 and to aide land management decision making by spatial optimisation herzig et al 2013 2016 2018 lumass provides a visual programming environment to enable scientists and gis professionals with little to no programming skills to develop complex dynamic geospatial raster based models operating on large datasets herzig and rutledge 2013 users combine atomic processing components e g for map algebra spatial optimisation or the processing of tabular data to processing pipelines and group these into higher level aggregate components which can be nested to build hierarchies of processing steps the execution order of individual processing steps starts at the highest level of aggregation and is recursively propagated to lower aggregation levels at each aggregation level the order of execution is determined by time levels associated with each component and progresses from higher time levels to lower time levels to enable dynamics and feedbacks individual processing components pipelines and aggregate components can be executed iteratively processing different datasets and or applying different processing parameters at each iteration step herzig 2017 to debug and test a model users can execute individual model components at selected iteration steps and inspect the parameters submitted to individual processing components at a given time step in the user interface however there has been no documentation of this information that is there has been no record of which datasets a model used modified or generated nor which processing components with which parameters were used at which time step to compute a particular result dataset unless special care and discipline is exercised by the modeller to manually record provenance information which is very time consuming at this level a non automated recording is particularly error prone especially in multi scenario modelling exercises the automated documentation of fine grained model provenance not only supports the modeller in these multi scenario modelling exercises but also serves as a reliable record to demonstrate to end users which algorithms were applied with which parameters to which dataset in the following sub sections we present the conceptual application of the w3c prov dm w3c 2013c to track fine grained provenance information for models implemented in the lumass modelling framework we then describe how we retro fitted fine grained model provenance tracking into the lumass source code to produce prov n w3c 2013d encoded provenance records finally we describe our use case to test the automated fine grained tracking of model provenance information implemented in lumass using the sednetnz model implementation 2 2 2 application of the w3c prov data model to track fine grained model provenance in lumass each model component of a lumass model is represented as an agent prov n softwareagent fig 2 and the hierarchical structure of model components is captured by the concept of delegation prov n actedonbehalfof the execution of a lumass model component is represented as a prov n activity that is associated with prov n wasassociatedwith the respective agent properties describing the model structure are assigned to the agents while runtime specific processing parameters equations etc are attributes assigned to the respective activity an agent is responsible for lumass processing pipelines including the flow of data between them is captured as information exchange between activities prov n wasinformedby input and output data of a model are represented as prov n entities that are used prov n generated prov n wasgeneratedby and revised prov n wasderivedfrom by activities start and end times of processing pipelines and individual activities as well as the time of usage generation and revision of data is recorded 2 2 3 implementation of fine grained model provenance tracking the execution of lumass models is internally governed by a modelcontroller object it manages the register of model components and model wide settings such as whether to track provenance information or not by default lumass records model provenance each time a model is executed and saves the information using prov n w3c 2013d notation in a text file provn in the user specified lumass working directory files are named after the model name and the date and time the model run was started e g hillslopeerosion 20180331t123039 provn to ensure cross platform usability dashes and colons are removed from the iso 8601 time stamp notation at the source code level provenance tracking was retro fitted to the modelling framework by the following adaptations the modelcontroller class was extended to i create a new model provenance file each time its associated model is executed ii receive provenance records from the framework objects while the model is running and append them to the provenance file iii collect optional prov n attribute value pairs for a given component or process object iv keep track of the number of revisions of each entity and v finalise the provenance file and close it since individual model component and process objects are unaware of the structure of the model and which dataset has been processed by which other processes before writing a prov n expression out the model controller filters potentially duplicate entries and adjusts entity identifiers to reflect the correct number of revisions a dataset has undergone the lumass logging framework was extended by the logprovn method to communicate provenance information among the framework objects and to facilitate the generation of prov n conformant provenance records nmlogger logprovn const nmprovconcept concept const qstringlist args qstringlist attr thereby concept is an enumeration type and denotes one of the prov dm data types or relations used by lumass i e entity activity generation usage communication start end revision agent attribution association or delegation the argument args contains the list of required model component or process object identifiers representing the required elements of the particular prov n expression for the given data type or relation e g entity e1 or wasderivedfrom e2 e1 the argument attr contains a list of optional attribute value pairs that are used to further specify the prov n data type or relation and to provide additional information to describe the data type or relation for example the data type agent exclusively refers to softwareagents hence the logprovn method always inserts the attribute prov type prov softwareagent at the front of the list of attributes when the data type agent is specified similarly the relation wasassocitedwith is further specified by the attribute prov role operator to indicate that a process component is executing the underlying process and the relation actedonbehalfof is further specified by the attribute prov type delegation to indicate that an aggregate component delegates the execution of enclosed processes to the respective process components based on these three blocks of information the method then builds a prov n conformant expression including the removal of any new line and carriage return characters and sends this record to the modelcontroller who writes into the provn file the base class for aggregate and process components was extended to capture provenance information on the hierarchical structure of the model delegation and association as well as on activities including start and end the base class of process objects was extended to capture relational information such as communication generation usage and derivation individual process classes were extended to capture process specific runtime parameters testing automated fine grained model provenance tracking by the lumass framework the sednetnz model use case sednetnz is an erosion model that estimates the generation and transport of sediment through river networks based on a simplified physical representation of hillslope and channel processes at small sub catchment scale the model provides estimates of long term annual average sediment load and yield sednetnz spatially distributes budgets of fine sediment in the landscape it incorporates landslide gully earthflow surficial and bank erosion as well as flood plain deposition the important forms of soil erosion in new zealand these estimates enable improved targeting of erosion mitigation to the key contributing processes and analysis of the linkages between upstream sediment generation and downstream sediment loading for further information regarding details of the sednetnz model we refer to dymond et al 2016 although sednetnz is a model that continues to be developed and improved it is simultaneously being applied to provide clients e g local government with estimates of sediment yields for catchments throughout new zealand within this dynamic context strategies must be in place to eliminate potential inconsistencies and confusion these include i version control and archiving of the input data sub models and results and ii documentation of the workflow and changes thereof to prevent loss of organisational memory a further challenge relates to the clarification of licencing and copyright restrictions i e the implications of licences associated with input data on derived products these challenges are more likely to be overcome through a comprehensive approach to modelling which needs to i adhere to best practice guidelines for modelling and ii incorporate data provenance in the model development and application sednetnz draws on existing datasets as inputs for various components of the model which are sourced from different organisations within new zealand these include datasets such as the river environment classification the land cover data base nz erosion terrains 15 m digital elevation model such input data can often follow different standards with respect to metadata documentation and can also lack detailed provenance information for the sednetnz application the first step was to include the standardisation and updating of provenance information to incorporate all metadata relevant to further processing and dissemination of results this provenance information was recorded in an xml format which in the case of spatial data follows the schema defined by the attached metadata or a modified version thereof this provenance information can be made available independently of the fine grained model provenance recorded by the lumass modelling framework to test provenance tracking for the sednetnz model we implemented it using the lumass modelling framework and provided provenance information for individual major components of the model i e hillslope erosion bank erosion floodplain deposition and mean erosion per catchment as well as for the entire model itself lumass tracks the hierarchical model structure as well as runtime iteration specific model parameters as used in equations sql expressions and other processing parameters e g filenames the tracked information provides a complete account of what operations were applied to a specific file when and in which order 2 3 visualization of data provenance with proviz given the scale of provenance data produced by pyluc and lumass presented in section 3 and its complex nature a visualization tool was required in order to make this data more meaningful to end users the interactive provstore provenance visualization tools can be useful for helping to understand the structure of the logic behind a model or classification algorithm huynh and moreau 2014 2017 currently four layouts are available sankey wheel hive and gantt diagrams however the utility of the provstore visualization tools diminishes quickly with even moderately complicated fine grained provenance data other efforts to create improved visualization tools can be attributed to macko and seltzer 2011 boyd 2012 borkin et al 2013 karsai et al 2016 kohwalter et al 2016 stitz et al 2016 and oliveira et al 2017 oliveira et al 2017 reviewed the most prominent visualization tools that are compatible with the prov model these include the inprov tool which is a radial format offering a range of navigation features boyd 2012 however it does not appear to have been further developed and is not readily accessible prov o viz hoekstra and growth 2014 uses a sankey layout much like that available on provstore karsai et al 2016 uses a node layout that provides a clustering function however the grouping of nodes is currently a manual process as is the creation of meaningful names more recent efforts include avocado stitz et al 2016 and prov viewer kohwalter et al 2016 avocado was designed for data driven biomedical research and caters in particular for workflow derived data provenance of strong hierarchical nature prov viewer has useful functionality such as meaningful shapes and colours of vertices and edges a vertex collapse feature a temporal vertex filter and an automatic collapse feature that summarizes the provenance information through reversible deduplication the visionary framework oliveira et al 2017 builds on prov viewer suggesting that analysis of provenance graphs using complex networks techniques can generate metrics to further understanding and use of data provenance for the purposes of environmental models it is questionable whether such analysis of provenance data can help the model user develop a greater appreciation of the intricacies of the model our focus is therefore on creating an optimized interactive visualization of fine grained data provenance for environmental modelling to this end we created provis a web based tool utilising the d3 javascript library that is capable of visualising very large provenance files in a way that makes them easier for modellers and researchers to understand the intention is for the tool to be extended in the future to increasing its utility for visualising provenance data of environmental models we placed considerable emphasis on improving the user experience and developed a tool that supports two visualizations of prov n files two important requirements were providing i choice of visualization and ii interactive features to improve scalability for large and complex prov n files this is achieved by representing the prov n document as a directed graph and integrating interactive features activities agents entities form the nodes of the graphs the relations between nodes actedonbehalfof wasgeneratedby wasassociatedwith form the edges the increased scalability achieved in these visualizations allows the user to explore the provenance information in an interactive manner and thus reduces the perceived complexity of the data representation 1 standard interactive graph the first of the visualizations uses the same style of visualization as the standard export graphic option on provstore but is interactively filterable an index of nodes and edges grouped by type allow items in the index to be selected in the same way nodes on the graph are selectable selecting an edge will select the output of the edge on selection the item is displayed along with its neighbours within a secondary visualization in the primary visualization as is the case in the graphics on provstore nodes that are neighbours can be drawn far apart from one another which is an impediment to viewing and understanding the relationships between elements of the model therefore and unlike provstore a secondary neighbourhood visualization renders the selected nodes and its neighbours in a local visualization to improve understanding of relationships between the activities agents and entities selecting elements in the secondary graphic or from the index will cause the primary graphic to zoom in on the selected thing nodes in the graph can also be filtered by name 2 the force directed graph visualization the second of the visualizations employs a physics based analogy whereby nodes e g activities and agents repel each other like charged particles but are attracted via their edges i e relationships and a central force ultimately attracts all nodes to the centre classes of nodes and edges can be excluded from the visualization removing nodes or edges can increase the number of components connected sets of nodes in the graph components of the graph are rendered in separate force simulation layouts each component is drawn inside a circle the visualization places a single entity that is considered the end product of a component into the centre of the circle the end product is estimated by calculating a topological graph if the graph is not acyclic then the topological sort of the subgraph containing only entities and activities is used each node n is positioned by its index in the topological sort t n the final product is the node in the component with lowest t n i e the first node in the sorted list of nodes this physics like visualization allows a large amount of data to be represented on screen moreover nodes agents activities entities can be selected to show a secondary node neighbourhood visualization of the selected element and its neighbours the secondary visualization places the selected node in the centre with inputs to the left and outputs to the right as for the first visualization there is an index of nodes grouped by type which are selectable and nodes can also be filtered by name 3 results 3 1 data provenance of lunz as produced by pyluc to increase science transparency effectively through application of data provenance the representation of the generated provenance information must be both user friendly and promote comprehension of the model itself land use classifications can be conceptually complicated but the mechanics of accomplishing this are relatively simple when compared to models typically implemented in lumass additionally the provenance information for a land use classification is intended for users of the generated dataset central and local government as well as technical researchers from a range of disciplines lumass provenance information however is intended to be used by modellers seeking to replicate or debug a complex model or part thereof and as such there is much more information to capture as a result the level of complexity of provenance information generated by pyluc shown in this subsection differs to that of lumass shown in subsection 3 2 as prov n notation is not easily human readable the provenance generated by pyluc for the lunz luc has been visualised by the proviz tool for this section following on from fig 1 which illustrates the conceptual framework for the pyluc application fig 3 is a standard representation of the provenance information resulting from a run of the lunz land use classification model the primary upper pane shows a graph of the entire provenance file the secondary lower pane is a gateway to explore the model s provenance by selecting parts of the upper pane in doing so the user i obtains in depth understanding of the model s processes ii verifies attribution of sources and entities and iii views specifics related to the generation of the model s outputs the lower panel in fig 3 shows the neighbourhood of one of the final activities in the pyluc model rules lunz final the algorithm used to define the final land use classes for new zealand two agents wasassociatedwith the activity lr jollyb and lr mandersona lr landcare research by selecting the agent lr jollyb the neighbourhood view shifts to show i the associations of the agent to all activities ii the attributes defining the agent which list the foaf name as ben jolly and prov type as prov person and iii that ben jolly actedonbehalfof an organisation named landcare research returning to the activity rules lunz final the seven entities used to generate resultslunz final can be viewed selecting the first of these provides the local neighbourhood of the entity results final orch which is attributed to the prov person lr mandersona and generated by the activity rules n5 orchard types by navigating to this next activity the 13 entities used within the algorithm are viewable in contrast to the lumass implementation pyluc provenance information does not contain the actual algorithm code the intent is for the provenance data to be consumed alongside the technical documentation automatically generated at the same time which includes the code and is perhaps a more suitable format in the context of pyluc to ensure complete reproducibility is met the pyluc definition file can be packaged alongside the provenance information and technical documentation 3 2 data provenance of sednetnz as produced by lumass fig 4 which illustrates the force based visualization tool developed specifically for large complex models is a representation of the components of the sednetnz model dymond et al 2016 in graph theory a component is a set of nodes which are linked together in some way this visualization draws each component of a prov n json document inside separate circles each activity entity or agent contained in sednetnz is represented by a small coloured node prov n defines different relations between these types objects relations are represented by the lines connecting the things in the relation in general the arrows point from the future to the past for example a grey arrow represents the relation wasderivedfrom in prov n meaning that entity a2 wasderivedfrom entity a1 sometimes in a prov n document a relation may refer to an activity entity or agent that has not been defined in the document in this visualization these things which have been referred to but not defined are drawn as a white circle with a coloured outline the panel on the right of fig 4 displays an activity nm mapalgebra9 update 1 of the sednetnz model in its immediate neighbourhood the attributes of the activity are listed in the associated table this activity was informed by four other activities imagereader this simply means that the activity is a function that reads images hence the activity nm mapalgebra9 update 1 reads the four images listed in its attribute table each image representing the sediment yield of each of the four erosion processes landslide surficial earthflow and gully erosion the mapexpression in the attribute table specifies the function which is the sum of the sediment yields of the four erosion processes an agent prov softwareagent wasassociatedwith the activity nm mapalgebra9 which specifies the number of iterations for the activity and the timelevel which defines the model hierarchy in lumass the generation of the new dataset resulting from this activity occurs in a second activity i e nm imagewriter11 upate 1 which wasinformedby nm mapalgebra9 update 1 by selecting this next activity the user can see that the entity img totalhillerosion img wasgeneratedby the image writer the visualization thus enables the user to navigate through a specific model run by viewing the relations between components and the respective attributes the data provenance provides detailed information of what was done how by which in this sednetnz example the agent is a prov n softwareagent which in contrast to pyluc where agents are persons and organisations who though the start and end times of processing pipelines and individual activities as well as the time of usage generation and revision of data are recorded in the prov n documents these attributes are excluded from representation to create a more user friendly experience 4 discussion transparency openness and reproducibility are readily recognized as vital features of science mcnutt 2014 miguel et al 2014 cohen boulakia et al 2017 croucher et al 2017 horsburgh athanasiadis 2018 brinckman et al 2019 it is critical that these features become an integral component of our science so that the credibility and legitimacy of science is upheld this requires that all data methods and results have a documented audit trail that tracks their origin and provenance the essence of data provenance application is to facilitate the process of understanding what a model does i e to increase transparency in science modelling data provenance is not primarily aimed at reproducibility though it does support reproducible science by increasing confidence in datasets we applied data provenance tracking to two different modelling frameworks each with a varied interpretation of the w3c prov standard new zealand s sediment budget model sednetnz and in a new tool pyluc developed to help generate land use classifications in both cases these frameworks are used to create data used by other scientists and policy makers where the provision of provenance information would be of great value once data provenance is systematically and comprehensively recorded the requirements of both users and owners of the data need to be considered in terms of the level of granularity of provenance information to be provided to different users for both sednetnz and pyluc the client is likely to be interested in the metadata describing the various model outputs the lineage or source of data inputs ownership and copyright licencing terms and conditions fine grain data provenance relating to the detailed processing of the data is likely to be more relevant to the creator and owner of the data and to other scientists to allow the replication of the workflow mechanisms that allow the extraction and integration of provenance information at different levels of granularity workflow and value level to support the information needs of different users is a desirable goal integration of data provenance tracking into the pyluc framework was relatively straightforward compared to retrospective implementation in lumass this is primarily due to design decisions made very early in the project with provenance tracking being part of the design pyluc follows an object oriented software paradigm where the overall operation input datasets rules and processing engine are represented by classes it was possible to add additional properties and methods to these classes to cater for provenance tracking and generation as well as a provenance class to collate the information in this case the value gained from implementing data provenance greatly surpassed the effort required to integrate it in lumass the provenance tracking was retro fitted to an existing framework this required significant resourcing and involved mapping the prov dm to the existing hierarchy and structure of the modelling framework because of the fine grained tracking of provenance information changes were required at many different levels of the code for comprehensive implementation the work required to retrofit provenance tracking to existing environmental modelling frameworks may be a disincentive to researchers however once provenance tracking is integrated into a framework users of that framework can apply it to any existing or new model different users will require the provenance information in different formats not all users will be familiar with working with xml rdf or json files for example provenance information needs to be provided in human readable forms but also increasingly in a form that supports machine to machine sharing to facilitate cross platform sharing of provenance information it is important to use an appropriate standardised model a number of models have been suggested with the prov family of standards w3c 2013a having the greatest uptake to date in both pyluc and lumass we were able to successfully gather provenance information create a prov n file and use the same data structure and encoding in our own interactive visualization tool however as mentioned above development of provenance tracking in each modelling framework was done independently w3c prov dm is a generic data model that is designed to support application specific extensions jiang et al 2018 and it transpired that the two frameworks interpreted the w3c standard slightly differently according to the purpose of the framework in part this reflects the fact that prov and more specifically prov dm is a generic data model able to document the provenance of a piece of data or a thing i e any piece of data or thing a generic provenance ontology needs to be interpreted differently to capture the diverse complex provenance requirements in specific domains for example a domain independent provenance question might ask for all the steps taken in a workflow that produced a named dataset a domain specific provenance question might ask for all the values of calibration parameters that have been used by a land use optimisation model applied over the upper sub catchments of a specific geographic area as a result we see the emergence of domain specific provenance applications that employ prov dm and use prov o as an upper level reference ontology ma et al 2014 stocker 2015 cuevas vicenttín 2015 closa et al 2017 valdez et al 2018 there is also scope within prov dm for far more granular and explicit representations using the various subclasses qualified properties and expanded terms as required the two frameworks introduced here adopted a different interpretation of prov dm to suit their respective purpose for example lumass used the software agent sub class to represent the hierarchical structure of lumass models whereas pyluc used agents to represent human agents and organisations even though both presented prov dm applications are compatible and could be extended to use the same conceptualisation for example lumass could use a higher level of agents to represent human agents and organisations to which data can be attributed in terms of activities processes in lumass are equivalent to rules in pyluc however compatibility of fine grained model provenance in the context of an environmental modelling workflow may not be given for a different set of models this raises the more general question of how incompatible model provenance could be integrated into an overarching workflow provenance of environmental modelling that can still be visualised and queried in a meaningful way this research question was initially addressed by chirigati and freire 2012 and in the context of workflow management systems by missier et al 2010 and liew et al 2016 while further research must focus on compatibility of different granularities of provenance for environmental modelling a way forward may be given through provenance bundles w3c 2013c which can support provenance of provenance and provenance aggregators are capable of merging bundles to maximise the value of provenance users need access to be able to comprehend and understand provenance data we found that the provenance data that are produced can be large and complex this is in line with the findings of other researchers e g macko and seltzer 2011 borkin et al 2013 and curcin 2017 as mentioned above it seems likely that provenance data need to be filtered and presented to users in different ways depending on their information needs as we demonstrated users can be assisted by tools that support querying browsing filtering and viewing of the provenance data the visualization tools created and presented here demonstrate a useable format for provenance representation as the user can interactively determine the level of detail viewed research into the visualization of linked data ontologies suggest well designed visualizations combined with appropriate interaction techniques enable users to navigate through and make sense of large and complex datasets schneiderman 1996 valsecchi et al 2015 ivanova et al 2016 visual exploration of provenance is likely to equally benefit users helping them understand both how data have been created and the interconnectivity of data most of the previously developed visualization tools are directed at workflow provenance which is generally not as rich in information as fine grained data provenance further interdisciplinary research and development is required to improve the usability of the tools for different types of users rigorous usability testing was beyond the scope of this study to determine how usability can best be achieved will require a deeper exploration of the nature of information needed by the users further development of the tools is also needed to create meaningful labelling of items represented in the provenance visualizations this could be achieved by using userids as names in the provenance log rather than the unique identifiers from the model run that are currently used additionally if a model such as sednetnz has multiple sub models the utility of the visualizations would be improved by having labelled clusters in the visualization each representing a sub model the user would thus be able to navigate to and explore a specific area of interest within the provenance graph currently the filtering option allows a user to search for items within the model meaningful labelling is however a pre requisite to assure functionality though increasing transparency improves the credibility of science it also brings potential risks for the researcher and other agents involved in the modelling e g the providers of the input data those involved in some way are made vulnerable to criticism when errors are exposed even if there was no ill intent at the core of environmental research is the endeavour to increase understanding of environmental systems and processes therefore greater openness at the risk of criticism is better for science and the scientist should embrace the opportunity for checks and balances however opening the science to allow competitors to view details of a model can be detrimental in what is a highly competitive science market mellor et al 2018 though provenance tracking can include attribution intellectual property rights associated with the development and maintenance of models must also be protected 5 conclusions provenance information is increasingly required in a world where the number and size of land resource datasets are continuously increasing provenance information greatly helps the management of land resource data it can be used to discover data based on how they were created by whom and why it helps understand the sources and transformations that were applied to data it can also be used for monitoring the usage and production of data by workflows to replay a workflow in order to regenerate the data from its original source and to provide a context for reusing data published to the community in addition we have found provenance information to be useful for model verification and debugging purposes the application of data provenance was showcased for two different frameworks to illustrate its value and usability for land resource related environmental modelling scenarios the first was a user led generation of a land use classification with pyluc the second demonstrated application within the existing framework of lumass using new zealand s sediment budget model sednetnz in both cases data provenance was generated and made available for archiving querying and visualization by different users of equal importance to the generation of data provenance is a suitable visualization of the provenance information to allow it to be explored and understood by users detailed provenance tracking in the two different applications resulted in a complexity of information which poses a challenge for the user who wants to engage with it the visualization tools developed to this end provide an interactive user friendly means to explore and understand the provenance information of the model outputs notwithstanding some of the issues raised in the discussion the collection and provision of provenance information are increasingly recognized as necessary we argue that best land resource data management and sharing practice should include data provenance only then can the increasing demand for the quality and integrity of environmental model outputs be met and evidenced based decision making in policy and practice be truly informed acknowledgements the research presented in this contribution was supported by the new zealand ministry of business innovation and employment mbie research project innovative data analysis contract number prop 38356 etr lcr we thank dr tom etherington for commenting on an earlier draft of this manuscript 
26203,land cover monitoring efforts are important for resource planning and ecosystem services in many countries collect earth online ceo is a new free open source and user friendly software tool for land monitoring it is the product of a collaborative effort between nasa food and agriculture organization of the united nations fao us forest service and google this paper provides a full overview of ceo s structure and functionality based on the cloud ceo s structure supports simultaneous data entry by multiple users no desktop installation is required and only an internet connection is required setting minimal requirements for using the software google earth engine widgets can be created for assisted plot interpretation such as image collection time series graphs featuring indices such as normalized difference vegetation index ndvi and related statistics we also provide a case study and related findings from a ceo workshop held in myanmar keywords land cover monitoring land use and ecosystems online collaborative and crowdsourcing platforms 1 introduction land cover data plays an indispensable role in policy development planning management and other data driven decisions in most sectors turner et al 1995 lambin et al 2001 poortinga et al 2018 examples of sectors that use land cover information include but are not limited to food security verburg et al 2013 bastiaanssen and ali 2003 hydrology modeling poortinga et al 2017 simons et al 2016 ecosystem services sturck et al 2014 troy and wilson 2006 simons et al 2017 and natural resource management planning however consistent and timely information on land cover remains an outstanding issue maps are updated infrequently while classification systems do not always meet needs of the user while data is not widely shared among different institutes traditional methods to create land cover maps required extensive field research however latest methods use satellite remote sensing anderson 1976 chen et al 2012 margono et al 2012 rogan and chen 2004 methods based on this technology still require large amounts of field data expertise and can be considered as fairly expensive when high resolution satellite imagery or expensive infrastructure to store and process the data is required recent advances in cloud based remote sensing technologies have overcome most technological challenges regarding storage capacity and computing power e g gorelick et al 2017 markert et al 2018a markert et al 2018b software from fao such as collect earth bey et al 2016 enables experts with a minimal background in remote sensing to conduct robust land assessments via interpretation of very high resolution satellite imagery vhr of any area using free and open source tools while collect earth offers exciting new functionality for land cover mapping it requires the users to share and regularly update the software and backup the data which can potentially become bottlenecks especially in the context of resource constrained environments and developing countries to address the various concerns in the use cases above collect earth online ceo is a custom built open source high resolution satellite image viewing and interpretation system developed by servir a joint venture between national aeronautics and space agency nasa and the u s agency for international development usaid in this context ceo was developed for use in projects that require land cover and or land use reference data ceo promotes consistency in locating interpreting and labeling reference data plots for use in classifying and monitoring land cover land use change the full functionality of collect earth online including collaborative compilation of reference point databases is implemented online so there is no need for localized desktop installations the ceo codebase has also been shared with the open foris initiative of the food and agriculture organization of the united nations it can be accessed at http collect earth this paper presents a detailed overview of collect earth s online architecture and data collection features in the following sections moreover an example of data collection in myanmar is presented with a list of practical guidelines for robust sampling strategies using ceo 2 user interface ceo fig 1 is a free and open source image viewing and interpretation tool ceo enables simultaneous visual interpretations of various sources of satellite imagery based on cloud computing the full functionality is implemented online no desktop installation is required 2 1 institutions after registration on the website users can create an institution or be added to an existing institution within an institution data collection projects and imagery sources are defined and user roles assigned the institution page offers space for institution description and for logo upload 2 2 users users can register as members or institution managers institution manager can create projects and collect data while users can only collect data the different available user roles are member or admin an admin can add users via email invitation membership requests to the institution can be changed by admins from pending to a specified user role statistics are available for each user for the following categories speed score completed plot number accuracy score for both a project basis and total as well as the overall ranking of the user within an institution 2 3 imagery ceo provides global coverage from digital globe and bing maps and a variety of data sets from google earth engine the imagery year can be selected for digital globe imagery on the fly enabling convenient comparison of different years a variety stacking profiles are available for digital globe focusing on either resolution minimized cloud cover content chronology or best color ceo offers the ability to connect to your own web map service wms or web map tile service wmts and hence use own imagery for data collection 2 4 projects projects are created under an institution project visibility and accessibility can be set to public to institution level or to group admins only these security levels are pre determined for each project by the project manager several publicly available crowd sourced mapathon projects are featured in the map window on the home screen anyone with an internet connection can log into ceo and begin collecting data for public projects the area of interest for the data collection is either specified in the project setup by drawing a bounding box or by uploading the sample plot locations as csv file plot locations are automatically generated based on the assigned number of plots and plot spacing according to a random or gridded design alternatively predefined plot locations can be uploaded each plot contains the actual sample points which are used for data collection sample design options are random or gridded distribution using a user specified number of samples per plot and sample resolution in meters analysis in ceo is assisted by the geodash which uses information from google earth engine based on landsat imagery the geodash can be set up to show time series or an image collection for normalized difference vegetation index enhanced vegetation index enhanced vegetation index 2 normalized difference moisture index normalized difference water index or a customized band combination project statistics are available in the project dashboard page and list members contributors the number of total flagged unanalyzed and analyzed plots as well as dates the user specified sampling scheme allows for multi attribute and hierarchical data collection each sample value group consists of a user specified number of sample values and color labels in a simple example a first attribute is land cover with options for tree water and other a second attribute is landscape change containing options for tree loss tree gain and no change projects can also be set up by copying another project s template 2 5 issue reporting the website contains a link to the github issues page which found in the support section and located under https github com openforis collect earth online issues users can get in direct contact with the developer team and report issues or suggest additional features or functionalities the support section contains also a user manual for ceo 2 6 data collection features the data collection page displays one sample plot which may contain one or many sample points the user can choose under imagery options from the available imagery datasets for digital globe specific years can be selected as well as the stacking profiles the geo dash opens in a new browser tab displaying time series such as ndvi or an image collection to assist with the interpretation the user assigns all sample points to the appropriate sample values a plot can be skipped for later analysis or flagged as bad if the imagery quality is not sufficient for analysis when all sample points of a plot are interpreted the points are saved by the user and the next plot appears project stats listing the number of assigned flagged completed and total number of plots are visible 3 system architecture collect earth online s system architecture provides a single uniform web interface that may be linked with multiple database back ends depending on user needs and technical skill the web server component of ceo is written in java 10 using the spark library 1 1 http sparkjava com for request routing and freemarker 2 2 https freemarker apache org for html templating the web client component of ceo is written in javascript ecmascript version 6 using react js 3 3 https reactjs org for the interactive user interface and openlayers 4 4 http openlayers org for embedded web maps all front end and back end code is written in a functional programming style to keep the code base simple and easy to reason about for data persistence ceo may be run with one of three database back ends embedded json database this option is the simplest to use as it requires no additional software to be installed by the system administrator it is best suited for small to medium sized instances postgresql for greater scalability performance and security ceo may store all of its information in the open source postgresql 5 5 https www postgresql org object relational database management system this option is of moderate complexity as it requires the system administrator to install and configure the postgresql database server on the same machine that runs ceo collect of users gateways for direct communication and data sharing with openforis collect and calc software tools 6 6 http openforis org ceo may be configured to persist projects and imagery through openforis collect gateway and users and institutions through openforis ofusers gateway the two gateways run as independent web applications that should be co hosted within a tomcat 7 7 http tomcat apache org application server alongside ceo 3 1 geodash the geo dash gives the user a collection of widgets to help identify features and aid in the classification process these widgets are pre configured by one of the institution administrators while setting up the project for collection the administrator can configure the widgets as image collections time series graphs statistics dual image collections or pre processed image assets with exception of the basemap imagery all of the data for the geo dash widgets come from google earth engine which is accessed through the gee gateway originally developed by the open foris team and now jointly maintained 3 2 database collect earth online can use three different back ends to get the data these are collect json and postgresql databases json database can be used for medium sized instances and does not need installation and configuration for better performance with larger data postgresql an object relational database management system ordbms can be used this database server needs to be installed and configured in the machine that runs collect earth online the schema of the postgresql database is shown in the figure fig 2 each table has multiple columns and the tables are related by primary key and foreign key columns there are database functions that update tables or retrieve data from the tables according to the user s interaction with the interface 4 case study 4 1 overview and study region myanmar is a lower mekong region country and is presently in the mandate of both servir mekong and servir hkh hubs servir is a collaborative project between the usaid and nasa covering and providing services in themes namely agriculture and food security climate and weather landscape and ecosystems and water and disasters myanmar s climate is classified as tropical monsoon climate and is characterized by strong monsoon influence hence it has intense rainy periods and high humidity with the annual average temperature ranging from 22 c to 27 c it has over 80 species across the greatest expanse of tropical forest in mainland southeast asia and a biodiversity greater than temperate forests as such myanmar faces deforestation and environmental degradation due to increasing urbanization and development in the country according to fao there was a loss of 19 of forest cover between 1990 and 2010 fao and global forest resources assessment 2010 in this regard effective measures are being taken to avert such losses with the development of a land cover monitoring system as a key strategy towards developing a reliable national land cover monitoring system to address rising issues the forest department of myanmar was involved in a series of stakeholder consultation meetings on land cover mapping with servir mekong and hkh teams as an outcome of these events on 17 january 2018 in nay pyi taw clear needs were indicated for servir mekong and hkh representatives for a national land cover system which will support reporting for forest resource assessment fra to fao and intergovernmental panel for climate change ipcc reporting to united nations framework for climate change unfccc as top priorities in order to develop the national land cover monitoring system nlcms which provides high accuracy annual land cover map in the region servir partnered with multiple stakeholders including the forest department of myanmar and agreed to work on the following a national land cover monitoring system for myanmar which can produce accurate lulc baseline datasets and be updated annually to contribute to unfccc reporting a high quality national forest type and change map to fulfill the fra reporting to fao capacity building of technical staff of forest department and other partners to be able to operate the above systems in order to start implementing these objectives a workshop and capacity building training on land cover classification system lccs and sample data collection using high resolution images in the collect earth online ceo was conducted from june 18 22 2018 in nai pyi taw myanmar in the first three days introduction to the lccs and ceo were provided while for the rest of the workshop data collection was conducted using ceo as such a main objective of the training workshop was training on ceo to collect land cover training sample for national land cover mapping for myanmar 4 1 1 analysis the first and all subsequent day s training data were exported from ceo compiled and then analyzed to assess concordance or inter rater agreement the data tables were imported into r 3 5 0 r core team 2018 and processed using the tidyr package v0 8 1 wickham and henry 2018 and analyzed using the irr package v0 84 gamer and jim lemon 2012 plots flagged as unusable were removed from the data for the small training data sets we calculated iota ι a multivariate measure of agreement between raters or interpreters essentially a generalized form of kappa conger 1980 janson and olsson 2001 in the first two days of data collection the session started with 20 points which were focused upon with the attendees and their performance was shared with emphasis on difficulties faced in particular classes at the end of the day 300 points were further analyzed and ι was determined after each session s data was processed and metrics were calculated the data was reviewed to find plots that showed the most disagreement a slide was created for each plot showing an image of the plot and how it had been interpreted by each group along with a set of brief discussion points and tips on how to improve interpretation of similar plots a brief internal document detailing the overall performance of the workshop participants what types of errors seemed to be most common and how those errors might be addressed was also produced the presentation and internal report were used to provide feedback to the participants at the beginning of the following day over the course of the workshop as indicated by findings shown in table 1 the additional trainings on particularly challenging classes helped for the overall data set collected each day interclass correlation coefficients icc for each land cover class were calculated for each of the 20 classes in addition and shown in table 2 bartko 1966 here too improvement in the icc for most classes are indicated as the training and data collection were conducted in tandem the overall number of plots collected in the workshop was 2669 across 20 classes 5 discussion providing specific feedback tailored directly to the previous day s performance appears to have generated improvement in agreement between image interpreters after the initial training groups worked with a larger set of plots and agreement suffered considerably with many classes having poor agreement 0 4 as shown in table 2 after reviewing their previous work and receiving tips on how to improve performance the score associated with both the training and subsequent larger data sets also improved many classes saw their icc improve by 0 2 or more several of the very rare classes saw negative changes in their icc scores likely due to a majority of but not all groups changing how they assigned plots to those groups two classes mining and other had no plots assigned to them during the second cross validation set and as such no icc could be calculated some amount of improvement over the course of the workshop may also have been due to easier communication among the smaller number of participants as some participants left and the number of groups decreased a notable issue in the training was the lack of reliable internet connectivity the participants relayed that interruptions of this service hampered their learning and work flow during the sessions in contrast to collect earth desktop this limitation requires careful consideration especially in the context of applicability of ceo in resource constrained environments overall the findings indicate that preparatory sessions help in facilitating ceo data collection especially for users who are unfamiliar with the interface and do not have appropriate background in remote sensing 6 conclusion ceo is a new free open source and user friendly software tool for land monitoring which is important for resource planning and ecosystem services across the globe this paper provides a full overview of ceo s structure and functionality where we have summarized software features and present user case studies to illustrate the application of the tool it is the product of a collaborative effort between nasa food and agriculture organization of the united nations fao us forest service and google ceo enables crowd sourced visual interpretation with satellite imagery such as digitalglobe and bing maps and the ability to connect to user wms wmts feeds ceo is based on the cloud which supports simultaneous data entry by multiple users requiring minimal requirement of an internet connection one of ceo s functionalities include google earth engine widgets these can be calculated for plot interpretation such as image collection time series graphs featuring indices such as ndvi and compute related statistics non expert users can analyze over 100 sites day using a fairly simple classification scheme in this work the data collection process is described comprising sample point classification multi attribute options toggling between imagery years as well as data export and analysis options to showcase a recent application we present information and user experiences from a crowd sourced ceo mapathon hosted in collaboration with forest department myanmar as part of service delivery by servir mekong and servir hkh hubs we present salient findings including increased performance of ceo in land cover typology classification for improving reference data collection after delivery of key training procedures acknowledgements this work is dedicated to memory of our wonderful colleague joshua goldstein who unfortunately passed away recently we thank fao nasa and usaid for their support for the development of ceo we also extend our appreciation to our partners in servir hubs including the forest department myanmar for facilitating the study and delivery of services to meet their specific requirements for their national land monitoring program appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 004 
26203,land cover monitoring efforts are important for resource planning and ecosystem services in many countries collect earth online ceo is a new free open source and user friendly software tool for land monitoring it is the product of a collaborative effort between nasa food and agriculture organization of the united nations fao us forest service and google this paper provides a full overview of ceo s structure and functionality based on the cloud ceo s structure supports simultaneous data entry by multiple users no desktop installation is required and only an internet connection is required setting minimal requirements for using the software google earth engine widgets can be created for assisted plot interpretation such as image collection time series graphs featuring indices such as normalized difference vegetation index ndvi and related statistics we also provide a case study and related findings from a ceo workshop held in myanmar keywords land cover monitoring land use and ecosystems online collaborative and crowdsourcing platforms 1 introduction land cover data plays an indispensable role in policy development planning management and other data driven decisions in most sectors turner et al 1995 lambin et al 2001 poortinga et al 2018 examples of sectors that use land cover information include but are not limited to food security verburg et al 2013 bastiaanssen and ali 2003 hydrology modeling poortinga et al 2017 simons et al 2016 ecosystem services sturck et al 2014 troy and wilson 2006 simons et al 2017 and natural resource management planning however consistent and timely information on land cover remains an outstanding issue maps are updated infrequently while classification systems do not always meet needs of the user while data is not widely shared among different institutes traditional methods to create land cover maps required extensive field research however latest methods use satellite remote sensing anderson 1976 chen et al 2012 margono et al 2012 rogan and chen 2004 methods based on this technology still require large amounts of field data expertise and can be considered as fairly expensive when high resolution satellite imagery or expensive infrastructure to store and process the data is required recent advances in cloud based remote sensing technologies have overcome most technological challenges regarding storage capacity and computing power e g gorelick et al 2017 markert et al 2018a markert et al 2018b software from fao such as collect earth bey et al 2016 enables experts with a minimal background in remote sensing to conduct robust land assessments via interpretation of very high resolution satellite imagery vhr of any area using free and open source tools while collect earth offers exciting new functionality for land cover mapping it requires the users to share and regularly update the software and backup the data which can potentially become bottlenecks especially in the context of resource constrained environments and developing countries to address the various concerns in the use cases above collect earth online ceo is a custom built open source high resolution satellite image viewing and interpretation system developed by servir a joint venture between national aeronautics and space agency nasa and the u s agency for international development usaid in this context ceo was developed for use in projects that require land cover and or land use reference data ceo promotes consistency in locating interpreting and labeling reference data plots for use in classifying and monitoring land cover land use change the full functionality of collect earth online including collaborative compilation of reference point databases is implemented online so there is no need for localized desktop installations the ceo codebase has also been shared with the open foris initiative of the food and agriculture organization of the united nations it can be accessed at http collect earth this paper presents a detailed overview of collect earth s online architecture and data collection features in the following sections moreover an example of data collection in myanmar is presented with a list of practical guidelines for robust sampling strategies using ceo 2 user interface ceo fig 1 is a free and open source image viewing and interpretation tool ceo enables simultaneous visual interpretations of various sources of satellite imagery based on cloud computing the full functionality is implemented online no desktop installation is required 2 1 institutions after registration on the website users can create an institution or be added to an existing institution within an institution data collection projects and imagery sources are defined and user roles assigned the institution page offers space for institution description and for logo upload 2 2 users users can register as members or institution managers institution manager can create projects and collect data while users can only collect data the different available user roles are member or admin an admin can add users via email invitation membership requests to the institution can be changed by admins from pending to a specified user role statistics are available for each user for the following categories speed score completed plot number accuracy score for both a project basis and total as well as the overall ranking of the user within an institution 2 3 imagery ceo provides global coverage from digital globe and bing maps and a variety of data sets from google earth engine the imagery year can be selected for digital globe imagery on the fly enabling convenient comparison of different years a variety stacking profiles are available for digital globe focusing on either resolution minimized cloud cover content chronology or best color ceo offers the ability to connect to your own web map service wms or web map tile service wmts and hence use own imagery for data collection 2 4 projects projects are created under an institution project visibility and accessibility can be set to public to institution level or to group admins only these security levels are pre determined for each project by the project manager several publicly available crowd sourced mapathon projects are featured in the map window on the home screen anyone with an internet connection can log into ceo and begin collecting data for public projects the area of interest for the data collection is either specified in the project setup by drawing a bounding box or by uploading the sample plot locations as csv file plot locations are automatically generated based on the assigned number of plots and plot spacing according to a random or gridded design alternatively predefined plot locations can be uploaded each plot contains the actual sample points which are used for data collection sample design options are random or gridded distribution using a user specified number of samples per plot and sample resolution in meters analysis in ceo is assisted by the geodash which uses information from google earth engine based on landsat imagery the geodash can be set up to show time series or an image collection for normalized difference vegetation index enhanced vegetation index enhanced vegetation index 2 normalized difference moisture index normalized difference water index or a customized band combination project statistics are available in the project dashboard page and list members contributors the number of total flagged unanalyzed and analyzed plots as well as dates the user specified sampling scheme allows for multi attribute and hierarchical data collection each sample value group consists of a user specified number of sample values and color labels in a simple example a first attribute is land cover with options for tree water and other a second attribute is landscape change containing options for tree loss tree gain and no change projects can also be set up by copying another project s template 2 5 issue reporting the website contains a link to the github issues page which found in the support section and located under https github com openforis collect earth online issues users can get in direct contact with the developer team and report issues or suggest additional features or functionalities the support section contains also a user manual for ceo 2 6 data collection features the data collection page displays one sample plot which may contain one or many sample points the user can choose under imagery options from the available imagery datasets for digital globe specific years can be selected as well as the stacking profiles the geo dash opens in a new browser tab displaying time series such as ndvi or an image collection to assist with the interpretation the user assigns all sample points to the appropriate sample values a plot can be skipped for later analysis or flagged as bad if the imagery quality is not sufficient for analysis when all sample points of a plot are interpreted the points are saved by the user and the next plot appears project stats listing the number of assigned flagged completed and total number of plots are visible 3 system architecture collect earth online s system architecture provides a single uniform web interface that may be linked with multiple database back ends depending on user needs and technical skill the web server component of ceo is written in java 10 using the spark library 1 1 http sparkjava com for request routing and freemarker 2 2 https freemarker apache org for html templating the web client component of ceo is written in javascript ecmascript version 6 using react js 3 3 https reactjs org for the interactive user interface and openlayers 4 4 http openlayers org for embedded web maps all front end and back end code is written in a functional programming style to keep the code base simple and easy to reason about for data persistence ceo may be run with one of three database back ends embedded json database this option is the simplest to use as it requires no additional software to be installed by the system administrator it is best suited for small to medium sized instances postgresql for greater scalability performance and security ceo may store all of its information in the open source postgresql 5 5 https www postgresql org object relational database management system this option is of moderate complexity as it requires the system administrator to install and configure the postgresql database server on the same machine that runs ceo collect of users gateways for direct communication and data sharing with openforis collect and calc software tools 6 6 http openforis org ceo may be configured to persist projects and imagery through openforis collect gateway and users and institutions through openforis ofusers gateway the two gateways run as independent web applications that should be co hosted within a tomcat 7 7 http tomcat apache org application server alongside ceo 3 1 geodash the geo dash gives the user a collection of widgets to help identify features and aid in the classification process these widgets are pre configured by one of the institution administrators while setting up the project for collection the administrator can configure the widgets as image collections time series graphs statistics dual image collections or pre processed image assets with exception of the basemap imagery all of the data for the geo dash widgets come from google earth engine which is accessed through the gee gateway originally developed by the open foris team and now jointly maintained 3 2 database collect earth online can use three different back ends to get the data these are collect json and postgresql databases json database can be used for medium sized instances and does not need installation and configuration for better performance with larger data postgresql an object relational database management system ordbms can be used this database server needs to be installed and configured in the machine that runs collect earth online the schema of the postgresql database is shown in the figure fig 2 each table has multiple columns and the tables are related by primary key and foreign key columns there are database functions that update tables or retrieve data from the tables according to the user s interaction with the interface 4 case study 4 1 overview and study region myanmar is a lower mekong region country and is presently in the mandate of both servir mekong and servir hkh hubs servir is a collaborative project between the usaid and nasa covering and providing services in themes namely agriculture and food security climate and weather landscape and ecosystems and water and disasters myanmar s climate is classified as tropical monsoon climate and is characterized by strong monsoon influence hence it has intense rainy periods and high humidity with the annual average temperature ranging from 22 c to 27 c it has over 80 species across the greatest expanse of tropical forest in mainland southeast asia and a biodiversity greater than temperate forests as such myanmar faces deforestation and environmental degradation due to increasing urbanization and development in the country according to fao there was a loss of 19 of forest cover between 1990 and 2010 fao and global forest resources assessment 2010 in this regard effective measures are being taken to avert such losses with the development of a land cover monitoring system as a key strategy towards developing a reliable national land cover monitoring system to address rising issues the forest department of myanmar was involved in a series of stakeholder consultation meetings on land cover mapping with servir mekong and hkh teams as an outcome of these events on 17 january 2018 in nay pyi taw clear needs were indicated for servir mekong and hkh representatives for a national land cover system which will support reporting for forest resource assessment fra to fao and intergovernmental panel for climate change ipcc reporting to united nations framework for climate change unfccc as top priorities in order to develop the national land cover monitoring system nlcms which provides high accuracy annual land cover map in the region servir partnered with multiple stakeholders including the forest department of myanmar and agreed to work on the following a national land cover monitoring system for myanmar which can produce accurate lulc baseline datasets and be updated annually to contribute to unfccc reporting a high quality national forest type and change map to fulfill the fra reporting to fao capacity building of technical staff of forest department and other partners to be able to operate the above systems in order to start implementing these objectives a workshop and capacity building training on land cover classification system lccs and sample data collection using high resolution images in the collect earth online ceo was conducted from june 18 22 2018 in nai pyi taw myanmar in the first three days introduction to the lccs and ceo were provided while for the rest of the workshop data collection was conducted using ceo as such a main objective of the training workshop was training on ceo to collect land cover training sample for national land cover mapping for myanmar 4 1 1 analysis the first and all subsequent day s training data were exported from ceo compiled and then analyzed to assess concordance or inter rater agreement the data tables were imported into r 3 5 0 r core team 2018 and processed using the tidyr package v0 8 1 wickham and henry 2018 and analyzed using the irr package v0 84 gamer and jim lemon 2012 plots flagged as unusable were removed from the data for the small training data sets we calculated iota ι a multivariate measure of agreement between raters or interpreters essentially a generalized form of kappa conger 1980 janson and olsson 2001 in the first two days of data collection the session started with 20 points which were focused upon with the attendees and their performance was shared with emphasis on difficulties faced in particular classes at the end of the day 300 points were further analyzed and ι was determined after each session s data was processed and metrics were calculated the data was reviewed to find plots that showed the most disagreement a slide was created for each plot showing an image of the plot and how it had been interpreted by each group along with a set of brief discussion points and tips on how to improve interpretation of similar plots a brief internal document detailing the overall performance of the workshop participants what types of errors seemed to be most common and how those errors might be addressed was also produced the presentation and internal report were used to provide feedback to the participants at the beginning of the following day over the course of the workshop as indicated by findings shown in table 1 the additional trainings on particularly challenging classes helped for the overall data set collected each day interclass correlation coefficients icc for each land cover class were calculated for each of the 20 classes in addition and shown in table 2 bartko 1966 here too improvement in the icc for most classes are indicated as the training and data collection were conducted in tandem the overall number of plots collected in the workshop was 2669 across 20 classes 5 discussion providing specific feedback tailored directly to the previous day s performance appears to have generated improvement in agreement between image interpreters after the initial training groups worked with a larger set of plots and agreement suffered considerably with many classes having poor agreement 0 4 as shown in table 2 after reviewing their previous work and receiving tips on how to improve performance the score associated with both the training and subsequent larger data sets also improved many classes saw their icc improve by 0 2 or more several of the very rare classes saw negative changes in their icc scores likely due to a majority of but not all groups changing how they assigned plots to those groups two classes mining and other had no plots assigned to them during the second cross validation set and as such no icc could be calculated some amount of improvement over the course of the workshop may also have been due to easier communication among the smaller number of participants as some participants left and the number of groups decreased a notable issue in the training was the lack of reliable internet connectivity the participants relayed that interruptions of this service hampered their learning and work flow during the sessions in contrast to collect earth desktop this limitation requires careful consideration especially in the context of applicability of ceo in resource constrained environments overall the findings indicate that preparatory sessions help in facilitating ceo data collection especially for users who are unfamiliar with the interface and do not have appropriate background in remote sensing 6 conclusion ceo is a new free open source and user friendly software tool for land monitoring which is important for resource planning and ecosystem services across the globe this paper provides a full overview of ceo s structure and functionality where we have summarized software features and present user case studies to illustrate the application of the tool it is the product of a collaborative effort between nasa food and agriculture organization of the united nations fao us forest service and google ceo enables crowd sourced visual interpretation with satellite imagery such as digitalglobe and bing maps and the ability to connect to user wms wmts feeds ceo is based on the cloud which supports simultaneous data entry by multiple users requiring minimal requirement of an internet connection one of ceo s functionalities include google earth engine widgets these can be calculated for plot interpretation such as image collection time series graphs featuring indices such as ndvi and compute related statistics non expert users can analyze over 100 sites day using a fairly simple classification scheme in this work the data collection process is described comprising sample point classification multi attribute options toggling between imagery years as well as data export and analysis options to showcase a recent application we present information and user experiences from a crowd sourced ceo mapathon hosted in collaboration with forest department myanmar as part of service delivery by servir mekong and servir hkh hubs we present salient findings including increased performance of ceo in land cover typology classification for improving reference data collection after delivery of key training procedures acknowledgements this work is dedicated to memory of our wonderful colleague joshua goldstein who unfortunately passed away recently we thank fao nasa and usaid for their support for the development of ceo we also extend our appreciation to our partners in servir hubs including the forest department myanmar for facilitating the study and delivery of services to meet their specific requirements for their national land monitoring program appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 05 004 
26204,this work proposes a many objective approach where a schedule of new dams is optimised along side dam selections and operating rules we investigate the extent to which changing management rules during infrastructure system expansion increases the ability to identify best performing plans the approach links river basin simulation to many objective robust optimisation so that expansion plans are optimised across multiple scenarios and considering multiple metrics the approach is demonstrated using the blue nile hydropower reservoir system identifying dam activation schedules that achieve efficient trade offs for various conflicting objectives including discounted net benefits reliable downstream releases and energy generation multi reservoir system expansion scheduling formulations with three different levels of operating rule responsiveness to expansion are compared for performance and computational requirements results show benefits increase when release rules change as the multi reservoir system expands with failure to optimally adapt operating rules loosing up to 25 of npv keywords infrastructure development planning capacity expansion trans boundary systems many objective optimisation nile river basin 1 introduction meeting growing food and energy demands in many regions will require the expansion of water resources infrastructure spiertz and ewert 2009 bieker et al 2010 qu et al 2013 however infrastructure developments such as new dams can negatively impact existing downstream users potentially leading to conflict petersson et al 2007 yüksel 2009 the impact of large reservoirs can be contentious as evidenced by water disputes in some of the world s large river basins block and strzepek 2010 dinar 2012 räsänen et al 2012 planning the expansion of multi reservoir systems is a complex task and typically simplifying assumptions are used such as neglecting the ability to change operating rules during expansion in this paper we investigate the impact of allowing different extents of reservoir rule changes during multi reservoir system expansion on design recommendations and their scheduling over time the paper shows that operating rules of interdependent reservoirs should ideally change during new reservoir filling to accommodate its impact on inflow to other reservoirs and improve performance infrastructure capacity expansion planning involves identifying schedules of interventions new assets or demand management efforts in supply demand systems that meet service provision goals and other criteria hall and buras 1961 mortazavi naeini et al 2014 zeff et al 2016 while the scheduling of water supply system infrastructure investments has traditionally been driven by minimizing total discounted costs lund 1987 mousavi and ramamurthy 2000 luss 2010 padula et al 2013 the importance of environmental and economic downstream impacts of reservoirs is increasingly recognised the world bank 2009 galaz et al 2012 beh et al 2014 king et al 2014 sandoval solis and mckinney 2014 sahin et al 2016 new development can change the balance of benefits between actors and regions explicitly considering trade offs between many objectives elucidates the interdependences between scheme selection and its benefits and can be helpful in defining acceptable compromise plans kollat and reed 2007 richter and thomas 2007 kasprzyk et al 2009 woodruff et al 2013 hurford et al 2014 for example kasprzyk et al 2009 couple interactive visualisation and many objective optimization to innovate urban water portfolio planning under uncertainty many objective visual analytics woodruff et al 2013 abstracts design as a continual learning process wherein decision makers come to understand a problem while seeking its solution and emphasises learning through problem reformulation in some multi reservoir systems the greatest hindrance to further development is their downstream impact during the period when new reservoirs fill which can last several years new reservoirs in a river reach could also impact other new reservoirs downstream of them depending on the relative position scale and sequence of new infrastructure implementation delayed filling and excessive downstream impact can make dam projects financially infeasible and politically unacceptable one important issue in changing reservoir systems are the operating rules of reservoirs water researchers have considered the optimization of reservoir operating rules in an extensive literature for example using parameterization simulation optimization guariso et al 1986 oliveira and loucks 1997 koutsoyiannis and economou 2003 also known as direct policy search dps giuliani et al 2014 in dps the operating policy is assigned a functional form and its parameters are optimised to meet one or more objectives chang et al 2005 hurford and harou 2014 and giuliani et al 2014 use multi objective evolutionary optimization deb et al 2002 coello et al 2007 and visual analytics fu et al 2012 vitiello et al 2012 reed and kollat 2013 to discover operational trade offs that help balance a reservoir system s competing demands despite their advantages in handling optimization problems with non convex discontinuous functions the use of moeas has high computational costs maier et al 2014 salazar et al 2017 for example while considering flexibility of operating rules to evolve as the multi reservoir changes could potentially improve performance representing evolving reservoir operating rule coordination in large multi reservoir systems imposes an increased computational burden on the search process hence simplifying assumptions in formulating planning problems are frequently made takeda and papalambros 2012 beh et al 2014 galelli et al 2014 we examine the extent to which allowing for the evolution of operating rules in an optimised multi reservoir system design impacts the trade offs of benefits obtained from reservoir systems and impacts their optimised schedules we consider the same capacity expansion problem as mortazavi naeini et al 2014 and beh et al 2014 combinations of assets their activation dates and operating rules but in the case of a large multi reservoir hydropower system expansion to examine the link between timings of assets and their performance trade offs we introduce a customised parallel axis plot inselberg 2009 fu et al 2012 vitiello et al 2012 reed and kollat 2013 that combines information on asset choice performance and the asset investment schedule the proposed approach is applied to investigate the development of new reservoirs on the blue nile in past system expansion studies e g block and strzepek 2010 jeuland and whittington 2014 operating rules that are best for standalone reservoirs have been taken as an approximation of the optimal operating rule of reservoirs in the potential multi reservoir system the multi reservoir system sequencing and scheduling the time of implementation is then optimised ignoring the possible link between the flexibility of operating rules and optimal schedule of dam implementations this work extends the application of many objective search to nile planning of geressu and harou 2015 by also considering the filling periods of dams the current study considers the trade offs in energy generation and environmental benefits maximizing releases to minimise downstream impacts during reservoir filling periods with economic discounted net present value npv performance goals the npv metric measures the difference between time discounted future benefits from energy production and discounted capital costs the case study shows the importance impact on system design of evolving operating rules during multi reservoir system expansion and its planning the following section describes the proposed method followed by the case study context and the problem formulation results discussion and conclusion sections 2 methods here we describe the multi reservoir system scheduling problem and the proposed approach 2 1 many objective planning problem formulation in order to search for high value designs and their sequencing for this problem we propose to use many objective optimization and visual analysis of trade offs helps with the discovery of alternative designs which present acceptable performance trade offs kollat and reed 2007 kasprzyk et al 2009 woodruff et al 2013 the general problem formulation is the minimization maximization of multiple performance objectives eq 1 which guides the search algorithm to look through the decision space eq 2 subject to any constraints such as water balance mutual exclusivity of dam options etc 1 minimise f x f i 2 x ω f x target function f i performance metrics such as net present value of investments average energy etc the decision variables include choice of reservoir portfolios the timing of their implementation and their management e g coordinates of storage based release rules to investigate the impact of simplifying assumption on quality of the infrastructure investment decision we compare the optimised performance of the multi reservoir scheduled designs under different levels of operating rule responsiveness to system expansion operating rules responsiveness levels include a rules optimised for individual reservoirs then fixed in a second stage many objective optimization that sequences the pre defined investments b rules are optimised jointly with dam selections but the rules are changed only once and c rules of each reservoir are optimised for each unique system expansion stage these levels of responsiveness in reservoir operation optimization schemes a b and c are displayed graphically in fig 1using panel letters with the same letter 2 2 operating rules like giuliani et al 2014 we apply direct policy search where the operating policy is first parameterised within a given family of functions and then the parameters optimised with respect to the operating objectives we parameterise the control policies using gaussian radial basis function rbf which have been used to map the reservoir storage and time index into release decisions giuliani et al 2014 salazar et al 2017 and take the form of 3 ϕ t z t i 1 n exp j 1 m z t j c i j 2 b i j 2 following maier et al 2014 salazar et al 2017 we use n 4 rbfs where m is the number of input variables two the storage in the reservoir and time of year are used as input to the release rule the inputs in zt are uniformed on 0 1 while the centres and radii take value in c i j 1 1 b i j 0 1 w i j 0 1 i 1 n w i 1 each of the multi reservoir expansion stage represented with distinct colour in fig 1 require a separate operating rule each reservoir is allocated a unique release rule based on its own storage level and time of the year we set the number of rbfs equal to one more than the sum of the number of inputs 2 and outputs 1 each rbf has associated with it 4 weights 4 2 8 centres and 4 2 8 radii that need to be optimised a total of 21 variables need to be optimised considering the further one variable which will be multiplied with the result of rbf storage and time function which is normalised on 0 1 to give the actual release magnitude the optimal value of this variable can take a value from 0 to maximum release capacity of the dam hence a radial basis function based operating rule for a four reservoir system that is expanding could require up to 21 20 420 decision variables i e for the most detailed operating rule representation in fig 1 panel c while increased degrees of freedom for the operating rule of reservoirs could improve performance it could lead to higher stochasticity of results in problems where decision variables are interdependent resulting in high computational cost the increase in computational requirements with increased responsiveness of the operating rule is compared the storage targets of a large reservoir are typically varied during its filling period of several years length the radial basis function based operating rule considers varying storage targets at various points in the filling period time span of each reservoir which could be more than a year in contrast to the after filling period where an optimised operating rule specifies storage targets for a typical year 3 the blue nile case study 3 1 context the blue nile river is the largest tributary of the nile river contributing more than half of the annual nile flow at the high aswan dam the basin hosts large potential for hydropower production in ethiopia however because of long standing disputes on water use rights in the nile basin and ethiopia s lack of capacity to self finance the large projects the basin remains under developed amer et al 2005 arsano and tamrat 2005 cascao 2008 the blue nile presents an interesting case for evaluating multi reservoir system investment scheduling because of the number of proposed reservoirs still on the drawing board their large sizes and their close proximity which requires consideration of the impact of flow attenuation by upstream dams on downstream ones the nile riparian countries through the nile basin initiative have been engaged in building a shared vision model to aid with decision support for a consensus development plans the impact of filling the large dams on downstream water use is a major concern and could be the biggest hindrance to rapid development even in the presence of agreement despite the potential of the proposed blue nile dams to alleviate the electricity shortage in the region the period needed to construct and fill the large dams could make them less attractive financially block et al 2008 moreover the blue nile river has high seasonal and inter annual variability which could affect the filling period of reservoirs and hence their financial viability this along with the need to minimise impact on existing users in downstream countries needs to be considered in planning of the potential multi reservoir system ethiopia is currently constructing the grand ethiopian renaissance dam gerd which is located downstream of all other proposed blue nile reservoirs fig 2 the dam with 6000 mw installed capacity would impound a 72 bcm billion cubic meters reservoir the main challenge against implementing the proposed blue nile investments is the hydro political disagreement between the eastern nile countries emanating from the possible impact on the reliable flow to downstream countries when new dams will be filled zhang et al 2015 and wheeler 2016 compare various allocation strategies for filling the gerd to avoid critical downstream impacts in this study we assess alternative scheduling options considering the reduction in downstream flow in filling periods the study investigates best options choice or reservoirs the timing of their activation and their operating rules that adapt to system expansion for various balances of benefit from new reservoirs and their downstream impacts the study evaluates the impact of alternative problem formulations on decision support and trade offs between conflicting performance of interest to upstream and downstream countries visualising the benefits and impact of alternative plans and performance trade offs of the best plans can help foster understanding and help parties negotiate benefit sharing however due to limited number of future hydrological realisations considered the results of this study is meant to demonstrate the approach rather than guide actual infrastructure development decisions 3 2 many objective formulation explicitly considering trade offs between key objectives elucidates the interdependences between scheme selection and its benefits and can be helpful in defining acceptable compromise plans richter and thomas 2007 kasprzyk et al 2009 woodruff et al 2013 hurford et al 2014 for the blue nile the amount of reliable downstream flow are of concern as are the financial feasibility and energy generation capacity of new developments the performance objectives considered in this study include maximizing the average and lower quartile net present value of future benefits f a v e n p v f n p v 25 maximizing final once reservoirs have filled average annual firm annual and firm monthly energy generation f a v e a e f m i n f a e f m i n f m e respectively and the average annual f a v e r and the reliable 99 exceeded 3 year cumulative downstream releases f m i n 3 r the simulation period length is set at 80 years to accommodate the assumptions that each dam will have a project life of 50 years maximise f x f a v e n p v f n p v 25 f a v e a e f m i n f a e f m i n f m e f a v e r f m i n 3 r 4 x ω x y i t i o p i s f o p i s f l s y i 0 1 i m subject to y i y j 2 i j m k the decision variables include the activation of new reservoirs table 1 their implementation dates their reservoir release rule parameters at various multi reservoir system expansion stages i e first second third etc filling and steady state operating periods f x target function f a v e n p v q u a n t i l e i 1 f n p v 0 5 average net present value of benefits i e present worth of benefits from energy generation minus present worth of capital cost of reservoirs f n p v 25 q u a n t i l e i 1 f n p v 0 25 75 exceeded net present value of benefits f a v e a e q u a n t i l e i 1 f j t l e s e a n n l i j 0 5 average energy generation in regular operating periods tl and es are fixed at 30 and 50 years for assessing the capacity of the dam system within reasonable implementation period i e where the majority of designs show all dams can be implemented and made operational f m i n f a e q u a n t i l e i 1 f j t l e s e a n n l i j 0 95 95 exceeded near minimum annual energy generation of the reservoir system in the last steady state f m i n f m e q u a n t i l e i 1 f j t l e s e m i j 0 99 95 exceeded near minimum monthly energy generation of the reservoir system in the last steady state f m i n r 3 y r q u a n t i l e i 1 f j 1 e s r 3 i j 0 95 99 exceeded near minimum cumulative releases over a 3 year consecutive period the maximization of cumulative release in consecutive years is considered as a proxy for egyptian interests in reducing the impact of filling of ethiopian reservoirs on the high aswan reservoir f a v e r q u a n t i l e i 1 f j 1 40 y r s r 3 i j 0 5 average annual flow over the first 40 years set sufficiently large so that all reservoirs could potentially be implemented and filled in all pareto optimal designs e t ρ h t q t energy generation at month t a function of head and discharge and a constant that depends on gravity and length of month e a n n l y s u m e t t y the sum of monthly energy generations in the year y r 3 t t 36 t r t cumulative release of past 36 months at time t n p v b e n e f i t s y 1 80 e v a l e a n n l y 1 d y time discounted economic returns from energy generation n p v c o s t s 1 n o r e s 1 l e n c o n s c o s t i l e n c o n s i 1 d i m p i t i c time discounted cost of infrastructure n p v n p v b e n e f i t s n p v c o s t s time discounted net present value l e n c o n s i length of construction of dam i i m p i implementation date of dam i t i c construction year between 1 and max length of construction period of dam i see table 1 d discount rate assumed at 10 c o s t i capital cost of reservoir i i dam notation s expansion stage y i decision to activate storage option i t i implementation date for reservoir number i o p i s steady state operation rule coordinates of reservoir i in period s f o i s filling period operation rule of reservoir i in period s f l s length of filling period at stage s e v a l value of energy per kwh where m is the set of all possible reservoir options while m k m are the sets of mutually exclusive designs given as rows in table 1 the performance of water resources infrastructure is dependent on several uncertain factors including hydrologic variability climate change future water demands and evolving institutions conway et al 1996 block and strzepek 2010 swain 2011 in this application only the first source of uncertainty is considered 3 3 stochastic hydrology we use an implicit stochastic optimization approach labadie 2004 to assess benefits and impacts of alternative schedules given river flow variability we use the moving block bootstrapping method srivastav and simonovic 2014 that conserves the majority of seasonal and interannual statistical measures as well as the spatial correlation of historical stream flows the many objective search is applied over 10 realizations of synthetically generated stream flows each with 80 years length randomly selected among a total of 1000 streamflow series to represent a wide range of plausible hydrologic conditions at the different stages of system expansion the performance of the pareto optimal designs are tested under the complete set of synthetic stream flows post optimization to ascertain the randomly selected subset is representative of the range of plausible hydrology section 8 2 we optimise the aggregate average net present worth performance measure and the 99 exceeded 3 year cumulative downstream releases over the set of streamflow realizations this is considering possible differences in the risk taking preferences of stakeholders the best staged investment portfolios are those that do well with aggregate performance measure when considering the diverse streamflow realizations and are not optimised for any individual hydrologic scenario 3 4 computational details and data we employ a heuristic optimization approach where a search algorithm kollat and reed 2006 reed et al 2013 is coupled with a simulation model of the water resources system the problem is formulated as a six objective optimization problem with 7 proposed reservoir designs each with optimised operating rules for each unique stage of system expansion the objectives are evaluated by simulating the system monthly using 10 synthetically generated monthly flow data of 80 years length each the water system simulation model representing the blue nile includes 7 reservoir nodes and 10 junction nodes and 6 links representing river reaches the system model was built using the computationally efficient interactive river aquifer simulation 2010 iras 2010 described by matrosov et al 2011 we use a publicly available database of proposed blue nile reservoirs and their characteristics collected by the nile basin initiative nbi entro 2015 the maximum annual filling rate for new reservoirs is assumed to be one third of their maximum storage volume considering construction quality monitoring requirements variable construction period of lengths of from 3 to 8 years is assumed based on the size of the reservoirs see table 1 with the capital cost of construction spread equally in the construction years the simulation period for this analysis is set at 80 years assuming a life expectancy of 50 years for the dams while it could be argued some dams could serve for longer this doesn t significantly change the reported average present value of benefits for example assuming a 50 year or 100 year gerd life span which generates 1 2 billion usd per annum assuming 15 twh year on average would result in less than a 1 change in average net present value given our discount rate of 10 this is because the discounting of benefits that come beyond 50 years into the future reduces their impact on the net present value of investments following previous studies on the nile e g whittington and mcclelland 1992 block and strzepek 2010 we assume a price of energy to be 0 08 usd kwh and monetary discount rate of 10 both constant throughout the simulated period we use the epsilon dominance non dominated sorted genetic algorithm ii kollat and reed 2006 tang et al 2006 to find approximately pareto optimal dam designs in our case study the best most efficient approximately pareto optimal designs are those where none of the objectives can be further improved without deterioration of at least one other objective olenik and haimes 1979 mavrotas and florios 2013 heuristic search results cannot be mathematically proven to be pareto optimal hence the term pareto approximate datta et al 2008 but we refer to them as pareto optimal hereafter to simplify communication 4 results next and for our simplified system representation we present the proposed blue nile reservoirs potential and the trade offs among different performance objectives under operating rules increasingly adapted to the new system as it expands section 4 1 examines the trade offs between the average net present value npv and the reliable downstream flow from the blue nile at the ethiopia sudan border for a one reservoir design section 4 2 compares estimated performance of the multi reservoir system as operating rules are increasingly refined to address each unique stage of system expansion section 4 3 examines the pareto optimal dam investment schedules across multiple metrics of performance for operating rule scheme c fig 1 the npv metric measures the difference between time discounted future benefits from energy sales and immediate and future for reservoirs planned to be build in the future capital costs 4 1 single reservoir design this section considers the performance trade offs implied by a range of operating rules for the simplified case where only one reservoir is to be built to quickly meet their design potential and generate benefits new reservoirs are ideally filled quickly and operated at full energy generation capacity however rapid filling of reservoirs can adversely affect existing downstream water uses this trade off between loss of new dam performance and downstream impact will depend on the location size and filling strategy of the new dam fig 3 shows the trade off between the reliable 99 exceeded 3 year cumulative releases and the average npv for a single new reservoir operated with one optimised rule during filling and another after filling i e as per stage 1 of fig 1 panel a each marker represents an optimal filling and steady state operating strategy in addition to a dam selection choice 4 2 comparison of performance as operating rules adapt to new system designs this section considers the case of a two reservoir system and assesses the gains in performance when using an operating rule set which is increasingly responsive to the system s expansion we compare the two responsive schemes b and c of fig 1 with static rules that are not changed when the reservoir system expands i e the operating rules are optimised for each dam regardless of other dams i e a o of fig 3 fig 4 shows the difference in the average net present worth of investments estimated under the different rules i e a e and g is less than 5 when there is no downstream release requirement low performance on the y axis however differences in net present value increase with higher requirements in downstream flow performance for the two reservoir system design the average net present value for similar reliable releases requirements varies by more than 1 billion us dollars busd when the release requirement is high e g comparing designs labelled k and i showing the value of considering the responsiveness of reservoir operating rules to dam system expansion fig 4 panel b shows how recommended dam schedules selection of reservoirs and gap between their implementations change e g compare designs h d and f under differing levels of operating rule responsiveness to system expansion efficient designs in figs 3 and 4 showed the trade off between two performance objectives however stakeholders are likely to track multiple objectives simultaneously results in section 4 3 will show how pareto optimal designs can address other important criteria 4 3 performance trade offs across multi reservoir expansion stages in this section only the designs optimised under the most responsive scheme c optimised operating rules fig 1 panel c are considered parallel axis plots inselberg 2009 steed et al 2012 are used here to show multiple objectives the right most panel iii allow to visually represent dam scheduling over time fig 5 shows the trade offs between multiple performance goals and corresponding design parameters of pareto optimal investment schedules the figure includes 6 panels i through iii from left to right and a b from top and bottom panel i shows performance directly related to upstream interests i e the net present value and annual energy generation capacity the net present value tracks the difference between time discounted future benefits and costs the energy metrics report performance in steady state after filling of all dams panel ii shows the 99 exceeded 3 year cumulative downstream release including in the filling period panel iii shows the optimal sequencing of reservoirs over time via the x axis the gerd has been under construction since 2011 and was originally scheduled to be completed by 2019 panel a in fig 5 shows pareto optimal designs for a problem formulation that recognises the gerd a pre made decision dam portfolio schedules with different numbers of reservoirs represented by marker shapes are recommended as optimal for differing balances of the conflicting performance objectives i e downstream release energy generation and net present value the majority of designs that seek to maximise the financial benefit and minimise downstream impact e g green and blue lines in panels a suggest implementing the first 3 designs in the first 20 years and delaying the fourth one up to 60 years could be needed to implement all the blue nile reservoirs with these optimised development programs the blue nile multi reservoir system has a potential to generate up to 39 twh year of which 34 5 twh year is the reliable energy i e which can be meet 99 out of 100 years despite this relatively high potential for energy generation the cost of the reservoirs and time taken to construct and fill the reservoir limits their financial potential i e net present worth to a maximum average of 3 1 billion usd a 3 dam plan including gerd followed by beko abo high and then upper mandaya could maximise the reliable 3 year cumulative downstream flow solid light blue line on fig 5 panel a maximising any of the energy objectives i e average and 99 exceeded annual and the 99 exceeded monthly energy leads to an alternative 4 dam portfolio with gerd followed by upper mandaya and karaodobi and beko abo low as the third and fourth dams however if maximising the financial benefits i e the average and the 75 exceeded net present value is a priority karoadobi and beko abo low dams should follow the gerd with upper mandaya being the last to be implemented comparing compromise designs dashed lines in fig 5 panel a shows different reservoir portfolio options could achieve close performance levels demonstrating that the sequence of reservoir implementation is not the primary factor in performance levels but that operating policies of reservoirs also strongly influence the performance and benefit trade offs to the nile stakeholders reservoir designs with energy generation capacity of up to 30 twh year with 21 5 twh year of which is a firm annual energy are possible while also meeting the highest possible downstream flow reliability 99 exceeded 3 year cummulative flow and above zero net present value shown with solid blue line in fig 5 while the reliable 3 year cumulative downstream flow metric trade off with the average net present value it does not have a clear trade off with the potential average annual energy generation possible in the years 2031 2051 20 years from the gerd implementation period in 2011 this is also shown by how relaxing the downstream release requirement from the maximum possible solid line to 90 does not improve the average annual energy generation performance the results show regulation from blue nile dams can also improve the reliability of downstream flows however the long term average of the downstream flow could be reduced by up to 5 bcm fig 5 panel b shows in addition to being the option with highest potential energy generation the gerd maximises the reliable 3 year cumulative downstream flow compared to other options however other options such as mandaya could have done better financially maximizing the net present value given the uncertainty on how many of the potential dams will be implemented and what their filling and steady states management will be and when the next dam will be implemented it is not possible to assess the current decision by ethiopia to select gerd as the first investment the rate of filling and its management is still being negotiated among the thee eastern nile countries egypt ethiopia and sudan fig 5 panel b shows the gerd could be a compromise decision as it can meet a number of performance objectives i e maximizing the average and reliable annual energy red and dark red solid lines and maximizing the minimum net present value under various management strategies 5 discussion a multi criteria multi reservoir capacity expansion scheduling approach is proposed and applied to the possible blue nile multi reservoir system components we use visual analytic techniques to present alternative pareto optimal designs given many objectives 6 results show the performance trade offs and optimised schedule of infrastructure depend on the degree of responsiveness of operating policies to system changes i e a change in the optimization formulation the approach helps stakeholders visualise trade offs between multiple performance goals and helps identify the best infrastructure and operating rule design choices under various social environmental and downstream performance requirements the nile water allocation and how the filling of the blue nile reservoirs will be managed is yet to be negotiated among the nile riparian countries the modeling approach demonstrated in this study helps explore the trade offs implied in alternative plans for blue nile reservoir development including selection of reservoirs scheduling and filling steady state operating policies a decision making approach seeking to maximise a single measure of performance identifies one plan as best explicit consideration of competing goals helps identify potential compromise designs parallel axis plots such as fig 5 are best used interactively allowing decision makers to internalise how designs choices lead to performance trade offs results demonstrate that low dimensional optimization i e one or few objectives could over estimate the benefit of some designs by ignoring large reductions in filling period benefits e g designs labelled a e g in fig 4 the results also show not considering responsiveness of operating rules to system expansion leads to overestimating the cost of compromises e g from a to l instead of e to i in fig 4 panel a thereby overemphasising the trade offs and perpetrating the perception of the intractability of nile development challenge hence excessively simple assumptions on the number of reservoirs their implementation gap and rigid operating rules in nile development assessment can lead to excessively optimistic or pesimistic performance estimates overstate trade offs and bias estimates of performance for various stakeholders the study contains several limitations that could be addressed in future work firstly the current study considers the flexibility of reservoir operating policies to change at different stages of system expansion but operating rules are assumed fixed throughout the period for which they are optimised and do not vary based on hydrologic conditions in practice when dam operators find themselves in a particular hydrologic or demand conditions they typically adapt rules to current conditions e g change priorities between filling reservoirs and releasing in cases of prolonged droughts considering operating rules that change based on hydrologic conditions e g in recent years would likely attain more efficient trade offs and change which investment schedules are deemed pareto optimal in this study we focus on the benefits of dam investment planning that considers changing adapting operating policies as reservoir system configurations change the method produces a scheduled sequence of optimised dam system upgrades we do not consider the flexibility to change or abandon plans in future when more information is available other recent papers particularly under non stationary conditions with deep uncertainty propose an adaptive approach walker et al 2010 haasnoot et al 2013 some of which use optimisation formulations which are explicitly adaptive beh et al 2015 kwakkel et al 2015 zeff et al 2016 erfani et al 2018 these approaches are promising but we consider them outside the scope of this particular study which identifies the impact of a more limited form of adaptation namely the adapting of operating rules as infrastructure systems evolve but without adapting to exogenous uncertainties such as those involving filling period droughts fully considering adaptivity would likely change the estimated performance and trade offs in this study given climate change trends for eastern africa aren t clearly wetting or drying conway 2017 and that energy demands are largely unmet arsano and tamrat 2005 block and strzepek 2012 this may be an acceptable initial simplifying planning assumption other limitations include that the study considers a limited number of performance objectives and ignores the financial potential of peak power energy sales finally although the sizing of reservoirs can also be optimised within the proposed approach as in geressu and harou 2015 this was not done here to help simplify results including performance measures such as internal rate of return irr or revenue self sufficiency conditions in future studies could suggest designs with a more desirable cashflow e g where the revenue from initial investments could cover part of the cost of subsequent dams alternatively access to capital could be constrained in the search formulation by using 10 hydrologic realizations we are assuming this ensemble is sufficiently large to represent uniformly distributed peaks and droughts that do not bias the investment scheduling optimization over larger numbers of streamflow time series could better represent the plausible hydrologic variability at different periods of the simulated period however the computational requirements of the simulation optimization approach make it currently difficult to apply the multi objective optimization on a large number of streamflow realizations also uncertainty due to climate change can be an important for long term assets block and strzepek 2010 jeuland and whittington 2014 but this not considered in our study which assumes climate stationarity moreover because information on methods used for filling gaps in the observed streamflow series and estimating flow in ungauged catchments is not accessible block and strzepek 2010 alan 2012 nbi entro 2015 results are only indicative and intended to demonstrate the methodology but should not to be taken as prescriptive recommendations given the shortcomings in the current study we recognise the limitations of the results and do not claim our results should directly impact current decision making despite the limitations of the computational infrastructure available for this study we are able to show initial results to demonstrate the formulation works and has the potential to be solved satisfactorily for large problems 6 conclusion this study proposed a multi objective optimization and visual analytics approach to help decision makers consider multiple objectives when deliberating the timings of reservoir system expansion the approach allows decision makers to deliberate on future decisions considering many environmental and societal goals in addition to economic and financial objectives without having to first agree on their monetization results provide the set of efficient pareto optimal system designs i e sequence implementation dates and operating rules of reservoirs at various expansion stages and the performance trade offs they imply the proposed scheduling approach enables planning without having to depend on initial assumptions about filling rate operating rules sequence of implementations and future priorities how reservoirs may be operated in future the approach screens designs by considering the optimally coordinated operation of infrastructures and flexibility of reservoir operation at different expansion stages results show that assessing the scheduling of reservoirs assuming fixed operating rules overestimates the compromises required in agreeing on a rate of dam filling optimizing operating rules for each reservoir and for each unique system expansion stage ensures that impacts of scheduling designs are de linked from the infrastructure choices thereby revealing the best compromise designs i e whether a change in physical or operating rule design is needed to minimise impact and maximise benefits the results showed the possible impact of alternative problem formulations responsiveness of dam management to system expansion on decision support while increasing the detail with which operating rule flexibility is represented is computationally costly it allows a more accurate identification of efficient development plans and reveals less dramatic trade offs between performance objectives of upstream and downstream stakeholders failure to consider the coordinated use of a multiple reservoirs achieved through re operation of rules as the reservoir systems expands under estimates benefits and could lead to sub optimal design recommendations acknowledgements the first author was supported primarily by the engineering and physical sciences research council epsrc grant reference ep j500331 1 uk research and innovation ukri provided support through the global challenge research fund gcrf futuredams project es p011373 1 university college london faculty of engineering also provided support appendix a supplementary data the following is the supplementary data to this article supplementry supplementry appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 002 
26204,this work proposes a many objective approach where a schedule of new dams is optimised along side dam selections and operating rules we investigate the extent to which changing management rules during infrastructure system expansion increases the ability to identify best performing plans the approach links river basin simulation to many objective robust optimisation so that expansion plans are optimised across multiple scenarios and considering multiple metrics the approach is demonstrated using the blue nile hydropower reservoir system identifying dam activation schedules that achieve efficient trade offs for various conflicting objectives including discounted net benefits reliable downstream releases and energy generation multi reservoir system expansion scheduling formulations with three different levels of operating rule responsiveness to expansion are compared for performance and computational requirements results show benefits increase when release rules change as the multi reservoir system expands with failure to optimally adapt operating rules loosing up to 25 of npv keywords infrastructure development planning capacity expansion trans boundary systems many objective optimisation nile river basin 1 introduction meeting growing food and energy demands in many regions will require the expansion of water resources infrastructure spiertz and ewert 2009 bieker et al 2010 qu et al 2013 however infrastructure developments such as new dams can negatively impact existing downstream users potentially leading to conflict petersson et al 2007 yüksel 2009 the impact of large reservoirs can be contentious as evidenced by water disputes in some of the world s large river basins block and strzepek 2010 dinar 2012 räsänen et al 2012 planning the expansion of multi reservoir systems is a complex task and typically simplifying assumptions are used such as neglecting the ability to change operating rules during expansion in this paper we investigate the impact of allowing different extents of reservoir rule changes during multi reservoir system expansion on design recommendations and their scheduling over time the paper shows that operating rules of interdependent reservoirs should ideally change during new reservoir filling to accommodate its impact on inflow to other reservoirs and improve performance infrastructure capacity expansion planning involves identifying schedules of interventions new assets or demand management efforts in supply demand systems that meet service provision goals and other criteria hall and buras 1961 mortazavi naeini et al 2014 zeff et al 2016 while the scheduling of water supply system infrastructure investments has traditionally been driven by minimizing total discounted costs lund 1987 mousavi and ramamurthy 2000 luss 2010 padula et al 2013 the importance of environmental and economic downstream impacts of reservoirs is increasingly recognised the world bank 2009 galaz et al 2012 beh et al 2014 king et al 2014 sandoval solis and mckinney 2014 sahin et al 2016 new development can change the balance of benefits between actors and regions explicitly considering trade offs between many objectives elucidates the interdependences between scheme selection and its benefits and can be helpful in defining acceptable compromise plans kollat and reed 2007 richter and thomas 2007 kasprzyk et al 2009 woodruff et al 2013 hurford et al 2014 for example kasprzyk et al 2009 couple interactive visualisation and many objective optimization to innovate urban water portfolio planning under uncertainty many objective visual analytics woodruff et al 2013 abstracts design as a continual learning process wherein decision makers come to understand a problem while seeking its solution and emphasises learning through problem reformulation in some multi reservoir systems the greatest hindrance to further development is their downstream impact during the period when new reservoirs fill which can last several years new reservoirs in a river reach could also impact other new reservoirs downstream of them depending on the relative position scale and sequence of new infrastructure implementation delayed filling and excessive downstream impact can make dam projects financially infeasible and politically unacceptable one important issue in changing reservoir systems are the operating rules of reservoirs water researchers have considered the optimization of reservoir operating rules in an extensive literature for example using parameterization simulation optimization guariso et al 1986 oliveira and loucks 1997 koutsoyiannis and economou 2003 also known as direct policy search dps giuliani et al 2014 in dps the operating policy is assigned a functional form and its parameters are optimised to meet one or more objectives chang et al 2005 hurford and harou 2014 and giuliani et al 2014 use multi objective evolutionary optimization deb et al 2002 coello et al 2007 and visual analytics fu et al 2012 vitiello et al 2012 reed and kollat 2013 to discover operational trade offs that help balance a reservoir system s competing demands despite their advantages in handling optimization problems with non convex discontinuous functions the use of moeas has high computational costs maier et al 2014 salazar et al 2017 for example while considering flexibility of operating rules to evolve as the multi reservoir changes could potentially improve performance representing evolving reservoir operating rule coordination in large multi reservoir systems imposes an increased computational burden on the search process hence simplifying assumptions in formulating planning problems are frequently made takeda and papalambros 2012 beh et al 2014 galelli et al 2014 we examine the extent to which allowing for the evolution of operating rules in an optimised multi reservoir system design impacts the trade offs of benefits obtained from reservoir systems and impacts their optimised schedules we consider the same capacity expansion problem as mortazavi naeini et al 2014 and beh et al 2014 combinations of assets their activation dates and operating rules but in the case of a large multi reservoir hydropower system expansion to examine the link between timings of assets and their performance trade offs we introduce a customised parallel axis plot inselberg 2009 fu et al 2012 vitiello et al 2012 reed and kollat 2013 that combines information on asset choice performance and the asset investment schedule the proposed approach is applied to investigate the development of new reservoirs on the blue nile in past system expansion studies e g block and strzepek 2010 jeuland and whittington 2014 operating rules that are best for standalone reservoirs have been taken as an approximation of the optimal operating rule of reservoirs in the potential multi reservoir system the multi reservoir system sequencing and scheduling the time of implementation is then optimised ignoring the possible link between the flexibility of operating rules and optimal schedule of dam implementations this work extends the application of many objective search to nile planning of geressu and harou 2015 by also considering the filling periods of dams the current study considers the trade offs in energy generation and environmental benefits maximizing releases to minimise downstream impacts during reservoir filling periods with economic discounted net present value npv performance goals the npv metric measures the difference between time discounted future benefits from energy production and discounted capital costs the case study shows the importance impact on system design of evolving operating rules during multi reservoir system expansion and its planning the following section describes the proposed method followed by the case study context and the problem formulation results discussion and conclusion sections 2 methods here we describe the multi reservoir system scheduling problem and the proposed approach 2 1 many objective planning problem formulation in order to search for high value designs and their sequencing for this problem we propose to use many objective optimization and visual analysis of trade offs helps with the discovery of alternative designs which present acceptable performance trade offs kollat and reed 2007 kasprzyk et al 2009 woodruff et al 2013 the general problem formulation is the minimization maximization of multiple performance objectives eq 1 which guides the search algorithm to look through the decision space eq 2 subject to any constraints such as water balance mutual exclusivity of dam options etc 1 minimise f x f i 2 x ω f x target function f i performance metrics such as net present value of investments average energy etc the decision variables include choice of reservoir portfolios the timing of their implementation and their management e g coordinates of storage based release rules to investigate the impact of simplifying assumption on quality of the infrastructure investment decision we compare the optimised performance of the multi reservoir scheduled designs under different levels of operating rule responsiveness to system expansion operating rules responsiveness levels include a rules optimised for individual reservoirs then fixed in a second stage many objective optimization that sequences the pre defined investments b rules are optimised jointly with dam selections but the rules are changed only once and c rules of each reservoir are optimised for each unique system expansion stage these levels of responsiveness in reservoir operation optimization schemes a b and c are displayed graphically in fig 1using panel letters with the same letter 2 2 operating rules like giuliani et al 2014 we apply direct policy search where the operating policy is first parameterised within a given family of functions and then the parameters optimised with respect to the operating objectives we parameterise the control policies using gaussian radial basis function rbf which have been used to map the reservoir storage and time index into release decisions giuliani et al 2014 salazar et al 2017 and take the form of 3 ϕ t z t i 1 n exp j 1 m z t j c i j 2 b i j 2 following maier et al 2014 salazar et al 2017 we use n 4 rbfs where m is the number of input variables two the storage in the reservoir and time of year are used as input to the release rule the inputs in zt are uniformed on 0 1 while the centres and radii take value in c i j 1 1 b i j 0 1 w i j 0 1 i 1 n w i 1 each of the multi reservoir expansion stage represented with distinct colour in fig 1 require a separate operating rule each reservoir is allocated a unique release rule based on its own storage level and time of the year we set the number of rbfs equal to one more than the sum of the number of inputs 2 and outputs 1 each rbf has associated with it 4 weights 4 2 8 centres and 4 2 8 radii that need to be optimised a total of 21 variables need to be optimised considering the further one variable which will be multiplied with the result of rbf storage and time function which is normalised on 0 1 to give the actual release magnitude the optimal value of this variable can take a value from 0 to maximum release capacity of the dam hence a radial basis function based operating rule for a four reservoir system that is expanding could require up to 21 20 420 decision variables i e for the most detailed operating rule representation in fig 1 panel c while increased degrees of freedom for the operating rule of reservoirs could improve performance it could lead to higher stochasticity of results in problems where decision variables are interdependent resulting in high computational cost the increase in computational requirements with increased responsiveness of the operating rule is compared the storage targets of a large reservoir are typically varied during its filling period of several years length the radial basis function based operating rule considers varying storage targets at various points in the filling period time span of each reservoir which could be more than a year in contrast to the after filling period where an optimised operating rule specifies storage targets for a typical year 3 the blue nile case study 3 1 context the blue nile river is the largest tributary of the nile river contributing more than half of the annual nile flow at the high aswan dam the basin hosts large potential for hydropower production in ethiopia however because of long standing disputes on water use rights in the nile basin and ethiopia s lack of capacity to self finance the large projects the basin remains under developed amer et al 2005 arsano and tamrat 2005 cascao 2008 the blue nile presents an interesting case for evaluating multi reservoir system investment scheduling because of the number of proposed reservoirs still on the drawing board their large sizes and their close proximity which requires consideration of the impact of flow attenuation by upstream dams on downstream ones the nile riparian countries through the nile basin initiative have been engaged in building a shared vision model to aid with decision support for a consensus development plans the impact of filling the large dams on downstream water use is a major concern and could be the biggest hindrance to rapid development even in the presence of agreement despite the potential of the proposed blue nile dams to alleviate the electricity shortage in the region the period needed to construct and fill the large dams could make them less attractive financially block et al 2008 moreover the blue nile river has high seasonal and inter annual variability which could affect the filling period of reservoirs and hence their financial viability this along with the need to minimise impact on existing users in downstream countries needs to be considered in planning of the potential multi reservoir system ethiopia is currently constructing the grand ethiopian renaissance dam gerd which is located downstream of all other proposed blue nile reservoirs fig 2 the dam with 6000 mw installed capacity would impound a 72 bcm billion cubic meters reservoir the main challenge against implementing the proposed blue nile investments is the hydro political disagreement between the eastern nile countries emanating from the possible impact on the reliable flow to downstream countries when new dams will be filled zhang et al 2015 and wheeler 2016 compare various allocation strategies for filling the gerd to avoid critical downstream impacts in this study we assess alternative scheduling options considering the reduction in downstream flow in filling periods the study investigates best options choice or reservoirs the timing of their activation and their operating rules that adapt to system expansion for various balances of benefit from new reservoirs and their downstream impacts the study evaluates the impact of alternative problem formulations on decision support and trade offs between conflicting performance of interest to upstream and downstream countries visualising the benefits and impact of alternative plans and performance trade offs of the best plans can help foster understanding and help parties negotiate benefit sharing however due to limited number of future hydrological realisations considered the results of this study is meant to demonstrate the approach rather than guide actual infrastructure development decisions 3 2 many objective formulation explicitly considering trade offs between key objectives elucidates the interdependences between scheme selection and its benefits and can be helpful in defining acceptable compromise plans richter and thomas 2007 kasprzyk et al 2009 woodruff et al 2013 hurford et al 2014 for the blue nile the amount of reliable downstream flow are of concern as are the financial feasibility and energy generation capacity of new developments the performance objectives considered in this study include maximizing the average and lower quartile net present value of future benefits f a v e n p v f n p v 25 maximizing final once reservoirs have filled average annual firm annual and firm monthly energy generation f a v e a e f m i n f a e f m i n f m e respectively and the average annual f a v e r and the reliable 99 exceeded 3 year cumulative downstream releases f m i n 3 r the simulation period length is set at 80 years to accommodate the assumptions that each dam will have a project life of 50 years maximise f x f a v e n p v f n p v 25 f a v e a e f m i n f a e f m i n f m e f a v e r f m i n 3 r 4 x ω x y i t i o p i s f o p i s f l s y i 0 1 i m subject to y i y j 2 i j m k the decision variables include the activation of new reservoirs table 1 their implementation dates their reservoir release rule parameters at various multi reservoir system expansion stages i e first second third etc filling and steady state operating periods f x target function f a v e n p v q u a n t i l e i 1 f n p v 0 5 average net present value of benefits i e present worth of benefits from energy generation minus present worth of capital cost of reservoirs f n p v 25 q u a n t i l e i 1 f n p v 0 25 75 exceeded net present value of benefits f a v e a e q u a n t i l e i 1 f j t l e s e a n n l i j 0 5 average energy generation in regular operating periods tl and es are fixed at 30 and 50 years for assessing the capacity of the dam system within reasonable implementation period i e where the majority of designs show all dams can be implemented and made operational f m i n f a e q u a n t i l e i 1 f j t l e s e a n n l i j 0 95 95 exceeded near minimum annual energy generation of the reservoir system in the last steady state f m i n f m e q u a n t i l e i 1 f j t l e s e m i j 0 99 95 exceeded near minimum monthly energy generation of the reservoir system in the last steady state f m i n r 3 y r q u a n t i l e i 1 f j 1 e s r 3 i j 0 95 99 exceeded near minimum cumulative releases over a 3 year consecutive period the maximization of cumulative release in consecutive years is considered as a proxy for egyptian interests in reducing the impact of filling of ethiopian reservoirs on the high aswan reservoir f a v e r q u a n t i l e i 1 f j 1 40 y r s r 3 i j 0 5 average annual flow over the first 40 years set sufficiently large so that all reservoirs could potentially be implemented and filled in all pareto optimal designs e t ρ h t q t energy generation at month t a function of head and discharge and a constant that depends on gravity and length of month e a n n l y s u m e t t y the sum of monthly energy generations in the year y r 3 t t 36 t r t cumulative release of past 36 months at time t n p v b e n e f i t s y 1 80 e v a l e a n n l y 1 d y time discounted economic returns from energy generation n p v c o s t s 1 n o r e s 1 l e n c o n s c o s t i l e n c o n s i 1 d i m p i t i c time discounted cost of infrastructure n p v n p v b e n e f i t s n p v c o s t s time discounted net present value l e n c o n s i length of construction of dam i i m p i implementation date of dam i t i c construction year between 1 and max length of construction period of dam i see table 1 d discount rate assumed at 10 c o s t i capital cost of reservoir i i dam notation s expansion stage y i decision to activate storage option i t i implementation date for reservoir number i o p i s steady state operation rule coordinates of reservoir i in period s f o i s filling period operation rule of reservoir i in period s f l s length of filling period at stage s e v a l value of energy per kwh where m is the set of all possible reservoir options while m k m are the sets of mutually exclusive designs given as rows in table 1 the performance of water resources infrastructure is dependent on several uncertain factors including hydrologic variability climate change future water demands and evolving institutions conway et al 1996 block and strzepek 2010 swain 2011 in this application only the first source of uncertainty is considered 3 3 stochastic hydrology we use an implicit stochastic optimization approach labadie 2004 to assess benefits and impacts of alternative schedules given river flow variability we use the moving block bootstrapping method srivastav and simonovic 2014 that conserves the majority of seasonal and interannual statistical measures as well as the spatial correlation of historical stream flows the many objective search is applied over 10 realizations of synthetically generated stream flows each with 80 years length randomly selected among a total of 1000 streamflow series to represent a wide range of plausible hydrologic conditions at the different stages of system expansion the performance of the pareto optimal designs are tested under the complete set of synthetic stream flows post optimization to ascertain the randomly selected subset is representative of the range of plausible hydrology section 8 2 we optimise the aggregate average net present worth performance measure and the 99 exceeded 3 year cumulative downstream releases over the set of streamflow realizations this is considering possible differences in the risk taking preferences of stakeholders the best staged investment portfolios are those that do well with aggregate performance measure when considering the diverse streamflow realizations and are not optimised for any individual hydrologic scenario 3 4 computational details and data we employ a heuristic optimization approach where a search algorithm kollat and reed 2006 reed et al 2013 is coupled with a simulation model of the water resources system the problem is formulated as a six objective optimization problem with 7 proposed reservoir designs each with optimised operating rules for each unique stage of system expansion the objectives are evaluated by simulating the system monthly using 10 synthetically generated monthly flow data of 80 years length each the water system simulation model representing the blue nile includes 7 reservoir nodes and 10 junction nodes and 6 links representing river reaches the system model was built using the computationally efficient interactive river aquifer simulation 2010 iras 2010 described by matrosov et al 2011 we use a publicly available database of proposed blue nile reservoirs and their characteristics collected by the nile basin initiative nbi entro 2015 the maximum annual filling rate for new reservoirs is assumed to be one third of their maximum storage volume considering construction quality monitoring requirements variable construction period of lengths of from 3 to 8 years is assumed based on the size of the reservoirs see table 1 with the capital cost of construction spread equally in the construction years the simulation period for this analysis is set at 80 years assuming a life expectancy of 50 years for the dams while it could be argued some dams could serve for longer this doesn t significantly change the reported average present value of benefits for example assuming a 50 year or 100 year gerd life span which generates 1 2 billion usd per annum assuming 15 twh year on average would result in less than a 1 change in average net present value given our discount rate of 10 this is because the discounting of benefits that come beyond 50 years into the future reduces their impact on the net present value of investments following previous studies on the nile e g whittington and mcclelland 1992 block and strzepek 2010 we assume a price of energy to be 0 08 usd kwh and monetary discount rate of 10 both constant throughout the simulated period we use the epsilon dominance non dominated sorted genetic algorithm ii kollat and reed 2006 tang et al 2006 to find approximately pareto optimal dam designs in our case study the best most efficient approximately pareto optimal designs are those where none of the objectives can be further improved without deterioration of at least one other objective olenik and haimes 1979 mavrotas and florios 2013 heuristic search results cannot be mathematically proven to be pareto optimal hence the term pareto approximate datta et al 2008 but we refer to them as pareto optimal hereafter to simplify communication 4 results next and for our simplified system representation we present the proposed blue nile reservoirs potential and the trade offs among different performance objectives under operating rules increasingly adapted to the new system as it expands section 4 1 examines the trade offs between the average net present value npv and the reliable downstream flow from the blue nile at the ethiopia sudan border for a one reservoir design section 4 2 compares estimated performance of the multi reservoir system as operating rules are increasingly refined to address each unique stage of system expansion section 4 3 examines the pareto optimal dam investment schedules across multiple metrics of performance for operating rule scheme c fig 1 the npv metric measures the difference between time discounted future benefits from energy sales and immediate and future for reservoirs planned to be build in the future capital costs 4 1 single reservoir design this section considers the performance trade offs implied by a range of operating rules for the simplified case where only one reservoir is to be built to quickly meet their design potential and generate benefits new reservoirs are ideally filled quickly and operated at full energy generation capacity however rapid filling of reservoirs can adversely affect existing downstream water uses this trade off between loss of new dam performance and downstream impact will depend on the location size and filling strategy of the new dam fig 3 shows the trade off between the reliable 99 exceeded 3 year cumulative releases and the average npv for a single new reservoir operated with one optimised rule during filling and another after filling i e as per stage 1 of fig 1 panel a each marker represents an optimal filling and steady state operating strategy in addition to a dam selection choice 4 2 comparison of performance as operating rules adapt to new system designs this section considers the case of a two reservoir system and assesses the gains in performance when using an operating rule set which is increasingly responsive to the system s expansion we compare the two responsive schemes b and c of fig 1 with static rules that are not changed when the reservoir system expands i e the operating rules are optimised for each dam regardless of other dams i e a o of fig 3 fig 4 shows the difference in the average net present worth of investments estimated under the different rules i e a e and g is less than 5 when there is no downstream release requirement low performance on the y axis however differences in net present value increase with higher requirements in downstream flow performance for the two reservoir system design the average net present value for similar reliable releases requirements varies by more than 1 billion us dollars busd when the release requirement is high e g comparing designs labelled k and i showing the value of considering the responsiveness of reservoir operating rules to dam system expansion fig 4 panel b shows how recommended dam schedules selection of reservoirs and gap between their implementations change e g compare designs h d and f under differing levels of operating rule responsiveness to system expansion efficient designs in figs 3 and 4 showed the trade off between two performance objectives however stakeholders are likely to track multiple objectives simultaneously results in section 4 3 will show how pareto optimal designs can address other important criteria 4 3 performance trade offs across multi reservoir expansion stages in this section only the designs optimised under the most responsive scheme c optimised operating rules fig 1 panel c are considered parallel axis plots inselberg 2009 steed et al 2012 are used here to show multiple objectives the right most panel iii allow to visually represent dam scheduling over time fig 5 shows the trade offs between multiple performance goals and corresponding design parameters of pareto optimal investment schedules the figure includes 6 panels i through iii from left to right and a b from top and bottom panel i shows performance directly related to upstream interests i e the net present value and annual energy generation capacity the net present value tracks the difference between time discounted future benefits and costs the energy metrics report performance in steady state after filling of all dams panel ii shows the 99 exceeded 3 year cumulative downstream release including in the filling period panel iii shows the optimal sequencing of reservoirs over time via the x axis the gerd has been under construction since 2011 and was originally scheduled to be completed by 2019 panel a in fig 5 shows pareto optimal designs for a problem formulation that recognises the gerd a pre made decision dam portfolio schedules with different numbers of reservoirs represented by marker shapes are recommended as optimal for differing balances of the conflicting performance objectives i e downstream release energy generation and net present value the majority of designs that seek to maximise the financial benefit and minimise downstream impact e g green and blue lines in panels a suggest implementing the first 3 designs in the first 20 years and delaying the fourth one up to 60 years could be needed to implement all the blue nile reservoirs with these optimised development programs the blue nile multi reservoir system has a potential to generate up to 39 twh year of which 34 5 twh year is the reliable energy i e which can be meet 99 out of 100 years despite this relatively high potential for energy generation the cost of the reservoirs and time taken to construct and fill the reservoir limits their financial potential i e net present worth to a maximum average of 3 1 billion usd a 3 dam plan including gerd followed by beko abo high and then upper mandaya could maximise the reliable 3 year cumulative downstream flow solid light blue line on fig 5 panel a maximising any of the energy objectives i e average and 99 exceeded annual and the 99 exceeded monthly energy leads to an alternative 4 dam portfolio with gerd followed by upper mandaya and karaodobi and beko abo low as the third and fourth dams however if maximising the financial benefits i e the average and the 75 exceeded net present value is a priority karoadobi and beko abo low dams should follow the gerd with upper mandaya being the last to be implemented comparing compromise designs dashed lines in fig 5 panel a shows different reservoir portfolio options could achieve close performance levels demonstrating that the sequence of reservoir implementation is not the primary factor in performance levels but that operating policies of reservoirs also strongly influence the performance and benefit trade offs to the nile stakeholders reservoir designs with energy generation capacity of up to 30 twh year with 21 5 twh year of which is a firm annual energy are possible while also meeting the highest possible downstream flow reliability 99 exceeded 3 year cummulative flow and above zero net present value shown with solid blue line in fig 5 while the reliable 3 year cumulative downstream flow metric trade off with the average net present value it does not have a clear trade off with the potential average annual energy generation possible in the years 2031 2051 20 years from the gerd implementation period in 2011 this is also shown by how relaxing the downstream release requirement from the maximum possible solid line to 90 does not improve the average annual energy generation performance the results show regulation from blue nile dams can also improve the reliability of downstream flows however the long term average of the downstream flow could be reduced by up to 5 bcm fig 5 panel b shows in addition to being the option with highest potential energy generation the gerd maximises the reliable 3 year cumulative downstream flow compared to other options however other options such as mandaya could have done better financially maximizing the net present value given the uncertainty on how many of the potential dams will be implemented and what their filling and steady states management will be and when the next dam will be implemented it is not possible to assess the current decision by ethiopia to select gerd as the first investment the rate of filling and its management is still being negotiated among the thee eastern nile countries egypt ethiopia and sudan fig 5 panel b shows the gerd could be a compromise decision as it can meet a number of performance objectives i e maximizing the average and reliable annual energy red and dark red solid lines and maximizing the minimum net present value under various management strategies 5 discussion a multi criteria multi reservoir capacity expansion scheduling approach is proposed and applied to the possible blue nile multi reservoir system components we use visual analytic techniques to present alternative pareto optimal designs given many objectives 6 results show the performance trade offs and optimised schedule of infrastructure depend on the degree of responsiveness of operating policies to system changes i e a change in the optimization formulation the approach helps stakeholders visualise trade offs between multiple performance goals and helps identify the best infrastructure and operating rule design choices under various social environmental and downstream performance requirements the nile water allocation and how the filling of the blue nile reservoirs will be managed is yet to be negotiated among the nile riparian countries the modeling approach demonstrated in this study helps explore the trade offs implied in alternative plans for blue nile reservoir development including selection of reservoirs scheduling and filling steady state operating policies a decision making approach seeking to maximise a single measure of performance identifies one plan as best explicit consideration of competing goals helps identify potential compromise designs parallel axis plots such as fig 5 are best used interactively allowing decision makers to internalise how designs choices lead to performance trade offs results demonstrate that low dimensional optimization i e one or few objectives could over estimate the benefit of some designs by ignoring large reductions in filling period benefits e g designs labelled a e g in fig 4 the results also show not considering responsiveness of operating rules to system expansion leads to overestimating the cost of compromises e g from a to l instead of e to i in fig 4 panel a thereby overemphasising the trade offs and perpetrating the perception of the intractability of nile development challenge hence excessively simple assumptions on the number of reservoirs their implementation gap and rigid operating rules in nile development assessment can lead to excessively optimistic or pesimistic performance estimates overstate trade offs and bias estimates of performance for various stakeholders the study contains several limitations that could be addressed in future work firstly the current study considers the flexibility of reservoir operating policies to change at different stages of system expansion but operating rules are assumed fixed throughout the period for which they are optimised and do not vary based on hydrologic conditions in practice when dam operators find themselves in a particular hydrologic or demand conditions they typically adapt rules to current conditions e g change priorities between filling reservoirs and releasing in cases of prolonged droughts considering operating rules that change based on hydrologic conditions e g in recent years would likely attain more efficient trade offs and change which investment schedules are deemed pareto optimal in this study we focus on the benefits of dam investment planning that considers changing adapting operating policies as reservoir system configurations change the method produces a scheduled sequence of optimised dam system upgrades we do not consider the flexibility to change or abandon plans in future when more information is available other recent papers particularly under non stationary conditions with deep uncertainty propose an adaptive approach walker et al 2010 haasnoot et al 2013 some of which use optimisation formulations which are explicitly adaptive beh et al 2015 kwakkel et al 2015 zeff et al 2016 erfani et al 2018 these approaches are promising but we consider them outside the scope of this particular study which identifies the impact of a more limited form of adaptation namely the adapting of operating rules as infrastructure systems evolve but without adapting to exogenous uncertainties such as those involving filling period droughts fully considering adaptivity would likely change the estimated performance and trade offs in this study given climate change trends for eastern africa aren t clearly wetting or drying conway 2017 and that energy demands are largely unmet arsano and tamrat 2005 block and strzepek 2012 this may be an acceptable initial simplifying planning assumption other limitations include that the study considers a limited number of performance objectives and ignores the financial potential of peak power energy sales finally although the sizing of reservoirs can also be optimised within the proposed approach as in geressu and harou 2015 this was not done here to help simplify results including performance measures such as internal rate of return irr or revenue self sufficiency conditions in future studies could suggest designs with a more desirable cashflow e g where the revenue from initial investments could cover part of the cost of subsequent dams alternatively access to capital could be constrained in the search formulation by using 10 hydrologic realizations we are assuming this ensemble is sufficiently large to represent uniformly distributed peaks and droughts that do not bias the investment scheduling optimization over larger numbers of streamflow time series could better represent the plausible hydrologic variability at different periods of the simulated period however the computational requirements of the simulation optimization approach make it currently difficult to apply the multi objective optimization on a large number of streamflow realizations also uncertainty due to climate change can be an important for long term assets block and strzepek 2010 jeuland and whittington 2014 but this not considered in our study which assumes climate stationarity moreover because information on methods used for filling gaps in the observed streamflow series and estimating flow in ungauged catchments is not accessible block and strzepek 2010 alan 2012 nbi entro 2015 results are only indicative and intended to demonstrate the methodology but should not to be taken as prescriptive recommendations given the shortcomings in the current study we recognise the limitations of the results and do not claim our results should directly impact current decision making despite the limitations of the computational infrastructure available for this study we are able to show initial results to demonstrate the formulation works and has the potential to be solved satisfactorily for large problems 6 conclusion this study proposed a multi objective optimization and visual analytics approach to help decision makers consider multiple objectives when deliberating the timings of reservoir system expansion the approach allows decision makers to deliberate on future decisions considering many environmental and societal goals in addition to economic and financial objectives without having to first agree on their monetization results provide the set of efficient pareto optimal system designs i e sequence implementation dates and operating rules of reservoirs at various expansion stages and the performance trade offs they imply the proposed scheduling approach enables planning without having to depend on initial assumptions about filling rate operating rules sequence of implementations and future priorities how reservoirs may be operated in future the approach screens designs by considering the optimally coordinated operation of infrastructures and flexibility of reservoir operation at different expansion stages results show that assessing the scheduling of reservoirs assuming fixed operating rules overestimates the compromises required in agreeing on a rate of dam filling optimizing operating rules for each reservoir and for each unique system expansion stage ensures that impacts of scheduling designs are de linked from the infrastructure choices thereby revealing the best compromise designs i e whether a change in physical or operating rule design is needed to minimise impact and maximise benefits the results showed the possible impact of alternative problem formulations responsiveness of dam management to system expansion on decision support while increasing the detail with which operating rule flexibility is represented is computationally costly it allows a more accurate identification of efficient development plans and reveals less dramatic trade offs between performance objectives of upstream and downstream stakeholders failure to consider the coordinated use of a multiple reservoirs achieved through re operation of rules as the reservoir systems expands under estimates benefits and could lead to sub optimal design recommendations acknowledgements the first author was supported primarily by the engineering and physical sciences research council epsrc grant reference ep j500331 1 uk research and innovation ukri provided support through the global challenge research fund gcrf futuredams project es p011373 1 university college london faculty of engineering also provided support appendix a supplementary data the following is the supplementary data to this article supplementry supplementry appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 04 002 
