index,text
26055,this paper presents a version of swat that uses modflow to simulate groundwater flow and groundwater surface interactions within a watershed system the modeling code is applied to the 470 km2 middle bosque river watershed texas usa to demonstrate accuracy and differences with swat the model is tested against field measured stream discharge groundwater levels and fluctuation patterns runoff baseflow recharge watershed boundary groundwater flow groundwater surface water interactions and evapotranspiration the model captures the dominant baseflow patterns throughout the watershed and the key patterns of water table dynamics and relation to nearby streams whereas annual rates of runoff lateral flow and annual percolation are unchanged from a stand alone swat simulation annual groundwater discharge rates increase by 50 37 mm 54 mm in particular temporal patterns of groundwater discharge are governed by the physically based rise and fall of groundwater near streams rather than storm induced recharge pulses as in the swat simulation keywords swat modflow groundwater surface water interactions watershed modeling 1 introduction the swat soil and water assessment tool watershed model arnold et al 1998 is the most frequently used watershed model in the world with over 3700 published studies since the year 2000 however the lack of hydrologic connectivity between hydrologic response units hrus within subbasins the lack of landscape routing and the inability to model certain anthropogenic features e g irrigation canals animal herds led recently to the development of swat a complete restructuring of the code into module form bieger et al 2017 2019 arnold et al 2018 hrus aquifers channels reservoirs ponds point sources and inlets are designated as separate spatial objects with their hydrologic interaction defined by the user fig 1 a there are also new spatial objects such as outlets irrigation canals pumps and decision tables for watersheds with complex management schemes subbasins are divided into one or more landscape units e g upland areas and floodplains hrus are defined within the landscape units swat uses routing units to lump hydrographs and route them to any other spatial object by default routing units will be equivalent to landscape units but while landscape units can only contain hrus routing units can consist of any combination of spatial objects however as with the original swat code the swat groundwater module uses simplified flow equations that can yield poor results in groundwater driven watersheds potentially leading to erroneous results when assessing nutrient management practices water rights and ecosystem services the following provides a description of swat s groundwater flow module followed by a discussion of the limitations this module in groundwater driven watersheds and then a brief description of the key differences in swat within swat the water balance for the shallow aquifer is 1 g w s i g w s i 1 q r e c h q p u m p q r e v a p q g w where gw s represents groundwater storage mm in the shallow aquifer i represents time days q represents flow rates and rech pump revap and gw represent recharge groundwater pumping water moving into the soil zone and discharge to the main channel i e baseflow respectively q rech is calculated using an exponential decay weighting function that transfers percolation from the base of the soil profile to the water table based on a specified delay time δ gw days that accounts for the thickness and transmissive properties of the vadose zone material 2 q r e c h i 1 e 1 δ g w q p e r c e 1 δ g w q r e c h i 1 where q perc is the soil water leaving the soil profile mm i represents the current day i 1 represents the previous day q gw for each hru is calculated using simplified equations of groundwater flow and water table fluctuation the change in groundwater flow q gw and hence baseflow is calculated by inserting a water table fluctuation equation into a steady state groundwater flow equation neitsch et al 2011 3 d q g w d t 8000 k l g w 2 q r e c h q g w 800 s y where k is hydraulic conductivity of the aquifer material mm day l gw is the distance m between the groundwater divide and the main channel within the subbasin and s y is the specific yield of the aquifer material which is the ratio of the volume of water that drains by gravity to the total bulk volume of the porous medium therefore the second parenthetic term represents the change in water table height due to an increase in water depth recharge groundwater flow and s y of the porous media the second term therefore represents the driving force of groundwater flow and the first parenthetic term represents the transmissive properties of the aquifer the first parenthetic term and s y are combined to yield the baseflow recession constant α gw which relates a recharge event to baseflow 4 d q g w d t 10 k l g w 2 s y q r e c h q g w α g w q r e c h q g w this equation is integrated to solve for q gw 5 q g w i q g w i 1 exp α g w q r e c h 1 exp α g w this equation is used only if groundwater storage exceeds a user specified storage value the use of the above equations results in the following assumptions in the application of swat to groundwater systems time dependent water table fluctuation is simulated but within the context of steady state groundwater flow therefore a new steady state condition is assumed to occur instantaneously following a change in water table height groundwater flow is based on the assumption of a groundwater divide within the hru which may not actually be present in the real system groundwater flow to streams only occurs if groundwater storage exceeds a storage threshold in reality groundwater flow to streams occurs if water table elevation is higher than river stage also the flow of stream water to the aquifer for the condition of lower water table elevation is not simulated variation in groundwater flow is linearly related to the rate of change in water table height this is true only if the groundwater hydraulic gradient is linear when in reality the gradient in unconfined aquifers is nonlinear combining all aquifer properties k s y l gw into a single term α gw results in the following additional assumptions α gw is used as a fitting parameter without regard to the actual physical properties of the aquifer each hru aquifer is treated as a homogeneous system in which k and s y do not vary spatially as most swat model applications consider a constant value of α gw for each hru this results in a homogeneous aquifer system for the entire watershed with constant k s y and l gw which often is not realistic the combination of these assumptions often leads to a poor performance of swat in application to watersheds wherein baseflow is a large component of streamflow peterson and hamlet 1998 spruill et al 2000 srivastava et al 2006 gassman et al 2007 deb et al 2019a due to complex rainfall runoff relationships deb et al 2019b in recognition of this weakness researchers have provided modified versions of swat that more closely align groundwater flow simulation with groundwater flow theory several studies perkins and sophocleous 1999 galbiati et al 2006 kim et al 2008 guzman et al 2015 deb et al 2019a have published versions of swat that link with modflow harbaugh 2005 niswonger et al 2011 a physically based spatially distributed three dimensional groundwater flow model typically recharge is passed from hrus to modflow grid cells and then modflow simulated stream aquifer exchange rates are passed to swat subbasin channels for water routing the most recent version of swat modflow is provided by bailey et al 2016 with this version being used worldwide to improve simulation of streamflow and groundwater surface water interaction assess watershed water resources quantify impact of pumping on streamflow and quantify impact of climate change semiromi and koch 2019 gao et al 2019 chunn et al 2019 aliyari et al 2019 molina navarro et al 2019 wei and bailey 2019 other swat versions include swat lud sun et al 2016 which modifies the swat landscape unit code to include darcy s law to simulate exchange rates between the river and the aquifer and the multicell aquifer approach of nguyen and dietrich 2018 in which flow between cells within the regional aquifer system is simulated using darcy s law groundwater flow in swat is simulated using the same methods as in swat with the exception that groundwater can be passed from one aquifer to another within the same subbasin see fig 1a and aquifers are no longer tied to hrus i e the number of aquifers and their interaction with other spatial objects are defined by the user as groundwater is passed from one aquifer to another in its flow path to the stream a regional flow pattern can be simulated within a subbasin however this transfer of water is not governed by groundwater hydraulic gradient but rather by a simple passing of flow rates as swat uses the same groundwater flow equations as swat the weaknesses inherent in swat regarding groundwater flow will persist in swat modeling applications however no modifications to the swat groundwater flow theory have yet been made as a growing number of watershed modelers rely on swat to fulfill research project objectives there is a need for physically based spatially distributed groundwater flow modeling to be imbedded within the swat modeling code the objective of this paper is to present a new version of swat that uses modflow as a groundwater flow simulator based on the framework of swat modflow bailey et al 2016 this new version of swat imbeds modflow codes within the main modeling code calling them as subroutines in place of the original swat groundwater module hydrologic values are passed between hrus grid cells and stream channels with modflow river cells treated as spatial objects that can exchange water with swat stream channels the official swat modeling code available at https swat tamu edu software plus includes the modflow framework presented in this paper the model will be referred to as swat modflow throughout the remainder of this paper swat modflow is applied to the middle bosque river watershed mbrw in central texas usa as a demonstration case study due to significant groundwater surface water exchange rates and is tested against a suite of hydrologic response variables within the watershed the remainder of the paper is organized as follows section 2 description of the mbrw to provide context for the explanation of swat modflow theory section 3 description of swat modflow theory section 4 model application to the mbrw section 5 summary and conclusions 2 study watershed middle bosque river watershed texas the middle bosque river watershed mbrw is a 470 km2 watershed located in central texas usa covering portions of mclennan bosque and coryell counties fig 2 a the middle bosque river is a tributary to the bosque river which encompasses an area of 4300 km2 that drains to lake waco the elevation of the study region varies from 367 m at the highest point on the northwestern edge to 161 m on the eastern edge the climate of the study area is characterized as semi arid with long hot summers and brief mild winters the warmest month august has an average maximum temperature of 36 5 c while the coldest month january has an average minimum temperature of 2 8 c the annual precipitation ranges from 602 to 1560 mm year with an average annual depth of 800 mm myrick 1989 the wettest months of a normal year are may and october most rainfall occurs as severe thunderstorm events in the summer months causing large differences in recorded depths between stations in the surrounding counties larkin and bomar 1983 land use is predominantly pasture 65 4 of area or cultivation 20 3 of area with minor land covers of forest 8 5 and residential areas 3 2 the geologic units of the region are all sedimentary and cretaceous in age and consist of the middle and upper fredericksburg group overlain by the washita group with the latter comprised of the georgetown formation which consists of the denton fort worth duck creek kiamichi weno mainstreet and pawpaw members see fig 2a the fredericksburg group is comprised of the edwards and comanche peak formations overall the georgetown formation members and the edwards formation comprise the aquifer within the study region hydraulic conductivity k of the surface units is estimated to be between 6 ft day and 800 ft day myrick 1989 the aquifer contains numerous fracture lineations likely due to overburden release and weathering however field studies suggest that groundwater flow is not controlled by preferential flow along fractures due to the extensive network of fractures rather flow direction is governed by topography and groundwater moves as diffuse flow myrick 1989 groundwater levels respond rapidly to rainfall events indicating a short travel time from the ground surface to the water table downdip flow occurs along the eastern edge of the mbrw streamflow is measured at a usgs gage at the outlet of the mbrw with measurements beginning in 1993 myrick 1989 performed a comprehensive field campaign to investigate groundwater surface water interactions in the mbrw measuring stream discharge at 15 locations in january 1988 and groundwater levels in a network of monitoring wells results indicate that regional groundwater level fluctuates about 1 5 m annually returning approximately to the same level each year most areas have groundwater gradients toward the streams which act as discharge points for the aquifer groundwater loss from streambeds is not a major process in the region as upper parts of tributaries discharge groundwater even during dry periods on an annual basis baseflow accounts for more stream discharge than surface runoff with their combined water yield approximately 8 21 of annual rainfall evapotranspiration accounts for 79 92 of rainfall loss myrick 1989 these data will be used to test the swat modflow model for the mbrw 3 swat modification to include modflow routines the conceptual flow path of swat modflow is shown in fig 1b with the original swat groundwater module replaced by modflow and its associated packages from the modflow nwt niswonger et al 2011 modeling code a complete description of the base swat and modflow models is presented in section 4 1 the process of linking swat and modflow variables is explained within the context of the mbrw with swat routing units swat river network and the modflow finite difference grid of the mbrw shown in fig 3 however the general geoprocessing routines for linking hrus routing units and stream channels to modflow grid cells are the same for any watershed 3 1 mapping swat hru variables to modflow grid cells percolation from the bottom of the soil profile is transferred from swat hrus to modflow grid cells as water table recharge using a delay function equation 2 these cell by cell recharge rates are provided to modflow s recharge package in recognition that et might also occur from shallow groundwater the unsatisfied et potential et actual et from the soil profile as calculated by swat from each hru also is transferred to modflow grid cells with rates provided to modflow s evt package this package requires a potential et rate and an extinction depth for each grid cell with the linkage with swat the potential et rate is set to the unsatisfied et rate and the extinction depth is set by the user in the evt package input file if the water table is below the extinction depth the et flux will be zero if the water table is above the extinction depth the et flux will be linearly interpolated between the unsatisfied et rate and zero in this way the total combined et simulated by swat and modflow is the potential et rate calculated by swat the geographic relationship between hrus and modflow grid cells is demonstrated in fig 4 using routing unit 28 as an example hru 734 is shown in gray which first is separated into individual polygons termed disaggregated hrus or dhrus this is performed to provide a geographic location to the hrus this separation results in 32 distinct dhrus these dhrus are then intersected with the modflow grid cells for example dhru 6354 shown in the upper frame in fig 4 is intersected by 6 grid cells the process of separating hrus to dhrus and of intersecting dhrus to grid cells is performed in a geographic information system such as arcmap or qgis the list of dhrus belonging to each hru is stored in a text file swatmf dhru2hru txt and the list of grid cells belonging to each dhru is stored in a text file swatmf grid2dhru txt the latter also contains the portion of grid cell overlain by the dhru so that weighted values of recharge and unsatisfied et can be computed for each grid cell these linkages are the same as used in the original swat modflow model bailey et al 2016 3 2 mapping swat routing unit variables to modflow river cells in swat modflow bailey et al 2016 the river package of modflow is used to simulate the exchange of water between the aquifer and streams with river cells designated by intersecting the modflow grid with the swat river network darcy s law is used to simulate the rate of exchange with flow from the aquifer to the stream simulated if the water table is higher than the streambed and flow from the stream to the aquifer simulated if the water table is below the streambed the set of river cells in each subbasin is determined by intersecting the river cells with the subbasin map stream stage is provided to each river cell by adding the swat subbasin channel depth to each river cell s bottom elevation upon completion of the daily modflow time step the exchange rate is computed for each river cell and the total rate is summed for each subbasin channel this water volume is then added to the subbasin channel water volume and routed through the channel network using swat s routing algorithms the essence of this linkage between river cells and channels is retained in swat modflow for example 18 river cells are identified in routing unit 28 see fig 4 and the combined exchange between the aquifer and the stream along these 18 cells will be transferred to this channel for routing however to coincide with the modular construction of the swat code the river cells are designated as spatial objects and are connected with swat stream channels swat uses connect files to connect spatial objects such as routing units to channels and a similar file is created for the connection between river cells and channels labeled modflow con this file lists the river cells and their respective connected channels a portion of the connect file for the mbrw is shown in fig 5 the file lists the modflow cell index number for each river cell the number of intersected channels the channel id and the fraction of the river cell overlapped with each routing unit typically a river cell is connected to only one channel however a river cell could intersect multiple routing units at the junction of a tributary and main channel 3 3 data flow within swat modflow the data flow within the swat fortran modeling code is shown in fig 6 original swat subroutines are shaded in blue and added subroutines for modflow linkage are shaded in red the swat modflow linkage is initialized and the linkage files are read into memory whereupon the simulation time loop commences for each daily time step the hru hru control and routing unit ru control calculations are performed followed by the conversion of swat variables to modflow grid cells and then the call to the modflow groundwater flow equation solver the subroutine smrt conversion2swat calculates the exchange rate between the aquifer and stream for each river cell and writes out groundwater balance components to a file finally the channel control subroutine is called to route water through the channel network 3 4 running a swat modflow simulation a swat modflow simulation requires the swat input files the hru dhru and dhru grid linkage files the modflow con connection file and the modflow input files as the modflow linkage is provided in the official swat published code it is available as open access on the swat model website https swat tamu edu software plus 4 model application to the middle bosque river watershed 4 1 model construction and simulation 4 1 1 swat model preparation of the swat modflow model for the mbrw includes the construction of a swat model and a modflow model whereupon the linkage can be performed the following provides a description of the swat model construction and initial calibration the watershed delineation for the mbrw was performed using a 30 m dem from the national elevation dataset u s geological survey and resulted in 69 subbasins the subbasins were not subdivided into upland and floodplain areas so there was one landscape unit per subbasin routing units were defined equivalent to landscape units land use data with a resolution of 30 m 30 m was obtained from the u s geological survey national land cover data and soil data with the same resolution from the natural resources conservation service of the united states department of agriculture usda nrcs ssurgo data sources are indicated in table 1 only one slope class was used and no thresholds were applied during hru definition to provide a complete cover of the watershed area the overlay of land use and soil maps resulted in the definition of 1693 hrus a corn winter wheat rotation was implemented for all agricultural fields in the watershed all areas that were identified in the land use or the soil map as water were defined as ponds in swat climate data measured at the noaa weather station usc00415757 in mcgregor were used to drive the model daily streamflow data at the main watershed outlet were obtained from the u s geological survey database and used for calibrating and testing the stand alone swat model pest parameter estimation tools doherty 2006 were used to perform automatic calibration for model parameters the model was calibrated for the years 1993 through 2005 and tested for the years 2006 through 2012 the first 5 years of simulation were used as a five year warm up period 10 parameters were included in the calibration process the alpha factor for the groundwater recession curve alpha time required for water leaving the bottom of the root zone to reach the shallow aquifer delay curve number for all land use types cn2 available water capacity of all soils and soil layers awc soil evaporation compensation factor esco plant uptake compensation factor epco minimum aquifer storage to allow return flow flo min threshold depth of water in the shallow aquifer for revap or percolation to the deep aquifer to occur revap min revap of percolation coefficient revap co and the saturated hydraulic conductivity of all soils and soil layers k 4 1 2 modflow model the spatial extent of the modflow model is shown in fig 3 the finite difference grid contains 268 columns and 154 rows of square cells with each cell measuring 150 m a side grid cells are specified as active if they are within the watershed boundary and inactive otherwise the model has five layers discretized to encompass the geologic units the description of model layers thicknesses and geologic units is shown in fig 2b the top 3 layers represent georgetown limestone and include various geologic units kc comanche peak kdfdc denton fort worth and duck creek kked kiamichi edwards kms mainstreet kpw pawpaw weno qal alluvium and qt terrace deposits these geologic units are associated with flat lying uplands overlying upper georgetown formations the 4th layer of the model represents edwards limestone with thickness up to 35 feet and the bottom layer represents comanche peak limestone with thickness from 50 to 100 feet cannata 1988 pearson 2013 park 2018 the hydraulic properties of these units included in the upstream weighting upw package of modflow nwt are summarized in table 2 the texas geologic map pearson 2013 and published estimates cannata 1988 park 2018 were used to assign a parameter distribution to the five layers grid cells representing confined units were assigned values of specific storage all other grid cells were assigned values of specific storage and specific yield intersection between the grid cells and the swat stream network resulted in 1378 river cells river bed elevation for each cell was determined from the dem the river bed conductance was calculated using stream width the length of the stream in the cell and an estimate of river bed hydraulic conductivity 3 6 m day resulting in values that ranged from 1 7 m2 day to 395 m2 day with an average of 44 8 m2 day river cells were divided into three groups based on location within the watershed upper region middle region lower region with each region assigned a value of hydraulic conductivity for calibration cells along the eastern boundary of the grid were designated as constant head cells to simulate down dip flow i e groundwater that leaves the watershed boundary due to regional groundwater flow patterns these cells are highlighted in red in fig 3 a map of initial groundwater head values for january 1 1980 was created by interpolating groundwater levels between 28 observation well locations using the regionalized variable theory rvt matheron 1963 an initial calibration was performed using the pest software doherty 2006 to provide the best match between the simulated groundwater head values and the interpolated map river bed conductance k of each geologic unit and s y of each geologic unit were used as parameters during the calibration process final parameter values are shown in table 2 4 1 3 linking swat and modflow as noted in section 4 1 2 the intersection between modflow grid cells and swat channels resulted in 1378 river cells therefore there are 1378 river cell spatial objects listed in the modflow con input file with many of these objects connected to multiple swat channels disaggregating the 1721 hrus resulted in 12610 dhrus which were then intersected with the modflow grid cells 4 1 4 simulations and testing the swat modflow simulation was run for 1980 2012 with the first 5 years used as a warm up period groundwater delay was set to 5 days for each hru to preserve the observed rapid recharge following rainfall events in the mbrw cannata 1988 myrick 1989 beyond the initial calibration of the stand alone swat and modflow models no additional calibration was performed on the coupled swat modflow model whereas the stand alone swat model required 3 min to complete the simulation swat modflow required 7 h due to the daily solving of the groundwater flow equation for each grid cell all simulations were run on a desktop intel core i7 3770 cpu 3 40 ghz 16 0 gb ram a comprehensive test of the model was pursued in which a suite of observed point level and watershed level system response variable data were compared with model output the following data were used in the testing process watershed wide water balance fluxes discharge runoff baseflow et recharge down dip flow from 5 years daily stream discharge at the watershed outlet for 1993 2012 groundwater head at observation wells during 1988 1989 regional patterns of groundwater fluctuation during 1988 1989 the relationship between contributing watershed area and stream discharge and overall pattern of groundwater surface water interactions i e the predominance of streams acting as groundwater discharge points the exact geographic location of the observation wells is not provided in myrick 1989 therefore the location on the modflow grid cell map was approximated based on the map provided in myrick 1989 in general the magnitude and fluctuation of groundwater levels are very sensitive to location with levels varying by meters over a horizontal distance of hundreds of meters as a result comparison between simulated and observed groundwater levels focuses more on regional groundwater flow patterns and fluctuations rather than point values due to the often significant impact of groundwater level on watershed response the impact of groundwater delay i e the timing of recharge to the water table and extinction depth exdp on hydrologic responses was quantified using sensitivity analysis the method of morris a global sensitivity analysis method was used to quantify the average effect of these two parameters on the following hydrologic responses groundwater head stream discharge surface runoff lateral flow et recharge rate groundwater discharge to streams stream seepage to the aquifer and shallow groundwater et using this method the global sensitivity index is calculated as the average of a set of elementary effects of the parameter on the system response i e the change in the response value due to a change in the parameter value 4 2 results and discussion 4 2 1 watershed water balance the average annual fluxes for each hydrologic pathway in the watershed during the 1985 2012 time period are shown in fig 7 a average rainfall is 951 mm of which 762 mm are lost to et from the soil profile and 31 mm are lost to et from shallow groundwater for total et of 793 mm percolation and recharge are both 99 mm lateral flow to streams is 49 mm and runoff is 29 mm groundwater surface water interactions are mainly in the form of groundwater discharge to streams 54 mm compared to 2 mm of stream seepage which is in agreement with field conditions observed in the mbrw myrick 1989 down dip flow i e groundwater outflow along the eastern boundary of the watershed is 11 mm these results compare very well with observed watershed level data hydrologic fluxes as a percentage of annual rainfall were estimated by myrick 1989 for the year 1985 with results shown in fig 7b in comparison with results from swat modflow as seen from fig 7b simulated and observed values match very well for example discharge is estimated to be 12 and 14 of rainfall based on field observations and model results and et is estimated to be 84 and 83 from these results we conclude that the model is able to capture the general hydrologic flux patterns of the actual system the agreement between field measurements and model output is further demonstrated by a comparison of general relationships between rainfall and the hydrologic pathways watershed wide estimates of hydrologic fluxes as a percentage of annual rainfall similar to those shown in fig 7b are also available for years 1966 1972 1976 and 1978 myrick 1989 while the model is not run for these earlier time periods and thus a direct comparison is not performed for these years the general relationship between rainfall and hydrologic pathway fluxes can be compared these relationships are shown in fig 8 for runoff baseflow et discharge recharge and down dip flow down dip flow is available only for 1985 in general the model results follow the temporal pattern of the field estimates for example higher annual rainfall results in a smaller percentage attributed to et and a higher percentage for stream discharge and recharge annual rainfall depths 700 mm result in approximately the same baseflow portion between 5 and 10 the model is also able to capture the general relationship between contributing drainage area km2 and stream discharge m3 sec stream discharge was measured at 15 locations on january 16 1988 and a drainage area was estimated for each location with the trend shown in fig 9 the trend between discharge and drainage area is also plotted for the 69 routing units of swat modflow for the same day of the simulation with a general agreement between field estimated and modeled values one discrepancy is a downstream location with a drainage area of 11 km2 with a recorded discharge of 0 0017 m3 s the simulated discharge value for this point in the watershed is 0 05 m3 s a difference of 0 048 m3 s another discrepancy is the downstream location with a 147 km2 drainage area with a recorded discharge value of 0 11 m3 s and a simulated value of 0 77 m3 s overall the model is able to capture a daily snapshot of stream discharge patterns throughout the watershed indicating accuracy in both surface runoff and groundwater discharge rates 4 2 2 outlet streamflow observed and simulated daily stream discharge is shown in fig 10 a for 1993 2012 with a more detailed presentation of individual storm hydrographs shown in fig 10b results for both swat and swat modflow are shown in general both models over predict streamflow after a storm event with swat modflow yielding higher values the nse coefficient is 0 207 for swat and 0 212 for swat modflow indicating poor model performance however these results likely are due to insufficient accuracy in weather forcing data rainfall temperature rather than an inherent error in the model as seen in fig 10a many of the hydrograph peaks are matched by the models but in some instances the models grossly under predict peak flow as indicated by the storm events circled in dotted orange ovals according to myrick 1989 most rainfall occurs during large summer thunderstorm events resulting in a high degree of spatial variation in recorded precipitation between weather stations in addition the large storm events evidenced by the high peak flows in the hydrograph in fig 10a do not have corresponding high rainfall depths in the weather station data used to force the model as only one weather station is available for the mbrw it seems that either this one station is not representative of the entire watershed or else data for large thunderstorms were mis recorded at the weather station as the point of this paper is to introduce the swat modflow modeling code and since the model is tested against several other watershed hydrologic responses the mis match between observed and simulated stream discharge for several large storm events does not seem to be a serious issue 4 2 3 groundwater groundwater results are presented in figs 11 13 fig 11 shows watershed wide groundwater results average groundwater head is shown for each grid cell in fig 11a with the contours indicating flow towards the main stem of the middle bosque river and in general parallel to the topography as found by myrick 1989 the depth to water table is shown in fig 11b showing generally shallow water tables along the stream network and hence a condition for significant groundwater discharge to the streams the increase in water table elevation from 1987 to 1988 is shown in fig 11c this time period coincides with the field study of myrick 1989 in which he observed that groundwater levels return approximately to the same level each year the map in fig 11c indicates that for the vast majority of the watershed simulated water table returns to approximately the same elevation after one year agreeing with the observation of myrick 1989 the map of average annual difference between maximum water table elevation and minimum water table elevation fig 11d shows that year to year fluctuations of water table elevation are within 1 5 m again in agreement with the observation of myrick 1989 fig 12 shows the comparison between simulated and observed groundwater head at four observation wells showing that although magnitudes have errors of several meters the fluctuation pattern is similar again exact locations of observation wells are not known and therefore the comparison method is inaccurate the overall groundwater balance from 1985 to 2012 is shown in fig 13 with individual balance components shown in fig 13a recharge fluctuates with rainfall patterns and groundwater et is highest during summer months due to high water tables caused by summer rainfall events conversely groundwater discharge is relatively constant due to the slow movement of groundwater and its response to recharge events fig 13b shows the temporal pattern of groundwater storage from 1985 to 2012 as groundwater discharge groundwater et and head boundary fluxes are relatively constant from year to year fig 13a the change in groundwater storage is due to mainly to changes in recharge this is seen clearly for the time period of 2000 2007 in which the gradual rise in groundwater storage corresponds to a period of consistent recharge fig 13a periods of storage decline are due to low recharge rates in the summer months e g 1996 when groundwater et is the highest there are no year to year field data to test the results shown in fig 13 but the accuracy of watershed wide fluxes shown in figs 7 and 8 particularly recharge groundwater discharge baseflow and head boundary discharge down dip flow yield confidence to the results shown in fig 13 4 2 4 groundwater surface water interactions the overall net exchange between the aquifer and streams was shown in fig 7a with annual averages of 54 mm of groundwater discharge and 2 mm of stream seepage however clarifying spatial patterns of groundwater surface interaction can be helpful for understanding patterns of contaminant transport and guiding implementation of best management practices bmps for nutrients and ecosystem health the average daily groundwater discharge m3 day to streams is shown in fig 14 a for each of the 69 routing units although there are several subbasins in which the overall net exchange between the aquifer and streams is stream seepage denoted by yellow polygons these are for upstream routing units and the seepage rates are small 400 m3 day compared to the groundwater discharge rates of the other routing units up to 9000 m3 day although reach by reach exchange rates and patterns have not been measured in the field results overall coincide with the conclusion of myrick 1989 that groundwater discharge is a dominant process in the mbrw as streams act as discharge points for groundwater throughout the watershed results from the swat model fig 14b indicate that stream seepage is never the dominant exchange process for any routing unit and that simulating groundwater flow in a simplified fashion as outlined in section 1 leads to vastly different results than using physically based spatially distributed groundwater modeling this is further demonstrated with trends between annual rainfall mm and the hydrologic pathways runoff lateral flow percolation groundwater discharge for each year of the simulation shown by plots in fig 15 for both swat and swat modflow for runoff lateral flow latq and percolation results are identical as implementing modflow does not affect land surface and soil hydrological processes however results for groundwater discharge gwq are vastly different most interesting there is a clear positive relationship between rainfall and gwq for swat as swat discharge is governed by recharge pulses see equations 2 5 the standard deviation of annual gwq values for swat is 10 3 and the trend has an r2 value of 0 54 conversely swat modflow discharge is not governed by recharge pulses but rather by the slow rise or fall of the water table which dictates groundwater gradients to streams and hence the volume of groundwater discharged to the stream channels there is therefore a much stronger auto correlation between year to year values the standard deviation of gwq annual values for swat modflow is 1 9 and the r2 value is only 0 074 indicating a slight dependence on annual rainfall these results highlight the principal difference between swat and swat modflow the latter simulates groundwater flow in a spatially distributed manner that adheres to the transient groundwater flow equation flow rates and residence times in the aquifer are governed by spatially distributed hydraulic conductivity specific yield and recharge therefore if a watershed has a water yield of which groundwater discharge is a significant portion then swat modflow can be a useful tool to accurately simulate flow rates and groundwater surface water exchange this can be especially important for watersheds in which groundwater supply or the effect of bmps on contaminant mass transport and loading are to be estimated if groundwater residence times and discharge rates are simulated incorrectly then the simulated effects of bmps will be wrong this is particularly true for land management bmps that affect groundwater contaminants such as fertilizer control these bmps will have an effect on river water quality only as quickly as groundwater reaches the stream which may be years to decades using a watershed model that does not simulate spatially distributed groundwater flow may under predict groundwater travel time and therefore under predict the time to achieve higher stream water quality 4 2 5 sensitivity of system response to groundwater parameters since groundwater delay gw delay and the extinction depth exdp of groundwater et have a range of possible values a sensitivity analysis for these parameters can provide guidance as to their importance in controlling hydrologic pathways e g groundwater discharge gwq and groundwater et gwet fig 16 shows the results of applying the morris method to determine the effect of these two parameters on a suite of watershed response variables as expected gw delay has the strongest influence on recharge followed by gwq water yield gwet and average stream discharge at the outlet exdp has the strongest influence on gwet followed by gwq water yield and average stream discharge at the outlet they have a minimal influence on average groundwater head and none on land surface hydrology plots of trends between these two parameters and gwq and gwet are shown on the right of fig 16 as seen in the trend between gw delay and gwq there is a significant relationship when gw delay is increased to years however between gw delay values of 0 and 100 days the value of gwq decreases from 1110 mm to 1100 mm a decrease of only 0 9 if gw delay is increased to 365 days gwq decreases to 1072 mm a decrease of only 3 4 a similar pattern occurs for gw et with a 0 5 and 3 decrease for gw delay values of 100 days and 365 days respectively therefore at least for the mbrw the choice of gw delay does not influence the overall watershed response results for watersheds with a thick vadose zone however wherein percolate may take years to decades to reach the water table gwq will be affected significantly exdp has a strong influence on gwet and gwq highlighting the importance of having good estimates of rooting depths for crops and vegetation which can transpire groundwater if the water table is shallow 5 conclusions this paper presents an updated version of swat that includes modflow to simulate groundwater flow and groundwater surface water interactions swat provides recharge and stream stage to modflow and modflow provides spatial values of groundwater head groundwater flow rate and exchange rates of groundwater and surface water that are provided to swat stream channels for stream routing the swat modflow model was applied to the 470 km2 middle bosque river watershed mbrw in central texas usa wherein groundwater makes up a significant portion of watershed water yield the model was tested against a suite of field measurements annual average hydrologic fluxes stream discharge runoff baseflow down dip flow et and recharge trends between annual rainfall rates and hydrologic fluxes stream discharge at the watershed outlet groundwater head and water table fluctuations and groundwater surface water interactions simulated results agree well with observed values in each case swat modflow can be used to simulate all major hydrologic pathways in a watershed using swat capability for land surface hydrology and hydrologic connections and modflow for groundwater residence time and flow rates the use of swat modflow also allows for the mapping of water table elevation water table depth and stream aquifer exchange rates for all grid cells in the model domain the model provides an excellent tool to investigate water supply and the impact of conservation practices and climate change for watersheds wherein baseflow is a significant portion of streamflow this is particularly true for management practices that effect groundwater quality as years to decades may be required for the effect to reach the stream due to the often low groundwater flow rates and associated long aquifer residence time software availability the swat modflow code is the official swat code on the swat model website https swat tamu edu software plus it is open source and available for download and code modification language fortran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the united state department of agriculture agricultural research service through cooperative agreement number 59 3098 8 002 author contributions r b and s p modified the swat code to include modflow routines with the help of j a k b and s p created and calibrated the swat model for the middle bosque river watershed mbrw s p prepared and calibrated the modflow model for the mbrw r b applied swat modflow to the mbrw and ran all simulations p a provided expertise in field data for the mbrw and comparison methods between swat modflow and field data appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104660 
26055,this paper presents a version of swat that uses modflow to simulate groundwater flow and groundwater surface interactions within a watershed system the modeling code is applied to the 470 km2 middle bosque river watershed texas usa to demonstrate accuracy and differences with swat the model is tested against field measured stream discharge groundwater levels and fluctuation patterns runoff baseflow recharge watershed boundary groundwater flow groundwater surface water interactions and evapotranspiration the model captures the dominant baseflow patterns throughout the watershed and the key patterns of water table dynamics and relation to nearby streams whereas annual rates of runoff lateral flow and annual percolation are unchanged from a stand alone swat simulation annual groundwater discharge rates increase by 50 37 mm 54 mm in particular temporal patterns of groundwater discharge are governed by the physically based rise and fall of groundwater near streams rather than storm induced recharge pulses as in the swat simulation keywords swat modflow groundwater surface water interactions watershed modeling 1 introduction the swat soil and water assessment tool watershed model arnold et al 1998 is the most frequently used watershed model in the world with over 3700 published studies since the year 2000 however the lack of hydrologic connectivity between hydrologic response units hrus within subbasins the lack of landscape routing and the inability to model certain anthropogenic features e g irrigation canals animal herds led recently to the development of swat a complete restructuring of the code into module form bieger et al 2017 2019 arnold et al 2018 hrus aquifers channels reservoirs ponds point sources and inlets are designated as separate spatial objects with their hydrologic interaction defined by the user fig 1 a there are also new spatial objects such as outlets irrigation canals pumps and decision tables for watersheds with complex management schemes subbasins are divided into one or more landscape units e g upland areas and floodplains hrus are defined within the landscape units swat uses routing units to lump hydrographs and route them to any other spatial object by default routing units will be equivalent to landscape units but while landscape units can only contain hrus routing units can consist of any combination of spatial objects however as with the original swat code the swat groundwater module uses simplified flow equations that can yield poor results in groundwater driven watersheds potentially leading to erroneous results when assessing nutrient management practices water rights and ecosystem services the following provides a description of swat s groundwater flow module followed by a discussion of the limitations this module in groundwater driven watersheds and then a brief description of the key differences in swat within swat the water balance for the shallow aquifer is 1 g w s i g w s i 1 q r e c h q p u m p q r e v a p q g w where gw s represents groundwater storage mm in the shallow aquifer i represents time days q represents flow rates and rech pump revap and gw represent recharge groundwater pumping water moving into the soil zone and discharge to the main channel i e baseflow respectively q rech is calculated using an exponential decay weighting function that transfers percolation from the base of the soil profile to the water table based on a specified delay time δ gw days that accounts for the thickness and transmissive properties of the vadose zone material 2 q r e c h i 1 e 1 δ g w q p e r c e 1 δ g w q r e c h i 1 where q perc is the soil water leaving the soil profile mm i represents the current day i 1 represents the previous day q gw for each hru is calculated using simplified equations of groundwater flow and water table fluctuation the change in groundwater flow q gw and hence baseflow is calculated by inserting a water table fluctuation equation into a steady state groundwater flow equation neitsch et al 2011 3 d q g w d t 8000 k l g w 2 q r e c h q g w 800 s y where k is hydraulic conductivity of the aquifer material mm day l gw is the distance m between the groundwater divide and the main channel within the subbasin and s y is the specific yield of the aquifer material which is the ratio of the volume of water that drains by gravity to the total bulk volume of the porous medium therefore the second parenthetic term represents the change in water table height due to an increase in water depth recharge groundwater flow and s y of the porous media the second term therefore represents the driving force of groundwater flow and the first parenthetic term represents the transmissive properties of the aquifer the first parenthetic term and s y are combined to yield the baseflow recession constant α gw which relates a recharge event to baseflow 4 d q g w d t 10 k l g w 2 s y q r e c h q g w α g w q r e c h q g w this equation is integrated to solve for q gw 5 q g w i q g w i 1 exp α g w q r e c h 1 exp α g w this equation is used only if groundwater storage exceeds a user specified storage value the use of the above equations results in the following assumptions in the application of swat to groundwater systems time dependent water table fluctuation is simulated but within the context of steady state groundwater flow therefore a new steady state condition is assumed to occur instantaneously following a change in water table height groundwater flow is based on the assumption of a groundwater divide within the hru which may not actually be present in the real system groundwater flow to streams only occurs if groundwater storage exceeds a storage threshold in reality groundwater flow to streams occurs if water table elevation is higher than river stage also the flow of stream water to the aquifer for the condition of lower water table elevation is not simulated variation in groundwater flow is linearly related to the rate of change in water table height this is true only if the groundwater hydraulic gradient is linear when in reality the gradient in unconfined aquifers is nonlinear combining all aquifer properties k s y l gw into a single term α gw results in the following additional assumptions α gw is used as a fitting parameter without regard to the actual physical properties of the aquifer each hru aquifer is treated as a homogeneous system in which k and s y do not vary spatially as most swat model applications consider a constant value of α gw for each hru this results in a homogeneous aquifer system for the entire watershed with constant k s y and l gw which often is not realistic the combination of these assumptions often leads to a poor performance of swat in application to watersheds wherein baseflow is a large component of streamflow peterson and hamlet 1998 spruill et al 2000 srivastava et al 2006 gassman et al 2007 deb et al 2019a due to complex rainfall runoff relationships deb et al 2019b in recognition of this weakness researchers have provided modified versions of swat that more closely align groundwater flow simulation with groundwater flow theory several studies perkins and sophocleous 1999 galbiati et al 2006 kim et al 2008 guzman et al 2015 deb et al 2019a have published versions of swat that link with modflow harbaugh 2005 niswonger et al 2011 a physically based spatially distributed three dimensional groundwater flow model typically recharge is passed from hrus to modflow grid cells and then modflow simulated stream aquifer exchange rates are passed to swat subbasin channels for water routing the most recent version of swat modflow is provided by bailey et al 2016 with this version being used worldwide to improve simulation of streamflow and groundwater surface water interaction assess watershed water resources quantify impact of pumping on streamflow and quantify impact of climate change semiromi and koch 2019 gao et al 2019 chunn et al 2019 aliyari et al 2019 molina navarro et al 2019 wei and bailey 2019 other swat versions include swat lud sun et al 2016 which modifies the swat landscape unit code to include darcy s law to simulate exchange rates between the river and the aquifer and the multicell aquifer approach of nguyen and dietrich 2018 in which flow between cells within the regional aquifer system is simulated using darcy s law groundwater flow in swat is simulated using the same methods as in swat with the exception that groundwater can be passed from one aquifer to another within the same subbasin see fig 1a and aquifers are no longer tied to hrus i e the number of aquifers and their interaction with other spatial objects are defined by the user as groundwater is passed from one aquifer to another in its flow path to the stream a regional flow pattern can be simulated within a subbasin however this transfer of water is not governed by groundwater hydraulic gradient but rather by a simple passing of flow rates as swat uses the same groundwater flow equations as swat the weaknesses inherent in swat regarding groundwater flow will persist in swat modeling applications however no modifications to the swat groundwater flow theory have yet been made as a growing number of watershed modelers rely on swat to fulfill research project objectives there is a need for physically based spatially distributed groundwater flow modeling to be imbedded within the swat modeling code the objective of this paper is to present a new version of swat that uses modflow as a groundwater flow simulator based on the framework of swat modflow bailey et al 2016 this new version of swat imbeds modflow codes within the main modeling code calling them as subroutines in place of the original swat groundwater module hydrologic values are passed between hrus grid cells and stream channels with modflow river cells treated as spatial objects that can exchange water with swat stream channels the official swat modeling code available at https swat tamu edu software plus includes the modflow framework presented in this paper the model will be referred to as swat modflow throughout the remainder of this paper swat modflow is applied to the middle bosque river watershed mbrw in central texas usa as a demonstration case study due to significant groundwater surface water exchange rates and is tested against a suite of hydrologic response variables within the watershed the remainder of the paper is organized as follows section 2 description of the mbrw to provide context for the explanation of swat modflow theory section 3 description of swat modflow theory section 4 model application to the mbrw section 5 summary and conclusions 2 study watershed middle bosque river watershed texas the middle bosque river watershed mbrw is a 470 km2 watershed located in central texas usa covering portions of mclennan bosque and coryell counties fig 2 a the middle bosque river is a tributary to the bosque river which encompasses an area of 4300 km2 that drains to lake waco the elevation of the study region varies from 367 m at the highest point on the northwestern edge to 161 m on the eastern edge the climate of the study area is characterized as semi arid with long hot summers and brief mild winters the warmest month august has an average maximum temperature of 36 5 c while the coldest month january has an average minimum temperature of 2 8 c the annual precipitation ranges from 602 to 1560 mm year with an average annual depth of 800 mm myrick 1989 the wettest months of a normal year are may and october most rainfall occurs as severe thunderstorm events in the summer months causing large differences in recorded depths between stations in the surrounding counties larkin and bomar 1983 land use is predominantly pasture 65 4 of area or cultivation 20 3 of area with minor land covers of forest 8 5 and residential areas 3 2 the geologic units of the region are all sedimentary and cretaceous in age and consist of the middle and upper fredericksburg group overlain by the washita group with the latter comprised of the georgetown formation which consists of the denton fort worth duck creek kiamichi weno mainstreet and pawpaw members see fig 2a the fredericksburg group is comprised of the edwards and comanche peak formations overall the georgetown formation members and the edwards formation comprise the aquifer within the study region hydraulic conductivity k of the surface units is estimated to be between 6 ft day and 800 ft day myrick 1989 the aquifer contains numerous fracture lineations likely due to overburden release and weathering however field studies suggest that groundwater flow is not controlled by preferential flow along fractures due to the extensive network of fractures rather flow direction is governed by topography and groundwater moves as diffuse flow myrick 1989 groundwater levels respond rapidly to rainfall events indicating a short travel time from the ground surface to the water table downdip flow occurs along the eastern edge of the mbrw streamflow is measured at a usgs gage at the outlet of the mbrw with measurements beginning in 1993 myrick 1989 performed a comprehensive field campaign to investigate groundwater surface water interactions in the mbrw measuring stream discharge at 15 locations in january 1988 and groundwater levels in a network of monitoring wells results indicate that regional groundwater level fluctuates about 1 5 m annually returning approximately to the same level each year most areas have groundwater gradients toward the streams which act as discharge points for the aquifer groundwater loss from streambeds is not a major process in the region as upper parts of tributaries discharge groundwater even during dry periods on an annual basis baseflow accounts for more stream discharge than surface runoff with their combined water yield approximately 8 21 of annual rainfall evapotranspiration accounts for 79 92 of rainfall loss myrick 1989 these data will be used to test the swat modflow model for the mbrw 3 swat modification to include modflow routines the conceptual flow path of swat modflow is shown in fig 1b with the original swat groundwater module replaced by modflow and its associated packages from the modflow nwt niswonger et al 2011 modeling code a complete description of the base swat and modflow models is presented in section 4 1 the process of linking swat and modflow variables is explained within the context of the mbrw with swat routing units swat river network and the modflow finite difference grid of the mbrw shown in fig 3 however the general geoprocessing routines for linking hrus routing units and stream channels to modflow grid cells are the same for any watershed 3 1 mapping swat hru variables to modflow grid cells percolation from the bottom of the soil profile is transferred from swat hrus to modflow grid cells as water table recharge using a delay function equation 2 these cell by cell recharge rates are provided to modflow s recharge package in recognition that et might also occur from shallow groundwater the unsatisfied et potential et actual et from the soil profile as calculated by swat from each hru also is transferred to modflow grid cells with rates provided to modflow s evt package this package requires a potential et rate and an extinction depth for each grid cell with the linkage with swat the potential et rate is set to the unsatisfied et rate and the extinction depth is set by the user in the evt package input file if the water table is below the extinction depth the et flux will be zero if the water table is above the extinction depth the et flux will be linearly interpolated between the unsatisfied et rate and zero in this way the total combined et simulated by swat and modflow is the potential et rate calculated by swat the geographic relationship between hrus and modflow grid cells is demonstrated in fig 4 using routing unit 28 as an example hru 734 is shown in gray which first is separated into individual polygons termed disaggregated hrus or dhrus this is performed to provide a geographic location to the hrus this separation results in 32 distinct dhrus these dhrus are then intersected with the modflow grid cells for example dhru 6354 shown in the upper frame in fig 4 is intersected by 6 grid cells the process of separating hrus to dhrus and of intersecting dhrus to grid cells is performed in a geographic information system such as arcmap or qgis the list of dhrus belonging to each hru is stored in a text file swatmf dhru2hru txt and the list of grid cells belonging to each dhru is stored in a text file swatmf grid2dhru txt the latter also contains the portion of grid cell overlain by the dhru so that weighted values of recharge and unsatisfied et can be computed for each grid cell these linkages are the same as used in the original swat modflow model bailey et al 2016 3 2 mapping swat routing unit variables to modflow river cells in swat modflow bailey et al 2016 the river package of modflow is used to simulate the exchange of water between the aquifer and streams with river cells designated by intersecting the modflow grid with the swat river network darcy s law is used to simulate the rate of exchange with flow from the aquifer to the stream simulated if the water table is higher than the streambed and flow from the stream to the aquifer simulated if the water table is below the streambed the set of river cells in each subbasin is determined by intersecting the river cells with the subbasin map stream stage is provided to each river cell by adding the swat subbasin channel depth to each river cell s bottom elevation upon completion of the daily modflow time step the exchange rate is computed for each river cell and the total rate is summed for each subbasin channel this water volume is then added to the subbasin channel water volume and routed through the channel network using swat s routing algorithms the essence of this linkage between river cells and channels is retained in swat modflow for example 18 river cells are identified in routing unit 28 see fig 4 and the combined exchange between the aquifer and the stream along these 18 cells will be transferred to this channel for routing however to coincide with the modular construction of the swat code the river cells are designated as spatial objects and are connected with swat stream channels swat uses connect files to connect spatial objects such as routing units to channels and a similar file is created for the connection between river cells and channels labeled modflow con this file lists the river cells and their respective connected channels a portion of the connect file for the mbrw is shown in fig 5 the file lists the modflow cell index number for each river cell the number of intersected channels the channel id and the fraction of the river cell overlapped with each routing unit typically a river cell is connected to only one channel however a river cell could intersect multiple routing units at the junction of a tributary and main channel 3 3 data flow within swat modflow the data flow within the swat fortran modeling code is shown in fig 6 original swat subroutines are shaded in blue and added subroutines for modflow linkage are shaded in red the swat modflow linkage is initialized and the linkage files are read into memory whereupon the simulation time loop commences for each daily time step the hru hru control and routing unit ru control calculations are performed followed by the conversion of swat variables to modflow grid cells and then the call to the modflow groundwater flow equation solver the subroutine smrt conversion2swat calculates the exchange rate between the aquifer and stream for each river cell and writes out groundwater balance components to a file finally the channel control subroutine is called to route water through the channel network 3 4 running a swat modflow simulation a swat modflow simulation requires the swat input files the hru dhru and dhru grid linkage files the modflow con connection file and the modflow input files as the modflow linkage is provided in the official swat published code it is available as open access on the swat model website https swat tamu edu software plus 4 model application to the middle bosque river watershed 4 1 model construction and simulation 4 1 1 swat model preparation of the swat modflow model for the mbrw includes the construction of a swat model and a modflow model whereupon the linkage can be performed the following provides a description of the swat model construction and initial calibration the watershed delineation for the mbrw was performed using a 30 m dem from the national elevation dataset u s geological survey and resulted in 69 subbasins the subbasins were not subdivided into upland and floodplain areas so there was one landscape unit per subbasin routing units were defined equivalent to landscape units land use data with a resolution of 30 m 30 m was obtained from the u s geological survey national land cover data and soil data with the same resolution from the natural resources conservation service of the united states department of agriculture usda nrcs ssurgo data sources are indicated in table 1 only one slope class was used and no thresholds were applied during hru definition to provide a complete cover of the watershed area the overlay of land use and soil maps resulted in the definition of 1693 hrus a corn winter wheat rotation was implemented for all agricultural fields in the watershed all areas that were identified in the land use or the soil map as water were defined as ponds in swat climate data measured at the noaa weather station usc00415757 in mcgregor were used to drive the model daily streamflow data at the main watershed outlet were obtained from the u s geological survey database and used for calibrating and testing the stand alone swat model pest parameter estimation tools doherty 2006 were used to perform automatic calibration for model parameters the model was calibrated for the years 1993 through 2005 and tested for the years 2006 through 2012 the first 5 years of simulation were used as a five year warm up period 10 parameters were included in the calibration process the alpha factor for the groundwater recession curve alpha time required for water leaving the bottom of the root zone to reach the shallow aquifer delay curve number for all land use types cn2 available water capacity of all soils and soil layers awc soil evaporation compensation factor esco plant uptake compensation factor epco minimum aquifer storage to allow return flow flo min threshold depth of water in the shallow aquifer for revap or percolation to the deep aquifer to occur revap min revap of percolation coefficient revap co and the saturated hydraulic conductivity of all soils and soil layers k 4 1 2 modflow model the spatial extent of the modflow model is shown in fig 3 the finite difference grid contains 268 columns and 154 rows of square cells with each cell measuring 150 m a side grid cells are specified as active if they are within the watershed boundary and inactive otherwise the model has five layers discretized to encompass the geologic units the description of model layers thicknesses and geologic units is shown in fig 2b the top 3 layers represent georgetown limestone and include various geologic units kc comanche peak kdfdc denton fort worth and duck creek kked kiamichi edwards kms mainstreet kpw pawpaw weno qal alluvium and qt terrace deposits these geologic units are associated with flat lying uplands overlying upper georgetown formations the 4th layer of the model represents edwards limestone with thickness up to 35 feet and the bottom layer represents comanche peak limestone with thickness from 50 to 100 feet cannata 1988 pearson 2013 park 2018 the hydraulic properties of these units included in the upstream weighting upw package of modflow nwt are summarized in table 2 the texas geologic map pearson 2013 and published estimates cannata 1988 park 2018 were used to assign a parameter distribution to the five layers grid cells representing confined units were assigned values of specific storage all other grid cells were assigned values of specific storage and specific yield intersection between the grid cells and the swat stream network resulted in 1378 river cells river bed elevation for each cell was determined from the dem the river bed conductance was calculated using stream width the length of the stream in the cell and an estimate of river bed hydraulic conductivity 3 6 m day resulting in values that ranged from 1 7 m2 day to 395 m2 day with an average of 44 8 m2 day river cells were divided into three groups based on location within the watershed upper region middle region lower region with each region assigned a value of hydraulic conductivity for calibration cells along the eastern boundary of the grid were designated as constant head cells to simulate down dip flow i e groundwater that leaves the watershed boundary due to regional groundwater flow patterns these cells are highlighted in red in fig 3 a map of initial groundwater head values for january 1 1980 was created by interpolating groundwater levels between 28 observation well locations using the regionalized variable theory rvt matheron 1963 an initial calibration was performed using the pest software doherty 2006 to provide the best match between the simulated groundwater head values and the interpolated map river bed conductance k of each geologic unit and s y of each geologic unit were used as parameters during the calibration process final parameter values are shown in table 2 4 1 3 linking swat and modflow as noted in section 4 1 2 the intersection between modflow grid cells and swat channels resulted in 1378 river cells therefore there are 1378 river cell spatial objects listed in the modflow con input file with many of these objects connected to multiple swat channels disaggregating the 1721 hrus resulted in 12610 dhrus which were then intersected with the modflow grid cells 4 1 4 simulations and testing the swat modflow simulation was run for 1980 2012 with the first 5 years used as a warm up period groundwater delay was set to 5 days for each hru to preserve the observed rapid recharge following rainfall events in the mbrw cannata 1988 myrick 1989 beyond the initial calibration of the stand alone swat and modflow models no additional calibration was performed on the coupled swat modflow model whereas the stand alone swat model required 3 min to complete the simulation swat modflow required 7 h due to the daily solving of the groundwater flow equation for each grid cell all simulations were run on a desktop intel core i7 3770 cpu 3 40 ghz 16 0 gb ram a comprehensive test of the model was pursued in which a suite of observed point level and watershed level system response variable data were compared with model output the following data were used in the testing process watershed wide water balance fluxes discharge runoff baseflow et recharge down dip flow from 5 years daily stream discharge at the watershed outlet for 1993 2012 groundwater head at observation wells during 1988 1989 regional patterns of groundwater fluctuation during 1988 1989 the relationship between contributing watershed area and stream discharge and overall pattern of groundwater surface water interactions i e the predominance of streams acting as groundwater discharge points the exact geographic location of the observation wells is not provided in myrick 1989 therefore the location on the modflow grid cell map was approximated based on the map provided in myrick 1989 in general the magnitude and fluctuation of groundwater levels are very sensitive to location with levels varying by meters over a horizontal distance of hundreds of meters as a result comparison between simulated and observed groundwater levels focuses more on regional groundwater flow patterns and fluctuations rather than point values due to the often significant impact of groundwater level on watershed response the impact of groundwater delay i e the timing of recharge to the water table and extinction depth exdp on hydrologic responses was quantified using sensitivity analysis the method of morris a global sensitivity analysis method was used to quantify the average effect of these two parameters on the following hydrologic responses groundwater head stream discharge surface runoff lateral flow et recharge rate groundwater discharge to streams stream seepage to the aquifer and shallow groundwater et using this method the global sensitivity index is calculated as the average of a set of elementary effects of the parameter on the system response i e the change in the response value due to a change in the parameter value 4 2 results and discussion 4 2 1 watershed water balance the average annual fluxes for each hydrologic pathway in the watershed during the 1985 2012 time period are shown in fig 7 a average rainfall is 951 mm of which 762 mm are lost to et from the soil profile and 31 mm are lost to et from shallow groundwater for total et of 793 mm percolation and recharge are both 99 mm lateral flow to streams is 49 mm and runoff is 29 mm groundwater surface water interactions are mainly in the form of groundwater discharge to streams 54 mm compared to 2 mm of stream seepage which is in agreement with field conditions observed in the mbrw myrick 1989 down dip flow i e groundwater outflow along the eastern boundary of the watershed is 11 mm these results compare very well with observed watershed level data hydrologic fluxes as a percentage of annual rainfall were estimated by myrick 1989 for the year 1985 with results shown in fig 7b in comparison with results from swat modflow as seen from fig 7b simulated and observed values match very well for example discharge is estimated to be 12 and 14 of rainfall based on field observations and model results and et is estimated to be 84 and 83 from these results we conclude that the model is able to capture the general hydrologic flux patterns of the actual system the agreement between field measurements and model output is further demonstrated by a comparison of general relationships between rainfall and the hydrologic pathways watershed wide estimates of hydrologic fluxes as a percentage of annual rainfall similar to those shown in fig 7b are also available for years 1966 1972 1976 and 1978 myrick 1989 while the model is not run for these earlier time periods and thus a direct comparison is not performed for these years the general relationship between rainfall and hydrologic pathway fluxes can be compared these relationships are shown in fig 8 for runoff baseflow et discharge recharge and down dip flow down dip flow is available only for 1985 in general the model results follow the temporal pattern of the field estimates for example higher annual rainfall results in a smaller percentage attributed to et and a higher percentage for stream discharge and recharge annual rainfall depths 700 mm result in approximately the same baseflow portion between 5 and 10 the model is also able to capture the general relationship between contributing drainage area km2 and stream discharge m3 sec stream discharge was measured at 15 locations on january 16 1988 and a drainage area was estimated for each location with the trend shown in fig 9 the trend between discharge and drainage area is also plotted for the 69 routing units of swat modflow for the same day of the simulation with a general agreement between field estimated and modeled values one discrepancy is a downstream location with a drainage area of 11 km2 with a recorded discharge of 0 0017 m3 s the simulated discharge value for this point in the watershed is 0 05 m3 s a difference of 0 048 m3 s another discrepancy is the downstream location with a 147 km2 drainage area with a recorded discharge value of 0 11 m3 s and a simulated value of 0 77 m3 s overall the model is able to capture a daily snapshot of stream discharge patterns throughout the watershed indicating accuracy in both surface runoff and groundwater discharge rates 4 2 2 outlet streamflow observed and simulated daily stream discharge is shown in fig 10 a for 1993 2012 with a more detailed presentation of individual storm hydrographs shown in fig 10b results for both swat and swat modflow are shown in general both models over predict streamflow after a storm event with swat modflow yielding higher values the nse coefficient is 0 207 for swat and 0 212 for swat modflow indicating poor model performance however these results likely are due to insufficient accuracy in weather forcing data rainfall temperature rather than an inherent error in the model as seen in fig 10a many of the hydrograph peaks are matched by the models but in some instances the models grossly under predict peak flow as indicated by the storm events circled in dotted orange ovals according to myrick 1989 most rainfall occurs during large summer thunderstorm events resulting in a high degree of spatial variation in recorded precipitation between weather stations in addition the large storm events evidenced by the high peak flows in the hydrograph in fig 10a do not have corresponding high rainfall depths in the weather station data used to force the model as only one weather station is available for the mbrw it seems that either this one station is not representative of the entire watershed or else data for large thunderstorms were mis recorded at the weather station as the point of this paper is to introduce the swat modflow modeling code and since the model is tested against several other watershed hydrologic responses the mis match between observed and simulated stream discharge for several large storm events does not seem to be a serious issue 4 2 3 groundwater groundwater results are presented in figs 11 13 fig 11 shows watershed wide groundwater results average groundwater head is shown for each grid cell in fig 11a with the contours indicating flow towards the main stem of the middle bosque river and in general parallel to the topography as found by myrick 1989 the depth to water table is shown in fig 11b showing generally shallow water tables along the stream network and hence a condition for significant groundwater discharge to the streams the increase in water table elevation from 1987 to 1988 is shown in fig 11c this time period coincides with the field study of myrick 1989 in which he observed that groundwater levels return approximately to the same level each year the map in fig 11c indicates that for the vast majority of the watershed simulated water table returns to approximately the same elevation after one year agreeing with the observation of myrick 1989 the map of average annual difference between maximum water table elevation and minimum water table elevation fig 11d shows that year to year fluctuations of water table elevation are within 1 5 m again in agreement with the observation of myrick 1989 fig 12 shows the comparison between simulated and observed groundwater head at four observation wells showing that although magnitudes have errors of several meters the fluctuation pattern is similar again exact locations of observation wells are not known and therefore the comparison method is inaccurate the overall groundwater balance from 1985 to 2012 is shown in fig 13 with individual balance components shown in fig 13a recharge fluctuates with rainfall patterns and groundwater et is highest during summer months due to high water tables caused by summer rainfall events conversely groundwater discharge is relatively constant due to the slow movement of groundwater and its response to recharge events fig 13b shows the temporal pattern of groundwater storage from 1985 to 2012 as groundwater discharge groundwater et and head boundary fluxes are relatively constant from year to year fig 13a the change in groundwater storage is due to mainly to changes in recharge this is seen clearly for the time period of 2000 2007 in which the gradual rise in groundwater storage corresponds to a period of consistent recharge fig 13a periods of storage decline are due to low recharge rates in the summer months e g 1996 when groundwater et is the highest there are no year to year field data to test the results shown in fig 13 but the accuracy of watershed wide fluxes shown in figs 7 and 8 particularly recharge groundwater discharge baseflow and head boundary discharge down dip flow yield confidence to the results shown in fig 13 4 2 4 groundwater surface water interactions the overall net exchange between the aquifer and streams was shown in fig 7a with annual averages of 54 mm of groundwater discharge and 2 mm of stream seepage however clarifying spatial patterns of groundwater surface interaction can be helpful for understanding patterns of contaminant transport and guiding implementation of best management practices bmps for nutrients and ecosystem health the average daily groundwater discharge m3 day to streams is shown in fig 14 a for each of the 69 routing units although there are several subbasins in which the overall net exchange between the aquifer and streams is stream seepage denoted by yellow polygons these are for upstream routing units and the seepage rates are small 400 m3 day compared to the groundwater discharge rates of the other routing units up to 9000 m3 day although reach by reach exchange rates and patterns have not been measured in the field results overall coincide with the conclusion of myrick 1989 that groundwater discharge is a dominant process in the mbrw as streams act as discharge points for groundwater throughout the watershed results from the swat model fig 14b indicate that stream seepage is never the dominant exchange process for any routing unit and that simulating groundwater flow in a simplified fashion as outlined in section 1 leads to vastly different results than using physically based spatially distributed groundwater modeling this is further demonstrated with trends between annual rainfall mm and the hydrologic pathways runoff lateral flow percolation groundwater discharge for each year of the simulation shown by plots in fig 15 for both swat and swat modflow for runoff lateral flow latq and percolation results are identical as implementing modflow does not affect land surface and soil hydrological processes however results for groundwater discharge gwq are vastly different most interesting there is a clear positive relationship between rainfall and gwq for swat as swat discharge is governed by recharge pulses see equations 2 5 the standard deviation of annual gwq values for swat is 10 3 and the trend has an r2 value of 0 54 conversely swat modflow discharge is not governed by recharge pulses but rather by the slow rise or fall of the water table which dictates groundwater gradients to streams and hence the volume of groundwater discharged to the stream channels there is therefore a much stronger auto correlation between year to year values the standard deviation of gwq annual values for swat modflow is 1 9 and the r2 value is only 0 074 indicating a slight dependence on annual rainfall these results highlight the principal difference between swat and swat modflow the latter simulates groundwater flow in a spatially distributed manner that adheres to the transient groundwater flow equation flow rates and residence times in the aquifer are governed by spatially distributed hydraulic conductivity specific yield and recharge therefore if a watershed has a water yield of which groundwater discharge is a significant portion then swat modflow can be a useful tool to accurately simulate flow rates and groundwater surface water exchange this can be especially important for watersheds in which groundwater supply or the effect of bmps on contaminant mass transport and loading are to be estimated if groundwater residence times and discharge rates are simulated incorrectly then the simulated effects of bmps will be wrong this is particularly true for land management bmps that affect groundwater contaminants such as fertilizer control these bmps will have an effect on river water quality only as quickly as groundwater reaches the stream which may be years to decades using a watershed model that does not simulate spatially distributed groundwater flow may under predict groundwater travel time and therefore under predict the time to achieve higher stream water quality 4 2 5 sensitivity of system response to groundwater parameters since groundwater delay gw delay and the extinction depth exdp of groundwater et have a range of possible values a sensitivity analysis for these parameters can provide guidance as to their importance in controlling hydrologic pathways e g groundwater discharge gwq and groundwater et gwet fig 16 shows the results of applying the morris method to determine the effect of these two parameters on a suite of watershed response variables as expected gw delay has the strongest influence on recharge followed by gwq water yield gwet and average stream discharge at the outlet exdp has the strongest influence on gwet followed by gwq water yield and average stream discharge at the outlet they have a minimal influence on average groundwater head and none on land surface hydrology plots of trends between these two parameters and gwq and gwet are shown on the right of fig 16 as seen in the trend between gw delay and gwq there is a significant relationship when gw delay is increased to years however between gw delay values of 0 and 100 days the value of gwq decreases from 1110 mm to 1100 mm a decrease of only 0 9 if gw delay is increased to 365 days gwq decreases to 1072 mm a decrease of only 3 4 a similar pattern occurs for gw et with a 0 5 and 3 decrease for gw delay values of 100 days and 365 days respectively therefore at least for the mbrw the choice of gw delay does not influence the overall watershed response results for watersheds with a thick vadose zone however wherein percolate may take years to decades to reach the water table gwq will be affected significantly exdp has a strong influence on gwet and gwq highlighting the importance of having good estimates of rooting depths for crops and vegetation which can transpire groundwater if the water table is shallow 5 conclusions this paper presents an updated version of swat that includes modflow to simulate groundwater flow and groundwater surface water interactions swat provides recharge and stream stage to modflow and modflow provides spatial values of groundwater head groundwater flow rate and exchange rates of groundwater and surface water that are provided to swat stream channels for stream routing the swat modflow model was applied to the 470 km2 middle bosque river watershed mbrw in central texas usa wherein groundwater makes up a significant portion of watershed water yield the model was tested against a suite of field measurements annual average hydrologic fluxes stream discharge runoff baseflow down dip flow et and recharge trends between annual rainfall rates and hydrologic fluxes stream discharge at the watershed outlet groundwater head and water table fluctuations and groundwater surface water interactions simulated results agree well with observed values in each case swat modflow can be used to simulate all major hydrologic pathways in a watershed using swat capability for land surface hydrology and hydrologic connections and modflow for groundwater residence time and flow rates the use of swat modflow also allows for the mapping of water table elevation water table depth and stream aquifer exchange rates for all grid cells in the model domain the model provides an excellent tool to investigate water supply and the impact of conservation practices and climate change for watersheds wherein baseflow is a significant portion of streamflow this is particularly true for management practices that effect groundwater quality as years to decades may be required for the effect to reach the stream due to the often low groundwater flow rates and associated long aquifer residence time software availability the swat modflow code is the official swat code on the swat model website https swat tamu edu software plus it is open source and available for download and code modification language fortran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the united state department of agriculture agricultural research service through cooperative agreement number 59 3098 8 002 author contributions r b and s p modified the swat code to include modflow routines with the help of j a k b and s p created and calibrated the swat model for the middle bosque river watershed mbrw s p prepared and calibrated the modflow model for the mbrw r b applied swat modflow to the mbrw and ran all simulations p a provided expertise in field data for the mbrw and comparison methods between swat modflow and field data appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104660 
26056,a hybrid bayesian network bn was developed for predicting the acute toxicity of chemicals to fish using data from fish embryo toxicity fet testing in combination with other information this model can support the use of fet data in a weight of evidence woe approach for replacing the use of ju venile fish the bn predicted correct toxicity intervals for 69 80 of the tested substances the model was most sensitive to components quantified by toxicity data and least sensitive to compo nents quantified by expert knowledge the model is publicly available through a web interface fur ther development of this model should include additional lines of evidence refinement of the discre tisation and training with a larger dataset for weighting of the lines of evidence a refined version of this model can be a useful tool for predicting acute fish toxicity and a contribution to more quantitative woe approaches for ecotoxicology and environmental assessment more generally graphical abstract image 1 keywords hybrid bayesian network model acute fish toxicity fish embryo toxicity weight of evidence animal alternatives risk assessment abbreviations aft acute fish toxicity bn bayesian network cv coefficient of variation ec50 effect concentration for 50 of the test individuals echa european chemicals agency fet fish embryo toxicity lc50 lethal concentration for 50 of the test individuals qsar quantitative structure activity relationship reach registration evaluation authorisation and restriction of chemicals woe weight of evidence 1 introduction weight of evidence woe is a term commonly used about the assembly weighting and integration of multiple pieces of evidence for environmental assessment and decision making however the term has been used in multiple ways including metaphorical theoretical and methodological without consensus about its meaning weed 2005 historically environmental risk assessors have relied upon narrative and qualitative approaches such as expert knowledge or limited quantitative methods such as direct scoring to integrate multiple lines of evidence linkov et al 2009 these historical methods lacked transparency and reproducibility a review of woe methods applied for ecological risk assessment linkov et al 2009 showed that only 9 out of 44 published studies had used a quantitative approach scoring indexing or other type of quantification therefore woe approaches have been criticised for being too vague intransparent and subjective linkov et al 2016 nevertheless methods are needed for integrating evidence and assess their uncertainty to support decision making for environmental protection these processes would benefit from more quantitative and rigorous approaches to woe more systematic frameworks for woe approaches in scientific assessments have recently been proposed for example by the european chemicals agency echa 2016 by the european food safety authority efsa scientific committee et al 2017 and by the us environmental protection agency usepa 2016 to assist ecological assessment the basic steps in these frameworks are to 1 assemble evidence 2 to weight the evidence by assigning scores depending on e g relevance strength and reliability and 3 to integrate the evidence weigh the body of evidence suter et al describe how the us epa framework can be used to infer qualitative properties 2017a as well as quantitative estimates 2017b use of such frameworks can increase the consistency and rigor of woe practices and provide greater transparency than ad hoc and narrative based approaches the weight of evidence was first proposed as a bayesian statistical approach good 1960 in bayesian statistics a model is based on updating prior beliefs in probabilities of a hypothesis after evaluating of evidence in order to achieve a posterior belief in this context the woe is defined as the logarithm of the bayes factor which is calculated as the ratio of the posterior odds to the prior odds good 1985 in this paper we return to the bayesian origin of the woe term and develop a bayesian network to combine lines of information in a probabilistic model to support risk assessment of chemicals environmental risk assessment is a process for quantifying the probability of an adverse effect from a chemical exposure traditionally acute toxicity data from three trophic levels i e fish invertebrates algae are required for risk assessments however ethical considerations and recent eu regulation have required that the number of animals used for toxicity testing is kept at a minimum and that the use of vertebrate species such as fish should be avoided as far as possible oecd 1996 therefore various alternative methods have been developed and proposed to replace the use of live fish in toxicity testing lillicrap et al 2016 including predictive toxicity modelling luechtefeld et al 2018 legislative authorities in some geographic regions have encouraged the use of fish embryos which are in this context considered as a non protected life stage over more developed life stages such as juvenile fish the test of fish embryo toxicity fet oecd method 236 oecd 2013 has therefore been proposed and evaluated as an alternative to using juvenile fish for testing acute fish toxicity aft oecd method 203 oecd 1992 busquet et al 2014 previous studies show a good correlation of fet with the standard acute fish toxicity aft test belanger et al 2013 rawlings et al 2019 nevertheless it has been found that the fish embryo toxicity test alone is currently not sufficient to replace the acute fish toxicity data as required by the european reach regulation regulation evaluation authorisation and restriction of chemicals sobanska et al 2018 however the authors have suggested that the test may be used within weight of evidence approaches together with other independent relevant and reliable sources of information to this end we have developed a bayesian network bn model for integrating fish embryo toxicity data with other information to predict the acute toxicity to juvenile fish for any given chemical substance a bayesian network is a probabilistic graphical model over a set of random variables bayesian networks are commonly used in environmental modelling aguilera et al 2011 barton et al 2012 landuyt et al 2013 for example there are numerous bn models developed for assessment and management of water quality barton et al 2014 borsuk et al 2004 2012 moe et al 2016 2019 bn modelling has been applied more rarely within ecotoxicology and ecological risk assessment but recent publications have demonstrated the applicability of bn modelling in these fields graham et al 2019 landis et al 2017 2019 lehikoinen et al 2015 compared to traditional qualitative woe approaches the network structure and probabilistic framework of bn provide advantages by capturing the impacts of multiple sources of quantifiable uncertainty on predictions of ecological risk carriger et al 2016 another example of applying a bn model for a quantitative woe and testing strategy is provided by jaworska et al 2015 here we develop a bn model for integrating four lines of evidence in a quantitative framework 1 information on physical and chemical properties of the substance 2 toxicity data for chemically related substances 3 toxicity data for other species crustaceans and algae and 4 fish embryo toxicity data by implementing this woe model in a bayesian network we aim to meet the demands for making woe approaches more transparent structured and quantitative linkov et al 2016 the purpose of this paper is to describe the development parameterisation and evaluation of the bn model the paper will also present an online web interface to the model more detailed information on this model s performance for specific chemical substances and endpoints and the implications for current legislation and guidelines for toxicity testing are addressed by lillicrap et al 2020 feedback from researchers regulators or other potential users to this first bn version will be used for improving future versions of the model with the aim of making it a useful tool in a woe approach for predicting acute fish toxicity 2 materials and methods 2 1 data an expanded version of the threshold database rawlings et al 2019 was obtained from procter gamble and used to construct the bn this database contains acute freshwater toxicity values for fish aft fish embryos fet algae and invertebrates for 237 chemical substances the toxicity values are measured as ec50 effect concentration for 50 of the test individuals for non lethal effects and as lc50 lethal concentration of 50 of the test individuals for lethal effects the dataset used for development of this model which will be referred to as our dataset contained the following toxicity data algae 264 values of ec50 based on oecd test no 201 acute algal growth inhibition tests oecd 2006 daphnia 1164 values ec50 based on oecd test no 202 daphnia sp acute immobilisation tests oecd 2004 using crustaceans of the genus daphnia daphnids juvenile fish 1459 lc50 values based on oecd test no 203 acute fish toxicity test oecd 1992 fish embryo 541 lc50 values based on oecd test no 236 fish embryo test oecd 2013 in addition to the data obtained from laboratory assays modelled acute fish lc50 values were calculated using quantitative structure activity relationships qsars for each chemical including the us epa ecological structure activity relationship ecosar 1 11 models and the danish qsar database extracting values for leadscope and sciqsar sars data were then averaged for each chemical resulting in 152 qsar values representing modelled toxicity to juvenile fish more details on qsar model selection equation acceptability criteria and results can be found in lillicrap et al 2020 2 2 bn model objective and structure a bn model consists of a directed acyclic graph with nodes representing the random variables and arrows arcs representing conditional probability distributions kjærulff and madsen 2008 each node has a probability distribution conditional on its parents in the graph the probability distributions quantify the strengths for the dependence relations defined by the structure of the graph the nodes are typically defined by limited number of discrete states categories or intervals which are quantified by a prior probability distribution new evidence is combined with the prior probabilities to calculate posterior probability distributions using bayes rule from 1763 see e g linkov et al 2016 the objective of the bn model presented here is to predict the acute toxicity of a chemical substance to juvenile fish corresponding to the interval of lc50 values from the aft assay oecd 1992 by integrating toxicity data from testing of fish embryos oecd 2013 with other relevant physical chemical and toxicological information for the given substance the bn model is structured along the four lines of evidence fig 1 representing different types of information for a given chemical substance for which a user wants to predict the acute fish toxicity the parameterisation of all lines is described in supplementary material tables s 1 s 21 while more details on the assumptions are provided by lillicrap et al 2020 line 1 physical and chemical properties of the substance the selected nodes quantify the size molecular weight g mol hydrophobicity octanol water partitioning coefficient logkow and modelled toxicity to juvenile fish based on the structure qsar of the substance substances with a low molecular size and low hydrophobicity i e high solubility are assumed to have a high ability to cross a biological membrane for a substance that is modelled via qsar to be toxic low lc50 value and have a high ability of membrane crossing the predicted toxicity to juvenile fish from this line of evidence will have highest probability of the higher toxicity intervals line 2 chemical category of the substance this line of evidence is based on existing data on toxicity of substances to juvenile fish aggregated by chemical category the 237 chemicals used to parametrize this model were assigned to 42 different chemical categories table s 11 based on model output ecosar and further refined by expert knowledge s belanger line 3 toxicity to other taxa representing lower trophic levels crustaceans daphnia magna or d pulex or unicellular algae this line of evidence also considers whether the substance has a species specific or more generally taxon specific mode of action by examining the ratio of daphnia and algal toxicity if the ratio of toxicity to daphnia vs algae is below 0 5 daphnia has a considerably lower tolerance lower ec50 value to the substance that the algae in this case the substance s mode of action is assumed to be specific to invertebrates e g an insecticide likewise if the ratio is above 2 then the mode of action is assumed to be more specific to algae e g a herbicide in either case the substance can be assumed to have low toxicity to fish conversely if the toxicity ratio is between 0 5 and 2 then the toxicity to fish is assumed to be approximately equal to daphnia and algae and a toxicity at the same level although with more uncertainty is assumed for fish the conventional cut off values of 0 5 and 2 0 and more details underlying these assumptions are described by ecetoc 2005 line 4 toxicity to fish embryo the measured toxicity of a substance to fish embryos is used directly as the fourth line of evidence the development and application of this bn is comparable to a weight of evidence approach e g suter et al 2017a with the assignment of conditional probabilities to different variables within in a line of evidence in the bn corresponding to setting weights to pieces of evidence in a woe for example assignment of low weight to a piece of evidence can be obtained by setting wide probability distributions representing high uncertainty or variability in the relation from this node to its child node within a line of evidence e g line 3 toxicity to other taxa fig 1 the calculation of posterior probabilities for the last child node of this line toxicity to fish predicted from other taxa accumulates the weights given to all parent nodes in this line when using the bn to predict the toxicity to juvenile fish from all lines of evidence for a chemical of concern the weighing of the total evidence for each hypothesis in a woe suter et al 2017a can correspond to calculating the posterior probability of each toxicity level very low low etc in the bn for example given two lines of evidence with posterior probability distributions both centred around high toxicity the line with the more narrow probability distribution will typically contribute more to the posterior probability of high toxicity in the final child node toxicity to fish predicted from other taxa in a woe approach this would correspond to this line of evidence having a higher weight for the hypothesis high toxicity to juvenile fish than the line with a wider probability distribution the bn was implemented in the software hugin researcher version 8 7 developed by hugin expert a s http www hugin com the model is available through a web interface described in section 3 which is published on the demonstration web site http demo hugin com example fet the aim of this web interface is to facilitate feedback for further improvement of the model as recommended by marcot 2017 2 3 model parametrization 2 3 1 node types and discretisation bn models are usually constructed with discrete nodes with a low number of states such as categories or intervals kjærulff and madsen 2008 in these cases the bn is an efficient and compact representation of a joint probability distribution eq 1 1 p v x v p x p a x where v is the set of discrete nodes x 1 to x n and pa x are the parents of x an area of recent interest and progress is the development of continuous bn models qian and miltner 2015 where quantitative variables are not discretised into intervals but instead are represented by equations or statistical distributions or hybrid bns e g aguilera et al 2010 which contain both discrete and continuous nodes marcot and penman 2019 for our model categorical nodes with few states would be the most convenient for parameterisation of conditional probability tables in cases where expert knowledge was required on the other hand continuous nodes would be preferable to optimise the use of the continuous input values e g measured toxicity values and their variability consequently this model is a hybrid bn with both discrete and continuous nodes fig 1 in this model the continuous nodes are assumed to follow the conditional linear gaussian distribution eq 2 2 p y i i z z n α i j β j i z j γ i where y is a continuous node i is a set of discrete nodes z is a set of continuous nodes and i z is the set of parents of y as an intermediate step between the continuous nodes and the discrete categorical nodes we used discrete interval nodes table 1 for example the categorical node toxicity to daphnia level has five labelled states such as low and medium the corresponding interval node toxicity to daphnia interval has intervals such as 5 100 and 0 5 5 mg l in addition the interval nodes have two more extreme states with very low prior probability to avoid the dominance of extreme values explained below we let interval nodes represent the true but unknown toxicity of a substance this true toxicity property was considered to be the cause of the observed toxicity values therefore the toxicity observations were modelled by continuous nodes as children realisations of the interval nodes the parameterisation of all nodes i e priors conditional probabilities and conditional density functions is described in supplementary material tables s 1 to s 21 following the recommendations for good practice in bn modelling chen and pollino 2012 for the physical and chemical properties line 1 we chose only two intervals table 2 with cut off values commonly used in ecotoxicology molecular weight with the threshold 600 g mol brooke et al 1986 and hydrophobicity with the threshold log kow 5 5 oecd 2012 because the use of strict cut off criteria to define bioaccumulation potential has been criticized arnot et al 2010 the child node membrane crossing was defined by three intervals low medium and high to allow for intermediate levels of membrane crossing potential the mode of action node line 3 also had three intervals based on the pre defined cut off values of ratio 0 5 or 2 ecetoc 2005 the predicted toxicity nodes e g toxicity to algae level were initially modelled by categorical nodes with 5 states ranging from very low toxicity 100 mg l to very high toxicity 0 01 mg l note that increasing toxicity level corresponds to decreasing concentration of the substance table 3 the discretisation of continuous toxicity values for the discrete nodes table 3 was based on the classification and labelling of toxicity levels in the globally harmonized system oecd 2001 which has four toxicity levels with interval boundaries 1 10 and 100 mg l to increase the resolution of the highest toxicity levels we selected 5 intervals with the boundaries 0 01 0 5 5 and 100 mg l the integration of the four lines of evidence was based on these 5 state categorical nodes the toxicity interval nodes e g toxicity to algae interval were added to provide a link to the continuous nodes for input values e g toxicity to algae value 1 since interval nodes must have intervals in increasing order the order of the toxicity intervals was reversed compared to the order of toxicity states it was not strictly necessary to keep both the categorical and the interval version of the same toxicity node e g toxicity to algae but we found that it facilitated the interpretation and helped avoid confusion regarding the order of toxicity intervals vs toxicity levels an additional benefit of the interval nodes was to enable the extraction of a single expected value mean value from a toxicity node which could for example facilitate the comparison of posterior probability distributions of toxicity nodes for different substances for this purpose the toxicity interval nodes were given two additional extreme states table 3 with very low probability the extreme states served as buffers for obtaining reasonable calculations of mean values to summarise the probability distribution of a node in this paper however we will focus on the probability distribution of the 5 state categorical toxicity nodes rather than the mean values of the corresponding interval nodes the continuous input value nodes fig 1 allowed users to provide their toxicity data as original values ec50 or lc50 instead of discretised states the function of the continuous nodes is to create a likelihood on the interval nodes using multiple findings entered as exact values although the continuous value nodes serve as input nodes for this bn they are defined as child nodes of toxicity interval nodes fig 1 the reasoning is that the observed toxicity values for a given substance e g toxicity to algae value 1 are realisations of an inherent true toxicity value for this substance this true value is unknown but can be modelled by the probability distribution of the toxicity interval node toxicity to algae interval the bn has multiple continuous input nodes for each of the algae daphnia and fish embryo endpoints fig 1 the purpose was to let the bn model 1 retain as much details as possible from the input values 2 account for variability in the provided data i e variation among input values and 3 account for inherent uncertainty in the toxicity values e g a toxicity value closer to an interval boundary would have higher probability of being assigned to the next interval in the current version of the web interface there are ten continuous input nodes for each of these endpoints assuming that this is a reasonable maximum number of toxicity values available for a chemical substance in this dataset the number of observations per substance exceeds ten for only 3 19 and 7 of the substances for algae daphnia and fish embryos respectively if needed the number of continuous input nodes can easily be increased without affecting the model prediction the unused input nodes will be so called barren variables kjærulff and madsen 2008 which will be updated by evidence in their sibling nodes but will not themselves influence other nodes 2 3 2 prior probabilities prior probability distributions were defined for all root nodes i e all nodes with no parent nodes in the graph table 1 for all root nodes of interval type the prior probability distribution was set equal to the frequency distribution of our dataset this approach was used for prior probabilities of the following interval type nodes molecular weight table s 2 hydrophobicity table s 4 toxicity based on qsar table s 7 toxicity to algae table s 13 toxicity to daphnia table s 14 ratio toxicity daphnia algae table s 15 and toxicity to embryo table s 18 for example for the node molecular weight interval 231 substances had low molecular weight 600 g mol while only 2 substances had high molecular weight 600 g mol the resulting prior probability distribution across these two states was 99 14 and 0 86 table 2 for the toxicity interval nodes for qsar algae daphnia and embryo the probabilities of states were also based on the counts of values however because of low numbers of values in the most extreme intervals the counts were merged for the two intervals 100 1000 and 1000 inf and the probabilities of these intervals were set to respectively 99 9 and 0 01 of the proportion of values in toxicity level very low table 3 the probability of the intervals 0 001 0 01 and 0 0 001 likewise were set to respectively 99 9 and 0 01 of the proportion of values in toxicity level very high one root node was of categorical type chemical category table 1 since this was also an input node to be instantiated by the user the prior distribution was not of importance and was set to uniform distribution table s 11 2 3 3 conditional probability tables and density functions the conditional probability tables cpts of the bn were parametrized by four main approaches illustrated by the four examples in fig 2 1 expert knowledge fig 2a 2 frequency distributions derived from our dataset fig 2b 3 statistical considerations fig 2c and a rule for combining the lines of evidence fig 2d expert knowledge a lillicrap was used for cpt of the nodes membrane crossing table s 6 toxicity to fish predicted from chemical properties table s 10 and toxicity to fish predicted from other taxa table s 17 the ability of a molecule to crossing a biological membrane decreases both with its size and its hydrophobicity for membrane crossing therefore the combination of low hydrophobicity and low molecular weight was assumed to result in respectively 0 25 and 75 probability of low medium and high ability of membrane crossing while the opposite combination of hydrophobicity and molecular weight was assigned the opposite probability distribution the two combinations of high low and low high hydrophobicity and molecular weight were assigned a probability distribution of 25 50 and 25 for the three states of membrane crossing more details on the underlying assumptions are given by lillicrap et al 2020 we have assumed this very simple relationship as a starting point which can later be refined with more indicators of molecular properties an example is shown for toxicity to fish predicted from chemical properties fig 2a when a substance s ability to cross a biological membrane is high the predicted toxicity to fish based on chemical properties is equal to the acute fish toxicity modelled by qsar when the membrane crossing ability is low we assigned only 80 chance that the predicted acute toxicity corresponds to the modelled qsar result the reasoning behind the cpt of toxicity to fish predicted from other taxa is explained for all 50 combinations of the parent states by lillicrap et al 2020 in brief if the ratio of the average toxicity values to daphnia vs algae is between 0 5 and 2 it can be assumed that the chemical has a similar mode of action for the two taxa and that the chemical will affect fish in a similar way therefore the cpt converts the ec50 values from algae and daphnia to lc50 values for fish with high precision a narrow distribution which corresponds to assigning a high weight to these pieces of evidence in a woe approach moreover more weight i e narrower probability distributions has been assigned to the evidence from daphnia than from algae assuming that the closer phylogenetic relationship of fish with invertebrates than with plants makes their responses to a chemical more similar conversely a ratio of 0 5 or 2 indicates that the chemical has a species specific mode of action which affects daphnia more strongly than algae or vice versa in these cases we have less confidence in extrapolating the toxicity data from daphnia or algae to fish and have therefore set wider probability distributions in this part of the cpt this wider distribution corresponds to lower weighting in a woe approach cpts for the node toxicity to fish predicted from chemical category as well as for all continuous nodes identified in table 1 were created using frequency distributions derived from our dataset the cpt for toxicity to fish predicted from chemical category is illustrated in fig 2b for each chemical category the values represent the proportion of juvenile fish lc50 values in each of the given toxicity levels for example the 50 observations in the chemical category aniline comprised 4 observations of the toxicity level very low 40 observations of low 4 observations of medium and 2 observations of high from altogether 13 chemical substances the number of observations per chemical category was sometimes quite low e g imidazole fig 2b to optimise the use of the available data multiple toxicity observations for the same substance were counted independently for chemical categories with a low total number of observations table s 11 the proportions would not properly represent a probability distribution marcot 2017 therefore the model predictions will be less reliable for these chemical categories in addition a chemical category unknown other can be selected when this state is chosen this line of evidence is excluded from the final predicted toxicity node for the continuous nodes the conditional probability distributions were quantified by a conditional density function cdf defined by a gaussian distribution with mean and variance specified for each interval of the parent node i e the corresponding interval node see table 1 for the physico chemical nodes of line 1 mean values were calculated to reflect the chemicals in our dataset variances were calculated according to a pre defined coefficient of variation cv standard deviation divided by the mean we chose not to let the variance be calculated directly from our dataset as the variance would have been sensitive to the number of observations in each interval instead we assumed low cv of physico chemical properties of substances such as molecular weight 10 cv and hydrophobicity 5 cv for example for the node molecular weight value the cv was set to 5 of the mean value in the cdf for this node table s 3 the two intervals 600 g mol and 600 g mol had mean values of 179 and 200 380 respectively calculated from the respective counts of 231 and 2 observations see table 2 for all continuous nodes representing toxicity values based on qsar algae daphnia or embryo the density functions were defined in the same way the mean for each interval of the parent node was calculated as the midpoint of the interval except for the extreme interval 1000 inf here the mean was set to 1550 which was equal to the lower boundary plus the midpoint of the previous interval for each interval the variance was set so that a 90 of a gaussian distribution was within the interval while 5 of the distribution was in either of the neighbour intervals see appendix a figure a 1 the resulting variances fig 2c table s 8 correspond to ca 30 cv of the upper interval boundary for all intervals except the highest inf for toxicity data from standard assays performed in various laboratories cv up to 30 is a reasonable assumption rawlings et al 2019 the cpt of the final child node toxicity to fish predicted from all evidence was defined by a combination rule to ensure that all four lines of evidence were given equal weight following the recommendation of marcot 2017 to obtain more tractable cpt dimensionality we first set the probabilities for combination of two lines fig 2d the combination of lines 1 2 and of lines 3 4 each resulted in two intermediate child nodes which were subsequently combined with the same rule fig 2d to produce a new grandchild node the two intermediate nodes were then absorbed which resulted in a cpt combining the four grandparents nodes with the one grandchild node as described in the supplementary material table s 21 2 4 model assessment 2 4 1 sensitivity analysis the sensitivity of the target node toxicity to fish predicted from all evidence to the different lines of evidence was analysed by two methods parameter sensitivity analysis and value of information analysis the parameter sensitivity analysis measures the functional relationships between a parameter i e a probability value in the cpt of a node and the posterior probability of a given state of the target node chan and darwiche 2002 in general the posterior probability p h ε is a ratio of two linear functions eq 3 in any parameter t of the bn where t is an entry in a cpt of the model 3 p h ε t p h ε t p ε t α t β γ t δ where ε denotes the evidence and h denotes a specific state of the target node here referred to as the hypothesis the sensitivity analysis was run relative to all states toxicity levels of the target node under the default scenario of no evidence i e no data were used to run the model due to the structure of the evidence in this scenario i e no downstream evidence relative to the target variable the functional relationship shown in eq 3 is linear coupé and van der gaag 2002 value of information voi analysis can quantify the potential benefit of additional information in the face of uncertainty keisler et al 2014 and can aid the decision on allocation of resources between obtaining new information and improving management actions mäntyniemi et al 2009 the method quantifies the voi as reduction of entropy a measure of uncertainty for a given node in isolation averaging the values of other nodes the voi analysis was run under different scenarios of evidence i e different information applied to the model the default scenario of no evidence as well as data for three example substances using all available data either for the first three lines of evidence excluding embryo data or for all four lines of evidence the purpose was to assess the potential benefit of including embryo data as a line of evidence 2 4 2 model evaluation the bn model performance and uncertainty was evaluated by several of the metrics recommended marcot 2012 model performance was assessed by running with input data from our dataset and comparing the outcome i e the predicted acute toxicity of selected chemical substances to juvenile fish to the observed toxicity of the same substances to this endpoint the model validation should ideally be performed with an independent dataset that had not already been used for parametrization of the model however additional fish embryo toxicity data are not widely available and our dataset represents the largest collection of oecd 236 fet data in the public domain to our knowledge instead we applied four different criteria for selecting subsets of our dataset for validation in order to consider the robustness of the model from different perspectives subset 1 all chemical substances containing at least one observation of juvenile fish toxicity aft data number of substances 159 which was the minimum requirement for comparison with the model prediction for substances missing one or more input values e g toxicity to algae the prediction would be based on the prior probabilities of this node the exception was line 2 toxicity of the chemical category which allowed for the state unknown subset 2 only the chemical categories that were represented with minimum 10 different chemical substances in our dataset number of substances 106 using phenol as an example the conditional probabilities of a chemical category in the node toxicity to fish predicted from chemical category fig 2b was calculated as the frequency distribution of observed toxicity to juvenile fish for all substances in the category phenol when the predicted toxicity of a substance belonging to the category phenol was assessed e g triclosan the observed toxicity values of triclosan used for comparison have also been used in the cpt for chemical categories containing few substances in our dataset there will be a high overlap between the data used for parametrization and for validation for chemical categories with 10 or more substances however the data used for validation for one of these substances will constitute only a small proportion of the frequency distribution of the cpt therefore a validation based on such subsets will be more independent of the parametrization data subset 3 all substances with complete cases for input nodes number of substances 77 this means that our dataset contained minimum one value for each node defined as input node table 1 molecular weight hydrophobicity qsar toxicity to algae toxicity to daphnia and toxicity to embryo as well as toxicity to juvenile for this subset the validation would be less influenced by the prior probabilities tables 2 and 3 since evidence would be available to update the probabilities of all input nodes subset 4 cases with a minimum of 3 observations for both embryo and juvenile toxicity data number of substances 20 the reasoning was that these two nodes are the most crucial for the purpose of the model a higher number of observations would make the model predictions more precise and reduce bias due to variability in these types of data for the purpose of comparing predicted and observed probability distributions it is common to define the state with the highest probability as the correct state marcot 2012 we followed this practice and defined the toxicity interval with the highest probability as the correct predicted or observed state this way we could calculate the number of correct predictions for each toxicity interval and for each data subset this practice does not however account for the whole probability distribution more advanced methods for model validation consider the probability distributions of predicted and observed variables will be investigated in further work with this model 3 web interface to the bn model a web based user interface to the bn model for demonstration purposes has been published on hugin s web portal for bn examples http demo hugin com example fet our model is the first example of a bn within the field of ecotoxicology and ecological risk assessment on this platform the web interface to this preliminary bn version will give researchers and other potential users the opportunity to provide feedback for improving the model in the longer term our intention is to further develop this model into a more comprehensive online tool which can be useful for regulatory authorities and chemical industries wanting to submit fish embryo toxicity data in place of acute fish toxicity data here we briefly describe the two interactive pages of the web interface enter values supplementary information figure s 1 and results figure s 2 in the tab enter values a user can insert the requested information for any chemical substance for which they want to predict the acute toxicity to juvenile fish the user should provide the requested information as follows for the four lines of evidence line 1 hydrophobicity log kow molecular weight g mol toxicity based on qsar mg l the user must enter one continuous value for each node line 2 chemical category the user must select the category from a drop down list which includes the state unknown other more chemical categories can be added upon request from users line 3 toxicity of the substance to other taxa algae oecd 201 and daphnia oecd 202 the user should first select the number of values up to ten for each taxon then enter the ec50 values mg l line 4 toxicity of the substance to fish embryos oecd 236 the user should first select the number of values up to ten then enter the lc50 values mg l the tab enter values contains the buttons compute which will carry out the calculation based on these input values and load case which will load a built in example the substance carbamazepine the tab results displays the posterior probability distributions across the five toxicity levels for all four lines of evidence toxicity to fish predicted from chemical properties toxicity to fish predicted from chemical category toxicity to fish predicted from other taxa and toxicity to embryo level as well as for the final child node toxicity to fish predicted from all evidence see supplementary information two buttons generate tables as pop up windows the button view input values provides a table of the entered values as well as the calculated ratio of toxicity to daphnia vs algae table 4 a this table also identifies the most sensitive endpoint algae daphnia or fish embryo which is relevant for the application of these results for regulatory risk assessment lillicrap et al 2020 the button view output values provides a table with the posterior probability distributions for the five selected nodes mentioned above table 4b the results tab also provides conclusive statements based on the calculated values such as the toxicity level of carbamazepine to juvenile fish is most likely low 52 28 probability 4 results and discussion 4 1 examples of bn model predictions examples of model predictions for three selected substances are presented in fig 3 all examples are from subset 4 see section 2 4 2 which has minimum three toxicity data for both embryo and juvenile fish the three examples represent three different levels of observed toxicity to juvenile fish low medium and high respectively the first example fig 3a is carbamazepine an antiepileptic drug in the chemical category substituted urea this substance is also used as a built in example in the online demonstration model figures s1 s2 the observed toxicity to juvenile fish to which the predicted toxicity will be compared is 100 in the low toxicity interval four observations not shown the node toxicity to embryo level was almost 100 very low while the toxicity based on qsar algae and daphnia was low to very low the information from the chemical category however indicated a 50 probability of high toxicity to juvenile fish this line of evidence contributed to higher predicted toxicity to juvenile fish 26 probability of medium or higher toxicity than the other three lines after combination of the four lines the most likely state was low toxicity to juvenile fish 52 probability which was consistent with the measured toxicity interval although with higher uncertainty in this case information on toxicity to embryo very low alone would have underestimated the risk to juvenile fish low while the embryo data in combination with the other three lines of evidence resulted in a more accurate prediction on toxicity to juvenile fish the second example fig 3b was tetradecyl sulfate a drug in the chemical category anionic surfactant our dataset contained three lc50 values from aft assays toxicity to juvenile fish all of which fell into the medium toxicity interval the node toxicity to embryo level predicted a 100 probability the high toxicity interval while the predictions from the other lines of evidence were centred around low to medium toxicity interval the resulting predicted toxicity to juvenile fish had highest probability 41 of the medium state which was consistent with the measured toxicity in this case information on toxicity to embryo alone would have overestimated the risk to juvenile fish while the embryo data in combination with the other lines of evidence again resulted in a more accurate prediction although with lower precision than the original aft data the third example fig 3c triclosan is an antimicrobial agent in the chemical category phenol this substance had six lc50 values for juvenile fish in the high toxicity interval the data based on embryo daphnia and qsar showed the same toxicity level while toxicity to algae was very high toxicity based on the chemical category on the other hand was almost 50 likely to be medium or lower when the four lines of evidence were combined the most probable toxicity level was high 60 which was again consistent with the observations 4 2 sensitivity analysis the parameter sensitivity analysis table 5 showed that under the default evidence scenario of no evidence the state very high of the target node toxicity to fish predicted from all evidence as expected was the most sensitive to changes in single parameters in the probability tables of the parent nodes the target node was most sensitive to parameters of the nodes toxicity to embryo interval and toxicity based on qsar interval the maximum sensitivity value averaged across all states of the target node was 0 16 for both parent nodes this means that an increase of 0 1 in the parameters of toxicity to embryo interval will result in an increase in 0 016 in the posterior probability of toxicity to fish predicted from all evidence see eq 3 the highest sensitivity was found for the state high toxicity of the target node with slopes 0 21 qsar and 0 19 embryo these nodes are root nodes table 1 with probability tables containing prior probabilities tables s 6 and s 18 the analysis shows that the prior probabilities of these nodes may have a strong influence on the prediction which implies that the approach used for setting priors appendix a may need to be refined the third most influential parameters were for chemical category to which the sensitivity of the target node was 0 08 on average and 0 12 for the state high toxicity the prior probability table of this root node has a uniform distribution assuming that for a new chemical substance all chemical categories are equally likely but considering the high influence of this probability table on the target node future research may explore a more informative prior probability distribution e g which better reflects the frequency of the different chemical categories in our dataset or other larger datasets the fourth and fifth most influential parameters were for toxicity to daphnia interval and toxicity to algae interval the influence of these nodes on the target node were weakened by the additional node species specific mode of action which is meant to introduce uncertainty related to the extrapolation of results from plants and invertebrates to fish the target node is slightly more sensitive to changes in parameters for daphnia than for algae this is consistent with the closer phylogenetic relationship of fish with invertebrates than with plants which we tried to account for in the cpt table s 17 lillicrap et al 2020 in general this sensitivity analysis reflects the strongest influence of parameters in the parts of the model that are most directly based on data such as toxicity based on qsar and toxicity to embryo conversely the analysis indicated weaker influence of lines of evidence that rely strongly on expert knowledge such as membrane crossing and toxicity predicted from other taxa although these lines of evidence are also based on data e g measured toxicity to algae and daphnia expert knowledge is applied in weighting and combining these pieces of evidence hence there is a potential for making the model more sensitive by refining the cpts that are currently based on expert knowledge e g fig 2a the sensitivity analysis also shows that the posterior distribution of the target node is robust to changes in any single parameter of the bn as all sensitivity values are less than one the value of information voi analysis table 6 showed that under the default scenario of no evidence all four lines of evidence contribute almost equally and relatively little to entropy reduction no more than 7 0 09 1 36 of the entropy of the target node line of evidence 3 other taxa contribute slightly more than the other lines this can reflect the fact that the conditional probabilities of toxicity predicted from other taxa table s 17 can be either highly correlated with the target node or be non informative flat distribution depending on the input values node ratio toxicity daphnia algae lillicrap et al 2020 for scenarios where information is provided only for the lines of evidence 1 3 i e excluding information on toxicity to embryos additional information on embryo toxicity can reduce the uncertainty of the target node by 5 7 table 6 the relative importance of information on embryo toxicity varies among the three example substances which suggests that the importance of this type of information cannot be generalised across substances under the scenario of full information lines 1 4 further information on embryo toxicity will not contribute to reduction in entropy this result suggests that the information on embryo toxicity used in each example has already exerted a strong influence on the prediction which is consistent with the outcome of the parameter sensitivity analysis table 5 4 3 model performance the performance of the bn for the four different subsets of our dataset is reported in table 7 and summarised in fig 4 for each substance the predicted juvenile toxicity state with the highest probability was compared with the most frequently observed juvenile toxicity state the first three subsets which have a number of substances ranging from 77 to 159 showed very similar results the percentage of correctly predicted toxicity level was 69 71 while the percentage of overestimated toxicity was 14 18 and the percentage of underestimated toxicity was 12 16 the model typically overestimated toxicity when the observed toxicity was very low or low conversely the few observed cases of high toxicity were sometimes underestimated as medium the selected subsets of dataset were dominated by substances with very low to medium toxicity which can explain the slightly higher proportion of cases with overestimated toxicity if the model is applied to a new substance with high toxicity the model is more likely to underestimate its toxicity as medium than to overestimate it as very high the fourth subset which had the strictest criteria for selection of substances minimum 3 observations of both juvenile and embryo had the highest correct prediction rate 80 although this increase in prediction rate may simply be a random change due to the lower sample size n 20 this improvement could also indicate that the higher number of observations leads to more accurate predictions and therefore better model performance the better performance of this final subset therefore lends support to our decision of designing the bn with multiple continuous input nodes which allows for the use of more observations in practice the risk assessment for a chemical substance is determined by the most sensitive endpoint algae daphnia or fish for example for the selected substances in subset 4 table 7d the four substances with underestimated toxicity to juvenile fish were all more toxic to algae or daphnia than to fish embryos in such cases the predicted toxicity to juvenile fish is less relevant since the risk assessment will be driven by another endpoint for this reason in the web interface to the model information on the most sensitive endpoint is extracted from the input data and provided in the input table table 4a and in the conclusive statements figure s 2 these implications are further discussed by lillicrap et al 2020 4 4 further development of the bn model although this preliminary version of the bn model shows a high rate of correct predictions up to 80 fig 4 the model performance can be further improved moreover the model showed relatively low value of information for reducing uncertainty of model predictions as indicated in table 6 which suggests that the model needs refinement for making better use of provided input data cf table 4a to increase the model sensitivity to the lines with weakest influence and to improve the accuracy of model predictions the following issues should be addressed for most nodes in lines 1 3 fig 1 which do not depend on fish embryo toxicity data the prior probability tables and conditional probability tables could be parametrized based on independent datasets this way our dataset could be reserved for model validation a candidate dataset for model parametrization is the aquatic toxicology database envirotox connors et al 2019 however the envirotox database has not yet received the same level of curation as our dataset and may introduce additional noise into our model for components of the model where data are not available the cpts based on expert knowledge by the authors can be strengthened by a more formal approach for external expert elicitation e g castelletti and soncini sessa 2007 marcot 2017 the toxicity intervals are relatively wide some spanning more than an order of magnitude table 3 toxicity nodes with higher resolution more toxicity intervals may be perceived as more informative however a model with more than 5 toxicity levels would require different approaches to parametrization of the cpts whether these are based on expert knowledge e g fig 2a or on frequency distributions e g fig 2b for future versions of this bn we will explore more automated approaches to parametrization of ctps to obtain smoother probability distributions and to avoid the extreme 0 and 100 probabilities the width of the toxicity intervals is decreasing exponentially e g the state low has width 95 mg l medium has width 4 5 mg l and high has width 0 49 mg l while this non linear scale reflects the intervals and threshold values typically used in regulatory ecotoxicology it introduces a bias in the probability calculations explained in appendix a figure a 2 lower toxicity levels corresponding to higher concentrations with wider toxicity intervals will inherently have lower probability than higher toxicity levels the bn model may therefore show a tendency of overestimating toxicity fig 4 from regulatory point of view such a bias may not be a problem since the biased predictions will be more protective for the environment however while risk assessment calculations often have built in safety factors to obtain more protective assessments we aim for accuracy of the model predictions for this bn model a possible solution for reducing the bias is to model the toxicity values on logarithmic scale so that the toxicity intervals will get more equal widths the four lines of evidence has so far been integrated by equal weighting as a starting point the cpt for combining the four lines table s 21 could instead be trained by data to optimise the weighting of the four lines however training the model by advanced methods such as machine learning algorithms marcot and penman 2019 will require a higher number of substances with complete set of input data including fish embryo toxicity data than what is currently available in principle this hybrid bn could be further developed into a continuous variable bn an example of a continuous variable bn modelling framework developed for setting nitrogen criteria in streams and rivers is presented by qian and miltner 2015 their model retained the bn s graphical representation of hypothesized causal connections among variables while employing statistical modelling approaches for establishing functional relationships among these variables a continuous bn model can avoid the problems related to discretisation of continuous toxicity value into intervals nojavan a et al 2017 and provide more precise predictions however constructing a continuous variable bn poses other challenges in our case converting the bn into a continuous model would require more advanced approaches to model parameterisation especially for cpts that are currently based on expert knowledge 5 conclusions we have developed a bayesian network model for predicting the acute toxicity of chemical substances to juvenile fish based on information on fish embryo toxicity in combination with physical and chemical properties chemical category and toxicity of the substance to other taxa this model can support a weight of evidence approach for replacing the oecd 203 acute fish toxicity assay based on juvenile fish with animal alternative approaches such as the oecd 236 fish embryo toxicity assay as a measure of model performance the bn predicted the correct toxicity interval for 69 80 of substances in our dataset given different quality criteria for the subset of 20 substances with the highest quality criteria the prediction rate was 80 correct the model predictions were most sensitive to the model components that were quantified by toxicity data such as fish embryo and qsar conversely the model was least sensitive to the components that were quantified by expert knowledge e g involving the chemical s mode of action inferred from testing of other taxa the model is publicly available through a web interface for testing and feedback future development this model should include more lines of evidence refinement of the discretisation of toxicity intervals training of the model with a larger toxicity dataset to weight the lines of evidence differently a more mature version of this model can be a useful woe tool for predicting fish acute toxicity from fish embryo toxicity data the model will also be a contribution to the trend of developing more quantitative weight of evidence approaches which are needed both in the context of animal alternatives in ecotoxicology and in environmental assessment more generally declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding this work was supported by niva s research programme digisis new digital methods for monitoring and research we thank the norwegian environment agency and members of the animal alternatives in environmental science interest group of setac society of environmental toxicology and chemistry for feedback on earlier versions of the model and web interface and three anonymous reviewers for their helpful comments author contributions conceptualization a d l s e b and s j m data curation k a c j m r and s e b data analysis s j m a l m and r w methodology s j m a d l w g l a l m and r w funding acquisition s j m and a d l project administration s j m web interface a l m visualization s j m writing original draft s j m and a l m writing review editing s j m r w a l m a d l k a c j m r w g l appendix a supplementary data the following are the supplementary data to this article appendix a contains explanation to the conditional probability tables for continuous value nodes figures a 1 and a 2 multimedia component 1 the file fet bn supplementary figures 20190828 docx contains pictures figures s 1 and s 2 showing an example of running the bn model from the web interface http demo hugin com example fet multimedia component 1 multimedia component 2 the file fet bn supplementary tables 20190828 docx contains the tables of conditional dependencies of all nodes in the bn model tables s 1 to s 21 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104655 appendix a conditional probability distributions for continuous toxicity value nodes the continuous toxicity values nodes for qsar algae daphnia and embryo are input nodes for the bn model but also child nodes of the respective interval type nodes therefore their conditional probability distributions must be specified by mean and variance for the seven toxicity intervals as described in section 2 4 3 conditional probability tables figure a1 illustrates the probability distributions for each of the states ranging from very high toxicity a to very low toxicity e defined as gaussian probability density functions for each interval the variance of the probability density function was set so that 90 of the distribution was within the interval while 5 of the distribution was in either of the neighbour intervals for example for the interval medium toxicity figure a1c 90 of the area below the curve is contained within the interval 0 5 5 mg l 5 of the area is in the range 0 5 mg l high toxicity or higher and 5 of the area is in the range 5 mg l low toxicity or lower this can be interpreted as follows let us assume that the true but unknown toxicity of a substance to algae is in the interval medium toxicity then there is a 5 probability that a toxicity test of this substance will wrongly result in low toxicity to algae and 5 probability that the test will wrongly result in high toxicity for all intervals displayed in figure a1 the resulting variances fig 2c table s1 13 correspond to ca 30 cv of the upper interval boundary i e the lowest toxicity of the interval figure a 1 probability density function used for setting conditional probabilities of the continuous interval nodes for the five toxicity intervals ranging from very high a to very low e coloured curves are the probability density function generated by gaussian distribution with mean and variance as specified in fig 2c vertical lines indicate the toxicity interval boundaries and the grey areas represent the probability of values occuring in this interval figure a 1 when the bn model is run the cpt of the continuous toxicity nodes e g toxicity to algae value 1 are used together with evidence inserted continuous toxicity values to update the probabilities of the interval nodes e g toxicity to algae interval by backward calculation since the conditional probability distributions of the higher toxicity states have more narrow intervals than the lower toxicity states the higher toxicity states will generally have higher probability density figure a1 as a consequence if one enters a continuous toxicity value that is close to the lower boundary i e close to the higher toxicity interval then the posterior distribution of the interval toxicity node is more likely to be in the higher toxicity interval for example figure a2a consider an observed toxicity value of 6 mg l which belongs to the interval low toxicity 5 100 mg l however for this interval the conditional probability density of 6 mg l is only 0 0038 while for the neighbour interval medium toxicity the conditional probability density is five times as high 0 019 therefore this observation will result in a relatively high posterior probability of the medium toxicity interval in contrast consider an observed value is 4 mg l which belongs to the interval medium toxicity but is close low toxicity the probability density of this value of the correct interval medium toxicity is 0 19 figure a2b which is 56 times as high as the probability density of the neighbour interval low toxicity 0 0034 hence this observation result in virtually zero probability of low probability in summary the lower toxicity levels corresponding to higher concentrations with wider toxicity intervals will inherently have lower probability than the higher toxicity levels figure a 2 probability density functions of conditional probabilities for two neighbour toxicity intervals medium toxicity 0 5 5 mg l and low toxicity 5 100 mg l the shaded yellow area represents the probability that a toxicity test of a substance with true medium toxicity results in an observation of low toxicity conversely the shaded green area represents the probability that a toxicity test of a substance with true low toxicity results in an observation of medium toxicity note the logarithmic scale of the y axis figure a 2 
26056,a hybrid bayesian network bn was developed for predicting the acute toxicity of chemicals to fish using data from fish embryo toxicity fet testing in combination with other information this model can support the use of fet data in a weight of evidence woe approach for replacing the use of ju venile fish the bn predicted correct toxicity intervals for 69 80 of the tested substances the model was most sensitive to components quantified by toxicity data and least sensitive to compo nents quantified by expert knowledge the model is publicly available through a web interface fur ther development of this model should include additional lines of evidence refinement of the discre tisation and training with a larger dataset for weighting of the lines of evidence a refined version of this model can be a useful tool for predicting acute fish toxicity and a contribution to more quantitative woe approaches for ecotoxicology and environmental assessment more generally graphical abstract image 1 keywords hybrid bayesian network model acute fish toxicity fish embryo toxicity weight of evidence animal alternatives risk assessment abbreviations aft acute fish toxicity bn bayesian network cv coefficient of variation ec50 effect concentration for 50 of the test individuals echa european chemicals agency fet fish embryo toxicity lc50 lethal concentration for 50 of the test individuals qsar quantitative structure activity relationship reach registration evaluation authorisation and restriction of chemicals woe weight of evidence 1 introduction weight of evidence woe is a term commonly used about the assembly weighting and integration of multiple pieces of evidence for environmental assessment and decision making however the term has been used in multiple ways including metaphorical theoretical and methodological without consensus about its meaning weed 2005 historically environmental risk assessors have relied upon narrative and qualitative approaches such as expert knowledge or limited quantitative methods such as direct scoring to integrate multiple lines of evidence linkov et al 2009 these historical methods lacked transparency and reproducibility a review of woe methods applied for ecological risk assessment linkov et al 2009 showed that only 9 out of 44 published studies had used a quantitative approach scoring indexing or other type of quantification therefore woe approaches have been criticised for being too vague intransparent and subjective linkov et al 2016 nevertheless methods are needed for integrating evidence and assess their uncertainty to support decision making for environmental protection these processes would benefit from more quantitative and rigorous approaches to woe more systematic frameworks for woe approaches in scientific assessments have recently been proposed for example by the european chemicals agency echa 2016 by the european food safety authority efsa scientific committee et al 2017 and by the us environmental protection agency usepa 2016 to assist ecological assessment the basic steps in these frameworks are to 1 assemble evidence 2 to weight the evidence by assigning scores depending on e g relevance strength and reliability and 3 to integrate the evidence weigh the body of evidence suter et al describe how the us epa framework can be used to infer qualitative properties 2017a as well as quantitative estimates 2017b use of such frameworks can increase the consistency and rigor of woe practices and provide greater transparency than ad hoc and narrative based approaches the weight of evidence was first proposed as a bayesian statistical approach good 1960 in bayesian statistics a model is based on updating prior beliefs in probabilities of a hypothesis after evaluating of evidence in order to achieve a posterior belief in this context the woe is defined as the logarithm of the bayes factor which is calculated as the ratio of the posterior odds to the prior odds good 1985 in this paper we return to the bayesian origin of the woe term and develop a bayesian network to combine lines of information in a probabilistic model to support risk assessment of chemicals environmental risk assessment is a process for quantifying the probability of an adverse effect from a chemical exposure traditionally acute toxicity data from three trophic levels i e fish invertebrates algae are required for risk assessments however ethical considerations and recent eu regulation have required that the number of animals used for toxicity testing is kept at a minimum and that the use of vertebrate species such as fish should be avoided as far as possible oecd 1996 therefore various alternative methods have been developed and proposed to replace the use of live fish in toxicity testing lillicrap et al 2016 including predictive toxicity modelling luechtefeld et al 2018 legislative authorities in some geographic regions have encouraged the use of fish embryos which are in this context considered as a non protected life stage over more developed life stages such as juvenile fish the test of fish embryo toxicity fet oecd method 236 oecd 2013 has therefore been proposed and evaluated as an alternative to using juvenile fish for testing acute fish toxicity aft oecd method 203 oecd 1992 busquet et al 2014 previous studies show a good correlation of fet with the standard acute fish toxicity aft test belanger et al 2013 rawlings et al 2019 nevertheless it has been found that the fish embryo toxicity test alone is currently not sufficient to replace the acute fish toxicity data as required by the european reach regulation regulation evaluation authorisation and restriction of chemicals sobanska et al 2018 however the authors have suggested that the test may be used within weight of evidence approaches together with other independent relevant and reliable sources of information to this end we have developed a bayesian network bn model for integrating fish embryo toxicity data with other information to predict the acute toxicity to juvenile fish for any given chemical substance a bayesian network is a probabilistic graphical model over a set of random variables bayesian networks are commonly used in environmental modelling aguilera et al 2011 barton et al 2012 landuyt et al 2013 for example there are numerous bn models developed for assessment and management of water quality barton et al 2014 borsuk et al 2004 2012 moe et al 2016 2019 bn modelling has been applied more rarely within ecotoxicology and ecological risk assessment but recent publications have demonstrated the applicability of bn modelling in these fields graham et al 2019 landis et al 2017 2019 lehikoinen et al 2015 compared to traditional qualitative woe approaches the network structure and probabilistic framework of bn provide advantages by capturing the impacts of multiple sources of quantifiable uncertainty on predictions of ecological risk carriger et al 2016 another example of applying a bn model for a quantitative woe and testing strategy is provided by jaworska et al 2015 here we develop a bn model for integrating four lines of evidence in a quantitative framework 1 information on physical and chemical properties of the substance 2 toxicity data for chemically related substances 3 toxicity data for other species crustaceans and algae and 4 fish embryo toxicity data by implementing this woe model in a bayesian network we aim to meet the demands for making woe approaches more transparent structured and quantitative linkov et al 2016 the purpose of this paper is to describe the development parameterisation and evaluation of the bn model the paper will also present an online web interface to the model more detailed information on this model s performance for specific chemical substances and endpoints and the implications for current legislation and guidelines for toxicity testing are addressed by lillicrap et al 2020 feedback from researchers regulators or other potential users to this first bn version will be used for improving future versions of the model with the aim of making it a useful tool in a woe approach for predicting acute fish toxicity 2 materials and methods 2 1 data an expanded version of the threshold database rawlings et al 2019 was obtained from procter gamble and used to construct the bn this database contains acute freshwater toxicity values for fish aft fish embryos fet algae and invertebrates for 237 chemical substances the toxicity values are measured as ec50 effect concentration for 50 of the test individuals for non lethal effects and as lc50 lethal concentration of 50 of the test individuals for lethal effects the dataset used for development of this model which will be referred to as our dataset contained the following toxicity data algae 264 values of ec50 based on oecd test no 201 acute algal growth inhibition tests oecd 2006 daphnia 1164 values ec50 based on oecd test no 202 daphnia sp acute immobilisation tests oecd 2004 using crustaceans of the genus daphnia daphnids juvenile fish 1459 lc50 values based on oecd test no 203 acute fish toxicity test oecd 1992 fish embryo 541 lc50 values based on oecd test no 236 fish embryo test oecd 2013 in addition to the data obtained from laboratory assays modelled acute fish lc50 values were calculated using quantitative structure activity relationships qsars for each chemical including the us epa ecological structure activity relationship ecosar 1 11 models and the danish qsar database extracting values for leadscope and sciqsar sars data were then averaged for each chemical resulting in 152 qsar values representing modelled toxicity to juvenile fish more details on qsar model selection equation acceptability criteria and results can be found in lillicrap et al 2020 2 2 bn model objective and structure a bn model consists of a directed acyclic graph with nodes representing the random variables and arrows arcs representing conditional probability distributions kjærulff and madsen 2008 each node has a probability distribution conditional on its parents in the graph the probability distributions quantify the strengths for the dependence relations defined by the structure of the graph the nodes are typically defined by limited number of discrete states categories or intervals which are quantified by a prior probability distribution new evidence is combined with the prior probabilities to calculate posterior probability distributions using bayes rule from 1763 see e g linkov et al 2016 the objective of the bn model presented here is to predict the acute toxicity of a chemical substance to juvenile fish corresponding to the interval of lc50 values from the aft assay oecd 1992 by integrating toxicity data from testing of fish embryos oecd 2013 with other relevant physical chemical and toxicological information for the given substance the bn model is structured along the four lines of evidence fig 1 representing different types of information for a given chemical substance for which a user wants to predict the acute fish toxicity the parameterisation of all lines is described in supplementary material tables s 1 s 21 while more details on the assumptions are provided by lillicrap et al 2020 line 1 physical and chemical properties of the substance the selected nodes quantify the size molecular weight g mol hydrophobicity octanol water partitioning coefficient logkow and modelled toxicity to juvenile fish based on the structure qsar of the substance substances with a low molecular size and low hydrophobicity i e high solubility are assumed to have a high ability to cross a biological membrane for a substance that is modelled via qsar to be toxic low lc50 value and have a high ability of membrane crossing the predicted toxicity to juvenile fish from this line of evidence will have highest probability of the higher toxicity intervals line 2 chemical category of the substance this line of evidence is based on existing data on toxicity of substances to juvenile fish aggregated by chemical category the 237 chemicals used to parametrize this model were assigned to 42 different chemical categories table s 11 based on model output ecosar and further refined by expert knowledge s belanger line 3 toxicity to other taxa representing lower trophic levels crustaceans daphnia magna or d pulex or unicellular algae this line of evidence also considers whether the substance has a species specific or more generally taxon specific mode of action by examining the ratio of daphnia and algal toxicity if the ratio of toxicity to daphnia vs algae is below 0 5 daphnia has a considerably lower tolerance lower ec50 value to the substance that the algae in this case the substance s mode of action is assumed to be specific to invertebrates e g an insecticide likewise if the ratio is above 2 then the mode of action is assumed to be more specific to algae e g a herbicide in either case the substance can be assumed to have low toxicity to fish conversely if the toxicity ratio is between 0 5 and 2 then the toxicity to fish is assumed to be approximately equal to daphnia and algae and a toxicity at the same level although with more uncertainty is assumed for fish the conventional cut off values of 0 5 and 2 0 and more details underlying these assumptions are described by ecetoc 2005 line 4 toxicity to fish embryo the measured toxicity of a substance to fish embryos is used directly as the fourth line of evidence the development and application of this bn is comparable to a weight of evidence approach e g suter et al 2017a with the assignment of conditional probabilities to different variables within in a line of evidence in the bn corresponding to setting weights to pieces of evidence in a woe for example assignment of low weight to a piece of evidence can be obtained by setting wide probability distributions representing high uncertainty or variability in the relation from this node to its child node within a line of evidence e g line 3 toxicity to other taxa fig 1 the calculation of posterior probabilities for the last child node of this line toxicity to fish predicted from other taxa accumulates the weights given to all parent nodes in this line when using the bn to predict the toxicity to juvenile fish from all lines of evidence for a chemical of concern the weighing of the total evidence for each hypothesis in a woe suter et al 2017a can correspond to calculating the posterior probability of each toxicity level very low low etc in the bn for example given two lines of evidence with posterior probability distributions both centred around high toxicity the line with the more narrow probability distribution will typically contribute more to the posterior probability of high toxicity in the final child node toxicity to fish predicted from other taxa in a woe approach this would correspond to this line of evidence having a higher weight for the hypothesis high toxicity to juvenile fish than the line with a wider probability distribution the bn was implemented in the software hugin researcher version 8 7 developed by hugin expert a s http www hugin com the model is available through a web interface described in section 3 which is published on the demonstration web site http demo hugin com example fet the aim of this web interface is to facilitate feedback for further improvement of the model as recommended by marcot 2017 2 3 model parametrization 2 3 1 node types and discretisation bn models are usually constructed with discrete nodes with a low number of states such as categories or intervals kjærulff and madsen 2008 in these cases the bn is an efficient and compact representation of a joint probability distribution eq 1 1 p v x v p x p a x where v is the set of discrete nodes x 1 to x n and pa x are the parents of x an area of recent interest and progress is the development of continuous bn models qian and miltner 2015 where quantitative variables are not discretised into intervals but instead are represented by equations or statistical distributions or hybrid bns e g aguilera et al 2010 which contain both discrete and continuous nodes marcot and penman 2019 for our model categorical nodes with few states would be the most convenient for parameterisation of conditional probability tables in cases where expert knowledge was required on the other hand continuous nodes would be preferable to optimise the use of the continuous input values e g measured toxicity values and their variability consequently this model is a hybrid bn with both discrete and continuous nodes fig 1 in this model the continuous nodes are assumed to follow the conditional linear gaussian distribution eq 2 2 p y i i z z n α i j β j i z j γ i where y is a continuous node i is a set of discrete nodes z is a set of continuous nodes and i z is the set of parents of y as an intermediate step between the continuous nodes and the discrete categorical nodes we used discrete interval nodes table 1 for example the categorical node toxicity to daphnia level has five labelled states such as low and medium the corresponding interval node toxicity to daphnia interval has intervals such as 5 100 and 0 5 5 mg l in addition the interval nodes have two more extreme states with very low prior probability to avoid the dominance of extreme values explained below we let interval nodes represent the true but unknown toxicity of a substance this true toxicity property was considered to be the cause of the observed toxicity values therefore the toxicity observations were modelled by continuous nodes as children realisations of the interval nodes the parameterisation of all nodes i e priors conditional probabilities and conditional density functions is described in supplementary material tables s 1 to s 21 following the recommendations for good practice in bn modelling chen and pollino 2012 for the physical and chemical properties line 1 we chose only two intervals table 2 with cut off values commonly used in ecotoxicology molecular weight with the threshold 600 g mol brooke et al 1986 and hydrophobicity with the threshold log kow 5 5 oecd 2012 because the use of strict cut off criteria to define bioaccumulation potential has been criticized arnot et al 2010 the child node membrane crossing was defined by three intervals low medium and high to allow for intermediate levels of membrane crossing potential the mode of action node line 3 also had three intervals based on the pre defined cut off values of ratio 0 5 or 2 ecetoc 2005 the predicted toxicity nodes e g toxicity to algae level were initially modelled by categorical nodes with 5 states ranging from very low toxicity 100 mg l to very high toxicity 0 01 mg l note that increasing toxicity level corresponds to decreasing concentration of the substance table 3 the discretisation of continuous toxicity values for the discrete nodes table 3 was based on the classification and labelling of toxicity levels in the globally harmonized system oecd 2001 which has four toxicity levels with interval boundaries 1 10 and 100 mg l to increase the resolution of the highest toxicity levels we selected 5 intervals with the boundaries 0 01 0 5 5 and 100 mg l the integration of the four lines of evidence was based on these 5 state categorical nodes the toxicity interval nodes e g toxicity to algae interval were added to provide a link to the continuous nodes for input values e g toxicity to algae value 1 since interval nodes must have intervals in increasing order the order of the toxicity intervals was reversed compared to the order of toxicity states it was not strictly necessary to keep both the categorical and the interval version of the same toxicity node e g toxicity to algae but we found that it facilitated the interpretation and helped avoid confusion regarding the order of toxicity intervals vs toxicity levels an additional benefit of the interval nodes was to enable the extraction of a single expected value mean value from a toxicity node which could for example facilitate the comparison of posterior probability distributions of toxicity nodes for different substances for this purpose the toxicity interval nodes were given two additional extreme states table 3 with very low probability the extreme states served as buffers for obtaining reasonable calculations of mean values to summarise the probability distribution of a node in this paper however we will focus on the probability distribution of the 5 state categorical toxicity nodes rather than the mean values of the corresponding interval nodes the continuous input value nodes fig 1 allowed users to provide their toxicity data as original values ec50 or lc50 instead of discretised states the function of the continuous nodes is to create a likelihood on the interval nodes using multiple findings entered as exact values although the continuous value nodes serve as input nodes for this bn they are defined as child nodes of toxicity interval nodes fig 1 the reasoning is that the observed toxicity values for a given substance e g toxicity to algae value 1 are realisations of an inherent true toxicity value for this substance this true value is unknown but can be modelled by the probability distribution of the toxicity interval node toxicity to algae interval the bn has multiple continuous input nodes for each of the algae daphnia and fish embryo endpoints fig 1 the purpose was to let the bn model 1 retain as much details as possible from the input values 2 account for variability in the provided data i e variation among input values and 3 account for inherent uncertainty in the toxicity values e g a toxicity value closer to an interval boundary would have higher probability of being assigned to the next interval in the current version of the web interface there are ten continuous input nodes for each of these endpoints assuming that this is a reasonable maximum number of toxicity values available for a chemical substance in this dataset the number of observations per substance exceeds ten for only 3 19 and 7 of the substances for algae daphnia and fish embryos respectively if needed the number of continuous input nodes can easily be increased without affecting the model prediction the unused input nodes will be so called barren variables kjærulff and madsen 2008 which will be updated by evidence in their sibling nodes but will not themselves influence other nodes 2 3 2 prior probabilities prior probability distributions were defined for all root nodes i e all nodes with no parent nodes in the graph table 1 for all root nodes of interval type the prior probability distribution was set equal to the frequency distribution of our dataset this approach was used for prior probabilities of the following interval type nodes molecular weight table s 2 hydrophobicity table s 4 toxicity based on qsar table s 7 toxicity to algae table s 13 toxicity to daphnia table s 14 ratio toxicity daphnia algae table s 15 and toxicity to embryo table s 18 for example for the node molecular weight interval 231 substances had low molecular weight 600 g mol while only 2 substances had high molecular weight 600 g mol the resulting prior probability distribution across these two states was 99 14 and 0 86 table 2 for the toxicity interval nodes for qsar algae daphnia and embryo the probabilities of states were also based on the counts of values however because of low numbers of values in the most extreme intervals the counts were merged for the two intervals 100 1000 and 1000 inf and the probabilities of these intervals were set to respectively 99 9 and 0 01 of the proportion of values in toxicity level very low table 3 the probability of the intervals 0 001 0 01 and 0 0 001 likewise were set to respectively 99 9 and 0 01 of the proportion of values in toxicity level very high one root node was of categorical type chemical category table 1 since this was also an input node to be instantiated by the user the prior distribution was not of importance and was set to uniform distribution table s 11 2 3 3 conditional probability tables and density functions the conditional probability tables cpts of the bn were parametrized by four main approaches illustrated by the four examples in fig 2 1 expert knowledge fig 2a 2 frequency distributions derived from our dataset fig 2b 3 statistical considerations fig 2c and a rule for combining the lines of evidence fig 2d expert knowledge a lillicrap was used for cpt of the nodes membrane crossing table s 6 toxicity to fish predicted from chemical properties table s 10 and toxicity to fish predicted from other taxa table s 17 the ability of a molecule to crossing a biological membrane decreases both with its size and its hydrophobicity for membrane crossing therefore the combination of low hydrophobicity and low molecular weight was assumed to result in respectively 0 25 and 75 probability of low medium and high ability of membrane crossing while the opposite combination of hydrophobicity and molecular weight was assigned the opposite probability distribution the two combinations of high low and low high hydrophobicity and molecular weight were assigned a probability distribution of 25 50 and 25 for the three states of membrane crossing more details on the underlying assumptions are given by lillicrap et al 2020 we have assumed this very simple relationship as a starting point which can later be refined with more indicators of molecular properties an example is shown for toxicity to fish predicted from chemical properties fig 2a when a substance s ability to cross a biological membrane is high the predicted toxicity to fish based on chemical properties is equal to the acute fish toxicity modelled by qsar when the membrane crossing ability is low we assigned only 80 chance that the predicted acute toxicity corresponds to the modelled qsar result the reasoning behind the cpt of toxicity to fish predicted from other taxa is explained for all 50 combinations of the parent states by lillicrap et al 2020 in brief if the ratio of the average toxicity values to daphnia vs algae is between 0 5 and 2 it can be assumed that the chemical has a similar mode of action for the two taxa and that the chemical will affect fish in a similar way therefore the cpt converts the ec50 values from algae and daphnia to lc50 values for fish with high precision a narrow distribution which corresponds to assigning a high weight to these pieces of evidence in a woe approach moreover more weight i e narrower probability distributions has been assigned to the evidence from daphnia than from algae assuming that the closer phylogenetic relationship of fish with invertebrates than with plants makes their responses to a chemical more similar conversely a ratio of 0 5 or 2 indicates that the chemical has a species specific mode of action which affects daphnia more strongly than algae or vice versa in these cases we have less confidence in extrapolating the toxicity data from daphnia or algae to fish and have therefore set wider probability distributions in this part of the cpt this wider distribution corresponds to lower weighting in a woe approach cpts for the node toxicity to fish predicted from chemical category as well as for all continuous nodes identified in table 1 were created using frequency distributions derived from our dataset the cpt for toxicity to fish predicted from chemical category is illustrated in fig 2b for each chemical category the values represent the proportion of juvenile fish lc50 values in each of the given toxicity levels for example the 50 observations in the chemical category aniline comprised 4 observations of the toxicity level very low 40 observations of low 4 observations of medium and 2 observations of high from altogether 13 chemical substances the number of observations per chemical category was sometimes quite low e g imidazole fig 2b to optimise the use of the available data multiple toxicity observations for the same substance were counted independently for chemical categories with a low total number of observations table s 11 the proportions would not properly represent a probability distribution marcot 2017 therefore the model predictions will be less reliable for these chemical categories in addition a chemical category unknown other can be selected when this state is chosen this line of evidence is excluded from the final predicted toxicity node for the continuous nodes the conditional probability distributions were quantified by a conditional density function cdf defined by a gaussian distribution with mean and variance specified for each interval of the parent node i e the corresponding interval node see table 1 for the physico chemical nodes of line 1 mean values were calculated to reflect the chemicals in our dataset variances were calculated according to a pre defined coefficient of variation cv standard deviation divided by the mean we chose not to let the variance be calculated directly from our dataset as the variance would have been sensitive to the number of observations in each interval instead we assumed low cv of physico chemical properties of substances such as molecular weight 10 cv and hydrophobicity 5 cv for example for the node molecular weight value the cv was set to 5 of the mean value in the cdf for this node table s 3 the two intervals 600 g mol and 600 g mol had mean values of 179 and 200 380 respectively calculated from the respective counts of 231 and 2 observations see table 2 for all continuous nodes representing toxicity values based on qsar algae daphnia or embryo the density functions were defined in the same way the mean for each interval of the parent node was calculated as the midpoint of the interval except for the extreme interval 1000 inf here the mean was set to 1550 which was equal to the lower boundary plus the midpoint of the previous interval for each interval the variance was set so that a 90 of a gaussian distribution was within the interval while 5 of the distribution was in either of the neighbour intervals see appendix a figure a 1 the resulting variances fig 2c table s 8 correspond to ca 30 cv of the upper interval boundary for all intervals except the highest inf for toxicity data from standard assays performed in various laboratories cv up to 30 is a reasonable assumption rawlings et al 2019 the cpt of the final child node toxicity to fish predicted from all evidence was defined by a combination rule to ensure that all four lines of evidence were given equal weight following the recommendation of marcot 2017 to obtain more tractable cpt dimensionality we first set the probabilities for combination of two lines fig 2d the combination of lines 1 2 and of lines 3 4 each resulted in two intermediate child nodes which were subsequently combined with the same rule fig 2d to produce a new grandchild node the two intermediate nodes were then absorbed which resulted in a cpt combining the four grandparents nodes with the one grandchild node as described in the supplementary material table s 21 2 4 model assessment 2 4 1 sensitivity analysis the sensitivity of the target node toxicity to fish predicted from all evidence to the different lines of evidence was analysed by two methods parameter sensitivity analysis and value of information analysis the parameter sensitivity analysis measures the functional relationships between a parameter i e a probability value in the cpt of a node and the posterior probability of a given state of the target node chan and darwiche 2002 in general the posterior probability p h ε is a ratio of two linear functions eq 3 in any parameter t of the bn where t is an entry in a cpt of the model 3 p h ε t p h ε t p ε t α t β γ t δ where ε denotes the evidence and h denotes a specific state of the target node here referred to as the hypothesis the sensitivity analysis was run relative to all states toxicity levels of the target node under the default scenario of no evidence i e no data were used to run the model due to the structure of the evidence in this scenario i e no downstream evidence relative to the target variable the functional relationship shown in eq 3 is linear coupé and van der gaag 2002 value of information voi analysis can quantify the potential benefit of additional information in the face of uncertainty keisler et al 2014 and can aid the decision on allocation of resources between obtaining new information and improving management actions mäntyniemi et al 2009 the method quantifies the voi as reduction of entropy a measure of uncertainty for a given node in isolation averaging the values of other nodes the voi analysis was run under different scenarios of evidence i e different information applied to the model the default scenario of no evidence as well as data for three example substances using all available data either for the first three lines of evidence excluding embryo data or for all four lines of evidence the purpose was to assess the potential benefit of including embryo data as a line of evidence 2 4 2 model evaluation the bn model performance and uncertainty was evaluated by several of the metrics recommended marcot 2012 model performance was assessed by running with input data from our dataset and comparing the outcome i e the predicted acute toxicity of selected chemical substances to juvenile fish to the observed toxicity of the same substances to this endpoint the model validation should ideally be performed with an independent dataset that had not already been used for parametrization of the model however additional fish embryo toxicity data are not widely available and our dataset represents the largest collection of oecd 236 fet data in the public domain to our knowledge instead we applied four different criteria for selecting subsets of our dataset for validation in order to consider the robustness of the model from different perspectives subset 1 all chemical substances containing at least one observation of juvenile fish toxicity aft data number of substances 159 which was the minimum requirement for comparison with the model prediction for substances missing one or more input values e g toxicity to algae the prediction would be based on the prior probabilities of this node the exception was line 2 toxicity of the chemical category which allowed for the state unknown subset 2 only the chemical categories that were represented with minimum 10 different chemical substances in our dataset number of substances 106 using phenol as an example the conditional probabilities of a chemical category in the node toxicity to fish predicted from chemical category fig 2b was calculated as the frequency distribution of observed toxicity to juvenile fish for all substances in the category phenol when the predicted toxicity of a substance belonging to the category phenol was assessed e g triclosan the observed toxicity values of triclosan used for comparison have also been used in the cpt for chemical categories containing few substances in our dataset there will be a high overlap between the data used for parametrization and for validation for chemical categories with 10 or more substances however the data used for validation for one of these substances will constitute only a small proportion of the frequency distribution of the cpt therefore a validation based on such subsets will be more independent of the parametrization data subset 3 all substances with complete cases for input nodes number of substances 77 this means that our dataset contained minimum one value for each node defined as input node table 1 molecular weight hydrophobicity qsar toxicity to algae toxicity to daphnia and toxicity to embryo as well as toxicity to juvenile for this subset the validation would be less influenced by the prior probabilities tables 2 and 3 since evidence would be available to update the probabilities of all input nodes subset 4 cases with a minimum of 3 observations for both embryo and juvenile toxicity data number of substances 20 the reasoning was that these two nodes are the most crucial for the purpose of the model a higher number of observations would make the model predictions more precise and reduce bias due to variability in these types of data for the purpose of comparing predicted and observed probability distributions it is common to define the state with the highest probability as the correct state marcot 2012 we followed this practice and defined the toxicity interval with the highest probability as the correct predicted or observed state this way we could calculate the number of correct predictions for each toxicity interval and for each data subset this practice does not however account for the whole probability distribution more advanced methods for model validation consider the probability distributions of predicted and observed variables will be investigated in further work with this model 3 web interface to the bn model a web based user interface to the bn model for demonstration purposes has been published on hugin s web portal for bn examples http demo hugin com example fet our model is the first example of a bn within the field of ecotoxicology and ecological risk assessment on this platform the web interface to this preliminary bn version will give researchers and other potential users the opportunity to provide feedback for improving the model in the longer term our intention is to further develop this model into a more comprehensive online tool which can be useful for regulatory authorities and chemical industries wanting to submit fish embryo toxicity data in place of acute fish toxicity data here we briefly describe the two interactive pages of the web interface enter values supplementary information figure s 1 and results figure s 2 in the tab enter values a user can insert the requested information for any chemical substance for which they want to predict the acute toxicity to juvenile fish the user should provide the requested information as follows for the four lines of evidence line 1 hydrophobicity log kow molecular weight g mol toxicity based on qsar mg l the user must enter one continuous value for each node line 2 chemical category the user must select the category from a drop down list which includes the state unknown other more chemical categories can be added upon request from users line 3 toxicity of the substance to other taxa algae oecd 201 and daphnia oecd 202 the user should first select the number of values up to ten for each taxon then enter the ec50 values mg l line 4 toxicity of the substance to fish embryos oecd 236 the user should first select the number of values up to ten then enter the lc50 values mg l the tab enter values contains the buttons compute which will carry out the calculation based on these input values and load case which will load a built in example the substance carbamazepine the tab results displays the posterior probability distributions across the five toxicity levels for all four lines of evidence toxicity to fish predicted from chemical properties toxicity to fish predicted from chemical category toxicity to fish predicted from other taxa and toxicity to embryo level as well as for the final child node toxicity to fish predicted from all evidence see supplementary information two buttons generate tables as pop up windows the button view input values provides a table of the entered values as well as the calculated ratio of toxicity to daphnia vs algae table 4 a this table also identifies the most sensitive endpoint algae daphnia or fish embryo which is relevant for the application of these results for regulatory risk assessment lillicrap et al 2020 the button view output values provides a table with the posterior probability distributions for the five selected nodes mentioned above table 4b the results tab also provides conclusive statements based on the calculated values such as the toxicity level of carbamazepine to juvenile fish is most likely low 52 28 probability 4 results and discussion 4 1 examples of bn model predictions examples of model predictions for three selected substances are presented in fig 3 all examples are from subset 4 see section 2 4 2 which has minimum three toxicity data for both embryo and juvenile fish the three examples represent three different levels of observed toxicity to juvenile fish low medium and high respectively the first example fig 3a is carbamazepine an antiepileptic drug in the chemical category substituted urea this substance is also used as a built in example in the online demonstration model figures s1 s2 the observed toxicity to juvenile fish to which the predicted toxicity will be compared is 100 in the low toxicity interval four observations not shown the node toxicity to embryo level was almost 100 very low while the toxicity based on qsar algae and daphnia was low to very low the information from the chemical category however indicated a 50 probability of high toxicity to juvenile fish this line of evidence contributed to higher predicted toxicity to juvenile fish 26 probability of medium or higher toxicity than the other three lines after combination of the four lines the most likely state was low toxicity to juvenile fish 52 probability which was consistent with the measured toxicity interval although with higher uncertainty in this case information on toxicity to embryo very low alone would have underestimated the risk to juvenile fish low while the embryo data in combination with the other three lines of evidence resulted in a more accurate prediction on toxicity to juvenile fish the second example fig 3b was tetradecyl sulfate a drug in the chemical category anionic surfactant our dataset contained three lc50 values from aft assays toxicity to juvenile fish all of which fell into the medium toxicity interval the node toxicity to embryo level predicted a 100 probability the high toxicity interval while the predictions from the other lines of evidence were centred around low to medium toxicity interval the resulting predicted toxicity to juvenile fish had highest probability 41 of the medium state which was consistent with the measured toxicity in this case information on toxicity to embryo alone would have overestimated the risk to juvenile fish while the embryo data in combination with the other lines of evidence again resulted in a more accurate prediction although with lower precision than the original aft data the third example fig 3c triclosan is an antimicrobial agent in the chemical category phenol this substance had six lc50 values for juvenile fish in the high toxicity interval the data based on embryo daphnia and qsar showed the same toxicity level while toxicity to algae was very high toxicity based on the chemical category on the other hand was almost 50 likely to be medium or lower when the four lines of evidence were combined the most probable toxicity level was high 60 which was again consistent with the observations 4 2 sensitivity analysis the parameter sensitivity analysis table 5 showed that under the default evidence scenario of no evidence the state very high of the target node toxicity to fish predicted from all evidence as expected was the most sensitive to changes in single parameters in the probability tables of the parent nodes the target node was most sensitive to parameters of the nodes toxicity to embryo interval and toxicity based on qsar interval the maximum sensitivity value averaged across all states of the target node was 0 16 for both parent nodes this means that an increase of 0 1 in the parameters of toxicity to embryo interval will result in an increase in 0 016 in the posterior probability of toxicity to fish predicted from all evidence see eq 3 the highest sensitivity was found for the state high toxicity of the target node with slopes 0 21 qsar and 0 19 embryo these nodes are root nodes table 1 with probability tables containing prior probabilities tables s 6 and s 18 the analysis shows that the prior probabilities of these nodes may have a strong influence on the prediction which implies that the approach used for setting priors appendix a may need to be refined the third most influential parameters were for chemical category to which the sensitivity of the target node was 0 08 on average and 0 12 for the state high toxicity the prior probability table of this root node has a uniform distribution assuming that for a new chemical substance all chemical categories are equally likely but considering the high influence of this probability table on the target node future research may explore a more informative prior probability distribution e g which better reflects the frequency of the different chemical categories in our dataset or other larger datasets the fourth and fifth most influential parameters were for toxicity to daphnia interval and toxicity to algae interval the influence of these nodes on the target node were weakened by the additional node species specific mode of action which is meant to introduce uncertainty related to the extrapolation of results from plants and invertebrates to fish the target node is slightly more sensitive to changes in parameters for daphnia than for algae this is consistent with the closer phylogenetic relationship of fish with invertebrates than with plants which we tried to account for in the cpt table s 17 lillicrap et al 2020 in general this sensitivity analysis reflects the strongest influence of parameters in the parts of the model that are most directly based on data such as toxicity based on qsar and toxicity to embryo conversely the analysis indicated weaker influence of lines of evidence that rely strongly on expert knowledge such as membrane crossing and toxicity predicted from other taxa although these lines of evidence are also based on data e g measured toxicity to algae and daphnia expert knowledge is applied in weighting and combining these pieces of evidence hence there is a potential for making the model more sensitive by refining the cpts that are currently based on expert knowledge e g fig 2a the sensitivity analysis also shows that the posterior distribution of the target node is robust to changes in any single parameter of the bn as all sensitivity values are less than one the value of information voi analysis table 6 showed that under the default scenario of no evidence all four lines of evidence contribute almost equally and relatively little to entropy reduction no more than 7 0 09 1 36 of the entropy of the target node line of evidence 3 other taxa contribute slightly more than the other lines this can reflect the fact that the conditional probabilities of toxicity predicted from other taxa table s 17 can be either highly correlated with the target node or be non informative flat distribution depending on the input values node ratio toxicity daphnia algae lillicrap et al 2020 for scenarios where information is provided only for the lines of evidence 1 3 i e excluding information on toxicity to embryos additional information on embryo toxicity can reduce the uncertainty of the target node by 5 7 table 6 the relative importance of information on embryo toxicity varies among the three example substances which suggests that the importance of this type of information cannot be generalised across substances under the scenario of full information lines 1 4 further information on embryo toxicity will not contribute to reduction in entropy this result suggests that the information on embryo toxicity used in each example has already exerted a strong influence on the prediction which is consistent with the outcome of the parameter sensitivity analysis table 5 4 3 model performance the performance of the bn for the four different subsets of our dataset is reported in table 7 and summarised in fig 4 for each substance the predicted juvenile toxicity state with the highest probability was compared with the most frequently observed juvenile toxicity state the first three subsets which have a number of substances ranging from 77 to 159 showed very similar results the percentage of correctly predicted toxicity level was 69 71 while the percentage of overestimated toxicity was 14 18 and the percentage of underestimated toxicity was 12 16 the model typically overestimated toxicity when the observed toxicity was very low or low conversely the few observed cases of high toxicity were sometimes underestimated as medium the selected subsets of dataset were dominated by substances with very low to medium toxicity which can explain the slightly higher proportion of cases with overestimated toxicity if the model is applied to a new substance with high toxicity the model is more likely to underestimate its toxicity as medium than to overestimate it as very high the fourth subset which had the strictest criteria for selection of substances minimum 3 observations of both juvenile and embryo had the highest correct prediction rate 80 although this increase in prediction rate may simply be a random change due to the lower sample size n 20 this improvement could also indicate that the higher number of observations leads to more accurate predictions and therefore better model performance the better performance of this final subset therefore lends support to our decision of designing the bn with multiple continuous input nodes which allows for the use of more observations in practice the risk assessment for a chemical substance is determined by the most sensitive endpoint algae daphnia or fish for example for the selected substances in subset 4 table 7d the four substances with underestimated toxicity to juvenile fish were all more toxic to algae or daphnia than to fish embryos in such cases the predicted toxicity to juvenile fish is less relevant since the risk assessment will be driven by another endpoint for this reason in the web interface to the model information on the most sensitive endpoint is extracted from the input data and provided in the input table table 4a and in the conclusive statements figure s 2 these implications are further discussed by lillicrap et al 2020 4 4 further development of the bn model although this preliminary version of the bn model shows a high rate of correct predictions up to 80 fig 4 the model performance can be further improved moreover the model showed relatively low value of information for reducing uncertainty of model predictions as indicated in table 6 which suggests that the model needs refinement for making better use of provided input data cf table 4a to increase the model sensitivity to the lines with weakest influence and to improve the accuracy of model predictions the following issues should be addressed for most nodes in lines 1 3 fig 1 which do not depend on fish embryo toxicity data the prior probability tables and conditional probability tables could be parametrized based on independent datasets this way our dataset could be reserved for model validation a candidate dataset for model parametrization is the aquatic toxicology database envirotox connors et al 2019 however the envirotox database has not yet received the same level of curation as our dataset and may introduce additional noise into our model for components of the model where data are not available the cpts based on expert knowledge by the authors can be strengthened by a more formal approach for external expert elicitation e g castelletti and soncini sessa 2007 marcot 2017 the toxicity intervals are relatively wide some spanning more than an order of magnitude table 3 toxicity nodes with higher resolution more toxicity intervals may be perceived as more informative however a model with more than 5 toxicity levels would require different approaches to parametrization of the cpts whether these are based on expert knowledge e g fig 2a or on frequency distributions e g fig 2b for future versions of this bn we will explore more automated approaches to parametrization of ctps to obtain smoother probability distributions and to avoid the extreme 0 and 100 probabilities the width of the toxicity intervals is decreasing exponentially e g the state low has width 95 mg l medium has width 4 5 mg l and high has width 0 49 mg l while this non linear scale reflects the intervals and threshold values typically used in regulatory ecotoxicology it introduces a bias in the probability calculations explained in appendix a figure a 2 lower toxicity levels corresponding to higher concentrations with wider toxicity intervals will inherently have lower probability than higher toxicity levels the bn model may therefore show a tendency of overestimating toxicity fig 4 from regulatory point of view such a bias may not be a problem since the biased predictions will be more protective for the environment however while risk assessment calculations often have built in safety factors to obtain more protective assessments we aim for accuracy of the model predictions for this bn model a possible solution for reducing the bias is to model the toxicity values on logarithmic scale so that the toxicity intervals will get more equal widths the four lines of evidence has so far been integrated by equal weighting as a starting point the cpt for combining the four lines table s 21 could instead be trained by data to optimise the weighting of the four lines however training the model by advanced methods such as machine learning algorithms marcot and penman 2019 will require a higher number of substances with complete set of input data including fish embryo toxicity data than what is currently available in principle this hybrid bn could be further developed into a continuous variable bn an example of a continuous variable bn modelling framework developed for setting nitrogen criteria in streams and rivers is presented by qian and miltner 2015 their model retained the bn s graphical representation of hypothesized causal connections among variables while employing statistical modelling approaches for establishing functional relationships among these variables a continuous bn model can avoid the problems related to discretisation of continuous toxicity value into intervals nojavan a et al 2017 and provide more precise predictions however constructing a continuous variable bn poses other challenges in our case converting the bn into a continuous model would require more advanced approaches to model parameterisation especially for cpts that are currently based on expert knowledge 5 conclusions we have developed a bayesian network model for predicting the acute toxicity of chemical substances to juvenile fish based on information on fish embryo toxicity in combination with physical and chemical properties chemical category and toxicity of the substance to other taxa this model can support a weight of evidence approach for replacing the oecd 203 acute fish toxicity assay based on juvenile fish with animal alternative approaches such as the oecd 236 fish embryo toxicity assay as a measure of model performance the bn predicted the correct toxicity interval for 69 80 of substances in our dataset given different quality criteria for the subset of 20 substances with the highest quality criteria the prediction rate was 80 correct the model predictions were most sensitive to the model components that were quantified by toxicity data such as fish embryo and qsar conversely the model was least sensitive to the components that were quantified by expert knowledge e g involving the chemical s mode of action inferred from testing of other taxa the model is publicly available through a web interface for testing and feedback future development this model should include more lines of evidence refinement of the discretisation of toxicity intervals training of the model with a larger toxicity dataset to weight the lines of evidence differently a more mature version of this model can be a useful woe tool for predicting fish acute toxicity from fish embryo toxicity data the model will also be a contribution to the trend of developing more quantitative weight of evidence approaches which are needed both in the context of animal alternatives in ecotoxicology and in environmental assessment more generally declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding this work was supported by niva s research programme digisis new digital methods for monitoring and research we thank the norwegian environment agency and members of the animal alternatives in environmental science interest group of setac society of environmental toxicology and chemistry for feedback on earlier versions of the model and web interface and three anonymous reviewers for their helpful comments author contributions conceptualization a d l s e b and s j m data curation k a c j m r and s e b data analysis s j m a l m and r w methodology s j m a d l w g l a l m and r w funding acquisition s j m and a d l project administration s j m web interface a l m visualization s j m writing original draft s j m and a l m writing review editing s j m r w a l m a d l k a c j m r w g l appendix a supplementary data the following are the supplementary data to this article appendix a contains explanation to the conditional probability tables for continuous value nodes figures a 1 and a 2 multimedia component 1 the file fet bn supplementary figures 20190828 docx contains pictures figures s 1 and s 2 showing an example of running the bn model from the web interface http demo hugin com example fet multimedia component 1 multimedia component 2 the file fet bn supplementary tables 20190828 docx contains the tables of conditional dependencies of all nodes in the bn model tables s 1 to s 21 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104655 appendix a conditional probability distributions for continuous toxicity value nodes the continuous toxicity values nodes for qsar algae daphnia and embryo are input nodes for the bn model but also child nodes of the respective interval type nodes therefore their conditional probability distributions must be specified by mean and variance for the seven toxicity intervals as described in section 2 4 3 conditional probability tables figure a1 illustrates the probability distributions for each of the states ranging from very high toxicity a to very low toxicity e defined as gaussian probability density functions for each interval the variance of the probability density function was set so that 90 of the distribution was within the interval while 5 of the distribution was in either of the neighbour intervals for example for the interval medium toxicity figure a1c 90 of the area below the curve is contained within the interval 0 5 5 mg l 5 of the area is in the range 0 5 mg l high toxicity or higher and 5 of the area is in the range 5 mg l low toxicity or lower this can be interpreted as follows let us assume that the true but unknown toxicity of a substance to algae is in the interval medium toxicity then there is a 5 probability that a toxicity test of this substance will wrongly result in low toxicity to algae and 5 probability that the test will wrongly result in high toxicity for all intervals displayed in figure a1 the resulting variances fig 2c table s1 13 correspond to ca 30 cv of the upper interval boundary i e the lowest toxicity of the interval figure a 1 probability density function used for setting conditional probabilities of the continuous interval nodes for the five toxicity intervals ranging from very high a to very low e coloured curves are the probability density function generated by gaussian distribution with mean and variance as specified in fig 2c vertical lines indicate the toxicity interval boundaries and the grey areas represent the probability of values occuring in this interval figure a 1 when the bn model is run the cpt of the continuous toxicity nodes e g toxicity to algae value 1 are used together with evidence inserted continuous toxicity values to update the probabilities of the interval nodes e g toxicity to algae interval by backward calculation since the conditional probability distributions of the higher toxicity states have more narrow intervals than the lower toxicity states the higher toxicity states will generally have higher probability density figure a1 as a consequence if one enters a continuous toxicity value that is close to the lower boundary i e close to the higher toxicity interval then the posterior distribution of the interval toxicity node is more likely to be in the higher toxicity interval for example figure a2a consider an observed toxicity value of 6 mg l which belongs to the interval low toxicity 5 100 mg l however for this interval the conditional probability density of 6 mg l is only 0 0038 while for the neighbour interval medium toxicity the conditional probability density is five times as high 0 019 therefore this observation will result in a relatively high posterior probability of the medium toxicity interval in contrast consider an observed value is 4 mg l which belongs to the interval medium toxicity but is close low toxicity the probability density of this value of the correct interval medium toxicity is 0 19 figure a2b which is 56 times as high as the probability density of the neighbour interval low toxicity 0 0034 hence this observation result in virtually zero probability of low probability in summary the lower toxicity levels corresponding to higher concentrations with wider toxicity intervals will inherently have lower probability than the higher toxicity levels figure a 2 probability density functions of conditional probabilities for two neighbour toxicity intervals medium toxicity 0 5 5 mg l and low toxicity 5 100 mg l the shaded yellow area represents the probability that a toxicity test of a substance with true medium toxicity results in an observation of low toxicity conversely the shaded green area represents the probability that a toxicity test of a substance with true low toxicity results in an observation of medium toxicity note the logarithmic scale of the y axis figure a 2 
26057,systems models to improve ecosystems often identify flows to meet minimum instream flow requirements or minimize deviations from a predefined flow regime here we present a new systems optimization model that determines when where and how much to allocate scarce water financial resources and revegetation efforts to improve aquatic floodplain and wetland habitat areas and quality this optimization is subject to constraints on water mass balance vegetation growth infrastructure capacities and meeting existing agricultural and urban water demands we followed a participatory approach to apply and validate our model in the lower bear river watershed ut results show that increasing winter reservoir releases minimizing spring spills and planting native floodplain vegetation early in the growing season can increase suitable habitat area beyond managing water alone additional flow on the little bear river between august and december will most increase habitat area and quality compared to other locations keywords systems models river habitat quality reservoir operations floodplain revegetation bear river utah watershed management web mapping application software and data availability name of software watershed area of suitable habitat wash optimization model developers ayman h alafifi and david e rosenberg contact aafifi aggiemail usu edu year first available 2017 hardware required a personal computer software required general algebraic modeling system software gams with non linear global solver such as branch and reduce optimization navigator baron ms excel 2016 r 3 3 0 and a web browser software availability all source code input data post processing file and documentation are available on alafifi 2017 the application of wash to the lower bear river utah for one year 2003 is displayed on an open access web map at https www washmap usu edu the source code is available on github at https github com ayman510 wash cost the source code is released under the bsd 3 clause which allows for reuse of the code 1 introduction rivers and their riparian and wetland areas are managed to supply domestic and agricultural water users generate hydropower reduce flood damages and support habitat for flora and fauna bernhardt et al 2005 although river managers often prioritize human beneficial uses flow management can also improve habitat jager and smith 2008 tharme 2003 improving river habitat requires defining ecological objectives and determining the timing magnitude and locations of reservoir releases diversions and restoration efforts to advance the objectives determining timings magnitudes and locations of management efforts often requires navigating a complex set of ecological hydrological modeling and stakeholder considerations first managers must identify and locate the aquatic floodplain and wetland habitat areas in the basin that need improvement second they should select indicator species from among the numerous species available in each habitat whose habitat needs should be satisfied or improved habitat needs can encompass flow vegetation and other conditions third managers may use models to mathematically quantify each indicator species response to habitat conditions and finally managers must collaborate with watershed stakeholders to identify when where and how to allocate water financial resources revegetation efforts to improve habitat while meeting other water uses in the basin barbour et al 2016 some quantification and modeling approaches such as the natural flow paradigm estimate species hydrologic requirements to mimic important timing duration magnitude and frequency features of the natural flow regime poff et al 1997 these approaches assume that historical natural flows are known and adequate to support ecosystem functions baron et al 2002 other approaches such as habitat suitability indices hsi u s fish and wildlife service 1981 and derivatives hickey and fields 2013 use empirical relationships to describe the suitability of habitat to support the survival and productivity of a single species suitability is a function of single or multiple habitat attributes such as instream water depth water temperature substrate or flow duration hsi values range from 0 poor to 1 excellent hemker et al 2008 hooper 2010 pinto et al 2009 the weighted useable area wua method multiplies the habitat suitability index by the reach surface area and divides by reach length stalnaker 1995 wua can be used to describe habitat quality for a particular species at a specific site and time under prior or specified flow scenario garcia rodriguez et al 2008 moir et al 2005 souchon and capra 2004 these approaches cannot determine whether a proposed flow regime is feasible nor do they recommend locations timings or magnitudes of water allocations or other restoration efforts across a watershed to improve multiple habitat types and species water resources systems models can include multiple ecosystem assets as part of a connected network of reservoir river tributary diversion demand and return flow components and can determine the feasibility of proposed flow regimes many models include habitat considerations as constraints such as to meet a minimum required instream flow see for example cioffi and gallerano 2012 harman and stewardson 2005 porse et al 2015 ryu et al 2003 in other cases a suitability index is maximized or minimized as a single objective or tradeoff with water delivery hydropower generation or other objectives null et al 2014 simonović and nirupama 2005 yang 2011 alternatively models try to minimize deviations from a pre defined target value for example higgins et al 2011 developed a heuristic nonlinear integer optimization model to minimize the difference between managed and natural flow regimes in the murray river australia steinschneider et al 2014 used linear programming to minimize the deviation between model recommended reservoir releases and estimated natural flows in the connecticut river basin szemis et al 2012 2014 developed a heuristic ant colony nonlinear model for the murray river to minimize the inverse of an ecological index plus constraint violations minimizing deviations from an ecosystem target poses challenges because managers must define the target such as natural flow regime or species required flow barbour et al 2016 further indices may not have physical meaning and are difficult to measure validate and communicate also changes in the deviation objective depend on how close the current system state is to the ecological target managers who minimize indices or deviations from targets find it difficult to identify opportunities to improve habitat and compare potential improvements across watershed sites and time additionally all these models focus solely on managing water to improve flow based habitat objectives this paper develops the watershed area of suitable habitat wash systems model which formulates and embeds a suitable habitat area metric as an ecological objective to maximize suitable habitat area represents the combination of habitat quality and area is based on flow and vegetation conditions that support and enhance the life needs of priority species and is measured as the sum of monthly suitable aquatic floodplain and wetland habitat areas across the watershed for each priority species this summation means suitable areas are specific to the species and species life stage and can vary though time wash synergistically recommends water allocation and revegetation efforts to improve suitable habitat area for priority species this modeling approach allows managers to i compare ecological measures across sites ii identify where and when to apply scarce water money and planting efforts to most improve habitat quality and area and iii involve stakeholders to help define indicator species view and validate results section 2 introduces the study area for the lower bear river utah section 3 describes the model formulation and system components the remaining sections present results implications to manage water and vegetation and conclusions 2 study area the lower bear river lbr study is part of the longer 491 mile bear river that starts in utah flows north through wyoming and idaho then returns south to utah the study area includes the bear river from the utah idaho state line to the river s terminus at the great salt lake and tributaries fig 1 the utah division of water resources 2004 estimates that approximately 60 of lbr flow comes from snowmelt runoff in april may and june the river and its tributaries irrigate over 300 000 acres of agricultural land and supply water to numerous cities and communities as well as run of river hydroelectric plants udwr 2004 udwre 2000 the river is central to a development debate for several growing areas within and outside the basin such as cache and box elder counties and the wasatch front metropolitan region to the south udwr 2004 udwre 2000 the lbr is also vital to maintaining critical wildlife habitat for many native and threatened fish riparian plants and migratory bird species bio west 2015 intensive urbanization water development fish barriers and grazing for over a century have disturbed flow regimes for native and game fish species reduced floodplain connectivity and altered native plant community composition bear river cap 2008 bio west 2015 at the river s terminus at the great salt lake the u s fish and wildlife service fws manages the 300 km2 bear river migratory bird refuge hereafter the bird refuge here impounded wetlands provide feeding resting and breeding grounds for over 250 globally significant populations of migratory birds alminagorta et al 2016 according to western u s prior appropriation doctrine and utah state water law the bird refuge holds a more recent water right that is junior to more senior upstream agricultural users downard et al 2014 thus senior irrigators take their entire water rights before the refuge receives any water most other land in the lbr is privately owned and few formal or legal mechanisms exist to provide water to improve fish riparian plant and migratory bird habitats lane and rosenberg 2020 the nature conservancy trout unlimited landowners and local state and federal agencies have identified low flows as a major threat to fish populations riparian plants and migratory birds in the watershed bear river cap 2008 the current regime is highly disturbed this condition makes it extremely difficult and unlikely to return to the natural without development regime further the natural regime is not known thus systems modeling can help managers identify synergistic water volume timing location and planting area efforts to improve ecosystem health for priority aquatic floodplain and wetland species over current conditions 3 model development improving river habitat quality and area requires a collective effort among researchers managers and stakeholders to identify habitat types priority sites indicator species habitat characteristics suitability of habitat for species and the network of water system components here we demonstrate a participatory approach to develop a systems model that can recommend scarce water financial resources and revegetation efforts to improve habitat area and quality we began by engaging with the managers organizations and stakeholders working to implement the bear river conservation action plan bear river cap 2008 working with these individuals and organizations we selected key indicator species collected model input data and formulated model scenarios below we describe the general model formulation of decision variables objective function and constraints in the formulation capitalized terms represent variables lower case terms indicate parameters and model inputs and lettered subscripts denote indices for space time species and habitat types bottom of fig 2 3 1 select indicator species the presence and abundance of indicator species is a strong signal of ecosystem health and response to alterations in flow regimes carignan and villard 2002 we identified key native and game fish riparian plants and wetland migratory bird species in the lbr watershed based on their abundance in the watershed and sensitivity to changes in flow regimes for each species we defined suitable ranges of habitat attributes such as water depth and flood recurrence in each month and for each life stage table 1 we derived habitat attribute ranges from literature empirical studies and other models and verified them with project stakeholders fish spawning seed recruitment and migratory bird feeding nesting and breeding occur during particular months we selected a monthly time step t for wash because watershed managers manage flows and plan revegetation efforts at monthly intervals 3 2 decision variables decision variables include water management and planting area actions to improve habitat quality managers can adjust reservoir releases rr v t million cubic meters per month mm3 at reservoir v in month t they also control diversion volumes qj dem t mm3 month from the river at node j to demand sites dem in month t to satisfy urban and agricultural demand managers can also plant rv j k t n mm2 the floodplain adjacent to the river reach from node j to node k during month t by seeding or planting species n these variables control a group of state variables that include reservoir storage volume stor v t mm3 reservoir surface area ra v t mm2 river flow q j k t mm3 month from node j to node k in month t river water depth d j k t m month channel surface area aj k t mm2 channel width wd j k t m and floodplain plant cover c j k t n mm2 3 3 objective function the objective function maximizes the weighted sum of the suitable areas of aquatic ind aquatic j k t floodplain ind floodplain j k t and wetland ind wetland j k t habitats mm2 across reaches j to k and time month t where w g h t s j k t are stakeholder decided weights for habitat indictor s in reach j to k at month t weight values range from 0 not important to 1 important 1 m a x z s j k t w g h t s j k t i n d s j k t the value of each habitat indicator is the product of a suitability index representing habitat quality and an affected area using the habitat suitability ranges in table 1 we designed suitability indices sis unitless for aquatic floodplain and impounded wetland habitats as functions of hydrologic and ecological habitat attributes that influence priority species survival and abundance such as water depth flood recurrence and plant cover functions defining sis are specific to the reach species species life stage and habitat attribute the sis approach 1 excellent conditions when values for the habitat attribute support densities for the priority species that exceed a certain threshold in contrast sis approach 0 poor conditions when the density of a priority species is below a threshold roloff and kernohan 1999 sis are constructed using empirical data literature and expert opinion affected areas are the reach specific habitat areas in the watershed at which each suitability index applies fig 2 affected areas in these reaches are functions of flow and plant cover habitat attributes the summation of suitable areas across habitat types and species life stages follows szemis et al 2012 2014 and assumes species life stages have independent ranges of flow and vegetation attributes that define their suitable habitat suitable areas vary dynamically in time and summing suitable areas across time allows the model to consider compensatory effects across life stages e g moderate habitat for fry maturing in summer months may still lead to excellent habitat for adults to spawn in winter this summation differs from other methods that use multiplication geometric average or lowest values to aggregate multiple suitability indices into a composite index ahmadi nedushan et al 2006 these aggregation methods assume poor habitat for one species or life stage in one location leads to poor habitat in subsequent life stages the wash metric instead seeks to identify where and when to improve habitat for particular species and life stages a aquatic habitat managers can improve fish habitat in the lbr by improving flow regimes that shape physical habitat health and determine biotic composition of riverine species bunn and arthington 2002 here we use water depth and temperature as two primary abiotic factors that define aquatic habitat quality and suitability for fish jackson et al 2001 we designed water depth suitability curves and adjusted them to fish species tolerance for water temperature the bonneville cutthroat trout bct oncorhynchus clarki utah is a critical native fish species in the blacksmith fork and little bear rivers two bear river tributaries and is the target of many restoration efforts because of declining numbers in recent decades bio west 2015 brown trout salmo trytta is a popular non native game fish species that has high tolerance to low summer flows warmer temperatures and parasites causing whirling disease compared to other members of the trout family utahfishinginfo website 2016 the lower elevation bear river main stem has warmer summer water temperatures that reach 26 c the higher elevation little bear and blacksmith fork rivers have cooler water temperatures that do not exceed 22 5 c watershed sciences 2007 johnstone and rahel 2003 report that water temperature at or above 25 c could be lethal for bct while raleigh et al 1984 reported that brown trout can tolerate water temperature up to 27 2 c currently bct is only abundant in the headwaters of the blacksmith and little bear rivers derito pers comm 2016 thus we assigned bct as the indicator fish species in the headwater reaches and brown trout in the remaining reaches we developed the aquatic suitability index rsi unitless as a function of water depth d j k t the rsi curves vary between 0 at water depths in lbr reaches where empirical studies found no fish to 1 at water depths where fish or redd counts were abundant the corresponding water depth ranges for bct were obtained from a 2 year study in the nearby strawberry river by braithwaite 2011 and for brown trout from gosse et al 1977 and gosse 1981 on the logan and provo rivers in northern utah water depth ranges were also verified by the project stakeholders for brown trout we assigned a poor suitability index value of 0 at 10 cm water depth because brown trout can tolerate very shallow depths raleigh et al 1984 we used boltzmann and exponential decay functions to specify the shapes of suitability index curves for bct and brown trout fig 3 top and middle based on similar fws hsi curves for water depth hickman and raleigh 1982 raleigh et al 1984 the aquatic habitat indicator is the product of rsi for each reach j k month t and fish species y and the corresponding channel surface area eq 2 with multiple fish species y we multiply suitability indices together to emphasize the concurrent need for suitable water depths for all species at the same time and location 2 i n d a q u a t i c j k t y r s i j k t y d j k t a j k t j k t floodplain areas are adjacent to streams and are periodically inundated with water seasonally high water levels in these areas inundate riparian plant roots and keep soil moist meier and hauer 2010 the lateral connectivity between the river channel and its floodplain area is a primary factor shaping plant community composition abundance and survival merritt et al 2010 poff et al 1997 rivaes et al 2013 rood et al 2005 managers can improve floodplain connectivity by concurrently managing flows and plants cover in connected floodplains plant recruitment and seed germination coincide with overbank flood events that occur when discharge exceeds the bankfull flood level meier and hauer 2010 yarnell et al 2010 this level is defined as the visible break in slope between the un vegetated bank and the adjacent vegetated floodplain surface li et al 2015 parker et al 2007 bankfull discharge is often associated with the 1 5 year flood recurrence interval kilpatrick and barnes 1964 noaa 2015 rosgen 1994 therefore to restore lateral connectivity managers need to determine the proximity of priority floodplain plants to riverbanks and manage streamflow to exceed bankfull discharge and inundate target plants during their seed germination season we selected cottonwood trees populus fremontii as an indicator native plant species in the lbr because it predominates in basin floodplains and provides shade food and habitat for mammals birds and insects bio west 2015 cottonwoods release seeds just after peak flows in snowmelt driven rivers bhattacharjee et al 2006 mahoney and rood 1998 thus lateral connectivity between the channel and floodplains is most important between april and june for successful seed dispersal and through august for the continued soil moisture needed to establish dispersed seeds bhattacharjee et al 2008 mahoney and rood 1998 cottonwood trees grow adjacent to river channels and are likely to be inundated by flow magnitudes over bankfull flow 1 5 year flood recurrence value therefore we designed the floodplain suitability index fsi unitless as a function of streamflow q j k t the index curves transition from 0 poor lateral connectivity when flow is at or below the 1 year recurrence value to 1 excellent connectivity when the instream flow equals or exceeds the 2 year recurrence flow fig 3 bottom the 1 and 2 year recurrence flow thresholds at different reaches in the basin are determined from historical flow records using the log pearson type iii distribution with mean and standard deviation of the log transformed annual flow series we measured initial existing cottonwood tree cover alongside every reach from naip imagery the floodplain connectivity indicator is calculated by multiplying fsi for reach month and riparian plant species by the area of plant cover c and then summing the values for each plant species n eq 3 the summation across plant species in eq 3 allows one or multiple plant species to coexist at different lateral distances from the riverbank and these different lateral distances require different flood magnitudes to establish connectivity here we only use cottonwood trees which live adjacent to river banks and require flood recurrence of 2 year for lateral connectivity richter and richter 2000 other riparian trees such as pacific willow salix lasiandra live further upslope in the floodplain and require a higher flood frequency interval for lateral connectivity dettenmaier and howe 2015 rood et al 2003 3 i n d f l o o d p l a i n s j k t n f c i j k t n q j k t c j k t n j k t c impounded wetlands wetlands are recognized as one of the most productive ecosystems for a variety of wildlife species particularly water birds nikouei et al 2012 impounded wetlands have dikes gates weirs canals or other hydraulic structures that allow managers to control flows into and out of wetlands the bird refuge comprises 25 impounded wetland units that draw water from the bear river downard et al 2014 maintaining wetland ecological services at the bird refuge requires managing habitat y for the different water depths and plant cover that different bird species need to feed nest rest and breed downard and endter wada 2013 faulkner et al 2010 rogers and ralph 2011 prior work by alminagorta et al 2016 developed a composite useable area for wetland wu metric for the bird refuge measured in km2 the study embedded the metric in a systems model and maximized the wetland surface area with suitable habitat for black necked stilt american avocet and tundra swan table 1 these three priority bird species were selected because they use a range of shallow medium and deep water habitat that encompass depths used by 20 other priority bird species at the refuge we related the wu and river flow outputs of alminagorta et al 2016 work at the bird refuge by running alminagorta et al s model to obtain the monthly wu values from the annual water availability scenarios between 1992 and 2011 next we divided monthly wu areas by the total refuge area to get a monthly wetland suitability index value then we developed monthly relationships between the suitaibilty index values and monthly river flows measured just upstream of the bird refuge at the corinne ut usgs station one example shown in fig a1 the impounded wetland indicator is calculated by multiplying a wsi index as a function of streamflow by the total wetland surface area aw mm2 in eq 4 the wsi defines an aggregate suitability index for multiple wetland bird species 4 i n d w e t l a n d s j k t w s i j k t q j k t a w j k t j k t 3 3 1 constraints reservoir releases diversions planting and other decisions are bound by physical infrastructure and management constraints appendix b eqs 5 18 physical constraints include low order finite difference approximation to conservation of water mass balance at each reservoir node and demand site they also include equations to constrain plant cover growth over time and define channel topography infrastructure constraints place minimum and maximum limits on reservoir and diversion canals capacity management constraints include urban and agricultural demand requirements and available budget to plant floodplain vegetation nonlinear objective and constraint functions in the wash formulation are all continuous and smooth to avoid numerical difficulties in the optimization 3 4 model input data the wash model requires hydrologic ecological topological and management data appendix a table a1 we collected the required data from sources including the utah division of water resources dwre water supply demand simulation model for the lower bear river adams et al 1992 between august 2012 and november 2016 we also established two monitoring sites on the bear river mainstem and one site on the cub river to collect and ground truth hydrologic and ecological data we assumed a budget of 650 000 to plant floodplain areas with cottonwoods based on a cache county water masterplan estimated budget for future ecosystem projects jub engineers 2013 this budgeting assumes there is no cost to change reservoir releases or diversion flows which infrastructure constraints require to stay within existing infrastructure capacities processed hydrologic and ecological data are available at the bear river fellows website http bearriverfellows usu edu wash model input data and code are available at the wash github repository alafifi 2017 3 5 model scenarios we implemented the model on a monthly time step for scenarios that test the effects of simulation vs optimization inflow hydrology and length of modeled time period table 2 3 6 model implementation we segmented the bear river and its main tributaries into a network of 43 nodes and 56 links with 3 reservoirs 12 municipal and agricultural demand sites and 26 environmental sites where species of concern live fig a2 we ran the model with the same weight value of 1 for all indicators to equally favor all locations species and months we coded the wash model equations 1 17 using the general algebraic modeling system software gams hozlar 1990 and solved the model using the non linear global solver branch and reduce optimization navigator baron sahinidis 1996 the gams code uses gdx gams data exchange format to read all input data from an ms excel spreadsheet and pass data to the model the 1 year implementation of the model for the lower bear river system has approximately 27 000 variables and 5 300 equations and takes 2 h and 15 min to find a global optimal solution on a dell xps windows10 64 bit computer 3 7 model outputs and visualization wash results include recommended flows reservoir releases storage volumes planted area vegetation cover and temporal and spatial variations of suitable aquatic floodplain and impounded wetland habitat area we used excel r and arcgis online to post process and display model results in an open access interactive web map http washmap usu edu with the web map users can compare modeled and simulated results add base maps and data layers and customize the tool the wash map displays results in us customary units to communicate better with local stakeholders all wash model input data code and post processing files are available at the wash github repository alafifi 2017 4 results the model run that simulated 2003 flows shows that modeled suitable aquatic floodplain and wetland habitat area in the watershed is far from the maximum habitat area fig 4 hashed bars compared to dashed lines in contrast wash recommended water allocations and planting of native vegetation have potential to increase the overall suitable habitat area by 25 000 acres fig 4 solid compared to hashed bars this overall increase is achieved with up to 3 10 and 7 fold increases over 2003 modeled historical conditions of aquatic suitable area in all months floodplain suitable areas in april to august and wetland suitable areas at the bird refuge from june to august fig 6 the model identifies increases in habitat in every month for all fish species life stages with the largest increases in may june and november the largest floodplain habitat increases for plants occur in july and august on bear river reaches above cutler reservoir these suitable areas approach 53 3 and 40 of the total aquatic floodplain and wetland habitat areas in the basin the wash model improves suitable habitat area by recommending monthly reservoir release storage and floodplain planting operations wash recommends increasing dec to mar releases at hyrum reservoir which creates additional empty storage in winter and spring that permits reducing late spring apr to may spills from hyrum and porcupine reservoirs fig 5 this recommended release pattern allows a small increase in late fall and winter base flows that support brown trout spawning in late fall and maintains the eggs in gravel redds until they hatch in spring releasing more water in early spring supports floodplain connectivity and helps to mitigate flooding impacts the gradual release of water from reservoirs also protects trout eggs from winter and spring flood events that could scour or kill incubated eggs and newly emerged fry george et al 2015 and could be harmful to cottonwood seedlings the model also recommends planting cottonwoods starting in march that increase vegetation cover and floodplain suitable area increased suitable floodplain area occurs even with declining or stagnant flows in several reaches fig 6 the model increases habitat area while continuing to meet urban and agricultural water uses at all demand sites during all months although wetland suitable area at the bird refuge increases to only 40 of the total suitable area improvements occur during critical summer months when bear river flows at corrine typically did not satisfy the bird refuge s junior water rights fig 7 overall the model recommended habitat area approaches 18 of the total available habitat area in the watershed if all suitability index values are at 1 additional river flow habitat suitability reservoir release and demand site results are available at http washmap usu edu running the model for 2005 wet year increased the suitable habitat area by 18 000acres fig 8 red circle while using 2004 flows dry year decreased the suitable habitat area by 15 000 acres fig 8 orange triangle reducing urban and agricultural demand in 10 increments increased habitat suitability area by approximately 4 000 acres per 10 reduction with most of the initial increases in habitat area occurring at the bird refuge and in aquatic habitat on the little bear river the model becomes infeasible at 110 of existing demand because agricultural and urban demands exceed available water running the model for 5 years from january 2003 to september 2007 shows that the model can sustain habitat increases across annual variations in flows fig 9 compares the monthly changes in inflow recommended reservoir operations and aquatic habitat suitability just downstream of hyrum and porcupine reservoirs on the little bear river hydrologic and ecological patterns are very similar and conform to storage and release patterns seen in the single year run fig 5 to identify when and where additional flow and floodplain vegetation will have the greatest system wide ecological benefits we examined the shadow values lagrange multipliers associated with the water mass balance constraints at nodes with headwater flow and vegetation growth constraints along each reach of ecological interest appendix b eqs 7 and 9 shadow values for flow report the increase in the wash objective function value habitat area measured in acres per one additional flow unit cfs table 3 the largest shadow values occur on the east fork of the little bear river for most months of the year shadow values greater than 2 5 acres cfs are also seen on the bear river in august on blacksmith fork from april to october and on the south fork of the little bear in two months the east and south forks in addition to blacksmith fork had the largest shadow values in the system because both active reservoirs hyrum and porcupine are located on these reaches there are more opportunities to temporally redistribute available water and instream water can also benefit lower reaches shadow values associated with the vegetation growth constraint are largest for all rivers and reaches in winter and spring fig 10 reaches on the malad river and bear river below cutler reservoir will have the largest increase in suitable habitat area per acre of native vegetated area added we also examined the shadow values for the budget constraint appendix b eq 18 and found a 30 acre increase in suitable habitat per additional 10 000 available for planting floodplains 5 discussion formulating the wash model objective function as a habitat area to maximize allowed us to examine ways to synergistically manage water and plants in the lower bear river to increase aquatic floodplain and wetland habitat area for priority species while satisfying water demands of existing agricultural and urban users additional suitable area can be achieved by 1 releasing water from reservoirs at times priority species needs water to complete key lifecycle functions and 2 planting more riparian trees in spring spring planting increases vegetation cover and takes advantage of natural vegetation growth over time to increase habitat area in the lower bear river the model recommends releasing more water from porcupine and hyrum reservoirs in december to march to capture and store april and may spring runoff and reduce spills this recommended seasonal drawdown supports floodplain connectivity and reduces winter and spring flood events that could scour or kill incubated eggs newly emerged fry george et al 2015 or cottonwood seedlings seasonal drawdown could simultaneously help reduce late spring flooding downstream of the reservoirs improvements in floodplain habitat area are small relative to aquatic habitat because several summertime diversions lower instream flows and decrease lateral connectivity to adjacent floodplains at the same time many reaches border private agricultural fields and grazing lands and have narrow riparian corridors planting native floodplain vegetation according to model recommendations will require managers to set up agreements and easements with riparian landowners wash recommended flow regimes will also increase overbank flood recurrence to improve floodplain connectivity therefore managers need to consider flooding effects on neighboring farmers ranchers and hunters recent conservation easements made by pacificorp which owns large floodplain areas within the study area illustrate one way to co manage for multiple objectives these easements have allowed riparian plant restoration projects to also serve as flood buffer zones wash results can help identify promising locations to site additional conservation easements to improve habitat quality for multiple aquatic and floodplain priority species changing reservoir operations diversions and other management actions higher up in the basin can also increase impounded wetland habitat during summer months these results support bird refuge managers recent efforts to establish conservation easements with upstream landowners acquiring upstream storage rights would allow refuge managers to store winter flows and release in summer to beneficially improve habitat for avocets and stilts storage rights could also help managers plan for droughts formulating the wash objective function as a habitat area to maximize also shows where and when to direct scarce water money and planting efforts to most improve habitat water is scarce during summer months but wash results suggest managers can create 2 5 12 acres of additional suitable habitat per additional cfs of flow acquired during summer fall or winter on the east fork of the little bear river or during late summer and fall months on the blacksmith fork or south fork of the little bear river the little bear had the greatest return per additional cfs because it has aquatic and floodplain habitat that need just a little more water to improve to good habitat condition and those instream flows will also benefit downstream reaches similarly wash model recommendations to plant native floodplain vegetation in spring replicate findings by alminagorta et al 2016 to remove invasive wetland vegetation at the beginning of the growing season to see the largest increases in suitable wetland habitat area wash results were corroborated using a participatory modeling approach bockstaller and girardin 2003 langsdale et al 2013 stakeholders initially helped to define the aquatic floodplain and wetland metrics later we presented results to project stakeholders using wash interactive web map http washmap usu edu for example during an august 2016 model workshop we presented key reservoir release and habitat area results earlier versions of figs 4 5 and 7 9 while stakeholders simultaneously explored results in real time on their phones tablets and laptops their explorations identified a problematic aspect of reservoir releases for bct and motivated us to update aquatic suitability indices to reflect temperature water depth relationships base water depth ranges on recent fish ecology studies and differentiate bct and brown trout distributions because wash multiplies habitat suitability indices by affected areas the model structure is flexible and can be extended to explicitly include additional water quality parameters such as dissolved oxygen or turbidity similarly modelers can add other species habitat attributes or habitat types such as natural oxbow seasonal or other wetlands in the watershed that were not included in the lower bear river study the wash model has several limitations and we discuss these limitations in order from what we assess as least to most important the wash model allocates water and vegetation using perfect foresight of future water flows and vegetation growth rates managers never have perfect information about these factors however bear river flows are snowmelt driven and managers use snowpack measurements throughout the winter and spring to forecast spring summer and fall river flows forecast reliability decreases in successive years thus multi year scenario results are more appropriately interpreted as the upper bound on potential habitat gains when future flows are known perfectly the wash model quantifies some habitat quality conditions that are necessary for the survival and productivity of priority species however it does not predict or model species distribution across the watershed while we have validated habitat quality conditions with available and collected cutthroat trout and cottonwood tree sightings we see further value to collect additional field data that describe fish growth in migration out migration predators natural die off and other factors and couple models of these species distribution processes to the wash model the wash model assumes that measured and modeled water depths and channel widths are uniform along reaches that are a few miles long also that suitable area varies in time and is separable by habitat type species species life stage and location these assumptions were made using the best available measured data and do not capture species resilience to or recovery from very poor conditions a finer spatial resolution or coupling our model with a hydrologic model could improve our findings at the same time the model results increase or sustain suitable habitat areas for each habitat type species and species life stage compared to modeled historical conditions we also assume that inundating the floodplains during seed germination and dispersal will help riparian plants to reestablish this assumption neglects seedling survival which requires other biotic and abiotic conditions such as groundwater level soil salinity and plant competition for water bhattacharjee et al 2008 we recognize that each of the aquatic floodplain and impounded wetland suitability indices si carry along statistical errors that result from measurement error spatial and temporal variability and function form van der lee et al 2006 more spatially resolved ecological data can help determine where finer and coarser data is appropriate for modeling in ongoing research we are evaluating and quantifying uncertainties in si curves and other model parameters and their implications for water management alafifi 2018 implementing wash recommendations to improve habitat will also require recognizing and protecting environmental flows in the water permitting process utah water law has limited methods to appropriate water for instream flow and must allow more entities to donate lease or purchase existing water rights lane and rosenberg 2020 szeptycki et al 2015 expanding the allowed entities will reduce risks that a downstream water right holder will appropriate an instream flow for their beneficial use currently managers in the basin such as the cache water district are investigating ways they and others can hold instream flow rights to implement model recommendations 6 conclusions improving habitat in a watershed requires determining when where and how to allocate water and vegetation planting efforts within a basin prior systems models to improve habitat have principally focused on the water allocation component these models either maximized human benefits of water or included habitat quality as a flow constraint other models minimized deviations from natural or species required flow regimes here we developed a suitable habitat area metric measured in acres that is tied to flow and native vegetation conditions in the watershed we embedded the metric in a systems optimization model named wash that identified the timing and location of reservoir releases river flows and planting efforts to maximize habitat area subject to physical infrastructure and management constraints we applied wash to the lower bear river ut using stakeholder verified species and site specific habitat suitability curves for cutthroat trout brown trout cottonwood black necked stilt american avocet and tundra swan wash identified opportunities to increase aquatic floodplain and impounded wetland habitat area by 25 000 acres over existing conditions this increase can be realized by releasing more water from porcupine and hyrum reservoirs in winter months using vacated storage to capture spring runoff and reducing late spring spills also by planting native floodplain vegetation in spring months in reaches with low vegetation cover further procuring additional flow in the east fork of the little bear river during summer fall and winter months would most increase habitat area per cfs of new flow the wash web map application provided managers with direct access to model results helped us validate results and motivated further model development to make scenarios and results more relevant to managers overall developing and embedding a habitat area metric in a systems model as an ecological objective to maximize has allowed us to compare habitats across watershed sites and identify sites and times where managers can apply scarce water money and planting efforts to most improve habitat quality and area declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by national science foundation nsf grant 1149297 joan degiorgio bryan dixon bob fotheringham james derito paul thompson sarah null and karin kettenring contributed to the model development and provided feedback on results 16 undergraduate bear river fellows helped collect synthesize and analyze flow stage and cross sectional data plus tested model scenarios nour attallah reviewed the github reporistory and reproduced the model results appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104643 appendix a additional figures and tables fig a2 wash model schematic for the lower bear river utah basin fig a2 fig a1 example wash wetland suitability index for february fig a1 table a1data required for wash model components model component data item source s component type aquatic habitat reach lengths nhdplus v2 2016 usgs 2012 field measurements link water depth ecological suitability curves fws stakeholders and literature link floodplain habitat plant cover and distance from banks usda 2014 naip imagery field measurements link floodplain area naip imagery field measurements link flow ecological suitability curves fws stakeholders and literature link wetland habitat wetland unit water level storage curves lidar field measurements link invasive plan cover landsat satellite imagery link evaporation rates western regional climate center link flow ecological suitability curves fws stakeholders and literature link physical constraints reservoirs storage elevation area evaporation and capacity adams et al 1992 u s bor node diversions capacity adams et al 1992 link natural constraints headwater and local inflows usgs nhdplus v2 uwrl 2009 node water level and channel cross section field measurements link evaporative losses on reaches nhdplus v2 link natural growth of riparian plants stakeholders link management constraints urban and agricultural demand genres node consumptive use of flow genres node instream flow requirements stakeholders link budget and unit costs stakeholders link model formulation weights stakeholders link appendix b wash model constraints this appendix describes the physical eqs 5 14 infrastructure eq 15 and management eq 16 18 constraints that limit reservoir release diversion planting and other decisions in the wash model a reservoir storage balance a low order finite difference mass balance requires that reservoir storage for each reservoir v at the beginning of each time step t 1 equal storage at the beginning of the prior time step t plus net flows of links leading to the reservoir minus reservoir releases and minus evaporation losses eq 5 reservoir releases are flows along all links that leave reservoir v in month t eq 6 evaporation losses are estimated by multiplying a monthly evaporative rate e v a p v t m month by the reservoir surface area r a v t is a function of reservoir storage the term l s s j v t is the net loss rate on links connecting to reservoir v and is expressed as a fraction of link flow 5 s t o r v t 1 s t o r v t j q j v t 1 l s s j v t r r v t e v a p v t r a v t s t o r v t v t 6 r r v t j q v j t v t b mass balance at junctions flows entering each non reservoir node j must equal or exceed evaporative losses plus flows leaving the node eq 7 localinflow j t mm3 month are reach gains groundwater inflows or other flows that accumulate at node j in time t at the most upstream nodes in a network localinflow is the head flow and represents the boundary condition and cumulative contribution of climate runoff and other hydrologic processes linkevap m month describes the evaporative loss rate on links link evaporation m3 month is the product of the evaporative loss rate and channel surface area 7 l o c a l i n f l o w j t k q k j t 1 l s s k j t k a k j t l i n k e v a p k j t k q j k t j t c mass balance at each demand site total flow to each demand site dem in time t must equal or exceed the return flow back to the river eq 8 total flow is reduced by the depleted flow amounts that include diversion losses l s s k d e m t and urban or agricultural consumptive use fraction c o n s k d e m t both of inflow received 8 k q k d e m t 1 l s s k d e m t c o n s d e m t k q d e m k t d e m t d plant cover plant cover c j k t n mm2 for each species n in each link j to k at time step t equals cover at prior time step t 1 plus planted areas r v j k n mm2 and natural growth or death g j k n mm2 eq 9 g j k n describes the natural increase or decrease in riparian plant cover without any interference from managers both plant cover c j k t n and planting area cannot exceed the total floodplain area adjacent to each reach c m a x j k eqs 10 and 11 additionally planting r v j k n can only occur during the growing season eq 11 9 c j k t n c j k t 1 n r v j k t n g j k n j k t n 10 n c j k t n c m a x j k j k t 11 n r v j k t n c m a x j k t g r o w i n g s e a s o n 0 o t h e r w i s e j k t j k t e channel topology relationships river flow channel stage width and surface area are related on each link j to k in each time step t eqs 12 14 these relationships are established based on measured data we use leopold and maddock 1953 linear relationships for stage flow s f and width flow w f relationships l n g j k is the length of each river segment m 12 stage flow relationships d j k t s f 1 j k q j k t s f 2 j k j k t 13 width flow relationships w d j k t w f 1 j k q j k t w f 2 j k j k t 14 channel surface area a j k t w d j k l n g j k j k t f reservoir storage limits storage in each reservoir v cannot go below a minimum storage volume minstor v mm3 which is the reservoir dead pool similarly storage can exceed the flood control pool level or storage capacity maxstor v mm3 at any time t eq 15 15 m i n s t o r v s t o r v t m a x s t o r v v t g meet demand requirements diversions to each demand site dem should meet requirements dreq dem t mm3 month in each time t eq 16 16 k q k d e m t 1 l s s k d e m t d r e q d e m t d e m t h flow limits minimum and maximum values q m i n j k t and q m a x j k t bound flow in each link j to k in time t eq 17 minimum levels may be minimum instream flow or diversion requirements maximum bounds can be channel diversion or other capacities 17 q m i n j k t q j k t q m a x j k t j k t i management budget the total cost to plant floodplain species ct n m2 make reservoir releases or adjust diversion gates st n m3 should not exceed the financial budget b eq 18 18 j k n t c t n r v j k t n j j t s t j k t q j k t b 
26057,systems models to improve ecosystems often identify flows to meet minimum instream flow requirements or minimize deviations from a predefined flow regime here we present a new systems optimization model that determines when where and how much to allocate scarce water financial resources and revegetation efforts to improve aquatic floodplain and wetland habitat areas and quality this optimization is subject to constraints on water mass balance vegetation growth infrastructure capacities and meeting existing agricultural and urban water demands we followed a participatory approach to apply and validate our model in the lower bear river watershed ut results show that increasing winter reservoir releases minimizing spring spills and planting native floodplain vegetation early in the growing season can increase suitable habitat area beyond managing water alone additional flow on the little bear river between august and december will most increase habitat area and quality compared to other locations keywords systems models river habitat quality reservoir operations floodplain revegetation bear river utah watershed management web mapping application software and data availability name of software watershed area of suitable habitat wash optimization model developers ayman h alafifi and david e rosenberg contact aafifi aggiemail usu edu year first available 2017 hardware required a personal computer software required general algebraic modeling system software gams with non linear global solver such as branch and reduce optimization navigator baron ms excel 2016 r 3 3 0 and a web browser software availability all source code input data post processing file and documentation are available on alafifi 2017 the application of wash to the lower bear river utah for one year 2003 is displayed on an open access web map at https www washmap usu edu the source code is available on github at https github com ayman510 wash cost the source code is released under the bsd 3 clause which allows for reuse of the code 1 introduction rivers and their riparian and wetland areas are managed to supply domestic and agricultural water users generate hydropower reduce flood damages and support habitat for flora and fauna bernhardt et al 2005 although river managers often prioritize human beneficial uses flow management can also improve habitat jager and smith 2008 tharme 2003 improving river habitat requires defining ecological objectives and determining the timing magnitude and locations of reservoir releases diversions and restoration efforts to advance the objectives determining timings magnitudes and locations of management efforts often requires navigating a complex set of ecological hydrological modeling and stakeholder considerations first managers must identify and locate the aquatic floodplain and wetland habitat areas in the basin that need improvement second they should select indicator species from among the numerous species available in each habitat whose habitat needs should be satisfied or improved habitat needs can encompass flow vegetation and other conditions third managers may use models to mathematically quantify each indicator species response to habitat conditions and finally managers must collaborate with watershed stakeholders to identify when where and how to allocate water financial resources revegetation efforts to improve habitat while meeting other water uses in the basin barbour et al 2016 some quantification and modeling approaches such as the natural flow paradigm estimate species hydrologic requirements to mimic important timing duration magnitude and frequency features of the natural flow regime poff et al 1997 these approaches assume that historical natural flows are known and adequate to support ecosystem functions baron et al 2002 other approaches such as habitat suitability indices hsi u s fish and wildlife service 1981 and derivatives hickey and fields 2013 use empirical relationships to describe the suitability of habitat to support the survival and productivity of a single species suitability is a function of single or multiple habitat attributes such as instream water depth water temperature substrate or flow duration hsi values range from 0 poor to 1 excellent hemker et al 2008 hooper 2010 pinto et al 2009 the weighted useable area wua method multiplies the habitat suitability index by the reach surface area and divides by reach length stalnaker 1995 wua can be used to describe habitat quality for a particular species at a specific site and time under prior or specified flow scenario garcia rodriguez et al 2008 moir et al 2005 souchon and capra 2004 these approaches cannot determine whether a proposed flow regime is feasible nor do they recommend locations timings or magnitudes of water allocations or other restoration efforts across a watershed to improve multiple habitat types and species water resources systems models can include multiple ecosystem assets as part of a connected network of reservoir river tributary diversion demand and return flow components and can determine the feasibility of proposed flow regimes many models include habitat considerations as constraints such as to meet a minimum required instream flow see for example cioffi and gallerano 2012 harman and stewardson 2005 porse et al 2015 ryu et al 2003 in other cases a suitability index is maximized or minimized as a single objective or tradeoff with water delivery hydropower generation or other objectives null et al 2014 simonović and nirupama 2005 yang 2011 alternatively models try to minimize deviations from a pre defined target value for example higgins et al 2011 developed a heuristic nonlinear integer optimization model to minimize the difference between managed and natural flow regimes in the murray river australia steinschneider et al 2014 used linear programming to minimize the deviation between model recommended reservoir releases and estimated natural flows in the connecticut river basin szemis et al 2012 2014 developed a heuristic ant colony nonlinear model for the murray river to minimize the inverse of an ecological index plus constraint violations minimizing deviations from an ecosystem target poses challenges because managers must define the target such as natural flow regime or species required flow barbour et al 2016 further indices may not have physical meaning and are difficult to measure validate and communicate also changes in the deviation objective depend on how close the current system state is to the ecological target managers who minimize indices or deviations from targets find it difficult to identify opportunities to improve habitat and compare potential improvements across watershed sites and time additionally all these models focus solely on managing water to improve flow based habitat objectives this paper develops the watershed area of suitable habitat wash systems model which formulates and embeds a suitable habitat area metric as an ecological objective to maximize suitable habitat area represents the combination of habitat quality and area is based on flow and vegetation conditions that support and enhance the life needs of priority species and is measured as the sum of monthly suitable aquatic floodplain and wetland habitat areas across the watershed for each priority species this summation means suitable areas are specific to the species and species life stage and can vary though time wash synergistically recommends water allocation and revegetation efforts to improve suitable habitat area for priority species this modeling approach allows managers to i compare ecological measures across sites ii identify where and when to apply scarce water money and planting efforts to most improve habitat quality and area and iii involve stakeholders to help define indicator species view and validate results section 2 introduces the study area for the lower bear river utah section 3 describes the model formulation and system components the remaining sections present results implications to manage water and vegetation and conclusions 2 study area the lower bear river lbr study is part of the longer 491 mile bear river that starts in utah flows north through wyoming and idaho then returns south to utah the study area includes the bear river from the utah idaho state line to the river s terminus at the great salt lake and tributaries fig 1 the utah division of water resources 2004 estimates that approximately 60 of lbr flow comes from snowmelt runoff in april may and june the river and its tributaries irrigate over 300 000 acres of agricultural land and supply water to numerous cities and communities as well as run of river hydroelectric plants udwr 2004 udwre 2000 the river is central to a development debate for several growing areas within and outside the basin such as cache and box elder counties and the wasatch front metropolitan region to the south udwr 2004 udwre 2000 the lbr is also vital to maintaining critical wildlife habitat for many native and threatened fish riparian plants and migratory bird species bio west 2015 intensive urbanization water development fish barriers and grazing for over a century have disturbed flow regimes for native and game fish species reduced floodplain connectivity and altered native plant community composition bear river cap 2008 bio west 2015 at the river s terminus at the great salt lake the u s fish and wildlife service fws manages the 300 km2 bear river migratory bird refuge hereafter the bird refuge here impounded wetlands provide feeding resting and breeding grounds for over 250 globally significant populations of migratory birds alminagorta et al 2016 according to western u s prior appropriation doctrine and utah state water law the bird refuge holds a more recent water right that is junior to more senior upstream agricultural users downard et al 2014 thus senior irrigators take their entire water rights before the refuge receives any water most other land in the lbr is privately owned and few formal or legal mechanisms exist to provide water to improve fish riparian plant and migratory bird habitats lane and rosenberg 2020 the nature conservancy trout unlimited landowners and local state and federal agencies have identified low flows as a major threat to fish populations riparian plants and migratory birds in the watershed bear river cap 2008 the current regime is highly disturbed this condition makes it extremely difficult and unlikely to return to the natural without development regime further the natural regime is not known thus systems modeling can help managers identify synergistic water volume timing location and planting area efforts to improve ecosystem health for priority aquatic floodplain and wetland species over current conditions 3 model development improving river habitat quality and area requires a collective effort among researchers managers and stakeholders to identify habitat types priority sites indicator species habitat characteristics suitability of habitat for species and the network of water system components here we demonstrate a participatory approach to develop a systems model that can recommend scarce water financial resources and revegetation efforts to improve habitat area and quality we began by engaging with the managers organizations and stakeholders working to implement the bear river conservation action plan bear river cap 2008 working with these individuals and organizations we selected key indicator species collected model input data and formulated model scenarios below we describe the general model formulation of decision variables objective function and constraints in the formulation capitalized terms represent variables lower case terms indicate parameters and model inputs and lettered subscripts denote indices for space time species and habitat types bottom of fig 2 3 1 select indicator species the presence and abundance of indicator species is a strong signal of ecosystem health and response to alterations in flow regimes carignan and villard 2002 we identified key native and game fish riparian plants and wetland migratory bird species in the lbr watershed based on their abundance in the watershed and sensitivity to changes in flow regimes for each species we defined suitable ranges of habitat attributes such as water depth and flood recurrence in each month and for each life stage table 1 we derived habitat attribute ranges from literature empirical studies and other models and verified them with project stakeholders fish spawning seed recruitment and migratory bird feeding nesting and breeding occur during particular months we selected a monthly time step t for wash because watershed managers manage flows and plan revegetation efforts at monthly intervals 3 2 decision variables decision variables include water management and planting area actions to improve habitat quality managers can adjust reservoir releases rr v t million cubic meters per month mm3 at reservoir v in month t they also control diversion volumes qj dem t mm3 month from the river at node j to demand sites dem in month t to satisfy urban and agricultural demand managers can also plant rv j k t n mm2 the floodplain adjacent to the river reach from node j to node k during month t by seeding or planting species n these variables control a group of state variables that include reservoir storage volume stor v t mm3 reservoir surface area ra v t mm2 river flow q j k t mm3 month from node j to node k in month t river water depth d j k t m month channel surface area aj k t mm2 channel width wd j k t m and floodplain plant cover c j k t n mm2 3 3 objective function the objective function maximizes the weighted sum of the suitable areas of aquatic ind aquatic j k t floodplain ind floodplain j k t and wetland ind wetland j k t habitats mm2 across reaches j to k and time month t where w g h t s j k t are stakeholder decided weights for habitat indictor s in reach j to k at month t weight values range from 0 not important to 1 important 1 m a x z s j k t w g h t s j k t i n d s j k t the value of each habitat indicator is the product of a suitability index representing habitat quality and an affected area using the habitat suitability ranges in table 1 we designed suitability indices sis unitless for aquatic floodplain and impounded wetland habitats as functions of hydrologic and ecological habitat attributes that influence priority species survival and abundance such as water depth flood recurrence and plant cover functions defining sis are specific to the reach species species life stage and habitat attribute the sis approach 1 excellent conditions when values for the habitat attribute support densities for the priority species that exceed a certain threshold in contrast sis approach 0 poor conditions when the density of a priority species is below a threshold roloff and kernohan 1999 sis are constructed using empirical data literature and expert opinion affected areas are the reach specific habitat areas in the watershed at which each suitability index applies fig 2 affected areas in these reaches are functions of flow and plant cover habitat attributes the summation of suitable areas across habitat types and species life stages follows szemis et al 2012 2014 and assumes species life stages have independent ranges of flow and vegetation attributes that define their suitable habitat suitable areas vary dynamically in time and summing suitable areas across time allows the model to consider compensatory effects across life stages e g moderate habitat for fry maturing in summer months may still lead to excellent habitat for adults to spawn in winter this summation differs from other methods that use multiplication geometric average or lowest values to aggregate multiple suitability indices into a composite index ahmadi nedushan et al 2006 these aggregation methods assume poor habitat for one species or life stage in one location leads to poor habitat in subsequent life stages the wash metric instead seeks to identify where and when to improve habitat for particular species and life stages a aquatic habitat managers can improve fish habitat in the lbr by improving flow regimes that shape physical habitat health and determine biotic composition of riverine species bunn and arthington 2002 here we use water depth and temperature as two primary abiotic factors that define aquatic habitat quality and suitability for fish jackson et al 2001 we designed water depth suitability curves and adjusted them to fish species tolerance for water temperature the bonneville cutthroat trout bct oncorhynchus clarki utah is a critical native fish species in the blacksmith fork and little bear rivers two bear river tributaries and is the target of many restoration efforts because of declining numbers in recent decades bio west 2015 brown trout salmo trytta is a popular non native game fish species that has high tolerance to low summer flows warmer temperatures and parasites causing whirling disease compared to other members of the trout family utahfishinginfo website 2016 the lower elevation bear river main stem has warmer summer water temperatures that reach 26 c the higher elevation little bear and blacksmith fork rivers have cooler water temperatures that do not exceed 22 5 c watershed sciences 2007 johnstone and rahel 2003 report that water temperature at or above 25 c could be lethal for bct while raleigh et al 1984 reported that brown trout can tolerate water temperature up to 27 2 c currently bct is only abundant in the headwaters of the blacksmith and little bear rivers derito pers comm 2016 thus we assigned bct as the indicator fish species in the headwater reaches and brown trout in the remaining reaches we developed the aquatic suitability index rsi unitless as a function of water depth d j k t the rsi curves vary between 0 at water depths in lbr reaches where empirical studies found no fish to 1 at water depths where fish or redd counts were abundant the corresponding water depth ranges for bct were obtained from a 2 year study in the nearby strawberry river by braithwaite 2011 and for brown trout from gosse et al 1977 and gosse 1981 on the logan and provo rivers in northern utah water depth ranges were also verified by the project stakeholders for brown trout we assigned a poor suitability index value of 0 at 10 cm water depth because brown trout can tolerate very shallow depths raleigh et al 1984 we used boltzmann and exponential decay functions to specify the shapes of suitability index curves for bct and brown trout fig 3 top and middle based on similar fws hsi curves for water depth hickman and raleigh 1982 raleigh et al 1984 the aquatic habitat indicator is the product of rsi for each reach j k month t and fish species y and the corresponding channel surface area eq 2 with multiple fish species y we multiply suitability indices together to emphasize the concurrent need for suitable water depths for all species at the same time and location 2 i n d a q u a t i c j k t y r s i j k t y d j k t a j k t j k t floodplain areas are adjacent to streams and are periodically inundated with water seasonally high water levels in these areas inundate riparian plant roots and keep soil moist meier and hauer 2010 the lateral connectivity between the river channel and its floodplain area is a primary factor shaping plant community composition abundance and survival merritt et al 2010 poff et al 1997 rivaes et al 2013 rood et al 2005 managers can improve floodplain connectivity by concurrently managing flows and plants cover in connected floodplains plant recruitment and seed germination coincide with overbank flood events that occur when discharge exceeds the bankfull flood level meier and hauer 2010 yarnell et al 2010 this level is defined as the visible break in slope between the un vegetated bank and the adjacent vegetated floodplain surface li et al 2015 parker et al 2007 bankfull discharge is often associated with the 1 5 year flood recurrence interval kilpatrick and barnes 1964 noaa 2015 rosgen 1994 therefore to restore lateral connectivity managers need to determine the proximity of priority floodplain plants to riverbanks and manage streamflow to exceed bankfull discharge and inundate target plants during their seed germination season we selected cottonwood trees populus fremontii as an indicator native plant species in the lbr because it predominates in basin floodplains and provides shade food and habitat for mammals birds and insects bio west 2015 cottonwoods release seeds just after peak flows in snowmelt driven rivers bhattacharjee et al 2006 mahoney and rood 1998 thus lateral connectivity between the channel and floodplains is most important between april and june for successful seed dispersal and through august for the continued soil moisture needed to establish dispersed seeds bhattacharjee et al 2008 mahoney and rood 1998 cottonwood trees grow adjacent to river channels and are likely to be inundated by flow magnitudes over bankfull flow 1 5 year flood recurrence value therefore we designed the floodplain suitability index fsi unitless as a function of streamflow q j k t the index curves transition from 0 poor lateral connectivity when flow is at or below the 1 year recurrence value to 1 excellent connectivity when the instream flow equals or exceeds the 2 year recurrence flow fig 3 bottom the 1 and 2 year recurrence flow thresholds at different reaches in the basin are determined from historical flow records using the log pearson type iii distribution with mean and standard deviation of the log transformed annual flow series we measured initial existing cottonwood tree cover alongside every reach from naip imagery the floodplain connectivity indicator is calculated by multiplying fsi for reach month and riparian plant species by the area of plant cover c and then summing the values for each plant species n eq 3 the summation across plant species in eq 3 allows one or multiple plant species to coexist at different lateral distances from the riverbank and these different lateral distances require different flood magnitudes to establish connectivity here we only use cottonwood trees which live adjacent to river banks and require flood recurrence of 2 year for lateral connectivity richter and richter 2000 other riparian trees such as pacific willow salix lasiandra live further upslope in the floodplain and require a higher flood frequency interval for lateral connectivity dettenmaier and howe 2015 rood et al 2003 3 i n d f l o o d p l a i n s j k t n f c i j k t n q j k t c j k t n j k t c impounded wetlands wetlands are recognized as one of the most productive ecosystems for a variety of wildlife species particularly water birds nikouei et al 2012 impounded wetlands have dikes gates weirs canals or other hydraulic structures that allow managers to control flows into and out of wetlands the bird refuge comprises 25 impounded wetland units that draw water from the bear river downard et al 2014 maintaining wetland ecological services at the bird refuge requires managing habitat y for the different water depths and plant cover that different bird species need to feed nest rest and breed downard and endter wada 2013 faulkner et al 2010 rogers and ralph 2011 prior work by alminagorta et al 2016 developed a composite useable area for wetland wu metric for the bird refuge measured in km2 the study embedded the metric in a systems model and maximized the wetland surface area with suitable habitat for black necked stilt american avocet and tundra swan table 1 these three priority bird species were selected because they use a range of shallow medium and deep water habitat that encompass depths used by 20 other priority bird species at the refuge we related the wu and river flow outputs of alminagorta et al 2016 work at the bird refuge by running alminagorta et al s model to obtain the monthly wu values from the annual water availability scenarios between 1992 and 2011 next we divided monthly wu areas by the total refuge area to get a monthly wetland suitability index value then we developed monthly relationships between the suitaibilty index values and monthly river flows measured just upstream of the bird refuge at the corinne ut usgs station one example shown in fig a1 the impounded wetland indicator is calculated by multiplying a wsi index as a function of streamflow by the total wetland surface area aw mm2 in eq 4 the wsi defines an aggregate suitability index for multiple wetland bird species 4 i n d w e t l a n d s j k t w s i j k t q j k t a w j k t j k t 3 3 1 constraints reservoir releases diversions planting and other decisions are bound by physical infrastructure and management constraints appendix b eqs 5 18 physical constraints include low order finite difference approximation to conservation of water mass balance at each reservoir node and demand site they also include equations to constrain plant cover growth over time and define channel topography infrastructure constraints place minimum and maximum limits on reservoir and diversion canals capacity management constraints include urban and agricultural demand requirements and available budget to plant floodplain vegetation nonlinear objective and constraint functions in the wash formulation are all continuous and smooth to avoid numerical difficulties in the optimization 3 4 model input data the wash model requires hydrologic ecological topological and management data appendix a table a1 we collected the required data from sources including the utah division of water resources dwre water supply demand simulation model for the lower bear river adams et al 1992 between august 2012 and november 2016 we also established two monitoring sites on the bear river mainstem and one site on the cub river to collect and ground truth hydrologic and ecological data we assumed a budget of 650 000 to plant floodplain areas with cottonwoods based on a cache county water masterplan estimated budget for future ecosystem projects jub engineers 2013 this budgeting assumes there is no cost to change reservoir releases or diversion flows which infrastructure constraints require to stay within existing infrastructure capacities processed hydrologic and ecological data are available at the bear river fellows website http bearriverfellows usu edu wash model input data and code are available at the wash github repository alafifi 2017 3 5 model scenarios we implemented the model on a monthly time step for scenarios that test the effects of simulation vs optimization inflow hydrology and length of modeled time period table 2 3 6 model implementation we segmented the bear river and its main tributaries into a network of 43 nodes and 56 links with 3 reservoirs 12 municipal and agricultural demand sites and 26 environmental sites where species of concern live fig a2 we ran the model with the same weight value of 1 for all indicators to equally favor all locations species and months we coded the wash model equations 1 17 using the general algebraic modeling system software gams hozlar 1990 and solved the model using the non linear global solver branch and reduce optimization navigator baron sahinidis 1996 the gams code uses gdx gams data exchange format to read all input data from an ms excel spreadsheet and pass data to the model the 1 year implementation of the model for the lower bear river system has approximately 27 000 variables and 5 300 equations and takes 2 h and 15 min to find a global optimal solution on a dell xps windows10 64 bit computer 3 7 model outputs and visualization wash results include recommended flows reservoir releases storage volumes planted area vegetation cover and temporal and spatial variations of suitable aquatic floodplain and impounded wetland habitat area we used excel r and arcgis online to post process and display model results in an open access interactive web map http washmap usu edu with the web map users can compare modeled and simulated results add base maps and data layers and customize the tool the wash map displays results in us customary units to communicate better with local stakeholders all wash model input data code and post processing files are available at the wash github repository alafifi 2017 4 results the model run that simulated 2003 flows shows that modeled suitable aquatic floodplain and wetland habitat area in the watershed is far from the maximum habitat area fig 4 hashed bars compared to dashed lines in contrast wash recommended water allocations and planting of native vegetation have potential to increase the overall suitable habitat area by 25 000 acres fig 4 solid compared to hashed bars this overall increase is achieved with up to 3 10 and 7 fold increases over 2003 modeled historical conditions of aquatic suitable area in all months floodplain suitable areas in april to august and wetland suitable areas at the bird refuge from june to august fig 6 the model identifies increases in habitat in every month for all fish species life stages with the largest increases in may june and november the largest floodplain habitat increases for plants occur in july and august on bear river reaches above cutler reservoir these suitable areas approach 53 3 and 40 of the total aquatic floodplain and wetland habitat areas in the basin the wash model improves suitable habitat area by recommending monthly reservoir release storage and floodplain planting operations wash recommends increasing dec to mar releases at hyrum reservoir which creates additional empty storage in winter and spring that permits reducing late spring apr to may spills from hyrum and porcupine reservoirs fig 5 this recommended release pattern allows a small increase in late fall and winter base flows that support brown trout spawning in late fall and maintains the eggs in gravel redds until they hatch in spring releasing more water in early spring supports floodplain connectivity and helps to mitigate flooding impacts the gradual release of water from reservoirs also protects trout eggs from winter and spring flood events that could scour or kill incubated eggs and newly emerged fry george et al 2015 and could be harmful to cottonwood seedlings the model also recommends planting cottonwoods starting in march that increase vegetation cover and floodplain suitable area increased suitable floodplain area occurs even with declining or stagnant flows in several reaches fig 6 the model increases habitat area while continuing to meet urban and agricultural water uses at all demand sites during all months although wetland suitable area at the bird refuge increases to only 40 of the total suitable area improvements occur during critical summer months when bear river flows at corrine typically did not satisfy the bird refuge s junior water rights fig 7 overall the model recommended habitat area approaches 18 of the total available habitat area in the watershed if all suitability index values are at 1 additional river flow habitat suitability reservoir release and demand site results are available at http washmap usu edu running the model for 2005 wet year increased the suitable habitat area by 18 000acres fig 8 red circle while using 2004 flows dry year decreased the suitable habitat area by 15 000 acres fig 8 orange triangle reducing urban and agricultural demand in 10 increments increased habitat suitability area by approximately 4 000 acres per 10 reduction with most of the initial increases in habitat area occurring at the bird refuge and in aquatic habitat on the little bear river the model becomes infeasible at 110 of existing demand because agricultural and urban demands exceed available water running the model for 5 years from january 2003 to september 2007 shows that the model can sustain habitat increases across annual variations in flows fig 9 compares the monthly changes in inflow recommended reservoir operations and aquatic habitat suitability just downstream of hyrum and porcupine reservoirs on the little bear river hydrologic and ecological patterns are very similar and conform to storage and release patterns seen in the single year run fig 5 to identify when and where additional flow and floodplain vegetation will have the greatest system wide ecological benefits we examined the shadow values lagrange multipliers associated with the water mass balance constraints at nodes with headwater flow and vegetation growth constraints along each reach of ecological interest appendix b eqs 7 and 9 shadow values for flow report the increase in the wash objective function value habitat area measured in acres per one additional flow unit cfs table 3 the largest shadow values occur on the east fork of the little bear river for most months of the year shadow values greater than 2 5 acres cfs are also seen on the bear river in august on blacksmith fork from april to october and on the south fork of the little bear in two months the east and south forks in addition to blacksmith fork had the largest shadow values in the system because both active reservoirs hyrum and porcupine are located on these reaches there are more opportunities to temporally redistribute available water and instream water can also benefit lower reaches shadow values associated with the vegetation growth constraint are largest for all rivers and reaches in winter and spring fig 10 reaches on the malad river and bear river below cutler reservoir will have the largest increase in suitable habitat area per acre of native vegetated area added we also examined the shadow values for the budget constraint appendix b eq 18 and found a 30 acre increase in suitable habitat per additional 10 000 available for planting floodplains 5 discussion formulating the wash model objective function as a habitat area to maximize allowed us to examine ways to synergistically manage water and plants in the lower bear river to increase aquatic floodplain and wetland habitat area for priority species while satisfying water demands of existing agricultural and urban users additional suitable area can be achieved by 1 releasing water from reservoirs at times priority species needs water to complete key lifecycle functions and 2 planting more riparian trees in spring spring planting increases vegetation cover and takes advantage of natural vegetation growth over time to increase habitat area in the lower bear river the model recommends releasing more water from porcupine and hyrum reservoirs in december to march to capture and store april and may spring runoff and reduce spills this recommended seasonal drawdown supports floodplain connectivity and reduces winter and spring flood events that could scour or kill incubated eggs newly emerged fry george et al 2015 or cottonwood seedlings seasonal drawdown could simultaneously help reduce late spring flooding downstream of the reservoirs improvements in floodplain habitat area are small relative to aquatic habitat because several summertime diversions lower instream flows and decrease lateral connectivity to adjacent floodplains at the same time many reaches border private agricultural fields and grazing lands and have narrow riparian corridors planting native floodplain vegetation according to model recommendations will require managers to set up agreements and easements with riparian landowners wash recommended flow regimes will also increase overbank flood recurrence to improve floodplain connectivity therefore managers need to consider flooding effects on neighboring farmers ranchers and hunters recent conservation easements made by pacificorp which owns large floodplain areas within the study area illustrate one way to co manage for multiple objectives these easements have allowed riparian plant restoration projects to also serve as flood buffer zones wash results can help identify promising locations to site additional conservation easements to improve habitat quality for multiple aquatic and floodplain priority species changing reservoir operations diversions and other management actions higher up in the basin can also increase impounded wetland habitat during summer months these results support bird refuge managers recent efforts to establish conservation easements with upstream landowners acquiring upstream storage rights would allow refuge managers to store winter flows and release in summer to beneficially improve habitat for avocets and stilts storage rights could also help managers plan for droughts formulating the wash objective function as a habitat area to maximize also shows where and when to direct scarce water money and planting efforts to most improve habitat water is scarce during summer months but wash results suggest managers can create 2 5 12 acres of additional suitable habitat per additional cfs of flow acquired during summer fall or winter on the east fork of the little bear river or during late summer and fall months on the blacksmith fork or south fork of the little bear river the little bear had the greatest return per additional cfs because it has aquatic and floodplain habitat that need just a little more water to improve to good habitat condition and those instream flows will also benefit downstream reaches similarly wash model recommendations to plant native floodplain vegetation in spring replicate findings by alminagorta et al 2016 to remove invasive wetland vegetation at the beginning of the growing season to see the largest increases in suitable wetland habitat area wash results were corroborated using a participatory modeling approach bockstaller and girardin 2003 langsdale et al 2013 stakeholders initially helped to define the aquatic floodplain and wetland metrics later we presented results to project stakeholders using wash interactive web map http washmap usu edu for example during an august 2016 model workshop we presented key reservoir release and habitat area results earlier versions of figs 4 5 and 7 9 while stakeholders simultaneously explored results in real time on their phones tablets and laptops their explorations identified a problematic aspect of reservoir releases for bct and motivated us to update aquatic suitability indices to reflect temperature water depth relationships base water depth ranges on recent fish ecology studies and differentiate bct and brown trout distributions because wash multiplies habitat suitability indices by affected areas the model structure is flexible and can be extended to explicitly include additional water quality parameters such as dissolved oxygen or turbidity similarly modelers can add other species habitat attributes or habitat types such as natural oxbow seasonal or other wetlands in the watershed that were not included in the lower bear river study the wash model has several limitations and we discuss these limitations in order from what we assess as least to most important the wash model allocates water and vegetation using perfect foresight of future water flows and vegetation growth rates managers never have perfect information about these factors however bear river flows are snowmelt driven and managers use snowpack measurements throughout the winter and spring to forecast spring summer and fall river flows forecast reliability decreases in successive years thus multi year scenario results are more appropriately interpreted as the upper bound on potential habitat gains when future flows are known perfectly the wash model quantifies some habitat quality conditions that are necessary for the survival and productivity of priority species however it does not predict or model species distribution across the watershed while we have validated habitat quality conditions with available and collected cutthroat trout and cottonwood tree sightings we see further value to collect additional field data that describe fish growth in migration out migration predators natural die off and other factors and couple models of these species distribution processes to the wash model the wash model assumes that measured and modeled water depths and channel widths are uniform along reaches that are a few miles long also that suitable area varies in time and is separable by habitat type species species life stage and location these assumptions were made using the best available measured data and do not capture species resilience to or recovery from very poor conditions a finer spatial resolution or coupling our model with a hydrologic model could improve our findings at the same time the model results increase or sustain suitable habitat areas for each habitat type species and species life stage compared to modeled historical conditions we also assume that inundating the floodplains during seed germination and dispersal will help riparian plants to reestablish this assumption neglects seedling survival which requires other biotic and abiotic conditions such as groundwater level soil salinity and plant competition for water bhattacharjee et al 2008 we recognize that each of the aquatic floodplain and impounded wetland suitability indices si carry along statistical errors that result from measurement error spatial and temporal variability and function form van der lee et al 2006 more spatially resolved ecological data can help determine where finer and coarser data is appropriate for modeling in ongoing research we are evaluating and quantifying uncertainties in si curves and other model parameters and their implications for water management alafifi 2018 implementing wash recommendations to improve habitat will also require recognizing and protecting environmental flows in the water permitting process utah water law has limited methods to appropriate water for instream flow and must allow more entities to donate lease or purchase existing water rights lane and rosenberg 2020 szeptycki et al 2015 expanding the allowed entities will reduce risks that a downstream water right holder will appropriate an instream flow for their beneficial use currently managers in the basin such as the cache water district are investigating ways they and others can hold instream flow rights to implement model recommendations 6 conclusions improving habitat in a watershed requires determining when where and how to allocate water and vegetation planting efforts within a basin prior systems models to improve habitat have principally focused on the water allocation component these models either maximized human benefits of water or included habitat quality as a flow constraint other models minimized deviations from natural or species required flow regimes here we developed a suitable habitat area metric measured in acres that is tied to flow and native vegetation conditions in the watershed we embedded the metric in a systems optimization model named wash that identified the timing and location of reservoir releases river flows and planting efforts to maximize habitat area subject to physical infrastructure and management constraints we applied wash to the lower bear river ut using stakeholder verified species and site specific habitat suitability curves for cutthroat trout brown trout cottonwood black necked stilt american avocet and tundra swan wash identified opportunities to increase aquatic floodplain and impounded wetland habitat area by 25 000 acres over existing conditions this increase can be realized by releasing more water from porcupine and hyrum reservoirs in winter months using vacated storage to capture spring runoff and reducing late spring spills also by planting native floodplain vegetation in spring months in reaches with low vegetation cover further procuring additional flow in the east fork of the little bear river during summer fall and winter months would most increase habitat area per cfs of new flow the wash web map application provided managers with direct access to model results helped us validate results and motivated further model development to make scenarios and results more relevant to managers overall developing and embedding a habitat area metric in a systems model as an ecological objective to maximize has allowed us to compare habitats across watershed sites and identify sites and times where managers can apply scarce water money and planting efforts to most improve habitat quality and area declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by national science foundation nsf grant 1149297 joan degiorgio bryan dixon bob fotheringham james derito paul thompson sarah null and karin kettenring contributed to the model development and provided feedback on results 16 undergraduate bear river fellows helped collect synthesize and analyze flow stage and cross sectional data plus tested model scenarios nour attallah reviewed the github reporistory and reproduced the model results appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104643 appendix a additional figures and tables fig a2 wash model schematic for the lower bear river utah basin fig a2 fig a1 example wash wetland suitability index for february fig a1 table a1data required for wash model components model component data item source s component type aquatic habitat reach lengths nhdplus v2 2016 usgs 2012 field measurements link water depth ecological suitability curves fws stakeholders and literature link floodplain habitat plant cover and distance from banks usda 2014 naip imagery field measurements link floodplain area naip imagery field measurements link flow ecological suitability curves fws stakeholders and literature link wetland habitat wetland unit water level storage curves lidar field measurements link invasive plan cover landsat satellite imagery link evaporation rates western regional climate center link flow ecological suitability curves fws stakeholders and literature link physical constraints reservoirs storage elevation area evaporation and capacity adams et al 1992 u s bor node diversions capacity adams et al 1992 link natural constraints headwater and local inflows usgs nhdplus v2 uwrl 2009 node water level and channel cross section field measurements link evaporative losses on reaches nhdplus v2 link natural growth of riparian plants stakeholders link management constraints urban and agricultural demand genres node consumptive use of flow genres node instream flow requirements stakeholders link budget and unit costs stakeholders link model formulation weights stakeholders link appendix b wash model constraints this appendix describes the physical eqs 5 14 infrastructure eq 15 and management eq 16 18 constraints that limit reservoir release diversion planting and other decisions in the wash model a reservoir storage balance a low order finite difference mass balance requires that reservoir storage for each reservoir v at the beginning of each time step t 1 equal storage at the beginning of the prior time step t plus net flows of links leading to the reservoir minus reservoir releases and minus evaporation losses eq 5 reservoir releases are flows along all links that leave reservoir v in month t eq 6 evaporation losses are estimated by multiplying a monthly evaporative rate e v a p v t m month by the reservoir surface area r a v t is a function of reservoir storage the term l s s j v t is the net loss rate on links connecting to reservoir v and is expressed as a fraction of link flow 5 s t o r v t 1 s t o r v t j q j v t 1 l s s j v t r r v t e v a p v t r a v t s t o r v t v t 6 r r v t j q v j t v t b mass balance at junctions flows entering each non reservoir node j must equal or exceed evaporative losses plus flows leaving the node eq 7 localinflow j t mm3 month are reach gains groundwater inflows or other flows that accumulate at node j in time t at the most upstream nodes in a network localinflow is the head flow and represents the boundary condition and cumulative contribution of climate runoff and other hydrologic processes linkevap m month describes the evaporative loss rate on links link evaporation m3 month is the product of the evaporative loss rate and channel surface area 7 l o c a l i n f l o w j t k q k j t 1 l s s k j t k a k j t l i n k e v a p k j t k q j k t j t c mass balance at each demand site total flow to each demand site dem in time t must equal or exceed the return flow back to the river eq 8 total flow is reduced by the depleted flow amounts that include diversion losses l s s k d e m t and urban or agricultural consumptive use fraction c o n s k d e m t both of inflow received 8 k q k d e m t 1 l s s k d e m t c o n s d e m t k q d e m k t d e m t d plant cover plant cover c j k t n mm2 for each species n in each link j to k at time step t equals cover at prior time step t 1 plus planted areas r v j k n mm2 and natural growth or death g j k n mm2 eq 9 g j k n describes the natural increase or decrease in riparian plant cover without any interference from managers both plant cover c j k t n and planting area cannot exceed the total floodplain area adjacent to each reach c m a x j k eqs 10 and 11 additionally planting r v j k n can only occur during the growing season eq 11 9 c j k t n c j k t 1 n r v j k t n g j k n j k t n 10 n c j k t n c m a x j k j k t 11 n r v j k t n c m a x j k t g r o w i n g s e a s o n 0 o t h e r w i s e j k t j k t e channel topology relationships river flow channel stage width and surface area are related on each link j to k in each time step t eqs 12 14 these relationships are established based on measured data we use leopold and maddock 1953 linear relationships for stage flow s f and width flow w f relationships l n g j k is the length of each river segment m 12 stage flow relationships d j k t s f 1 j k q j k t s f 2 j k j k t 13 width flow relationships w d j k t w f 1 j k q j k t w f 2 j k j k t 14 channel surface area a j k t w d j k l n g j k j k t f reservoir storage limits storage in each reservoir v cannot go below a minimum storage volume minstor v mm3 which is the reservoir dead pool similarly storage can exceed the flood control pool level or storage capacity maxstor v mm3 at any time t eq 15 15 m i n s t o r v s t o r v t m a x s t o r v v t g meet demand requirements diversions to each demand site dem should meet requirements dreq dem t mm3 month in each time t eq 16 16 k q k d e m t 1 l s s k d e m t d r e q d e m t d e m t h flow limits minimum and maximum values q m i n j k t and q m a x j k t bound flow in each link j to k in time t eq 17 minimum levels may be minimum instream flow or diversion requirements maximum bounds can be channel diversion or other capacities 17 q m i n j k t q j k t q m a x j k t j k t i management budget the total cost to plant floodplain species ct n m2 make reservoir releases or adjust diversion gates st n m3 should not exceed the financial budget b eq 18 18 j k n t c t n r v j k t n j j t s t j k t q j k t b 
26058,this paper introduces a polynomial feedforward neural network based on chebyshev polynomials able to effectively model non linear and highly complex environmental data the data sets were cautiously selected from the fields of biology ecology climate and environmental management and economics as to represent a scientifically meaningful and consistent corpus of disparate domains of intensive focus and interest in current ecological environmental research covering issues related to body growth age biomass production energy efficiency consumption and ecology geographic extension the proposed network uses a number of layers to estimate the output in terms of a weighted sum of truncated chebyshev series expansions applied to linear combinations of the input variables and it is trained by the differential evolution algorithm its performance was compared to three neural networks first a polynomial feedforward network that uses hermite polynomials as activation function in the hidden nodes second a radial basis function neural network third a takagi sugeno kang neuro fuzzy network all the above networks were trained by evolutionary optimization algorithms the comparison was carried out by standard criteria such as the root mean square error and the mean absolute error moreover a non parametric kruskal wallis statistical test used to compare the median values of the root mean square errors between methods the main experimental outcomes are a the network s efficiency improves for higher polynomial orders b the statistical analysis suggests that the proposed network appears to be very competitive to the other three networks keywords polynomial neural networks chebyshev polynomials environmental modelling ecological data environmental data 1 introduction environmental data are related to highly non linear and complex processes rendering the model development a tough problem to cope with the difficulties arising from the complexity of such kind of data many researchers have employed neural network nn structures as alternative ways to carry out data analysis and modelling various environmental applications have demonstrated the determinant contribution ascribed to nns coad et al 2014 lein 2009 millie et al 2012 monteil et al 2004 rath et al 2017 shoji and kawakami 2006 a type of nn with very effective performance is the feedforward neural network fnn several variants of fnn have been used such as standard fnn with one hidden layer general regression neural networks grnn and multiple layer perceptron mlp the usage of grnns mlps and standard fnns in water quality assessment was investigated by schleiter et al 2001 and souza da costa et al 2009 deweber and wagner 2014 developed an ensemble of fnns to predict the mean daily water temperature in individual stream reaches li et al 2014 selected appropriate smoothing parameters for grnn and evaluated the resulting network in water resources applications while adamowski and chan 2011 employed a standard fnn to perform groundwater level forecasting papantoniou and kolokotsa 2016 and alimissis et al 2018 performed comparative analysis between different fnns as well as between ffns and more traditional approaches for the test case of the urban air quality prediction young et al 2011 discussed the utilization of mlp in modeling net ecosystem metabolism within a freshwater wetland tian et al 2017 used an input stabilized mlp to increase the prediction accuracy of chlorophyll dynamics of algal blooms santos et al 2014 used a standard fnn to extract patterns from soil chemical variables and microbial community structures in forest soils tracey et al 2011 employed an evolutionary computation based fnn to simulate the animals movement in their natural environment balsamà et al 2014 combined grnn with mlp to detect non linear behaviors shared by several chemical species where the data were taken from the emissions database for global atmospheric research mikulandric et al 2014 applied fnn in modeling the biomass gasification process in fixed bed gasifiers while king et al 2014 in detecting organic pollutants in microbial fuel cell based biosensing in this paper a polynomial nn pnn is introduced that establishes nonlinear local regression models in terms of truncated chebyshev series which possess strong approximation capabilities patra and kot 2002 lee and jeng 1998 rivlin 1974 the result is a novel approach regarding the use of nns in modelling highly nonlinear and complex environmental data although too much effort has been put on using typical nns in environmental data analysis there are relatively few studies that address the investigation of more sophisticated schemes such as pnns rigos et al 2014 2016a 2016b tomić et al 2018 tsekouras et al 2018 pnns are networks with polynomial node activation functions able to capture highly non linear and complex input output relationships ivakhnenko 1971 liu and wang 2012 maric 2013 purwar et al 2007 vukovic et al 2018 so far many different types of pnns have been implemented the first attempt to develop such kind of structures is credit to ivakhnenko 1971 who introduced the group method of data handling which was based on the well known kolmogorov gabor polynomials oh et al 2012 demonstrated the applicability of this type of polynomials in designing efficient polynomial radial basis functions neural networks rbfnn lee and jeng 1998 used tensor analysis in order to produce a unified model where the node activation functions were realized with the aid of chebyshev polynomials however as this configuration lies in the realm of functional nns its size grows fast with the number of input variables rendering the network hardly applicable in high dimensional problems later studies investigated different frameworks to embed chebyshev polynomials in fnns and rbfnns and applied them in nonlinear system identification and automated shoreline extraction rigos et al 2014 2016b other approaches have been associated with the incorporation of hermite and legendre polynomials in network s structure regarding various types of applications ma and khorasani 2005 mall and chakraverty 2016 rigos et al 2016a tsekouras et al 2018 a problem associated with most of the above approaches is that the inputs to different polynomial activation functions are different to one another meaning that the obtained series expansion is not exact ma and khorasani 2005 this fact might negatively affect the network s approximation capabilities because the polynomial representations might be compromised in this study we introduce a feedforward chebyshev pnn which generates nonlinear regression models for seven data sets the data sets come from public domains and they are related to high dimensional and complex processes chebyshev polynomials possess strong approximation capabilities and a number of properties that make them a very effective modelling tool in such kind of data andrews et al 2000 lee and jeng 1998 patra and kot 2002 rigos et al 2016b rivlin 1974 the network s structure encompasses a number of design phases that act in sequence the input variables are aggregated to produce a number of linear combinations these combinations are appropriately scaled i e normalized and enter the layer with the polynomial nodes contrary to the methods developed in ma and khorasani 2005 rigos et al 2016a where each node in polynomial layer encodes only one polynomial herein each node encodes a chebyshev polynomial series thus obtaining exact series expansions of the set of input variables tsekouras et al 2018 finally the network s output comes in the form of the weighted sum of the above mentioned series expansions the training process is based on using the differential evolution price et al 2005 to minimize the square error with respect to the network s design parameters the main advantage of the proposed algorithm is its ability to effectively model complex ecological environmental data using a relatively small number of nodes making feasible the use of higher order polynomials to capture the nonlinearity of the problem at hand thus producing more robust results the rest of the paper is organized as follows section 2 provides the mathematical description of the proposed pnn section 3 reports the data sets used section 4 focuses on the simulation experiments and discusses the corresponding results finally the paper concludes in section 5 2 the proposed chebyshev polynomial neural network 2 1 chebyshev polynomials since the proposed network utilizes chebyshev polynomials there are some properties characterizing their functionality which are presented below in general the nth order chebyshev polynomial is given as 1 p n x cos n ϕ i 0 n 2 1 i j i n 2 n 2 j j i x n 2 i where x cos ϕ and stands for the integer part i e floor function and the parameter i takes integer values between 0 and n 2 there are two categories of chebyshev polynomials namely polynomials of the first kind and polynomials of the second kind in this paper we employ chebyshev polynomials of the second kind which are generated by the subsequent recurrence relations rivlin 1974 2 p 0 x 1 3 p 1 x 2 x 4 p n 1 x 2 x p n x p n 1 x as an example using the above recurrence relations the resulting polynomials with orders n 0 1 2 3 and 4 are given below 5 p 0 x 1 p 1 x 2 x p 2 x 4 x 2 1 p 3 x 8 x 3 4 x p 4 x 16 x 4 12 x 2 1 this kind of chebyshev polynomials are orthogonal in 1 1 with respect to the weight 1 x 2 6 1 1 p n x p m x 1 x 2 d x 0 n m π 2 n m finally it can easily be proved that the nth order polynomial is written in the form rivlin 1974 7 p n x i 0 n 2 ψ n n 2 i x n 2 i with 8 ψ n n 2 i 1 i 2 n 2 i n i i where the parameter i has been defined previously 2 2 structure of the network let us assume that we are given a system with p inputs x x 1 x 2 x p t r p and one output y r the proposed feedforward neural network comprises four layers operating in sequence and its structure is illustrated in fig 1 a in view of fig 1 a layer 1 generates linear combinations of the input variables denoted as λ i x for i 1 2 c the network s design parameters related to this layer are the coefficients of the linear combinations symbolized as b i j i 1 2 c j 1 2 p in layer 2 the values λ i x are scaled in the interval 1 1 resulting in the normalized values λ i x with 0 λ i x 1 i as sated in the previous section the reason behind the scaling process is that the chebyshev polynomials are orthogonal in 1 1 and thus able to effectively operate only in that interval layer 3 is the hidden layer and its main functionality is to expand the normalized values λ i x coming from the previous layer into truncated chebyshev series of order n which are denoted as s i n i 1 2 c the general structure of the ith hidden node in that layer is depicted in fig 1 b where its output is the ith truncated chebyshev series s i n based on this figure the network s design parameters related to this layer are the coefficients μ i ℓ i 1 2 c ℓ 0 1 n that are multiplied with the chebyshev polynomials p ℓ ℓ 0 1 n finally layer 4 linearly combines the resulting series expansions and generates the network s output the network s design parameters related to this layer are the coefficients z i i 1 2 c that are multiplied with the truncated chebyshev series s i n i 1 2 c the above layers are analytically described in the subsequent paragraphs layer 1 includes c summation nodes each of which generates linear combinations of the input variables 9 λ i x j 1 p b i j x j where i 1 2 c and b i j are the design parameters of network related to this layer see fig 1 a note that λ i x r i layer 2 performs a scaling process to map λ i x into λ i x 1 1 i let us assume that λ i x φ min φ max then the objective is to map the interval φ min φ max into 1 1 as follows 10 λ i x r λ i x g r j 1 p b i j x j g with 11 r 2 φ max φ min and g φ max φ min φ max φ min thus λ i x 1 1 which is a linear combination of the input variables x 1 x 2 x p also fig 2 describes the above scaling transformation what remains to be done is to calculate the interval φ min φ max let us assume that b i j β min β max and x j x j min x j max for j 1 2 p then b i j x j φ j min φ j max where φ j min φ j max β min β max x j min x j max i e φ j min φ j max is calculated by the standard multiplication of the intervals β min β max and x j min x j max moore 1966 to this end using the above result and the definition of λ i x in eq 9 we get 12 φ min j 1 p φ j min λ i x j 1 p φ j max φ max in this paper the parameters β min and β max are predefined by a trial and error approach layer 3 generates the nth order truncated chebyshev series for each λ i x 13 s i n ℓ 0 n μ i ℓ p ℓ λ i x where μ i ℓ r i 1 2 c ℓ 0 1 n are network s design parameters related to layer 3 see fig 1 b finally layer 4 combines the above mentioned truncated series in order to estimate the network s output 14 y ˆ i 1 c z i s i n i 1 c ℓ 0 n z i μ i ℓ p ℓ λ i x where z i 1 i c are network s design parameters related to layer 4 see fig 1 a using 9 eq 14 is modified as to include all the network s design parameters 15 y ˆ i 1 c ℓ 0 n z i μ i ℓ p ℓ r j 1 p b i j x j g 2 3 training of the network let us assume that we are given a set of n input output data pairs x k y k k 1 n with x k r p and y k r the problem addressed is to minimize the next square error with respect to the network s design parameters b i j μ i ℓ and z i for i 1 2 c j 1 2 p and ℓ 0 1 n 16 j s e k 1 n y k y ˆ k 2 k 1 n y k i 1 c ℓ 0 n z i μ i ℓ p ℓ r j 1 p b i j x k j g 2 to minimize the above mentioned square error we use differential evolution de which is a population based evolutionary algorithm price et al 2005 the reason behind this choice comes from several figurative advantages provided by this algorithm lin et al 2018 price et al 2005 subudhia and jena 2011 first de is a derivative free optimization mechanism that does not use any gradient descent procedure therefore it can be easily applied to highly non linear optimization problems second it does not depend on the random initialization a fact that ensures the convergence to an optimal solution third it is less vulnerable to get trapped in local minima because it performs a parallel search in the feature space fourth contrary to other heuristic algorithms it overcomes the problem of premature convergence by maintaining a balance between the local and the global exploration of the searching space finally due to its stochastic nature it can exhibit flexible and robust optimization behavior de involves a population of m individuals d t d t 1 d t 2 d t q t r q that search the space for an optimal solution note that t 1 2 m and q is the dimension of the individuals search space de employs three evolving phases mutation crossover and selection in this paper the learning rule in the mutation phase is 17 δ t d b e s t f d t 1 d t 2 where δ t δ t 1 δ t 2 δ t q t is the mutation vector d t 1 and d t 2 two distinct individuals different from d t d b e s t is the position of the best solution found so far and f 0 1 the crossover phase is typically executed by the next rule 18 v t s δ t s i f γ c r s s 0 d t s o t h e r w i s e where s 1 2 q ν t v t 1 v t 2 v t q t is a trial vector γ is a random number in 0 1 i e γ u 0 1 c r 0 1 is the crossover factor and s 0 is a random integer in 1 2 q to this end the selection phase applies a greedy selection between ν t and d t to decide which of those will be transferred to the next learning cycle the above phases are implemented for a maximum number of learning cycles denoted as t max the search space is defined by the number of the network s design parameters i e b i j μ i ℓ and z i i 1 2 c j 1 2 p ℓ 0 1 n and its dimension is equal to q c p n 2 fig 3 shows the structure of each individual involved in the learning process finally the parameters b i j 1 i c 1 j p correspond to the first c p elements of each individual and their domain of values is the interval β min β max therefore in each iteration after the calculation of vector ν t in 18 the following correction takes place 19 v t s β min v t s β min v t s β max v t s β max f o r t 1 2 m a n d s 1 2 c p implementing 19 ensures that the parameters b i j 1 i c 1 j p will always lie in the interval β min β max and therefore λ i x will always be confined in 1 1 3 description of data sets seven public domain and highly cited datasets are studied which were taken from disparate domains of interest in the current ecological environmental research table 1 reports the basic properties of the data sets which are grouped into four main categories namely body growth age biomass production energy efficiency consumption and ecosystem geographic extension the abalone data set concerns the prediction of the age of a member of the abalone species from physical measurements and it includes 8 input variables related to the size length diameter height and the weight whole shucked viscera and shell waugh 1995 here the first input variable was removed because it was a trichotomous categorical variable and obtained a system with 7 inputs and one output source uci machine learning repository https archive ics uci edu ml index php the data set fish abundance knouft and anthony 2018 is used for two different species of fishes namely campostoma anomalum commonly known as central stoneroller and cyprinella spiloptera known as spotfin shiner both are from the genus cyprinella and are freshwater fishes endemic to the eastern central and midwestern u s they are not considered to be in danger so they are not under special conservation protection the input variables used in predicting their population were the longitude and latitude of the sample the annual mean temperature the mean diurnal temperature range the temperature seasonality the minimum and maximum temperatures measured the annual range of the temperature the total annual precipitation the precipitation during the wettest and driest month and the precipitation seasonality while the predicted output is the population of the species source http rsos royalsocietypublishing org content 3 6 160093 the energy efficiency example deals with the prediction of energy efficiency with respect to twelve different building shapes tsanas and xifara 2012 the buildings differ with respect to the relative compactness surface area wall area roof area overall height orientation glazing area and glazing area distribution the output variable is the heating load of a building the data set was taken from the uci machine learning repository the daily average price of electricity energy dee refers to the prediction of daily average price of tkwhe electricity energy for the year 2003 in spain coming from hydroelectric nuclear electric carbon fuel natural gas and other special sources of energy source keel software https sci2s ugr es keel alcalá fdez et al 2009 the data set forest fires aims to predict the burned area of forest fires in the northeast region of portugal by using meteorological and other data cortez and morais 2007 there are 12 input variables and one output i e the burned area which was post processed with the ln x 1 transform the data were taken from the uci machine learning repository finally the data set ground temperature is used to predict current species distributions and future changes through ground temperatures modelled at high resolutions across european landscapes in particular it includes recorded soil temperatures for two sets of sites in europe from two different periods gunton et al 2015 the data were taken from the dryad digital repository https datadryad org resource doi 10 5061 dryad fr075 1 the overall epistemic rationale for the selection of testing datasets is presented in fig 4 based on this figure the datasets cover a large array of ever evolving conceptually but fundamentally intriguing environmental research questions i e 1 developmental biology and allometry of body growth the exemplary case of haliotis rubra vernacular name abalone 2 eco physiology of population community level determinants of freshwater fish abundance in climatic eco regions 3 high resolution geo physical variables determining individual species extension after climate forcing such as ground temperature 4 intensity of multi determined extreme catastrophic events such as wildland fires 5 technological dimensions of indoor housing micro climate conditions relating to or modulated by energy consumption and 6 economic dimension or pricing of electricity energy daily in addition they encompass two important multi layered and multi faceted scales of interest the first relates to time scale the categories of datasets are arranged across a time dependent gradient of ecological processes from daily variations to multi annual fluctuations of dependent variables the second relates to the complexity scale of the studied systems inherent adaptive responses of single biological ecological entity vs higher order complex anthropogenic systems such as household electricity consumption finally as shown in fig 4 the selected data sets do not address some very specific combinations e g daily variations in attributes of organisms and populations it is the case of microbes and long living species it is the case of tree species and big mammals however the relevant issues to 1 body growth age 2 biomass production 3 energy efficiency consumption and 4 ecosystem geographic extension are accurately and properly addressed 4 simulation results and discussion in this section we apply the proposed network to the seven case studies described in section 3 as mentioned previously to implement the scaling process the parameters β min and β max that define the domain of values of the network s design parameters b i j i 1 2 c j 1 2 p were selected by a trial and error as to obtain the best possible performance their values for each data set are depicted in table 2 to implement the differential evolution we used a moderate population size equal to m 20 this selection is expected to provide a fast learning process while maintaining effective optimization results price et al 2005 the maximum number of learning cycles was selected as t max 500 which will provide enough time to the de to converge on the other hand for each data set the values of parameters f and c r see eqs 17 and 18 were prefixed within the interval 0 1 lin et al 2018 price et al 2005 subudhia and jena 2011 table 2 presents the values selected for each data set to validate the performance of the proposed pnn its results were compared with the respective results obtained by three other networks first the hermite neural network hnn developed by ma and khorasani 2005 which is a feedforward network encompassing one hidden layer of c nodes the activation function of the ith node is the hermite polynomial with order equal to i 1 i c the input layer creates c linear combinations of the input variables using a set of optimized weights the ith linear combination enters the node with activation function the ith order hermite polynomial then the network s output is calculated as the weighted sum of the above polynomials in this paper all weight parameters of the hnn are optimized by minimizing the square error using the particle swarm optimization pso algorithm kennedy and eberhart 2001 pso involves a swarm of m particles p i r q each of which is assigned a velocity v i r q the position p i b e s t is the best solution obtained so far by the particle p i and p b e s t is the best solution obtained so far by all particles then the velocity is calculated as 20 v i t 1 η v i t χ 1 φ 1 p i b e s t t p i t χ 2 φ 2 p b e s t t p i t where χ 1 χ 2 are randomly selected in 0 1 η φ 1 and φ 2 are positive constant numbers called the inertia cognitive and social parameter respectively finally the position of each particle is 21 p i t 1 p i t v i t 1 herein each particle encodes all the design parameters of hnn to conduct a fair comparison the population size and maximum number of learning cycles were selected to be the same to the respective parameters used in the proposed method m 20 and t max 500 respectively the rest of the design parameters of pso were assigned the typical values commonly used in the literature cao et al 2018 chen et al 2018 kennedy and eberhart 2001 i e η randomly selected in the interval 0 5 1 and φ 1 φ 2 2 the second network is a radial basis functions neural network rbfnn where the hidden layer includes c nodes with gaussian type activation functions this network is trained by the de algorithms the parameter setting of which is the same to the parameter setting used by the proposed method finally the third network is a typical takagi sugeno kang tsk neuro fuzzy network which employs gaussian type membership functions this network is optimized in terms of the pso algorithm with the same parameter setting as described in the case of hnn to carry out the comparison study two performance indices were used as reliable indicators of the networks performances namely the root mean square error rmse and the mean absolute error mae 22 rmse k 1 n y k y ˆ k 2 n 23 mae k 1 n y k y ˆ k n to conduct the experiments each data set was randomly divided into a training subset containing the 60 of the original data and a testing subset consisting of the rest 40 all networks were run for c 2 4 6 8 and 10 nodes in addition the proposed network was run for truncated chebyshev series s i n with orders n 2 3 and 4 for each method number of nodes and polynomial order 20 random initializations were considered where the first 10 were used to measure the rmse performance index and the last 10 to measure the mae performance index tables 3 and 4 depict the rmse mean values along with the respective standard deviations while tables 5 and 6 illustrate the respective results for the mae index in those tables the best results for each number of nodes in both the training and testing data are indicated by the bold fonts it can be easily seen that apart from few cases in the fish abundance campostoma building energy efficiency and ground temperature data sets the proposed method obtained better results for both the rmse and mae performance indices to further assess the results a rigorous non parametric statistical comparison between the proposed network for n 4 and the other networks takes place the comparison is realized in respect to the testing data and values for the number of nodes as c 2 4 6 8 and 10 figs 5 and 6 illustrate the boxplots of the descriptive statistics simulation results by method and number of nodes for each dataset nonparametric statistical inference was performed in order to compare median values of rmse index between methods an overall 5 significance level was maintained for comparisons within each dataset a kruskal wallis test was performed to detect statistically significant differences in median between the four methods studied for each level of nodes separately the bonferroni adjusted overall p values for comparing the methods are given in table 7 the column of p values follow up testing consisted of performing mann whitney tests to compare our method to the remaining three separately for each level of number of nodes again maintaining an overall 5 significance level within each dataset by using adjusted p values the adjusted p values are shown in table 7 also we note here that since random initializations were used for each method number of nodes polynomial order combination our nonparametric statistical analysis was based on independent samples thus the use of the sign and the wilcoxon s signed rank test was not warranted in our application to test equal predictive accuracy of the methods diebold and mariano 1995 for an interesting forecasting application where the sign and the wilcoxon s signed rank test are appropriate we refer the interested reader to tzanis et al 2019 by considering solely the comparisons with associated adjusted p values less than 0 05 we conclude that statistically significant differences between methods for all values of c were detected in all data sets of the organizational levels of body growth age biomass production and energy efficiency consumption the same holds for the ground temperature data set of the ecosystem geographic extension level however as far as the forest fires example is concerned statistically significant differences between methods were detected only for c 4 6 and 8 more specifically the proposed chebyshev pnn clearly outperformed the other three methods for all values of c in the cases of the abalone fish abundance cyprinella and daily average price of electricity energy data sets in fish abundance campostoma dataset it was superior to hnn and tsk for c 2 6 8 and 10 when compared to rbfnn however no statistically significant differences were observed for the building energy efficiency dataset it obtained better approximation behavior when compared to the hnn for all values of c while outperforming the rbfnn for c 2 and the tsk for c 2 4 only regarding the forest fires dataset it achieved more efficient performance when compared to the rbfnn and tsk only in the case where c 6 finally in the ground temperature dataset it was superior to rbfnn for all values of c to the hnn for c 2 4 6 and to tsk for c 10 the application of the proposed pnn on the selected datasets as indicative examples highlighted its performance and value validity as a general predictor on a series of core issues of modern environmental ecological science policy and decision making for instance the accurate prediction of the age of specimens from physical measurements is capital for several conservation biology applications e g holmes 2004 such as population viability analysis fisheries management or dendrochronology and reconstruction of past climatic conditions this is the example of abalone data set in view of the above analysis the proposed pnn obtained the best performance and therefore it has the potential of greatly assisting the above mentioned research areas changes in territorial and geographic extension of individual species and species assemblages combined with relative abundance rarefaction is a priority for global change biology projecting such trends from climatic variables under various global change scenarios allows for better understanding future biodiversity conditions kocsis and hufnagel 2011 sala et al 2000 this is the case of the fish abundance data set in the case of campostoma species there is a clear dominance credit to the polynomial order n 2 however even when using n 4 in the statistical analysis the proposed pnn outperformed the other networks as far as the cyprinella species is concerned the most appropriate polynomial order seems to be n 4 since the influence of habitat quality on population has been well studied sala et al 2000 and taking the network s performance into account its ability to predict the effects of climate conditions especially in the era of increasing climate change effects will be useful in making conservation efforts more targeted and effected as episodes of extreme climatic conditions become more frequent due to climate change and the urban wildland interface expands worldwide occurrence of natural hazards such as forest fires increase causing severe ecosystemic losses and economic burdens upon affected communities and civil protection mechanisms e g cortez and morais 2007 the simulation results of the forest fires example suggest that the proposed network can be efficiently used in improving fire behavior prediction which is of outmost importance especially when estimates of the potential burned area using mostly meteorological data is concerned on the other hand home level energy demand for heating determines significantly the energy consumption profile of an agglomeration which refers to the energy efficiency example the daily average price of electricity energy example focuses on that subject i e the pricing of energy supply and drives both energy governance decisions and individual behavior of household consumers gyamfi et al 2013 in the former example the network appears to yield the best results for n 3 while in the latter for n 4 as these examples refer to strategies concerning the determination of the energy mix at an institutional level and the consumption attitude of individual consumers the usage of the network appears to be of major significance finally the wide variation and fast fluctuation of ambient temperatures especially over short distances impose certain challenges in revealing species under changing climates and therefore to accurately assess species distributions temperatures must be modelled at high resolutions gunton et al 2015 this is the case of ground temperature example the performance of the proposed network seems to imply that the most appropriate polynomial order to model the data is n 3 which obtained the best average rmses in most of the numbers of nodes used the network with n 4 that was used in the statistical comparisons achieved very good performance outperforming hnn and rbfnn and losing to tsk only for c 10 the network would be useful in modelling and comprehending community dynamics and organisms niches and investigate adaptation chances under climate change 5 conclusions the study attempted to investigate the potential of using a novel chebyshev polynomial feedforward network to efficiently analyze and model a number of complex ecological environmental data which represent different research areas in environmental science the main functionality of the proposed network was to estimate the corresponding output in terms of weighted sum of chebyshev truncated polynomial series which were applied to linear combinations of the input variables the network s mathematical structure was analytically presented while its training process was accomplished by using differential evolution that lies in the realm of evolutionary computation algorithms the proposed method was compared against three other methods using the same input output data sets moreover thorough statistical analysis of the results showed that the proposed method performs better in most of the experimental cases than all other tested methods and thus it might be a flexible predictor across biological and ecological phenomena of various scales and different natural and or anthropogenic environments meaning that its performance should be regarded within a range of meaningful conditions i e combination of time scale resolution and organizational complexity our current work also showed that there are paths of future research in this interesting interdisciplinary field one of the future goals is to apply the present methodology in data sets that address some very specific combinations not considered here e g daily variations in attributes of organisms and populations it is the case of microbes and long living species it is the case of tree species and big mammals our second goal is to extend the current idea in neuro fuzzy networks utilizing orthogonal polynomials declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the anonymous referees for their effort to provide valuable comments on this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104663 
26058,this paper introduces a polynomial feedforward neural network based on chebyshev polynomials able to effectively model non linear and highly complex environmental data the data sets were cautiously selected from the fields of biology ecology climate and environmental management and economics as to represent a scientifically meaningful and consistent corpus of disparate domains of intensive focus and interest in current ecological environmental research covering issues related to body growth age biomass production energy efficiency consumption and ecology geographic extension the proposed network uses a number of layers to estimate the output in terms of a weighted sum of truncated chebyshev series expansions applied to linear combinations of the input variables and it is trained by the differential evolution algorithm its performance was compared to three neural networks first a polynomial feedforward network that uses hermite polynomials as activation function in the hidden nodes second a radial basis function neural network third a takagi sugeno kang neuro fuzzy network all the above networks were trained by evolutionary optimization algorithms the comparison was carried out by standard criteria such as the root mean square error and the mean absolute error moreover a non parametric kruskal wallis statistical test used to compare the median values of the root mean square errors between methods the main experimental outcomes are a the network s efficiency improves for higher polynomial orders b the statistical analysis suggests that the proposed network appears to be very competitive to the other three networks keywords polynomial neural networks chebyshev polynomials environmental modelling ecological data environmental data 1 introduction environmental data are related to highly non linear and complex processes rendering the model development a tough problem to cope with the difficulties arising from the complexity of such kind of data many researchers have employed neural network nn structures as alternative ways to carry out data analysis and modelling various environmental applications have demonstrated the determinant contribution ascribed to nns coad et al 2014 lein 2009 millie et al 2012 monteil et al 2004 rath et al 2017 shoji and kawakami 2006 a type of nn with very effective performance is the feedforward neural network fnn several variants of fnn have been used such as standard fnn with one hidden layer general regression neural networks grnn and multiple layer perceptron mlp the usage of grnns mlps and standard fnns in water quality assessment was investigated by schleiter et al 2001 and souza da costa et al 2009 deweber and wagner 2014 developed an ensemble of fnns to predict the mean daily water temperature in individual stream reaches li et al 2014 selected appropriate smoothing parameters for grnn and evaluated the resulting network in water resources applications while adamowski and chan 2011 employed a standard fnn to perform groundwater level forecasting papantoniou and kolokotsa 2016 and alimissis et al 2018 performed comparative analysis between different fnns as well as between ffns and more traditional approaches for the test case of the urban air quality prediction young et al 2011 discussed the utilization of mlp in modeling net ecosystem metabolism within a freshwater wetland tian et al 2017 used an input stabilized mlp to increase the prediction accuracy of chlorophyll dynamics of algal blooms santos et al 2014 used a standard fnn to extract patterns from soil chemical variables and microbial community structures in forest soils tracey et al 2011 employed an evolutionary computation based fnn to simulate the animals movement in their natural environment balsamà et al 2014 combined grnn with mlp to detect non linear behaviors shared by several chemical species where the data were taken from the emissions database for global atmospheric research mikulandric et al 2014 applied fnn in modeling the biomass gasification process in fixed bed gasifiers while king et al 2014 in detecting organic pollutants in microbial fuel cell based biosensing in this paper a polynomial nn pnn is introduced that establishes nonlinear local regression models in terms of truncated chebyshev series which possess strong approximation capabilities patra and kot 2002 lee and jeng 1998 rivlin 1974 the result is a novel approach regarding the use of nns in modelling highly nonlinear and complex environmental data although too much effort has been put on using typical nns in environmental data analysis there are relatively few studies that address the investigation of more sophisticated schemes such as pnns rigos et al 2014 2016a 2016b tomić et al 2018 tsekouras et al 2018 pnns are networks with polynomial node activation functions able to capture highly non linear and complex input output relationships ivakhnenko 1971 liu and wang 2012 maric 2013 purwar et al 2007 vukovic et al 2018 so far many different types of pnns have been implemented the first attempt to develop such kind of structures is credit to ivakhnenko 1971 who introduced the group method of data handling which was based on the well known kolmogorov gabor polynomials oh et al 2012 demonstrated the applicability of this type of polynomials in designing efficient polynomial radial basis functions neural networks rbfnn lee and jeng 1998 used tensor analysis in order to produce a unified model where the node activation functions were realized with the aid of chebyshev polynomials however as this configuration lies in the realm of functional nns its size grows fast with the number of input variables rendering the network hardly applicable in high dimensional problems later studies investigated different frameworks to embed chebyshev polynomials in fnns and rbfnns and applied them in nonlinear system identification and automated shoreline extraction rigos et al 2014 2016b other approaches have been associated with the incorporation of hermite and legendre polynomials in network s structure regarding various types of applications ma and khorasani 2005 mall and chakraverty 2016 rigos et al 2016a tsekouras et al 2018 a problem associated with most of the above approaches is that the inputs to different polynomial activation functions are different to one another meaning that the obtained series expansion is not exact ma and khorasani 2005 this fact might negatively affect the network s approximation capabilities because the polynomial representations might be compromised in this study we introduce a feedforward chebyshev pnn which generates nonlinear regression models for seven data sets the data sets come from public domains and they are related to high dimensional and complex processes chebyshev polynomials possess strong approximation capabilities and a number of properties that make them a very effective modelling tool in such kind of data andrews et al 2000 lee and jeng 1998 patra and kot 2002 rigos et al 2016b rivlin 1974 the network s structure encompasses a number of design phases that act in sequence the input variables are aggregated to produce a number of linear combinations these combinations are appropriately scaled i e normalized and enter the layer with the polynomial nodes contrary to the methods developed in ma and khorasani 2005 rigos et al 2016a where each node in polynomial layer encodes only one polynomial herein each node encodes a chebyshev polynomial series thus obtaining exact series expansions of the set of input variables tsekouras et al 2018 finally the network s output comes in the form of the weighted sum of the above mentioned series expansions the training process is based on using the differential evolution price et al 2005 to minimize the square error with respect to the network s design parameters the main advantage of the proposed algorithm is its ability to effectively model complex ecological environmental data using a relatively small number of nodes making feasible the use of higher order polynomials to capture the nonlinearity of the problem at hand thus producing more robust results the rest of the paper is organized as follows section 2 provides the mathematical description of the proposed pnn section 3 reports the data sets used section 4 focuses on the simulation experiments and discusses the corresponding results finally the paper concludes in section 5 2 the proposed chebyshev polynomial neural network 2 1 chebyshev polynomials since the proposed network utilizes chebyshev polynomials there are some properties characterizing their functionality which are presented below in general the nth order chebyshev polynomial is given as 1 p n x cos n ϕ i 0 n 2 1 i j i n 2 n 2 j j i x n 2 i where x cos ϕ and stands for the integer part i e floor function and the parameter i takes integer values between 0 and n 2 there are two categories of chebyshev polynomials namely polynomials of the first kind and polynomials of the second kind in this paper we employ chebyshev polynomials of the second kind which are generated by the subsequent recurrence relations rivlin 1974 2 p 0 x 1 3 p 1 x 2 x 4 p n 1 x 2 x p n x p n 1 x as an example using the above recurrence relations the resulting polynomials with orders n 0 1 2 3 and 4 are given below 5 p 0 x 1 p 1 x 2 x p 2 x 4 x 2 1 p 3 x 8 x 3 4 x p 4 x 16 x 4 12 x 2 1 this kind of chebyshev polynomials are orthogonal in 1 1 with respect to the weight 1 x 2 6 1 1 p n x p m x 1 x 2 d x 0 n m π 2 n m finally it can easily be proved that the nth order polynomial is written in the form rivlin 1974 7 p n x i 0 n 2 ψ n n 2 i x n 2 i with 8 ψ n n 2 i 1 i 2 n 2 i n i i where the parameter i has been defined previously 2 2 structure of the network let us assume that we are given a system with p inputs x x 1 x 2 x p t r p and one output y r the proposed feedforward neural network comprises four layers operating in sequence and its structure is illustrated in fig 1 a in view of fig 1 a layer 1 generates linear combinations of the input variables denoted as λ i x for i 1 2 c the network s design parameters related to this layer are the coefficients of the linear combinations symbolized as b i j i 1 2 c j 1 2 p in layer 2 the values λ i x are scaled in the interval 1 1 resulting in the normalized values λ i x with 0 λ i x 1 i as sated in the previous section the reason behind the scaling process is that the chebyshev polynomials are orthogonal in 1 1 and thus able to effectively operate only in that interval layer 3 is the hidden layer and its main functionality is to expand the normalized values λ i x coming from the previous layer into truncated chebyshev series of order n which are denoted as s i n i 1 2 c the general structure of the ith hidden node in that layer is depicted in fig 1 b where its output is the ith truncated chebyshev series s i n based on this figure the network s design parameters related to this layer are the coefficients μ i ℓ i 1 2 c ℓ 0 1 n that are multiplied with the chebyshev polynomials p ℓ ℓ 0 1 n finally layer 4 linearly combines the resulting series expansions and generates the network s output the network s design parameters related to this layer are the coefficients z i i 1 2 c that are multiplied with the truncated chebyshev series s i n i 1 2 c the above layers are analytically described in the subsequent paragraphs layer 1 includes c summation nodes each of which generates linear combinations of the input variables 9 λ i x j 1 p b i j x j where i 1 2 c and b i j are the design parameters of network related to this layer see fig 1 a note that λ i x r i layer 2 performs a scaling process to map λ i x into λ i x 1 1 i let us assume that λ i x φ min φ max then the objective is to map the interval φ min φ max into 1 1 as follows 10 λ i x r λ i x g r j 1 p b i j x j g with 11 r 2 φ max φ min and g φ max φ min φ max φ min thus λ i x 1 1 which is a linear combination of the input variables x 1 x 2 x p also fig 2 describes the above scaling transformation what remains to be done is to calculate the interval φ min φ max let us assume that b i j β min β max and x j x j min x j max for j 1 2 p then b i j x j φ j min φ j max where φ j min φ j max β min β max x j min x j max i e φ j min φ j max is calculated by the standard multiplication of the intervals β min β max and x j min x j max moore 1966 to this end using the above result and the definition of λ i x in eq 9 we get 12 φ min j 1 p φ j min λ i x j 1 p φ j max φ max in this paper the parameters β min and β max are predefined by a trial and error approach layer 3 generates the nth order truncated chebyshev series for each λ i x 13 s i n ℓ 0 n μ i ℓ p ℓ λ i x where μ i ℓ r i 1 2 c ℓ 0 1 n are network s design parameters related to layer 3 see fig 1 b finally layer 4 combines the above mentioned truncated series in order to estimate the network s output 14 y ˆ i 1 c z i s i n i 1 c ℓ 0 n z i μ i ℓ p ℓ λ i x where z i 1 i c are network s design parameters related to layer 4 see fig 1 a using 9 eq 14 is modified as to include all the network s design parameters 15 y ˆ i 1 c ℓ 0 n z i μ i ℓ p ℓ r j 1 p b i j x j g 2 3 training of the network let us assume that we are given a set of n input output data pairs x k y k k 1 n with x k r p and y k r the problem addressed is to minimize the next square error with respect to the network s design parameters b i j μ i ℓ and z i for i 1 2 c j 1 2 p and ℓ 0 1 n 16 j s e k 1 n y k y ˆ k 2 k 1 n y k i 1 c ℓ 0 n z i μ i ℓ p ℓ r j 1 p b i j x k j g 2 to minimize the above mentioned square error we use differential evolution de which is a population based evolutionary algorithm price et al 2005 the reason behind this choice comes from several figurative advantages provided by this algorithm lin et al 2018 price et al 2005 subudhia and jena 2011 first de is a derivative free optimization mechanism that does not use any gradient descent procedure therefore it can be easily applied to highly non linear optimization problems second it does not depend on the random initialization a fact that ensures the convergence to an optimal solution third it is less vulnerable to get trapped in local minima because it performs a parallel search in the feature space fourth contrary to other heuristic algorithms it overcomes the problem of premature convergence by maintaining a balance between the local and the global exploration of the searching space finally due to its stochastic nature it can exhibit flexible and robust optimization behavior de involves a population of m individuals d t d t 1 d t 2 d t q t r q that search the space for an optimal solution note that t 1 2 m and q is the dimension of the individuals search space de employs three evolving phases mutation crossover and selection in this paper the learning rule in the mutation phase is 17 δ t d b e s t f d t 1 d t 2 where δ t δ t 1 δ t 2 δ t q t is the mutation vector d t 1 and d t 2 two distinct individuals different from d t d b e s t is the position of the best solution found so far and f 0 1 the crossover phase is typically executed by the next rule 18 v t s δ t s i f γ c r s s 0 d t s o t h e r w i s e where s 1 2 q ν t v t 1 v t 2 v t q t is a trial vector γ is a random number in 0 1 i e γ u 0 1 c r 0 1 is the crossover factor and s 0 is a random integer in 1 2 q to this end the selection phase applies a greedy selection between ν t and d t to decide which of those will be transferred to the next learning cycle the above phases are implemented for a maximum number of learning cycles denoted as t max the search space is defined by the number of the network s design parameters i e b i j μ i ℓ and z i i 1 2 c j 1 2 p ℓ 0 1 n and its dimension is equal to q c p n 2 fig 3 shows the structure of each individual involved in the learning process finally the parameters b i j 1 i c 1 j p correspond to the first c p elements of each individual and their domain of values is the interval β min β max therefore in each iteration after the calculation of vector ν t in 18 the following correction takes place 19 v t s β min v t s β min v t s β max v t s β max f o r t 1 2 m a n d s 1 2 c p implementing 19 ensures that the parameters b i j 1 i c 1 j p will always lie in the interval β min β max and therefore λ i x will always be confined in 1 1 3 description of data sets seven public domain and highly cited datasets are studied which were taken from disparate domains of interest in the current ecological environmental research table 1 reports the basic properties of the data sets which are grouped into four main categories namely body growth age biomass production energy efficiency consumption and ecosystem geographic extension the abalone data set concerns the prediction of the age of a member of the abalone species from physical measurements and it includes 8 input variables related to the size length diameter height and the weight whole shucked viscera and shell waugh 1995 here the first input variable was removed because it was a trichotomous categorical variable and obtained a system with 7 inputs and one output source uci machine learning repository https archive ics uci edu ml index php the data set fish abundance knouft and anthony 2018 is used for two different species of fishes namely campostoma anomalum commonly known as central stoneroller and cyprinella spiloptera known as spotfin shiner both are from the genus cyprinella and are freshwater fishes endemic to the eastern central and midwestern u s they are not considered to be in danger so they are not under special conservation protection the input variables used in predicting their population were the longitude and latitude of the sample the annual mean temperature the mean diurnal temperature range the temperature seasonality the minimum and maximum temperatures measured the annual range of the temperature the total annual precipitation the precipitation during the wettest and driest month and the precipitation seasonality while the predicted output is the population of the species source http rsos royalsocietypublishing org content 3 6 160093 the energy efficiency example deals with the prediction of energy efficiency with respect to twelve different building shapes tsanas and xifara 2012 the buildings differ with respect to the relative compactness surface area wall area roof area overall height orientation glazing area and glazing area distribution the output variable is the heating load of a building the data set was taken from the uci machine learning repository the daily average price of electricity energy dee refers to the prediction of daily average price of tkwhe electricity energy for the year 2003 in spain coming from hydroelectric nuclear electric carbon fuel natural gas and other special sources of energy source keel software https sci2s ugr es keel alcalá fdez et al 2009 the data set forest fires aims to predict the burned area of forest fires in the northeast region of portugal by using meteorological and other data cortez and morais 2007 there are 12 input variables and one output i e the burned area which was post processed with the ln x 1 transform the data were taken from the uci machine learning repository finally the data set ground temperature is used to predict current species distributions and future changes through ground temperatures modelled at high resolutions across european landscapes in particular it includes recorded soil temperatures for two sets of sites in europe from two different periods gunton et al 2015 the data were taken from the dryad digital repository https datadryad org resource doi 10 5061 dryad fr075 1 the overall epistemic rationale for the selection of testing datasets is presented in fig 4 based on this figure the datasets cover a large array of ever evolving conceptually but fundamentally intriguing environmental research questions i e 1 developmental biology and allometry of body growth the exemplary case of haliotis rubra vernacular name abalone 2 eco physiology of population community level determinants of freshwater fish abundance in climatic eco regions 3 high resolution geo physical variables determining individual species extension after climate forcing such as ground temperature 4 intensity of multi determined extreme catastrophic events such as wildland fires 5 technological dimensions of indoor housing micro climate conditions relating to or modulated by energy consumption and 6 economic dimension or pricing of electricity energy daily in addition they encompass two important multi layered and multi faceted scales of interest the first relates to time scale the categories of datasets are arranged across a time dependent gradient of ecological processes from daily variations to multi annual fluctuations of dependent variables the second relates to the complexity scale of the studied systems inherent adaptive responses of single biological ecological entity vs higher order complex anthropogenic systems such as household electricity consumption finally as shown in fig 4 the selected data sets do not address some very specific combinations e g daily variations in attributes of organisms and populations it is the case of microbes and long living species it is the case of tree species and big mammals however the relevant issues to 1 body growth age 2 biomass production 3 energy efficiency consumption and 4 ecosystem geographic extension are accurately and properly addressed 4 simulation results and discussion in this section we apply the proposed network to the seven case studies described in section 3 as mentioned previously to implement the scaling process the parameters β min and β max that define the domain of values of the network s design parameters b i j i 1 2 c j 1 2 p were selected by a trial and error as to obtain the best possible performance their values for each data set are depicted in table 2 to implement the differential evolution we used a moderate population size equal to m 20 this selection is expected to provide a fast learning process while maintaining effective optimization results price et al 2005 the maximum number of learning cycles was selected as t max 500 which will provide enough time to the de to converge on the other hand for each data set the values of parameters f and c r see eqs 17 and 18 were prefixed within the interval 0 1 lin et al 2018 price et al 2005 subudhia and jena 2011 table 2 presents the values selected for each data set to validate the performance of the proposed pnn its results were compared with the respective results obtained by three other networks first the hermite neural network hnn developed by ma and khorasani 2005 which is a feedforward network encompassing one hidden layer of c nodes the activation function of the ith node is the hermite polynomial with order equal to i 1 i c the input layer creates c linear combinations of the input variables using a set of optimized weights the ith linear combination enters the node with activation function the ith order hermite polynomial then the network s output is calculated as the weighted sum of the above polynomials in this paper all weight parameters of the hnn are optimized by minimizing the square error using the particle swarm optimization pso algorithm kennedy and eberhart 2001 pso involves a swarm of m particles p i r q each of which is assigned a velocity v i r q the position p i b e s t is the best solution obtained so far by the particle p i and p b e s t is the best solution obtained so far by all particles then the velocity is calculated as 20 v i t 1 η v i t χ 1 φ 1 p i b e s t t p i t χ 2 φ 2 p b e s t t p i t where χ 1 χ 2 are randomly selected in 0 1 η φ 1 and φ 2 are positive constant numbers called the inertia cognitive and social parameter respectively finally the position of each particle is 21 p i t 1 p i t v i t 1 herein each particle encodes all the design parameters of hnn to conduct a fair comparison the population size and maximum number of learning cycles were selected to be the same to the respective parameters used in the proposed method m 20 and t max 500 respectively the rest of the design parameters of pso were assigned the typical values commonly used in the literature cao et al 2018 chen et al 2018 kennedy and eberhart 2001 i e η randomly selected in the interval 0 5 1 and φ 1 φ 2 2 the second network is a radial basis functions neural network rbfnn where the hidden layer includes c nodes with gaussian type activation functions this network is trained by the de algorithms the parameter setting of which is the same to the parameter setting used by the proposed method finally the third network is a typical takagi sugeno kang tsk neuro fuzzy network which employs gaussian type membership functions this network is optimized in terms of the pso algorithm with the same parameter setting as described in the case of hnn to carry out the comparison study two performance indices were used as reliable indicators of the networks performances namely the root mean square error rmse and the mean absolute error mae 22 rmse k 1 n y k y ˆ k 2 n 23 mae k 1 n y k y ˆ k n to conduct the experiments each data set was randomly divided into a training subset containing the 60 of the original data and a testing subset consisting of the rest 40 all networks were run for c 2 4 6 8 and 10 nodes in addition the proposed network was run for truncated chebyshev series s i n with orders n 2 3 and 4 for each method number of nodes and polynomial order 20 random initializations were considered where the first 10 were used to measure the rmse performance index and the last 10 to measure the mae performance index tables 3 and 4 depict the rmse mean values along with the respective standard deviations while tables 5 and 6 illustrate the respective results for the mae index in those tables the best results for each number of nodes in both the training and testing data are indicated by the bold fonts it can be easily seen that apart from few cases in the fish abundance campostoma building energy efficiency and ground temperature data sets the proposed method obtained better results for both the rmse and mae performance indices to further assess the results a rigorous non parametric statistical comparison between the proposed network for n 4 and the other networks takes place the comparison is realized in respect to the testing data and values for the number of nodes as c 2 4 6 8 and 10 figs 5 and 6 illustrate the boxplots of the descriptive statistics simulation results by method and number of nodes for each dataset nonparametric statistical inference was performed in order to compare median values of rmse index between methods an overall 5 significance level was maintained for comparisons within each dataset a kruskal wallis test was performed to detect statistically significant differences in median between the four methods studied for each level of nodes separately the bonferroni adjusted overall p values for comparing the methods are given in table 7 the column of p values follow up testing consisted of performing mann whitney tests to compare our method to the remaining three separately for each level of number of nodes again maintaining an overall 5 significance level within each dataset by using adjusted p values the adjusted p values are shown in table 7 also we note here that since random initializations were used for each method number of nodes polynomial order combination our nonparametric statistical analysis was based on independent samples thus the use of the sign and the wilcoxon s signed rank test was not warranted in our application to test equal predictive accuracy of the methods diebold and mariano 1995 for an interesting forecasting application where the sign and the wilcoxon s signed rank test are appropriate we refer the interested reader to tzanis et al 2019 by considering solely the comparisons with associated adjusted p values less than 0 05 we conclude that statistically significant differences between methods for all values of c were detected in all data sets of the organizational levels of body growth age biomass production and energy efficiency consumption the same holds for the ground temperature data set of the ecosystem geographic extension level however as far as the forest fires example is concerned statistically significant differences between methods were detected only for c 4 6 and 8 more specifically the proposed chebyshev pnn clearly outperformed the other three methods for all values of c in the cases of the abalone fish abundance cyprinella and daily average price of electricity energy data sets in fish abundance campostoma dataset it was superior to hnn and tsk for c 2 6 8 and 10 when compared to rbfnn however no statistically significant differences were observed for the building energy efficiency dataset it obtained better approximation behavior when compared to the hnn for all values of c while outperforming the rbfnn for c 2 and the tsk for c 2 4 only regarding the forest fires dataset it achieved more efficient performance when compared to the rbfnn and tsk only in the case where c 6 finally in the ground temperature dataset it was superior to rbfnn for all values of c to the hnn for c 2 4 6 and to tsk for c 10 the application of the proposed pnn on the selected datasets as indicative examples highlighted its performance and value validity as a general predictor on a series of core issues of modern environmental ecological science policy and decision making for instance the accurate prediction of the age of specimens from physical measurements is capital for several conservation biology applications e g holmes 2004 such as population viability analysis fisheries management or dendrochronology and reconstruction of past climatic conditions this is the example of abalone data set in view of the above analysis the proposed pnn obtained the best performance and therefore it has the potential of greatly assisting the above mentioned research areas changes in territorial and geographic extension of individual species and species assemblages combined with relative abundance rarefaction is a priority for global change biology projecting such trends from climatic variables under various global change scenarios allows for better understanding future biodiversity conditions kocsis and hufnagel 2011 sala et al 2000 this is the case of the fish abundance data set in the case of campostoma species there is a clear dominance credit to the polynomial order n 2 however even when using n 4 in the statistical analysis the proposed pnn outperformed the other networks as far as the cyprinella species is concerned the most appropriate polynomial order seems to be n 4 since the influence of habitat quality on population has been well studied sala et al 2000 and taking the network s performance into account its ability to predict the effects of climate conditions especially in the era of increasing climate change effects will be useful in making conservation efforts more targeted and effected as episodes of extreme climatic conditions become more frequent due to climate change and the urban wildland interface expands worldwide occurrence of natural hazards such as forest fires increase causing severe ecosystemic losses and economic burdens upon affected communities and civil protection mechanisms e g cortez and morais 2007 the simulation results of the forest fires example suggest that the proposed network can be efficiently used in improving fire behavior prediction which is of outmost importance especially when estimates of the potential burned area using mostly meteorological data is concerned on the other hand home level energy demand for heating determines significantly the energy consumption profile of an agglomeration which refers to the energy efficiency example the daily average price of electricity energy example focuses on that subject i e the pricing of energy supply and drives both energy governance decisions and individual behavior of household consumers gyamfi et al 2013 in the former example the network appears to yield the best results for n 3 while in the latter for n 4 as these examples refer to strategies concerning the determination of the energy mix at an institutional level and the consumption attitude of individual consumers the usage of the network appears to be of major significance finally the wide variation and fast fluctuation of ambient temperatures especially over short distances impose certain challenges in revealing species under changing climates and therefore to accurately assess species distributions temperatures must be modelled at high resolutions gunton et al 2015 this is the case of ground temperature example the performance of the proposed network seems to imply that the most appropriate polynomial order to model the data is n 3 which obtained the best average rmses in most of the numbers of nodes used the network with n 4 that was used in the statistical comparisons achieved very good performance outperforming hnn and rbfnn and losing to tsk only for c 10 the network would be useful in modelling and comprehending community dynamics and organisms niches and investigate adaptation chances under climate change 5 conclusions the study attempted to investigate the potential of using a novel chebyshev polynomial feedforward network to efficiently analyze and model a number of complex ecological environmental data which represent different research areas in environmental science the main functionality of the proposed network was to estimate the corresponding output in terms of weighted sum of chebyshev truncated polynomial series which were applied to linear combinations of the input variables the network s mathematical structure was analytically presented while its training process was accomplished by using differential evolution that lies in the realm of evolutionary computation algorithms the proposed method was compared against three other methods using the same input output data sets moreover thorough statistical analysis of the results showed that the proposed method performs better in most of the experimental cases than all other tested methods and thus it might be a flexible predictor across biological and ecological phenomena of various scales and different natural and or anthropogenic environments meaning that its performance should be regarded within a range of meaningful conditions i e combination of time scale resolution and organizational complexity our current work also showed that there are paths of future research in this interesting interdisciplinary field one of the future goals is to apply the present methodology in data sets that address some very specific combinations not considered here e g daily variations in attributes of organisms and populations it is the case of microbes and long living species it is the case of tree species and big mammals our second goal is to extend the current idea in neuro fuzzy networks utilizing orthogonal polynomials declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the anonymous referees for their effort to provide valuable comments on this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104663 
26059,automated and accurate wetland identification algorithms are increasingly important for wetland conservation and environmental planning deep learning for wetland identification is an emerging field that shows promise for advancing these efforts deep learning is unique to traditional machine learning techniques for its ability to consider the spatial context of object characteristics within a landscape scene however applying deep learning typically requires very large datasets for training the algorithms which limits their application for many environmental applications including wetland identification using four study sites across virginia with field delineated wetlands we provide insight into the potential for deep learning for wetland detection from limited but typical wetland delineation training data our proposed workflow performs a wetland semantic segmentation using deepnets a deep learning architecture for remote sensing data and an input dataset consisting of high resolution topographic indices and the normalized difference vegetation index results show that models trained and evaluated for a single site were able to achieve high accuracy up to 91 recall and 56 precision and similar accuracy can be obtained for models trained across multiple sites up to 91 recall and 57 precision through this analysis we found that across all sites input data configurations taking advantage of hydrologic properties derived from elevation data consistently outperformed models using the elevation data directly showing the benefit of physically informed inputs in deep learning training for wetland identification by refining the wetland identification workflow presented in this paper and collecting additional training data across landscapes there is potential for deep learning algorithms to support a range wetland conservation efforts 1 introduction wetlands are important ecosystems that are threatened by development climate change and pollution klemas 2011 wetland loss is both a global davidson 2014 and national problem as half of the wetlands of the conterminous u s have been lost since 1600 dahl et al 1991 in the u s federal regulations such as section 404 of the clean water act play an important role in wetland protection laws require environmental impact assessments prior to land development and water resources projects which entails the creation of detailed wetland surveys page and wilcher 1990 conducting these surveys with the level of spatial resolution and accuracy needed to abide by federal regulations and meet the goal of avoiding adverse impact to wetlands can be time consuming and costly to support these efforts methods for more rapidly identifying wetland locations are needed although manual surveys will continue to be the most accurate method to map wetlands there is potential for supporting these efforts by using machine learning approaches including deep learning to identify wetland features at varying scales guo et al 2017 lang et al 2013 lang and mccarty 2014 despite the many types of protected wetlands that exist all wetlands can be identified by common features these include the presence of hydrologic conditions that inundate the area vegetation adapted for life in saturated soil conditions and hydric soils us corps of engineers 1987 researchers have demonstrated the ability to detect these features from multispectral imagery radar and light detection and ranging lidar data guo et al 2017 multispectral imagery are the most commonly applied data in wetland studies guo et al 2017 klemas 2011 however spectral variables alone may be unable to distinguish wetlands due to spectral confusions from reflectance and backscattering dronova 2015 kim et al 2011 lidar data are well suited to complement multispectral analyses due to its wide and growing availability and demonstrated benefit to wetland mapping guo et al 2017 klemas 2011 kloiber et al 2015 lang and mccarty 2014 snyder and lang 2012 lidar returns can be interpolated to create high resolution digital elevation models dems from which wetland indicators based on flow convergence and near surface soil moisture can be derived lang et al 2013 lang and mccarty 2014 millard and richardson 2013 2015 o neil et al 2018 2019 moreover researchers have shown the benefit of lidar dem metrics as input variables to traditional machine learning techniques such as random forests for wetland mapping and classification e g deng et al 2017 kloiber et al 2015 millard and richardson 2013 millard and richardson 2015 o neil et al 2018 2019 zhu and pierskalla 2016 the successful coupling of lidar and multispectral imagery with traditional machine learning techniques for wetland identification is well documented however deep learning for remote sensing studies including wetland identification is a new application space ma et al 2017 zhang et al 2016 that shows promise for fulfilling the unmet need for wetland inventory creation deep learning architectures are modeled after the architecture of the mammal brain serre et al 2007 where inputs are perceived and processed through multiple layers of abstraction convolutional neural networks cnns lecun et al 1998 are a representative form of deep learning that is used for visual recognition cnns utilize the spatial context of detected features to identify objects and classify scenes the distinguishing element of cnn architectures are the convolutional layers which convolve spatial filters over input images to identify patterns that are characteristic of target classes deep convolutional neural networks dcnns he et al 2016 krizhevsky et al 2017 simonyan and zisserman 2014 and fully convolutional neural networks fcns long et al 2015 are extensions of the cnn framework that can output dense pixel wise classifications within images i e semantic segmentation where each pixel of the input image is assigned a class since the formalization of the concept in 2006 hinton et al 2006 deep learning has advanced the fields of speech recognition medical diagnosis and autonomous driving applications and has since motivated new applications in environmental and water resources management liu et al 2018 pan et al 2019 shen 2018 zhang et al 2016 researchers have shown the ability of dcnns fcns and other cnn extensions to delineate urban and natural landscape classes using multispectral imagery and topographic data audebert et al 2017 2018 multispectral imagery and lidar point clouds xu et al 2018 and multispectral imagery alone hu et al 2018 kemker et al 2018b kemker et al 2018a scott et al 2017 few researchers have applied dcnns and fcns specifically to wetland classification these include liu et al 2018 who applied orthoimagery and elevation information to deep learning models for wetland segmentation in addition rezaee et al 2018 used multispectral imagery in a wetland deep learning model and posited that predictions would improve with the incorporation of physical information from radar or lidar sources the typical need for massive validation sets to train deep learning models is a significant deterrent to environmental and water resources researchers shen 2018 zhang et al 2016 as reliable training data is often lacking in these applications this issue is especially prevalent for wetland identification that is intended to inform conservation and permitting efforts where training data for computational models are ideally manually derived and confirmed by regulatory entities the effects of training data limits for wetland semantic segmentation have been investigated by liu et al 2018 where comparisons were drawn for a single study area using dcnns fcns random forests and support vector machines with privately contracted aerial imagery and surface elevation information as input features while this is an important stride in gaining insight into the training data needs for deep learning of wetlands an analysis has yet to be done that utilizes freely available data and is completed over multiple geographic regions the growing research area of deep learning for remote sensing applications shows promise for advancing wetland mapping although researchers have begun to show the potential for wetland identification at a high resolution using deep learning approaches research gaps remain specifically analyses are needed to identify the deep learning performance potential for different geographic regions when limited to relatively small quantities of verification data and freely available input data which are typical in practice we aim to contribute to this field by presenting a novel wetland identification methodology that implements a basic semantic segmentation architecture and is generalizable because it leverages freely available geospatial and remote sensing data our input data configuration consists of lidar dem derivatives that describe geomorphologic and hydrologic contributors to wetland formation as well as a commonly used vegetative index using four study sites across virginia we build and evaluate several wetland models to demonstrate the potential for wetland semantic segmentation given typical training data resources through this research we seek to answer the following questions i across geographically distinct study sites what wetland prediction accuracy is achievable by building site specific models from typically available amounts of wetland delineation training data ii what is the potential for a single combined site model trained using data from across geographic regions to predict wetlands at each individual site 2 methodology 2 1 study areas four study areas across virginia usa are used in this analysis fig 1 a data for each study area include the extents of wetland surveys and the surrounding hydrologic unit code huc 12 watershed usgs 2019 fig 1b the huc 12 watersheds were used as processing extents and surveyed areas provided the validation data also referred to as the study sites the study areas span four level iii ecoregions as shown in table 1 the sites also vary by size land cover and topographic characteristics notable differences include the higher rate of development in sites 1 and 2 and the mild topography of site 4 in addition wetlands are much more abundant in site 4 where the wetland to nonwetland ratio is 0 42 compared to less than 0 1 in the other sites note that all surveyed wetland types were merged into a single wetland category prior to use as verification data 2 2 input data this study used publicly available lidar dems national agriculture imagery program naip aerial imagery and field mapped wetland surveys lidar dems were obtained from the virginia information technologies agency vita vita 2016 as hydro flattened bare earth dems the lidar data used were collected and processed between 2010 and 2015 and have horizontal resolutions ranging from 0 76 m to 1 5 m naip imagery are provided by the united sates department of agriculture farm service agency 2017 naip imagery were used to derive the ndvi naip imagery contain four spectral bands red green blue and near infrared at a 1 m spatial resolution imagery used in this study were collected near the dates of wetland surveying and images were resampled to match the resolution of the lidar dems if necessary wetland delineations and survey limits were provided by the virginia department of transportation vdot in polygon vector format and served as validation data for this study all verification wetlands were manually surveyed during summer months may august between 2013 and 2016 by professional wetland scientists in compliance with transportation planning permitting wetland delineations for sites 2 3 and 4 were also jurisdictionally confirmed by the us army corps of engineers usace binary wetland nonwetland geotiffs were created from these data with resolutions matching those of the site lidar dems visual analyses of google earth images showed that the study site landscapes changed minimally between lidar acquisition and wetland delineation timeframes 2 3 wetland identification method the wetland identification method consists of three main parts preprocessing feature creation and semantic segmentation and accuracy assessment fig 2 input data required include high resolution dem data four band aerial imagery and validated wetland nonwetland distribution data all in geotiff format from these data topographic indices curvature topographic wetness index and cartographic depth to water index and the normalized difference vegetation index are calculated these input features are merged into a single four band composite grid smaller image tiles are created from the composite grid and validation data and the pairs of corresponding image tiles are randomly separated into training and testing datasets finally dense pixel wise wetland predictions are made using a deep learning architecture created for remote sensing data deepnets for earth observation audebert et al 2018 and the accuracy of wetland predictions is assessed the main outputs are geotiff wetland predictions for each image tile and an accuracy report for the entire validation data area the method was implemented using open source python libraries and is available under an mit license see software availability section 2 4 preprocessing dem preprocessing was necessary to create an improved land surface representation from which to calculate indicators of wetland geomorphology first dem smoothing is performed which is necessary to addresses microtopographic noise microtopographic noise is common in high resolution dems and can be representative of either erroneous data or true variations in the elevation of vegetated surfaces jyotsna and haff 1997 dem conditioning is then executed which is necessary prior to modeling hydrologic flow paths as it addresses topographic depressions jenson and domingue 1988 o callaghan and mark 1984 topographic depressions interfere with overland flow path modeling by creating discontinuities in flow paths and accumulating water which negatively influences modeled watershed processes grimaldi et al 2007 lindsay 2016 lindsay and creed 2005 dem conditioning is particularly important for hydrologic modeling from high resolution dems as researchers have found that sensitivity of hydrologic parameter extraction to conditioning technique increases significantly with dem resolution woodrow et al 2016 although many techniques have been proposed for both dem smoothing and conditioning we apply the perona malik smoothing and a least cost path conditioning this preprocessing combination was found to considerably improve wetland identification for the study sites in a prior study see o neil et al 2019 the perona malik filter perona and malik 1990 performs a nonlinear anisotropic diffusion that preserves feature edges by penalizing smoothing across estimated feature boundaries passalacqua et al 2010a 2010b perona malik smoothing was implemented using code from the nonlinear filtering module from pygeonet an open source software for automatic channel network extraction from dems passalacqua et al 2010a sangireddy et al 2016 the a least cost path algorithm hart et al 1968 determines the least cost drainage paths through unaltered terrain and out of sinks thus avoiding unnecessary modification of the input dem metz et al 2011 the a conditioning method was executed using the grass gis r watershed module grass development team 2017 metz et al 2011 2 4 1 feature creation 2 4 1 1 topographic features in a prior study we concluded that the curvature topographic wetness index twi and cartographic depth to water index dtw are successful topographic metrics for wetland identification for our study sites o neil et al 2018 2019 curvature of a surface can describe the degree of convergence and acceleration of flow moore et al 1991 and studies have shown its capability to indicate saturated and channelized areas ågren et al 2014 hogg and todd 2007 kloiber et al 2015 millard and richardson 2015 o neil et al 2018 2019 sangireddy et al 2016 here we use laplacian curvature defined as the second derivative of the elevation grid laplacian curvature has been shown to favor the extraction of natural channels rather than artificial drainage paths and to more effectively identify channels in flat developed landscapes compared to alternative curvature forms passalacqua et al 2012 thus we found the laplacian curvature to be most suitable for our study areas which all encompass corridor projects and are partially developed o neil et al 2019 the curvature grid is created from the smoothed dem using code adopted from pygeonet passalacqua et al 2010a pygeonet 2019 sangireddy et al 2016 the ability of the twi to indicate saturated areas is well documented in the literature ågren et al 2014 lang et al 2013 millard and richardson 2015 murphy et al 2009 o neil et al 2018 2019 the twi relates the potential for an area to accumulate water to its tendency to drain water defined as 1 t w i ln α tan β where α is the specific catchment area contributing area per unit contour length and tan β is the local slope beven and kirkby 1979 the twi was created from the smoothed conditioned dem using the r watershed program of grass gis this module calculates the α term using the multiple flow direction algorithm holmgren 1994 and the β term using a grass gis calculated slope researchers have demonstrated the capability of the dtw to capture saturated areas as well murphy et al 2007 2009 2011 o neil et al 2018 2019 oltean et al 2016 white et al 2012 the dtw assumes that the likelihood for soil to be saturated increases with its proximity to surface water in terms of distance and elevation murphy et al 2007 calculated on a per pixel basis the dtw is defined as 2 d t w m d z i d x i a x p where d z d x is the downward slope of pixel i along the least cost i e slope path to the nearest surface water pixel a is a factor accounting for flow moving parallel or diagonal across pixel boundaries and x p is the pixel resolution murphy et al 2007 inputs required to calculate the dtw include a slope grid representing cost and a surface water grid representing the source from which distance is calculated we create the surface water grid directly from the lidar dem using pygeonet which performs a statistical analysis of curvature and uses geodesic minimization principles to predict stream lines passalacqua et al 2010a sangireddy et al 2016 visual analyses showed that streams created by pygeonet better aligned with aerial imagery compared to national hydrography data i e nhd streams and streams generated from the flow initiation threshold method band 1986 o callaghan and mark 1984 tarboton 1991 that is commonly used pygeonet was executed using parameters suggested for engineered landscapes see sangireddy et al 2016 which was found to produce accurate results across all sites in prior wetland model development o neil et al 2019 the pygeonet streams and slope grid were used as inputs to the grass gis r cost module grass development team 2017 to create the dtw grid 2 4 1 2 ndvi the ndvi is a commonly used spectral index that relates plant biomass and stress and separates wet versus dry areas klemas 2011 ozesmi and bauer 2002 researchers have used the ndvi as a wetland indicator in traditional machine learning frameworks corcoran et al 2013 dronova 2015 dronova et al 2011 guo et al 2017 mui et al 2015 rampi et al 2014 tian et al 2016 as well as for general land cover classifications using deep learning frameworks audebert et al 2017 2018 lee et al 2019 xu et al 2018 the ndvi utilizes the red and the near infrared bands carlson and riziley 1997 defined as 3 n d v i i n f r a r e d r e d i n f r a r e d r e d the red band indicates surface layer chlorophyll and therefore surface conditions of plants and the near infrared band is reflected from the inner leaf cell structure indicating the abundance of plant tissue klemas 2011 to calculate the ndvi eq 3 was executed using numpy operations and the appropriate naip imagery bands 2 4 1 3 image dataset creation the image dataset creation produces two sets of image tiles i feature tiles representative of the composite grid of input features and ii validation tiles representative of ground truth wetland and nonwetland locations due to the irregular shapes of the field surveys nodata pixels existed within the rectangular extent of the validation data rather than reduce our validation data to an extent without unverified area nodata pixels were treated as an additional target landscape class thus all pixels in the validation data were categorized as nodata 0 nonwetland 1 or wetland 2 as a first step in the image dataset creation process to build the dataset of feature tiles each band of the composite grid is rescaled to a range of 0 1 per the requirements of the deepnets algorithm rescaling the ndvi band was nontrivial as these values have global minimum and maximum of 1 and 1 conversely the range of values for each of the topographic features depends on the landscape they are calculated from therefore it was necessary to assume global minimum and maximum values the range of each topographic input was analyzed across the study sites and global minimum and maximum values that encompassed roughly 90 of the values were chosen note that only global maximum values had to be assumed for the twi and dtw which both have global lower bounds of 0 or nearly 0 although this step generalizes portions of the study areas this occurs only where there are extreme topographic features that occur infrequently in addition by limiting the range applied to each topographic input feature rather than choosing extreme but encompassing values the significance of the relative distance between values is minimally affected the minimum and maximum values used to rescale topographic features and the ndvi to a range of 0 1 are shown in table 2 following these steps the categorized validation grid and scaled composite grid were each separated into image tiles of size 320 x 320 pixels we chose the 320 pixel size constraint to balance the desire to use image tiles large enough to depict heterogeneous landscapes and the need to separate the study site into enough images to sample training and testing tiles that were randomly dispersed feature and labeled image tiles sets were not considered for either training or testing if more than 80 of the area was populated with nodata pixels 2 4 2 semantic segmentation model deepnets for earth observation our model performs a semantic segmentation of input images where each pixel of an input image is labeled as either nodata nonwetland or wetland that is a trained semantic segmentation model will assign a class prediction to each pixel in an image however different instances of target class objects are not defined i e instance segmentation as an initial step in developing a deep learning wetland model the current work is intended to demonstrate the suitability of a cnn to identify planning scale wetlands in the landscape we implemented a multimodal deep network deepnets for earth observation for semantic segmentation classification audebert et al 2017 deepnets has emerged as a state of the art tool for segmentation of high resolution remote sensing data demir et al 2018 and has been implemented and validated for automating segmentation of remote sensing data audebert et al 2016 2017 2018 although deepnets was chosen as a vehicle to address the guiding research questions of this work it is among several deep learning architecture currently achieving competitively in semantic segmentation of satellite imagery ghosh et al 2018 applied a stacked u nets architecture to achieve high quality satellite imagery segmentation with relatively few prediction parameters volpi and tuia 2016 use a cnn to segment very high resolution imagery to achieve f1 scores of about 85 marmanis et al 2018 propose a downsample upsample and achieve similar results while each of these approaches are likely to achieve good results with wetlands segmentation deepnets achieved slightly higher results on segmentation of benchmark imagery datasets demir et al 2018 thus it was adopted for this study an important future step in progressing this research would be to perform a comparative analysis of other emerging deep learning techniques for wetland segmentation as a starting point in the development of our deep learning wetland model the baseline deepnets architecture is implemented here audebert et al 2018 2019 deepnets builds on the segnet architecture badrinarayanan et al 2017 and is implemented using pytorch paszke et al 2017 segnet produces predictions with the same resolution as the input image by using an encoder decoder structure making it well suited for classification of landscape objects from georeferenced images audebert et al 2018 badrinarayanan et al 2017 the encoder portion of segnet is based on the convolutional layers of vgg 16 simonyan and zisserman 2014 and consists of convolutional layers batch normalization a rectified linear unit and max pooling as shown in the inset image defined by audebert et al 2018 in fig 2 the decoder is structurally symmetrical to the encoder pooling layers are replaced with unpooling layers that relocate pixel activations from the smaller feature maps to corresponding indices of zero padded upsampled images convolution blocks are then used to densify the sparse pixel activations this sequence of unpooling and convolutions is repeated until feature maps reach the original spatial resolution following this a softmax layer is used to compute multinomial logistic loss another feature of the deepnets approach is the generation of predictions at several resolutions and the calculation of loss at these intermediate resolutions in doing so the deepnets model predicts a semantic map at full resolution as well as smaller resolutions which are averaged together to obtain a final full resolution semantic prediction lastly a sliding window approach is used to extract smaller patches within each input image which acts as data augmentation for further details on the deepnets architecture we direct readers to audebert et al 2018 following procedures demonstrated by audebert et al 2016 2017 2018 we incorporate the ndvi and elevation data into our deepnets model however rather than using the original elevation grid as an input we guide the learning of the model by deriving specific geomorphic and hydrologic features from the dem as inputs this strategy was chosen following a hypothesis that wetland predictions would improve if a deep learning model trained from explanatory variables that are specific to wetlands in our implementation of deepnets we also applied class weights which are related to the importance of correct predictions for a specific class when calculating the loss we used this feature to account for the imbalance between the wetland and nonwetland classes across all sites as well as to decrease the importance of nodata areas lastly we allow for data augmentation in the form of mirroring images and flipping the orientation parameters for the deepnets model incorporated into our wetland model workflow are given in table 3 note that these parameters were chosen as starting points to be later refined through additional model testing 2 4 3 accuracy assessment in line with the intended environmental planning and permitting application accuracy metrics were selected considering the higher importance of true positive i e wetland predictions versus true negative i e nonwetland predictions to wetland conservation model performance was evaluated in terms of wetland recall and wetland precision calculated using the scikit learn python library scikit learn developers 2017 recall also known as the true positive rate represents the percentage of true wetlands that were predicted and is defined as 4 r e c a l l t r u e w e t l a n d p r e d i c t i o n s t o t a l t r u e w e t l a n d s recall can be considered the priority indicator of model performance given the importance of the minority wetland class a choice also supported by statistical literature branco et al 2016 chen et al 2004 sun et al 2007 precision is used to account for model overprediction unlike the commonly used specificity precision is not biased by large numbers of true negative instances and therefore can be considered more representative for imbalanced scenarios branco et al 2016 sun et al 2007 precision represents the percentage of wetland predictions made that were correct defined as 5 p r e c i s i o n t r u e w e t l a n d p r e d i c t i o n s t o t a l w e t l a n d p r e d i c t i o n s it should be noted that the appropriate selection of accuracy metrics remains an open problem not only for semantic segmentation but for classification tasks in general and additional criteria have been proposed and widely used we found recall and precision to be more suitable for model assessment compared to commonly used options such as overall accuracy kappa statistic and matthews correlation coefficient mcc when using overall accuracy detection rate of the minority class has a lower impact than that of the majority class branco et al 2016 chen et al 2004 misrepresenting a wetland model predicting all nonwetland instances as very accurate moreover the kappa statistic is biased by sample size and can increase as the wetlands to nonwetlands ratio increases even if wetland recall decreases ali et al 2014 byrt et al 1993 both overall accuracy and the kappa statistics have been omitted from wetland classification studies for these reasons baig et al 2014 zhu and pierskalla 2016 although the mcc metric has been shown to be suitable for imbalanced scenarios e g boughorbel et al 2017 its takes into account the number of true negative samples 2 5 experimental setup 2 5 1 addressing research question 1 creating site specific models experiments 1 and 2 fig 3 a were designed to offer insight into potential wetland accuracy given varying sizes of reliable training sets evaluated over four geographic regions in experiment 1 we created models that sample training images from the area to be mapped i e site specific models for each site 70 of eligible image sets were randomly selected producing the maximum training set size available which varied based on site size table 4 to compare how models of different ecoregions perform given the same training resources site specific models were created and evaluated at each threshold of training set size experiment 2 applied the site specific models created through experiment 1 those using the maximum training set size to predict wetlands in the other sites thus experiment 2 represents the scenario where a pretrained wetland model is applied for a new area for which training data is unavailable 2 5 2 addressing research question 2 creating combined site models experiments 3 and 4 fig 3b aim to evaluate the potential for improving wetland accuracy by incorporating training data from different geographic regions into a single model in experiment 3 a wetland model is trained using the largest training sets available from each site i e general model in experiment 4 a model is created using the maximum training data from two sites within the same ecoregion site 2 and site 3 i e ecoregion model both experiments aim to gain insight into the change in wetland predictions when the model learns wetland characteristics that exist for a range of landscapes 3 results 3 1 performance of site specific models for experiment 1 site specific models were built using training data quantities ranging from 9 to 77 images depending on validation data extents fig 4 the resulting 10 sets of wetland predictions were evaluated for the testing area complementing the training data quantity used results show that the best performing models for each site were those trained using the maximum training set size available equal to 70 of the validation area conversely the lowest performing models across all sites occurred when using the fewest training data nine images the site 4 model trained with 77 images achieved the highest wetland recall and precision across all site models the site 4 model also outperformed other sites when limited to the same number of training images fig 4 the overall lowest performing model was built for site 2 which also had the smallest training dataset available only nine images while the improvements in prediction accuracy as training data increased were expected intermediate changes in accuracy were inconsistent for site 3 recall increased considerably 46 85 and precision increased slightly 17 20 when increasing training images from 9 to 28 however changes in model accuracy were less significant for site 1 where the most notable accuracy improvement occurred when increasing training data from 28 to 31 images which increased recall from 70 to 81 and precision from 22 to 25 models built for site 4 performed consistently maintaining high performance regardless of training set sizes ranging from 9 to 77 images for site 4 recall only varied between 84 and 91 and precision between 50 and 56 it was unexpected that site 4 did not improve more notably when increasing the training dataset from 31 to 77 images as this was the largest increase in training set studied this may be due to the fact site 4 has the most balanced wetland to non wetland areas so fewer training images are needed to create an accurate model 3 2 using site specific models to predict wetlands in other sites experiment 2 resulted in an additional 12 sets of results where the best performing site specific models i e those trained with the maximum training data set size were used to predict wetlands in the other sites the evaluation of these trials represents wetland prediction accuracy for the entirety of the site validation area and the results achieved by applying the site specific models for their own areas are also shown for reference fig 5 in most cases utilizing training information from a different area even if this represented a greater quantity of data did not improve predictions compared to those resulting from a model trained for its own area site 2 was the exception for this trend as both recall and precision improved when using any of the models built for other sites compared to using the site 2 model moreover the site 2 model produced more accurate wetland predictions when applied to the other sites compared to its own testing area although the predictions for others sites resulting from the site 2 model were still among the lowest accuracies per site this suggests there may be topographic or spectral confusion between site 2 training and testing data also there was an unexpected increase in precision when applying the site 1 model versus the site 4 model for site 4 predictions however since both wetland precision and wetland recall should be considered when summarizing model performance the significantly greater recall achieved by the site 4 model leads us to conclude that the site 4 model outperformed the site 1 model here lastly the site 4 model resulted in the highest recall scores and among the lowest precision scores across all trials for sites 1 2 and 3 this reflects a tendency of the site 4 model to overpredict wetlands in other sites this may be because site 4 includes large areal wetlands common in the coastal plain given its low relief topography but uncommon in the other three sites that are outside of the coastal plain 3 3 performance of combined site models experiment 3 resulted in the general model trained with the maximum available training images from each site when applying the general model to site 1 testing areas recall increased from to 81 89 and precision decreased from 25 to 18 relative to the best performing site 1 model fig 6 for site 2 testing areas the general model considerably improved wetland recall 28 40 and minimally changed precision 3 2 compared to the best performing site specific model fig 6 the general model produced worse predictions than the site specific model for site 3 decreasing recall from 85 to 73 and precision from 20 to 15 the general model performed nearly the same for site 4 compared to the site specific model where recall remained high at 91 and precision increased by a small margin from 56 to 57 these results suggest that a general model trained with data collected across all sites would not be a suitable method for wetland prediction at least with the current methodology and data availability experiment 4 resulted in the ecoregion model trained with the maximum available training images from sites 2 and 3 which share the northern piedmont ecoregion this experiment tested the idea that a general wetland classification may be possible but only within a single ecoregion and not across ecoregions as was attempted in experiment 3 for site 2 the ecoregion model produced worse predictions than the general model and the site specific model with recall decreasing to 21 and precision remaining nearly the same at 2 fig 6 in contrast the ecoregion model improved wetland recall and precision for site 3 77 and 22 respectively compared to the general model however this was not an improvement from the site 3 specific model fig 6 this suggests that an ecoregion specific classification model may be useful but not more so than a site specific model given the data available here 4 discussion 4 1 potential for site specific models we found that site specific models improved as more training data was sampled from the area to be mapped with the best models created from the maximum training datasets studied 70 of the validation area however performance did not improve consistently for sites at the intermediate training data thresholds this outcome exemplifies that model improvement is an issue of not only increasing the quantity of training data but also the quality the performance inconsistencies may be due to unequal wetland distributions in each training image for example the training images introduced for site 1 when increasing the training data threshold from 9 to 28 images may have provided very few wetland areas if the random selection included scenes with few or only small wetlands in addition it is possible that the random nature of the training image set creation led to the introduction of some scenes with conflicting wetland nonwetland signatures as there is a benefit to identifying a training area threshold that begins to improve model performance across different sites future work should include repeating this experiment with quality controlled training data images and thresholds evaluating model performance across sites with training image thresholds at even increments of wetland and nonwetland area would result in more conclusive insights as to the changes in model performance as more training data becomes available this being said the overall improvements across the sites as training data increased to the maximum available set are likely due to the ability of the model to learn a wider range of wetland characteristics that exist in the additional landscape scenes fig 7 demonstrates the changes in wetland predictions as a result of increasing training data from nine training images column a to the maximum training images per site column b for sites 1 3 and 4 increased training data reduced wetland overprediction surrounding the extents of ground truth wetlands most notably for narrow wetland segments in sites 1 and 3 in addition wetland predictions for these sites encompassed more of the true wetland area most apparent for site 4 where predictions densified for a relatively large wetland as a result of increasing the training data fig 7 also exemplifies the poor performance of the site 2 model although the site 2 model predicts wetlands as small linear features that are representative of the nature of ground truth wetlands in the area the predictions are relatively sparse and incorrect by visually examining the input features and testing data for site 2 we found that validation wetlands existed underneath dense tree canopy along a road corridor topographic metrics in this area indicated values corresponding to wetness within the true wetland boundaries however the ndvi showed constant values for most of the forested area the lack of distinction between values by the ndvi is likely due to the source imagery the naip which is collected during the growing season with leaf on conditions and is therefore affected by tree canopy moreover the better performance for site 4 even when using few training data suggests that this landscape was particularly well suited to the deep learning approach this may be due to the large distribution of wetlands in site 4 leading to a higher quantity of wetlands in the entire training data set as well as more significant presence of wetlands in each training image fig 7 also shows model predictions when using the site 4 specific model the site 4 model produced predictions with the highest recall scores of all model trials for sites 1 2 and 3 as indicated by the increases in recall predictions resulting from the site 4 more densely encompassed the ground truth wetlands fig 7 column c relative to results for the site specific models fig 7 columns a and b attributing to the lower precision scores also produced by the site 4 model wetland overprediction is apparent in the scenes for site 1 2 and 3 fig 7 column c the wetland predictions for these sites are also made at a coarse resolution within image tile extents evident by the rectangular edges of wetland predictions in sites 1 and 3 fig 7 column c in addition a segment of a narrow wetland feature is omitted for site 3 when applying the model trained for site 4 overall these shortcomings demonstrate the potential for bias to a specific landscape and wetland type in site specific models which may lead to decreased accuracies when applied to different landscapes this may be overcome by changing the classification strategy away from a simple wetland non wetland classification to one that classifies different wetland types although this strategy was not explored through this research the increase in recall scores when using the site 4 model and the concentration of wetland overprediction occurring in the adjacent and surrounding areas of the ground truth wetlands suggests the noted shortcomings may also be addressed by using a more balanced sampling of different wetland types 4 2 potential for combined site models compared to the site specific models the general model mostly resulted in more wetland overprediction but in some cases increased coverage of ground truth wetlands fig 8 column b this trend is likely due to the bias of the general model to favor wetland types present in the site 4 landscape as more than half of all the training images used were from site 4 while the general model results do not present an improvement from the site specific models there are improvements compared to wetland predictions resulting from a model trained only on site 4 see fig 7 column c by supplementing the site 4 training data with wetland information from other landscapes we see finer more precise wetland prediction boundaries fig 8 site 1 b and site 3 b for site 2 the general model produced a greater overall amount of wetland predictions compared to the site specific model but predictions were inaccurate fig 8 column a vs column b however the quantity of erroneous wetland predictions for site 2 was greater when using the site 4 model versus the general model it was expected that predictions for site 4 would be mostly unchanged between the site specific model and the general model due to the significant presence of site 4 training data however the weak training data influence from other sites did slightly improve precision for site 4 demonstrated by finer scale edges of wetland predictions fig 8 site 4 a vs site 4 b the ecoregion model explored the potential for creating combined site models that are specific to certain landscape characteristics by including training data only from within the same ecoregion i e sites 2 and 3 fewer wetland predictions were made overall for site 2 using the ecoregion model fig 8 column c which considerably reduced recall compared to the general model but also resulted in sparser correct wetland predictions than the site 2 specific model for site 3 the ecoregion model improved both precision and recall compared to the general model but results were still less accurate than the site specific model compared to general model predictions the ecoregion model regained correct wetland predictions for narrow riparian wetland features for site 3 fig 8 column c the ecoregion model also reduced wetland overprediction compared to the general and site specific models in the scenes shown in fig 8 representative of the higher precision produced by the ecoregion model 22 vs 20 by the site specific model and 15 by the general model however wetland predictions resulting from the ecoregion model encompassed less ground truth wetland area overall relative to the site 3 specific model although neither approach for creating a combined site model was able to outperform site specific models results show potential to refine and improve these methods we found that the relatively poor performances of the general and ecoregion models were not likely caused by the unequal sampling of training data from the different geographic study areas to investigate this potential source of error the general model and the ecoregion model were recreated by limiting training data from sites to just nine images each balancing the representation from each site for all sites the general model built with equal but limited training data performed worse than the proposed general model for site 3 the ecoregion model built with limited training data performed considerably worse where recall decreased from 77 to 30 and precision improved slightly from 15 to 17 for site 2 however the limited ecoregion model improved results slightly recall increasing from 21 to 27 and precision remaining at 2 but still not to an acceptable level of accuracy thus improving the combined site model approach may not just be a matter of equally sampling different landscapes but also balancing an adequate amount of training data from different landscapes lastly the lack of consistent improvement to site 2 and site 3 predictions when applying the ecoregion model suggests it would be beneficial to consider additional landscape similarities when building combined site models landscape characteristics to consider may be those that affect the distributions of topographic inputs such as influence of built environment drainage and land cover 4 3 utility of the proposed input data configuration this study explored an input data configuration unique to most deep learning applications where topographic derivatives of the input image i e lidar dem are predetermined and specific to the target object i e wetlands the hypothesis was that predetermined elevation derivatives twi dtw and curvature would improve wetland classification training by including hydrologic information compared to training directly from the elevation data to evaluate the efficacy of this method we compared the accuracy achieved using our novel input data configuration versus two band images composed of the lidar dem and the ndvi which is more representative of the common input data approach taken e g audebert et al 2017 2018 latifovic et al 2018 liu et al 2018 silburt et al 2018 xu et al 2018 the lidar dems used to create the two band images were smoothed and hydrologically corrected as suggested by o neil et al 2019 and 70 of the areas were used for training for both model sets for sites 1 2 and 3 the proposed input data configuration outperformed the typical approach in terms of both recall and precision wetlands predicted from only the dem and ndvi for site 1 achieved lower recall 73 vs 81 and precision 21 vs 25 compared to the models using the derived topographic indices and the ndvi this suggests that combining physical understanding of the system in this case hydrological and ecological characteristics of wetlands helps to guide the deep learning algorithm so that it is able to obtain increased predictive skill for site 2 predictions learned from the dem and ndvi encompassed only 12 of the ground truth wetlands with near 0 precision compared to 28 recall and 3 precision achieved by the proposed approach wetland predictions for site 3 lost considerable accuracy with the typical input data approach producing 24 recall and 9 precision whereas our approach resulted in 85 recall and 20 precision for site 4 this comparison showed that the model that learned from the dem and ndvi alone produced a higher recall 96 vs 91 and lower precision 49 vs 56 while this indicates that more ground truth wetlands were detected using the typical approach it is slightly outweighed by the loss in wetland precision considering the consistent improvement to the other three sites the lack of significant change in site 4 when applying only the dem and ndvi may suggest that the deep learning model relies more heavily on the vegetative characteristics provided by the ndvi than the geomorphologic and hydrologic information that the elevation data offers this is likely due to the fact that site 4 had the least topographic relief being within the coastal plain results for site 4 using a random forest classification see o neil et al 2019 also support this idea showing that the topographic input variables were insufficient for describing wetland characteristics unless preprocessing methods were calibrated specifically to the area thus it is logical that wetlands in site 4 are better described by vegetative characteristics than topography explaining the lack of change in predictions when replacing the topographic inputs with the dem and leaving the ndvi input unchanged 4 4 comparison of deep learning to a random forest implementation to examine the potential for deep learning to advance the more commonly used random forest approach for wetland classification e g o neil et al 2019 we compared the performance of the site specific deep learning models to a random forest classification with the same set of input variables the random forest implementation follows the approach of o neil et al 2019 but with the addition of the ndvi to the original set of inputs the twi curvature and dtw the training sampling used in the o neil et al 2019 study was maintained where training data consists of randomly dispersed pixels that encompass only 15 of the validated wetland area and up to 8 of the validated nonwetland area however accuracy assessments for both the deep learning and random forest models were limited to the extents of the testing image tiles that correspond to the deep learning approach compared to the site 1 deep learning model the random forest classification resulted in an improvement in recall from 81 to 91 but a decrease in precision from 25 to 19 for site 2 random forest improved recall considerably from 28 to 78 and slightly improved precision from 3 to 5 the site 3 random forest model produced no change in recall 85 and a slight decrease in precision 20 vs 18 compared to deep learning finally the site 4 random forest model considerably decreased recall from 91 to 70 and increased precision from 56 to 64 relative to the deep learning model with the exception of site 2 these findings show that deep learning was able to perform similarly to random forests e g site 1 and site 3 and arguably better in some cases e g site 4 the poor performance in site 2 further supports that the deep learning model was not sufficiently able to learn characteristics of wetland features that were very small and sparse relative to the landscape scenes in each training image similarly the site 4 results again support the idea that deep learning is better suited to detecting wetlands where they are areal and large relative to the landscape scene in addition an evaluation of the entire testing areas corresponding to the random forest models shows that the inclusion of the ndvi as a wetland indicator improves on the o neil et al 2019 approach compared to the random forest models using only the topographic inputs the addition of the ndvi improved wetland recall and precision in site 1 81 vs 88 and 19 vs 24 site 2 82 vs 88 and 16 vs 22 site 3 83 vs 86 and 22 vs 25 and site 4 58 vs 68 and 47 vs 54 overall it is important to note that the random forest models were able to achieve these accuracies by sampling much less training data than was required for deep learning models however this result also shows that deep learning models can approach the same accuracies using training data resources that are considerably smaller relative to most deep learning applications in addition the similar performance of deep learning to random forests in three of the study sites supports findings by other researchers that state deep learning can improve landscape segmentation accuracy over traditional machine learning such as support vector machine maximum likelihood classification and random forests given enough training data e g hu et al 2018 latifovic et al 2018 liu et al 2018 mahdianpari et al 2018 4 5 limitations limitations of this approach could be addressed through additional research for example incorporating class activation mapping cam zhou et al 2016 which highlights scene elements that are most influential during classifications would offer further insight into model learning by utilizing cam model refinements could be made by quantifying the impact of the input data and identifying sources of error considering additional remote sensing data may also improve model performance these may include lidar point clouds which researchers have incorporated into 3 dimensional cnns for wetland identification e g xu et al 2018 also incorporating radar data may reduce errors where the ndvi is affected by tree canopy as it is able to penetrate this layer and provide vegetation density and inundation information for wetland mapping allen et al 2013 behnamian et al 2017 corcoran et al 2013 kloiber et al 2015 millard and richardson 2013 also on this point the contribution of each input data source throughout the deepnets workflow can be handled in a more sophisticated way this was demonstrated by audebert et al 2018 who proposed novel data fusing methods for elevation data and the ndvi within the deepnets workflow to improve land cover classifications additional training information that consists of accurately delineated wetlands from across different ecoregions should improve the deep learning classification results also additional training data would make it possible to train models for specific wetland types rather than a simple binary wetland nonwetland classification these training data are likely available from state and federal agencies given the need for wetland assessments under the clean water act but are not collected into a single standardized repository future work could focus on building such a training and testing repository for wetland classification furthermore to more efficiently make use of any amount of reliable training information available applying more sophisticated data augmentation techniques may improve wetland predictions as demonstrated by stivaktakis et al 2019 refinements to the current approach should also include more robust accuracy assessments the current accuracy metrics are transparent and represent the two factors that are needed for reliable implementation coverage of ground truth wetlands and limited overprediction however a single accuracy metric that encompasses both of these factors while also acknowledging the significantly higher importance of wetland recall would improve the interpretation of model results model evaluation improvements should also take into account the diffuse boundaries of wetlands which may fluctuate seasonally by penalizing overprediction less if it occurs adjacent to or surrounding defined ground truth wetland extents lastly this study did not test the effect of tuning the deepnets parameters among other parameter adjustments future work should explore the benefit of adjusting window sizes based on target wetland size and the accuracy tradeoffs when training the model for more epochs 5 conclusions we explore a wetland identification workflow that implements a basic semantic segmentation architecture and an input data configuration that consists of the ndvi and lidar dem derived indicators of wetland hydrology and geomorphology the workflow was trained and evaluated using available data resources from four geographic regions of virginia from this work we draw the following conclusions i site specific deep learning models created from relatively small training datasets can achieve accurate results for three of the four study sites wetland recall ranged from 81 to 91 and precision ranged from 20 to 56 when training models with 70 of site area and testing on the remaining 30 of the site area ii site specific models were more successful for areas where wetlands are abundant and occupy a significant portion of training images for a site with large areal wetlands that were almost evenly balanced with nonwetland areas high accuracy was achieved with 7 5 km2 70 of training area 91 recall and 56 precision using a much smaller training area 0 4 km2 10 of the study area still resulted in a fairly accurate model 84 recall and 50 precision iii in most cases accuracy decreased when using models trained for another site however the site specific model trained with the largest area studied 7 5 km2 increased wetland recall in all other sites although model predictions were imprecise and showed a bias towards the types of wetlands for which it was trained i e large areal wetlands the correct localization of wetland predictions suggests there is potential for this approach if models are trained with sufficient data and for areas with similar landscapes iv combined site models can produce accurate wetland predictions but training data contributions from the target landscapes should be balanced the general model revealed the potential for bias towards landscape characteristics more heavily represented in the training data however the influence of less represented sites was still apparent as wetland predictions were more inclusive of different wetland types compared to a model created without training data from these sites v shared ecoregion alone may not offer sufficient landscape similarities to improve the training sampling approach for combined site models the ecoregion model showed accuracy improvements from the general model for one site however wetland predictions for the other site were less accurate future work should explore the benefit of creating combined site models from areas that share additional characteristics that would affect the distributions of the topographic derivatives such as level of development land cover and topography vi the proposed input data configuration improves wetland identification compared to a more typical approach of using the ndvi and the lidar dem alone by predetermining the derivatives of the dem that are wetland indicators based on physical understanding of hydrology and wetland formation rather than allowing the deep learning network to determine these through convolutions on raw data wetland predictions were more accurate in three sites this speaks to the benefit and power of combining physical understanding along with machine and deep learning algorithms for improved predictive skill for the remaining site accuracy was nearly unchanged between the two approaches however analyses show that this is likely due to the greater importance of the ndvi for identifying wetlands in the topographically mild landscape vii compared to a random forest approach the best performing models produced comparable accuracy using more training data than required for random forest but still significantly less than what is typical in most deep learning applications our results demonstrate the potential for deep learning to not only improve accuracy compared to traditional machine learning algorithms but also provide flexible models that are accurate for a range of landscapes paramount to achieving this will be larger efforts within the research community to gather reliable training data and pretrained models stored as open source repositories as has been done for established deep learning fields e g lecun 1999 lin et al 2014 the wetland models created through this research may offer a starting point for creating a repository open to other researchers by refining this implementation of the deep learning wetland workflow and further training the created models there is potential for deep learning to support a range of wetland conservation efforts by producing accurate wetland inventories across many landscapes software and data availability software created through this research along with documentation is available under an mit license from https github com uva hydroinformatics wetland id all input data required to run the model are publicly available through federal and state data providers wetland delineation datasets used for training and evaluation was made available to the researchers through a relationship with the virginia department of transportation to retrain the model for a new landscape similar wetland delineation data for that area may be required declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors wish to thank dr nicolas audebert and his co authors for creating the deepnets architecture used here and making their work publicly available for other researchers to use and build from in addition thank you to the virginia department of transportation for providing the validation data required to complete this research this work was funded by a graduate assistance in areas of national need gaann fellowship through grants provided by the united states department of education grant numbers p200a180035 and p200a160322 
26059,automated and accurate wetland identification algorithms are increasingly important for wetland conservation and environmental planning deep learning for wetland identification is an emerging field that shows promise for advancing these efforts deep learning is unique to traditional machine learning techniques for its ability to consider the spatial context of object characteristics within a landscape scene however applying deep learning typically requires very large datasets for training the algorithms which limits their application for many environmental applications including wetland identification using four study sites across virginia with field delineated wetlands we provide insight into the potential for deep learning for wetland detection from limited but typical wetland delineation training data our proposed workflow performs a wetland semantic segmentation using deepnets a deep learning architecture for remote sensing data and an input dataset consisting of high resolution topographic indices and the normalized difference vegetation index results show that models trained and evaluated for a single site were able to achieve high accuracy up to 91 recall and 56 precision and similar accuracy can be obtained for models trained across multiple sites up to 91 recall and 57 precision through this analysis we found that across all sites input data configurations taking advantage of hydrologic properties derived from elevation data consistently outperformed models using the elevation data directly showing the benefit of physically informed inputs in deep learning training for wetland identification by refining the wetland identification workflow presented in this paper and collecting additional training data across landscapes there is potential for deep learning algorithms to support a range wetland conservation efforts 1 introduction wetlands are important ecosystems that are threatened by development climate change and pollution klemas 2011 wetland loss is both a global davidson 2014 and national problem as half of the wetlands of the conterminous u s have been lost since 1600 dahl et al 1991 in the u s federal regulations such as section 404 of the clean water act play an important role in wetland protection laws require environmental impact assessments prior to land development and water resources projects which entails the creation of detailed wetland surveys page and wilcher 1990 conducting these surveys with the level of spatial resolution and accuracy needed to abide by federal regulations and meet the goal of avoiding adverse impact to wetlands can be time consuming and costly to support these efforts methods for more rapidly identifying wetland locations are needed although manual surveys will continue to be the most accurate method to map wetlands there is potential for supporting these efforts by using machine learning approaches including deep learning to identify wetland features at varying scales guo et al 2017 lang et al 2013 lang and mccarty 2014 despite the many types of protected wetlands that exist all wetlands can be identified by common features these include the presence of hydrologic conditions that inundate the area vegetation adapted for life in saturated soil conditions and hydric soils us corps of engineers 1987 researchers have demonstrated the ability to detect these features from multispectral imagery radar and light detection and ranging lidar data guo et al 2017 multispectral imagery are the most commonly applied data in wetland studies guo et al 2017 klemas 2011 however spectral variables alone may be unable to distinguish wetlands due to spectral confusions from reflectance and backscattering dronova 2015 kim et al 2011 lidar data are well suited to complement multispectral analyses due to its wide and growing availability and demonstrated benefit to wetland mapping guo et al 2017 klemas 2011 kloiber et al 2015 lang and mccarty 2014 snyder and lang 2012 lidar returns can be interpolated to create high resolution digital elevation models dems from which wetland indicators based on flow convergence and near surface soil moisture can be derived lang et al 2013 lang and mccarty 2014 millard and richardson 2013 2015 o neil et al 2018 2019 moreover researchers have shown the benefit of lidar dem metrics as input variables to traditional machine learning techniques such as random forests for wetland mapping and classification e g deng et al 2017 kloiber et al 2015 millard and richardson 2013 millard and richardson 2015 o neil et al 2018 2019 zhu and pierskalla 2016 the successful coupling of lidar and multispectral imagery with traditional machine learning techniques for wetland identification is well documented however deep learning for remote sensing studies including wetland identification is a new application space ma et al 2017 zhang et al 2016 that shows promise for fulfilling the unmet need for wetland inventory creation deep learning architectures are modeled after the architecture of the mammal brain serre et al 2007 where inputs are perceived and processed through multiple layers of abstraction convolutional neural networks cnns lecun et al 1998 are a representative form of deep learning that is used for visual recognition cnns utilize the spatial context of detected features to identify objects and classify scenes the distinguishing element of cnn architectures are the convolutional layers which convolve spatial filters over input images to identify patterns that are characteristic of target classes deep convolutional neural networks dcnns he et al 2016 krizhevsky et al 2017 simonyan and zisserman 2014 and fully convolutional neural networks fcns long et al 2015 are extensions of the cnn framework that can output dense pixel wise classifications within images i e semantic segmentation where each pixel of the input image is assigned a class since the formalization of the concept in 2006 hinton et al 2006 deep learning has advanced the fields of speech recognition medical diagnosis and autonomous driving applications and has since motivated new applications in environmental and water resources management liu et al 2018 pan et al 2019 shen 2018 zhang et al 2016 researchers have shown the ability of dcnns fcns and other cnn extensions to delineate urban and natural landscape classes using multispectral imagery and topographic data audebert et al 2017 2018 multispectral imagery and lidar point clouds xu et al 2018 and multispectral imagery alone hu et al 2018 kemker et al 2018b kemker et al 2018a scott et al 2017 few researchers have applied dcnns and fcns specifically to wetland classification these include liu et al 2018 who applied orthoimagery and elevation information to deep learning models for wetland segmentation in addition rezaee et al 2018 used multispectral imagery in a wetland deep learning model and posited that predictions would improve with the incorporation of physical information from radar or lidar sources the typical need for massive validation sets to train deep learning models is a significant deterrent to environmental and water resources researchers shen 2018 zhang et al 2016 as reliable training data is often lacking in these applications this issue is especially prevalent for wetland identification that is intended to inform conservation and permitting efforts where training data for computational models are ideally manually derived and confirmed by regulatory entities the effects of training data limits for wetland semantic segmentation have been investigated by liu et al 2018 where comparisons were drawn for a single study area using dcnns fcns random forests and support vector machines with privately contracted aerial imagery and surface elevation information as input features while this is an important stride in gaining insight into the training data needs for deep learning of wetlands an analysis has yet to be done that utilizes freely available data and is completed over multiple geographic regions the growing research area of deep learning for remote sensing applications shows promise for advancing wetland mapping although researchers have begun to show the potential for wetland identification at a high resolution using deep learning approaches research gaps remain specifically analyses are needed to identify the deep learning performance potential for different geographic regions when limited to relatively small quantities of verification data and freely available input data which are typical in practice we aim to contribute to this field by presenting a novel wetland identification methodology that implements a basic semantic segmentation architecture and is generalizable because it leverages freely available geospatial and remote sensing data our input data configuration consists of lidar dem derivatives that describe geomorphologic and hydrologic contributors to wetland formation as well as a commonly used vegetative index using four study sites across virginia we build and evaluate several wetland models to demonstrate the potential for wetland semantic segmentation given typical training data resources through this research we seek to answer the following questions i across geographically distinct study sites what wetland prediction accuracy is achievable by building site specific models from typically available amounts of wetland delineation training data ii what is the potential for a single combined site model trained using data from across geographic regions to predict wetlands at each individual site 2 methodology 2 1 study areas four study areas across virginia usa are used in this analysis fig 1 a data for each study area include the extents of wetland surveys and the surrounding hydrologic unit code huc 12 watershed usgs 2019 fig 1b the huc 12 watersheds were used as processing extents and surveyed areas provided the validation data also referred to as the study sites the study areas span four level iii ecoregions as shown in table 1 the sites also vary by size land cover and topographic characteristics notable differences include the higher rate of development in sites 1 and 2 and the mild topography of site 4 in addition wetlands are much more abundant in site 4 where the wetland to nonwetland ratio is 0 42 compared to less than 0 1 in the other sites note that all surveyed wetland types were merged into a single wetland category prior to use as verification data 2 2 input data this study used publicly available lidar dems national agriculture imagery program naip aerial imagery and field mapped wetland surveys lidar dems were obtained from the virginia information technologies agency vita vita 2016 as hydro flattened bare earth dems the lidar data used were collected and processed between 2010 and 2015 and have horizontal resolutions ranging from 0 76 m to 1 5 m naip imagery are provided by the united sates department of agriculture farm service agency 2017 naip imagery were used to derive the ndvi naip imagery contain four spectral bands red green blue and near infrared at a 1 m spatial resolution imagery used in this study were collected near the dates of wetland surveying and images were resampled to match the resolution of the lidar dems if necessary wetland delineations and survey limits were provided by the virginia department of transportation vdot in polygon vector format and served as validation data for this study all verification wetlands were manually surveyed during summer months may august between 2013 and 2016 by professional wetland scientists in compliance with transportation planning permitting wetland delineations for sites 2 3 and 4 were also jurisdictionally confirmed by the us army corps of engineers usace binary wetland nonwetland geotiffs were created from these data with resolutions matching those of the site lidar dems visual analyses of google earth images showed that the study site landscapes changed minimally between lidar acquisition and wetland delineation timeframes 2 3 wetland identification method the wetland identification method consists of three main parts preprocessing feature creation and semantic segmentation and accuracy assessment fig 2 input data required include high resolution dem data four band aerial imagery and validated wetland nonwetland distribution data all in geotiff format from these data topographic indices curvature topographic wetness index and cartographic depth to water index and the normalized difference vegetation index are calculated these input features are merged into a single four band composite grid smaller image tiles are created from the composite grid and validation data and the pairs of corresponding image tiles are randomly separated into training and testing datasets finally dense pixel wise wetland predictions are made using a deep learning architecture created for remote sensing data deepnets for earth observation audebert et al 2018 and the accuracy of wetland predictions is assessed the main outputs are geotiff wetland predictions for each image tile and an accuracy report for the entire validation data area the method was implemented using open source python libraries and is available under an mit license see software availability section 2 4 preprocessing dem preprocessing was necessary to create an improved land surface representation from which to calculate indicators of wetland geomorphology first dem smoothing is performed which is necessary to addresses microtopographic noise microtopographic noise is common in high resolution dems and can be representative of either erroneous data or true variations in the elevation of vegetated surfaces jyotsna and haff 1997 dem conditioning is then executed which is necessary prior to modeling hydrologic flow paths as it addresses topographic depressions jenson and domingue 1988 o callaghan and mark 1984 topographic depressions interfere with overland flow path modeling by creating discontinuities in flow paths and accumulating water which negatively influences modeled watershed processes grimaldi et al 2007 lindsay 2016 lindsay and creed 2005 dem conditioning is particularly important for hydrologic modeling from high resolution dems as researchers have found that sensitivity of hydrologic parameter extraction to conditioning technique increases significantly with dem resolution woodrow et al 2016 although many techniques have been proposed for both dem smoothing and conditioning we apply the perona malik smoothing and a least cost path conditioning this preprocessing combination was found to considerably improve wetland identification for the study sites in a prior study see o neil et al 2019 the perona malik filter perona and malik 1990 performs a nonlinear anisotropic diffusion that preserves feature edges by penalizing smoothing across estimated feature boundaries passalacqua et al 2010a 2010b perona malik smoothing was implemented using code from the nonlinear filtering module from pygeonet an open source software for automatic channel network extraction from dems passalacqua et al 2010a sangireddy et al 2016 the a least cost path algorithm hart et al 1968 determines the least cost drainage paths through unaltered terrain and out of sinks thus avoiding unnecessary modification of the input dem metz et al 2011 the a conditioning method was executed using the grass gis r watershed module grass development team 2017 metz et al 2011 2 4 1 feature creation 2 4 1 1 topographic features in a prior study we concluded that the curvature topographic wetness index twi and cartographic depth to water index dtw are successful topographic metrics for wetland identification for our study sites o neil et al 2018 2019 curvature of a surface can describe the degree of convergence and acceleration of flow moore et al 1991 and studies have shown its capability to indicate saturated and channelized areas ågren et al 2014 hogg and todd 2007 kloiber et al 2015 millard and richardson 2015 o neil et al 2018 2019 sangireddy et al 2016 here we use laplacian curvature defined as the second derivative of the elevation grid laplacian curvature has been shown to favor the extraction of natural channels rather than artificial drainage paths and to more effectively identify channels in flat developed landscapes compared to alternative curvature forms passalacqua et al 2012 thus we found the laplacian curvature to be most suitable for our study areas which all encompass corridor projects and are partially developed o neil et al 2019 the curvature grid is created from the smoothed dem using code adopted from pygeonet passalacqua et al 2010a pygeonet 2019 sangireddy et al 2016 the ability of the twi to indicate saturated areas is well documented in the literature ågren et al 2014 lang et al 2013 millard and richardson 2015 murphy et al 2009 o neil et al 2018 2019 the twi relates the potential for an area to accumulate water to its tendency to drain water defined as 1 t w i ln α tan β where α is the specific catchment area contributing area per unit contour length and tan β is the local slope beven and kirkby 1979 the twi was created from the smoothed conditioned dem using the r watershed program of grass gis this module calculates the α term using the multiple flow direction algorithm holmgren 1994 and the β term using a grass gis calculated slope researchers have demonstrated the capability of the dtw to capture saturated areas as well murphy et al 2007 2009 2011 o neil et al 2018 2019 oltean et al 2016 white et al 2012 the dtw assumes that the likelihood for soil to be saturated increases with its proximity to surface water in terms of distance and elevation murphy et al 2007 calculated on a per pixel basis the dtw is defined as 2 d t w m d z i d x i a x p where d z d x is the downward slope of pixel i along the least cost i e slope path to the nearest surface water pixel a is a factor accounting for flow moving parallel or diagonal across pixel boundaries and x p is the pixel resolution murphy et al 2007 inputs required to calculate the dtw include a slope grid representing cost and a surface water grid representing the source from which distance is calculated we create the surface water grid directly from the lidar dem using pygeonet which performs a statistical analysis of curvature and uses geodesic minimization principles to predict stream lines passalacqua et al 2010a sangireddy et al 2016 visual analyses showed that streams created by pygeonet better aligned with aerial imagery compared to national hydrography data i e nhd streams and streams generated from the flow initiation threshold method band 1986 o callaghan and mark 1984 tarboton 1991 that is commonly used pygeonet was executed using parameters suggested for engineered landscapes see sangireddy et al 2016 which was found to produce accurate results across all sites in prior wetland model development o neil et al 2019 the pygeonet streams and slope grid were used as inputs to the grass gis r cost module grass development team 2017 to create the dtw grid 2 4 1 2 ndvi the ndvi is a commonly used spectral index that relates plant biomass and stress and separates wet versus dry areas klemas 2011 ozesmi and bauer 2002 researchers have used the ndvi as a wetland indicator in traditional machine learning frameworks corcoran et al 2013 dronova 2015 dronova et al 2011 guo et al 2017 mui et al 2015 rampi et al 2014 tian et al 2016 as well as for general land cover classifications using deep learning frameworks audebert et al 2017 2018 lee et al 2019 xu et al 2018 the ndvi utilizes the red and the near infrared bands carlson and riziley 1997 defined as 3 n d v i i n f r a r e d r e d i n f r a r e d r e d the red band indicates surface layer chlorophyll and therefore surface conditions of plants and the near infrared band is reflected from the inner leaf cell structure indicating the abundance of plant tissue klemas 2011 to calculate the ndvi eq 3 was executed using numpy operations and the appropriate naip imagery bands 2 4 1 3 image dataset creation the image dataset creation produces two sets of image tiles i feature tiles representative of the composite grid of input features and ii validation tiles representative of ground truth wetland and nonwetland locations due to the irregular shapes of the field surveys nodata pixels existed within the rectangular extent of the validation data rather than reduce our validation data to an extent without unverified area nodata pixels were treated as an additional target landscape class thus all pixels in the validation data were categorized as nodata 0 nonwetland 1 or wetland 2 as a first step in the image dataset creation process to build the dataset of feature tiles each band of the composite grid is rescaled to a range of 0 1 per the requirements of the deepnets algorithm rescaling the ndvi band was nontrivial as these values have global minimum and maximum of 1 and 1 conversely the range of values for each of the topographic features depends on the landscape they are calculated from therefore it was necessary to assume global minimum and maximum values the range of each topographic input was analyzed across the study sites and global minimum and maximum values that encompassed roughly 90 of the values were chosen note that only global maximum values had to be assumed for the twi and dtw which both have global lower bounds of 0 or nearly 0 although this step generalizes portions of the study areas this occurs only where there are extreme topographic features that occur infrequently in addition by limiting the range applied to each topographic input feature rather than choosing extreme but encompassing values the significance of the relative distance between values is minimally affected the minimum and maximum values used to rescale topographic features and the ndvi to a range of 0 1 are shown in table 2 following these steps the categorized validation grid and scaled composite grid were each separated into image tiles of size 320 x 320 pixels we chose the 320 pixel size constraint to balance the desire to use image tiles large enough to depict heterogeneous landscapes and the need to separate the study site into enough images to sample training and testing tiles that were randomly dispersed feature and labeled image tiles sets were not considered for either training or testing if more than 80 of the area was populated with nodata pixels 2 4 2 semantic segmentation model deepnets for earth observation our model performs a semantic segmentation of input images where each pixel of an input image is labeled as either nodata nonwetland or wetland that is a trained semantic segmentation model will assign a class prediction to each pixel in an image however different instances of target class objects are not defined i e instance segmentation as an initial step in developing a deep learning wetland model the current work is intended to demonstrate the suitability of a cnn to identify planning scale wetlands in the landscape we implemented a multimodal deep network deepnets for earth observation for semantic segmentation classification audebert et al 2017 deepnets has emerged as a state of the art tool for segmentation of high resolution remote sensing data demir et al 2018 and has been implemented and validated for automating segmentation of remote sensing data audebert et al 2016 2017 2018 although deepnets was chosen as a vehicle to address the guiding research questions of this work it is among several deep learning architecture currently achieving competitively in semantic segmentation of satellite imagery ghosh et al 2018 applied a stacked u nets architecture to achieve high quality satellite imagery segmentation with relatively few prediction parameters volpi and tuia 2016 use a cnn to segment very high resolution imagery to achieve f1 scores of about 85 marmanis et al 2018 propose a downsample upsample and achieve similar results while each of these approaches are likely to achieve good results with wetlands segmentation deepnets achieved slightly higher results on segmentation of benchmark imagery datasets demir et al 2018 thus it was adopted for this study an important future step in progressing this research would be to perform a comparative analysis of other emerging deep learning techniques for wetland segmentation as a starting point in the development of our deep learning wetland model the baseline deepnets architecture is implemented here audebert et al 2018 2019 deepnets builds on the segnet architecture badrinarayanan et al 2017 and is implemented using pytorch paszke et al 2017 segnet produces predictions with the same resolution as the input image by using an encoder decoder structure making it well suited for classification of landscape objects from georeferenced images audebert et al 2018 badrinarayanan et al 2017 the encoder portion of segnet is based on the convolutional layers of vgg 16 simonyan and zisserman 2014 and consists of convolutional layers batch normalization a rectified linear unit and max pooling as shown in the inset image defined by audebert et al 2018 in fig 2 the decoder is structurally symmetrical to the encoder pooling layers are replaced with unpooling layers that relocate pixel activations from the smaller feature maps to corresponding indices of zero padded upsampled images convolution blocks are then used to densify the sparse pixel activations this sequence of unpooling and convolutions is repeated until feature maps reach the original spatial resolution following this a softmax layer is used to compute multinomial logistic loss another feature of the deepnets approach is the generation of predictions at several resolutions and the calculation of loss at these intermediate resolutions in doing so the deepnets model predicts a semantic map at full resolution as well as smaller resolutions which are averaged together to obtain a final full resolution semantic prediction lastly a sliding window approach is used to extract smaller patches within each input image which acts as data augmentation for further details on the deepnets architecture we direct readers to audebert et al 2018 following procedures demonstrated by audebert et al 2016 2017 2018 we incorporate the ndvi and elevation data into our deepnets model however rather than using the original elevation grid as an input we guide the learning of the model by deriving specific geomorphic and hydrologic features from the dem as inputs this strategy was chosen following a hypothesis that wetland predictions would improve if a deep learning model trained from explanatory variables that are specific to wetlands in our implementation of deepnets we also applied class weights which are related to the importance of correct predictions for a specific class when calculating the loss we used this feature to account for the imbalance between the wetland and nonwetland classes across all sites as well as to decrease the importance of nodata areas lastly we allow for data augmentation in the form of mirroring images and flipping the orientation parameters for the deepnets model incorporated into our wetland model workflow are given in table 3 note that these parameters were chosen as starting points to be later refined through additional model testing 2 4 3 accuracy assessment in line with the intended environmental planning and permitting application accuracy metrics were selected considering the higher importance of true positive i e wetland predictions versus true negative i e nonwetland predictions to wetland conservation model performance was evaluated in terms of wetland recall and wetland precision calculated using the scikit learn python library scikit learn developers 2017 recall also known as the true positive rate represents the percentage of true wetlands that were predicted and is defined as 4 r e c a l l t r u e w e t l a n d p r e d i c t i o n s t o t a l t r u e w e t l a n d s recall can be considered the priority indicator of model performance given the importance of the minority wetland class a choice also supported by statistical literature branco et al 2016 chen et al 2004 sun et al 2007 precision is used to account for model overprediction unlike the commonly used specificity precision is not biased by large numbers of true negative instances and therefore can be considered more representative for imbalanced scenarios branco et al 2016 sun et al 2007 precision represents the percentage of wetland predictions made that were correct defined as 5 p r e c i s i o n t r u e w e t l a n d p r e d i c t i o n s t o t a l w e t l a n d p r e d i c t i o n s it should be noted that the appropriate selection of accuracy metrics remains an open problem not only for semantic segmentation but for classification tasks in general and additional criteria have been proposed and widely used we found recall and precision to be more suitable for model assessment compared to commonly used options such as overall accuracy kappa statistic and matthews correlation coefficient mcc when using overall accuracy detection rate of the minority class has a lower impact than that of the majority class branco et al 2016 chen et al 2004 misrepresenting a wetland model predicting all nonwetland instances as very accurate moreover the kappa statistic is biased by sample size and can increase as the wetlands to nonwetlands ratio increases even if wetland recall decreases ali et al 2014 byrt et al 1993 both overall accuracy and the kappa statistics have been omitted from wetland classification studies for these reasons baig et al 2014 zhu and pierskalla 2016 although the mcc metric has been shown to be suitable for imbalanced scenarios e g boughorbel et al 2017 its takes into account the number of true negative samples 2 5 experimental setup 2 5 1 addressing research question 1 creating site specific models experiments 1 and 2 fig 3 a were designed to offer insight into potential wetland accuracy given varying sizes of reliable training sets evaluated over four geographic regions in experiment 1 we created models that sample training images from the area to be mapped i e site specific models for each site 70 of eligible image sets were randomly selected producing the maximum training set size available which varied based on site size table 4 to compare how models of different ecoregions perform given the same training resources site specific models were created and evaluated at each threshold of training set size experiment 2 applied the site specific models created through experiment 1 those using the maximum training set size to predict wetlands in the other sites thus experiment 2 represents the scenario where a pretrained wetland model is applied for a new area for which training data is unavailable 2 5 2 addressing research question 2 creating combined site models experiments 3 and 4 fig 3b aim to evaluate the potential for improving wetland accuracy by incorporating training data from different geographic regions into a single model in experiment 3 a wetland model is trained using the largest training sets available from each site i e general model in experiment 4 a model is created using the maximum training data from two sites within the same ecoregion site 2 and site 3 i e ecoregion model both experiments aim to gain insight into the change in wetland predictions when the model learns wetland characteristics that exist for a range of landscapes 3 results 3 1 performance of site specific models for experiment 1 site specific models were built using training data quantities ranging from 9 to 77 images depending on validation data extents fig 4 the resulting 10 sets of wetland predictions were evaluated for the testing area complementing the training data quantity used results show that the best performing models for each site were those trained using the maximum training set size available equal to 70 of the validation area conversely the lowest performing models across all sites occurred when using the fewest training data nine images the site 4 model trained with 77 images achieved the highest wetland recall and precision across all site models the site 4 model also outperformed other sites when limited to the same number of training images fig 4 the overall lowest performing model was built for site 2 which also had the smallest training dataset available only nine images while the improvements in prediction accuracy as training data increased were expected intermediate changes in accuracy were inconsistent for site 3 recall increased considerably 46 85 and precision increased slightly 17 20 when increasing training images from 9 to 28 however changes in model accuracy were less significant for site 1 where the most notable accuracy improvement occurred when increasing training data from 28 to 31 images which increased recall from 70 to 81 and precision from 22 to 25 models built for site 4 performed consistently maintaining high performance regardless of training set sizes ranging from 9 to 77 images for site 4 recall only varied between 84 and 91 and precision between 50 and 56 it was unexpected that site 4 did not improve more notably when increasing the training dataset from 31 to 77 images as this was the largest increase in training set studied this may be due to the fact site 4 has the most balanced wetland to non wetland areas so fewer training images are needed to create an accurate model 3 2 using site specific models to predict wetlands in other sites experiment 2 resulted in an additional 12 sets of results where the best performing site specific models i e those trained with the maximum training data set size were used to predict wetlands in the other sites the evaluation of these trials represents wetland prediction accuracy for the entirety of the site validation area and the results achieved by applying the site specific models for their own areas are also shown for reference fig 5 in most cases utilizing training information from a different area even if this represented a greater quantity of data did not improve predictions compared to those resulting from a model trained for its own area site 2 was the exception for this trend as both recall and precision improved when using any of the models built for other sites compared to using the site 2 model moreover the site 2 model produced more accurate wetland predictions when applied to the other sites compared to its own testing area although the predictions for others sites resulting from the site 2 model were still among the lowest accuracies per site this suggests there may be topographic or spectral confusion between site 2 training and testing data also there was an unexpected increase in precision when applying the site 1 model versus the site 4 model for site 4 predictions however since both wetland precision and wetland recall should be considered when summarizing model performance the significantly greater recall achieved by the site 4 model leads us to conclude that the site 4 model outperformed the site 1 model here lastly the site 4 model resulted in the highest recall scores and among the lowest precision scores across all trials for sites 1 2 and 3 this reflects a tendency of the site 4 model to overpredict wetlands in other sites this may be because site 4 includes large areal wetlands common in the coastal plain given its low relief topography but uncommon in the other three sites that are outside of the coastal plain 3 3 performance of combined site models experiment 3 resulted in the general model trained with the maximum available training images from each site when applying the general model to site 1 testing areas recall increased from to 81 89 and precision decreased from 25 to 18 relative to the best performing site 1 model fig 6 for site 2 testing areas the general model considerably improved wetland recall 28 40 and minimally changed precision 3 2 compared to the best performing site specific model fig 6 the general model produced worse predictions than the site specific model for site 3 decreasing recall from 85 to 73 and precision from 20 to 15 the general model performed nearly the same for site 4 compared to the site specific model where recall remained high at 91 and precision increased by a small margin from 56 to 57 these results suggest that a general model trained with data collected across all sites would not be a suitable method for wetland prediction at least with the current methodology and data availability experiment 4 resulted in the ecoregion model trained with the maximum available training images from sites 2 and 3 which share the northern piedmont ecoregion this experiment tested the idea that a general wetland classification may be possible but only within a single ecoregion and not across ecoregions as was attempted in experiment 3 for site 2 the ecoregion model produced worse predictions than the general model and the site specific model with recall decreasing to 21 and precision remaining nearly the same at 2 fig 6 in contrast the ecoregion model improved wetland recall and precision for site 3 77 and 22 respectively compared to the general model however this was not an improvement from the site 3 specific model fig 6 this suggests that an ecoregion specific classification model may be useful but not more so than a site specific model given the data available here 4 discussion 4 1 potential for site specific models we found that site specific models improved as more training data was sampled from the area to be mapped with the best models created from the maximum training datasets studied 70 of the validation area however performance did not improve consistently for sites at the intermediate training data thresholds this outcome exemplifies that model improvement is an issue of not only increasing the quantity of training data but also the quality the performance inconsistencies may be due to unequal wetland distributions in each training image for example the training images introduced for site 1 when increasing the training data threshold from 9 to 28 images may have provided very few wetland areas if the random selection included scenes with few or only small wetlands in addition it is possible that the random nature of the training image set creation led to the introduction of some scenes with conflicting wetland nonwetland signatures as there is a benefit to identifying a training area threshold that begins to improve model performance across different sites future work should include repeating this experiment with quality controlled training data images and thresholds evaluating model performance across sites with training image thresholds at even increments of wetland and nonwetland area would result in more conclusive insights as to the changes in model performance as more training data becomes available this being said the overall improvements across the sites as training data increased to the maximum available set are likely due to the ability of the model to learn a wider range of wetland characteristics that exist in the additional landscape scenes fig 7 demonstrates the changes in wetland predictions as a result of increasing training data from nine training images column a to the maximum training images per site column b for sites 1 3 and 4 increased training data reduced wetland overprediction surrounding the extents of ground truth wetlands most notably for narrow wetland segments in sites 1 and 3 in addition wetland predictions for these sites encompassed more of the true wetland area most apparent for site 4 where predictions densified for a relatively large wetland as a result of increasing the training data fig 7 also exemplifies the poor performance of the site 2 model although the site 2 model predicts wetlands as small linear features that are representative of the nature of ground truth wetlands in the area the predictions are relatively sparse and incorrect by visually examining the input features and testing data for site 2 we found that validation wetlands existed underneath dense tree canopy along a road corridor topographic metrics in this area indicated values corresponding to wetness within the true wetland boundaries however the ndvi showed constant values for most of the forested area the lack of distinction between values by the ndvi is likely due to the source imagery the naip which is collected during the growing season with leaf on conditions and is therefore affected by tree canopy moreover the better performance for site 4 even when using few training data suggests that this landscape was particularly well suited to the deep learning approach this may be due to the large distribution of wetlands in site 4 leading to a higher quantity of wetlands in the entire training data set as well as more significant presence of wetlands in each training image fig 7 also shows model predictions when using the site 4 specific model the site 4 model produced predictions with the highest recall scores of all model trials for sites 1 2 and 3 as indicated by the increases in recall predictions resulting from the site 4 more densely encompassed the ground truth wetlands fig 7 column c relative to results for the site specific models fig 7 columns a and b attributing to the lower precision scores also produced by the site 4 model wetland overprediction is apparent in the scenes for site 1 2 and 3 fig 7 column c the wetland predictions for these sites are also made at a coarse resolution within image tile extents evident by the rectangular edges of wetland predictions in sites 1 and 3 fig 7 column c in addition a segment of a narrow wetland feature is omitted for site 3 when applying the model trained for site 4 overall these shortcomings demonstrate the potential for bias to a specific landscape and wetland type in site specific models which may lead to decreased accuracies when applied to different landscapes this may be overcome by changing the classification strategy away from a simple wetland non wetland classification to one that classifies different wetland types although this strategy was not explored through this research the increase in recall scores when using the site 4 model and the concentration of wetland overprediction occurring in the adjacent and surrounding areas of the ground truth wetlands suggests the noted shortcomings may also be addressed by using a more balanced sampling of different wetland types 4 2 potential for combined site models compared to the site specific models the general model mostly resulted in more wetland overprediction but in some cases increased coverage of ground truth wetlands fig 8 column b this trend is likely due to the bias of the general model to favor wetland types present in the site 4 landscape as more than half of all the training images used were from site 4 while the general model results do not present an improvement from the site specific models there are improvements compared to wetland predictions resulting from a model trained only on site 4 see fig 7 column c by supplementing the site 4 training data with wetland information from other landscapes we see finer more precise wetland prediction boundaries fig 8 site 1 b and site 3 b for site 2 the general model produced a greater overall amount of wetland predictions compared to the site specific model but predictions were inaccurate fig 8 column a vs column b however the quantity of erroneous wetland predictions for site 2 was greater when using the site 4 model versus the general model it was expected that predictions for site 4 would be mostly unchanged between the site specific model and the general model due to the significant presence of site 4 training data however the weak training data influence from other sites did slightly improve precision for site 4 demonstrated by finer scale edges of wetland predictions fig 8 site 4 a vs site 4 b the ecoregion model explored the potential for creating combined site models that are specific to certain landscape characteristics by including training data only from within the same ecoregion i e sites 2 and 3 fewer wetland predictions were made overall for site 2 using the ecoregion model fig 8 column c which considerably reduced recall compared to the general model but also resulted in sparser correct wetland predictions than the site 2 specific model for site 3 the ecoregion model improved both precision and recall compared to the general model but results were still less accurate than the site specific model compared to general model predictions the ecoregion model regained correct wetland predictions for narrow riparian wetland features for site 3 fig 8 column c the ecoregion model also reduced wetland overprediction compared to the general and site specific models in the scenes shown in fig 8 representative of the higher precision produced by the ecoregion model 22 vs 20 by the site specific model and 15 by the general model however wetland predictions resulting from the ecoregion model encompassed less ground truth wetland area overall relative to the site 3 specific model although neither approach for creating a combined site model was able to outperform site specific models results show potential to refine and improve these methods we found that the relatively poor performances of the general and ecoregion models were not likely caused by the unequal sampling of training data from the different geographic study areas to investigate this potential source of error the general model and the ecoregion model were recreated by limiting training data from sites to just nine images each balancing the representation from each site for all sites the general model built with equal but limited training data performed worse than the proposed general model for site 3 the ecoregion model built with limited training data performed considerably worse where recall decreased from 77 to 30 and precision improved slightly from 15 to 17 for site 2 however the limited ecoregion model improved results slightly recall increasing from 21 to 27 and precision remaining at 2 but still not to an acceptable level of accuracy thus improving the combined site model approach may not just be a matter of equally sampling different landscapes but also balancing an adequate amount of training data from different landscapes lastly the lack of consistent improvement to site 2 and site 3 predictions when applying the ecoregion model suggests it would be beneficial to consider additional landscape similarities when building combined site models landscape characteristics to consider may be those that affect the distributions of topographic inputs such as influence of built environment drainage and land cover 4 3 utility of the proposed input data configuration this study explored an input data configuration unique to most deep learning applications where topographic derivatives of the input image i e lidar dem are predetermined and specific to the target object i e wetlands the hypothesis was that predetermined elevation derivatives twi dtw and curvature would improve wetland classification training by including hydrologic information compared to training directly from the elevation data to evaluate the efficacy of this method we compared the accuracy achieved using our novel input data configuration versus two band images composed of the lidar dem and the ndvi which is more representative of the common input data approach taken e g audebert et al 2017 2018 latifovic et al 2018 liu et al 2018 silburt et al 2018 xu et al 2018 the lidar dems used to create the two band images were smoothed and hydrologically corrected as suggested by o neil et al 2019 and 70 of the areas were used for training for both model sets for sites 1 2 and 3 the proposed input data configuration outperformed the typical approach in terms of both recall and precision wetlands predicted from only the dem and ndvi for site 1 achieved lower recall 73 vs 81 and precision 21 vs 25 compared to the models using the derived topographic indices and the ndvi this suggests that combining physical understanding of the system in this case hydrological and ecological characteristics of wetlands helps to guide the deep learning algorithm so that it is able to obtain increased predictive skill for site 2 predictions learned from the dem and ndvi encompassed only 12 of the ground truth wetlands with near 0 precision compared to 28 recall and 3 precision achieved by the proposed approach wetland predictions for site 3 lost considerable accuracy with the typical input data approach producing 24 recall and 9 precision whereas our approach resulted in 85 recall and 20 precision for site 4 this comparison showed that the model that learned from the dem and ndvi alone produced a higher recall 96 vs 91 and lower precision 49 vs 56 while this indicates that more ground truth wetlands were detected using the typical approach it is slightly outweighed by the loss in wetland precision considering the consistent improvement to the other three sites the lack of significant change in site 4 when applying only the dem and ndvi may suggest that the deep learning model relies more heavily on the vegetative characteristics provided by the ndvi than the geomorphologic and hydrologic information that the elevation data offers this is likely due to the fact that site 4 had the least topographic relief being within the coastal plain results for site 4 using a random forest classification see o neil et al 2019 also support this idea showing that the topographic input variables were insufficient for describing wetland characteristics unless preprocessing methods were calibrated specifically to the area thus it is logical that wetlands in site 4 are better described by vegetative characteristics than topography explaining the lack of change in predictions when replacing the topographic inputs with the dem and leaving the ndvi input unchanged 4 4 comparison of deep learning to a random forest implementation to examine the potential for deep learning to advance the more commonly used random forest approach for wetland classification e g o neil et al 2019 we compared the performance of the site specific deep learning models to a random forest classification with the same set of input variables the random forest implementation follows the approach of o neil et al 2019 but with the addition of the ndvi to the original set of inputs the twi curvature and dtw the training sampling used in the o neil et al 2019 study was maintained where training data consists of randomly dispersed pixels that encompass only 15 of the validated wetland area and up to 8 of the validated nonwetland area however accuracy assessments for both the deep learning and random forest models were limited to the extents of the testing image tiles that correspond to the deep learning approach compared to the site 1 deep learning model the random forest classification resulted in an improvement in recall from 81 to 91 but a decrease in precision from 25 to 19 for site 2 random forest improved recall considerably from 28 to 78 and slightly improved precision from 3 to 5 the site 3 random forest model produced no change in recall 85 and a slight decrease in precision 20 vs 18 compared to deep learning finally the site 4 random forest model considerably decreased recall from 91 to 70 and increased precision from 56 to 64 relative to the deep learning model with the exception of site 2 these findings show that deep learning was able to perform similarly to random forests e g site 1 and site 3 and arguably better in some cases e g site 4 the poor performance in site 2 further supports that the deep learning model was not sufficiently able to learn characteristics of wetland features that were very small and sparse relative to the landscape scenes in each training image similarly the site 4 results again support the idea that deep learning is better suited to detecting wetlands where they are areal and large relative to the landscape scene in addition an evaluation of the entire testing areas corresponding to the random forest models shows that the inclusion of the ndvi as a wetland indicator improves on the o neil et al 2019 approach compared to the random forest models using only the topographic inputs the addition of the ndvi improved wetland recall and precision in site 1 81 vs 88 and 19 vs 24 site 2 82 vs 88 and 16 vs 22 site 3 83 vs 86 and 22 vs 25 and site 4 58 vs 68 and 47 vs 54 overall it is important to note that the random forest models were able to achieve these accuracies by sampling much less training data than was required for deep learning models however this result also shows that deep learning models can approach the same accuracies using training data resources that are considerably smaller relative to most deep learning applications in addition the similar performance of deep learning to random forests in three of the study sites supports findings by other researchers that state deep learning can improve landscape segmentation accuracy over traditional machine learning such as support vector machine maximum likelihood classification and random forests given enough training data e g hu et al 2018 latifovic et al 2018 liu et al 2018 mahdianpari et al 2018 4 5 limitations limitations of this approach could be addressed through additional research for example incorporating class activation mapping cam zhou et al 2016 which highlights scene elements that are most influential during classifications would offer further insight into model learning by utilizing cam model refinements could be made by quantifying the impact of the input data and identifying sources of error considering additional remote sensing data may also improve model performance these may include lidar point clouds which researchers have incorporated into 3 dimensional cnns for wetland identification e g xu et al 2018 also incorporating radar data may reduce errors where the ndvi is affected by tree canopy as it is able to penetrate this layer and provide vegetation density and inundation information for wetland mapping allen et al 2013 behnamian et al 2017 corcoran et al 2013 kloiber et al 2015 millard and richardson 2013 also on this point the contribution of each input data source throughout the deepnets workflow can be handled in a more sophisticated way this was demonstrated by audebert et al 2018 who proposed novel data fusing methods for elevation data and the ndvi within the deepnets workflow to improve land cover classifications additional training information that consists of accurately delineated wetlands from across different ecoregions should improve the deep learning classification results also additional training data would make it possible to train models for specific wetland types rather than a simple binary wetland nonwetland classification these training data are likely available from state and federal agencies given the need for wetland assessments under the clean water act but are not collected into a single standardized repository future work could focus on building such a training and testing repository for wetland classification furthermore to more efficiently make use of any amount of reliable training information available applying more sophisticated data augmentation techniques may improve wetland predictions as demonstrated by stivaktakis et al 2019 refinements to the current approach should also include more robust accuracy assessments the current accuracy metrics are transparent and represent the two factors that are needed for reliable implementation coverage of ground truth wetlands and limited overprediction however a single accuracy metric that encompasses both of these factors while also acknowledging the significantly higher importance of wetland recall would improve the interpretation of model results model evaluation improvements should also take into account the diffuse boundaries of wetlands which may fluctuate seasonally by penalizing overprediction less if it occurs adjacent to or surrounding defined ground truth wetland extents lastly this study did not test the effect of tuning the deepnets parameters among other parameter adjustments future work should explore the benefit of adjusting window sizes based on target wetland size and the accuracy tradeoffs when training the model for more epochs 5 conclusions we explore a wetland identification workflow that implements a basic semantic segmentation architecture and an input data configuration that consists of the ndvi and lidar dem derived indicators of wetland hydrology and geomorphology the workflow was trained and evaluated using available data resources from four geographic regions of virginia from this work we draw the following conclusions i site specific deep learning models created from relatively small training datasets can achieve accurate results for three of the four study sites wetland recall ranged from 81 to 91 and precision ranged from 20 to 56 when training models with 70 of site area and testing on the remaining 30 of the site area ii site specific models were more successful for areas where wetlands are abundant and occupy a significant portion of training images for a site with large areal wetlands that were almost evenly balanced with nonwetland areas high accuracy was achieved with 7 5 km2 70 of training area 91 recall and 56 precision using a much smaller training area 0 4 km2 10 of the study area still resulted in a fairly accurate model 84 recall and 50 precision iii in most cases accuracy decreased when using models trained for another site however the site specific model trained with the largest area studied 7 5 km2 increased wetland recall in all other sites although model predictions were imprecise and showed a bias towards the types of wetlands for which it was trained i e large areal wetlands the correct localization of wetland predictions suggests there is potential for this approach if models are trained with sufficient data and for areas with similar landscapes iv combined site models can produce accurate wetland predictions but training data contributions from the target landscapes should be balanced the general model revealed the potential for bias towards landscape characteristics more heavily represented in the training data however the influence of less represented sites was still apparent as wetland predictions were more inclusive of different wetland types compared to a model created without training data from these sites v shared ecoregion alone may not offer sufficient landscape similarities to improve the training sampling approach for combined site models the ecoregion model showed accuracy improvements from the general model for one site however wetland predictions for the other site were less accurate future work should explore the benefit of creating combined site models from areas that share additional characteristics that would affect the distributions of the topographic derivatives such as level of development land cover and topography vi the proposed input data configuration improves wetland identification compared to a more typical approach of using the ndvi and the lidar dem alone by predetermining the derivatives of the dem that are wetland indicators based on physical understanding of hydrology and wetland formation rather than allowing the deep learning network to determine these through convolutions on raw data wetland predictions were more accurate in three sites this speaks to the benefit and power of combining physical understanding along with machine and deep learning algorithms for improved predictive skill for the remaining site accuracy was nearly unchanged between the two approaches however analyses show that this is likely due to the greater importance of the ndvi for identifying wetlands in the topographically mild landscape vii compared to a random forest approach the best performing models produced comparable accuracy using more training data than required for random forest but still significantly less than what is typical in most deep learning applications our results demonstrate the potential for deep learning to not only improve accuracy compared to traditional machine learning algorithms but also provide flexible models that are accurate for a range of landscapes paramount to achieving this will be larger efforts within the research community to gather reliable training data and pretrained models stored as open source repositories as has been done for established deep learning fields e g lecun 1999 lin et al 2014 the wetland models created through this research may offer a starting point for creating a repository open to other researchers by refining this implementation of the deep learning wetland workflow and further training the created models there is potential for deep learning to support a range of wetland conservation efforts by producing accurate wetland inventories across many landscapes software and data availability software created through this research along with documentation is available under an mit license from https github com uva hydroinformatics wetland id all input data required to run the model are publicly available through federal and state data providers wetland delineation datasets used for training and evaluation was made available to the researchers through a relationship with the virginia department of transportation to retrain the model for a new landscape similar wetland delineation data for that area may be required declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors wish to thank dr nicolas audebert and his co authors for creating the deepnets architecture used here and making their work publicly available for other researchers to use and build from in addition thank you to the virginia department of transportation for providing the validation data required to complete this research this work was funded by a graduate assistance in areas of national need gaann fellowship through grants provided by the united states department of education grant numbers p200a180035 and p200a160322 
