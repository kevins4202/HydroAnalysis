index,text
26185,growing demands for geospatial application of environmental models have led to tool development for conducting simulations spatially the model independent open source tool geospatial simulation geosim has been developed previously based on previous applications at field scale this study advances geosim application for national scale and multi year simulations the widely applied aquacrop model was implemented by geosim to simulate wheat yield and irrigation requirements on a daily step across china from 2000 to 2009 the spatial inputs required by aquacrop were minimized and 6915 unique response units were identified among the primary 116 801 polygons it took around 20 h to perform the 10 year simulations post processing of simulation outputs permitted mapping at the original 5 arc minute resolution the novel methods developed in this study demonstrate new opportunities for efficiently managing national scale and multi year simulations with high resolution they render aquacrop more suitable for studies on the water food nexus at large scales which are more policy relevant keywords geospatial simulation aquacrop high spatiotemporal resolution water consumption yield 1 introduction over the past decades applications of agroecosystem models have rapidly expanded and the models have been applied in numerous areas such as resource use and efficiency food security and environmental performance holzworth et al 2015 there are more than 100 crop models available and they have been applied for the simulation of about 150 crops or land uses rivington and koo 2010 with the expanding application domain models are increasingly applied at diverse spatiotemporal scales bryan 2013 folberth et al 2012 liu 2009 zhao et al 2013 this has required either spatial simulation capability to be added to the models or constructing model wrappers which can provide this capability in recent years progress has been made on the development of geographic information systems gis to handle geo processing tasks store geo spatial data manage input and output data from the simulation model and visualize the results spatially bryan 2013 holzworth et al 2015 thorp and bronson 2013 thorp et al 2008 a few agroecosystem models have been improved as gis based modelling systems allowing simulations at multiple locations to be run simultaneously with dynamic interactions e g apollo dssat for dssat thorp et al 2008 gepic for epic liu 2009 and grid parallel apsim for apsim zhao et al 2013 however many gis based modelling systems have been developed for a specific application or model these systems are model dependent and are useful only for limited purposes in addition some systems which may be proprietary computer platform dependent now obsolete or cost prohibitive are not available for all users thorp and bronson 2013 to avoid these problems thorp and bronson 2013 developed a tool called geospatial simulation geosim which is a model independent open source system for managing point based model simulations at multiple locations the tool has been demonstrated on the widely applied aquacrop and dssat models memic et al 2018 thorp and bronson 2013 although geosim does not restrict model applications to any specific spatial scale the past applications of geosim were all conducted at the field scale to date no study has demonstrated geosim for broader spatial scales although it can potentially be applied as such there is a growing demand for large scale and high resolution modelling of crop yields and water consumption as the concerns for food and water security continue to increase under global change mcneill et al 2017 to demonstrate geosim s applicability to satisfy such demand this study used geosim to implement the widely applied fao aquacrop model for simulating wheat yield and irrigation water requirements across china from 2000 to 2009 other tools such as aquacrop gis lorite et al 2013 and aquacrop os foster et al 2017 have also been designed to implement aquacrop for multiple simulations however these tools work with old aquacrop versions and have limitations such as low simulation efficiency or need for programing skills which can constrain the model applicability the objective of this study was 1 to demonstrate that geosim can efficiently work with aquacrop for modelling at large spatiotemporal scales with high resolution and 2 to guide users on how to explore the flexibility of geosim on setting up the simulations to improve efficiency the outputs of national wheat yield and irrigation water requirements can be further applied to examine implications for an integrated management of water resources and food production in china while beyond the scope of this study geosim also has the potential to work with other models following a similar approach as such it facilitates addressing global challenges such as food security and environmental security by efficiently extending models for spatial simulations 2 methods 2 1 geosim and aquacrop geosim was designed to facilitate spatial simulations for any point based model which uses ascii files for input and output thorp and bronson 2013 it was developed as a plug in for quantum gis qgis https www qgis org and both of these software programs are open source and freely available geosim provides an interface to run point based models using geospatial data contained in a qgis database six tools are currently available in geosim these tools enable the system to prepare and process the required shapefile to manage input and output data for the polygons in that shapefile and to optimize model parameters to minimize errors fao s aquacrop http www fao org aquacrop is a water driven dynamic model which derives biomass gain as a function of water consumption under rain fed or different irrigation conditions on a daily step raes et al 2009 steduto et al 2009 the model performance has been widely tested for numerous crops under diverse environments and agricultural production systems around the world vanuytrecht et al 2014 in order to evaluate the model performance and judge the suitability of the model for water and food security research field experiments were conducted to calibrate and validate the aquacrop model for wheat and maize cropping systems in china han et al 2019 it showed reasonable agreement between simulated crop yields and observed yields indicating that the calibrated aquacrop model is robust at reproducing potential yields under full irrigation han et al 2019 2 2 pre processing of spatial inputs to present a national case study of a 10 year simulation relevant data for the simulations of wheat yield and irrigation water requirement were collected for the period from 2000 to 2009 in which all the required data are available six types of input data were used 1 crop distribution 2 climate data 3 crop parameters 4 soil parameters 5 initial soil water conditions and 6 management data all the unique input files required by aquacrop were prepared separately prior to conducting simulations with geosim a shapefile vector data of national wheat distribution was converted from a raster dataset with a resolution of 5 arc minutes in the year 2000 section 1 1 in appendix a climate files were prepared according to the aquacrop formats based on daily data from 825 meteorological stations provided by the national meteorological information centre nmic http data cma cn the coordinates of the meteorological stations were used to generate a shapefile containing the station codes section 1 2 in appendix a the default file for atmospheric co2 concentration in the aquacrop database was used two crop types winter and spring wheat were distinguished by the data obtained from the nmic and were contained in the attribute table of the shapefile with meteorological station codes crop files for both spring and winter wheat containing numerous parameters were prepared according to the aquacrop formats section 1 3 in appendix a day numbers which indicate the first and last days of cropping and simulation periods were calculated for china s 41 agro ecological zones aez and included in the attribute table of the aez shapefile fig a3 in appendix a the crop type which indicates whether the crop was spring or winter wheat was also included in the aez shapefile a soil shapefile containing 39 soil codes was prepared with national coverage section 1 4 in appendix a furthermore 39 soil files containing soil hydraulic parameters for each soil code in the shapefile were prepared according to the aquacrop formats initial soil water contents were assumed to be at field capacity section 1 5 in appendix a to model the irrigation water requirement of wheat under full irrigation the determination of net irrigation requirements option was selected in aquacrop to create an irrigation file section 1 5 in appendix a other management effects such as fertilizer application and ground surface cover were disregarded groundwater was not considered due to lacking detail in the national dataset all the prepared climate crop soil irrigation and initial condition files were stored in a directory can be several directories the flexibility of geosim permits a user to pass any spatial input data to the model files such as climate data and soil parameters however to minimize the input spatial variables this case study applied geosim by passing spatial data only to aquacrop s project file prm which controls aquacrop simulations fig 1 the directory or directories where input files were stored in was specified in aquacrop s project file prm then geosim was used to pass only the names of these files as spatial variables to the project file rather than passing the specific numerous parameters for each input file to further reduce the spatial variables the climate files for each simulation were named as the same code of the meteorological station from where the climate data were geosim passed a station code to different locations where the climate file names were required in the project file 2 3 identification of unique response units the shapefiles for wheat distribution meteorological station codes including crop types aez based day numbers and soil codes were used to create a base polygon shapefile required by geosim by intersecting these layers a new shapefile with 116 801 polygons was generated however efficiency of spatial simulations could be improved by combining some of these polygons for example because the wheat distribution information was only used to indicate the location of wheat cropping this spatial attribute information did not need to be passed to aquacrop the only required attributes for geospatial aquacrop simulations as managed by geosim were meteorological station codes wheat types soil codes and day numbers as some polygons shared these attributes in common there was opportunity to eliminate redundant aquacrop simulations to identify the unique response units which were defined as polygons with unique combination of meteorological station code crop type soil code and day numbers the dissolve tool in qgis was applied to merge polygons with the same attribute values finally only 6915 polygons remained in the base shapefile to be modelled fig 2 a this demonstrates an advantage of using geosim within qgis by using qgis tools to identify identical spatial zones the efficiency of simulations managed by geosim could be improved the input information for each polygon were appended in the different columns of the attribute table fig 2b while 20 blank columns two outputs with 10 years were set up to receive output data fig 2c 2 4 template instruction and control files in this study the template file gst is a replicate of the aquacrop s project file prm within the template file unique codes were included at the locations of day numbers and the names of climate crop and soil files as this case study conducted simulations in 10 years there were 10 sections indicating the simulation for each year in the template file accordingly there were 20 lines of commands in the instruction file gsi to retrieve the irrigation and yield results for each of the 10 years the names of the attributes in the base shapefile and their corresponding unique codes in the template file were provided in the control file gsc excerpts from the template instruction and control files are presented in appendix a further information for developing these geosim files can be found in thorp and bronson 2013 and the geosim manual after setting up the geosim files geosim conducted the simulations for all the polygons in the base shapefile 2 5 post processing of spatial outputs as the model completed the simulations results for wheat yield and irrigation requirement in each of the 10 years for each polygon were transferred from the model output files to the attribute table of the base shapefile then the vector geoprocessor tool within geosim was applied to calculate the mean values of irrigation and yield from 2000 to 2009 and the results were appended to the base shapefile to disaggregate the unique response units back to the original resolution we intersected the base shapefile with the primary wheat distribution layer this new shapefile contained 116 801 polygons as opposed to 6915 polygons used during the modelling the large number of spatial units equaled those resulting from the intersection of wheat distribution meteorological station codes including crop types aez based day numbers and soil codes layers section 2 3 and also contained all the result attributes the yield and irrigation attributes in the new shapefile were then converted to raster datasets with a resolution of 5 arc minutes by pairing native qgis tools with geosim to improve the efficiency of spatial simulations a novel method to develop national scale maps of wheat yield and irrigation requirements was demonstrated 3 results and discussion 3 1 case study the total running time to complete 69 150 simulations 6915 polygons with 10 years was about 20 h with single core processing the grid based results of wheat yield and irrigation requirement with a resolution of 5 arc minutes are presented in fig 3 depending on the research goals further analysis and application of these maps can provide implications for decision making related to water management food security and land use management the modelling quality depends on the input data and aquacrop model algorithms rather than geosim because geosim serves to help users manage the geospatial model inputs and outputs it does not change any input data or model algorithms within aquacrop an in depth analysis of the model outputs is beyond the scope of this paper by spatially applying aquacrop using geosim the detailed spatial and temporal variability of crop yield and irrigation requirements was easily demonstrated across china 3 2 advancement of geosim application previous applications of geosim at the field scale used the tool to pass spatial input data to aquacrop s soil file initial soil water file and irrigation files thorp and bronson 2013 however for large spatiotemporal simulations at the national scale a simpler approach was to pass spatial input data only to aquacrop s project file by setting up all of aquacrop s input files in a specific directory geosim was used to pass only four types of spatial variables meteorological station codes crop types soil codes and day numbers to the project file although geosim can be used to pass any spatial input data required by aquacrop passing data to the climate and soil files would have resulted in a massive and cumbersome base shapefile and required several template files to receive these data by directly incorporating spatial input data into unique climate and soil files and using geosim to adjust file names assigned to each spatial zone this study demonstrated an efficient way to use a base shapefile with few columns and used only one template file thus the effort for pre processing the geosim files was substantially reduced it also demonstrated the flexibility of geosim to be used in numerous different ways depending on the goals of simulation analysis to reduce the modelling time 6915 unique response units were identified in the base shapefile among the 116 801 primary polygons because qgis allowed the post processing of outputs the original resolution of the analysis could be regained by conducting simulations only for the unique spatial zones and subsequently expanding the simulation results to the primary zones simulation efficiency was improved by over 16 times while the primary spatial resolution was not changed these pre processing and post processing activities demonstrated the advantage and flexibility of using geosim in combination with native qgis functionality to achieve efficient simulation analyses at the national scale 3 3 comparison with alternative aquacrop wrappers compared with aquacrop gis lorite et al 2013 which is currently recommended by fao for a high number of simulations http www fao org aquacrop geosim significantly reduced the simulation time aquacrop gis uses an excel spreadsheet to control aquacrop and it creates all the possible combinations of inputs for each polygon thereby aquacrop gis wastes time by doing irrelevant simulations for example it will do 5 393 700 simulations 6915 polygons 39 soils 2 wheat types 10 years when applied for our case study in contrast geosim only passes polygon information to aquacrop that is of interest for the study therefore geosim required only 69 150 aquacrop simulations almost 80 times less moreover it may be impossible for a modern desktop computer to complete all the 5 393 700 simulations with aquacrop gis in a reasonable time it would also be time consuming to split the work into several parts also selecting targeted results from numerous output files would be cumbersome and time consuming with aquacrop gis but geosim automates this foster et al 2017 developed the aquacrop os model which also facilitates large numbers of aquacrop simulations however this model requires users to have programming experience and it has its own format requirements for input files and users must define separate sets of input files for each simulation run unlike aquacrop os geosim does not change the formats of any input files required by aquacrop all geosim does is to help users manage the geospatial model inputs and outputs within qgis a user who knows how to conduct the stand alone aquacrop simulations can easily learn to apply geosim for geospatial simulations 4 conclusions by using native qgis geo processing functions with geosim the efficiency of aquacrop for conducting national scale simulations was improved the flexibility of geosim permits its implementation in numerous different ways depending on the goals of the analysis other tools exist for spatial extension of aquacrop but none could perform with the ease and efficiency of geosim this study demonstrated a novel approach to apply geosim for large scale spatiotemporal simulations with high resolution which renders aquacrop more valuable for studying the national water food nexus geosim is a model independent tool and it can always work with the latest version of the model also as geosim was developed as an open source software users can improve or expand the source code for customized purposes declarations of interest none acknowledgements this work was supported by china s national key research and development program grant number 2016yfd0300210 and the longshan academic talent research supporting program of swust grant numbers 18lzxt06 and 18lzx449 jing huang is grateful for the scholarship she received from the china scholarship council grant number 201808510050 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 003 
26185,growing demands for geospatial application of environmental models have led to tool development for conducting simulations spatially the model independent open source tool geospatial simulation geosim has been developed previously based on previous applications at field scale this study advances geosim application for national scale and multi year simulations the widely applied aquacrop model was implemented by geosim to simulate wheat yield and irrigation requirements on a daily step across china from 2000 to 2009 the spatial inputs required by aquacrop were minimized and 6915 unique response units were identified among the primary 116 801 polygons it took around 20 h to perform the 10 year simulations post processing of simulation outputs permitted mapping at the original 5 arc minute resolution the novel methods developed in this study demonstrate new opportunities for efficiently managing national scale and multi year simulations with high resolution they render aquacrop more suitable for studies on the water food nexus at large scales which are more policy relevant keywords geospatial simulation aquacrop high spatiotemporal resolution water consumption yield 1 introduction over the past decades applications of agroecosystem models have rapidly expanded and the models have been applied in numerous areas such as resource use and efficiency food security and environmental performance holzworth et al 2015 there are more than 100 crop models available and they have been applied for the simulation of about 150 crops or land uses rivington and koo 2010 with the expanding application domain models are increasingly applied at diverse spatiotemporal scales bryan 2013 folberth et al 2012 liu 2009 zhao et al 2013 this has required either spatial simulation capability to be added to the models or constructing model wrappers which can provide this capability in recent years progress has been made on the development of geographic information systems gis to handle geo processing tasks store geo spatial data manage input and output data from the simulation model and visualize the results spatially bryan 2013 holzworth et al 2015 thorp and bronson 2013 thorp et al 2008 a few agroecosystem models have been improved as gis based modelling systems allowing simulations at multiple locations to be run simultaneously with dynamic interactions e g apollo dssat for dssat thorp et al 2008 gepic for epic liu 2009 and grid parallel apsim for apsim zhao et al 2013 however many gis based modelling systems have been developed for a specific application or model these systems are model dependent and are useful only for limited purposes in addition some systems which may be proprietary computer platform dependent now obsolete or cost prohibitive are not available for all users thorp and bronson 2013 to avoid these problems thorp and bronson 2013 developed a tool called geospatial simulation geosim which is a model independent open source system for managing point based model simulations at multiple locations the tool has been demonstrated on the widely applied aquacrop and dssat models memic et al 2018 thorp and bronson 2013 although geosim does not restrict model applications to any specific spatial scale the past applications of geosim were all conducted at the field scale to date no study has demonstrated geosim for broader spatial scales although it can potentially be applied as such there is a growing demand for large scale and high resolution modelling of crop yields and water consumption as the concerns for food and water security continue to increase under global change mcneill et al 2017 to demonstrate geosim s applicability to satisfy such demand this study used geosim to implement the widely applied fao aquacrop model for simulating wheat yield and irrigation water requirements across china from 2000 to 2009 other tools such as aquacrop gis lorite et al 2013 and aquacrop os foster et al 2017 have also been designed to implement aquacrop for multiple simulations however these tools work with old aquacrop versions and have limitations such as low simulation efficiency or need for programing skills which can constrain the model applicability the objective of this study was 1 to demonstrate that geosim can efficiently work with aquacrop for modelling at large spatiotemporal scales with high resolution and 2 to guide users on how to explore the flexibility of geosim on setting up the simulations to improve efficiency the outputs of national wheat yield and irrigation water requirements can be further applied to examine implications for an integrated management of water resources and food production in china while beyond the scope of this study geosim also has the potential to work with other models following a similar approach as such it facilitates addressing global challenges such as food security and environmental security by efficiently extending models for spatial simulations 2 methods 2 1 geosim and aquacrop geosim was designed to facilitate spatial simulations for any point based model which uses ascii files for input and output thorp and bronson 2013 it was developed as a plug in for quantum gis qgis https www qgis org and both of these software programs are open source and freely available geosim provides an interface to run point based models using geospatial data contained in a qgis database six tools are currently available in geosim these tools enable the system to prepare and process the required shapefile to manage input and output data for the polygons in that shapefile and to optimize model parameters to minimize errors fao s aquacrop http www fao org aquacrop is a water driven dynamic model which derives biomass gain as a function of water consumption under rain fed or different irrigation conditions on a daily step raes et al 2009 steduto et al 2009 the model performance has been widely tested for numerous crops under diverse environments and agricultural production systems around the world vanuytrecht et al 2014 in order to evaluate the model performance and judge the suitability of the model for water and food security research field experiments were conducted to calibrate and validate the aquacrop model for wheat and maize cropping systems in china han et al 2019 it showed reasonable agreement between simulated crop yields and observed yields indicating that the calibrated aquacrop model is robust at reproducing potential yields under full irrigation han et al 2019 2 2 pre processing of spatial inputs to present a national case study of a 10 year simulation relevant data for the simulations of wheat yield and irrigation water requirement were collected for the period from 2000 to 2009 in which all the required data are available six types of input data were used 1 crop distribution 2 climate data 3 crop parameters 4 soil parameters 5 initial soil water conditions and 6 management data all the unique input files required by aquacrop were prepared separately prior to conducting simulations with geosim a shapefile vector data of national wheat distribution was converted from a raster dataset with a resolution of 5 arc minutes in the year 2000 section 1 1 in appendix a climate files were prepared according to the aquacrop formats based on daily data from 825 meteorological stations provided by the national meteorological information centre nmic http data cma cn the coordinates of the meteorological stations were used to generate a shapefile containing the station codes section 1 2 in appendix a the default file for atmospheric co2 concentration in the aquacrop database was used two crop types winter and spring wheat were distinguished by the data obtained from the nmic and were contained in the attribute table of the shapefile with meteorological station codes crop files for both spring and winter wheat containing numerous parameters were prepared according to the aquacrop formats section 1 3 in appendix a day numbers which indicate the first and last days of cropping and simulation periods were calculated for china s 41 agro ecological zones aez and included in the attribute table of the aez shapefile fig a3 in appendix a the crop type which indicates whether the crop was spring or winter wheat was also included in the aez shapefile a soil shapefile containing 39 soil codes was prepared with national coverage section 1 4 in appendix a furthermore 39 soil files containing soil hydraulic parameters for each soil code in the shapefile were prepared according to the aquacrop formats initial soil water contents were assumed to be at field capacity section 1 5 in appendix a to model the irrigation water requirement of wheat under full irrigation the determination of net irrigation requirements option was selected in aquacrop to create an irrigation file section 1 5 in appendix a other management effects such as fertilizer application and ground surface cover were disregarded groundwater was not considered due to lacking detail in the national dataset all the prepared climate crop soil irrigation and initial condition files were stored in a directory can be several directories the flexibility of geosim permits a user to pass any spatial input data to the model files such as climate data and soil parameters however to minimize the input spatial variables this case study applied geosim by passing spatial data only to aquacrop s project file prm which controls aquacrop simulations fig 1 the directory or directories where input files were stored in was specified in aquacrop s project file prm then geosim was used to pass only the names of these files as spatial variables to the project file rather than passing the specific numerous parameters for each input file to further reduce the spatial variables the climate files for each simulation were named as the same code of the meteorological station from where the climate data were geosim passed a station code to different locations where the climate file names were required in the project file 2 3 identification of unique response units the shapefiles for wheat distribution meteorological station codes including crop types aez based day numbers and soil codes were used to create a base polygon shapefile required by geosim by intersecting these layers a new shapefile with 116 801 polygons was generated however efficiency of spatial simulations could be improved by combining some of these polygons for example because the wheat distribution information was only used to indicate the location of wheat cropping this spatial attribute information did not need to be passed to aquacrop the only required attributes for geospatial aquacrop simulations as managed by geosim were meteorological station codes wheat types soil codes and day numbers as some polygons shared these attributes in common there was opportunity to eliminate redundant aquacrop simulations to identify the unique response units which were defined as polygons with unique combination of meteorological station code crop type soil code and day numbers the dissolve tool in qgis was applied to merge polygons with the same attribute values finally only 6915 polygons remained in the base shapefile to be modelled fig 2 a this demonstrates an advantage of using geosim within qgis by using qgis tools to identify identical spatial zones the efficiency of simulations managed by geosim could be improved the input information for each polygon were appended in the different columns of the attribute table fig 2b while 20 blank columns two outputs with 10 years were set up to receive output data fig 2c 2 4 template instruction and control files in this study the template file gst is a replicate of the aquacrop s project file prm within the template file unique codes were included at the locations of day numbers and the names of climate crop and soil files as this case study conducted simulations in 10 years there were 10 sections indicating the simulation for each year in the template file accordingly there were 20 lines of commands in the instruction file gsi to retrieve the irrigation and yield results for each of the 10 years the names of the attributes in the base shapefile and their corresponding unique codes in the template file were provided in the control file gsc excerpts from the template instruction and control files are presented in appendix a further information for developing these geosim files can be found in thorp and bronson 2013 and the geosim manual after setting up the geosim files geosim conducted the simulations for all the polygons in the base shapefile 2 5 post processing of spatial outputs as the model completed the simulations results for wheat yield and irrigation requirement in each of the 10 years for each polygon were transferred from the model output files to the attribute table of the base shapefile then the vector geoprocessor tool within geosim was applied to calculate the mean values of irrigation and yield from 2000 to 2009 and the results were appended to the base shapefile to disaggregate the unique response units back to the original resolution we intersected the base shapefile with the primary wheat distribution layer this new shapefile contained 116 801 polygons as opposed to 6915 polygons used during the modelling the large number of spatial units equaled those resulting from the intersection of wheat distribution meteorological station codes including crop types aez based day numbers and soil codes layers section 2 3 and also contained all the result attributes the yield and irrigation attributes in the new shapefile were then converted to raster datasets with a resolution of 5 arc minutes by pairing native qgis tools with geosim to improve the efficiency of spatial simulations a novel method to develop national scale maps of wheat yield and irrigation requirements was demonstrated 3 results and discussion 3 1 case study the total running time to complete 69 150 simulations 6915 polygons with 10 years was about 20 h with single core processing the grid based results of wheat yield and irrigation requirement with a resolution of 5 arc minutes are presented in fig 3 depending on the research goals further analysis and application of these maps can provide implications for decision making related to water management food security and land use management the modelling quality depends on the input data and aquacrop model algorithms rather than geosim because geosim serves to help users manage the geospatial model inputs and outputs it does not change any input data or model algorithms within aquacrop an in depth analysis of the model outputs is beyond the scope of this paper by spatially applying aquacrop using geosim the detailed spatial and temporal variability of crop yield and irrigation requirements was easily demonstrated across china 3 2 advancement of geosim application previous applications of geosim at the field scale used the tool to pass spatial input data to aquacrop s soil file initial soil water file and irrigation files thorp and bronson 2013 however for large spatiotemporal simulations at the national scale a simpler approach was to pass spatial input data only to aquacrop s project file by setting up all of aquacrop s input files in a specific directory geosim was used to pass only four types of spatial variables meteorological station codes crop types soil codes and day numbers to the project file although geosim can be used to pass any spatial input data required by aquacrop passing data to the climate and soil files would have resulted in a massive and cumbersome base shapefile and required several template files to receive these data by directly incorporating spatial input data into unique climate and soil files and using geosim to adjust file names assigned to each spatial zone this study demonstrated an efficient way to use a base shapefile with few columns and used only one template file thus the effort for pre processing the geosim files was substantially reduced it also demonstrated the flexibility of geosim to be used in numerous different ways depending on the goals of simulation analysis to reduce the modelling time 6915 unique response units were identified in the base shapefile among the 116 801 primary polygons because qgis allowed the post processing of outputs the original resolution of the analysis could be regained by conducting simulations only for the unique spatial zones and subsequently expanding the simulation results to the primary zones simulation efficiency was improved by over 16 times while the primary spatial resolution was not changed these pre processing and post processing activities demonstrated the advantage and flexibility of using geosim in combination with native qgis functionality to achieve efficient simulation analyses at the national scale 3 3 comparison with alternative aquacrop wrappers compared with aquacrop gis lorite et al 2013 which is currently recommended by fao for a high number of simulations http www fao org aquacrop geosim significantly reduced the simulation time aquacrop gis uses an excel spreadsheet to control aquacrop and it creates all the possible combinations of inputs for each polygon thereby aquacrop gis wastes time by doing irrelevant simulations for example it will do 5 393 700 simulations 6915 polygons 39 soils 2 wheat types 10 years when applied for our case study in contrast geosim only passes polygon information to aquacrop that is of interest for the study therefore geosim required only 69 150 aquacrop simulations almost 80 times less moreover it may be impossible for a modern desktop computer to complete all the 5 393 700 simulations with aquacrop gis in a reasonable time it would also be time consuming to split the work into several parts also selecting targeted results from numerous output files would be cumbersome and time consuming with aquacrop gis but geosim automates this foster et al 2017 developed the aquacrop os model which also facilitates large numbers of aquacrop simulations however this model requires users to have programming experience and it has its own format requirements for input files and users must define separate sets of input files for each simulation run unlike aquacrop os geosim does not change the formats of any input files required by aquacrop all geosim does is to help users manage the geospatial model inputs and outputs within qgis a user who knows how to conduct the stand alone aquacrop simulations can easily learn to apply geosim for geospatial simulations 4 conclusions by using native qgis geo processing functions with geosim the efficiency of aquacrop for conducting national scale simulations was improved the flexibility of geosim permits its implementation in numerous different ways depending on the goals of the analysis other tools exist for spatial extension of aquacrop but none could perform with the ease and efficiency of geosim this study demonstrated a novel approach to apply geosim for large scale spatiotemporal simulations with high resolution which renders aquacrop more valuable for studying the national water food nexus geosim is a model independent tool and it can always work with the latest version of the model also as geosim was developed as an open source software users can improve or expand the source code for customized purposes declarations of interest none acknowledgements this work was supported by china s national key research and development program grant number 2016yfd0300210 and the longshan academic talent research supporting program of swust grant numbers 18lzxt06 and 18lzx449 jing huang is grateful for the scholarship she received from the china scholarship council grant number 201808510050 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 003 
26186,an integrated framework of surface and groundwater models is essential for the comprehensive understanding of impacts of droughts due to surface water and subsurface exchanges we evaluate the response of soil moisture and groundwater dynamics during drought in the northern atlantic coastal plain in the chesapeake bay watershed the variable infiltration capacity model coupled with modflow vicmf was implemented and several variables were used in the multivariate standardized drought index msdi that consider multivariate perspectives of droughts three drought indices were derived msdi psv msdi psm msdi pwm psv precipitation and soil moisture from vic psm precipitation and soil moisture from vicmf pwm precipitation and wte from vicmf and the accuracy of the results was verified using a performance measure drought area agreement da and a statistical test to evaluate spatial extent of the drought areas the msdi pwm showed better results for predicting drought events as it captured overall drought conditions 1 introduction a comprehensive understanding of the hydrologic cycle and watershed processes in the terrestrial environment is essential for water budget estimation and sustainable water management under changing weather and climate extremes specifically interactions between surface water and groundwater in the watersheds where the exchanges are dominant can have significant impacts on water resources watershed management and nutrient loadings bailey et al 2016 however the exchanges between these systems have been analyzed without coupling and many models have considered improving the exchanges in a vertical direction niu et al 2007 liang et al 2003 to give more accurate evaluations of these interactions a fully coupled model is necessary to provide a better understanding of both surface and groundwater conditions across multiple spatial and temporal scales numerous techniques and different approaches are available to investigate the patterns of groundwater and surface water interactions at various spatiotemporal scales panday and huyakorn 2004 bailey et al 2016 sridhar et al 2018 these techniques and advanced tools provide opportunities to explain the effects of these interactions on water availability hydrologic processes and drought conditions including the rates of evapotranspiration surface runoff soil moisture and groundwater level sridhar et al 2018 developed a new coupled framework using variable infiltration capacity vic liang et al 1994 and modflow models referred to as vicmf hereafter to investigate the influence of groundwater dynamics on water balance components and assess how changing hydrology alters recharge and thus groundwater levels the vic model can provide downward flux generated from the precipitation events and estimates surface atmospheric fluxes using atmospheric forcing soil and vegetation data over the land surface under certain conditions where the contribution of groundwater is adding moisture in the vadose zone the subsurface water movement can be significant as there is upward flux from below this can be useful for estimating the soil moisture in addition the vic model includes multiple soil layers with variable infiltration rates and a routing module that uses the linear transfer function model is available to simulate the streamflow in the river network system however the vic model does not estimate hydrologic dynamics related to groundwater or groundwater surface water interactions because it is applied in an uncoupled surface hydrologic model framework thus the coupled vicmf model was developed for a more accurate understanding of the physical processes in surface groundwater interactions hydrologic models have applied water and energy balance approaches and they are beneficial for addressing spatial distributions of water resources and drought conditions however an application of only surface hydrologic model does not capture integrated drought conditions of surface and groundwater dynamics at once oki and kanae 2006 the integrated framework somewhat complements these shortcomings of drought evaluations in addition drought indices are used for evaluating drought severity frequency and impact niemeyer 2008 as well as for improved drought prediction kang and sridhar 2018 sehgal and sridhar 2019 sehgal et al 2017 sehgal et al 2018 numerous drought indices have been applied to assess drought conditions and the multivariate standardized drought index msdi which is calculated using multiple hydrometeorological variables such as precipitation soil moisture and runoff is used to identify retrospective droughts in the united states us hao and aghakouchak 2013 2014 since the msdi can consider multiple hydrometeorological variables msdi applications enable us to analyze various aspects of droughts integrating surface and groundwater dynamics in this study the msdi was applied to represent multivariate perspectives on drought conditions with various combinations of variables e g soil moisture precipitation water table elevation wte the northern atlantic coastal plain nacp aquifer system is ranked the 13th in total groundwater withdrawals in the us reilly et al 2008 the supply of surface water in this region is limited because of numerous sources of the surface waters are brackish estuaries despite sufficient precipitation more than 1100 mm year besides many populations in the nacp rely heavily on groundwater to satisfy their water demand but water availabilities in the nacp region have been threatened by droughts available drawdowns agricultural demands and industrial contaminations masterson et al 2013 thus an accurate drought assessment that considers both surface and groundwater conditions would be useful for the nacp region the objectives of this study are 1 to identify the physical relationship between surface and groundwater variables such as soil moisture and wte during drought events using the coupled vicmf model and 2 to evaluate the drought conditions using various drought indices in the chesapeake bay watershed including the northern atlantic coastal plain nacp aquifer system since the msdi calculation with groundwater condition is the first attempted approach the results of this study would contribute to generating valuable information on surface and groundwater drought conditions with a dynamically coupled model integrated surface and groundwater systems provide new insights into soil moisture water table elevation baseflows and evapotranspiration gosselin et al 2006 jaksa and sridhar 2015 sridhar et al 2018 2 methods 2 1 study area and descriptions of groundwater model the nacp aquifer system in the chesapeake bay watershed occupies an area of approximately 28 200 km2 between latitude 36 40 and 39 45 n and longitude 77 30 and 79 15 w fig 1 the climate is temperate and humid with annual precipitation of 1020 mm and large metropolitan areas are included along with coastal areas such as washington d c and richmond in addition the nacp aquifer provides a widely used groundwater supply due to its thickness and large areal extents masterson et al 2016a the us geological survey usgs developed a groundwater flow model modflow nwt for the nacp aquifer system masterson et al 2016a 2016b this model provides a detailed evaluation of the groundwater availability of the nacp aquifer system the nacp aquifer system is hundreds of meters thick along the coastline with a maximum thickness of about 3000 m the aquifer system includes nine confined aquifers and nine confining units population and changes in land use during the last few decades have led to various increased freshwater demands for the 20 million people in the region water table elevations are decreasing by about 0 6 m year that results in large changes and difficult in water use and management of aquifer resources total groundwater withdrawal in 2013 was computed to be about 4 9 million cubic meters per day mm3 day and it provides 40 percent of the drinking water supply in highly populated regions precipitation amount for 2005 to 2009 is about 234 8 mm3 day but almost 70 percent is lost by evapotranspiration that led to 74 5 mm3 day entering the groundwater system as aquifer recharge masterson et al 2016a 2016b in this study only some parts of the nacp area chesapeake bay region will be considered for the coupled model as it is important to include the effect of upward flux and recharge estimates fig 1 masterson et al 2016a and masterson et al 2016b provide some packages and input parameters for the modflow simulation mcdonald and harbaugh 1988 in the nacp region since the original spatial resolution of modflow was 1 mile 1 6 km a spatial downscale method kriging was applied for the coupled model 1 km the required parameters and packages for the vicmf model were the horizontal and vertical hydraulic conductivities specific storage specific yield general head boundary ghb package recharge rch package river riv package and well wel package the drain drn package is also required to simulate the vicmf model and the drain elevation and conductance for the drn package are calculated based on equations 1 and 2 1 c k a x 1 x 0 2 d e s e s l where c is conductance l 2 t k is hydraulic conductivity l t a is the area l 2 x is the position l when head is measured d e is the drain elevation l s e is the surface elevation l and s l is the depth of the soil layers l from the vic model hildreth 2013 sridhar et al 2018 2 2 vic and modflow model the vic model is a physically based macroscale hydrologic model that includes a water and energy balance framework liang et al 1994 the model estimates several hydrologic and atmospheric variables such as soil moisture surface runoff baseflow and evapotranspiration and applies a conceptual system to demonstrate water and surface energy budgets furthermore the vic model includes multiple soil layers with variable infiltration rates and a routing module that uses the linear transfer function model is available to simulate the streamflow in the river network system these essential characteristics of the vic model provide an opportunity to couple the land surface scheme with a groundwater model for accurate descriptions of the entire hydrologic budgets however the model is applied as a land surface hydrologic model that does not estimate hydrologic dynamics with groundwater or groundwater surface water interactions thus a coupled framework is required to provide an understanding of the physical mechanisms of droughts considering both surface and groundwater conditions simultaneously the modular three dimensional finite difference ground water flow model modflow 2005 is a physically based three dimensional and distributed finite difference groundwater model that considers aquifer groundwater levels and routes groundwater flow in the system harbaugh 2005 also the coupled vicmf model is another variant of modflow that is used for this study with the modflow model estimations of groundwater recharge pumping vadose zone percolation discharge to subsurface drains and river aquifer interactions are available however modflow does not estimate the surface water budget components such as soil moisture and surface runoff bailey et al 2016 the vicmf coupled model was developed for a more accurate understanding of physical processes with surface groundwater interactions and the coupled model was successfully applied to the eastern snake plain aquifer espa region of snake river basin srb sridhar et al 2018 2 3 vic modflow vicmf coupled model the coupled vicmf model hildreth 2013 sridhar et al 2018 is integrated with surface and groundwater inputs in the nacp aquifer system region where the chesapeake bay is located fig 1 the vicmf model is based on the interdependent equations that estimate the flow of water in a surface and groundwater hydrologic system fig 2 the groundwater area includes the modflow applications in the nacp region and it estimates upward flux to surface zones soil layer the drainage package is used to calculate upward flux discharge from the modflow grid cells for the daily stress period which are integrated into a corresponding grid of the vic model for the daily simulation the amount of discharge u f into the unsaturated vic soil layers that are generated by the drain package is represented by equation 3 3  t z d  z k  z u f where  is the soil moisture content l 3 l 3 d  is the hydraulic diffusivity l 2 t k  is the hydraulic conductivity l t and z is the soil depth l the surface area consists of vegetation and soil layers vic model which combines the upward flux from the groundwater area in addition the vic model simulates infiltration that is recharged to groundwater zones finally the modflow model combines the recharge to simulate groundwater dynamic for the following day fig 2 the whole process of the coupled model generates wte soil moisture and energy fluxes the horizontal discretization of the groundwater model is designed to be consistent with the surface discretization and the discharges of the groundwater model improve the estimation of soil moisture by the surface model modflow packages such as the recharge drain well river and ghb general head boundary enable to drive the coupled model with the meteorological forcings the vic model was applied at a 1 16th degree resolution 0 0625 5 km with atmospheric forcing data daily precipitation maximum and minimum temperatures and wind speed and there were 50 rows and 38 columns the grid cells for the groundwater model downscaled as 250 rows and 190 columns and each cell is uniformly spaced at 1 km which is five times finer than the resolution of a vic grid cell thus the 25 grid cells 5 5 of the groundwater model were aggregated as a mean value for dynamic exchanges of the coupled model fig 2 in addition the nacp grid cells were realigned to match the vic model grid cells since the grid cells in the nacp model are aligned and not geographically directed north to south 50 727 from horizontal finally since the groundwater model was created as a daily basis a specific work for the temporal aggregation for the coupling was not required the daily outputs e g soil moisture wte were aggregated on a weekly time step for computations of drought indices other vic inputs such as soil and vegetation data were obtained from the ldas http ldas gsfc nasa gov at the same spatial resolution as the meteorological data moreover this study utilized the modflow parameters from the nacp groundwater model masterson et al 2016a b including the hydraulic conductivity the groundwater model was calibrated using the parameter estimation processes and by comparing with observations e g water table elevation of transient and steady state conditions masterson et al 2016a besides the drain package was applied to each modlfow grid cell to interact with vic grid cell the coupled model computed daily estimates of the wte soil moisture and other fluxes for the period from 1987 to 2016 2 4 drought indices and overall descriptions of analysis in this study the vic and vicmf models were used to compute the input variables required for computing the msdi and to evaluate the multivariate drought conditions in the nacp region the calculation of multivariate drought indices was based on the joint probability and distribution models hao and aghakouchak 2013 2014 this method is an extension of the generally used spi computation proposed by mckee et al 1993 extended to a bivariate model of precipitation soil moisture and other variables indicating two variables e g precipitation and soil moisture at a specific time scale e g 25 weeks as random variables x and y respectively the joint distribution of two variables x and y can be represented as 4 p x x y y p where p is the joint probability of the two variables the msdi was then computed as follows hao and aghakouchak 2014 5 msdi 1 p where is the standard normal distribution function hao and aghakouchak 2014 used a nonparametric joint distribution method to avoid computational difficulty in fitting parametric distributions an empirical joint probability in the bivariate variables is calculated by the gringorten plotting position formula gringorten 1963 yue et al 1999 benestad and haugen 2007 6 p x k y k m k 0 44 n 0 12 where n is the number of the observation and m k is the number of occurrences of the pair x i y i for x i x k and y i y k after the joint probability is derived from equation 6 it is used as an input variable to equation 5 to calculate the msdi in this study the msdi approach was mainly used for the estimation of drought indices but combinations of variables differed between the two models for example precipitation and soil moisture were used to compute msdi for the vic model but other combinations were available for the vicmf model such as precipitation soil moisture and wte for the vic model msdi with precipitation p and soil moisture s was computed and it was msdi psv for the vicmf model a drought index was computed in addition to the msdi with p and s msdi psm and it was the msdi with p and wte msdi pwm each drought index was computed from the simulated results of the vic and vicmf models and they were subsequently used to evaluate drought events in the nacp regions during the period of 2000 2016 because the us drought monitor usdm was only available from january 2000 the usdm is the most widely used drought product in the us and it provides a weekly drought map in collaboration with the national drought mitigation center ndmc the u s department of agriculture usda and the national oceanic and atmospheric administration noaa svoboda et al 2002 in this study weekly drought maps from the usdm were used to verify the drought areas of each drought index from the models in the nacp region the performances of the drought indices were evaluated by the drought area agreement da statistics which was calculated as the rate of intersected areas where drought areas were correctly captured per total area of the usdm fig 3 shows the time series plots of drought areas in the nacp regions from the usdm http droughtmonitor unl edu svoboda et al 2002 and there were four representative drought events in the periods of 2002 2008 2011 and 2012 all drought events were defined when more than 20 of areas were affected and they lasted longer than 40 weeks each drought event is highlighted with black and dashed boxes and they are named e1 to e4 respectively a statistical test was performed to check the significant differences in the performances of the drought indices e g msdi psv msdi psm and msdi pwm a one way analysis of variance anova method was applied to reject the null as the mean value of da from the same drought indices  0 05 for the multiple comparisons of two drought indices tukey s honest significant difference hds test was also carried out if zero was included within the 95 confidence interval and the p value was higher than 0 05 there was no significant difference between the drought indices 3 results and discussion 3 1 soil moisture estimation of the vic and vicmf models in this study soil moisture from the vic and vicmf models was compared to analyze the impacts of groundwater dynamics fig 4 a shows the seasonal comparison of total soil moisture during the study period and the red and blue lines represent the results of the vic and vicmf models each line was calculated by the mean value of soil moisture for each week during the study period 1987 2016 overall the mean value of soil moisture from the vic model was higher than that of the vicmf 209 4 mm and 206 7 mm respectively the red line in fig 4 b shows the difference in weekly soil moisture between the two models and a positive number means that soil moisture from the vic model was greater than that of the vicmf the soil moisture from the vicmf was slightly higher from january to april 1 16 weeks and the vic was higher for the rest of the period may to december the blue line represents the total upward flux produced by the vicmf model and the difference in soil moisture was higher when the upward flux was relatively low 31 45 weeks additionally there was a significant linear correlation between the soil moisture difference and upward flux fig 4 c in other words the upward flux calculated by the groundwater model was closely related to the soil moisture estimation vicmf which would affect the drought assessment soil moisture is one of the most appropriate variables to monitor and assess the impact of water shortage on vegetated land due to its effects on atmospheric dynamics cammalleri and micalevogt 2016 in addition augmenting soil moisture via irrigation to increase plant available water for root extraction when there is a precipitation deficit is required for crop growth and agricultural production bolten et al 2010 snchez et al 2016 therefore an estimation of reliable soil moisture using surface groundwater dynamics is essential for the computation of drought indices 3 2 drought area agreement and wte the drought maps of the drought indices from the vic and vicmf models were compared with the usdm drought maps to assess the accuracy of the drought indices fig 5 provides the da derived from the drought indices for each drought event which were msdi psv from the vic model and msdi psm and msdi pwm from the vicmf model da was calculated as the rate of intersected areas where drought areas were correctly captured per total area of the usdm the drought indices were computed at the same temporal scale with corresponding dates with usdm weekly four representative drought events lasted more than six months fig 3 the longest 63 weeks and most serious droughts occurred in 2002 which caused the greatest decrease in wte computed from the vicmf model overall the performances of the msdi pwm were higher than those of other indices for all drought events and it was affected by the different combinations of drought indices the da values of msdi ps from the vic msdi psv msdi psm and msdi pwm from the vicmf model msdi psm and msdi pwm were 97 5 97 7 and 99 9 for the first event e1 and 97 5 97 6 and 98 1 for e2 respectively the da values were 68 3 70 6 and 75 7 for e3 and 91 3 91 7 and 96 4 for the e4 fig 5 for all the drought events the mean values of msdi pwm were the highest in addition fig 6 presents the spatial comparisons of the representative drought conditions for each period and the msdi pwm showed higher da values however there are some poor agreements during the beginning of the drought period e g e1 and e3 the sources of the poor agreements are 1 the msdis used in this study only consider precipitation soil moisture and wte conditions while the usdm is a composite index that consider multiple drought indices e g pdsi spi swsi climate indices e g enso usda nass topsoil moisture noaa nesdis satellite vegetation health indices and local reports from more than 450 expert observers across the u s and 2 the spatial resolution of usdm 1 km is smaller than the outputs of the vic and vicmf models svoboda et al 2002 fig 7 represents the comparisons of the usdm drought areas msdi psm and msdi pwm drought severities for each event were represented by the mean values of msdi pwm and they were 1 83 for e1 1 41 for e2 0 82 for e3 and 1 05 for e4 based on the results the e1 period was the most severe drought in the last twenty years in the nacp region and msdi psm and msdi pwm accurately captured the drought conditions however the performances of msdi psm and msdi pwm differed during the drought termination period as shown in fig 10 b the green line represents the severe drought of msdi 1 5 and the black and blue lines indicate the msdi psm and msdi pwm respectively after october 2002 there was a drought recovery but the slope of msdi psm was much steeper than that of msdi pwm thus some of the drought areas after october 2002 were not adequately captured by msdi psm the da values of msdi psm were rapidly decreased at the end of the e1 period fig 5 a in addition fig 7 c represents the drought areas msdi psm and msdi pwm for the e4 period similar to the results of the e1 period the slope of msdi psm was slightly steeper than that of msdi pwm during the drought termination period which also affected the decrease in the da values at the end of the e4 period fig 5 d droughts have an enormous impact on hydrologic variables such as runoff soil moisture and groundwater but their responses vary the drought response of groundwater is slower than soil moisture regarding both drought onset and recovery van loon 2015 which shows that the drought assessment capability of msdi pwm was better during the drought recovery period for the other drought events such as the e3 period similar results were shown fig 5 c these results imply that a drought index that considers both meteorological and groundwater conditions are better suited to capture overall drought conditions specifically for the coastal estuaries it is evident that there is a time lag between groundwater and surface water systems in capturing drought conditions however it depends on the type of the groundwater systems shallow or deep soil moisture conditions and how fast the recovery happens sridhar et al 2006 sridhar and hubbard 2010 in the nacp region the fall line and geology determine the relatively short time of response also as a first attempt to integrate the groundwater system in the drought assessment our hypothesis is that groundwater integration for drought extent mapping is essential groundwater is generally viewed as a buffer resource and the groundwater use increased during drought periods uddameri et al 2017 besides a groundwater drought is defined by the periods of decreased groundwater recharge and groundwater levels van lanen and peters 2000 mishra and singh 2010 thus an analysis of wte during drought periods is essential because it presents a scenario of groundwater availability during droughts in the areas where the various land covers exist like the nacp region in this study we analyzed the effect of droughts on wte in selected urban and rural areas and compared the wet normal and drought conditions and analyzed the significance of the data through a statistical analysis since spi is the most commonly used drought index for the drought assessment guenang and kamga 2014 it is considered to be suitable for distinguishing between drought normal and wet conditions fig 8 a and b were developed using long term wte with the vicmf model and wet normal and drought conditions were divided based on the results of msdi pwm that showed the highest da the wet normal and drought conditions were defined by the ranges of the standardized precipitation index spi at the corresponding areas and they are based on the spi classification mckee et al 1993 if the spi values are less than 0 5 it is considered as drought if the spi values are between 0 5 and 0 5 it is a normal condition finally if the spi values are greater than 0 5 it is wet and hence no drought fig 8 a shows the urban area results washington dc and fig 8 b represents the rural area results downstream areas of the potomac river and the rappahannock river the locations and land use for the urban and rural areas are shown in fig 1 for both urban and rural areas wte decreases tended to be greater during drought spi 0 5 which was more than during normal 0 5 spi 0 5 and wet conditions spi 0 5 the mean values of the urban area were 32 8 m 33 2 m and 33 3 m for the drought normal and wet conditions respectively for the rural area the mean values were 10 3 m 10 5 m and 10 6 m respectively furthermore a statistical test was performed to check the significant differences in each condition a one way method was used to compare three conditions drought normal and wet  0 05 for the multiple comparisons of conditions tukey s hds test was also carried out if zero was included within the 95 confidence interval and the p value was higher than 0 05 there was no significant difference between the conditions for the urban area there was a significant difference between the conditions which were determined by one way anova p value 2e 16 and the tukey s hsd test indicated that there were significant differences among the three conditions fig 8 c for the rural area the differences were significant for the three conditions p value 5 3e 14 and the tukey s hsd test showed that there were significant differences between drought and normal p value 2e 16 and drought and wet conditions p value 2e 16 fig 8 d however there was no significant difference between normal and wet conditions overall the results imply that the wte was significantly influenced by the drought conditions in both urban and rural areas but the magnitude of differences between drought and normal conditions for the urban area was higher 0 4 m than for the rural area 0 2 m 3 3 statistical test for the da and wte to evaluate the performances of the drought indices from the vic and vicmf models a statistical test was carried out for the drought events and da values table 1 shows the anova results for each drought event the p values of e1 and e4 were less than 0 05 rejecting the null hypothesis which can imply that there was a significant difference in the mean values of da for e1 and e4 however the p values of e2 and e3 were 0 648 and 0 551 which did not reject the null hypothesis and there was no significant difference between the da values table 1 and fig 9 show the results of tukey s hsd test which confirm where the differences occurred between groups for the e1 period there was a significant difference between groups as determined by one way anova p value 0 0036 and the tukey s hsd test also revealed that there were significant differences between msdi psv and msdi pwm p value 0 007 and msdi psm and msdi pwm p value 0 014 for the e4 period the tukey s hsd test also revealed that there were significant differences between msdi psv and msdi pwm p value 0 004 and msdi psm and msdi pwm p value 0 08 the results of the e1 and e4 periods statistically prove that the performances of msdi pwm msdi with precipitation and wte showed better da compared to the usdm however no significant differences were found for e2 and e3 but the mean values of da from the msdi pwm were slightly better in addition even if not statistically significant the result of msdi psm from vicmf was slightly higher than that of msdi psm from the msdi psv and it can be inferred that the effect of surface groundwater dynamics can be important for drought assessment 4 conclusion the coupled framework including surface and groundwater conditions is useful for considering surface and groundwater dynamics while assessing the impact of changing hydrology on drought predictions the coupled vicmf model was applied to the nacp region in the chesapeake bay one of the largest and the most productive estuary at present additionally surface hydrologic simulations using the vic model were performed and the results of the vic and vicmf models were used to compute the drought indices and assess the drought conditions the critical findings of this study are as follows 1 the mean soil moisture values from the vic model were higher than those of vicmf and the soil moisture differences were greater when the upward fluxes were relatively low the soil moisture data from the two models implies that the upward flux was linearly related to the soil moisture estimation and therefore drought assessments 2 the performance of the msdi pwm was better than that of the other indices for all drought events for all events e1 to e4 the da values of msdi pwm were 99 9 98 1 75 7 and 96 4 respectively which were reliable results for the drought assessment since the drought evaluation capability of msdi pwm was better during the drought recovery period the da values from msdi pwm were better 3 the significance of the da values was verified by the statistical test one way anova and the e1 and e4 results showed that there were significant differences in the mean values of da in addition the results of tukey s hsd test revealed that there were significant differences between msdi pwm and the other drought indices for the e1 and e4 periods 4 in both urban and rural areas there were wte decreases during drought periods which were more than during normal and wet conditions however the difference between drought and normal conditions for the urban area was higher 0 4 m than that for the rural area 0 2 m 5 for drought assessment an integrated modeling framework that considers climate hydrologic human interactions including the groundwater condition uddameri et al 2017 is recommended thus the coupled approach investigated in this study may be useful to better characterize and simulate drought assessment that considers the surface groundwater dynamic which in turn can serve as an effective tool for decision making drought mitigation and risk management 6 the wte derived from the vicmf model is coupled with the surface water model vic and msdi calculated with this analysis is also the first attempted method in a region where groundwater dynamics is quite varied and insufficiently addressed it is considered to be somewhat novel since it can generate the information on surface water and groundwater drought conditions with a dynamically coupled model 7 the results of this study emphasize the need for an integrated framework with surface and groundwater models to assess more accurate drought conditions for the estuary regions in the globe acknowledgement this project was funded in part by the virginia agricultural experiment station blacksburg and the hatch program of the national institute of food and agriculture u s department of agriculture washington d c we also thank institute for critical technology and applied science ictas graduate school at virginia tech for the graduate fellowship provided to the first author during his graduate education appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 002 
26186,an integrated framework of surface and groundwater models is essential for the comprehensive understanding of impacts of droughts due to surface water and subsurface exchanges we evaluate the response of soil moisture and groundwater dynamics during drought in the northern atlantic coastal plain in the chesapeake bay watershed the variable infiltration capacity model coupled with modflow vicmf was implemented and several variables were used in the multivariate standardized drought index msdi that consider multivariate perspectives of droughts three drought indices were derived msdi psv msdi psm msdi pwm psv precipitation and soil moisture from vic psm precipitation and soil moisture from vicmf pwm precipitation and wte from vicmf and the accuracy of the results was verified using a performance measure drought area agreement da and a statistical test to evaluate spatial extent of the drought areas the msdi pwm showed better results for predicting drought events as it captured overall drought conditions 1 introduction a comprehensive understanding of the hydrologic cycle and watershed processes in the terrestrial environment is essential for water budget estimation and sustainable water management under changing weather and climate extremes specifically interactions between surface water and groundwater in the watersheds where the exchanges are dominant can have significant impacts on water resources watershed management and nutrient loadings bailey et al 2016 however the exchanges between these systems have been analyzed without coupling and many models have considered improving the exchanges in a vertical direction niu et al 2007 liang et al 2003 to give more accurate evaluations of these interactions a fully coupled model is necessary to provide a better understanding of both surface and groundwater conditions across multiple spatial and temporal scales numerous techniques and different approaches are available to investigate the patterns of groundwater and surface water interactions at various spatiotemporal scales panday and huyakorn 2004 bailey et al 2016 sridhar et al 2018 these techniques and advanced tools provide opportunities to explain the effects of these interactions on water availability hydrologic processes and drought conditions including the rates of evapotranspiration surface runoff soil moisture and groundwater level sridhar et al 2018 developed a new coupled framework using variable infiltration capacity vic liang et al 1994 and modflow models referred to as vicmf hereafter to investigate the influence of groundwater dynamics on water balance components and assess how changing hydrology alters recharge and thus groundwater levels the vic model can provide downward flux generated from the precipitation events and estimates surface atmospheric fluxes using atmospheric forcing soil and vegetation data over the land surface under certain conditions where the contribution of groundwater is adding moisture in the vadose zone the subsurface water movement can be significant as there is upward flux from below this can be useful for estimating the soil moisture in addition the vic model includes multiple soil layers with variable infiltration rates and a routing module that uses the linear transfer function model is available to simulate the streamflow in the river network system however the vic model does not estimate hydrologic dynamics related to groundwater or groundwater surface water interactions because it is applied in an uncoupled surface hydrologic model framework thus the coupled vicmf model was developed for a more accurate understanding of the physical processes in surface groundwater interactions hydrologic models have applied water and energy balance approaches and they are beneficial for addressing spatial distributions of water resources and drought conditions however an application of only surface hydrologic model does not capture integrated drought conditions of surface and groundwater dynamics at once oki and kanae 2006 the integrated framework somewhat complements these shortcomings of drought evaluations in addition drought indices are used for evaluating drought severity frequency and impact niemeyer 2008 as well as for improved drought prediction kang and sridhar 2018 sehgal and sridhar 2019 sehgal et al 2017 sehgal et al 2018 numerous drought indices have been applied to assess drought conditions and the multivariate standardized drought index msdi which is calculated using multiple hydrometeorological variables such as precipitation soil moisture and runoff is used to identify retrospective droughts in the united states us hao and aghakouchak 2013 2014 since the msdi can consider multiple hydrometeorological variables msdi applications enable us to analyze various aspects of droughts integrating surface and groundwater dynamics in this study the msdi was applied to represent multivariate perspectives on drought conditions with various combinations of variables e g soil moisture precipitation water table elevation wte the northern atlantic coastal plain nacp aquifer system is ranked the 13th in total groundwater withdrawals in the us reilly et al 2008 the supply of surface water in this region is limited because of numerous sources of the surface waters are brackish estuaries despite sufficient precipitation more than 1100 mm year besides many populations in the nacp rely heavily on groundwater to satisfy their water demand but water availabilities in the nacp region have been threatened by droughts available drawdowns agricultural demands and industrial contaminations masterson et al 2013 thus an accurate drought assessment that considers both surface and groundwater conditions would be useful for the nacp region the objectives of this study are 1 to identify the physical relationship between surface and groundwater variables such as soil moisture and wte during drought events using the coupled vicmf model and 2 to evaluate the drought conditions using various drought indices in the chesapeake bay watershed including the northern atlantic coastal plain nacp aquifer system since the msdi calculation with groundwater condition is the first attempted approach the results of this study would contribute to generating valuable information on surface and groundwater drought conditions with a dynamically coupled model integrated surface and groundwater systems provide new insights into soil moisture water table elevation baseflows and evapotranspiration gosselin et al 2006 jaksa and sridhar 2015 sridhar et al 2018 2 methods 2 1 study area and descriptions of groundwater model the nacp aquifer system in the chesapeake bay watershed occupies an area of approximately 28 200 km2 between latitude 36 40 and 39 45 n and longitude 77 30 and 79 15 w fig 1 the climate is temperate and humid with annual precipitation of 1020 mm and large metropolitan areas are included along with coastal areas such as washington d c and richmond in addition the nacp aquifer provides a widely used groundwater supply due to its thickness and large areal extents masterson et al 2016a the us geological survey usgs developed a groundwater flow model modflow nwt for the nacp aquifer system masterson et al 2016a 2016b this model provides a detailed evaluation of the groundwater availability of the nacp aquifer system the nacp aquifer system is hundreds of meters thick along the coastline with a maximum thickness of about 3000 m the aquifer system includes nine confined aquifers and nine confining units population and changes in land use during the last few decades have led to various increased freshwater demands for the 20 million people in the region water table elevations are decreasing by about 0 6 m year that results in large changes and difficult in water use and management of aquifer resources total groundwater withdrawal in 2013 was computed to be about 4 9 million cubic meters per day mm3 day and it provides 40 percent of the drinking water supply in highly populated regions precipitation amount for 2005 to 2009 is about 234 8 mm3 day but almost 70 percent is lost by evapotranspiration that led to 74 5 mm3 day entering the groundwater system as aquifer recharge masterson et al 2016a 2016b in this study only some parts of the nacp area chesapeake bay region will be considered for the coupled model as it is important to include the effect of upward flux and recharge estimates fig 1 masterson et al 2016a and masterson et al 2016b provide some packages and input parameters for the modflow simulation mcdonald and harbaugh 1988 in the nacp region since the original spatial resolution of modflow was 1 mile 1 6 km a spatial downscale method kriging was applied for the coupled model 1 km the required parameters and packages for the vicmf model were the horizontal and vertical hydraulic conductivities specific storage specific yield general head boundary ghb package recharge rch package river riv package and well wel package the drain drn package is also required to simulate the vicmf model and the drain elevation and conductance for the drn package are calculated based on equations 1 and 2 1 c k a x 1 x 0 2 d e s e s l where c is conductance l 2 t k is hydraulic conductivity l t a is the area l 2 x is the position l when head is measured d e is the drain elevation l s e is the surface elevation l and s l is the depth of the soil layers l from the vic model hildreth 2013 sridhar et al 2018 2 2 vic and modflow model the vic model is a physically based macroscale hydrologic model that includes a water and energy balance framework liang et al 1994 the model estimates several hydrologic and atmospheric variables such as soil moisture surface runoff baseflow and evapotranspiration and applies a conceptual system to demonstrate water and surface energy budgets furthermore the vic model includes multiple soil layers with variable infiltration rates and a routing module that uses the linear transfer function model is available to simulate the streamflow in the river network system these essential characteristics of the vic model provide an opportunity to couple the land surface scheme with a groundwater model for accurate descriptions of the entire hydrologic budgets however the model is applied as a land surface hydrologic model that does not estimate hydrologic dynamics with groundwater or groundwater surface water interactions thus a coupled framework is required to provide an understanding of the physical mechanisms of droughts considering both surface and groundwater conditions simultaneously the modular three dimensional finite difference ground water flow model modflow 2005 is a physically based three dimensional and distributed finite difference groundwater model that considers aquifer groundwater levels and routes groundwater flow in the system harbaugh 2005 also the coupled vicmf model is another variant of modflow that is used for this study with the modflow model estimations of groundwater recharge pumping vadose zone percolation discharge to subsurface drains and river aquifer interactions are available however modflow does not estimate the surface water budget components such as soil moisture and surface runoff bailey et al 2016 the vicmf coupled model was developed for a more accurate understanding of physical processes with surface groundwater interactions and the coupled model was successfully applied to the eastern snake plain aquifer espa region of snake river basin srb sridhar et al 2018 2 3 vic modflow vicmf coupled model the coupled vicmf model hildreth 2013 sridhar et al 2018 is integrated with surface and groundwater inputs in the nacp aquifer system region where the chesapeake bay is located fig 1 the vicmf model is based on the interdependent equations that estimate the flow of water in a surface and groundwater hydrologic system fig 2 the groundwater area includes the modflow applications in the nacp region and it estimates upward flux to surface zones soil layer the drainage package is used to calculate upward flux discharge from the modflow grid cells for the daily stress period which are integrated into a corresponding grid of the vic model for the daily simulation the amount of discharge u f into the unsaturated vic soil layers that are generated by the drain package is represented by equation 3 3  t z d  z k  z u f where  is the soil moisture content l 3 l 3 d  is the hydraulic diffusivity l 2 t k  is the hydraulic conductivity l t and z is the soil depth l the surface area consists of vegetation and soil layers vic model which combines the upward flux from the groundwater area in addition the vic model simulates infiltration that is recharged to groundwater zones finally the modflow model combines the recharge to simulate groundwater dynamic for the following day fig 2 the whole process of the coupled model generates wte soil moisture and energy fluxes the horizontal discretization of the groundwater model is designed to be consistent with the surface discretization and the discharges of the groundwater model improve the estimation of soil moisture by the surface model modflow packages such as the recharge drain well river and ghb general head boundary enable to drive the coupled model with the meteorological forcings the vic model was applied at a 1 16th degree resolution 0 0625 5 km with atmospheric forcing data daily precipitation maximum and minimum temperatures and wind speed and there were 50 rows and 38 columns the grid cells for the groundwater model downscaled as 250 rows and 190 columns and each cell is uniformly spaced at 1 km which is five times finer than the resolution of a vic grid cell thus the 25 grid cells 5 5 of the groundwater model were aggregated as a mean value for dynamic exchanges of the coupled model fig 2 in addition the nacp grid cells were realigned to match the vic model grid cells since the grid cells in the nacp model are aligned and not geographically directed north to south 50 727 from horizontal finally since the groundwater model was created as a daily basis a specific work for the temporal aggregation for the coupling was not required the daily outputs e g soil moisture wte were aggregated on a weekly time step for computations of drought indices other vic inputs such as soil and vegetation data were obtained from the ldas http ldas gsfc nasa gov at the same spatial resolution as the meteorological data moreover this study utilized the modflow parameters from the nacp groundwater model masterson et al 2016a b including the hydraulic conductivity the groundwater model was calibrated using the parameter estimation processes and by comparing with observations e g water table elevation of transient and steady state conditions masterson et al 2016a besides the drain package was applied to each modlfow grid cell to interact with vic grid cell the coupled model computed daily estimates of the wte soil moisture and other fluxes for the period from 1987 to 2016 2 4 drought indices and overall descriptions of analysis in this study the vic and vicmf models were used to compute the input variables required for computing the msdi and to evaluate the multivariate drought conditions in the nacp region the calculation of multivariate drought indices was based on the joint probability and distribution models hao and aghakouchak 2013 2014 this method is an extension of the generally used spi computation proposed by mckee et al 1993 extended to a bivariate model of precipitation soil moisture and other variables indicating two variables e g precipitation and soil moisture at a specific time scale e g 25 weeks as random variables x and y respectively the joint distribution of two variables x and y can be represented as 4 p x x y y p where p is the joint probability of the two variables the msdi was then computed as follows hao and aghakouchak 2014 5 msdi 1 p where is the standard normal distribution function hao and aghakouchak 2014 used a nonparametric joint distribution method to avoid computational difficulty in fitting parametric distributions an empirical joint probability in the bivariate variables is calculated by the gringorten plotting position formula gringorten 1963 yue et al 1999 benestad and haugen 2007 6 p x k y k m k 0 44 n 0 12 where n is the number of the observation and m k is the number of occurrences of the pair x i y i for x i x k and y i y k after the joint probability is derived from equation 6 it is used as an input variable to equation 5 to calculate the msdi in this study the msdi approach was mainly used for the estimation of drought indices but combinations of variables differed between the two models for example precipitation and soil moisture were used to compute msdi for the vic model but other combinations were available for the vicmf model such as precipitation soil moisture and wte for the vic model msdi with precipitation p and soil moisture s was computed and it was msdi psv for the vicmf model a drought index was computed in addition to the msdi with p and s msdi psm and it was the msdi with p and wte msdi pwm each drought index was computed from the simulated results of the vic and vicmf models and they were subsequently used to evaluate drought events in the nacp regions during the period of 2000 2016 because the us drought monitor usdm was only available from january 2000 the usdm is the most widely used drought product in the us and it provides a weekly drought map in collaboration with the national drought mitigation center ndmc the u s department of agriculture usda and the national oceanic and atmospheric administration noaa svoboda et al 2002 in this study weekly drought maps from the usdm were used to verify the drought areas of each drought index from the models in the nacp region the performances of the drought indices were evaluated by the drought area agreement da statistics which was calculated as the rate of intersected areas where drought areas were correctly captured per total area of the usdm fig 3 shows the time series plots of drought areas in the nacp regions from the usdm http droughtmonitor unl edu svoboda et al 2002 and there were four representative drought events in the periods of 2002 2008 2011 and 2012 all drought events were defined when more than 20 of areas were affected and they lasted longer than 40 weeks each drought event is highlighted with black and dashed boxes and they are named e1 to e4 respectively a statistical test was performed to check the significant differences in the performances of the drought indices e g msdi psv msdi psm and msdi pwm a one way analysis of variance anova method was applied to reject the null as the mean value of da from the same drought indices  0 05 for the multiple comparisons of two drought indices tukey s honest significant difference hds test was also carried out if zero was included within the 95 confidence interval and the p value was higher than 0 05 there was no significant difference between the drought indices 3 results and discussion 3 1 soil moisture estimation of the vic and vicmf models in this study soil moisture from the vic and vicmf models was compared to analyze the impacts of groundwater dynamics fig 4 a shows the seasonal comparison of total soil moisture during the study period and the red and blue lines represent the results of the vic and vicmf models each line was calculated by the mean value of soil moisture for each week during the study period 1987 2016 overall the mean value of soil moisture from the vic model was higher than that of the vicmf 209 4 mm and 206 7 mm respectively the red line in fig 4 b shows the difference in weekly soil moisture between the two models and a positive number means that soil moisture from the vic model was greater than that of the vicmf the soil moisture from the vicmf was slightly higher from january to april 1 16 weeks and the vic was higher for the rest of the period may to december the blue line represents the total upward flux produced by the vicmf model and the difference in soil moisture was higher when the upward flux was relatively low 31 45 weeks additionally there was a significant linear correlation between the soil moisture difference and upward flux fig 4 c in other words the upward flux calculated by the groundwater model was closely related to the soil moisture estimation vicmf which would affect the drought assessment soil moisture is one of the most appropriate variables to monitor and assess the impact of water shortage on vegetated land due to its effects on atmospheric dynamics cammalleri and micalevogt 2016 in addition augmenting soil moisture via irrigation to increase plant available water for root extraction when there is a precipitation deficit is required for crop growth and agricultural production bolten et al 2010 snchez et al 2016 therefore an estimation of reliable soil moisture using surface groundwater dynamics is essential for the computation of drought indices 3 2 drought area agreement and wte the drought maps of the drought indices from the vic and vicmf models were compared with the usdm drought maps to assess the accuracy of the drought indices fig 5 provides the da derived from the drought indices for each drought event which were msdi psv from the vic model and msdi psm and msdi pwm from the vicmf model da was calculated as the rate of intersected areas where drought areas were correctly captured per total area of the usdm the drought indices were computed at the same temporal scale with corresponding dates with usdm weekly four representative drought events lasted more than six months fig 3 the longest 63 weeks and most serious droughts occurred in 2002 which caused the greatest decrease in wte computed from the vicmf model overall the performances of the msdi pwm were higher than those of other indices for all drought events and it was affected by the different combinations of drought indices the da values of msdi ps from the vic msdi psv msdi psm and msdi pwm from the vicmf model msdi psm and msdi pwm were 97 5 97 7 and 99 9 for the first event e1 and 97 5 97 6 and 98 1 for e2 respectively the da values were 68 3 70 6 and 75 7 for e3 and 91 3 91 7 and 96 4 for the e4 fig 5 for all the drought events the mean values of msdi pwm were the highest in addition fig 6 presents the spatial comparisons of the representative drought conditions for each period and the msdi pwm showed higher da values however there are some poor agreements during the beginning of the drought period e g e1 and e3 the sources of the poor agreements are 1 the msdis used in this study only consider precipitation soil moisture and wte conditions while the usdm is a composite index that consider multiple drought indices e g pdsi spi swsi climate indices e g enso usda nass topsoil moisture noaa nesdis satellite vegetation health indices and local reports from more than 450 expert observers across the u s and 2 the spatial resolution of usdm 1 km is smaller than the outputs of the vic and vicmf models svoboda et al 2002 fig 7 represents the comparisons of the usdm drought areas msdi psm and msdi pwm drought severities for each event were represented by the mean values of msdi pwm and they were 1 83 for e1 1 41 for e2 0 82 for e3 and 1 05 for e4 based on the results the e1 period was the most severe drought in the last twenty years in the nacp region and msdi psm and msdi pwm accurately captured the drought conditions however the performances of msdi psm and msdi pwm differed during the drought termination period as shown in fig 10 b the green line represents the severe drought of msdi 1 5 and the black and blue lines indicate the msdi psm and msdi pwm respectively after october 2002 there was a drought recovery but the slope of msdi psm was much steeper than that of msdi pwm thus some of the drought areas after october 2002 were not adequately captured by msdi psm the da values of msdi psm were rapidly decreased at the end of the e1 period fig 5 a in addition fig 7 c represents the drought areas msdi psm and msdi pwm for the e4 period similar to the results of the e1 period the slope of msdi psm was slightly steeper than that of msdi pwm during the drought termination period which also affected the decrease in the da values at the end of the e4 period fig 5 d droughts have an enormous impact on hydrologic variables such as runoff soil moisture and groundwater but their responses vary the drought response of groundwater is slower than soil moisture regarding both drought onset and recovery van loon 2015 which shows that the drought assessment capability of msdi pwm was better during the drought recovery period for the other drought events such as the e3 period similar results were shown fig 5 c these results imply that a drought index that considers both meteorological and groundwater conditions are better suited to capture overall drought conditions specifically for the coastal estuaries it is evident that there is a time lag between groundwater and surface water systems in capturing drought conditions however it depends on the type of the groundwater systems shallow or deep soil moisture conditions and how fast the recovery happens sridhar et al 2006 sridhar and hubbard 2010 in the nacp region the fall line and geology determine the relatively short time of response also as a first attempt to integrate the groundwater system in the drought assessment our hypothesis is that groundwater integration for drought extent mapping is essential groundwater is generally viewed as a buffer resource and the groundwater use increased during drought periods uddameri et al 2017 besides a groundwater drought is defined by the periods of decreased groundwater recharge and groundwater levels van lanen and peters 2000 mishra and singh 2010 thus an analysis of wte during drought periods is essential because it presents a scenario of groundwater availability during droughts in the areas where the various land covers exist like the nacp region in this study we analyzed the effect of droughts on wte in selected urban and rural areas and compared the wet normal and drought conditions and analyzed the significance of the data through a statistical analysis since spi is the most commonly used drought index for the drought assessment guenang and kamga 2014 it is considered to be suitable for distinguishing between drought normal and wet conditions fig 8 a and b were developed using long term wte with the vicmf model and wet normal and drought conditions were divided based on the results of msdi pwm that showed the highest da the wet normal and drought conditions were defined by the ranges of the standardized precipitation index spi at the corresponding areas and they are based on the spi classification mckee et al 1993 if the spi values are less than 0 5 it is considered as drought if the spi values are between 0 5 and 0 5 it is a normal condition finally if the spi values are greater than 0 5 it is wet and hence no drought fig 8 a shows the urban area results washington dc and fig 8 b represents the rural area results downstream areas of the potomac river and the rappahannock river the locations and land use for the urban and rural areas are shown in fig 1 for both urban and rural areas wte decreases tended to be greater during drought spi 0 5 which was more than during normal 0 5 spi 0 5 and wet conditions spi 0 5 the mean values of the urban area were 32 8 m 33 2 m and 33 3 m for the drought normal and wet conditions respectively for the rural area the mean values were 10 3 m 10 5 m and 10 6 m respectively furthermore a statistical test was performed to check the significant differences in each condition a one way method was used to compare three conditions drought normal and wet  0 05 for the multiple comparisons of conditions tukey s hds test was also carried out if zero was included within the 95 confidence interval and the p value was higher than 0 05 there was no significant difference between the conditions for the urban area there was a significant difference between the conditions which were determined by one way anova p value 2e 16 and the tukey s hsd test indicated that there were significant differences among the three conditions fig 8 c for the rural area the differences were significant for the three conditions p value 5 3e 14 and the tukey s hsd test showed that there were significant differences between drought and normal p value 2e 16 and drought and wet conditions p value 2e 16 fig 8 d however there was no significant difference between normal and wet conditions overall the results imply that the wte was significantly influenced by the drought conditions in both urban and rural areas but the magnitude of differences between drought and normal conditions for the urban area was higher 0 4 m than for the rural area 0 2 m 3 3 statistical test for the da and wte to evaluate the performances of the drought indices from the vic and vicmf models a statistical test was carried out for the drought events and da values table 1 shows the anova results for each drought event the p values of e1 and e4 were less than 0 05 rejecting the null hypothesis which can imply that there was a significant difference in the mean values of da for e1 and e4 however the p values of e2 and e3 were 0 648 and 0 551 which did not reject the null hypothesis and there was no significant difference between the da values table 1 and fig 9 show the results of tukey s hsd test which confirm where the differences occurred between groups for the e1 period there was a significant difference between groups as determined by one way anova p value 0 0036 and the tukey s hsd test also revealed that there were significant differences between msdi psv and msdi pwm p value 0 007 and msdi psm and msdi pwm p value 0 014 for the e4 period the tukey s hsd test also revealed that there were significant differences between msdi psv and msdi pwm p value 0 004 and msdi psm and msdi pwm p value 0 08 the results of the e1 and e4 periods statistically prove that the performances of msdi pwm msdi with precipitation and wte showed better da compared to the usdm however no significant differences were found for e2 and e3 but the mean values of da from the msdi pwm were slightly better in addition even if not statistically significant the result of msdi psm from vicmf was slightly higher than that of msdi psm from the msdi psv and it can be inferred that the effect of surface groundwater dynamics can be important for drought assessment 4 conclusion the coupled framework including surface and groundwater conditions is useful for considering surface and groundwater dynamics while assessing the impact of changing hydrology on drought predictions the coupled vicmf model was applied to the nacp region in the chesapeake bay one of the largest and the most productive estuary at present additionally surface hydrologic simulations using the vic model were performed and the results of the vic and vicmf models were used to compute the drought indices and assess the drought conditions the critical findings of this study are as follows 1 the mean soil moisture values from the vic model were higher than those of vicmf and the soil moisture differences were greater when the upward fluxes were relatively low the soil moisture data from the two models implies that the upward flux was linearly related to the soil moisture estimation and therefore drought assessments 2 the performance of the msdi pwm was better than that of the other indices for all drought events for all events e1 to e4 the da values of msdi pwm were 99 9 98 1 75 7 and 96 4 respectively which were reliable results for the drought assessment since the drought evaluation capability of msdi pwm was better during the drought recovery period the da values from msdi pwm were better 3 the significance of the da values was verified by the statistical test one way anova and the e1 and e4 results showed that there were significant differences in the mean values of da in addition the results of tukey s hsd test revealed that there were significant differences between msdi pwm and the other drought indices for the e1 and e4 periods 4 in both urban and rural areas there were wte decreases during drought periods which were more than during normal and wet conditions however the difference between drought and normal conditions for the urban area was higher 0 4 m than that for the rural area 0 2 m 5 for drought assessment an integrated modeling framework that considers climate hydrologic human interactions including the groundwater condition uddameri et al 2017 is recommended thus the coupled approach investigated in this study may be useful to better characterize and simulate drought assessment that considers the surface groundwater dynamic which in turn can serve as an effective tool for decision making drought mitigation and risk management 6 the wte derived from the vicmf model is coupled with the surface water model vic and msdi calculated with this analysis is also the first attempted method in a region where groundwater dynamics is quite varied and insufficiently addressed it is considered to be somewhat novel since it can generate the information on surface water and groundwater drought conditions with a dynamically coupled model 7 the results of this study emphasize the need for an integrated framework with surface and groundwater models to assess more accurate drought conditions for the estuary regions in the globe acknowledgement this project was funded in part by the virginia agricultural experiment station blacksburg and the hatch program of the national institute of food and agriculture u s department of agriculture washington d c we also thank institute for critical technology and applied science ictas graduate school at virginia tech for the graduate fellowship provided to the first author during his graduate education appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 002 
26187,in this work a novel approach based on artificial intelligence ai to assess the efficiency of wave energy converter wec farms in coastal protection isdeveloped we consider as a case study a beach subjected to severe erosion playa granada s spain more specifically we analyse the changes in the dry beach area quantified through the pelnard considre equation with and without wave farm protection by means of an artificial neural network ann model the model is selected after a thorough comparative study involving forty ann architectures with one and two hidden layers and two training algorithms levenberg marquadt and bayesian regression the best results are obtained with a 5 10 1 architecture trained with the bayesian regression algorithm once validated this ann model is applied to optimize the design and position of the wave farm the results confirm that ann models are a useful design tool for hybrid wave farms keywords coastal erosion coastal processes ann model wave energy ocean energy renewable energy 1 introduction the demand of energy has soared over the last decades partly due to the growth of emerging economies this demand is in part met by fossil fuels albeit at the expense of severe environmental repercussions not least global warming in this context it is essential to develop and promote the use of renewable energy sources among them wave energy is one of the most promising thanks to its widespread availability greaves and iglesias 2018 cornett 2008 uihlein and magagna 2016 and comparatively low environmental effects clment et al 2002 palha et al 2010 the research progresses on wave energy so far have been focused on various aspects existing wave energy converter technologies can be divided into three main types oscillating water column owc viviano et al 2016 oscillating bodies falco 2007 and overtopping devices among the latter offshore floating type devices such as wave dragon kofoed et al 2006 and wavecat fernandez et al 2012 can be used for coastal protection purposes as they act as obstacles for wave propagation varying the wave height on its lee the interaction between wavecat and the wave field was studied by carballo and iglesias 2013 through laboratory experiments this was the basis to study the impacts of this wec device on the coast abanades et al 2014 and bergillos et al 2018 studied the variation of the beach profile produced by the presence of a wave farm on sandy and gravel beaches respectively showing that wave farms can be used to counter erosion these studies were extended to the whole coastline and the influence of different aspects such as the wave farm location rodriguez delgado et al 2018a layout rodriguez delgado et al 2018b and inter device spacing rodriguez delgado et al 2019 were assessed from these studies dual wave farms have been recently proposed to fulfil a dual function carbon free energy production and coastal protection this dual function becomes very important for the developing of wave energy converters as the number of eroded beaches have grown up in recent years mainly due to human interventions syvitski et al 2009 anthony et al 2014 climate change mills et al 2018 and the consequent sea level rise chu et al 2014 clough et al 2016 cozannet et al 2015 artificial intelligence has been used as a tool to solve numerically complex problems where a number of variables are involved in the field of wave energy and ocean engineering in recent years wave height forecasting has been improved by means of diverse artificial intelligence techniques such as genetic programming gaur and deo 2008 neuro wavelet technique deka and prahlada 2012 dixit and londhe 2016 and sequential learning neural networks kumar et al 2017 extreme learning machine kumar et al 2018 bayesian methods callaghan et al 2018 harpham et al 2016 and artificial neural networks oh and suh 2018 in addition artificial intelligence has been applied to optimize energy converter systems in wind farms lin et al 2013 ou et al 2017 pv generation systems ou and hong 2014 and transformers ou et al 2016 moreover artificial neural networks have been used to generate a virtual laboratory to optimize owc systems lpez and iglesias 2014 and to predict the evolution of headland bay beaches iglesias et al 2010 however to our best knowledge a methodological approach based on artificial intelligence has not been proposed and applied to the design of wave farms for coastal defence purposes artifical neural networks anns arise as a useful tool as they can deal with non linear function regression in this vein anns can be applied to optimize the coastal protection performance of a wave farm reducing the computational cost of physically based numerical models the main objective of the present paper is to develop an ann that enables the prediction of the efficiency in coastal protection of a wave farm at a certain location and with a certain layout the study area playa granada in southern spain is presented in detail in section 2 the data for training and validating the different anns considered in this work were obtained by means of a wave propagation model swan a longshore sediment transport lst equation van rijn 2014 and the one line model pelnard considre 1956 for different locations and configurations of the farm inter wec spacings and incident wave conditions as detailed in section 3 2 study area playa granada is a gravel dominated beach located on the mediterranean coast of southern spain fig 1 this stretch of coast which is limited to the west by the guadalfeo river mouth and to the east by a shoreline horn punta del santo has been experiencing coastline retreat and strong erosion problems over last decade fig 2 mainly induced by the river regulation in 2004 bergillos et al 2015 the reduction in dry beach area since then until 2014 was greater than 4000 m 2 year with a maximum coastline retreat equal to 87 m bergillos et al 2016a to mitigate these problems successive nourishment interventions have been performed however they have not been effective in the long term bergillos et al 2016b 2017 consequently other management strategies are required to mitigate coastal erosion problems on the other hand recent works devoted to the design of wecs have proven that wecs optimally extract wave energy when they are forced by short wave periods lpez et al 2015a 2015b 2015c jaln et al 2016 like those of the study area where the wave energy potential has been recently assessed lpez ruiz et al 2016 in addition wave propagation and sediment transport models have been previously validated in the study area through comparison with field data bergillos et al 2017 thus this site represents an ideal location for the design of wave farms with the hybrid function of wave energy generation and coastal defence 3 materials and methods the proposed methodology is depicted in fig 3 the dataset for the training process of the artificial neural networks was obtained by means of a set of numerical models including wave propagation sediment transport formulation and shoreline evolution different training algorithms and architectures were tested in order to choose the configuration that yielded the best fit to the available data once the final artificial neural network configuration was chosen the wave farm which provided the best performance in coastal protection was obtained 3 1 artificial neural networks artificial neural networks anns are systems composed by a group of individual elements called neurons which work in parallel haykin 1994 he and xu 2010 in this work multilayer feed forward anns are used as they have been successfully applied to function approximation and non linear regression they are based on the connections of real neurons inside the brain hence the name artificial intelligence anns learn from experience much as biological neurons in a process called training before the training the dataset is divided in two groups for training and testing during the training the weights and biases that form the connection between the artificial neurons are adjusted through the training dataset inputs and targets first an input vector is presented to the network and propagated forward until an output is produced then during the testing process the mse between the produced output and the desired target value is computed and the weights of each neuron are adjusted to minimise the error this process is repeated until the maximum number of iterations is reached or the desired gradient of the error is achieved finally the generalization capabilities of the ann are assessed through the comparison of the targets and the outputs produced by the ann for the test dataset a multilayer feedforward ann is composed by an input layer one or more hidden layers and an output layer fig 4 the output of each neuron is produced by means of a transfer function although a number of derivable functions can be used with this purpose in this work the sigmoidal transfer function was used for the hidden layer neurons this function is defined by 1 y 2 1 e 2 x 1 1 in the output layer a pure linear function was used this function is expressed as 2 y x two different training algorithms were applied in this work first the levenberg marquardt algorithm which results in a faster training process hagan and menhaj 1994 in this first case the test dataset is divided in two groups validation and test during each iteration the mse of the validation dataset is computed and the training finishes when this value increases between two iterations second the bayesian regularization algorithm which requires more time but usually achieves good generalization results for difficult or noisy data mackay 1992 in this case no validation dataset is required and the training is terminated based on weight minimization these two training algorithms have been successfully used in the field of coastal engineering in different studies involving wec efficiency lpez and iglesias 2014 wave forecasting jain and deo 2007 and shoreline position prediction iglesias et al 2009 3 2 numerical models the purpose of the ann developed in this work is to optimize the layout and position of a wave farm for coastal protection purposes the dataset used to train and test the ann was obtained through a coupled numerical scheme composed by a wave propagation model an lst formulation and a shoreline evolution model these models have been successfully applied to the study area in previous works bergillos et al 2017 rodriguez delgado et al 2018a 2018b combinations of two wave heights 3 1 and 0 5 m corresponding to high and low energy conditions and two mean wave directions west 238 and east 107 were considered these wave heights have been proved to be representative of the wave climate of playa granada in recent works rodriguez delgado et al 2018a 2018b 2019 bergillos et al 2016 the duration of the wave attack for both high and low energy conditions was 48 h these sea states were propagated by means of swan holthuijsen et al 1993 for this purpose two nested computational grids were used a first coarser grid composed by 82 82 cells with sizes varying with depth from 170 65 m to 80 80 m the finer grid covers the wave farm area it has 244 82 cells with a cell size of 25 15 m the spectral resolution of the frequency space consisted of 37 logarithmically distributed frequencies ranging from 0 03 to 1 hz for the directional space the 360 were covered by 72 directions in increments of 5 for the configuration of the wave farm the following parameters were included i the location of the farm ii the number of rows in which the wecs are arranged and iii the inter wec spacing altogether 6 different locations wave farms composed by 1 4 rows and four different spacings from 1d to 4d with d 90 m were implemented in the numerical models fig 5 the number of wecs in each wave farm configuration was fixed 11 each configuration was modelled under the four aforementioned sea states giving a total of 384 cases the sea states were propagated from deep water to the nearshore region using the wave propagation numerical model swan holthuijsen et al 1993 this numerical model was previously calibrated and validated in the study zone for more information about the calibration of the model the reader is referred to bergillos et al 2017 the influence of the wave farm on the wave propagation patterns was also computed by means of this numerical model the wave field wec interaction was simulated using the transmission k t and reflection k r coefficients obtained by 9 for the wavecat device this wec is an overtopping type device formed by two movable hulls similar to those found in a catamaran hence its name and has been widely used in the literature as a wec for dual wave farms i e energy generation and coastal protection purposes rodriguez delgado et al 2018a 2018b abanades et al 2018 breaking parameters obtained from swan were used as input variables for the lst formulation developed by van rijn 2014 which is applicable to sandy gravel and shingle beaches this equation can be expressed as follows 3 q m 0 00018 k s w e l l  s g 0 5 tan  0 4 d 50 0 6 h s b r 3 1 sin 2  b r where q m is the lst rate k s w e l l a coefficient accounting for the influence of the swell waves and peak period  s the sediment density g the acceleration of gravity tan  the bed slope of the surf zone d 50 the size of the sediment h s b r the significant wave height at breaking and  b r the angle between the wave front and the shoreline at breaking finally the lst rate obtained in this way were used to apply a one line model pelnard considre 1956 to account for the changes in the shoreline position induced by lst this model is described as 4 y s t 1 d c q t x where y s is the shoreline position d c is a characteristic length taken as the sum of the depth of closure and the height of the berm and q t is the lst rate the joint application of the formulation proposed by van rijn 2014 and the one line model was also validated for the study zone in previous works bergillos et al 2017 in this way the subaerial beach surface differences  a between the natural scenario and the different configurations of the wave farm were obtained as the difference between the final and the initial position of the shoreline in playa granada this allowed obtaining a dataset composed by 384 values of  a which were the basis for training and testing the ann 3 3 ann implementation once the dataset is obtained the first step in the ann implementation is to define the input and output variables as unique output variable in order to characterize the coastal protection performance of the wave farm the beach surface difference  a was chosen as input variables the characteristics of the wave farm were represented by the alongshore location l the number of rows r o and the inter wec spacing s fig 5 in addition the influence of the wave climate was included by means of the significant wave height h s and the mean wave direction  thus the ann model can be described as 5  a f a n n l r o s h s  the second step is to divide the full dataset into the training validation and test datasets required for the development of the ann the training dataset encompassed 264 cases 70 whereas 60 cases 15 were used for the validation dataset and the remaining 60 cases 15 formed the test dataset in order to represent the variety of the cases in the test dataset each one of the 6 locations studied were represented by 10 randomly chosen cases ensuring that these cases contain the full range of rows spacings wave heights and directions studied the 60 cases for the validation dataset were chosen in the same way and the remaining cases formed the training set the optimum architecture for an ann i e the optimum number of neurons and layers depends on the number of cases inputs and outputs variables the characteristics of the dataset and consequently the function to approximate a low number of neurons could lead to a poor function fitting by increasing the number of neurons the network is able to reproduce higher order statistics of the data but at the same time its ability to generalize and represent accurately the function for data other than those included in the training process decreases walczak and cerpa 1999 in this way the optimum architecture for an ann is frequently chosen based on a trial and error process haykin 1994 the sizes of the input and output layers are determined by the number of input 5 and output 1 variables respectively for the hidden layer s a comparative study including 72 different architectures with one and two hidden layers was carried out in the case of one hidden layer the architectures analysed were 5 2 1 5 5 1 5 10 1 5 15 1 5 20 1 5 25 1 5 30 1 and 5 35 1 the remaining 64 architectures included two hidden layers with 5 10 20 or 30 neurons in the first layer and the size of the second layer varying from 2 to 35 neurons the effect of the training algorithm was investigated as well each of the architectures were trained using the levenberg marquardt and the bayesian regularization algorithms to compensate for the effect on the training of the initial weights each ann was trained 101 times and the average ability to learn and generalize the dataset was established to properly compare the performance of each architecture the test dataset was the same throughout the study the benchmark selected to account for the ann performance was the root mean squared error rmse computed as 6 rmse i 1 n e i m i 2 n where e is the target value of  a m i is the output produced by the ann and n is the number of cases 3 4 wave farm optimization in the study zone once the final ann was selected based on the comparative study an optimization of the wave farm location and layout for the study area was carried out to illustrate the potential of the proposed methodology using the final ann the different configurations of rows inter device spacings significant wave heights and mean directions were assessed in 100 locations between l1 and l6 based on the outputs provided by the ann the optimum layout and location in terms of coastal protection were selected 4 results 4 1 comparison of the different architectures and algorithms the results of the comparative study of the different architectures are described in this section fig 6 depicts the average rmse for the architectures with one hidden layer in the case of the levenberg marquardt algorithm the over fitting of the ann with a higher number of neurons in the hidden layer is clearly noticeable the average rmse in the train dataset decreases from 8 78 m2 with the 5 2 1 architecture to 3 91 m2 with the 5 35 1 architecture however the validation and test dataset do not show this steady reduction for the 5 2 1 architecture the average rmse of the validation train dataset is 8 69 m2 9 m2 then the average error decreases by up to 5 84 m2 5 47 m2 with the 5 20 1 architecture and increases from that value up to 6 48 m2 6 09 m2 with the 5 35 1 architecture the architectures trained with the bayesian regularization algorithm do not improve the average rmse for the train dataset increasing the number of neurons in the hidden layer instead the average error decreases from 9 64 m2 in 5 2 1 to 3 92 m2 in 5 20 1 however from that point the average train rmse increases to a maximum of 12 87 m2 with the 5 35 1 architecture the good generalization ability of the anns tested is demonstrated by the average rmse with the test data an initial decrease of the error is observed up to a minimum average rmse of 4 77 m2 with the 5 10 1 architecture then the error increases up to the maximum 12 m2 with the 5 35 1 architecture the two hidden layer architectures trained with the levenberg marquardt algorithm show the same overt fitting behaviour than those with one hidden layer fig 7 the rmse for the train data decreases when the number of neurons in the second hidden layer increases reaching a minimum average rmse of 2 31 m2 in the 5 30 35 1 architecture however the average rmses with the validation and test data are higher the minimum average rmse with the test data is 5 72 m2 for the 5 5 20 1 architecture ranging between 7 8 m2 and 11 6 m2 for the architectures with 30 neurons in the first hidden layer this over fitting is even more significant for the architectures trained with the bayesian regression algorithm in this case the minimum average rmse is 2 5 10 5 m2 for the 5 20 10 1 architecture reaching the same order of magnitude in the architectures with 30 neurons in the first hidden layer despite these good training errors the generalization abilities of this kind of architectures are lower due to its greater errors for the test dataset the minimum average rmse of the test dataset is 5 1 m2 and is obtained with the 5 30 20 1 architecture whereas the maximum average error 8 2 m2 is provided by the 5 5 35 1 architecture the comparative study highlights that the architectures with one hidden layer have a better performance than the two hidden layer systems the minimum average rmse is 5 47 m2 4 77 m2 for one hidden layer architectures trained with the levenberg marquardt bayesian regularization algorithm whereas the minimum error is 5 72 m2 5 1 m2 in the two hidden layer systems regarding the algorithm it is clear that bayesian regularization performs better providing lower errors with both one and two hidden layer architectures however it is important to note that the bayesian regularization algorithm requires greater computational effort and the mean running time was 33 greater than for levenberg marquardt this is mainly due to the fact that equilibrium in mse is reached before with the levenberg marquadt algorithm as this method stops training due to the validation process 4 2 final ann model after the comparative study the architecture 5 10 1 trained with the bayesian regularization algorithm yielded the best results between the 101 training runs carried out the ann whose rmse was closer to the median value was chosen the good results in the training which is the first step for a good performance of the ann are proved by the rmse a mere 3 23 m2 with the training dataset as may be observed in fig 8 a the ann provides an excellent fit to the modelled data in the training dataset most importantly the rmse with the test data is 4 22 m2 this low error value indicates that the ann is able to calculate accurately the beach surface differences for cases which were not included in the training fig 8b the linear regressions for the training and test datasets confirm these results fig 9 in the case of the training dataset the best linear fit is given by 7 y t r a i n 0 9391 x 0 2705 which is really close to the perfect diagonal y x the correlation coefficient is r 0 9713 this means that the ann has been able to learn correctly the dataset which was presented during the training process finally the correlation coefficient with the test data is also excellent r 0 9489 in this case the best linear fit is 8 y t e s t 1 033 x 0 3225 which is also very close to the diagonal the low rmse and the good results of the linear fit with the test data confirm that the ann is able to calculate accurately the subaerial beach surface difference for cases that are new to the model i e that did not form part of the training data the composition of the test data which encompassed different locations number of rows spacings significant wave heights and mean directions are taken into account corroborates that the selected ann is able to reproduce the beach surface difference irrespectively of the wave farm layout and wave conditions on this basis the ann model is fully validated 4 3 wave farm optimization with the ann described in the previous section 100 different locations of the wave farm between l1 and l6 were studied the same ranges of rows spacings significant wave heights and mean directions as in the training were used the results vary depending on the mean direction under westerly waves the best location is situated close to the central part of playa granada fig 10 this wave farm is formed by 2 rows with an inter wec spacing of 180 m 2d and yields a surface difference of 27 4 m2 alongshore positions westwards provide worse results for all the studied layouts with even erosion of about 5 m2 for wave farms situated near salobrea rock under easterly waves the same layout is found to be optimum but located eastward close to punta del santo leading to a surface difference of 48 17 m2 on the contrary wave farms situated eastwards induce negative results with losses of dry beach area about 60 m2 due to lower sediment transport from poniente beach specially for wave farms composed by 2 and 3 rows and inter device spacings of 2 d and 3 d positive effects are observed in the rest of the alongshore positions and layouts although these positive effects are lower westwards in order to assess the best overall location for a wave farm in the study area the surface differences obtained for each layout and location were weighted taking into account the number of easterly and westerly sea states in the last 25 years wave farms situated at the east part of the study site provide negative results with erosion reaching 30 m2 for the 3 row layout with an inter device spacing of 1 d this is mainly due to the negative effects produced for easterly storms for wave farms situated to the west off salobrea rock the impact produced by the wave farm is lower with almost no difference in dry beach surface the full layout and location of the optimum wave farm is shown in fig 10 again 2 rows with an inter wec spacing of 2d is the best layout the optimum location is the central part of the beach slightly to the east of the best location for easterly waves the weighted beach surface difference obtained with the optimum wave farm during the duration considered 48 h section 3 2 is 29 59 m2 thus the optimum wave farm leads to an increase in dry beach area with respect to the natural no farm situation equal to 5400 18 m2 per year this would mitigate the erosion problems at the study area where the loss of dry beach area since river damming has been about 4300 m 2 year bergillos et al 2016a reverting the shoreline behaviour from erosion to accretion i e resulting in a gain of dry beach area fig 11 these results are in agreement with those achieved in previous studies rodriguez delgado et al 2018a studied the implication of the alongshore position of a wave farm keeping constant the layout and the best results were achieved in a similar position and with a similar dry beach surface difference than the obtained in this work 25 58 m2 per 48 h in addition with respect to the layout previous studies rodriguez delgado et al 2018b 2019 have also achieved similar results being the optimum layout that composed by two rows and an inter wec spacing of 2 d this consistency highlights the efficiency of the ann to reproduce the coastal protection performance of wave farms 5 conclusions wave farms have recently been proposed as elements for coastal defence as well as carbon free energy generators their efficiency in coastal protection depends on many factors the position of the wave farm with respect of the coast the layout adopted for the wecs the spacing between devices and the wave climate anns are able to provide the best fit to a function based on a dataset of inputs and targets in this work anns were applied to optimize the layout and position of a wave farm for coastal protection purposes forty different architectures including one and two hidden layer systems were considered and two training algorithms were used in a comparative study in which anns were trained 101 times architectures with one hidden layer yielded a lower rmse in the test dataset with 10 20 neurons in the hidden layer architectures with fewer hidden layers were not able to apprehend properly the information in the dataset and therefore yielded higher training and test errors ann models with a larger number of neurons improved the training rmse albeit at the expense of the test error implying that they are not able to properly generalize inputs not included in the training dataset two hidden layer architectures improved the training rmse but did not yield lower test errors than the one hidden layer models with respect to the training algorithms anns trained with the bayesian regression algorithm exhibited a certain superiority over those trained with the levenberg marquadt algorithm the best results overall were achieved by the 5 10 1 architecture trained with the bayesian regression algorithm the rmse was 3 23 m2 and 4 22 m2 for the test and train datasets respectively the validation of the ann was confirmed by the correlation coefficient for the train and test datasets 0 9713 and 0 9489 respectively in order to illustrate the potential of this methodology the ann model once validated was used to determine the optimum location and layout in terms of coastal protection for a wave farm in the study area the result was a wave farm with two rows and an inter wec spacing of 180 m 2d situated approximately off the centre of playa granada the methodology presented can be useful for designers managers and stakeholders to optimize the position and layout of a wave farm project where coastal protection in addition to carbon free energy is sought however further work is needed for the development of wave farms as coastal protection elements first more research is needed in order to generalize the results achieved in this paper to other beaches with a different sediment composition e g sandy or shingle beaches and also with a wider selection of sea states in the long term in addition more design variables as the different type of mooring or the control strategy could be included in the ann model this would be the basis to convert the proposed ann model to a real time application finally the influence of the geometry of wec devices needs to be considered also in future research works acknowledgements this research was supported by the projects ice intelligent community energy european commission contract no 5025 and waveimpact pcig 13 ga 2013 618556 european commission marie curie fellowship fellow g iglesias and the coastal ocean and sediment transport coast engineering research group university of plymouth uk the work of the second author was partly funded by the university of granada through programa de contratos puente 2017 and the spanish ministry of science innovation and universities through programa juan de la cierva 2017 fjci 2017 31781 maritime and bathymetric data were provided by puertos del estado and ministerio de agricultura y pesca alimentacin y medio ambiente spain respectively we thank four anonymous reviewers for their improvements to this work nomenclature  a subaerial beach surface differences  s sediment density tan  bed slope of the surf zone  mean wave direction  s b r angle between the wave front and the shoreline at breaking d c sum of the closure depth and the height of the berm d 50 sediment size h s significant wave height h s b r significant wave height at breaking k r reflection coefficient k t transmission coefficient k s w e l l coefficient accounting for the influence of swell waves and wave period on longshore sediment transport m i output produced by the artificial neural network q m longshore sediment transport rate mass kg s q t longshore sediment transport rate volume m 3 s y t e s t best linear fit of the test dataset y t r a i n best linear fit of the training dataset ai artificial intelligence ann artificial neural network e target value of  a g acceleration of gravity l alongshore location lst longshore sediment transport mse mean square error n number of cases r correlation coefficient r o number of rows rmse root mean squared error s inter wec spacing wec wave energy converter 
26187,in this work a novel approach based on artificial intelligence ai to assess the efficiency of wave energy converter wec farms in coastal protection isdeveloped we consider as a case study a beach subjected to severe erosion playa granada s spain more specifically we analyse the changes in the dry beach area quantified through the pelnard considre equation with and without wave farm protection by means of an artificial neural network ann model the model is selected after a thorough comparative study involving forty ann architectures with one and two hidden layers and two training algorithms levenberg marquadt and bayesian regression the best results are obtained with a 5 10 1 architecture trained with the bayesian regression algorithm once validated this ann model is applied to optimize the design and position of the wave farm the results confirm that ann models are a useful design tool for hybrid wave farms keywords coastal erosion coastal processes ann model wave energy ocean energy renewable energy 1 introduction the demand of energy has soared over the last decades partly due to the growth of emerging economies this demand is in part met by fossil fuels albeit at the expense of severe environmental repercussions not least global warming in this context it is essential to develop and promote the use of renewable energy sources among them wave energy is one of the most promising thanks to its widespread availability greaves and iglesias 2018 cornett 2008 uihlein and magagna 2016 and comparatively low environmental effects clment et al 2002 palha et al 2010 the research progresses on wave energy so far have been focused on various aspects existing wave energy converter technologies can be divided into three main types oscillating water column owc viviano et al 2016 oscillating bodies falco 2007 and overtopping devices among the latter offshore floating type devices such as wave dragon kofoed et al 2006 and wavecat fernandez et al 2012 can be used for coastal protection purposes as they act as obstacles for wave propagation varying the wave height on its lee the interaction between wavecat and the wave field was studied by carballo and iglesias 2013 through laboratory experiments this was the basis to study the impacts of this wec device on the coast abanades et al 2014 and bergillos et al 2018 studied the variation of the beach profile produced by the presence of a wave farm on sandy and gravel beaches respectively showing that wave farms can be used to counter erosion these studies were extended to the whole coastline and the influence of different aspects such as the wave farm location rodriguez delgado et al 2018a layout rodriguez delgado et al 2018b and inter device spacing rodriguez delgado et al 2019 were assessed from these studies dual wave farms have been recently proposed to fulfil a dual function carbon free energy production and coastal protection this dual function becomes very important for the developing of wave energy converters as the number of eroded beaches have grown up in recent years mainly due to human interventions syvitski et al 2009 anthony et al 2014 climate change mills et al 2018 and the consequent sea level rise chu et al 2014 clough et al 2016 cozannet et al 2015 artificial intelligence has been used as a tool to solve numerically complex problems where a number of variables are involved in the field of wave energy and ocean engineering in recent years wave height forecasting has been improved by means of diverse artificial intelligence techniques such as genetic programming gaur and deo 2008 neuro wavelet technique deka and prahlada 2012 dixit and londhe 2016 and sequential learning neural networks kumar et al 2017 extreme learning machine kumar et al 2018 bayesian methods callaghan et al 2018 harpham et al 2016 and artificial neural networks oh and suh 2018 in addition artificial intelligence has been applied to optimize energy converter systems in wind farms lin et al 2013 ou et al 2017 pv generation systems ou and hong 2014 and transformers ou et al 2016 moreover artificial neural networks have been used to generate a virtual laboratory to optimize owc systems lpez and iglesias 2014 and to predict the evolution of headland bay beaches iglesias et al 2010 however to our best knowledge a methodological approach based on artificial intelligence has not been proposed and applied to the design of wave farms for coastal defence purposes artifical neural networks anns arise as a useful tool as they can deal with non linear function regression in this vein anns can be applied to optimize the coastal protection performance of a wave farm reducing the computational cost of physically based numerical models the main objective of the present paper is to develop an ann that enables the prediction of the efficiency in coastal protection of a wave farm at a certain location and with a certain layout the study area playa granada in southern spain is presented in detail in section 2 the data for training and validating the different anns considered in this work were obtained by means of a wave propagation model swan a longshore sediment transport lst equation van rijn 2014 and the one line model pelnard considre 1956 for different locations and configurations of the farm inter wec spacings and incident wave conditions as detailed in section 3 2 study area playa granada is a gravel dominated beach located on the mediterranean coast of southern spain fig 1 this stretch of coast which is limited to the west by the guadalfeo river mouth and to the east by a shoreline horn punta del santo has been experiencing coastline retreat and strong erosion problems over last decade fig 2 mainly induced by the river regulation in 2004 bergillos et al 2015 the reduction in dry beach area since then until 2014 was greater than 4000 m 2 year with a maximum coastline retreat equal to 87 m bergillos et al 2016a to mitigate these problems successive nourishment interventions have been performed however they have not been effective in the long term bergillos et al 2016b 2017 consequently other management strategies are required to mitigate coastal erosion problems on the other hand recent works devoted to the design of wecs have proven that wecs optimally extract wave energy when they are forced by short wave periods lpez et al 2015a 2015b 2015c jaln et al 2016 like those of the study area where the wave energy potential has been recently assessed lpez ruiz et al 2016 in addition wave propagation and sediment transport models have been previously validated in the study area through comparison with field data bergillos et al 2017 thus this site represents an ideal location for the design of wave farms with the hybrid function of wave energy generation and coastal defence 3 materials and methods the proposed methodology is depicted in fig 3 the dataset for the training process of the artificial neural networks was obtained by means of a set of numerical models including wave propagation sediment transport formulation and shoreline evolution different training algorithms and architectures were tested in order to choose the configuration that yielded the best fit to the available data once the final artificial neural network configuration was chosen the wave farm which provided the best performance in coastal protection was obtained 3 1 artificial neural networks artificial neural networks anns are systems composed by a group of individual elements called neurons which work in parallel haykin 1994 he and xu 2010 in this work multilayer feed forward anns are used as they have been successfully applied to function approximation and non linear regression they are based on the connections of real neurons inside the brain hence the name artificial intelligence anns learn from experience much as biological neurons in a process called training before the training the dataset is divided in two groups for training and testing during the training the weights and biases that form the connection between the artificial neurons are adjusted through the training dataset inputs and targets first an input vector is presented to the network and propagated forward until an output is produced then during the testing process the mse between the produced output and the desired target value is computed and the weights of each neuron are adjusted to minimise the error this process is repeated until the maximum number of iterations is reached or the desired gradient of the error is achieved finally the generalization capabilities of the ann are assessed through the comparison of the targets and the outputs produced by the ann for the test dataset a multilayer feedforward ann is composed by an input layer one or more hidden layers and an output layer fig 4 the output of each neuron is produced by means of a transfer function although a number of derivable functions can be used with this purpose in this work the sigmoidal transfer function was used for the hidden layer neurons this function is defined by 1 y 2 1 e 2 x 1 1 in the output layer a pure linear function was used this function is expressed as 2 y x two different training algorithms were applied in this work first the levenberg marquardt algorithm which results in a faster training process hagan and menhaj 1994 in this first case the test dataset is divided in two groups validation and test during each iteration the mse of the validation dataset is computed and the training finishes when this value increases between two iterations second the bayesian regularization algorithm which requires more time but usually achieves good generalization results for difficult or noisy data mackay 1992 in this case no validation dataset is required and the training is terminated based on weight minimization these two training algorithms have been successfully used in the field of coastal engineering in different studies involving wec efficiency lpez and iglesias 2014 wave forecasting jain and deo 2007 and shoreline position prediction iglesias et al 2009 3 2 numerical models the purpose of the ann developed in this work is to optimize the layout and position of a wave farm for coastal protection purposes the dataset used to train and test the ann was obtained through a coupled numerical scheme composed by a wave propagation model an lst formulation and a shoreline evolution model these models have been successfully applied to the study area in previous works bergillos et al 2017 rodriguez delgado et al 2018a 2018b combinations of two wave heights 3 1 and 0 5 m corresponding to high and low energy conditions and two mean wave directions west 238 and east 107 were considered these wave heights have been proved to be representative of the wave climate of playa granada in recent works rodriguez delgado et al 2018a 2018b 2019 bergillos et al 2016 the duration of the wave attack for both high and low energy conditions was 48 h these sea states were propagated by means of swan holthuijsen et al 1993 for this purpose two nested computational grids were used a first coarser grid composed by 82 82 cells with sizes varying with depth from 170 65 m to 80 80 m the finer grid covers the wave farm area it has 244 82 cells with a cell size of 25 15 m the spectral resolution of the frequency space consisted of 37 logarithmically distributed frequencies ranging from 0 03 to 1 hz for the directional space the 360 were covered by 72 directions in increments of 5 for the configuration of the wave farm the following parameters were included i the location of the farm ii the number of rows in which the wecs are arranged and iii the inter wec spacing altogether 6 different locations wave farms composed by 1 4 rows and four different spacings from 1d to 4d with d 90 m were implemented in the numerical models fig 5 the number of wecs in each wave farm configuration was fixed 11 each configuration was modelled under the four aforementioned sea states giving a total of 384 cases the sea states were propagated from deep water to the nearshore region using the wave propagation numerical model swan holthuijsen et al 1993 this numerical model was previously calibrated and validated in the study zone for more information about the calibration of the model the reader is referred to bergillos et al 2017 the influence of the wave farm on the wave propagation patterns was also computed by means of this numerical model the wave field wec interaction was simulated using the transmission k t and reflection k r coefficients obtained by 9 for the wavecat device this wec is an overtopping type device formed by two movable hulls similar to those found in a catamaran hence its name and has been widely used in the literature as a wec for dual wave farms i e energy generation and coastal protection purposes rodriguez delgado et al 2018a 2018b abanades et al 2018 breaking parameters obtained from swan were used as input variables for the lst formulation developed by van rijn 2014 which is applicable to sandy gravel and shingle beaches this equation can be expressed as follows 3 q m 0 00018 k s w e l l  s g 0 5 tan  0 4 d 50 0 6 h s b r 3 1 sin 2  b r where q m is the lst rate k s w e l l a coefficient accounting for the influence of the swell waves and peak period  s the sediment density g the acceleration of gravity tan  the bed slope of the surf zone d 50 the size of the sediment h s b r the significant wave height at breaking and  b r the angle between the wave front and the shoreline at breaking finally the lst rate obtained in this way were used to apply a one line model pelnard considre 1956 to account for the changes in the shoreline position induced by lst this model is described as 4 y s t 1 d c q t x where y s is the shoreline position d c is a characteristic length taken as the sum of the depth of closure and the height of the berm and q t is the lst rate the joint application of the formulation proposed by van rijn 2014 and the one line model was also validated for the study zone in previous works bergillos et al 2017 in this way the subaerial beach surface differences  a between the natural scenario and the different configurations of the wave farm were obtained as the difference between the final and the initial position of the shoreline in playa granada this allowed obtaining a dataset composed by 384 values of  a which were the basis for training and testing the ann 3 3 ann implementation once the dataset is obtained the first step in the ann implementation is to define the input and output variables as unique output variable in order to characterize the coastal protection performance of the wave farm the beach surface difference  a was chosen as input variables the characteristics of the wave farm were represented by the alongshore location l the number of rows r o and the inter wec spacing s fig 5 in addition the influence of the wave climate was included by means of the significant wave height h s and the mean wave direction  thus the ann model can be described as 5  a f a n n l r o s h s  the second step is to divide the full dataset into the training validation and test datasets required for the development of the ann the training dataset encompassed 264 cases 70 whereas 60 cases 15 were used for the validation dataset and the remaining 60 cases 15 formed the test dataset in order to represent the variety of the cases in the test dataset each one of the 6 locations studied were represented by 10 randomly chosen cases ensuring that these cases contain the full range of rows spacings wave heights and directions studied the 60 cases for the validation dataset were chosen in the same way and the remaining cases formed the training set the optimum architecture for an ann i e the optimum number of neurons and layers depends on the number of cases inputs and outputs variables the characteristics of the dataset and consequently the function to approximate a low number of neurons could lead to a poor function fitting by increasing the number of neurons the network is able to reproduce higher order statistics of the data but at the same time its ability to generalize and represent accurately the function for data other than those included in the training process decreases walczak and cerpa 1999 in this way the optimum architecture for an ann is frequently chosen based on a trial and error process haykin 1994 the sizes of the input and output layers are determined by the number of input 5 and output 1 variables respectively for the hidden layer s a comparative study including 72 different architectures with one and two hidden layers was carried out in the case of one hidden layer the architectures analysed were 5 2 1 5 5 1 5 10 1 5 15 1 5 20 1 5 25 1 5 30 1 and 5 35 1 the remaining 64 architectures included two hidden layers with 5 10 20 or 30 neurons in the first layer and the size of the second layer varying from 2 to 35 neurons the effect of the training algorithm was investigated as well each of the architectures were trained using the levenberg marquardt and the bayesian regularization algorithms to compensate for the effect on the training of the initial weights each ann was trained 101 times and the average ability to learn and generalize the dataset was established to properly compare the performance of each architecture the test dataset was the same throughout the study the benchmark selected to account for the ann performance was the root mean squared error rmse computed as 6 rmse i 1 n e i m i 2 n where e is the target value of  a m i is the output produced by the ann and n is the number of cases 3 4 wave farm optimization in the study zone once the final ann was selected based on the comparative study an optimization of the wave farm location and layout for the study area was carried out to illustrate the potential of the proposed methodology using the final ann the different configurations of rows inter device spacings significant wave heights and mean directions were assessed in 100 locations between l1 and l6 based on the outputs provided by the ann the optimum layout and location in terms of coastal protection were selected 4 results 4 1 comparison of the different architectures and algorithms the results of the comparative study of the different architectures are described in this section fig 6 depicts the average rmse for the architectures with one hidden layer in the case of the levenberg marquardt algorithm the over fitting of the ann with a higher number of neurons in the hidden layer is clearly noticeable the average rmse in the train dataset decreases from 8 78 m2 with the 5 2 1 architecture to 3 91 m2 with the 5 35 1 architecture however the validation and test dataset do not show this steady reduction for the 5 2 1 architecture the average rmse of the validation train dataset is 8 69 m2 9 m2 then the average error decreases by up to 5 84 m2 5 47 m2 with the 5 20 1 architecture and increases from that value up to 6 48 m2 6 09 m2 with the 5 35 1 architecture the architectures trained with the bayesian regularization algorithm do not improve the average rmse for the train dataset increasing the number of neurons in the hidden layer instead the average error decreases from 9 64 m2 in 5 2 1 to 3 92 m2 in 5 20 1 however from that point the average train rmse increases to a maximum of 12 87 m2 with the 5 35 1 architecture the good generalization ability of the anns tested is demonstrated by the average rmse with the test data an initial decrease of the error is observed up to a minimum average rmse of 4 77 m2 with the 5 10 1 architecture then the error increases up to the maximum 12 m2 with the 5 35 1 architecture the two hidden layer architectures trained with the levenberg marquardt algorithm show the same overt fitting behaviour than those with one hidden layer fig 7 the rmse for the train data decreases when the number of neurons in the second hidden layer increases reaching a minimum average rmse of 2 31 m2 in the 5 30 35 1 architecture however the average rmses with the validation and test data are higher the minimum average rmse with the test data is 5 72 m2 for the 5 5 20 1 architecture ranging between 7 8 m2 and 11 6 m2 for the architectures with 30 neurons in the first hidden layer this over fitting is even more significant for the architectures trained with the bayesian regression algorithm in this case the minimum average rmse is 2 5 10 5 m2 for the 5 20 10 1 architecture reaching the same order of magnitude in the architectures with 30 neurons in the first hidden layer despite these good training errors the generalization abilities of this kind of architectures are lower due to its greater errors for the test dataset the minimum average rmse of the test dataset is 5 1 m2 and is obtained with the 5 30 20 1 architecture whereas the maximum average error 8 2 m2 is provided by the 5 5 35 1 architecture the comparative study highlights that the architectures with one hidden layer have a better performance than the two hidden layer systems the minimum average rmse is 5 47 m2 4 77 m2 for one hidden layer architectures trained with the levenberg marquardt bayesian regularization algorithm whereas the minimum error is 5 72 m2 5 1 m2 in the two hidden layer systems regarding the algorithm it is clear that bayesian regularization performs better providing lower errors with both one and two hidden layer architectures however it is important to note that the bayesian regularization algorithm requires greater computational effort and the mean running time was 33 greater than for levenberg marquardt this is mainly due to the fact that equilibrium in mse is reached before with the levenberg marquadt algorithm as this method stops training due to the validation process 4 2 final ann model after the comparative study the architecture 5 10 1 trained with the bayesian regularization algorithm yielded the best results between the 101 training runs carried out the ann whose rmse was closer to the median value was chosen the good results in the training which is the first step for a good performance of the ann are proved by the rmse a mere 3 23 m2 with the training dataset as may be observed in fig 8 a the ann provides an excellent fit to the modelled data in the training dataset most importantly the rmse with the test data is 4 22 m2 this low error value indicates that the ann is able to calculate accurately the beach surface differences for cases which were not included in the training fig 8b the linear regressions for the training and test datasets confirm these results fig 9 in the case of the training dataset the best linear fit is given by 7 y t r a i n 0 9391 x 0 2705 which is really close to the perfect diagonal y x the correlation coefficient is r 0 9713 this means that the ann has been able to learn correctly the dataset which was presented during the training process finally the correlation coefficient with the test data is also excellent r 0 9489 in this case the best linear fit is 8 y t e s t 1 033 x 0 3225 which is also very close to the diagonal the low rmse and the good results of the linear fit with the test data confirm that the ann is able to calculate accurately the subaerial beach surface difference for cases that are new to the model i e that did not form part of the training data the composition of the test data which encompassed different locations number of rows spacings significant wave heights and mean directions are taken into account corroborates that the selected ann is able to reproduce the beach surface difference irrespectively of the wave farm layout and wave conditions on this basis the ann model is fully validated 4 3 wave farm optimization with the ann described in the previous section 100 different locations of the wave farm between l1 and l6 were studied the same ranges of rows spacings significant wave heights and mean directions as in the training were used the results vary depending on the mean direction under westerly waves the best location is situated close to the central part of playa granada fig 10 this wave farm is formed by 2 rows with an inter wec spacing of 180 m 2d and yields a surface difference of 27 4 m2 alongshore positions westwards provide worse results for all the studied layouts with even erosion of about 5 m2 for wave farms situated near salobrea rock under easterly waves the same layout is found to be optimum but located eastward close to punta del santo leading to a surface difference of 48 17 m2 on the contrary wave farms situated eastwards induce negative results with losses of dry beach area about 60 m2 due to lower sediment transport from poniente beach specially for wave farms composed by 2 and 3 rows and inter device spacings of 2 d and 3 d positive effects are observed in the rest of the alongshore positions and layouts although these positive effects are lower westwards in order to assess the best overall location for a wave farm in the study area the surface differences obtained for each layout and location were weighted taking into account the number of easterly and westerly sea states in the last 25 years wave farms situated at the east part of the study site provide negative results with erosion reaching 30 m2 for the 3 row layout with an inter device spacing of 1 d this is mainly due to the negative effects produced for easterly storms for wave farms situated to the west off salobrea rock the impact produced by the wave farm is lower with almost no difference in dry beach surface the full layout and location of the optimum wave farm is shown in fig 10 again 2 rows with an inter wec spacing of 2d is the best layout the optimum location is the central part of the beach slightly to the east of the best location for easterly waves the weighted beach surface difference obtained with the optimum wave farm during the duration considered 48 h section 3 2 is 29 59 m2 thus the optimum wave farm leads to an increase in dry beach area with respect to the natural no farm situation equal to 5400 18 m2 per year this would mitigate the erosion problems at the study area where the loss of dry beach area since river damming has been about 4300 m 2 year bergillos et al 2016a reverting the shoreline behaviour from erosion to accretion i e resulting in a gain of dry beach area fig 11 these results are in agreement with those achieved in previous studies rodriguez delgado et al 2018a studied the implication of the alongshore position of a wave farm keeping constant the layout and the best results were achieved in a similar position and with a similar dry beach surface difference than the obtained in this work 25 58 m2 per 48 h in addition with respect to the layout previous studies rodriguez delgado et al 2018b 2019 have also achieved similar results being the optimum layout that composed by two rows and an inter wec spacing of 2 d this consistency highlights the efficiency of the ann to reproduce the coastal protection performance of wave farms 5 conclusions wave farms have recently been proposed as elements for coastal defence as well as carbon free energy generators their efficiency in coastal protection depends on many factors the position of the wave farm with respect of the coast the layout adopted for the wecs the spacing between devices and the wave climate anns are able to provide the best fit to a function based on a dataset of inputs and targets in this work anns were applied to optimize the layout and position of a wave farm for coastal protection purposes forty different architectures including one and two hidden layer systems were considered and two training algorithms were used in a comparative study in which anns were trained 101 times architectures with one hidden layer yielded a lower rmse in the test dataset with 10 20 neurons in the hidden layer architectures with fewer hidden layers were not able to apprehend properly the information in the dataset and therefore yielded higher training and test errors ann models with a larger number of neurons improved the training rmse albeit at the expense of the test error implying that they are not able to properly generalize inputs not included in the training dataset two hidden layer architectures improved the training rmse but did not yield lower test errors than the one hidden layer models with respect to the training algorithms anns trained with the bayesian regression algorithm exhibited a certain superiority over those trained with the levenberg marquadt algorithm the best results overall were achieved by the 5 10 1 architecture trained with the bayesian regression algorithm the rmse was 3 23 m2 and 4 22 m2 for the test and train datasets respectively the validation of the ann was confirmed by the correlation coefficient for the train and test datasets 0 9713 and 0 9489 respectively in order to illustrate the potential of this methodology the ann model once validated was used to determine the optimum location and layout in terms of coastal protection for a wave farm in the study area the result was a wave farm with two rows and an inter wec spacing of 180 m 2d situated approximately off the centre of playa granada the methodology presented can be useful for designers managers and stakeholders to optimize the position and layout of a wave farm project where coastal protection in addition to carbon free energy is sought however further work is needed for the development of wave farms as coastal protection elements first more research is needed in order to generalize the results achieved in this paper to other beaches with a different sediment composition e g sandy or shingle beaches and also with a wider selection of sea states in the long term in addition more design variables as the different type of mooring or the control strategy could be included in the ann model this would be the basis to convert the proposed ann model to a real time application finally the influence of the geometry of wec devices needs to be considered also in future research works acknowledgements this research was supported by the projects ice intelligent community energy european commission contract no 5025 and waveimpact pcig 13 ga 2013 618556 european commission marie curie fellowship fellow g iglesias and the coastal ocean and sediment transport coast engineering research group university of plymouth uk the work of the second author was partly funded by the university of granada through programa de contratos puente 2017 and the spanish ministry of science innovation and universities through programa juan de la cierva 2017 fjci 2017 31781 maritime and bathymetric data were provided by puertos del estado and ministerio de agricultura y pesca alimentacin y medio ambiente spain respectively we thank four anonymous reviewers for their improvements to this work nomenclature  a subaerial beach surface differences  s sediment density tan  bed slope of the surf zone  mean wave direction  s b r angle between the wave front and the shoreline at breaking d c sum of the closure depth and the height of the berm d 50 sediment size h s significant wave height h s b r significant wave height at breaking k r reflection coefficient k t transmission coefficient k s w e l l coefficient accounting for the influence of swell waves and wave period on longshore sediment transport m i output produced by the artificial neural network q m longshore sediment transport rate mass kg s q t longshore sediment transport rate volume m 3 s y t e s t best linear fit of the test dataset y t r a i n best linear fit of the training dataset ai artificial intelligence ann artificial neural network e target value of  a g acceleration of gravity l alongshore location lst longshore sediment transport mse mean square error n number of cases r correlation coefficient r o number of rows rmse root mean squared error s inter wec spacing wec wave energy converter 
26188,sediment connectivity in rivers directly links to fluvial processes and eco system services modelling network scale sediment connectivity and its response to anthropic alterations such as dams or land use changes is key to better understanding river processes and to inform river basin management this paper contributes a matlabtm toolbox for network scale sediment connectivity based on an implementation of the cascade catchment sediment connectivity and delivery model cascade combines concepts of graph theory with empirical sediment transport formulas to quantify sediment transfers between many connected sediment sources and sinks in a river network greater numerical efficiency compared to common hydrodynamic models enables application to large river networks stochastic simulations of sediment connectivity and screening impacts of many infrastructure portfolios input data requirements are flexible and basic functionality is available with globally available datasets to ensure applicability to data scarce basins the toolbox offers options for customization and interactive output visualization tools keywords sediment connectivity strategic dam planning river basin management matlab toolbox 1 introduction river sediment connectivity describes the transfer of sediment between many connected sources and sinks in a river network sediment connectivity plays a crucial role in river management and conservation as it drives processes forming the physical shape morphology of river channels which controls for example the provision of habitat or the stability of river bed and banks connectivity is an emergent property of river networks including not only spatial relations between sediment sources and sinks but also timing of sediment transfers and process rates fryirs et al 2007 bracken et al 2015 heckmann et al 2018 wohl et al 2019 these process rates vary across the many sediment sources in a river network as a function of supply rates supplied grain sizes and the transport capacity of the river network the transport capacity is in turn a function of the hydro morphologic properties of the network i e river gradient width and discharge and of the river grain size and the resulting grain size distribution of transported sediment these factors make sediment connectivity of significantly higher complexity than the mere topological connectivity of a river network supply and transport processes driving sediment connectivity have been greatly altered in many rivers by anthropic disturbances gregory 2019 clment and pigay 2003 surian and rinaldi 2003 simon and rinaldi 2006 poeppl et al 2017 land use change such as deforestation and mining increases rates and changes the grain size of sediment supply reservoirs typically trap parts of the incoming sediment and reduce sediment supply to the downstream network kondolf et al 2014 changing discharge magnitude and pattern e g because of hydroclimatic changes or hydropower operations alters the conveyance capacity of river channels bizzi et al 2015 together these competing or compounding drivers cumulatively impact sediment connectivity and lead to major shifts in river processes on local and whole network scales fryirs and brierley 2001 syvitski et al 2009 kondolf et al 2014 dufour et al 2015 network scale models for sediment transport are hence a prerequisite to analyse human impacts and develop river basins plans with less impacts on sediment transport and related ecosystems services currently there is a gap in the ability to model sediment connectivity at the network scale morphodynamic models for engineering and research applications allow modelling river morphologic processes in 1d 2d and 3d with high accuracy however their computational demand and their data needs make them generally limited to specific well monitored river sections or even in case of simplified model formulations to river segments where well defined boundary conditions of sediment supply and hydro morphology can be defined briere et al 2011 lammers and bledsoe 2018 even where data and computational resources are available such models can hence not take the connected nature of sediment transfers into account in which a single river segment is influenced by sediment supply and transport in the entire upstream river network merritt et al 2003 fryirs et al 2007 recently numerical models of network scale sediment connectivity emerged betrie et al 2011 ranzi et al 2012 czuba and foufoula georgiou 2014 2015 schmitt et al 2016 coulthard and van de wiel 2017 czuba et al 2017 czuba 2018 a development which was largely enabled by network scale derivation of relevant data from remote sensing schmitt et al 2014 demarchi et al 2017 bizzi et al 2018 cascade catchment sediment connectivity and delivery schmitt et al 2016 belongs to this group of models designed to merge the representation of relevant complexity in network sediment transport with simplifications required to make their application practical for large river networks and river basin management tasks schmitt et al 2018a 2018b in this paper we present the implementation of cascade as an open source matlab toolbox the toolbox simulates sediment fluxes and their provenances for any reach in a network and allows to integrate disturbances such as dams and barriers and natural and man made local sediment supply features such as debris flows flow alluvial fans or mines the toolbox comes with an interactive user interface supporting visual analytics of model outputs to explore how anthropic and natural alterations of sediment connectivity affect sediment fluxes from the reach to the network scale 2 cascade model fig 1 details the three steps to simulate sediment connectivity in a river network using the cascade model in a nutshell cascade first extracts the river network and assigns hydro morphologic parameters to each reach fig 1 a second additional features are added to the network such features can be sediment sources defined in terms of sediment supply rates and supplied grain size distribution according to the wentworth 1922 classification as well as dams and other disturbances fig 1 b third connectivity for each sediment class from each source is calculated by applying either of four empirical sediment transport formulas at the network scale the outputs can be interpreted in several ways allowing to track the fate of sediment from a specific source as well as determining the sediment flux and origins from many sediment sources from the perspective of a downstream reach fig 1 c 2 1 network extraction and hydromorphological characterization as input the model requires a river network fig 1 a represented as a directed graph consisting of nodes and reaches in the network each reach connects two nodes and represents a part of the river network with homogeneous geomorphic and hydraulic features the reach is the core modelling unit in cascade as sediment transport rates are calculated based on reach averaged values of geomorphic and hydraulic features extraction of reaches can be automatized based on a user defined maximum length network topology heckmann et al 2015 or by user supplied break points the latter can be used for example to segment the network according to a geomorphic analysis or at the location of dam sites the cascade toolbox includes all the functionalities to extract and preprocess a river network run the cascade model and visualize and analyse results the toolbox relies on topotoolbox schwanghart and scherler 2014 for river network extraction from a digital elevation model dem each reach is assigned a set of hydromorphologic attributes related to sediment transport see table 1 some of these attributes such as channel slope length and drainage area are derived from the dem the remaining attributes such as discharge and active channel width require external data these data can be derived from models surveys field data or a mixture of those finally grainsize distribution gsd in the bed surface d16 d50 d84 and manning s roughness coefficient need to be provided for each reach this data is rarely available on network scale and can be initialized using interpolation from scattered field observations or based on hypothesis about regime calculations ferguson et al 2015 cascade models the instantaneous sediment transport fluxes in kg s in each reach for a single discharge value e g the mean annual discharge sediment transport during different discharge conditions can be represented through individual model runs the hydrologic and geomorphic attributes are used to compute the transport capacity of each reach i e the amount of energy available in a reach for the transport of sediment of a specific size four alternative sediment transport formula are implemented in cascade wilcock and crowe 2003 engelund and hansen 1967 yang 1973 wong and parker 2006 2 2 external sediment sources and barriers the cascade toolbox can account for user defined sediment sources from the hillslopes or river banks in fig 1 b and dams and barriers that retain sediment triangle in fig 1 b sediment sources are defined according to their supply rate and the grain size distribution of supplied sediment dams and barriers can be modeled using simple representation of reservoir hydraulics schmitt et al 2018a empirical formulas stevens 2000 or from observed trapping rates 2 3 multigraph expansion and sediment routing after the network extraction the definition of reach attributes the addition of external sediment sources and the calculation of the transport capacity cascade can use these data to simulate sediment connectivity to do so cascade uses principles of graph theory to describe the spatial relation between sediment sources and sinks in the river network heckmann et al 2015 sediment transport in the river network is described as a combination of cascades each representing the sediment transport from a specific sediment source through the downstream network each cascade originates from a source in the network black dots in fig 1 c and its path toward the outlet node of the network is defined by the network topology as a cascade traverses multiple downstream reaches it might deposit part of its original sediment supply if the the sediment flux exceeds the transport capacity a cascade is terminated when the entire sediment supply has deposited or at the outlet a river reach can be traversed by many cascades each from a specific upstream source and hence with a specific sediment flux and grain size distribution the original river graph in which each edge represented a river reach is hence expanded into a multi graph in which each multi edge resents a sediment cascade in a specific river reach schmitt et al 2016 compared to previous cascade applications schmitt et al 2016 schmitt et al 2018b sediment supply in this toolbox is defined not by a single grain size but by sediment distribution this allows for a more thruthful representation of sediment sources thus each cascade is expanded to represent the transport of multiple grain size classes fig 2 a by default the toolbox uses 18 grain size classes in each reach the model calculates sediment transport for m x c cascades where m is the number of upstream sources and c is the user defined number of grainsize classes cascade computes a budget for each grain size in each reach this means that if the flux of sediment into an edge exceeds the transport capacity of that edge part of the sediment will be deposited on the river bed if instead transport capacity is higher than the influx new sediment can be entrained and transported if available from the river bed or hillslope contributions generating a new cascade 3 toolbox structure fig 3 shows the file structure of the cascade toolbox the application of the cascade model is executed in three steps 1 extraction of the river network from a dem identification of the river network reaches and definition of the reach features table 1 this information is organized as a data structure named reachdata 2 evaluation of the sediment fluxes in the network with the cascade model 3 visualization and interpretation of the model outputs the functions in folder extract river network extract the river network and some morphologic parameters length slope from a dem users can customize the resulting river network by changing the inputs to the network extraction function see list in table 2 the network is stored in the reachdata struct the main input to the cascade model each row in reachdata represents a single reach in the network and columns contain reach features listed in table 1 features not extracted from the dem are inserted by the user according to the case study the reachdata struct can be exported as shapefile for visualization and further processing in any gis environment cascade uses reachdata to model sediment transport and connectivity the main folder cascade model contains the operations necessary for running cascade the function in the folder requires the reachdata struct as an input if sediment sources or dams are present they are included in the model with the damdata and extdata struct for dams and external sediment flow respectively refer to the user manual for additional information of the implementation of these data structures the cascade toolbox allows for the different customization options for the framework including different methods of sediment transport capacity calculation and hydraulic features estimation the possibility of limiting the sediment availability of the reaches and thus the flow of sediment a reach can entrain in the model and the incorporation of dams and additional sediment fluxes table 2 contains the full list of the customization options available in the released version of the cascade toolbox a user interface with dialog boxes is also included to guide in the selection of the different options of the model for first time users 4 outputs and visualization options the main cascade toolbox outputs are three dimensional n x m x c matrices representing sediment transport and deposition in the river network q b t r and q b d e p in this representation n and m indicate the number of reaches and sediment sources in the network and c the number of sediment classes fig 2 c shows a graphical representation with n 4 and c 3 each cell n m c q b t r and q b d e p stores the sediment flux and deposition both in kg s of sediment class c in reach m originating from source n fig 2 b through this structure cascade allows to track the fate of each sediment class originating from any sediment source in the river network results can be aggregated on the scale of the single reach to analyse e g how much sediment is transported in total and what the statistic properties e g mean median percentile grain sizes of transported and deposited sediment are the cascade toolbox provides the user with several functions to visualize the outputs and aid in the understanding of model results the visualization tools in folder plot function include the interactive connectivity assessment ica function that guides the user in exploring the sediment connectivity on the scale of the a river network fig 4 a and fig 4 b and single reaches fig 4 d including transported load provenance and composition of the incoming cascades the function connectivity alteration assessment caa is included in the same folder caa provides together with the options in ica a visual framework of sediment connectivity on which the user can interactively include or remove dams and external sediment contributions and visualize the changes in sediment transport both at the whole network scale and at the reach scale fig 4 c 5 conclusions and outlook the cascade toolbox was developed to facilitate access to the cascade model schmitt et al 2016 schmitt et al 2018a 2018b a computationally efficient parsimonious and flexible tool for network scale assessments and management of sediment connectivity the cascade toolbox contains customization options to adapt the model to the different requirements of the case study data availability the specific research and management objectives the toolbox is supported with fully commented code and a comprehensive user guide the toolbox is designed for easy data exchange between matlab and a gis environment for input data preparation data sharing and visualization we encourage the use of cascade to evaluate for network and catchment scale sediment management to leverage novel network scale data of fluvial geomorphology for conceptualizing and quantifying river geomorphological processes at the basin scale cascade toolbox and the user documentation is freely available from the cascade website http www cascademodel org and from the github repository linked to the website software availability name of software cascade toolbox version 1 0 tested on matlab r2018b developers marco tangi rafael schmitt simone bizzi andrea castelletti contact email marco tangi polimi it year first available 2019 available from cascade website http www cascademodel org acknowledgement we would like to thank all the current and past members of the natural resources management group www nrm deib polimi it at politecnico di milano who contributed to the development of the cascade toolbox appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 008 
26188,sediment connectivity in rivers directly links to fluvial processes and eco system services modelling network scale sediment connectivity and its response to anthropic alterations such as dams or land use changes is key to better understanding river processes and to inform river basin management this paper contributes a matlabtm toolbox for network scale sediment connectivity based on an implementation of the cascade catchment sediment connectivity and delivery model cascade combines concepts of graph theory with empirical sediment transport formulas to quantify sediment transfers between many connected sediment sources and sinks in a river network greater numerical efficiency compared to common hydrodynamic models enables application to large river networks stochastic simulations of sediment connectivity and screening impacts of many infrastructure portfolios input data requirements are flexible and basic functionality is available with globally available datasets to ensure applicability to data scarce basins the toolbox offers options for customization and interactive output visualization tools keywords sediment connectivity strategic dam planning river basin management matlab toolbox 1 introduction river sediment connectivity describes the transfer of sediment between many connected sources and sinks in a river network sediment connectivity plays a crucial role in river management and conservation as it drives processes forming the physical shape morphology of river channels which controls for example the provision of habitat or the stability of river bed and banks connectivity is an emergent property of river networks including not only spatial relations between sediment sources and sinks but also timing of sediment transfers and process rates fryirs et al 2007 bracken et al 2015 heckmann et al 2018 wohl et al 2019 these process rates vary across the many sediment sources in a river network as a function of supply rates supplied grain sizes and the transport capacity of the river network the transport capacity is in turn a function of the hydro morphologic properties of the network i e river gradient width and discharge and of the river grain size and the resulting grain size distribution of transported sediment these factors make sediment connectivity of significantly higher complexity than the mere topological connectivity of a river network supply and transport processes driving sediment connectivity have been greatly altered in many rivers by anthropic disturbances gregory 2019 clment and pigay 2003 surian and rinaldi 2003 simon and rinaldi 2006 poeppl et al 2017 land use change such as deforestation and mining increases rates and changes the grain size of sediment supply reservoirs typically trap parts of the incoming sediment and reduce sediment supply to the downstream network kondolf et al 2014 changing discharge magnitude and pattern e g because of hydroclimatic changes or hydropower operations alters the conveyance capacity of river channels bizzi et al 2015 together these competing or compounding drivers cumulatively impact sediment connectivity and lead to major shifts in river processes on local and whole network scales fryirs and brierley 2001 syvitski et al 2009 kondolf et al 2014 dufour et al 2015 network scale models for sediment transport are hence a prerequisite to analyse human impacts and develop river basins plans with less impacts on sediment transport and related ecosystems services currently there is a gap in the ability to model sediment connectivity at the network scale morphodynamic models for engineering and research applications allow modelling river morphologic processes in 1d 2d and 3d with high accuracy however their computational demand and their data needs make them generally limited to specific well monitored river sections or even in case of simplified model formulations to river segments where well defined boundary conditions of sediment supply and hydro morphology can be defined briere et al 2011 lammers and bledsoe 2018 even where data and computational resources are available such models can hence not take the connected nature of sediment transfers into account in which a single river segment is influenced by sediment supply and transport in the entire upstream river network merritt et al 2003 fryirs et al 2007 recently numerical models of network scale sediment connectivity emerged betrie et al 2011 ranzi et al 2012 czuba and foufoula georgiou 2014 2015 schmitt et al 2016 coulthard and van de wiel 2017 czuba et al 2017 czuba 2018 a development which was largely enabled by network scale derivation of relevant data from remote sensing schmitt et al 2014 demarchi et al 2017 bizzi et al 2018 cascade catchment sediment connectivity and delivery schmitt et al 2016 belongs to this group of models designed to merge the representation of relevant complexity in network sediment transport with simplifications required to make their application practical for large river networks and river basin management tasks schmitt et al 2018a 2018b in this paper we present the implementation of cascade as an open source matlab toolbox the toolbox simulates sediment fluxes and their provenances for any reach in a network and allows to integrate disturbances such as dams and barriers and natural and man made local sediment supply features such as debris flows flow alluvial fans or mines the toolbox comes with an interactive user interface supporting visual analytics of model outputs to explore how anthropic and natural alterations of sediment connectivity affect sediment fluxes from the reach to the network scale 2 cascade model fig 1 details the three steps to simulate sediment connectivity in a river network using the cascade model in a nutshell cascade first extracts the river network and assigns hydro morphologic parameters to each reach fig 1 a second additional features are added to the network such features can be sediment sources defined in terms of sediment supply rates and supplied grain size distribution according to the wentworth 1922 classification as well as dams and other disturbances fig 1 b third connectivity for each sediment class from each source is calculated by applying either of four empirical sediment transport formulas at the network scale the outputs can be interpreted in several ways allowing to track the fate of sediment from a specific source as well as determining the sediment flux and origins from many sediment sources from the perspective of a downstream reach fig 1 c 2 1 network extraction and hydromorphological characterization as input the model requires a river network fig 1 a represented as a directed graph consisting of nodes and reaches in the network each reach connects two nodes and represents a part of the river network with homogeneous geomorphic and hydraulic features the reach is the core modelling unit in cascade as sediment transport rates are calculated based on reach averaged values of geomorphic and hydraulic features extraction of reaches can be automatized based on a user defined maximum length network topology heckmann et al 2015 or by user supplied break points the latter can be used for example to segment the network according to a geomorphic analysis or at the location of dam sites the cascade toolbox includes all the functionalities to extract and preprocess a river network run the cascade model and visualize and analyse results the toolbox relies on topotoolbox schwanghart and scherler 2014 for river network extraction from a digital elevation model dem each reach is assigned a set of hydromorphologic attributes related to sediment transport see table 1 some of these attributes such as channel slope length and drainage area are derived from the dem the remaining attributes such as discharge and active channel width require external data these data can be derived from models surveys field data or a mixture of those finally grainsize distribution gsd in the bed surface d16 d50 d84 and manning s roughness coefficient need to be provided for each reach this data is rarely available on network scale and can be initialized using interpolation from scattered field observations or based on hypothesis about regime calculations ferguson et al 2015 cascade models the instantaneous sediment transport fluxes in kg s in each reach for a single discharge value e g the mean annual discharge sediment transport during different discharge conditions can be represented through individual model runs the hydrologic and geomorphic attributes are used to compute the transport capacity of each reach i e the amount of energy available in a reach for the transport of sediment of a specific size four alternative sediment transport formula are implemented in cascade wilcock and crowe 2003 engelund and hansen 1967 yang 1973 wong and parker 2006 2 2 external sediment sources and barriers the cascade toolbox can account for user defined sediment sources from the hillslopes or river banks in fig 1 b and dams and barriers that retain sediment triangle in fig 1 b sediment sources are defined according to their supply rate and the grain size distribution of supplied sediment dams and barriers can be modeled using simple representation of reservoir hydraulics schmitt et al 2018a empirical formulas stevens 2000 or from observed trapping rates 2 3 multigraph expansion and sediment routing after the network extraction the definition of reach attributes the addition of external sediment sources and the calculation of the transport capacity cascade can use these data to simulate sediment connectivity to do so cascade uses principles of graph theory to describe the spatial relation between sediment sources and sinks in the river network heckmann et al 2015 sediment transport in the river network is described as a combination of cascades each representing the sediment transport from a specific sediment source through the downstream network each cascade originates from a source in the network black dots in fig 1 c and its path toward the outlet node of the network is defined by the network topology as a cascade traverses multiple downstream reaches it might deposit part of its original sediment supply if the the sediment flux exceeds the transport capacity a cascade is terminated when the entire sediment supply has deposited or at the outlet a river reach can be traversed by many cascades each from a specific upstream source and hence with a specific sediment flux and grain size distribution the original river graph in which each edge represented a river reach is hence expanded into a multi graph in which each multi edge resents a sediment cascade in a specific river reach schmitt et al 2016 compared to previous cascade applications schmitt et al 2016 schmitt et al 2018b sediment supply in this toolbox is defined not by a single grain size but by sediment distribution this allows for a more thruthful representation of sediment sources thus each cascade is expanded to represent the transport of multiple grain size classes fig 2 a by default the toolbox uses 18 grain size classes in each reach the model calculates sediment transport for m x c cascades where m is the number of upstream sources and c is the user defined number of grainsize classes cascade computes a budget for each grain size in each reach this means that if the flux of sediment into an edge exceeds the transport capacity of that edge part of the sediment will be deposited on the river bed if instead transport capacity is higher than the influx new sediment can be entrained and transported if available from the river bed or hillslope contributions generating a new cascade 3 toolbox structure fig 3 shows the file structure of the cascade toolbox the application of the cascade model is executed in three steps 1 extraction of the river network from a dem identification of the river network reaches and definition of the reach features table 1 this information is organized as a data structure named reachdata 2 evaluation of the sediment fluxes in the network with the cascade model 3 visualization and interpretation of the model outputs the functions in folder extract river network extract the river network and some morphologic parameters length slope from a dem users can customize the resulting river network by changing the inputs to the network extraction function see list in table 2 the network is stored in the reachdata struct the main input to the cascade model each row in reachdata represents a single reach in the network and columns contain reach features listed in table 1 features not extracted from the dem are inserted by the user according to the case study the reachdata struct can be exported as shapefile for visualization and further processing in any gis environment cascade uses reachdata to model sediment transport and connectivity the main folder cascade model contains the operations necessary for running cascade the function in the folder requires the reachdata struct as an input if sediment sources or dams are present they are included in the model with the damdata and extdata struct for dams and external sediment flow respectively refer to the user manual for additional information of the implementation of these data structures the cascade toolbox allows for the different customization options for the framework including different methods of sediment transport capacity calculation and hydraulic features estimation the possibility of limiting the sediment availability of the reaches and thus the flow of sediment a reach can entrain in the model and the incorporation of dams and additional sediment fluxes table 2 contains the full list of the customization options available in the released version of the cascade toolbox a user interface with dialog boxes is also included to guide in the selection of the different options of the model for first time users 4 outputs and visualization options the main cascade toolbox outputs are three dimensional n x m x c matrices representing sediment transport and deposition in the river network q b t r and q b d e p in this representation n and m indicate the number of reaches and sediment sources in the network and c the number of sediment classes fig 2 c shows a graphical representation with n 4 and c 3 each cell n m c q b t r and q b d e p stores the sediment flux and deposition both in kg s of sediment class c in reach m originating from source n fig 2 b through this structure cascade allows to track the fate of each sediment class originating from any sediment source in the river network results can be aggregated on the scale of the single reach to analyse e g how much sediment is transported in total and what the statistic properties e g mean median percentile grain sizes of transported and deposited sediment are the cascade toolbox provides the user with several functions to visualize the outputs and aid in the understanding of model results the visualization tools in folder plot function include the interactive connectivity assessment ica function that guides the user in exploring the sediment connectivity on the scale of the a river network fig 4 a and fig 4 b and single reaches fig 4 d including transported load provenance and composition of the incoming cascades the function connectivity alteration assessment caa is included in the same folder caa provides together with the options in ica a visual framework of sediment connectivity on which the user can interactively include or remove dams and external sediment contributions and visualize the changes in sediment transport both at the whole network scale and at the reach scale fig 4 c 5 conclusions and outlook the cascade toolbox was developed to facilitate access to the cascade model schmitt et al 2016 schmitt et al 2018a 2018b a computationally efficient parsimonious and flexible tool for network scale assessments and management of sediment connectivity the cascade toolbox contains customization options to adapt the model to the different requirements of the case study data availability the specific research and management objectives the toolbox is supported with fully commented code and a comprehensive user guide the toolbox is designed for easy data exchange between matlab and a gis environment for input data preparation data sharing and visualization we encourage the use of cascade to evaluate for network and catchment scale sediment management to leverage novel network scale data of fluvial geomorphology for conceptualizing and quantifying river geomorphological processes at the basin scale cascade toolbox and the user documentation is freely available from the cascade website http www cascademodel org and from the github repository linked to the website software availability name of software cascade toolbox version 1 0 tested on matlab r2018b developers marco tangi rafael schmitt simone bizzi andrea castelletti contact email marco tangi polimi it year first available 2019 available from cascade website http www cascademodel org acknowledgement we would like to thank all the current and past members of the natural resources management group www nrm deib polimi it at politecnico di milano who contributed to the development of the cascade toolbox appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 008 
26189,this study presents the first application of deep learning techniques in capturing long term time continuous forest cover dynamics at a continental scale we developed a spatially explicit ensemble model for projecting australia s forest cover change using long short term memory lstm deep learning neural networks applied to a multi dimensional high resolution spatiotemporal dataset and run on a high performance computing cluster we further quantified the influence of explanatory variables on the spatiotemporal dynamics of continental forest cover deep learning greatly outperformed a state of the art spatial econometric model at continental state and grid cell scales for example at the continental scale compared to the spatial econometric model the deep learning model improved projection performance by 44 root mean square error and 12 pseudo r squared the results illustrate the robustness and effectiveness of the lstm model this work provides a reliable tool for projecting forest cover and agricultural production under given future scenarios supporting decision making in sustainable land development management and conservation keywords long short term memory deep learning forest cover change spatiotemporal data projections deforestation 1 introduction as an indispensable natural resource forests deliver wood non wood products and significant ecosystem services including biodiversity protection climate regulation pollution control water supply soil erosion mitigation and human recreation alkama and cescatti 2016 but global forests have continued to experience astonishing losses in recent years fao 2016 hansen et al 2013 this is partly due to the lack of sustainable forest management strategies that can address forest cover loss and degradation as well as balance forest based economic social and environmental benefits changes in forest cover extent loss and gain affect the provision of forest goods and services forest cover dynamics is complex with a number of different drivers in different periods regions and socio economic conditions fao 2016 models are urgently required that can help stakeholders understand the spatiotemporal dynamics of forest cover identify critical factors that drive the dynamics macdicken et al 2015 un 2008 and better project possible future changes this will further accelerate finding pathways to forest sustainability gao and bryan 2017 design conservation policies and allocate scarce conservation resources betts et al 2017 while a number of models have been developed for forest cover dynamics change analysis these have typically been based upon discrete coarse time step data matthews et al 2007 for example kamusoko et al 2013 built a markov cellular automata model to simulate future forest cover changes in a district of laos using satellite derived forest cover maps from 1993 1996 2000 and 2004 mcroberts 2014 used a nonlinear logistic regression model to estimate the change in forest area of northeastern minnesota usa based on 2002 and 2007 landsat images kumar et al 2014 proposed a logistic regression model to capture the forest cover change from landsat satellite imagery from 1990 to 2000 and predict forest cover in 2010 these existing forest cover dynamics models do not capture the long term time continuous dynamics of forest cover and thus may ignore critical factors driving the dynamics but not represented or captured in discrete coarse time step data this is a significant limitation of land use change analyses models are constructed assuming that the state of the observed land use cover at time t was influenced by the contemporary status of a set of explanatory variables this approach ignores the fact that land use cover change may have occurred in a period at which land use data was not collected and may generate biased marginal effect estimates recent work has made progress toward addressing this shortcoming wheeler et al 2013 used a monthly database and spatial econometrics to assess forest cover change in indonesia marcos martinez et al 2018 developed an econometric model for forest cover dynamics in australia s intensive agricultural region using data for 15 annual time steps from 1988 to 2010 and validated the projection performance using a four year 2011 2014 spatiotemporal dataset multifaceted information of more land use cover projection models was compiled in a table and presented in the supplementary material of this paper meanwhile deep learning lecun et al 2015 schmidhuber 2015 a member in the family of machine learning methods has been making ground breaking advances in many fields such as speech recognition amodei et al 2016 image recognition ji et al 2013 natural language processing hirschberg and manning 2015 recommendation systems elkahky et al 2015 and forecasts in weather traffic and other aspects lv et al 2015 as a class of new neural networks that regularly outperforms traditional neural networks granter et al 2017 lipson and kurman 2016 deep neural networks have a large number of parameters and processing layers more than two processing layers in one of four fundamental network architectures unsupervised pre trained schmidhuber 1992 convolutional lecun and bengio 1995 recurrent medsker and jain 2001 and recursive neural networks medsker and jain 2001 recent work has shown the promising applications of deep neural networks in land use land cover lulc classification as the majority of lulc data are remotely sensed images convolutional neural networks cnns which were first successful in image recognition have been applied in the classification of remotely sensed images and proven highly effective in solving this problem ma et al 2019 luus et al 2015 proposed a multi view deep cnn which outperformed both scale invariant feature transform methods and unsupervised feature learning in land use classification a pretrained and supervised cnn classifier proposed by marmanis et al 2016 was proven to have good performance in tackling the limited labeled data problem in land use image classification yang et al 2018 compared the performance of different encoder decoder cnns in classifying land covers based on aerial images and an ensemble of these cnns achieved the best classification performance jiang et al 2019 applied a super resolution cnn approach into the paleovalley classification at the regional scale with a high resolution based on airborne electromagnetic images zhang et al 2019 developed a joint deep learning model that incorporated a multilayer perceptron and cnn for both land cover and land use classification from remotely sensed imagery there is a small amount of work that applied deep recurrent neural networks into land cover classification based on multitemporal spatial data ienco et al 2017 lyu et al 2016 ruwurm and krner 2018 however deep learning methods have not yet been applied in forest cover dynamics modelling among the four fundamental network architectures a type of recurrent neural network called long short term memory lstm hochreiter and schmidhuber 1997 is particularly suitable for capturing long term dependencies and has been found to be extremely successful in many applications such as traffic flow forecasting zhao et al 2017 electricity load forecasting kong et al 2017 human activity recognition ordez and roggen 2016 unconstrained handwriting recognition graves and schmidhuber 2009 speech recognition graves and jaitly 2014 handwriting generation breuel et al 2013 machine translation sutskever et al 2014 as well as image captioning xu et al 2015 and parsing sak et al 2014 the lstm approach has potential for increased predictive performance via its ability to account for the temporal dimension of land use cover change processes we assessed the potential for deep learning lstm networks to predict time series forest cover dynamics and compared its predictive ability against a state of the art spatial econometric approach based on a multi dimensional high resolution 1 1 km grid cells spatiotemporal dataset including forest cover climatic socioeconomic physiographic and state jurisdiction data for the australian intensive agricultural zone we developed a national scale lstm based forest cover projection ensemble model model development involved training a lstm for each grid cell using high performance computing clusters we assessed the drivers of spatiotemporal dynamics of australia s national forest cover by excluding each explanatory variable and assessing the impact on model predictive ability this work demonstrates the ability of deep learning methods to identify complex spatiotemporal relationships between forest cover and underlying environmental and socio economic drivers and make robust projections of future forest cover dynamics we discuss the potential for deep learning to serve as a reliable tool to project future forest cover and agricultural production given the projected hypothetical future changes in key drivers thereby supporting decision making in sustainable land development management and conservation 2 methods 2 1 study area and forest non forest data our study was carried out in australia s intensive agricultural region fig 1 the study area spans a range of climate types peel et al 2007 and accounts for 99 and 92 of the national gross value of crops and livestock respectively marcos martinez et al 2017 as the spatial resolution of most of the physiographic data e g climate and topography used in our analysis was at 0 01 i e around 1 1 km2 the study area was tessellated into grid cells of 1 1 km2 spatial resolution creating 1 376 866 individual grid cells this spatial resolution could sufficiently meet our requirements of capturing spatiotemporal differences in drivers dependent variables of forest cover changes the australian government national forest inventory 1998 defines forest cover as land with at least 20 of canopy cover and with the potential to reach over 2 m in height lehmann et al 2013 the 25 m resolution forest non forest data from the australian national carbon accounting system land cover change program ncas lccp was used to compute a 1 1 km2 grid cell resolution forest cover index the forest cover index represents the proportion of land covered by forest at each grid cell per observation year three types of data i e spatial temporal and spatiotemporal table 1 were used as exogenous inputs to the forest cover projection model spatial data included the relatively stable biophysical attributes of grid cells in the short term the accessibility and remoteness index of australia aria gisca 2001 spatial layers quantifying agricultural profit marinoni et al 2012 and distance to protected areas department of the environment 2014 were used to represent the population pressure opportunity costs and potential spillover effects of forest conservation andam et al 2008 pressey et al 2002 respectively accounting for geographical features that could affect forest and agricultural productivity we also included spatial layers quantifying slope elevation and soil characteristics temporal data included economic factors that varied over time but did not vary across the study area economic factors such as an index of agricultural commodity prices received by farmers and a timber price index abares 2015 imf 2015 were used to examine the impact of agricultural and timber market signals on the forest cropland dynamics simmons et al 2018 spatiotemporal data were a set of spatial layers that also varied year by year average forest cover index of neighboring grid cells within a 3 or 10 km radius and the distance to protected areas department of the environment 2014 were included to capture the spatial autocorrelation among forest cover index values i e the forest cover index value of a gird cell is expected to be significantly influenced by the forest cover index values of neighboring grid cells robertson et al 2009 and protected areas in the neighboring land since rainfall and temperature have direct impact on tree growth palmate et al 2014 we also used total annual rainfall average daily maximum temperature per year as well as five year moving averages and standard deviations of the total annual rainfall and the average maximum daily temperature as inputs to the model australian bureau of meteorology 2015b jones et al 2009 these five year moving averages and standard deviations were used to estimate the trend component of a non seasonal time series consisting of a trend component and an irregular component 2 2 lstm based model for projecting forest cover dynamics we built and trained a unique lstm network for each grid cell in the study area to learn and project forest cover dynamics based on the forest non forest data described above lstm was introduced by hochreiter and schmidhuber 1997 to resolve the fundamental issue of vanishing exploding gradients and the incapability of modelling long sequences in traditional recurrent neural networks the lstm introduces a new structure called a memory cell to capture long term dependencies we implemented the lstm with forget gates gers et al 1999 to screen out noise such a lstm model for a spatial grid cell g includes one input layer three lstm layers and one output layer atop the highest lstm layer see the left hand side of fig 2 the n t variable vector x t 1 t n which represents t year n variables see table 1 forms the input layer we used a 15 time step dataset spanning a 23 year 1988 2010 period as the training set and a 4 year 2011 2014 time series dataset as the test set the input layer reshapes the input variable vector x t 1 t n into a three dimension format b s n b and s are the batch size the number of training examples in one iteration epoch here b t and the width of time window here s 1 representing 1 year as a shifting step respectively a dropout operation with a dropout rate   0 5 see section 2 3 for details was applied to the output of each lstm layer to prevent the lstm model from being overfitted thus the three lstm layers have 128 64 and 32 memory cells respectively as a typical recurrent neural network a lstm network has loops allowing information to be passed from one step of the network to the next an unfolded lstm layer is shown in fig 2 a x l t n denotes the input n dimensional variables in year t at the l th lstm layer y  l t is the output vector in year t at lstm layer l c l t and h l t denote the cell state and the hidden state of a lstm memory cell in the l th lstm layer in year t respectively the lstm layer updates c l t and h l t as well as computes the output y  l t based on the input x l t n the previous memory cell state c l t 1 and the previous hidden state h l t 1 there are 128 memory cells in the first lstm layer therefore 128 memory cell states and 128 hidden states are produced in each time step t the past input information is propagated horizontally in each lstm layer through the memory cell state c l t and the hidden state h l t the internal structure of such a lstm layer is presented in fig 2 b where there is no communication among the 128 memory cells in the same layer each memory cell performs its own iterations of time steps years and each memory cell s output y  l t is propagated vertically from one layer to the next after the dropout operation a memory cell is mainly built with the following elements an input gate i l t a self recurrent connection a forget gate f l t and an output gate o l t fig 2 c w l u l and b l are the weight matrix of input hidden connections the weight matrix of hidden to hidden recurrent connections and the bias vectors in the l th lstm layer respectively the superscripts i f and o respectively stand for the input gate forget gate and output gate of memory cells respectively in a memory cell an input gate i l t is denoted as 1 i l t  w l i x l 1 t n u l i h l t 1 b l i where i l t is set into the range 0 1 via the gates activation function  we used the logistic sigmoid 1 1 e x as the gates activation function in a memory cell the value equals 0 when the data signal does not pass and equals 1 when the signal completely passes through similarly the forget gate f l t and the output gate o l t are computed as below 2 f l t  w l f x l 1 t n u l f h l t 1 b l f 3 o l t  w l o x l 1 t n u l o h l t 1 b l o the hidden state h l t and the memory cell state c l t pass the current states at time t to the next time step t 1 the hidden state h l t is also the output y  l t of the memory cell to the next layer at time t see fig 2 c the hidden state h l t and the memory cell state c l t are calculated as 4 h l t o l t tanh c l t 5 c l t f l t c l t 1 i l t c l t where is the element wise product of two vectors c l t is the internal state of the memory cell and expressed in eq 6 f l t c l t 1 indicates forgetting previous old information to the memory cell states and i l t c l t indicates adding current new information to the memory cell states 6 c l t tanh w l c x l 1 t n u l c h l t 1 b l c where hyperbolic tangent t a n h is used as the activation function of the internal state c l t the output layer is a fully connected neural layer which can be expressed by eq 7 7 y  t  w d y  l 3 t b d w d is the weight matrix for the output of the third lstm layer y  l 3 t and b d is the bias in the output layer we selected the logistic sigmoid 1 1 e x as the activation function  for the output layer to project forest cover index y  t in year t we used the mean squared error eq 8 as the loss function to control the training process 8 loss y  t 1 t y t 1 t 1 t t 1 t y  t y t 2 where y t stands for the observed values in year t 2 3 training projection models normalization dropout and input data in practical engineering applications it was found that the normalization process could greatly improve the resulting neural network model sola and sevilla 1997 we applied unity based normalization to each dimension d of the n dimensional input which generates data into the 0 1 scale following eq 9 9 z t d n o r m x t d x d x t d m i n x d m a x x d m i n x d d n where z t d is the normalized d th dimension variable in year t and x d stands for a full set of d th dimension of the input from year 1988 to year 2014 deep neural networks such as lstm networks could lead to overfitting hippert et al 2001 i e delivering very good performance in training but being very poor in testing we applied a method called dropout srivastava et al 2014 to prevent a lstm model from being overfitted the key idea of this method is to drop some units and their connections from a deep neural network during training to preserve the power of lstm networks in modelling sequences we followed the pham et al 2014 approach and applied the dropout method with a dropout rate  0 5 after each lstm layer we compared the projection performance of trained lstm networks for each grid cell based on all spatio temporal and temporal variables as spatial variables remained constant over time for each grid cell these were not included in the lstm training spatial time series variables together with temporal variables 11 variables in total were used as inputs into the lstm networks we compared the performance of the lstm network ensemble against that of marcos martinez et al s 2018 spatial econometric model trained using all spatial spatio temporal and temporal variables table 1 in projecting forest cover at the national scale 2 4 projection performance assessment we evaluated the projection performance of the proposed models at national state and grid cell scales two metrics root mean square error rmse and pseudo r squared elhorst 2014 were used to assess the projection performance of lstm and marcos martinez et al s spatial econometric model termed se 2018 at the national scale we also used rmse as a metric to compare the projection performance among different states from 2011 to 2014 at a grid cell scale we calculated the projection error as the difference between a projection and observation and mapped the spatial distribution of the average projection error during 2011 2014 for 1 376 866 grid cells for a grid cell g the average projection error e g during 2011 2014 was calculated grid cells with e g 0 3 were defined as highly overestimated and grid cells with e g 0 3 were defined as highly underestimated 2 5 identifying influential input variables identifying important variables can help understand the causes of forest gain and loss discarding redundant inputs and refining the projection model inform investment in gathering and analyzing additional data for improving model projection gao and bryan 2016 gao et al 2017 as well as producing insights for improving the mechanisms for modelling forest cover dynamics we developed a method to quantify the variable importance for a forest cover projection model the central idea of the method is to 1 remove single or groups of variables from the full set of variables n that are inputs into the forest cover projection model 2 train new lstm networks using the new variable set 3 quantify the variable importance metric and 4 rank variables in terms of quantified importance metrics in this way we assessed the importance of each single variable by removing the variable termed x n 1 from all n variables in the forest projection model considering that some spatiotemporal and temporal variables table 1 may exhibit strong correlations we also assessed the importance of groups of variables by removing the entire group of these m correlated variables termed x n m the groups of variables we assessed are presented in table 2 that represent the general influence of climate signals economic conditions and changes in forest management practices on forest cover projections for example rain5sd rain5ma and rain all represent the impact of rainfall and the rainfall information may still be incorporated into projection models when we only removed a single variable hence we also trained new lstm networks based on the remaining n 1 or n m variables and calculated the variable importance metrics in terms of both rmse and pseudo r squared eq 10 2 6 implementation we built an lstm model using tensorflow 1 0 abadi et al 2016 for each of 1 376 866 grid cells all the models were implemented in python 2 7 using message passing interface mpi for python and were executed on csiro s high performance computing cluster the cluster ran the suse linux 4 4 operating system with 114 nodes each with 14 core 256 gb of ram 4 nvidia tesla p100 and it used infiniband 100 gigabit gb for interconnection 1988 2010 and 2011 2014 data were used as training and test sets respectively we trained each lstm model for 100 epochs using the stochastic gradient descent method zinkevich et al 2010 to minimize the loss the 1 376 866 lstm models were packaged into 138 computation jobs job is a term used for a computation task that can be executed independently in the cluster submitted to the cluster and executed in parallel bryan 2013 the scheduler of the cluster allocated available resources to these computation jobs in terms of a predefined batch file the projection performance was visualized using arcgis 10 5 all scripts about data preparation modelling high performance computation analysis and visualization and relevant documents were presented on gitlab https gitlab com milesye fcp 3 results 3 1 forest cover projection performance assessment at national state and grid cell scales the lstm model outperformed the se model in terms of rmse and pseudo r squared table 3 compared to se the average rmse across 2011 2014 generated from lstm decreased from 0 125 to 0 070 decreased by 44 and the average pseudo r squared increased from 0 848 to 0 952 increased by 12 we further compared the performance of spatiotemporal consistency between the deep learning model lstm and the econometric model se the results revealed that the projections from lstm were much closer to the actual observations than those projected by se fig 3 a c the maps that visualized the average projection error during 2011 2014 showed that lstm exceeded se at the grid cell scale fig 3 d and e especially in the state of queensland lstm had fewer grid cells that were highly overestimated fig 3 d and e as well as table 4 under the lstm model approximately 81 of grid cells had average forest cover projection errors across 2011 14 that were within 5 and 90 of grid cells had average projection errors within 10 table 4 further only 1 5 of grid cells were highly underestimated or highly overestimated whose average projection errors were greater than 30 compared to lstm se performed worse 69 and 79 of grid cells had within 5 and within 10 average projection errors respectively and 4 5 of grid cells were highly underestimated or overestimated lstm outperformed se in terms of the percentages of both overestimated and underestimated grid cells in each state table 4 however except in queensland lstm had more highly overestimated but fewer highly underestimated grid cells than se in any of the rest states the projection performance of lstm was superior to that of se in each state fig 4 the rmse values in tas tasmania were prominent for lstm compared to those in other states while the rmse values in qld queensland were prominent for se for lstm and at the state scale sa south australia nsw new south wales and vic vitoria ranked among the top three in terms of projection performance while the projection performance of wa western australia qld and tas were below that of the national projection level 3 2 influential variables for capturing forest cover change variable importance for both single variables and groups of variables were consistent in two metrics rmse and pseudo r 2 table 5 excluding any variable or any group of variables the new projection performance did not significantly decrease the average rmse and pseudo r 2 varied from 0 0687 to 0 0785 and from 0 9399 to 0 9537 respectively the top five influential variables groups were neighborhood effects pricerec fidx3nn prices and maximum temperature for the variable group prices the variable component pricerec was more influential than the group for the other groups of variables neighborhood effects maximum temperature and rainfall the entire group was always more influential than any of its component variables excluding the variables rain rain5sd and timberpi improved the projection slightly 4 discussion 4 1 model projection performance the results suggest that employing deep learning to time series variables can yield much better projection results for forest cover dynamics than a state of the art spatial econometric model at national continental state and grid cell scales the results confirm that an ensemble of lstms can capture long term time continuous dynamics such as dynamic extent gain and loss of forest cover our projection model captures high resolution spatially differentiated forest cover patterns but the projection performance varies over grid cells table 4 due to the complex interactions between the lstm networks and spatially diverse explanatory variables on aggregate the deep learning method exhibits different state level projection performance fig 4 reflecting the effects of spatial variation in explanatory variables and forest dynamics on model projection for most grid cells the average projection errors across 2011 14 were acceptable and there were very few grid cells whose average projection errors were greater than 30 for example even in queensland which is a hotspot of deforestation simmons et al 2018 highly underestimated and overestimated forest cover whose average projection errors were greater than 30 were limited to 2 1 of grid cells these grid cells typically experienced a sudden and dramatic increase decrease in forest cover occurred in the grid cell characteristic of broad acre deforestation regrowth or artifacts error in the greenness indices derived from satellite imagery the increases mainly occurred in forest plantation zones while the decreases predominantly happened in native eucalypt and acacia forests marcos martinez et al 2018 this suggests that more influential factors need to be included to capture the sudden and dramatic changes in these grid cells such as land clearing effects and positive spillover effects of the deforestation ban simmons et al 2018 4 2 influential variable identification the neighborhood effects group which was represented by average forest indices of the neighboring grid cells in the previous year within 3 km and 10 km radii of a grid cell was identified as most influential to the projection performance at the national scale its components fidx3nn and fidx10nn ranked 3rd and 7th in terms of their influence on projection performance as a component in the prices group pricerec was identified as the second most influential variable this is consistent with the literature that reduction in forest cover was associated with an increase in agricultural product prices as agriculture becomes financially viable with increasing commodity price australian bureau of statistics 2012 department of agriculture fisheries and forestry 2001 excluding both economic variables via the economic group i e pricerec and timber price index timberpi only slightly decreased the projection performance the timber price index is not known as a strong driver for forest cover change in australia as most of the industrial wood in the country is produced in forest plantations where harvesting decisions do not follow short term price fluctuations marcos martinez et al 2018 both the maximum temperature and rainfall groups were generally regarded as strongly influential factors in affecting forest farmland land use particularly via their influence on agricultural productivity and economic competitiveness however the maximum temperature group ranked 5th was more influential than the rainfall group ranked the 8th in importance temperature can influence the growth rate of crops and trees bowman et al 2014 and is one of main factors that affects the frequency of wildfires in turn further affecting forest cover aryal and louvet 2016 bradstock and auld 1995 4 3 knowledge gained and lessons learned in deep learning modelling statistical and machine learning methods are often proposed as two major solutions to time series projection problems a recent study makridakis et al 2018 demonstrated seven traditional statistical methods outperformed 18 machine learning methods including rnn and lstm in both post sample forecasting accuracy and computational requirements however the dataset they used was a univariate time series in the m3 competition makridakis and hibon 2000 and the rnn and the lstm methods they built only comprised of a single hidden layer and a single output layer contrary to this our deep learning model was built for multivariate time series projections and the model was composed of an ensemble of approximately 1 38 million lstm networks each of which has three lstm layers and a fully connected output layer this work demonstrated that the projection accuracy of the deep learning method was higher than that of the statistical model the spatial econometric model at continental state and grid cell scales however as makridakis et al 2018 pointed out the computational requirements of machine learning methods were considerably greater than those of statistical methods this holds true in our case training and diagnosing each grid cell model is computationally intensive needs massive amounts of floating point operations and high performance parallel i o capabilities and this requirement naturally fits in the space of high performance computing we employed our projection model onto a csiro s linux cluster with 114 high performance parallel computation nodes and finished the execution within an acceptable time approximately 48 h for lstm users without access to such high performance computing resources may be challenged by the scale of such computation task compared with statistical or mechanistic models machine learning models especially neural networks are commonly regarded as black boxes cortez and embrechts 2013 however how the projections are generated is much less understood the structures of deep neural networks such as lstm networks are even more complicated therefore deriving variable importance by interpreting deep neural networks is extremely difficult de oa and garrido 2014 in recent years several methods have been proposed to identify the relative importance of explanatory variables de oa and garrido 2014 olden et al 2004 nevertheless these methods are difficult to apply to lstm networks mainly because the structure of lstm networks is too complicated to track a variable from beginning to end we therefore adopted an approach that enabled the successful comparison of projection performance between the full variable set trained model and the model trained by screening out single explanatory variables or variable groups to quantify and rank the influence of the explanatory variable s as a class of representation learning methods deep learning can automatically discover representations features needed for detection from raw data lecun et al 2015 our work has also demonstrated this characteristic fed with input variables the lstm network can handle noise and continuous representations however our work also demonstrated that the capability of lstm in automatically discovering representations is still restricted for example the trained lstm networks by including the variable timberpi obtained worse projection performance than those trained by excluding the variable an extra effort is required to sort out input variables to improve projection performance 4 4 implications for sustainable land use and land management a major impediment to sustainable land use and land management is the lack of tools and information to help planners and managers understand land cover dynamics models such as the one developed in this work can be used to project future forest cover and agricultural production given the projected future changes in key drivers such as climate economic conditions management practices and land management regulations fao 2016 land use planners and managers can use the projected land cover maps to identify the hot spot areas that need intervention implement land and forest management design conservation policies and progress towards sustainability thus forest managers can avoid using the one size fits all rule and design site specific management strategies to suit different conditions of hot spot and other areas more management resources need to be focused on the hot spot areas and the effects of the identified influential factors such as agricultural output prices and the scope of conservation areas need to be considered in the design of management and policy 4 5 advantages limitations and further research we built a bottom up ensemble and continental scale projection model for forest cover dynamics assembled with approximately 1 38 million grid cell lstm networks our projection model provides more detail than most projection models in accounting for spatial forest cover dynamics the spatial grid cell resolution 1 1 km used to build a lstm network for projecting forest cover dynamics not only covered a large geographical extent but also captured important heterogeneity in both biophysical and socio economic drivers over time bryan et al 2016 connor et al 2015 this addresses the significant limitation of previous land use change analyses based on non temporal data or discrete coarse time step data our high temporal resolution modelling approach captures the long term time continuous dynamics of forest cover and includes critical factors driving the dynamics another major advantage provided by this modelling approach is the function of plug and play different machine learning methods can be applied into different grid cells this allows improving the national scale projection model by selecting the best projection model for each grid cell the influential variable identification experiments clearly demonstrate the robustness and effectiveness of the forest projection model when influential variables or group of variables were removed from the training dataset the ensemble model could still achieve much better projection performance the average rmse values ranged from 0 0687 to 0 0785 and the average pseudo r 2 varied from 0 9399 to 0 9537 respectively than the existing spatial econometric model the average rmse and pseudo r 2 were 0 1248 and 0 8480 respectively the lstm model somehow re weights the remaining variables to compensate for the loss of information from the omitted variables while the se model could not do that there are four main limitations and areas for further improvement in lstm based forest cover projection modelling 1 in our model formulation it did not make sense to incorporate spatial variables that did not change over time i e spatial variables in table 1 into the training process of lstm networks as they simply appeared as a constant value for each grid cell and did not enhance predictive performance some research efforts have proven that the spatial non temporal variables such as biophysical variables have value for use in forest cover projections hansen et al 2013 kumar et al 2014 approaches that include spatial variables in the training of deep learning networks need to be developed to improve projection performance training a single deep learning network that incorporates all grid cells in a single network will enable this but may also be prohibitive in terms of computational load as parallelization will be challenging 2 the deep learning models performed well in most grid cells where forest cover change occurred gradually or remained relatively stable over time but did not perform well in the grid cells where accidental significant increases or decreases happened suddenly the future work will include relative explanatory variables such as changes in land clearing regulations simmons et al 2018 that may explain the increases decreases to capture the forest cover dynamics in these grid cells in addition more spatial temporal variables such as forest patch sizes and shapes that are closely associated with the acceleration of forest area decrease need to be considered for improving the overall projection performance in all grid cells 3 since the modelling work focused on the forest cover change in australia s intensive agricultural land with datasets used for projecting anthropogenic forest clearing i e clearing of remnant forests for agricultural use the trained model is likely to underperform for projecting forest cover or vegetation changes in some specific areas such as coastal areas in agricultural regions where some practice is not common in australia such as slash and burn agriculture or in places that were removed in this analysis such as fire affected forest areas to improve the projection performance of our model in these specific regions relevant explanatory variables possible drivers of the changes are needed for training the model for instance retraining the model to account for the impacts of climate e g hurricanes upstream pollution affecting coastal ecosystems and coastal land uses e g shrimp farming and tourism developments could help improve the projection performance in coastal areas stock et al 2018 4 this work did not quantify the uncertainty in projection results although uncertainty quantification has been widely included in classical prediction projection models e g dong et al 2015 gao et al 2016 peeters et al 2018 the popularity does not apply in machine learning models especially deep neural networks gal 2016 this is because traditional machine learning approaches to quantifying uncertainty are unable to scale to high dimensional inputs kendall and gal 2017 deep learning models that are usually computationally expensive cannot afford computational demands required by a uncertainty estimation method gal 2016 and the complex structure of a deep learning model as well as the large number of parameters make uncertainty quantification difficult lakshminarayanan et al 2017 however quantifying the uncertainty of a deep learning model in future projections is equally important as the accuracy of the model typical uncertainty sources include measurement noise of training data model parameter assumptions as well as model structure ghahramani 2015 knowing the uncertainty in the produced projections can help practitioners interpret the model and determine whether to apply the model into a high risk area such as self driving cars bayesian deep learning methods have been recently proven promising in estimating uncertainty in deep neural networks gal 2016 gal and ghahramani 2016 zhu and laptev 2017 these methods can offer a probabilistic interpretation to deep learning models via inferring distributions over the networks weights but it is still hard to apply these uncertainty estimation methods into our model although these methods avoid modelling distributions over models and their parameters which is difficult achieve at scale the computational cost is prohibitive in our case our current projection model required approximately 48 h execution time on a linux cluster with 114 high performance parallel computation nodes as a number of monte carlo runs is still required e g a monte carlo dropout sampling is required in these methods to apply a bernoulli distribution over the network s weights gal and ghahramani 2016 future work will design a single deep learning for projecting forest covers in all grid cells within the bayesian deep learning framework enabling the uncertainty quantification of the projection model 5 conclusion we successfully employed sophisticated deep learning methods to the challenging problem of projecting long term time continuous forest cover dynamics we developed an ensemble spatially explicit and continental scale projection model involving approximately 1 38 million grid cell sub models each grid cell model was built as a lstm network that included three lstm layers and a fully connected output layer and was built on a high performance computational cluster and a spatiotemporal dataset of relevant biophysical socioeconomic and institutional explanatory variables the deep learning model greatly outperformed a spatial econometric model with 44 improvement in the rmse indicator and 12 improvement in pseudo r squared the results also clearly demonstrate the robustness and effectiveness of the projection model in a broad range of parameter values and lack of influential training variables the relative importance of explanatory variables associated with the observed forest cover dynamics was also presented acknowledgements this work was supported by the csiro land and water l ye was supported by the shanghai 2016 teacher professional development project shec2016no 25 and l gao by a csiro julius career award the authors would also like to thank two anonymous reviewers for their constructive comments which have been very helpful for improving this manuscript appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 013 
26189,this study presents the first application of deep learning techniques in capturing long term time continuous forest cover dynamics at a continental scale we developed a spatially explicit ensemble model for projecting australia s forest cover change using long short term memory lstm deep learning neural networks applied to a multi dimensional high resolution spatiotemporal dataset and run on a high performance computing cluster we further quantified the influence of explanatory variables on the spatiotemporal dynamics of continental forest cover deep learning greatly outperformed a state of the art spatial econometric model at continental state and grid cell scales for example at the continental scale compared to the spatial econometric model the deep learning model improved projection performance by 44 root mean square error and 12 pseudo r squared the results illustrate the robustness and effectiveness of the lstm model this work provides a reliable tool for projecting forest cover and agricultural production under given future scenarios supporting decision making in sustainable land development management and conservation keywords long short term memory deep learning forest cover change spatiotemporal data projections deforestation 1 introduction as an indispensable natural resource forests deliver wood non wood products and significant ecosystem services including biodiversity protection climate regulation pollution control water supply soil erosion mitigation and human recreation alkama and cescatti 2016 but global forests have continued to experience astonishing losses in recent years fao 2016 hansen et al 2013 this is partly due to the lack of sustainable forest management strategies that can address forest cover loss and degradation as well as balance forest based economic social and environmental benefits changes in forest cover extent loss and gain affect the provision of forest goods and services forest cover dynamics is complex with a number of different drivers in different periods regions and socio economic conditions fao 2016 models are urgently required that can help stakeholders understand the spatiotemporal dynamics of forest cover identify critical factors that drive the dynamics macdicken et al 2015 un 2008 and better project possible future changes this will further accelerate finding pathways to forest sustainability gao and bryan 2017 design conservation policies and allocate scarce conservation resources betts et al 2017 while a number of models have been developed for forest cover dynamics change analysis these have typically been based upon discrete coarse time step data matthews et al 2007 for example kamusoko et al 2013 built a markov cellular automata model to simulate future forest cover changes in a district of laos using satellite derived forest cover maps from 1993 1996 2000 and 2004 mcroberts 2014 used a nonlinear logistic regression model to estimate the change in forest area of northeastern minnesota usa based on 2002 and 2007 landsat images kumar et al 2014 proposed a logistic regression model to capture the forest cover change from landsat satellite imagery from 1990 to 2000 and predict forest cover in 2010 these existing forest cover dynamics models do not capture the long term time continuous dynamics of forest cover and thus may ignore critical factors driving the dynamics but not represented or captured in discrete coarse time step data this is a significant limitation of land use change analyses models are constructed assuming that the state of the observed land use cover at time t was influenced by the contemporary status of a set of explanatory variables this approach ignores the fact that land use cover change may have occurred in a period at which land use data was not collected and may generate biased marginal effect estimates recent work has made progress toward addressing this shortcoming wheeler et al 2013 used a monthly database and spatial econometrics to assess forest cover change in indonesia marcos martinez et al 2018 developed an econometric model for forest cover dynamics in australia s intensive agricultural region using data for 15 annual time steps from 1988 to 2010 and validated the projection performance using a four year 2011 2014 spatiotemporal dataset multifaceted information of more land use cover projection models was compiled in a table and presented in the supplementary material of this paper meanwhile deep learning lecun et al 2015 schmidhuber 2015 a member in the family of machine learning methods has been making ground breaking advances in many fields such as speech recognition amodei et al 2016 image recognition ji et al 2013 natural language processing hirschberg and manning 2015 recommendation systems elkahky et al 2015 and forecasts in weather traffic and other aspects lv et al 2015 as a class of new neural networks that regularly outperforms traditional neural networks granter et al 2017 lipson and kurman 2016 deep neural networks have a large number of parameters and processing layers more than two processing layers in one of four fundamental network architectures unsupervised pre trained schmidhuber 1992 convolutional lecun and bengio 1995 recurrent medsker and jain 2001 and recursive neural networks medsker and jain 2001 recent work has shown the promising applications of deep neural networks in land use land cover lulc classification as the majority of lulc data are remotely sensed images convolutional neural networks cnns which were first successful in image recognition have been applied in the classification of remotely sensed images and proven highly effective in solving this problem ma et al 2019 luus et al 2015 proposed a multi view deep cnn which outperformed both scale invariant feature transform methods and unsupervised feature learning in land use classification a pretrained and supervised cnn classifier proposed by marmanis et al 2016 was proven to have good performance in tackling the limited labeled data problem in land use image classification yang et al 2018 compared the performance of different encoder decoder cnns in classifying land covers based on aerial images and an ensemble of these cnns achieved the best classification performance jiang et al 2019 applied a super resolution cnn approach into the paleovalley classification at the regional scale with a high resolution based on airborne electromagnetic images zhang et al 2019 developed a joint deep learning model that incorporated a multilayer perceptron and cnn for both land cover and land use classification from remotely sensed imagery there is a small amount of work that applied deep recurrent neural networks into land cover classification based on multitemporal spatial data ienco et al 2017 lyu et al 2016 ruwurm and krner 2018 however deep learning methods have not yet been applied in forest cover dynamics modelling among the four fundamental network architectures a type of recurrent neural network called long short term memory lstm hochreiter and schmidhuber 1997 is particularly suitable for capturing long term dependencies and has been found to be extremely successful in many applications such as traffic flow forecasting zhao et al 2017 electricity load forecasting kong et al 2017 human activity recognition ordez and roggen 2016 unconstrained handwriting recognition graves and schmidhuber 2009 speech recognition graves and jaitly 2014 handwriting generation breuel et al 2013 machine translation sutskever et al 2014 as well as image captioning xu et al 2015 and parsing sak et al 2014 the lstm approach has potential for increased predictive performance via its ability to account for the temporal dimension of land use cover change processes we assessed the potential for deep learning lstm networks to predict time series forest cover dynamics and compared its predictive ability against a state of the art spatial econometric approach based on a multi dimensional high resolution 1 1 km grid cells spatiotemporal dataset including forest cover climatic socioeconomic physiographic and state jurisdiction data for the australian intensive agricultural zone we developed a national scale lstm based forest cover projection ensemble model model development involved training a lstm for each grid cell using high performance computing clusters we assessed the drivers of spatiotemporal dynamics of australia s national forest cover by excluding each explanatory variable and assessing the impact on model predictive ability this work demonstrates the ability of deep learning methods to identify complex spatiotemporal relationships between forest cover and underlying environmental and socio economic drivers and make robust projections of future forest cover dynamics we discuss the potential for deep learning to serve as a reliable tool to project future forest cover and agricultural production given the projected hypothetical future changes in key drivers thereby supporting decision making in sustainable land development management and conservation 2 methods 2 1 study area and forest non forest data our study was carried out in australia s intensive agricultural region fig 1 the study area spans a range of climate types peel et al 2007 and accounts for 99 and 92 of the national gross value of crops and livestock respectively marcos martinez et al 2017 as the spatial resolution of most of the physiographic data e g climate and topography used in our analysis was at 0 01 i e around 1 1 km2 the study area was tessellated into grid cells of 1 1 km2 spatial resolution creating 1 376 866 individual grid cells this spatial resolution could sufficiently meet our requirements of capturing spatiotemporal differences in drivers dependent variables of forest cover changes the australian government national forest inventory 1998 defines forest cover as land with at least 20 of canopy cover and with the potential to reach over 2 m in height lehmann et al 2013 the 25 m resolution forest non forest data from the australian national carbon accounting system land cover change program ncas lccp was used to compute a 1 1 km2 grid cell resolution forest cover index the forest cover index represents the proportion of land covered by forest at each grid cell per observation year three types of data i e spatial temporal and spatiotemporal table 1 were used as exogenous inputs to the forest cover projection model spatial data included the relatively stable biophysical attributes of grid cells in the short term the accessibility and remoteness index of australia aria gisca 2001 spatial layers quantifying agricultural profit marinoni et al 2012 and distance to protected areas department of the environment 2014 were used to represent the population pressure opportunity costs and potential spillover effects of forest conservation andam et al 2008 pressey et al 2002 respectively accounting for geographical features that could affect forest and agricultural productivity we also included spatial layers quantifying slope elevation and soil characteristics temporal data included economic factors that varied over time but did not vary across the study area economic factors such as an index of agricultural commodity prices received by farmers and a timber price index abares 2015 imf 2015 were used to examine the impact of agricultural and timber market signals on the forest cropland dynamics simmons et al 2018 spatiotemporal data were a set of spatial layers that also varied year by year average forest cover index of neighboring grid cells within a 3 or 10 km radius and the distance to protected areas department of the environment 2014 were included to capture the spatial autocorrelation among forest cover index values i e the forest cover index value of a gird cell is expected to be significantly influenced by the forest cover index values of neighboring grid cells robertson et al 2009 and protected areas in the neighboring land since rainfall and temperature have direct impact on tree growth palmate et al 2014 we also used total annual rainfall average daily maximum temperature per year as well as five year moving averages and standard deviations of the total annual rainfall and the average maximum daily temperature as inputs to the model australian bureau of meteorology 2015b jones et al 2009 these five year moving averages and standard deviations were used to estimate the trend component of a non seasonal time series consisting of a trend component and an irregular component 2 2 lstm based model for projecting forest cover dynamics we built and trained a unique lstm network for each grid cell in the study area to learn and project forest cover dynamics based on the forest non forest data described above lstm was introduced by hochreiter and schmidhuber 1997 to resolve the fundamental issue of vanishing exploding gradients and the incapability of modelling long sequences in traditional recurrent neural networks the lstm introduces a new structure called a memory cell to capture long term dependencies we implemented the lstm with forget gates gers et al 1999 to screen out noise such a lstm model for a spatial grid cell g includes one input layer three lstm layers and one output layer atop the highest lstm layer see the left hand side of fig 2 the n t variable vector x t 1 t n which represents t year n variables see table 1 forms the input layer we used a 15 time step dataset spanning a 23 year 1988 2010 period as the training set and a 4 year 2011 2014 time series dataset as the test set the input layer reshapes the input variable vector x t 1 t n into a three dimension format b s n b and s are the batch size the number of training examples in one iteration epoch here b t and the width of time window here s 1 representing 1 year as a shifting step respectively a dropout operation with a dropout rate   0 5 see section 2 3 for details was applied to the output of each lstm layer to prevent the lstm model from being overfitted thus the three lstm layers have 128 64 and 32 memory cells respectively as a typical recurrent neural network a lstm network has loops allowing information to be passed from one step of the network to the next an unfolded lstm layer is shown in fig 2 a x l t n denotes the input n dimensional variables in year t at the l th lstm layer y  l t is the output vector in year t at lstm layer l c l t and h l t denote the cell state and the hidden state of a lstm memory cell in the l th lstm layer in year t respectively the lstm layer updates c l t and h l t as well as computes the output y  l t based on the input x l t n the previous memory cell state c l t 1 and the previous hidden state h l t 1 there are 128 memory cells in the first lstm layer therefore 128 memory cell states and 128 hidden states are produced in each time step t the past input information is propagated horizontally in each lstm layer through the memory cell state c l t and the hidden state h l t the internal structure of such a lstm layer is presented in fig 2 b where there is no communication among the 128 memory cells in the same layer each memory cell performs its own iterations of time steps years and each memory cell s output y  l t is propagated vertically from one layer to the next after the dropout operation a memory cell is mainly built with the following elements an input gate i l t a self recurrent connection a forget gate f l t and an output gate o l t fig 2 c w l u l and b l are the weight matrix of input hidden connections the weight matrix of hidden to hidden recurrent connections and the bias vectors in the l th lstm layer respectively the superscripts i f and o respectively stand for the input gate forget gate and output gate of memory cells respectively in a memory cell an input gate i l t is denoted as 1 i l t  w l i x l 1 t n u l i h l t 1 b l i where i l t is set into the range 0 1 via the gates activation function  we used the logistic sigmoid 1 1 e x as the gates activation function in a memory cell the value equals 0 when the data signal does not pass and equals 1 when the signal completely passes through similarly the forget gate f l t and the output gate o l t are computed as below 2 f l t  w l f x l 1 t n u l f h l t 1 b l f 3 o l t  w l o x l 1 t n u l o h l t 1 b l o the hidden state h l t and the memory cell state c l t pass the current states at time t to the next time step t 1 the hidden state h l t is also the output y  l t of the memory cell to the next layer at time t see fig 2 c the hidden state h l t and the memory cell state c l t are calculated as 4 h l t o l t tanh c l t 5 c l t f l t c l t 1 i l t c l t where is the element wise product of two vectors c l t is the internal state of the memory cell and expressed in eq 6 f l t c l t 1 indicates forgetting previous old information to the memory cell states and i l t c l t indicates adding current new information to the memory cell states 6 c l t tanh w l c x l 1 t n u l c h l t 1 b l c where hyperbolic tangent t a n h is used as the activation function of the internal state c l t the output layer is a fully connected neural layer which can be expressed by eq 7 7 y  t  w d y  l 3 t b d w d is the weight matrix for the output of the third lstm layer y  l 3 t and b d is the bias in the output layer we selected the logistic sigmoid 1 1 e x as the activation function  for the output layer to project forest cover index y  t in year t we used the mean squared error eq 8 as the loss function to control the training process 8 loss y  t 1 t y t 1 t 1 t t 1 t y  t y t 2 where y t stands for the observed values in year t 2 3 training projection models normalization dropout and input data in practical engineering applications it was found that the normalization process could greatly improve the resulting neural network model sola and sevilla 1997 we applied unity based normalization to each dimension d of the n dimensional input which generates data into the 0 1 scale following eq 9 9 z t d n o r m x t d x d x t d m i n x d m a x x d m i n x d d n where z t d is the normalized d th dimension variable in year t and x d stands for a full set of d th dimension of the input from year 1988 to year 2014 deep neural networks such as lstm networks could lead to overfitting hippert et al 2001 i e delivering very good performance in training but being very poor in testing we applied a method called dropout srivastava et al 2014 to prevent a lstm model from being overfitted the key idea of this method is to drop some units and their connections from a deep neural network during training to preserve the power of lstm networks in modelling sequences we followed the pham et al 2014 approach and applied the dropout method with a dropout rate  0 5 after each lstm layer we compared the projection performance of trained lstm networks for each grid cell based on all spatio temporal and temporal variables as spatial variables remained constant over time for each grid cell these were not included in the lstm training spatial time series variables together with temporal variables 11 variables in total were used as inputs into the lstm networks we compared the performance of the lstm network ensemble against that of marcos martinez et al s 2018 spatial econometric model trained using all spatial spatio temporal and temporal variables table 1 in projecting forest cover at the national scale 2 4 projection performance assessment we evaluated the projection performance of the proposed models at national state and grid cell scales two metrics root mean square error rmse and pseudo r squared elhorst 2014 were used to assess the projection performance of lstm and marcos martinez et al s spatial econometric model termed se 2018 at the national scale we also used rmse as a metric to compare the projection performance among different states from 2011 to 2014 at a grid cell scale we calculated the projection error as the difference between a projection and observation and mapped the spatial distribution of the average projection error during 2011 2014 for 1 376 866 grid cells for a grid cell g the average projection error e g during 2011 2014 was calculated grid cells with e g 0 3 were defined as highly overestimated and grid cells with e g 0 3 were defined as highly underestimated 2 5 identifying influential input variables identifying important variables can help understand the causes of forest gain and loss discarding redundant inputs and refining the projection model inform investment in gathering and analyzing additional data for improving model projection gao and bryan 2016 gao et al 2017 as well as producing insights for improving the mechanisms for modelling forest cover dynamics we developed a method to quantify the variable importance for a forest cover projection model the central idea of the method is to 1 remove single or groups of variables from the full set of variables n that are inputs into the forest cover projection model 2 train new lstm networks using the new variable set 3 quantify the variable importance metric and 4 rank variables in terms of quantified importance metrics in this way we assessed the importance of each single variable by removing the variable termed x n 1 from all n variables in the forest projection model considering that some spatiotemporal and temporal variables table 1 may exhibit strong correlations we also assessed the importance of groups of variables by removing the entire group of these m correlated variables termed x n m the groups of variables we assessed are presented in table 2 that represent the general influence of climate signals economic conditions and changes in forest management practices on forest cover projections for example rain5sd rain5ma and rain all represent the impact of rainfall and the rainfall information may still be incorporated into projection models when we only removed a single variable hence we also trained new lstm networks based on the remaining n 1 or n m variables and calculated the variable importance metrics in terms of both rmse and pseudo r squared eq 10 2 6 implementation we built an lstm model using tensorflow 1 0 abadi et al 2016 for each of 1 376 866 grid cells all the models were implemented in python 2 7 using message passing interface mpi for python and were executed on csiro s high performance computing cluster the cluster ran the suse linux 4 4 operating system with 114 nodes each with 14 core 256 gb of ram 4 nvidia tesla p100 and it used infiniband 100 gigabit gb for interconnection 1988 2010 and 2011 2014 data were used as training and test sets respectively we trained each lstm model for 100 epochs using the stochastic gradient descent method zinkevich et al 2010 to minimize the loss the 1 376 866 lstm models were packaged into 138 computation jobs job is a term used for a computation task that can be executed independently in the cluster submitted to the cluster and executed in parallel bryan 2013 the scheduler of the cluster allocated available resources to these computation jobs in terms of a predefined batch file the projection performance was visualized using arcgis 10 5 all scripts about data preparation modelling high performance computation analysis and visualization and relevant documents were presented on gitlab https gitlab com milesye fcp 3 results 3 1 forest cover projection performance assessment at national state and grid cell scales the lstm model outperformed the se model in terms of rmse and pseudo r squared table 3 compared to se the average rmse across 2011 2014 generated from lstm decreased from 0 125 to 0 070 decreased by 44 and the average pseudo r squared increased from 0 848 to 0 952 increased by 12 we further compared the performance of spatiotemporal consistency between the deep learning model lstm and the econometric model se the results revealed that the projections from lstm were much closer to the actual observations than those projected by se fig 3 a c the maps that visualized the average projection error during 2011 2014 showed that lstm exceeded se at the grid cell scale fig 3 d and e especially in the state of queensland lstm had fewer grid cells that were highly overestimated fig 3 d and e as well as table 4 under the lstm model approximately 81 of grid cells had average forest cover projection errors across 2011 14 that were within 5 and 90 of grid cells had average projection errors within 10 table 4 further only 1 5 of grid cells were highly underestimated or highly overestimated whose average projection errors were greater than 30 compared to lstm se performed worse 69 and 79 of grid cells had within 5 and within 10 average projection errors respectively and 4 5 of grid cells were highly underestimated or overestimated lstm outperformed se in terms of the percentages of both overestimated and underestimated grid cells in each state table 4 however except in queensland lstm had more highly overestimated but fewer highly underestimated grid cells than se in any of the rest states the projection performance of lstm was superior to that of se in each state fig 4 the rmse values in tas tasmania were prominent for lstm compared to those in other states while the rmse values in qld queensland were prominent for se for lstm and at the state scale sa south australia nsw new south wales and vic vitoria ranked among the top three in terms of projection performance while the projection performance of wa western australia qld and tas were below that of the national projection level 3 2 influential variables for capturing forest cover change variable importance for both single variables and groups of variables were consistent in two metrics rmse and pseudo r 2 table 5 excluding any variable or any group of variables the new projection performance did not significantly decrease the average rmse and pseudo r 2 varied from 0 0687 to 0 0785 and from 0 9399 to 0 9537 respectively the top five influential variables groups were neighborhood effects pricerec fidx3nn prices and maximum temperature for the variable group prices the variable component pricerec was more influential than the group for the other groups of variables neighborhood effects maximum temperature and rainfall the entire group was always more influential than any of its component variables excluding the variables rain rain5sd and timberpi improved the projection slightly 4 discussion 4 1 model projection performance the results suggest that employing deep learning to time series variables can yield much better projection results for forest cover dynamics than a state of the art spatial econometric model at national continental state and grid cell scales the results confirm that an ensemble of lstms can capture long term time continuous dynamics such as dynamic extent gain and loss of forest cover our projection model captures high resolution spatially differentiated forest cover patterns but the projection performance varies over grid cells table 4 due to the complex interactions between the lstm networks and spatially diverse explanatory variables on aggregate the deep learning method exhibits different state level projection performance fig 4 reflecting the effects of spatial variation in explanatory variables and forest dynamics on model projection for most grid cells the average projection errors across 2011 14 were acceptable and there were very few grid cells whose average projection errors were greater than 30 for example even in queensland which is a hotspot of deforestation simmons et al 2018 highly underestimated and overestimated forest cover whose average projection errors were greater than 30 were limited to 2 1 of grid cells these grid cells typically experienced a sudden and dramatic increase decrease in forest cover occurred in the grid cell characteristic of broad acre deforestation regrowth or artifacts error in the greenness indices derived from satellite imagery the increases mainly occurred in forest plantation zones while the decreases predominantly happened in native eucalypt and acacia forests marcos martinez et al 2018 this suggests that more influential factors need to be included to capture the sudden and dramatic changes in these grid cells such as land clearing effects and positive spillover effects of the deforestation ban simmons et al 2018 4 2 influential variable identification the neighborhood effects group which was represented by average forest indices of the neighboring grid cells in the previous year within 3 km and 10 km radii of a grid cell was identified as most influential to the projection performance at the national scale its components fidx3nn and fidx10nn ranked 3rd and 7th in terms of their influence on projection performance as a component in the prices group pricerec was identified as the second most influential variable this is consistent with the literature that reduction in forest cover was associated with an increase in agricultural product prices as agriculture becomes financially viable with increasing commodity price australian bureau of statistics 2012 department of agriculture fisheries and forestry 2001 excluding both economic variables via the economic group i e pricerec and timber price index timberpi only slightly decreased the projection performance the timber price index is not known as a strong driver for forest cover change in australia as most of the industrial wood in the country is produced in forest plantations where harvesting decisions do not follow short term price fluctuations marcos martinez et al 2018 both the maximum temperature and rainfall groups were generally regarded as strongly influential factors in affecting forest farmland land use particularly via their influence on agricultural productivity and economic competitiveness however the maximum temperature group ranked 5th was more influential than the rainfall group ranked the 8th in importance temperature can influence the growth rate of crops and trees bowman et al 2014 and is one of main factors that affects the frequency of wildfires in turn further affecting forest cover aryal and louvet 2016 bradstock and auld 1995 4 3 knowledge gained and lessons learned in deep learning modelling statistical and machine learning methods are often proposed as two major solutions to time series projection problems a recent study makridakis et al 2018 demonstrated seven traditional statistical methods outperformed 18 machine learning methods including rnn and lstm in both post sample forecasting accuracy and computational requirements however the dataset they used was a univariate time series in the m3 competition makridakis and hibon 2000 and the rnn and the lstm methods they built only comprised of a single hidden layer and a single output layer contrary to this our deep learning model was built for multivariate time series projections and the model was composed of an ensemble of approximately 1 38 million lstm networks each of which has three lstm layers and a fully connected output layer this work demonstrated that the projection accuracy of the deep learning method was higher than that of the statistical model the spatial econometric model at continental state and grid cell scales however as makridakis et al 2018 pointed out the computational requirements of machine learning methods were considerably greater than those of statistical methods this holds true in our case training and diagnosing each grid cell model is computationally intensive needs massive amounts of floating point operations and high performance parallel i o capabilities and this requirement naturally fits in the space of high performance computing we employed our projection model onto a csiro s linux cluster with 114 high performance parallel computation nodes and finished the execution within an acceptable time approximately 48 h for lstm users without access to such high performance computing resources may be challenged by the scale of such computation task compared with statistical or mechanistic models machine learning models especially neural networks are commonly regarded as black boxes cortez and embrechts 2013 however how the projections are generated is much less understood the structures of deep neural networks such as lstm networks are even more complicated therefore deriving variable importance by interpreting deep neural networks is extremely difficult de oa and garrido 2014 in recent years several methods have been proposed to identify the relative importance of explanatory variables de oa and garrido 2014 olden et al 2004 nevertheless these methods are difficult to apply to lstm networks mainly because the structure of lstm networks is too complicated to track a variable from beginning to end we therefore adopted an approach that enabled the successful comparison of projection performance between the full variable set trained model and the model trained by screening out single explanatory variables or variable groups to quantify and rank the influence of the explanatory variable s as a class of representation learning methods deep learning can automatically discover representations features needed for detection from raw data lecun et al 2015 our work has also demonstrated this characteristic fed with input variables the lstm network can handle noise and continuous representations however our work also demonstrated that the capability of lstm in automatically discovering representations is still restricted for example the trained lstm networks by including the variable timberpi obtained worse projection performance than those trained by excluding the variable an extra effort is required to sort out input variables to improve projection performance 4 4 implications for sustainable land use and land management a major impediment to sustainable land use and land management is the lack of tools and information to help planners and managers understand land cover dynamics models such as the one developed in this work can be used to project future forest cover and agricultural production given the projected future changes in key drivers such as climate economic conditions management practices and land management regulations fao 2016 land use planners and managers can use the projected land cover maps to identify the hot spot areas that need intervention implement land and forest management design conservation policies and progress towards sustainability thus forest managers can avoid using the one size fits all rule and design site specific management strategies to suit different conditions of hot spot and other areas more management resources need to be focused on the hot spot areas and the effects of the identified influential factors such as agricultural output prices and the scope of conservation areas need to be considered in the design of management and policy 4 5 advantages limitations and further research we built a bottom up ensemble and continental scale projection model for forest cover dynamics assembled with approximately 1 38 million grid cell lstm networks our projection model provides more detail than most projection models in accounting for spatial forest cover dynamics the spatial grid cell resolution 1 1 km used to build a lstm network for projecting forest cover dynamics not only covered a large geographical extent but also captured important heterogeneity in both biophysical and socio economic drivers over time bryan et al 2016 connor et al 2015 this addresses the significant limitation of previous land use change analyses based on non temporal data or discrete coarse time step data our high temporal resolution modelling approach captures the long term time continuous dynamics of forest cover and includes critical factors driving the dynamics another major advantage provided by this modelling approach is the function of plug and play different machine learning methods can be applied into different grid cells this allows improving the national scale projection model by selecting the best projection model for each grid cell the influential variable identification experiments clearly demonstrate the robustness and effectiveness of the forest projection model when influential variables or group of variables were removed from the training dataset the ensemble model could still achieve much better projection performance the average rmse values ranged from 0 0687 to 0 0785 and the average pseudo r 2 varied from 0 9399 to 0 9537 respectively than the existing spatial econometric model the average rmse and pseudo r 2 were 0 1248 and 0 8480 respectively the lstm model somehow re weights the remaining variables to compensate for the loss of information from the omitted variables while the se model could not do that there are four main limitations and areas for further improvement in lstm based forest cover projection modelling 1 in our model formulation it did not make sense to incorporate spatial variables that did not change over time i e spatial variables in table 1 into the training process of lstm networks as they simply appeared as a constant value for each grid cell and did not enhance predictive performance some research efforts have proven that the spatial non temporal variables such as biophysical variables have value for use in forest cover projections hansen et al 2013 kumar et al 2014 approaches that include spatial variables in the training of deep learning networks need to be developed to improve projection performance training a single deep learning network that incorporates all grid cells in a single network will enable this but may also be prohibitive in terms of computational load as parallelization will be challenging 2 the deep learning models performed well in most grid cells where forest cover change occurred gradually or remained relatively stable over time but did not perform well in the grid cells where accidental significant increases or decreases happened suddenly the future work will include relative explanatory variables such as changes in land clearing regulations simmons et al 2018 that may explain the increases decreases to capture the forest cover dynamics in these grid cells in addition more spatial temporal variables such as forest patch sizes and shapes that are closely associated with the acceleration of forest area decrease need to be considered for improving the overall projection performance in all grid cells 3 since the modelling work focused on the forest cover change in australia s intensive agricultural land with datasets used for projecting anthropogenic forest clearing i e clearing of remnant forests for agricultural use the trained model is likely to underperform for projecting forest cover or vegetation changes in some specific areas such as coastal areas in agricultural regions where some practice is not common in australia such as slash and burn agriculture or in places that were removed in this analysis such as fire affected forest areas to improve the projection performance of our model in these specific regions relevant explanatory variables possible drivers of the changes are needed for training the model for instance retraining the model to account for the impacts of climate e g hurricanes upstream pollution affecting coastal ecosystems and coastal land uses e g shrimp farming and tourism developments could help improve the projection performance in coastal areas stock et al 2018 4 this work did not quantify the uncertainty in projection results although uncertainty quantification has been widely included in classical prediction projection models e g dong et al 2015 gao et al 2016 peeters et al 2018 the popularity does not apply in machine learning models especially deep neural networks gal 2016 this is because traditional machine learning approaches to quantifying uncertainty are unable to scale to high dimensional inputs kendall and gal 2017 deep learning models that are usually computationally expensive cannot afford computational demands required by a uncertainty estimation method gal 2016 and the complex structure of a deep learning model as well as the large number of parameters make uncertainty quantification difficult lakshminarayanan et al 2017 however quantifying the uncertainty of a deep learning model in future projections is equally important as the accuracy of the model typical uncertainty sources include measurement noise of training data model parameter assumptions as well as model structure ghahramani 2015 knowing the uncertainty in the produced projections can help practitioners interpret the model and determine whether to apply the model into a high risk area such as self driving cars bayesian deep learning methods have been recently proven promising in estimating uncertainty in deep neural networks gal 2016 gal and ghahramani 2016 zhu and laptev 2017 these methods can offer a probabilistic interpretation to deep learning models via inferring distributions over the networks weights but it is still hard to apply these uncertainty estimation methods into our model although these methods avoid modelling distributions over models and their parameters which is difficult achieve at scale the computational cost is prohibitive in our case our current projection model required approximately 48 h execution time on a linux cluster with 114 high performance parallel computation nodes as a number of monte carlo runs is still required e g a monte carlo dropout sampling is required in these methods to apply a bernoulli distribution over the network s weights gal and ghahramani 2016 future work will design a single deep learning for projecting forest covers in all grid cells within the bayesian deep learning framework enabling the uncertainty quantification of the projection model 5 conclusion we successfully employed sophisticated deep learning methods to the challenging problem of projecting long term time continuous forest cover dynamics we developed an ensemble spatially explicit and continental scale projection model involving approximately 1 38 million grid cell sub models each grid cell model was built as a lstm network that included three lstm layers and a fully connected output layer and was built on a high performance computational cluster and a spatiotemporal dataset of relevant biophysical socioeconomic and institutional explanatory variables the deep learning model greatly outperformed a spatial econometric model with 44 improvement in the rmse indicator and 12 improvement in pseudo r squared the results also clearly demonstrate the robustness and effectiveness of the projection model in a broad range of parameter values and lack of influential training variables the relative importance of explanatory variables associated with the observed forest cover dynamics was also presented acknowledgements this work was supported by the csiro land and water l ye was supported by the shanghai 2016 teacher professional development project shec2016no 25 and l gao by a csiro julius career award the authors would also like to thank two anonymous reviewers for their constructive comments which have been very helpful for improving this manuscript appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 07 013 
