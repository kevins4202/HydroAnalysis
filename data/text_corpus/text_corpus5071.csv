index,text
25355,the two major aims of watershed management are regional economic development and watershed ecosystem protection in this paper constructed wetland cw technology is employed to examine watershed ecosystem protection under a certain regional economic load the chaohu lake watershed in china was chosen as the target study area and fuzzy random variables used to describe the uncertainties then a bi level optimization model for watershed scale cw planning was developed and applied to the chaohu lake watershed with the aim of protecting ecosystem health to solve the model a fuzzy random simulation based nested simulated annealing algorithm was designed the results showed that there was a finite sum game relationship between economic development and ecosystem health within the chaohu lake watershed under a probability and possibility of 0 9 to achieve the ecological goal f 1 70 it was estimated that a 348 19 ha cw needed to be constructed the impacts of environmental policies and economic development on the watershed ecosystem were found to be significant one of the key practical results indicated that in the chaohu lake watershed the water environmental treatment of rivers should take precedence over that of lakes a result which could be of assistance to local government policy making keywords constructed wetland ecosystem protection frs based nsaa fuzzy random variable optimization model notations indices i river i 1 2 i j industrial product j 1 2 j and k pollutant k 1 2 k fuzzy random parameters p ijk pollutant k emissions per unit of product j in river basin i kg de ik annual degradation of pollutant k per unit area of constructed wetland in river basin i kg ha e ij the economic benefits per unit of product j in river basin i 104 cny n i annual estimated visitor numbers in river basin i person ha yr and c i cw construction costs per unit area in river basin i 104 cny ha crisp parameters ra ij the production tax rate of industrial product j in river basin i w ij wages for per unit industrial product j in river basin i 104 cny yr rp ik the pigovian tax rate for pollutant k in river basin i 104 cny t al ik the pollutant k emissions allowances in river basin i t yr cap river ik maximum capacity of pollutant k in river i t ι ik policy control valve for pollutant k in river basin i y ij u maximum production capacity of product j in river basin i t i cw eco tourism scenic spot ticket price in river basin i cny person p i a pigovian tax function for pollution emissions in river basin i g i annual management fee for per unit constructed wetland i cny ha yr v river i the annual average flow of the river i m3 yr x i l x i u lower and upper limits of available land ha κ k policy control valve for pollutant k in lake cap lake k the maximum capacity of lake for pollutant k t c wetlandin k concentration of pollutant k in the constructed wetland inflow mg l q rain the annual precipitation mm α 1 α 2 ϑ i probability level β 1 β 2 θ i possibility level f 1 f i values for the upper and lower level budget objectives f 2 economic load 108 cny w 1 w 2 benchmark adjustment coefficients a bio a we normalization coefficient for biological abundance and water environment w 1 w 2 w 3 weighted coefficients and a 1 a 2 a 3 a the area of river lake wetland watershed km2 decision variables x i planned constructed wetland area in river basin i ha the upper level decision variables and y ij amount of product j in river basin i t the lower level decision variables 1 introduction the conflict between ecosystem protection and economic development has become a global watershed management issue patterson et al 2004 dong et al 2012 koundouri et al 2015 ecological mismanagement in the past decades has meant that in many parts of the world watershed ecosystem health has been seriously damaged because of rapid and uncontrolled economic development and urbanization moxnes 1998 with the ideology of development first environmental protection later this ecological mismanagement has been especially true in china liu 2010 where three decades of rapid economic growth has caused and then exacerbated water body problems wang et al 2008 liu and yang 2012 seriously damaging local ecosystems takungpao 2013 the chaohu lake watershed one of the most seriously polluted areas in china is a prime example xinhua news agency 2014 given its current economic development load it has now become critical to restore and protect the health of the chaohu lake watershed ecosystems to develop regional economies watershed managers usually local governments need to expand industrial production however these decisions inevitably lead to increased industrial waste such as waste water waste gas and solid waste the contamination carrying capacity of the natural watershed environment however is limited when pollution emissions exceed the carrying capacity upper bound the watershed ecosystem can be seriously damaged or destroyed these watershed carrying capacity limitations therefore mean that there is an inherent conflict between ecosystem protection and economic development chen and chen 2006 dong et al 2012 the two sustainable development aims for a watershed however are healthy ecosystems and developed economies liu et al 2008 parkes et al 2010 as cws have multiple service functions for example purifying water quality increasing biodiversity providing wildlife habitats and maintaining the stability and integrity of the ecosystem they can be implemented to resolve these conflicts martín et al 2013 jiang et al 2015 therefore in our research a cw is employed as a technical ecosystem protection tool constructed wetland as an artificial ecological engineering has been widely used for watershed management around the world xu et al 2015 studied the requirements for a new cw planning project and developed cw construction schemes based on the different demands of the regional economy social employment and water quality protection however the cw contributions to watershed ecosystem protection were not investigated seeking dynamic equilibrium ni et al studied the use of a cw to balance the needs of the watershed ecosystem and economic development ni et al 2016 however in watershed management the regional economic development budget is generally a constant value therefore to address these previous oversights in this paper cw based ecosystem protection is examined under a specific regional economic load for which new mathematical models algorithms and case validations are developed the identification of accurate quantitative expressions for the ecology the economy the pollutants and the environmental policies are essential in watershed management research as mathematical modelling techniques have been proven to be effective quantitative description tools they have been widely applied in many research disciplines including watershed management pahl wostl 2007 zhang and huang 2011 nonetheless as there are also many ecological economic and environmental uncertainties in watershed systems clark 1985 guillaume et al 2012 watershed management modelling is significantly more complex than traditional mathematical modelling methods xu et al 2013 to deal with the random and fuzzy uncertainty in previous modelling attempts stochastic fuzzy and interval approaches have been proposed huang and loucks 2000 saadatpour and afshar 2007 ni et al 2013 li et al 2014 zhang et al 2014 xie et al 2014 in which the fuzziness and randomness were separated however there are other uncertainties which have elements of both randomness and fuzziness such as pollutant degradation degree therefore to deal with this situation in this paper a fuzzy random variable kwakernaak 1978 1979 is employed to describe these kinds of uncertainties and a bi level optimization model is established to mimic the bi level watershed management structure based on the new large scale cw planning project at the chaohu lake watershed this paper focuses on ecosystem protection under a certain regional economic load the specific research objectives were 1 to embed the cw as a watershed management system component 2 to clarify the chaohu lake watershed bi level management structure 3 to establish a bi level optimization model for watershed ecosystem protection 4 to quantitatively describe the environmental policies in the model in accordance with the local environmental management status and 5 to investigate the practical results so as to give guidance to watershed management 2 methodology 2 1 conceptual model different from sewage treatment plants cw can not only treat various water pollutants but can also repair watershed ecosystems therefore watershed management departments usually local governments have begun to plan for watershed scale cw systems depending on the pollution treatments required in the cw local governments assign pollution emissions allowances to industrial enterprises in each river basin after which the industrial enterprises organize production based on the emissions allowances to maximize profits therefore watershed management has a bi level management structure in which the local government as the upper level decision maker has the authoritative position and the industrial enterprises in each watershed river basin as the lower level decision makers have a subordinate position these complex relationships can be shown in a conceptual model framework as shown in fig 1 2 2 uncertainty processing watershed management systems have many inherent uncertainties clark 1985 beck 1987 guillaume et al 2012 as an example here the cw pollutant degradation ability for a certain pollutant denoted de is taken to illustrate the uncertainty processing method used in this paper based on historical data the current situation and expert professional knowledge the degradation ability of the pollutant per cw unit area de can be determined by an expert group e n n 1 2 n and expressed in linguistic terms as an interval i e a n c n with a most possible value i e b n where n and e n denote n different experts and the nth expert therefore min n a n and max n c n are the lower and upper bounds of de by comparing the b n obtained from n different experts it can be found that b n is a random variable denoted b that approximately follows a normal distribution i e b n μ σ 2 which can be estimated using a maximum likelihood method and justified using a chi square goodness of fit test from these procedures it can be deduced that de has the fuzzy random variable characteristics proposed by kwakernaak 1978 1979 therefore de can be described as de and de a b c where a min n a n c max n c n b n μ σ 2 the entire process is shown in fig 2 other uncertainties in this paper are similarly handled 2 3 model formulation the cw based bi level optimization model for watershed ecosystem protection under a certain economic load is established in line with the conceptual model framework fig 1 and on the basis of the bi level optimization method proposed by bracken and mcgill 1973 in china all ecological indicators are quantitatively described as the weighted sum of the biological abundance index the vegetation cover index the water network density index the land degradation index and the environmental quality index in this study as only the watershed ecosystem is investigated the ecological sustainability was calculated from the chinese ecological environmental evaluation criteria the technical criterion for eco environmental status evaluation hj t192 2006 mep 2006 1 ei w 1 bio w 2 we where w 1 and w 2 are the dimension adjustment coefficients bio denotes the biological abundance indicator bio 1 a a bio l 1 3 w l a l w l is the weight a 1 a 2 a 3 and a respectively denote the river lake wetland and watershed areas we denotes the environmental water indicator and we 1 k k 1 k 100 a we j 1 j i 1 i p ijk y ij de ik x i q rain x i is the planned cw area in river basin i ha and is the upper level decision variable y ij is the amount of product j in the river basin i 103 kg yr and are the lower level decision variables p ijk denotes the pollutant k emissions per unit of product j in the river basin i kg de ik denotes the annual degradation of pollutant k per unit area of the cw i kg ha q rain denotes the annual precipitation mm a bio and a we are normalization coefficients therefore formula 1 can be rewritten as 2 ei 1 a w 1 a bio l 1 3 w l a l 1 3 w 2 k 1 k 100 a we j 1 j i 1 i p ijk y ij de ik x i q rain normally the local government has a target ecological budget f 1 a crisp number in formula 2 p ijk and de ij are fuzzy random variables therefore as the formula 2 result is also a fuzzy random number it cannot be compared with a crisp number to deal with this situation a chance measure xu and yao 2011 is used to describe the possibility that the formula 2 result is not less than the target budget f 1 under a probability level α 1 as shown in formula 3 3 pos pr 1 a w 1 a bio l 1 3 w l a l 1 3 w 2 k 1 k 100 a we j 1 j i 1 i p ijk y ij de ik x i q rain f 1 α 1 where pr denotes the probability of a random event pr α 1 denotes that the probability of a random event is not less than α 1 denotes not less than and pos denotes the possibility of a fuzzy event formula 3 denotes that under probability level α 1 the local government seeks a maximum possibility β 1 that the watershed ecological situation is no lower than the target budget f 1 therefore formula 3 can be converted into a dependent chance objective and its constraint xu and zhou 2011 as follows 4 max β 1 s t ch 1 a w 1 a bio σ l 1 3 w l a l 1 3 w 2 σ k 1 k 100 a we σ j 1 j σ i 1 i p ijk y ij de ik x i q rain f 1 α 1 β 1 developing the watershed economy is an important management task for the local government under a probability α 2 and a possibility β 2 the local government seeks to ensure that the total watershed economy is no less than a target budget f 2 in this research the watershed economy is considered to have three parts industrial taxes ra ij e ij y ij cw eco tourism income t i n i x i and the cw construction costs c i x i which can be written as j 1 j i 1 i ra ij e ij y ij i 1 i t i n i x i i 1 i c i x i where ra ij e ij t i n i and c i respectively denote in river basin i the production tax rate of industrial product j the economic benefits per unit for product j 104 cny cw eco tourism scenic spot ticket price cny person annual estimated visitor numbers person ha yr and cw construction costs per unit area 104 cny ha therefore the watershed economic development is described as a chance constraint xu and ding 2011 5 ch j 1 j i 1 i ra ij e ij y ij i 1 i t i n i x i i 1 i c i x i f 2 α 2 β 2 land area for the cw is limited in each river basin that is x i x i u x i u is the maximum usable area at the same time as a large scale project there are minimum construction area constraints x i l i e 0 x i l x i therefore the cw area constraints for the upper level model are 6 0 x i l x i x i u i to protect watershed ecosystem health the total amount of pollutant k discharged into the lake should not exceed κ k times the maximum capacity of the lake under natural conditions cap lake k κ k 0 1 κ k is the local government policy that regulates pollutant k emissions and specifies the local government administrative strictures to be applied to protect the lake water environment the smaller the value of κ k the more stringent the pollution control policy the total amount of pollutants discharged into the lake can be written as j 1 j i 1 i p ijk y ij de ik x i as p ijk and de ik are fuzzy random variables κ k cap lake k is an exact variable the fuzzy random variable cannot be directly compared with an exact variable therefore an expected value operator ev is employed for the fuzzy random variable xu and zhou 2011 and the environmental policy constraint for the lake can be written as 7 ev j 1 j i 1 i p ijk y ij de ik x i κ k cap lake k κ k 0 1 k the maximum capacity cap lake k is calculated using the vollenweider model vollenweider 1975 1976 the industrial enterprises decision goal is profit maximization which is determined from industrial production revenue e ij y ij minus the fines penalties for excessive pollution emissions p i j 1 j p ijk y ij the cw daily management fee g i x i and wages w ij y ij 8 j 1 j e ij y ij k 1 k p i j 1 j p ijk y ij g i x i j 1 j w ij y ij i where e ij p i g i and w ij respectively denote the economic benefits per unit of product j in river basin i 104 cny a pigovian tax function the annual management fee per unit area of cw i cny ha yr and the wages per unit of industrial product j in river basin i 104 cny yr under a probability ϑ i and possibility θ i the industrial enterprises located in river basin i seek to maximize profit f i formula 8 can therefore be rewritten as a chance objective xu and yao 2011 xu and zhou 2011 as follows 9 max f i s t ch σ j 1 j e ij y ij σ k 1 k p i σ j 1 j p ijk y ij g i x i σ j 1 j w ij y ij f i ϑ i θ i river pollution protection is similar to lake pollution protection that is the total quantity of pollutant k discharged into river i should not exceed ι ik times the maximum capacity of river i cap river ik that is the j 1 j p ijk y ij is not more than ι ik cap river ik ι ik ι ik 0 1 is another local government policy regulation using the expected value operator this can be written as 10 ev j 1 j p ijk y ij ι ik cap river ik ι ik 0 1 i k the value of cap river ik is calculated using the thomas model the pigovian tax is a commonly used method for regulating pollution emissions each industrial enterprise is allocated certain pollutant k emissions allowances al ik however if the pollutant k emissions exceed the given allowances a pigou tax is imposed which is mathematically expressed as 11 p i p ijk y ij k 1 k rp ik ev j 1 j p ijk y ij al ik ev j 1 j p ijk y ij al ik 2 where rp ik denotes the pigovian tax rate for pollutant k in river basin i 104 cny t as the cw is to be built in the river mouth the river outflow is the cw inflow according to the technical specification for constructed wetlands for wastewater treatment engineering hj 2005 2010 the pollutant k concentration in the river outflow i e j 1 j p ijk y ij v river i v river i denotes the flow of the river i must not exceed the cw input concentration standard that is 12 ev j 1 j p ijk y ij v river i c wetlandin k k the industrial enterprises production capacities are considered nonnegative and finite that is 13 0 y ij y ij u i j 2 4 global model from the integration of formulas 4 7 and 9 13 a cw based bi level optimization model under uncertainty for ecosystem protection under a certain economic load is obtained as follows 14 max β 1 s t ch 1 a w 1 a bio σ l 1 3 w l a l 1 3 w 2 σ k 1 k 100 a we σ j 1 j σ i 1 i p ijk y ij de ik x i q rain f 1 α 1 β 1 ch σ j 1 j σ i 1 i ra ij e ij y ij σ i 1 i t i n i x i σ i 1 i c i x i f 2 α 2 β 2 ev σ j 1 j σ i 1 i p ijk y ij de ik x i κ k cap lake k 0 x i l x i x i u max f i s t ch σ j 1 j e ij y ij σ k 1 k p i σ j 1 j p ijk y ij g i x i σ j 1 j w ij y ij f i ϑ i θ i ev σ j 1 j p ijk y ij ι ik cap river ik ev σ j 1 j p ijk y ij v river i c wetlandin k p i p ijk y ij σ k 1 k rp ik ev σ j 1 j p ijk y ij al ik ev σ j 1 j p ijk y ij al ik 2 0 y ij y ij u in model 14 as all model parameters are written as undetermined parameters the model can be easily applied to other watersheds for example the environmental policy parameters κ k and ι ik and the possibility θ i and probability ϑ i levels in model 14 can be calibrated according to local environmental policies when the model is applied to other watersheds other parameters can be similarly calibrated 2 5 algorithm design even in the simplest case solving a bi level programming problem is np hard bard 1991 therefore it is difficult to solve multi objective bi level programming in model 14 because it contains fuzzy random variables the simulated annealing technique sat proposed by kirkpatrick kirkpatrick et al 1983 however has been proven to be suitable for calculating the numerical solution to complex optimization problems in various fields liu et al 2017 haghi et al 2017 mahmoodpour and masihi 2016 therefore because of the sats good convergence it is used as the basic framework for the design of the new algorithm to solve model 14 another difficulty in model 14 is the calculations for the 119 fuzzy random variables there are two common ways to deal with fuzzy random variables 1 converting them into exact numbers using a mathematical operator li et al 2006 xu et al 2013 and 2 using fuzzy random simulation frs xu et al 2015 ni et al 2016 as frs has less information loss it is employed to deal with the fuzzy random parameters in model 14 based on the above and the bi level structure of model 14 a new algorithm the fuzzy random simulation based nested simulated annealing algorithm frs based nsaa is designed to solve the model the frs based nsaa algorithmic flow chart is shown in fig 3 in fig 3 the outer layer of the frs based nsaa is used to solve the upper level of model 14 and the inner layer is used to solve the lower level taking the β 1 in model 14 as an example the fuzzy random simulation procedure for the frs based nsaa fitness value is summarized as follows fuzzy random simulation fitness value procedure step 1 generate independently random numbers ω 1 ω 2 from ω1 ω2 according to the probability measure pr step 2 produce fuzzy variables p ijk ω 1 and de ik ω 2 based on the structure of the fuzzy random variables p ijk and de ik respectively step 3 let β 1 ε ε is a sufficiently small positive number step 4 randomly generate crisp numbers p ijk and de ik from the ε level set of the fuzzy variables p ijk ω 1 and de ik ω 2 respectively step 5 compute the value v 1 a w 1 a bio l 1 3 w l a l 1 3 w 2 k 1 k 100 a we j 1 j i 1 i p ijk y ij de ik x i q rain step 6 let μ μ p ijk p ijk μ de ik de ik step 7 if v f 1 and μ β 1 replace β 1 with μ step 8 repeat step 4 to step 7 m times return β 1 step 9 repeat step 1 to step 8 n times return β 1 1 β 1 2 β 1 n step 10 let n be the integer part of α 1 n step 11 return the n th largest element in β 1 1 β 1 2 β 1 n 3 case study 3 1 study area chaohu lake is situated in central anhui province in eastern china and is one of the five largest freshwater lakes in china the chaohu lake watershed has an area of 13 486km2 a catchment population of 10 2 million an average annual temperature of 15 16 c and a mean annual precipitation of 1000 1100mm thirty three rivers including seven main rivers are radially distributed in the watershed all of which finally flow into chaohu lake as shown in fig 4 3 2 data collection for this research seven major rivers three major pollutants and three industrial product categories at the chaohu lake watershed were included therefore the river pollutant and industrial product indices were set at i 7 k 3 and j 3 that is i 1 2 7 k 1 2 3 and j 1 2 3 to solve the model the data for the model parameters were collected and collated from the anhui province hydrology bureau and environmental protection office the anhui provincial statistics bureau the chaohu lake administration bureau the anhui province inland revenue bureau and the technical criterion for eco environmental status evaluation hj t192 2006 mep 2006 due to the local geography and chaohu lake management policies price levels across the entire chaohu lake watershed were very similar therefore the following parameters were considered to be the same throughout the chaohu lake watershed for all i 1 2 7 g i 1 5 104 cny ha yr t i 30 cny person w i1 13 104 cny person yr w i2 w i3 11 5 104 cny person yr ra i1 ra i3 5 rp i1 rp i2 rp i2 20 c i 135 c i 167 c i n 151 11 104 cny ha n i 9000 n i 12 000 n i n 11 000 200 person ha yr the other parameters are shown in tables 1 4 the basic chaohu lake watershed situation data are listed in table 1 the basic chaohu lake watershed data for ecosystem status are listed in table 2 the other fuzzy random parameters are listed in tables 3 and 4 3 3 model results based on the flow of the frs based nsaa algorithm fig 3 a computer program was developed to run model 14 using matlab software by comparing the results of the preliminary experiments which observed the behavior of frs based nsaa at different parameter settings the most reasonable parameters were set as follows initial temperatures for the outer and inner layers t 1 300 t 2 200 sampling times for the outer and inner layers l 1 500 l 2 300 and the outer and inner layer cooling rates λ 1 λ 2 0 95 the program was run on a pentium 4 2 40ghz clock pulse with a 1024mb memory personal computer and the model calculation results were obtained when the decision makers control parameters were set at f 1 70 f 2 4 108 α 1 α 2 0 9 β 2 0 9 κ k 0 5 ι ik 0 5 and ϑ i θ i 0 85 k 1 2 3 i 1 2 7 the optimal objective values and optimal schemes were derived as shown in table 5 the results showed that under a regional economic load of f 2 4 108 cny yr if the watershed ecological health index f 1 70 was obtained at a possibility level of 0 879 then 338 hectares of constructed wetlands would need to be built in the different river basins 4 discussion 4 1 impact of environmental policy on ecosystem health the decision makers control parameters were set at f 1 70 f 2 4 108 α 1 α 2 0 9 β 2 0 9 and ϑ i θ i 0 85 i 1 2 7 and the policy control valves κ k and ι ik k 1 2 3 i 1 2 7 were individually adjusted from 0 2 to 0 95 with a step size of 0 05 under a probability of 0 9 with the change in ι ik κ k the possibility of achieving the ecological target f 1 70 was calculated as shown in table 6 it can be seen from table 6 that the smaller the value for ι ik κ k the greater the possibility of achieving the target f 1 70 and the better the ecosystem in model 14 in the constraints ev j 1 j p ijk y ij ι ik cap river ik and ev j 1 j i 1 i p ijk y ij de ik x i κ k cap lake k the variables ι ik and κ k describe the intensity of the local government environmental policies as ι ik κ k 0 1 for all k 1 2 3 i 1 2 7 the smaller the value of ι ik κ k the less the river lake capacity to discharge pollutants and the greater the intensity of the environmental protection policies the better the watershed ecosystem therefore the results in table 6 reveal that environmental management policies play a key role in protecting watershed ecosystem health 4 2 insights from the model into the necessity for public resource management when all the environmental policy conditions in model 14 are removed and the watershed ecosystem protection objective is also removed then under the constraint ch j 1 j i 1 i ra ij e ij y ij f 2 α 2 β 2 model 14 degenerates to a single level optimization model the formula for which is as follows 15 max f i s t ch σ j 1 j e ij y ij σ j 1 j w ij y ij f i ϑ i θ i 0 y ij y ij u formula 15 indicates that the local government has an annual economic development budget f 2 and that the industrial enterprises located in each river basin are pursuing maximum profit under the premise that the economic development budget must be maintained therefore this could be seen to be an exact expression for the economic development pattern in initial economic development stage in developing countries under this development pattern the ecosystems damage is the most serious if constraint ch j 1 j i 1 i ra ij e ij y ij f 2 α 2 β 2 is removed formula 15 then implies that individual production is free driven by the profit maximization objective environmental resources are totally consumed by the industrial enterprises to expand production resulting in what is known as the tragedy of the commons hardin 1968 the above two ecosystem damage scenarios are a result of poor resource management moxnes 1998 therefore as environmental resource management needs to be strengthened ostrom 1990 model 14 is ideal for investigating the relationship between economic development ecosystem protection and environmental management policy 4 3 practical management insights in the chaohu lake watershed based on the results shown in table 6 the impact relationships between the river and lake environmental policies on the watershed ecosystem were identified as shown in fig 5 fig 5 shows that the watershed ecosystem is more sensitive to policies related to the environmental control of the river i e ι ik than to policies related to the environmental control of the lake i e κ k the impact on the watershed ecosystem of both river and lake environmental policies was found to be the most significant between 0 4 and 0 8 that is when pollution emissions reach 40 to 80 of the river lake carrying capacities policy intervention is necessary from above analyses two conclusions about the chaohu lake watershed management can be made 1 in the chaohu lake watershed the environmental treatment of the water in the river should take precedence over the environmental treatment of the water in the lake 2 environmental management policies should be strengthened when pollutant emissions reach 40 of the river lake carrying capacity if pollutant emissions exceeds 80 of the river lake carrying capacity the water environment will be damaged and the environmental policies will no longer be effective 4 4 the finite sum game relationship between economic development and ecosystem health to simulate the impact of economic development on the watershed ecosystem the decision makers control parameters were set at f 1 70 α 1 α 2 0 9 β 2 0 9 κ k 0 5 ι ik 0 5 and ϑ i θ i 0 85 k 1 2 3 i 1 2 7 and the economic budget f 2 108 cny was adjusted from 2 to 8 with a step size of 0 5 108 cny in this way the impact of economic development on the watershed ecosystem was simulated the simulation results are shown in table 7 the results showed that the impact of the regional economy on the watershed ecosystem was significant under a probability of 0 9 the possibility of achieving ecological goal f 1 70 was as high as 0 956 when the total regional economy was f 2 2 108 cny however when f 2 was increased to 8 108 cny the possibility fell to 0 685 to interpret the economic development impact on the watershed ecosystem the relationship between economic development level and the watershed ecosystem figures in table 7 were visualized in fig 6 table 7 and fig 6 show that due to the limited carrying capacity of the environment there is a finite sum game relationship between economic development and ecosystem health in the watershed indicating that ecosystem protection and economic development are conflicted in watershed management necessary measures such as the construction of cw are required to develop the regional economy while protecting the watershed ecosystem 4 5 effectiveness of the constructed wetland to protect the watershed ecosystem the parameters of the model remained set at f 1 70 α 1 α 2 0 9 β 2 0 9 κ k 0 5 ι ik 0 5 ϑ i θ i 0 85 k 1 2 3 i 1 2 7 and the economic budget f 2 108 cny was adjusted from 2 to 8 by a step size of 0 5 108 cny the effect of the cw on watershed ecosystem compensation was thus simulated the results for which were listed in table 8 the results showed that to ensure watershed ecosystem health the index cannot fall below 70 f 1 70 and the regional economy cannot exceed 2 0 108 cny if these were maintained at these levels there would be no need to construct wetlands to compensate for the loss of ecosystem health if this were not the case however a certain area must be allocated to constructed wetlands when the economy reaches 8 0 108 cny an area of 3351 36ha cw would be needed to compensate for the losses in watershed ecosystem health caused by the economic development the effect of the cw on watershed ecosystem compensation in table 8 was visualized in fig 7 from fig 7 it can be seen that a cw is effective in repairing watershed ecologies in the chaohu lake watershed when the size of the watershed economy is small i e f 2 2 108 cny and the total pollution generated by the economic development is not beyond the carrying capacity of the watershed environment there is no need for constructed wetlands as the economy grows however there is a commensurate rise in pollutant emissions which eventually exceed the environmental carrying capacity and cause ecosystem damage therefore an increasingly large cw would be needed to repair the watershed ecosystem when the size of the economy is larger than 6 0 108 cny the relationship between economic growth and the cw area is nearly linear which indicates that the degradation caused by the pollutants generated by the economic development is almost entirely dependent on an increase in the cw area and also reveals the effectiveness of the cw in ecosystem maintenance and restoration 5 conclusions a watershed management framework and an inexact bi level programming model were established to investigate ecosystem protection under a certain economic load in modelling a cw system was used as a technological tool and a new algorithm the frs based nsaa was designed to solve the model chaohu lake watershed in china was chosen as a case study area to verify the model some management insights were gained from the results of the model 1 there is a finite sum game relationship between ecosystem health and economic development in a watershed the underlying cause of this relationship is the limited environmental carrying capacity when economic development reaches a certain level the environmental carrying capacity is insufficient to simultaneously support an increased economic load and maintain ecosystem health at which point a conflict between economic development and the ecosystem emerges 2 environmental management policies play a key role in protecting watershed ecosystem health when pollution emissions reach around 40 to 80 of the environmental carrying capacity policy intervention is necessary for the chaohu lake watershed the environmental treatment of river water should take precedence over that of lake water the serious ecosystem damage in developing countries and the occurrence of the tragedy of the commons phenomenon have mainly been because of poor or no management of public resources 3 as a type of artificial ecological engineering cw systems are effective in maintaining and restoring ecosystems however this effectiveness is conditional as the land available to construct the cw is limited in most watersheds nonetheless the insights gained from this study could be helpful in guiding watershed management practices constructed wetland modelling for watershed ecosystem protection can be combined with other watershed management measures such as pollution emissions permit trading this would require the development of new mathematical models and algorithms and the uncertainties in the systems would need to be reinvestigated and processed all of which we plan to examine further in future work acknowledgements this research is supported by the key program of national natural science foundation of china grant no 70831005 and also supported by the research foundation of ministry of education for the doctoral program of higher education of china grant no 20130181110063 the social science foundation of anhui province grant no ahsky2017d83 and the quality engineering project of hefei university grant no 2017jyxm016 
25355,the two major aims of watershed management are regional economic development and watershed ecosystem protection in this paper constructed wetland cw technology is employed to examine watershed ecosystem protection under a certain regional economic load the chaohu lake watershed in china was chosen as the target study area and fuzzy random variables used to describe the uncertainties then a bi level optimization model for watershed scale cw planning was developed and applied to the chaohu lake watershed with the aim of protecting ecosystem health to solve the model a fuzzy random simulation based nested simulated annealing algorithm was designed the results showed that there was a finite sum game relationship between economic development and ecosystem health within the chaohu lake watershed under a probability and possibility of 0 9 to achieve the ecological goal f 1 70 it was estimated that a 348 19 ha cw needed to be constructed the impacts of environmental policies and economic development on the watershed ecosystem were found to be significant one of the key practical results indicated that in the chaohu lake watershed the water environmental treatment of rivers should take precedence over that of lakes a result which could be of assistance to local government policy making keywords constructed wetland ecosystem protection frs based nsaa fuzzy random variable optimization model notations indices i river i 1 2 i j industrial product j 1 2 j and k pollutant k 1 2 k fuzzy random parameters p ijk pollutant k emissions per unit of product j in river basin i kg de ik annual degradation of pollutant k per unit area of constructed wetland in river basin i kg ha e ij the economic benefits per unit of product j in river basin i 104 cny n i annual estimated visitor numbers in river basin i person ha yr and c i cw construction costs per unit area in river basin i 104 cny ha crisp parameters ra ij the production tax rate of industrial product j in river basin i w ij wages for per unit industrial product j in river basin i 104 cny yr rp ik the pigovian tax rate for pollutant k in river basin i 104 cny t al ik the pollutant k emissions allowances in river basin i t yr cap river ik maximum capacity of pollutant k in river i t ι ik policy control valve for pollutant k in river basin i y ij u maximum production capacity of product j in river basin i t i cw eco tourism scenic spot ticket price in river basin i cny person p i a pigovian tax function for pollution emissions in river basin i g i annual management fee for per unit constructed wetland i cny ha yr v river i the annual average flow of the river i m3 yr x i l x i u lower and upper limits of available land ha κ k policy control valve for pollutant k in lake cap lake k the maximum capacity of lake for pollutant k t c wetlandin k concentration of pollutant k in the constructed wetland inflow mg l q rain the annual precipitation mm α 1 α 2 ϑ i probability level β 1 β 2 θ i possibility level f 1 f i values for the upper and lower level budget objectives f 2 economic load 108 cny w 1 w 2 benchmark adjustment coefficients a bio a we normalization coefficient for biological abundance and water environment w 1 w 2 w 3 weighted coefficients and a 1 a 2 a 3 a the area of river lake wetland watershed km2 decision variables x i planned constructed wetland area in river basin i ha the upper level decision variables and y ij amount of product j in river basin i t the lower level decision variables 1 introduction the conflict between ecosystem protection and economic development has become a global watershed management issue patterson et al 2004 dong et al 2012 koundouri et al 2015 ecological mismanagement in the past decades has meant that in many parts of the world watershed ecosystem health has been seriously damaged because of rapid and uncontrolled economic development and urbanization moxnes 1998 with the ideology of development first environmental protection later this ecological mismanagement has been especially true in china liu 2010 where three decades of rapid economic growth has caused and then exacerbated water body problems wang et al 2008 liu and yang 2012 seriously damaging local ecosystems takungpao 2013 the chaohu lake watershed one of the most seriously polluted areas in china is a prime example xinhua news agency 2014 given its current economic development load it has now become critical to restore and protect the health of the chaohu lake watershed ecosystems to develop regional economies watershed managers usually local governments need to expand industrial production however these decisions inevitably lead to increased industrial waste such as waste water waste gas and solid waste the contamination carrying capacity of the natural watershed environment however is limited when pollution emissions exceed the carrying capacity upper bound the watershed ecosystem can be seriously damaged or destroyed these watershed carrying capacity limitations therefore mean that there is an inherent conflict between ecosystem protection and economic development chen and chen 2006 dong et al 2012 the two sustainable development aims for a watershed however are healthy ecosystems and developed economies liu et al 2008 parkes et al 2010 as cws have multiple service functions for example purifying water quality increasing biodiversity providing wildlife habitats and maintaining the stability and integrity of the ecosystem they can be implemented to resolve these conflicts martín et al 2013 jiang et al 2015 therefore in our research a cw is employed as a technical ecosystem protection tool constructed wetland as an artificial ecological engineering has been widely used for watershed management around the world xu et al 2015 studied the requirements for a new cw planning project and developed cw construction schemes based on the different demands of the regional economy social employment and water quality protection however the cw contributions to watershed ecosystem protection were not investigated seeking dynamic equilibrium ni et al studied the use of a cw to balance the needs of the watershed ecosystem and economic development ni et al 2016 however in watershed management the regional economic development budget is generally a constant value therefore to address these previous oversights in this paper cw based ecosystem protection is examined under a specific regional economic load for which new mathematical models algorithms and case validations are developed the identification of accurate quantitative expressions for the ecology the economy the pollutants and the environmental policies are essential in watershed management research as mathematical modelling techniques have been proven to be effective quantitative description tools they have been widely applied in many research disciplines including watershed management pahl wostl 2007 zhang and huang 2011 nonetheless as there are also many ecological economic and environmental uncertainties in watershed systems clark 1985 guillaume et al 2012 watershed management modelling is significantly more complex than traditional mathematical modelling methods xu et al 2013 to deal with the random and fuzzy uncertainty in previous modelling attempts stochastic fuzzy and interval approaches have been proposed huang and loucks 2000 saadatpour and afshar 2007 ni et al 2013 li et al 2014 zhang et al 2014 xie et al 2014 in which the fuzziness and randomness were separated however there are other uncertainties which have elements of both randomness and fuzziness such as pollutant degradation degree therefore to deal with this situation in this paper a fuzzy random variable kwakernaak 1978 1979 is employed to describe these kinds of uncertainties and a bi level optimization model is established to mimic the bi level watershed management structure based on the new large scale cw planning project at the chaohu lake watershed this paper focuses on ecosystem protection under a certain regional economic load the specific research objectives were 1 to embed the cw as a watershed management system component 2 to clarify the chaohu lake watershed bi level management structure 3 to establish a bi level optimization model for watershed ecosystem protection 4 to quantitatively describe the environmental policies in the model in accordance with the local environmental management status and 5 to investigate the practical results so as to give guidance to watershed management 2 methodology 2 1 conceptual model different from sewage treatment plants cw can not only treat various water pollutants but can also repair watershed ecosystems therefore watershed management departments usually local governments have begun to plan for watershed scale cw systems depending on the pollution treatments required in the cw local governments assign pollution emissions allowances to industrial enterprises in each river basin after which the industrial enterprises organize production based on the emissions allowances to maximize profits therefore watershed management has a bi level management structure in which the local government as the upper level decision maker has the authoritative position and the industrial enterprises in each watershed river basin as the lower level decision makers have a subordinate position these complex relationships can be shown in a conceptual model framework as shown in fig 1 2 2 uncertainty processing watershed management systems have many inherent uncertainties clark 1985 beck 1987 guillaume et al 2012 as an example here the cw pollutant degradation ability for a certain pollutant denoted de is taken to illustrate the uncertainty processing method used in this paper based on historical data the current situation and expert professional knowledge the degradation ability of the pollutant per cw unit area de can be determined by an expert group e n n 1 2 n and expressed in linguistic terms as an interval i e a n c n with a most possible value i e b n where n and e n denote n different experts and the nth expert therefore min n a n and max n c n are the lower and upper bounds of de by comparing the b n obtained from n different experts it can be found that b n is a random variable denoted b that approximately follows a normal distribution i e b n μ σ 2 which can be estimated using a maximum likelihood method and justified using a chi square goodness of fit test from these procedures it can be deduced that de has the fuzzy random variable characteristics proposed by kwakernaak 1978 1979 therefore de can be described as de and de a b c where a min n a n c max n c n b n μ σ 2 the entire process is shown in fig 2 other uncertainties in this paper are similarly handled 2 3 model formulation the cw based bi level optimization model for watershed ecosystem protection under a certain economic load is established in line with the conceptual model framework fig 1 and on the basis of the bi level optimization method proposed by bracken and mcgill 1973 in china all ecological indicators are quantitatively described as the weighted sum of the biological abundance index the vegetation cover index the water network density index the land degradation index and the environmental quality index in this study as only the watershed ecosystem is investigated the ecological sustainability was calculated from the chinese ecological environmental evaluation criteria the technical criterion for eco environmental status evaluation hj t192 2006 mep 2006 1 ei w 1 bio w 2 we where w 1 and w 2 are the dimension adjustment coefficients bio denotes the biological abundance indicator bio 1 a a bio l 1 3 w l a l w l is the weight a 1 a 2 a 3 and a respectively denote the river lake wetland and watershed areas we denotes the environmental water indicator and we 1 k k 1 k 100 a we j 1 j i 1 i p ijk y ij de ik x i q rain x i is the planned cw area in river basin i ha and is the upper level decision variable y ij is the amount of product j in the river basin i 103 kg yr and are the lower level decision variables p ijk denotes the pollutant k emissions per unit of product j in the river basin i kg de ik denotes the annual degradation of pollutant k per unit area of the cw i kg ha q rain denotes the annual precipitation mm a bio and a we are normalization coefficients therefore formula 1 can be rewritten as 2 ei 1 a w 1 a bio l 1 3 w l a l 1 3 w 2 k 1 k 100 a we j 1 j i 1 i p ijk y ij de ik x i q rain normally the local government has a target ecological budget f 1 a crisp number in formula 2 p ijk and de ij are fuzzy random variables therefore as the formula 2 result is also a fuzzy random number it cannot be compared with a crisp number to deal with this situation a chance measure xu and yao 2011 is used to describe the possibility that the formula 2 result is not less than the target budget f 1 under a probability level α 1 as shown in formula 3 3 pos pr 1 a w 1 a bio l 1 3 w l a l 1 3 w 2 k 1 k 100 a we j 1 j i 1 i p ijk y ij de ik x i q rain f 1 α 1 where pr denotes the probability of a random event pr α 1 denotes that the probability of a random event is not less than α 1 denotes not less than and pos denotes the possibility of a fuzzy event formula 3 denotes that under probability level α 1 the local government seeks a maximum possibility β 1 that the watershed ecological situation is no lower than the target budget f 1 therefore formula 3 can be converted into a dependent chance objective and its constraint xu and zhou 2011 as follows 4 max β 1 s t ch 1 a w 1 a bio σ l 1 3 w l a l 1 3 w 2 σ k 1 k 100 a we σ j 1 j σ i 1 i p ijk y ij de ik x i q rain f 1 α 1 β 1 developing the watershed economy is an important management task for the local government under a probability α 2 and a possibility β 2 the local government seeks to ensure that the total watershed economy is no less than a target budget f 2 in this research the watershed economy is considered to have three parts industrial taxes ra ij e ij y ij cw eco tourism income t i n i x i and the cw construction costs c i x i which can be written as j 1 j i 1 i ra ij e ij y ij i 1 i t i n i x i i 1 i c i x i where ra ij e ij t i n i and c i respectively denote in river basin i the production tax rate of industrial product j the economic benefits per unit for product j 104 cny cw eco tourism scenic spot ticket price cny person annual estimated visitor numbers person ha yr and cw construction costs per unit area 104 cny ha therefore the watershed economic development is described as a chance constraint xu and ding 2011 5 ch j 1 j i 1 i ra ij e ij y ij i 1 i t i n i x i i 1 i c i x i f 2 α 2 β 2 land area for the cw is limited in each river basin that is x i x i u x i u is the maximum usable area at the same time as a large scale project there are minimum construction area constraints x i l i e 0 x i l x i therefore the cw area constraints for the upper level model are 6 0 x i l x i x i u i to protect watershed ecosystem health the total amount of pollutant k discharged into the lake should not exceed κ k times the maximum capacity of the lake under natural conditions cap lake k κ k 0 1 κ k is the local government policy that regulates pollutant k emissions and specifies the local government administrative strictures to be applied to protect the lake water environment the smaller the value of κ k the more stringent the pollution control policy the total amount of pollutants discharged into the lake can be written as j 1 j i 1 i p ijk y ij de ik x i as p ijk and de ik are fuzzy random variables κ k cap lake k is an exact variable the fuzzy random variable cannot be directly compared with an exact variable therefore an expected value operator ev is employed for the fuzzy random variable xu and zhou 2011 and the environmental policy constraint for the lake can be written as 7 ev j 1 j i 1 i p ijk y ij de ik x i κ k cap lake k κ k 0 1 k the maximum capacity cap lake k is calculated using the vollenweider model vollenweider 1975 1976 the industrial enterprises decision goal is profit maximization which is determined from industrial production revenue e ij y ij minus the fines penalties for excessive pollution emissions p i j 1 j p ijk y ij the cw daily management fee g i x i and wages w ij y ij 8 j 1 j e ij y ij k 1 k p i j 1 j p ijk y ij g i x i j 1 j w ij y ij i where e ij p i g i and w ij respectively denote the economic benefits per unit of product j in river basin i 104 cny a pigovian tax function the annual management fee per unit area of cw i cny ha yr and the wages per unit of industrial product j in river basin i 104 cny yr under a probability ϑ i and possibility θ i the industrial enterprises located in river basin i seek to maximize profit f i formula 8 can therefore be rewritten as a chance objective xu and yao 2011 xu and zhou 2011 as follows 9 max f i s t ch σ j 1 j e ij y ij σ k 1 k p i σ j 1 j p ijk y ij g i x i σ j 1 j w ij y ij f i ϑ i θ i river pollution protection is similar to lake pollution protection that is the total quantity of pollutant k discharged into river i should not exceed ι ik times the maximum capacity of river i cap river ik that is the j 1 j p ijk y ij is not more than ι ik cap river ik ι ik ι ik 0 1 is another local government policy regulation using the expected value operator this can be written as 10 ev j 1 j p ijk y ij ι ik cap river ik ι ik 0 1 i k the value of cap river ik is calculated using the thomas model the pigovian tax is a commonly used method for regulating pollution emissions each industrial enterprise is allocated certain pollutant k emissions allowances al ik however if the pollutant k emissions exceed the given allowances a pigou tax is imposed which is mathematically expressed as 11 p i p ijk y ij k 1 k rp ik ev j 1 j p ijk y ij al ik ev j 1 j p ijk y ij al ik 2 where rp ik denotes the pigovian tax rate for pollutant k in river basin i 104 cny t as the cw is to be built in the river mouth the river outflow is the cw inflow according to the technical specification for constructed wetlands for wastewater treatment engineering hj 2005 2010 the pollutant k concentration in the river outflow i e j 1 j p ijk y ij v river i v river i denotes the flow of the river i must not exceed the cw input concentration standard that is 12 ev j 1 j p ijk y ij v river i c wetlandin k k the industrial enterprises production capacities are considered nonnegative and finite that is 13 0 y ij y ij u i j 2 4 global model from the integration of formulas 4 7 and 9 13 a cw based bi level optimization model under uncertainty for ecosystem protection under a certain economic load is obtained as follows 14 max β 1 s t ch 1 a w 1 a bio σ l 1 3 w l a l 1 3 w 2 σ k 1 k 100 a we σ j 1 j σ i 1 i p ijk y ij de ik x i q rain f 1 α 1 β 1 ch σ j 1 j σ i 1 i ra ij e ij y ij σ i 1 i t i n i x i σ i 1 i c i x i f 2 α 2 β 2 ev σ j 1 j σ i 1 i p ijk y ij de ik x i κ k cap lake k 0 x i l x i x i u max f i s t ch σ j 1 j e ij y ij σ k 1 k p i σ j 1 j p ijk y ij g i x i σ j 1 j w ij y ij f i ϑ i θ i ev σ j 1 j p ijk y ij ι ik cap river ik ev σ j 1 j p ijk y ij v river i c wetlandin k p i p ijk y ij σ k 1 k rp ik ev σ j 1 j p ijk y ij al ik ev σ j 1 j p ijk y ij al ik 2 0 y ij y ij u in model 14 as all model parameters are written as undetermined parameters the model can be easily applied to other watersheds for example the environmental policy parameters κ k and ι ik and the possibility θ i and probability ϑ i levels in model 14 can be calibrated according to local environmental policies when the model is applied to other watersheds other parameters can be similarly calibrated 2 5 algorithm design even in the simplest case solving a bi level programming problem is np hard bard 1991 therefore it is difficult to solve multi objective bi level programming in model 14 because it contains fuzzy random variables the simulated annealing technique sat proposed by kirkpatrick kirkpatrick et al 1983 however has been proven to be suitable for calculating the numerical solution to complex optimization problems in various fields liu et al 2017 haghi et al 2017 mahmoodpour and masihi 2016 therefore because of the sats good convergence it is used as the basic framework for the design of the new algorithm to solve model 14 another difficulty in model 14 is the calculations for the 119 fuzzy random variables there are two common ways to deal with fuzzy random variables 1 converting them into exact numbers using a mathematical operator li et al 2006 xu et al 2013 and 2 using fuzzy random simulation frs xu et al 2015 ni et al 2016 as frs has less information loss it is employed to deal with the fuzzy random parameters in model 14 based on the above and the bi level structure of model 14 a new algorithm the fuzzy random simulation based nested simulated annealing algorithm frs based nsaa is designed to solve the model the frs based nsaa algorithmic flow chart is shown in fig 3 in fig 3 the outer layer of the frs based nsaa is used to solve the upper level of model 14 and the inner layer is used to solve the lower level taking the β 1 in model 14 as an example the fuzzy random simulation procedure for the frs based nsaa fitness value is summarized as follows fuzzy random simulation fitness value procedure step 1 generate independently random numbers ω 1 ω 2 from ω1 ω2 according to the probability measure pr step 2 produce fuzzy variables p ijk ω 1 and de ik ω 2 based on the structure of the fuzzy random variables p ijk and de ik respectively step 3 let β 1 ε ε is a sufficiently small positive number step 4 randomly generate crisp numbers p ijk and de ik from the ε level set of the fuzzy variables p ijk ω 1 and de ik ω 2 respectively step 5 compute the value v 1 a w 1 a bio l 1 3 w l a l 1 3 w 2 k 1 k 100 a we j 1 j i 1 i p ijk y ij de ik x i q rain step 6 let μ μ p ijk p ijk μ de ik de ik step 7 if v f 1 and μ β 1 replace β 1 with μ step 8 repeat step 4 to step 7 m times return β 1 step 9 repeat step 1 to step 8 n times return β 1 1 β 1 2 β 1 n step 10 let n be the integer part of α 1 n step 11 return the n th largest element in β 1 1 β 1 2 β 1 n 3 case study 3 1 study area chaohu lake is situated in central anhui province in eastern china and is one of the five largest freshwater lakes in china the chaohu lake watershed has an area of 13 486km2 a catchment population of 10 2 million an average annual temperature of 15 16 c and a mean annual precipitation of 1000 1100mm thirty three rivers including seven main rivers are radially distributed in the watershed all of which finally flow into chaohu lake as shown in fig 4 3 2 data collection for this research seven major rivers three major pollutants and three industrial product categories at the chaohu lake watershed were included therefore the river pollutant and industrial product indices were set at i 7 k 3 and j 3 that is i 1 2 7 k 1 2 3 and j 1 2 3 to solve the model the data for the model parameters were collected and collated from the anhui province hydrology bureau and environmental protection office the anhui provincial statistics bureau the chaohu lake administration bureau the anhui province inland revenue bureau and the technical criterion for eco environmental status evaluation hj t192 2006 mep 2006 due to the local geography and chaohu lake management policies price levels across the entire chaohu lake watershed were very similar therefore the following parameters were considered to be the same throughout the chaohu lake watershed for all i 1 2 7 g i 1 5 104 cny ha yr t i 30 cny person w i1 13 104 cny person yr w i2 w i3 11 5 104 cny person yr ra i1 ra i3 5 rp i1 rp i2 rp i2 20 c i 135 c i 167 c i n 151 11 104 cny ha n i 9000 n i 12 000 n i n 11 000 200 person ha yr the other parameters are shown in tables 1 4 the basic chaohu lake watershed situation data are listed in table 1 the basic chaohu lake watershed data for ecosystem status are listed in table 2 the other fuzzy random parameters are listed in tables 3 and 4 3 3 model results based on the flow of the frs based nsaa algorithm fig 3 a computer program was developed to run model 14 using matlab software by comparing the results of the preliminary experiments which observed the behavior of frs based nsaa at different parameter settings the most reasonable parameters were set as follows initial temperatures for the outer and inner layers t 1 300 t 2 200 sampling times for the outer and inner layers l 1 500 l 2 300 and the outer and inner layer cooling rates λ 1 λ 2 0 95 the program was run on a pentium 4 2 40ghz clock pulse with a 1024mb memory personal computer and the model calculation results were obtained when the decision makers control parameters were set at f 1 70 f 2 4 108 α 1 α 2 0 9 β 2 0 9 κ k 0 5 ι ik 0 5 and ϑ i θ i 0 85 k 1 2 3 i 1 2 7 the optimal objective values and optimal schemes were derived as shown in table 5 the results showed that under a regional economic load of f 2 4 108 cny yr if the watershed ecological health index f 1 70 was obtained at a possibility level of 0 879 then 338 hectares of constructed wetlands would need to be built in the different river basins 4 discussion 4 1 impact of environmental policy on ecosystem health the decision makers control parameters were set at f 1 70 f 2 4 108 α 1 α 2 0 9 β 2 0 9 and ϑ i θ i 0 85 i 1 2 7 and the policy control valves κ k and ι ik k 1 2 3 i 1 2 7 were individually adjusted from 0 2 to 0 95 with a step size of 0 05 under a probability of 0 9 with the change in ι ik κ k the possibility of achieving the ecological target f 1 70 was calculated as shown in table 6 it can be seen from table 6 that the smaller the value for ι ik κ k the greater the possibility of achieving the target f 1 70 and the better the ecosystem in model 14 in the constraints ev j 1 j p ijk y ij ι ik cap river ik and ev j 1 j i 1 i p ijk y ij de ik x i κ k cap lake k the variables ι ik and κ k describe the intensity of the local government environmental policies as ι ik κ k 0 1 for all k 1 2 3 i 1 2 7 the smaller the value of ι ik κ k the less the river lake capacity to discharge pollutants and the greater the intensity of the environmental protection policies the better the watershed ecosystem therefore the results in table 6 reveal that environmental management policies play a key role in protecting watershed ecosystem health 4 2 insights from the model into the necessity for public resource management when all the environmental policy conditions in model 14 are removed and the watershed ecosystem protection objective is also removed then under the constraint ch j 1 j i 1 i ra ij e ij y ij f 2 α 2 β 2 model 14 degenerates to a single level optimization model the formula for which is as follows 15 max f i s t ch σ j 1 j e ij y ij σ j 1 j w ij y ij f i ϑ i θ i 0 y ij y ij u formula 15 indicates that the local government has an annual economic development budget f 2 and that the industrial enterprises located in each river basin are pursuing maximum profit under the premise that the economic development budget must be maintained therefore this could be seen to be an exact expression for the economic development pattern in initial economic development stage in developing countries under this development pattern the ecosystems damage is the most serious if constraint ch j 1 j i 1 i ra ij e ij y ij f 2 α 2 β 2 is removed formula 15 then implies that individual production is free driven by the profit maximization objective environmental resources are totally consumed by the industrial enterprises to expand production resulting in what is known as the tragedy of the commons hardin 1968 the above two ecosystem damage scenarios are a result of poor resource management moxnes 1998 therefore as environmental resource management needs to be strengthened ostrom 1990 model 14 is ideal for investigating the relationship between economic development ecosystem protection and environmental management policy 4 3 practical management insights in the chaohu lake watershed based on the results shown in table 6 the impact relationships between the river and lake environmental policies on the watershed ecosystem were identified as shown in fig 5 fig 5 shows that the watershed ecosystem is more sensitive to policies related to the environmental control of the river i e ι ik than to policies related to the environmental control of the lake i e κ k the impact on the watershed ecosystem of both river and lake environmental policies was found to be the most significant between 0 4 and 0 8 that is when pollution emissions reach 40 to 80 of the river lake carrying capacities policy intervention is necessary from above analyses two conclusions about the chaohu lake watershed management can be made 1 in the chaohu lake watershed the environmental treatment of the water in the river should take precedence over the environmental treatment of the water in the lake 2 environmental management policies should be strengthened when pollutant emissions reach 40 of the river lake carrying capacity if pollutant emissions exceeds 80 of the river lake carrying capacity the water environment will be damaged and the environmental policies will no longer be effective 4 4 the finite sum game relationship between economic development and ecosystem health to simulate the impact of economic development on the watershed ecosystem the decision makers control parameters were set at f 1 70 α 1 α 2 0 9 β 2 0 9 κ k 0 5 ι ik 0 5 and ϑ i θ i 0 85 k 1 2 3 i 1 2 7 and the economic budget f 2 108 cny was adjusted from 2 to 8 with a step size of 0 5 108 cny in this way the impact of economic development on the watershed ecosystem was simulated the simulation results are shown in table 7 the results showed that the impact of the regional economy on the watershed ecosystem was significant under a probability of 0 9 the possibility of achieving ecological goal f 1 70 was as high as 0 956 when the total regional economy was f 2 2 108 cny however when f 2 was increased to 8 108 cny the possibility fell to 0 685 to interpret the economic development impact on the watershed ecosystem the relationship between economic development level and the watershed ecosystem figures in table 7 were visualized in fig 6 table 7 and fig 6 show that due to the limited carrying capacity of the environment there is a finite sum game relationship between economic development and ecosystem health in the watershed indicating that ecosystem protection and economic development are conflicted in watershed management necessary measures such as the construction of cw are required to develop the regional economy while protecting the watershed ecosystem 4 5 effectiveness of the constructed wetland to protect the watershed ecosystem the parameters of the model remained set at f 1 70 α 1 α 2 0 9 β 2 0 9 κ k 0 5 ι ik 0 5 ϑ i θ i 0 85 k 1 2 3 i 1 2 7 and the economic budget f 2 108 cny was adjusted from 2 to 8 by a step size of 0 5 108 cny the effect of the cw on watershed ecosystem compensation was thus simulated the results for which were listed in table 8 the results showed that to ensure watershed ecosystem health the index cannot fall below 70 f 1 70 and the regional economy cannot exceed 2 0 108 cny if these were maintained at these levels there would be no need to construct wetlands to compensate for the loss of ecosystem health if this were not the case however a certain area must be allocated to constructed wetlands when the economy reaches 8 0 108 cny an area of 3351 36ha cw would be needed to compensate for the losses in watershed ecosystem health caused by the economic development the effect of the cw on watershed ecosystem compensation in table 8 was visualized in fig 7 from fig 7 it can be seen that a cw is effective in repairing watershed ecologies in the chaohu lake watershed when the size of the watershed economy is small i e f 2 2 108 cny and the total pollution generated by the economic development is not beyond the carrying capacity of the watershed environment there is no need for constructed wetlands as the economy grows however there is a commensurate rise in pollutant emissions which eventually exceed the environmental carrying capacity and cause ecosystem damage therefore an increasingly large cw would be needed to repair the watershed ecosystem when the size of the economy is larger than 6 0 108 cny the relationship between economic growth and the cw area is nearly linear which indicates that the degradation caused by the pollutants generated by the economic development is almost entirely dependent on an increase in the cw area and also reveals the effectiveness of the cw in ecosystem maintenance and restoration 5 conclusions a watershed management framework and an inexact bi level programming model were established to investigate ecosystem protection under a certain economic load in modelling a cw system was used as a technological tool and a new algorithm the frs based nsaa was designed to solve the model chaohu lake watershed in china was chosen as a case study area to verify the model some management insights were gained from the results of the model 1 there is a finite sum game relationship between ecosystem health and economic development in a watershed the underlying cause of this relationship is the limited environmental carrying capacity when economic development reaches a certain level the environmental carrying capacity is insufficient to simultaneously support an increased economic load and maintain ecosystem health at which point a conflict between economic development and the ecosystem emerges 2 environmental management policies play a key role in protecting watershed ecosystem health when pollution emissions reach around 40 to 80 of the environmental carrying capacity policy intervention is necessary for the chaohu lake watershed the environmental treatment of river water should take precedence over that of lake water the serious ecosystem damage in developing countries and the occurrence of the tragedy of the commons phenomenon have mainly been because of poor or no management of public resources 3 as a type of artificial ecological engineering cw systems are effective in maintaining and restoring ecosystems however this effectiveness is conditional as the land available to construct the cw is limited in most watersheds nonetheless the insights gained from this study could be helpful in guiding watershed management practices constructed wetland modelling for watershed ecosystem protection can be combined with other watershed management measures such as pollution emissions permit trading this would require the development of new mathematical models and algorithms and the uncertainties in the systems would need to be reinvestigated and processed all of which we plan to examine further in future work acknowledgements this research is supported by the key program of national natural science foundation of china grant no 70831005 and also supported by the research foundation of ministry of education for the doctoral program of higher education of china grant no 20130181110063 the social science foundation of anhui province grant no ahsky2017d83 and the quality engineering project of hefei university grant no 2017jyxm016 
25356,dams and road culverts fragment river ecosystems worldwide by restricting the movement of aquatic species in many watersheds a diverse set of actors coordinates the removal of these barriers non governmental organizations often focus on small dams and road culverts while large dam removal projects are coordinated by federal agencies or coalitions of partners here we evaluate the return on investment of these strategies by exploring a continuum of methods for selecting barrier removal projects ranging from a focus on many small barrier removal projects to a few large ones we used estimated removal costs of more than 100 000 barriers in the north american great lakes to construct economically realistic barrier removal scenarios we then simulated the movement of stream resident and anadromous fishes through model river networks with a few large dam removals many road culvert retrofits or a mix of both we found that the strategy of removing both dams and road culverts had the greatest potential to benefit both stream resident and anadromous fishes but only when projects were aligned longitudinally within the river network our results demonstrate the importance of allocating conservation resources to both small and large restoration projects and highlight a need for increased coordination and communication among the many different organizations investing in barrier removals our findings complement optimization approaches to prioritizing barrier removals by providing general guidelines for practitioners to follow when project selection must depart from a prescribed portfolio of projects keywords stream resident fish anadromous fish fragmentation river restoration freshwater conservation planning 1 introduction habitat fragmentation is a leading cause of global biodiversity decline fischer and lindenmayer 2007 perkin et al 2015 the impacts of fragmentation are particularly devastating for many freshwater fishes kanehl et al 1997 warren and pardew 1998 catalano et al 2007 because they are restricted to river networks fagan 2002 consequently a single barrier in a river network can completely block fish movements in most fragmented watersheds barriers include dams and road crossings both of which can be detrimental to stream fishes warren and pardew 1998 nilsson et al 2005 bouska and paukert 2010 januchowski hartley et al 2013 to remedy this situation local and national conservation organizations are increasingly interested in restoring freshwater connectivity by removing dams and retrofitting road culverts grossman 2002 magilligan et al 2016 in most cases completed barrier removal projects have been selected by a process of strategic opportunism magilligan et al 2016 which includes both strategic planning and the identification of unexpected opportunities to remove particular barriers at low economic or sociopolitical cost in many watersheds investments in restoring ecosystem connectivity are coordinated by a diverse group of governmental natural resource management agencies and non governmental conservation organizations with varying budgets focal geographies and species priorities neeson et al 2015 due to diverse institutional constraints different organizations often prefer to focus on different classes of barrier removal projects in general barrier removal strategies exist along a continuum ranging from efforts to remove a small number of large dams to a preference for many small dam and road culvert projects large dam removals are often complex costly highly politicized and can take years of effort by conservation and government organizations to be implemented grossman 2002 wildman 2013 notable examples include the recently removed elwha dam in washington service 2011 and the ongoing deliberation concerning the rodman dam in florida grossman 2002 though challenging to carry out large dam removals can be particularly beneficial for anadromous fishes providing a dramatic increase in access to the river network and upstream spawning habitat at the opposite end of the spectrum local watershed level organizations tend to focus on small dam removals and road culvert upgrades although removing these structures can still be contentious depending on ownership and location grossman 2002 fox et al 2016 they are typically much cheaper to execute and less controversial barrier removals in small headwater streams will not aid anadromous species if the mouth of the tributary remains blocked but can still benefit stream resident species by reconnecting previously isolated sub populations and increasing accessible habitat bednarek 2001 catalano et al 2007 given the growing interest in restoring ecosystem connectivity and a general lack of available funds for meeting conservation needs mccarthy et al 2012 it is critical to identify strategies that enable a diverse set of natural resource managers to collectively maximize return on investment roi murdoch et al 2007 from barrier removal projects in a conservation context roi is the amount of conservation benefit that could be achieved for a given budget murdoch et al 2007 for barrier removals the benefit is typically measured as the increase in accessible habitat across the river network i e connectivity kemp and o hanley 2010 although any barrier removal will improve connectivity benefits may vary dramatically among projects depending on available habitat for beneficiary species spatial context of the barrier within the river network and the set of other barrier removal projects completed or planned within the watershed inefficiencies can arise from lack of communication between agencies focused on different species or project classes o hanley et al 2013 or from piecemeal planning of projects leading to missed opportunities for aligning barrier removals neeson et al 2015 furthermore if species dispersal patterns life history strategies and habitats are not considered while planning a barrier removal the benefits can be limited to only a few species most previous research has focused on the use of optimization models kemp and o hanley 2010 mckay et al 2017 or spatial graph models mckay et al 2013 branco et al 2014 to identify a set of barrier removal projects that would result in the greatest benefit for stream fishes these optimal plans often rely on all proposed removals being implemented simultaneously or in the near future kemp and o hanley 2010 neeson et al 2015 and rarely consider the complex social and political factors that determine the feasibility of a barrier removal project grossman 2002 fox et al 2016 in reality social and political factors often limit conservation practitioners ability to implement a prescribed portfolio of barrier removal projects magilligan et al 2017 in these cases conservation organizations would benefit from a general barrier removal strategy that they could follow to maximize roi in the long term while responding to immediate opportunities to remove particular barriers at low socio political cost i e a policy of strategic opportunism isenberg 1987 here we aim to develop general guidelines for conservation practitioners to follow when prioritizing barrier removal projects specifically we calculate the roi for three common strategies for allocating conservation funds for barrier removals towards the removal of a few large dams the removal of a larger number of road culverts or a mixed strategy consisting of both dam and road culvert removals our aim was to draw general conclusions that were not specific to any one river network or a single fish species accordingly we created an individual based model ibm of two fish populations one of stream resident fish and the other of anadromous fish in a generalized representation of a fragmented river network the ibm approach allows us to examine variability in restoration efficiency resulting from spatial alignment of barrier removals as well as variability created by stochasticity in the spatial dynamics of the fish populations themselves focusing on this combined variability we investigate the best case worst case and average outcomes in terms of population distribution for stream resident and anadromous fishes under these three conservation strategies 2 methods we created an ibm to simulate movement patterns of stream resident and anadromous fishes through a fragmented river network the model consists of three components a river network a fish population and a set of barriers that block fish movements the cost of removing a barrier was based on stream order and derived from a database of more than 100 000 barriers in the north american great lakes neeson et al 2015 thus our barrier removal scenarios reflect the true range of project choices available to practitioners working in a large freshwater ecosystem we modeled two general life history strategies of stream fishes stream resident and anadromous to account for the impact of barrier removals on fishes with either type of migratory strategy we simulated each fish type separately which enabled us to describe the response of each type of fish to the barrier removal strategies 2 1 river network submodel the river network for all model runs is a symmetric dendritic river network with fifteen reaches fig 1 a in defining our river network we took a patch based graph approach eros et al 2012 in which each reach within the river network fig 1a is condensed into a patch or node and links or edges represent the possibility for movement between reaches fig 1b thus the river network overall is a graph g n l with nodes n indexed by n and links l indexed by l we define a reach as the section of river between two confluences and assume that each reach in the network provides an equivalent amount of fish habitat fig 1b each reach is directly connected to a maximum of three other reaches one downstream and two upstream in our model we assume that a barrier if present completely blocks both upstream and downstream movement of fishes between reaches and that barrier removal restores full movement between reaches fig 1c following perkin et al 2013 barriers are placed directly between reaches we refer to each barrier according to the strahler order of the upstream reach such that a barrier between a first order and a second order reach is a first order barrier fig 1c 2 2 fish submodel we hypothesized that the way in which individual fish interact with the complex shape of a fragmented river network would play a key role in structuring fish distributions neeson et al 2011 2012 accordingly we chose an ibm approach because it allowed us to capture these individual interactions we simulated fish populations based on two common life history strategies stream resident and anadromous allowing us to characterize the benefits of barrier removal projects for a diverse community of fishes though movement rates vary considerably among species and individuals mcintyre et al 2016 our intent was to focus on long term impacts of barrier removals on equilibrium distributions of stream fishes which will be insensitive to the speed at which individuals colonize recently connected habitat accordingly our model uses a weekly time step which is the finest temporal resolution that captures movement rates of an average migratory fish species okland et al 2001 at the coarse spatial resolution of our modeled river network fig 1 fast migrating species e g salmon would likely reach an equilibrium distribution across the river network more quickly than species with limited movement e g mottled sculpin at every time step t each individual fish must choose whether to move to a directly connected reach j or to stay within its current reach i the probability of transitioning from i to j is given by pi j in our model stream resident fishes have an equal chance of moving to any neighboring upstream or downstream reach that is accessible i e no barrier is present or remaining in the reach they currently occupy for example the transition probabilities at any time step for a stream resident fish in reach 4 fig 1 are p 4 4 p 4 2 p 4 8 p 4 9 0 25 the rules that govern anadromous fish are similar with the exception that they perform spawning runs and all movement is either upstream prior to spawning or downstream post spawning for an upstream migrating fish in reach 4 for example the transition probabilities at any time step are p 4 2 0p 4 8 p 4 9 p 4 4 0 33 and p 4 2 0 spawning occurs in the reach a fish is located in at the end of upstream migration three time steps or when no further upstream movement is possible i e when an individual fish reaches a first order reach or further upstream movement is blocked by a barrier after all spawning is complete all fish migrate downstream at the same time anadromous fish always begin at the mouth of the river network and return there during downstream migrations in the case of stream resident fish the entire population begins each run in a single reach this represents a maximally isolated stream resident population constrained to a fraction of their historical habitat we hypothesized that the benefits of a set of barrier removals for stream resident fish would depend on both the location of barrier removals and the location of the isolated population within the river network accordingly we performed a series of model runs with the entire stream resident population beginning in one of the fifteen reaches and initially no individual fish in any of the other fourteen reaches in describing the effects of barrier removals on fishes we focused on the evenness of the distribution of individuals across the entire river network for the purposes of this study we assumed that all reaches within the network had equal habitat and available resources so that when no barriers were present stream resident fishes would follow an ideal free distribution fretwell and lucas 1969 and distribute themselves equally throughout the river network while habitat and resources are likely to vary across reaches in real river networks their distribution is unlikely to vary systematically and will be specific to the targeted tributary in fragmented river networks fish distributions are often highly skewed because barriers inhibit dispersal of individuals towards an ideal free distribution perkin et al 2015 this aggregation increases density dependent effects and at the population level constitutes an underutilization of the total amount of suitable habitat available in the river network to quantify the degree of aggregation of the fish population in each model run we calculated the standard deviation s of fish abundance among reaches either after the final time step for stream resident fish or after the last step of the final upstream migration for anadromous fish as s i 1 n f i f 2 n 1 where f i is the number of fish in reach i and n is the number of nodes i e reaches in the river network a high standard deviation is indicative of a tightly clustered population with access to only one or a few reaches while a standard deviation of zero represents a perfectly even distribution of fish among all reaches in the river network 2 4 barrier removal scenarios for both dams and road culverts the size of a barrier and cost of removing it generally increases with stream size neeson et al 2015 to determine in more detail how the cost of a barrier removal depends on stream size we analyzed a database of estimated removal costs for 3954 dams and 99 940 road stream crossings in tributaries of the north american great lakes neeson et al 2015 barriers on small streams strahler order 1 or 2 were primarily road stream crossings 97 and had an average removal cost of us 125 073 n 86 541 including the cost of material and labor for removing the road culvert and replacing it with a fully passable bridge we did not consider other approaches to restoring fish passage such as fishways since these techniques typically target strong swimmers e g salmonids and may not benefit other fish species with weaker swimming abilities mallen cooper and brand 2007 bunt et al 2012 barrier removal costs averaged us 197 236 n 15 765 on medium streams strahler order 3 or 4 and us 320 110 n 310 on large streams strahler order 5 in this data set all barriers on large streams were dams all road stream crossings on high order streams in this dataset were bridges and fully passable to aquatic organisms and thus not candidates for removal using these average prices as a starting point we modeled the removal costs of small medium and large barriers as us 100k us 200k and us 300k respectively this modest deviation from the true project costs enabled us to compare barrier removal scenarios involving an integer number of barriers for example a budget of us 300k might be spent on one large three small or one medium and one small project furthermore given that 97 of barriers on small streams were road culverts we henceforth refer to all small barriers as road culverts similarly we refer to all barriers on large streams as large dams given that road culverts do not occur on any large streams in the great lakes using these estimates of barrier characteristics and costs we compared three barrier removal strategies commonly used by conservation practitioners in each case we fully allocate a total budget of us 600k the first strategy entails removing both of the large dams near the mouth of the river network fig 2 a the second is a mixed strategy entailing the removal of one large dam one road culvert and one medium barrier fig 2b the final strategy involves the removal of six road culverts fig 2c as a null model we also completed an additional set of runs on a free flowing river network without barriers fig 1b note that multiple permutations of the second n 64 and third strategies n 28 are possible and the benefits to stream fishes may differ dramatically among the permutations to understand the range of outcomes possible under each strategy we modeled all possible permutations of each strategy throughout the network fig 3 shows three examples of different configurations of the mixed removal strategy fig 2b in which all fig 3a two fig 3b or none fig 3c of the barrier removals are coordinated 2 5 computational details we developed and ran the ibm in java using the mason library for agent based simulation luke et al 2005 we simulated a population of 100 000 fish for each simulation run we ran each simulation for 600 time steps which was sufficiently long enough to allow the stream resident population to reach an equilibrium for their distribution which generally occurred after 100 time steps to capture the variability that resulted from the stochastic movement of the individual fish we performed 100 simulations of each permutation of each barrier removal strategy there was a single permutation of the dam removal strategy 64 of the mixed removal strategy and 28 of the road culvert removal strategy since we simulated the stream resident population beginning entirely in a single reach we performed 15 runs per strategy permutation with the stream resident population beginning in a different reach each time this resulted in a total of 141 000 and 9 400 simulation runs for stream resident and anadromous fishes respectively 3 results we found clear differences in the average roi of the three conservation strategies and between the best case and worst case outcomes for each strategy considering the average outcomes no single strategy was best for both stream resident and anadromous fishes on average the best strategy for stream resident fishes was the removal of six road culverts fig 4 a however anadromous fishes never benefitted from this strategy because both large dams remained in place blocking any upstream migrations fig 4b the best strategy on average for anadromous fishes was the removal of two large dams fig 4b but this strategy had low average benefit for stream resident fishes fig 4a in contrast to these average outcomes the best possible outcome for both stream resident and anadromous fishes occurred under the mixed strategy i e the removal of one small one medium and one large barrier fig 4a b this single strategy when optimally executed provided the maximum possible benefit to stream resident and anadromous fishes simultaneously worst case outcomes for stream resident fishes occurred under all three strategies highlighting the broad range of outcomes possible under each strategy while the mixed removal strategy can lead to the best possible outcomes for both stream resident and anadromous fishes these best case outcomes only occur under certain rare spatial arrangements of barrier removal projects fig 5 when all three barrier removals of the mixed removal strategy were aligned longitudinally in the river network as in fig 3a both stream resident and anadromous fishes experienced the overall greatest benefit however the longitudinal alignment of all three barrier removals projects occurred in only a small subset of the spatial permutations of this strategy peak a in fig 5a b the alignment of only two or no barrier removal projects peaks b and c in fig 5a b respectively occurred in a greater number of permutations and yet other permutations included cases where barrier removals reconnected reaches containing no stream resident fishes peak d in fig 5a thus while anadromous fishes always received at least some benefit from the mixed removal strategy stream resident fishes only benefited when the removals were adjacent to the stream reach in which the population resided peak d in fig 5a 4 discussion our results show clear differences in the roi from three common conservation strategies removing a few large dams many road culverts or a mix of both fig 4 the greatest benefit for stream resident and anadromous fishes occurred under the mixed removal strategy but only when the removals were aligned longitudinally within the river network fig 5 when barrier removals were not aligned which is a common outcome of piecemeal or individualistic planning magilligan et al 2016 benefit decreased for both stream resident and anadromous fishes to the point where a mixed removal strategy could be less effective than the removal of just dams or just road culverts when the conservation strategies targeted a single class of barriers i e the removal of two large dams or of six road culverts only fish of a single life history strategy benefited on average the large dam removal strategy was the most beneficial for anadromous fishes but the least beneficial for stream resident species in contrast removing road culverts was on average the best case scenario for stream resident fishes but under this scenario anadromous fishes never benefited though a mixed removal strategy can provide the greatest return on investment effective implementation of this strategy requires the coordinated efforts of practitioners from many different organizations as a result it is unlikely to occur under a piecemeal approach to project selection branco et al 2014 the mixed removal strategy also provides an opportunity to leverage the expertise of each type of organization local organizations often have a good understanding of where fish populations have historically occurred and of the level of support from the local community for a particular project as a result local conservation organizations are often ideally positioned for detailed evaluation of the ecological societal and economic costs and benefits of a particular barrier removal grossman 2002 fox et al 2016 conversely large scale planning initiatives typically overseen by national organizations may be naive to local social and political issues which can result in removals being delayed or stopped completely jorgensen and renofalt 2012 fox et al 2016 at the same time the regional perspective of federal agencies and national ngos can enable these groups to identify high priority watersheds and focal regions for investment and to coordinate investments across jurisdictional boundaries milt et al 2017 our aim was to identify barrier removal strategies that would be broadly applicable across species and river networks the long run equilibrium distributions of fishes across river networks figs 4 and 5 demonstrated in our model will be reached more quickly by fast migrating species e g salmon while slow migrating species e g sculpin may take many years to reach an equilibrium density across a reconnected river network furthermore our model contains no density dependence thus our results could be readily scaled to river systems involving much smaller or greater populations of migratory fishes while our model enabled us to characterize the relative benefits of different removal strategies it does not account for spatial variation in environmental factors such as habitat type and condition sociopolitical factors like ownership and barrier degradation or variation in barrier impacts including passability similarly our model does not account for the localized effects of some barriers on river flow temperature and sediments stanley and doyle 2003 that collectively transform river networks into highly novel ecosystems radeloff et al 2015 consideration of these factors is known to be critical for evaluating the potential benefit and feasibility of particular barrier removals poff and hart 2002 zheng and hobbs 2013 januchowski hartley et al 2014 fox et al 2016 however these factors are unlikely to vary in a systematic way across different river networks as a result inclusion of these additional factors into our model is unlikely to alter our general findings regarding the most cost effective strategy these site specific considerations will necessarily vary among barriers and among tributaries but conservation organizations should nevertheless be able to follow the general principle of the mixed removal strategy i e the removal of different sized barriers that are longitudinally aligned while evaluating site specific factors to achieve the greatest roi our results complement existing approaches for prioritizing barrier removals by providing conservation practitioners with general guidelines for project selection for example both federal agencies and local ngos commonly use optimization models to identify high priority barrier removal projects zheng et al 2009 o hanley et al 2013 neeson et al 2015 though optimization models can identify a mathematically optimal set of projects implementing these conservation plans at smaller scales is often complicated by local politics magilligan et al 2016 particularly with dams ownership can be ambiguous and the dam itself may have historical significance for the surrounding community fox et al 2016 neither are accounted for by optimization models and can cause opposition to the removal among the local community though optimization models are increasingly used to identify a set of high priority projects conservation practitioners in practice often implement an opportunistic strategy in which organizations target barriers that already need to be replaced or have cooperating owners magilligan et al 2016 as a result the set of projects completed deviates from the one prescribed by an optimization model our study complements both barrier removal optimization studies and opportunistic approaches by providing conservation practitioners with a general strategy to follow to maximize conservation outcomes when project selection must depart from the recommendations of an optimization model conservation practitioners should seek a balance of large and small projects and ensure that they are spatially aligned moreover decision makers could follow this general strategy while using one of the increasing number of online decision support tools moody et al 2017 to balance mathematically optimal project selection with a need to avoid known areas of socio political contention acknowledgements we thank p doran p mcintyre n sleight the lcluc research group at ou and two anonymous reviewers for thoughtful feedback on the ideas herein funding was provided by the upper midwest and great lakes lcc 
25356,dams and road culverts fragment river ecosystems worldwide by restricting the movement of aquatic species in many watersheds a diverse set of actors coordinates the removal of these barriers non governmental organizations often focus on small dams and road culverts while large dam removal projects are coordinated by federal agencies or coalitions of partners here we evaluate the return on investment of these strategies by exploring a continuum of methods for selecting barrier removal projects ranging from a focus on many small barrier removal projects to a few large ones we used estimated removal costs of more than 100 000 barriers in the north american great lakes to construct economically realistic barrier removal scenarios we then simulated the movement of stream resident and anadromous fishes through model river networks with a few large dam removals many road culvert retrofits or a mix of both we found that the strategy of removing both dams and road culverts had the greatest potential to benefit both stream resident and anadromous fishes but only when projects were aligned longitudinally within the river network our results demonstrate the importance of allocating conservation resources to both small and large restoration projects and highlight a need for increased coordination and communication among the many different organizations investing in barrier removals our findings complement optimization approaches to prioritizing barrier removals by providing general guidelines for practitioners to follow when project selection must depart from a prescribed portfolio of projects keywords stream resident fish anadromous fish fragmentation river restoration freshwater conservation planning 1 introduction habitat fragmentation is a leading cause of global biodiversity decline fischer and lindenmayer 2007 perkin et al 2015 the impacts of fragmentation are particularly devastating for many freshwater fishes kanehl et al 1997 warren and pardew 1998 catalano et al 2007 because they are restricted to river networks fagan 2002 consequently a single barrier in a river network can completely block fish movements in most fragmented watersheds barriers include dams and road crossings both of which can be detrimental to stream fishes warren and pardew 1998 nilsson et al 2005 bouska and paukert 2010 januchowski hartley et al 2013 to remedy this situation local and national conservation organizations are increasingly interested in restoring freshwater connectivity by removing dams and retrofitting road culverts grossman 2002 magilligan et al 2016 in most cases completed barrier removal projects have been selected by a process of strategic opportunism magilligan et al 2016 which includes both strategic planning and the identification of unexpected opportunities to remove particular barriers at low economic or sociopolitical cost in many watersheds investments in restoring ecosystem connectivity are coordinated by a diverse group of governmental natural resource management agencies and non governmental conservation organizations with varying budgets focal geographies and species priorities neeson et al 2015 due to diverse institutional constraints different organizations often prefer to focus on different classes of barrier removal projects in general barrier removal strategies exist along a continuum ranging from efforts to remove a small number of large dams to a preference for many small dam and road culvert projects large dam removals are often complex costly highly politicized and can take years of effort by conservation and government organizations to be implemented grossman 2002 wildman 2013 notable examples include the recently removed elwha dam in washington service 2011 and the ongoing deliberation concerning the rodman dam in florida grossman 2002 though challenging to carry out large dam removals can be particularly beneficial for anadromous fishes providing a dramatic increase in access to the river network and upstream spawning habitat at the opposite end of the spectrum local watershed level organizations tend to focus on small dam removals and road culvert upgrades although removing these structures can still be contentious depending on ownership and location grossman 2002 fox et al 2016 they are typically much cheaper to execute and less controversial barrier removals in small headwater streams will not aid anadromous species if the mouth of the tributary remains blocked but can still benefit stream resident species by reconnecting previously isolated sub populations and increasing accessible habitat bednarek 2001 catalano et al 2007 given the growing interest in restoring ecosystem connectivity and a general lack of available funds for meeting conservation needs mccarthy et al 2012 it is critical to identify strategies that enable a diverse set of natural resource managers to collectively maximize return on investment roi murdoch et al 2007 from barrier removal projects in a conservation context roi is the amount of conservation benefit that could be achieved for a given budget murdoch et al 2007 for barrier removals the benefit is typically measured as the increase in accessible habitat across the river network i e connectivity kemp and o hanley 2010 although any barrier removal will improve connectivity benefits may vary dramatically among projects depending on available habitat for beneficiary species spatial context of the barrier within the river network and the set of other barrier removal projects completed or planned within the watershed inefficiencies can arise from lack of communication between agencies focused on different species or project classes o hanley et al 2013 or from piecemeal planning of projects leading to missed opportunities for aligning barrier removals neeson et al 2015 furthermore if species dispersal patterns life history strategies and habitats are not considered while planning a barrier removal the benefits can be limited to only a few species most previous research has focused on the use of optimization models kemp and o hanley 2010 mckay et al 2017 or spatial graph models mckay et al 2013 branco et al 2014 to identify a set of barrier removal projects that would result in the greatest benefit for stream fishes these optimal plans often rely on all proposed removals being implemented simultaneously or in the near future kemp and o hanley 2010 neeson et al 2015 and rarely consider the complex social and political factors that determine the feasibility of a barrier removal project grossman 2002 fox et al 2016 in reality social and political factors often limit conservation practitioners ability to implement a prescribed portfolio of barrier removal projects magilligan et al 2017 in these cases conservation organizations would benefit from a general barrier removal strategy that they could follow to maximize roi in the long term while responding to immediate opportunities to remove particular barriers at low socio political cost i e a policy of strategic opportunism isenberg 1987 here we aim to develop general guidelines for conservation practitioners to follow when prioritizing barrier removal projects specifically we calculate the roi for three common strategies for allocating conservation funds for barrier removals towards the removal of a few large dams the removal of a larger number of road culverts or a mixed strategy consisting of both dam and road culvert removals our aim was to draw general conclusions that were not specific to any one river network or a single fish species accordingly we created an individual based model ibm of two fish populations one of stream resident fish and the other of anadromous fish in a generalized representation of a fragmented river network the ibm approach allows us to examine variability in restoration efficiency resulting from spatial alignment of barrier removals as well as variability created by stochasticity in the spatial dynamics of the fish populations themselves focusing on this combined variability we investigate the best case worst case and average outcomes in terms of population distribution for stream resident and anadromous fishes under these three conservation strategies 2 methods we created an ibm to simulate movement patterns of stream resident and anadromous fishes through a fragmented river network the model consists of three components a river network a fish population and a set of barriers that block fish movements the cost of removing a barrier was based on stream order and derived from a database of more than 100 000 barriers in the north american great lakes neeson et al 2015 thus our barrier removal scenarios reflect the true range of project choices available to practitioners working in a large freshwater ecosystem we modeled two general life history strategies of stream fishes stream resident and anadromous to account for the impact of barrier removals on fishes with either type of migratory strategy we simulated each fish type separately which enabled us to describe the response of each type of fish to the barrier removal strategies 2 1 river network submodel the river network for all model runs is a symmetric dendritic river network with fifteen reaches fig 1 a in defining our river network we took a patch based graph approach eros et al 2012 in which each reach within the river network fig 1a is condensed into a patch or node and links or edges represent the possibility for movement between reaches fig 1b thus the river network overall is a graph g n l with nodes n indexed by n and links l indexed by l we define a reach as the section of river between two confluences and assume that each reach in the network provides an equivalent amount of fish habitat fig 1b each reach is directly connected to a maximum of three other reaches one downstream and two upstream in our model we assume that a barrier if present completely blocks both upstream and downstream movement of fishes between reaches and that barrier removal restores full movement between reaches fig 1c following perkin et al 2013 barriers are placed directly between reaches we refer to each barrier according to the strahler order of the upstream reach such that a barrier between a first order and a second order reach is a first order barrier fig 1c 2 2 fish submodel we hypothesized that the way in which individual fish interact with the complex shape of a fragmented river network would play a key role in structuring fish distributions neeson et al 2011 2012 accordingly we chose an ibm approach because it allowed us to capture these individual interactions we simulated fish populations based on two common life history strategies stream resident and anadromous allowing us to characterize the benefits of barrier removal projects for a diverse community of fishes though movement rates vary considerably among species and individuals mcintyre et al 2016 our intent was to focus on long term impacts of barrier removals on equilibrium distributions of stream fishes which will be insensitive to the speed at which individuals colonize recently connected habitat accordingly our model uses a weekly time step which is the finest temporal resolution that captures movement rates of an average migratory fish species okland et al 2001 at the coarse spatial resolution of our modeled river network fig 1 fast migrating species e g salmon would likely reach an equilibrium distribution across the river network more quickly than species with limited movement e g mottled sculpin at every time step t each individual fish must choose whether to move to a directly connected reach j or to stay within its current reach i the probability of transitioning from i to j is given by pi j in our model stream resident fishes have an equal chance of moving to any neighboring upstream or downstream reach that is accessible i e no barrier is present or remaining in the reach they currently occupy for example the transition probabilities at any time step for a stream resident fish in reach 4 fig 1 are p 4 4 p 4 2 p 4 8 p 4 9 0 25 the rules that govern anadromous fish are similar with the exception that they perform spawning runs and all movement is either upstream prior to spawning or downstream post spawning for an upstream migrating fish in reach 4 for example the transition probabilities at any time step are p 4 2 0p 4 8 p 4 9 p 4 4 0 33 and p 4 2 0 spawning occurs in the reach a fish is located in at the end of upstream migration three time steps or when no further upstream movement is possible i e when an individual fish reaches a first order reach or further upstream movement is blocked by a barrier after all spawning is complete all fish migrate downstream at the same time anadromous fish always begin at the mouth of the river network and return there during downstream migrations in the case of stream resident fish the entire population begins each run in a single reach this represents a maximally isolated stream resident population constrained to a fraction of their historical habitat we hypothesized that the benefits of a set of barrier removals for stream resident fish would depend on both the location of barrier removals and the location of the isolated population within the river network accordingly we performed a series of model runs with the entire stream resident population beginning in one of the fifteen reaches and initially no individual fish in any of the other fourteen reaches in describing the effects of barrier removals on fishes we focused on the evenness of the distribution of individuals across the entire river network for the purposes of this study we assumed that all reaches within the network had equal habitat and available resources so that when no barriers were present stream resident fishes would follow an ideal free distribution fretwell and lucas 1969 and distribute themselves equally throughout the river network while habitat and resources are likely to vary across reaches in real river networks their distribution is unlikely to vary systematically and will be specific to the targeted tributary in fragmented river networks fish distributions are often highly skewed because barriers inhibit dispersal of individuals towards an ideal free distribution perkin et al 2015 this aggregation increases density dependent effects and at the population level constitutes an underutilization of the total amount of suitable habitat available in the river network to quantify the degree of aggregation of the fish population in each model run we calculated the standard deviation s of fish abundance among reaches either after the final time step for stream resident fish or after the last step of the final upstream migration for anadromous fish as s i 1 n f i f 2 n 1 where f i is the number of fish in reach i and n is the number of nodes i e reaches in the river network a high standard deviation is indicative of a tightly clustered population with access to only one or a few reaches while a standard deviation of zero represents a perfectly even distribution of fish among all reaches in the river network 2 4 barrier removal scenarios for both dams and road culverts the size of a barrier and cost of removing it generally increases with stream size neeson et al 2015 to determine in more detail how the cost of a barrier removal depends on stream size we analyzed a database of estimated removal costs for 3954 dams and 99 940 road stream crossings in tributaries of the north american great lakes neeson et al 2015 barriers on small streams strahler order 1 or 2 were primarily road stream crossings 97 and had an average removal cost of us 125 073 n 86 541 including the cost of material and labor for removing the road culvert and replacing it with a fully passable bridge we did not consider other approaches to restoring fish passage such as fishways since these techniques typically target strong swimmers e g salmonids and may not benefit other fish species with weaker swimming abilities mallen cooper and brand 2007 bunt et al 2012 barrier removal costs averaged us 197 236 n 15 765 on medium streams strahler order 3 or 4 and us 320 110 n 310 on large streams strahler order 5 in this data set all barriers on large streams were dams all road stream crossings on high order streams in this dataset were bridges and fully passable to aquatic organisms and thus not candidates for removal using these average prices as a starting point we modeled the removal costs of small medium and large barriers as us 100k us 200k and us 300k respectively this modest deviation from the true project costs enabled us to compare barrier removal scenarios involving an integer number of barriers for example a budget of us 300k might be spent on one large three small or one medium and one small project furthermore given that 97 of barriers on small streams were road culverts we henceforth refer to all small barriers as road culverts similarly we refer to all barriers on large streams as large dams given that road culverts do not occur on any large streams in the great lakes using these estimates of barrier characteristics and costs we compared three barrier removal strategies commonly used by conservation practitioners in each case we fully allocate a total budget of us 600k the first strategy entails removing both of the large dams near the mouth of the river network fig 2 a the second is a mixed strategy entailing the removal of one large dam one road culvert and one medium barrier fig 2b the final strategy involves the removal of six road culverts fig 2c as a null model we also completed an additional set of runs on a free flowing river network without barriers fig 1b note that multiple permutations of the second n 64 and third strategies n 28 are possible and the benefits to stream fishes may differ dramatically among the permutations to understand the range of outcomes possible under each strategy we modeled all possible permutations of each strategy throughout the network fig 3 shows three examples of different configurations of the mixed removal strategy fig 2b in which all fig 3a two fig 3b or none fig 3c of the barrier removals are coordinated 2 5 computational details we developed and ran the ibm in java using the mason library for agent based simulation luke et al 2005 we simulated a population of 100 000 fish for each simulation run we ran each simulation for 600 time steps which was sufficiently long enough to allow the stream resident population to reach an equilibrium for their distribution which generally occurred after 100 time steps to capture the variability that resulted from the stochastic movement of the individual fish we performed 100 simulations of each permutation of each barrier removal strategy there was a single permutation of the dam removal strategy 64 of the mixed removal strategy and 28 of the road culvert removal strategy since we simulated the stream resident population beginning entirely in a single reach we performed 15 runs per strategy permutation with the stream resident population beginning in a different reach each time this resulted in a total of 141 000 and 9 400 simulation runs for stream resident and anadromous fishes respectively 3 results we found clear differences in the average roi of the three conservation strategies and between the best case and worst case outcomes for each strategy considering the average outcomes no single strategy was best for both stream resident and anadromous fishes on average the best strategy for stream resident fishes was the removal of six road culverts fig 4 a however anadromous fishes never benefitted from this strategy because both large dams remained in place blocking any upstream migrations fig 4b the best strategy on average for anadromous fishes was the removal of two large dams fig 4b but this strategy had low average benefit for stream resident fishes fig 4a in contrast to these average outcomes the best possible outcome for both stream resident and anadromous fishes occurred under the mixed strategy i e the removal of one small one medium and one large barrier fig 4a b this single strategy when optimally executed provided the maximum possible benefit to stream resident and anadromous fishes simultaneously worst case outcomes for stream resident fishes occurred under all three strategies highlighting the broad range of outcomes possible under each strategy while the mixed removal strategy can lead to the best possible outcomes for both stream resident and anadromous fishes these best case outcomes only occur under certain rare spatial arrangements of barrier removal projects fig 5 when all three barrier removals of the mixed removal strategy were aligned longitudinally in the river network as in fig 3a both stream resident and anadromous fishes experienced the overall greatest benefit however the longitudinal alignment of all three barrier removals projects occurred in only a small subset of the spatial permutations of this strategy peak a in fig 5a b the alignment of only two or no barrier removal projects peaks b and c in fig 5a b respectively occurred in a greater number of permutations and yet other permutations included cases where barrier removals reconnected reaches containing no stream resident fishes peak d in fig 5a thus while anadromous fishes always received at least some benefit from the mixed removal strategy stream resident fishes only benefited when the removals were adjacent to the stream reach in which the population resided peak d in fig 5a 4 discussion our results show clear differences in the roi from three common conservation strategies removing a few large dams many road culverts or a mix of both fig 4 the greatest benefit for stream resident and anadromous fishes occurred under the mixed removal strategy but only when the removals were aligned longitudinally within the river network fig 5 when barrier removals were not aligned which is a common outcome of piecemeal or individualistic planning magilligan et al 2016 benefit decreased for both stream resident and anadromous fishes to the point where a mixed removal strategy could be less effective than the removal of just dams or just road culverts when the conservation strategies targeted a single class of barriers i e the removal of two large dams or of six road culverts only fish of a single life history strategy benefited on average the large dam removal strategy was the most beneficial for anadromous fishes but the least beneficial for stream resident species in contrast removing road culverts was on average the best case scenario for stream resident fishes but under this scenario anadromous fishes never benefited though a mixed removal strategy can provide the greatest return on investment effective implementation of this strategy requires the coordinated efforts of practitioners from many different organizations as a result it is unlikely to occur under a piecemeal approach to project selection branco et al 2014 the mixed removal strategy also provides an opportunity to leverage the expertise of each type of organization local organizations often have a good understanding of where fish populations have historically occurred and of the level of support from the local community for a particular project as a result local conservation organizations are often ideally positioned for detailed evaluation of the ecological societal and economic costs and benefits of a particular barrier removal grossman 2002 fox et al 2016 conversely large scale planning initiatives typically overseen by national organizations may be naive to local social and political issues which can result in removals being delayed or stopped completely jorgensen and renofalt 2012 fox et al 2016 at the same time the regional perspective of federal agencies and national ngos can enable these groups to identify high priority watersheds and focal regions for investment and to coordinate investments across jurisdictional boundaries milt et al 2017 our aim was to identify barrier removal strategies that would be broadly applicable across species and river networks the long run equilibrium distributions of fishes across river networks figs 4 and 5 demonstrated in our model will be reached more quickly by fast migrating species e g salmon while slow migrating species e g sculpin may take many years to reach an equilibrium density across a reconnected river network furthermore our model contains no density dependence thus our results could be readily scaled to river systems involving much smaller or greater populations of migratory fishes while our model enabled us to characterize the relative benefits of different removal strategies it does not account for spatial variation in environmental factors such as habitat type and condition sociopolitical factors like ownership and barrier degradation or variation in barrier impacts including passability similarly our model does not account for the localized effects of some barriers on river flow temperature and sediments stanley and doyle 2003 that collectively transform river networks into highly novel ecosystems radeloff et al 2015 consideration of these factors is known to be critical for evaluating the potential benefit and feasibility of particular barrier removals poff and hart 2002 zheng and hobbs 2013 januchowski hartley et al 2014 fox et al 2016 however these factors are unlikely to vary in a systematic way across different river networks as a result inclusion of these additional factors into our model is unlikely to alter our general findings regarding the most cost effective strategy these site specific considerations will necessarily vary among barriers and among tributaries but conservation organizations should nevertheless be able to follow the general principle of the mixed removal strategy i e the removal of different sized barriers that are longitudinally aligned while evaluating site specific factors to achieve the greatest roi our results complement existing approaches for prioritizing barrier removals by providing conservation practitioners with general guidelines for project selection for example both federal agencies and local ngos commonly use optimization models to identify high priority barrier removal projects zheng et al 2009 o hanley et al 2013 neeson et al 2015 though optimization models can identify a mathematically optimal set of projects implementing these conservation plans at smaller scales is often complicated by local politics magilligan et al 2016 particularly with dams ownership can be ambiguous and the dam itself may have historical significance for the surrounding community fox et al 2016 neither are accounted for by optimization models and can cause opposition to the removal among the local community though optimization models are increasingly used to identify a set of high priority projects conservation practitioners in practice often implement an opportunistic strategy in which organizations target barriers that already need to be replaced or have cooperating owners magilligan et al 2016 as a result the set of projects completed deviates from the one prescribed by an optimization model our study complements both barrier removal optimization studies and opportunistic approaches by providing conservation practitioners with a general strategy to follow to maximize conservation outcomes when project selection must depart from the recommendations of an optimization model conservation practitioners should seek a balance of large and small projects and ensure that they are spatially aligned moreover decision makers could follow this general strategy while using one of the increasing number of online decision support tools moody et al 2017 to balance mathematically optimal project selection with a need to avoid known areas of socio political contention acknowledgements we thank p doran p mcintyre n sleight the lcluc research group at ou and two anonymous reviewers for thoughtful feedback on the ideas herein funding was provided by the upper midwest and great lakes lcc 
25357,vegetation gross primary productivity gpp is an important component in the global carbon cycle and its accurate estimation is essential in ecosystem monitoring and simulation previous studies show that ecosystem models usually overestimate gpp under drought and during spring late fall and winter in this study these issues are addressed in the daily boreal ecosystem productivity simulator bepsd by introducing a new water stress factor f w to replace the old one and a designed fraction in term of the normalised difference vegetation index ndvi f ndvi to indicate the effect of chlorophyll on photosynthesis gpp simulations are conducted at 41 flux sites across europe to test bepsd with the new f w and f ndvi the new f w captures drought conditions well and f ndvi expresses the chlorophyll constraint on photosynthesis although bepsd with the old f w performs well for some plant function types pfts it is unsatisfactory for others bepsd incorporating both the new f w and f ndvi gives better simulations than the old version particularly for evergreen broadleaf forest deciduous broadleaf forest and closed shrub with r rmse value increasing decreasing from 0 69 3 20gcm 2 d 1 to 0 74 1 65gcm 2 d 1 0 72 4 01gcm 2 d 1 to 0 82 2 91gcm 2 d 1 0 54 1 82gcm 2 d 1 to 0 75 1 59gcm 2 d 1 respectively furthermore the new f w effectively mitigates gpp overestimates under drought and f ndvi counteracts gpp overestimates during spring late fall and winter overall the improved bepsd shows a satisfactory performance at flux sites over europe keywords gross primary productivity gpp daily boreal ecosystem productivity simulator bepsd water stress factor chlorophyll constraint ndvi europe 1 introduction gross primary productivity gpp is defined as the gross carbon fixed by terrestrial ecosystems through photosynthesis per unit time and area beer et al 2010 wu et al 2010b wu et al 2014 gpp is an important component in the terrestrial carbon cycle beer et al 2010 wang et al 2010 yuan et al 2014 li et al 2016 through which carbon dioxide from the atmosphere is fixed into vegetation it is also one of the major fluxes controlling the land atmosphere carbon exchanges raupach et al 2008 li et al 2016 an accurate estimate of gpp is particularly essential for quantifying other parameters in the carbon cycle such as net primary productivity npp and net ecosystem production nee wu et al 2014 as reported numerous ecosystem models are developed to simulate terrestrial gpp such as light use efficiency lue models and process based models li et al 2016 in lue models gpp is estimated through the monteith 1972 equation gpp lue f apar par where lue is the light use efficiency during a period and f apar represents the fraction of absorbed photosynthetically active radiation par in this type of model lue is the key parameter wu et al 2010d modified lue f apar in the monteith equation to vi vi and used the modified lue model to obtain an improved estimate of the gpp of wheat at the national experimental station for precision agriculture 40 10 6 n 116 26 3 e 20kilometres northeast of beijing china the modified lue model was also succussful for maize wu et al 2010b gpp estimation in the vegetation photosynthesis model vpm is also based on lue and lue in vpm is estimated as a function of temperature soil moisture and or vapor pressure deficit vpd xiao et al 2004 lue based models have been embraced for estimating spatial and temporal gpp dynamics on a large spatial scale wu et al 2010a as they are simple and easy to use although they lack strong theoretical basis and sufficient understanding of ecosystem function feng et al 2007 alternatively process based models are based on plant ecological mechanisms liu et al 1997 process based models try to simulate the sophisticated interaction processes between vegetation and atmosphere during plant growth such as photosynthesis respiration and evapotranspiration feng et al 2007 process based models include the boreal ecosystem productivity simulator beps liu et al 1997 and dynamic land ecosystem model dlem tian et al 2010 both beps and dlem can simulate the carbon cycle and the water cycle and their photosynthetic assimilations are based on the farquhar model liu et al 1999 tian et al 2010 as a big model dlem can also simulate the nitrogen cycle and it includes an agriculture module and a city module tian et al 2010 in contrast beps is simple and hence easier to use considering the advantages of process based models and that we only focused on gpp simulation in this study we selected daily beps bepsd this model was developed from the forest biogeochemical cycles forest bgc model liu et al 1997 and has been widely used in gpp npp and evapotranspiration et simulations liu et al 1999 2002 liu et al 2003 zhang et al 2012a there are also researches focusing on parameter optimisation in beps chen et al 2012 he et al 2014 chen et al 2012 studied the effect of the clumping index ω on gpp simulation he et al 2014 used the ensemble kalman filter method to optimise two key parameters the water stress factor f w and the maximum photosynthetic carboxylation rate at 25 c v m 25 in beps and demonstrated their seasonal variations f w and v m 25 are two of the most important parameters in ecosystem models related to carbon uptake by vegetation he et al 2014 the soil water stress factor is included in most ecosystem models e g w ε in carnegie ames stanford approach casa model potter et al 1993 f lwp in bepsd liu et al 1997 f w in half hourly or hourly beps bepsh ju et al 2006 chen et al 2012 and w scalar in vpm xiao et al 2004 these factors are calculated in different ways in lue based models e g casa and vpm the water stress factor is used to scale the maximum lue ε max potter et al 1993 xiao et al 2004 it is computed using evapotranspiration information in casa potter et al 1993 and other factors in vpm xiao et al 2004 in process based models such as bepsd and bepsh the water stress factor parameterised using the soil water content swc liu et al 1997 ju et al 2006 is included in the jarvis stomatal conductance g s model jarvis 1976 liu et al 1997 liu et al 1999 and in the ball woodrow berry bwb type equations ju et al 2006 swc used to calculate f w is computed using soil water balance modules with a single soil layer liu et al 1997 or multiple layers ju et al 2006 it is demonstrated that f w derived from a multi layer model is more effective than that from a single layer model results averaged from five models including bepsd showed that the gpp of evergreen mediterranean oak woodlands was overestimated under drought vargas et al 2013 probably because the stomatal conductance was overestimated under drought xu and baldocchi 2003 vargas et al 2013 the accurate estimation of f w is critical in ecosystem models and is essential for studying carbon and water cycles v m calculated from v m 25 has a significant impact on both vegetation photosynthesis and evapotranspiration in process based ecosystem models he et al 2014 houborg et al 2015 employed a semi mechanistic relationship between chlorophyll and v m 25 based on previous works sage et al 1987 evans 1989 friend 1995 houborg et al 2013 to study the constraint of leaf chlorophyll on gpp simulation in agricultural systems using chlorophyll as a constraint factor in an ecosystem model can improve the accuracy of gpp simulations croft et al 2017 showed that the leaf chlorophyll content lcc and v m 25 are highly correlated with an r2 value of 0 78 for deciduous forest samples including trembling aspen bigtooth aspen red maple and ash the intercomparison from 26 models suggested that gpp is overestimated during winter spring and fall schaefer et al 2012 when the lcc is low however few of the widely used ecosystem models include a chlorophyll constraint factor it is likely that the overestimation of gpp during these periods when chlorophyll stays at a low level early spring late fall and winter may result from the lack or insufficient consideration of chlorophyll constraint on modelling photosynthesis however a temporally and spatially continuous chlorophyll content record cannot be obtained easily previous studies demonstrated a strong relationship between chlorophyll and the vegetation index vi wu et al 2010c showed a close correlation between vegetation indices vis and canopy chlorophyll content ccc dian 2011 also showed a strong relationship between vis and both lcc and ccc thus using a vi e g the normalised difference vegetation index ndvi as the chlorophyll indicator is a good solution to the problem of attaining continuous chlorophyll however few of the widely used ecosystem models include a chlorophyll constraint factor thus in the present study we experiment by introducing a chlorophyll constraint factor into the bepsd ecosystem model the objectives of this paper are 1 to evaluate the performance of the original bepsd over europe 2 to improve the accuracy of gpp simulation using bepsd by incorporating a new f w in the g s calculation formula 3 to use ndvi as an indicator of chlorophyll to quantify the effect of chlorophyll on v m by introducing a designed chlorophyll constraint factor f ndvi into bepsd and 4 to validate and compare the improved versions of bepsd across europe 2 method and data 2 1 overview of beps beps was initially developed at the canada centre for remote sensing to assist in natural resources management liu et al 1997 especially for the boreal forest later most of its applications were in north america liu et al 1999 2002 liu et al 2003 liu et al 2005 ju et al 2006 mo et al 2008 govind et al 2011 zhang et al 2012a gonsamo et al 2013 he et al 2014 sprintsin et al 2015 and also expanded for global applications zheng et al 2015 it has been developed at the daily liu et al 1997 liu et al 1999 the half hourly ju et al 2006 and the hourly step chen et al 2012 in beps gpp is simulated by scaling farquhar s leaf level biochemical model up to the canopy level using a two leaf approach he et al 2014 in bepsd g s is calculated using the jarvis model and v m is parameterised by temperature and nitrogen content some key equations pertaining to the carbon cycle in bepsd are described as follows daily gross primary productivity daily gross primary productivity is calculated in bepsd as 1 a canopy a sun l a i sun a shade l a i shade 2 g p p a canopy d a y l e n g t h f a c t o r gpp where a canopy is the assimilation rate of canopy in μmolm 2 s 1 a sun and a shade are the assimilation rates of sunlit and shaded leaves respectively lai sun and lai shade are the leaf area index lai values of the sunlit and shaded leaves respectively gpp is the gross primary productivity in gcm 2 d 1 daylength is the day of length in second factor gpp converts gpp unit into gcm 2 day 1 assimilation rate a sun and a shade can be solved from the farquhar model and the stomatal conductance model 3 a min w c w j r d 4 w c v m c i γ c i k 5 w j j c i γ 4 5 c i 10 5 γ 6 r d 0 015 v m where a is the net photosynthesis rate in μmolm 2 s 1 w c is the rubisco limited gross photosynthesis rate in μmolm 2 s 1 w j is the light limited gross photosynthesis rate in μmolm 2 s 1 r d is the daytime leaf dark respiration in μmolm 2 s 1 c i is the intercellular co2 concentration in pa г is the co2 compensation point in the absence of dark respiration in pa k is a function of enzyme kinetics in pa j is the electron transport rate in μmolm 2 s 1 and v m is the maximum carboxylation rate in μmolm 2 s 1 components of k and γ are calculated as 7 k k c 1 o 2 k o 8 γ 4 04 1 75 t a 25 10 where k c and k o are the michaelis meton constants for co2 and o2 respectively details in liu et al 1999 o 2 is the intercellular o2 concentration in pa 21 000 and t a is air temperature in c components of v m and j are calculated as 9 v m v m 25 2 4 t a 25 10 f t a f n 10 j j max ppfd ppfd 2 1 j max 11 j max 29 1 1 64 v m where v m 25 is the maximum carboxylation rate at 25 c in μmolm 2 s 1 the functions of t a and n can be found in liu et al 1999 j max is the light saturated rate of electron transport in μmolm 2 s 1 ppfd is the photosynthetically active flux density in μmolm 2 s 1 because c i is not easy to obtain on a regional or global scale a cannot be calculated with the abovementioned formulas according to fluid physics a can also be described as 12 a c a c i g 13 g 10 6 g s r gas t a 273 14 g s g smax f p p f d f t a f v p d f l w p 15 f l w p l w p close l w p l w p close l w p open 16 l w p 0 2 s o i l water s o i l cap where c a is the co2 concentration in the atmosphere in pa g is the total conductance to co2 from cell to air in μmolm 2 s 1 pa 1 g s is the stomatal conductance to co2 in ms 1 and g smax is the maximum stomatal conductance to co2 in ms 1 lwp is the leaf water potential here for the evergreen needle leaf forest and the deciduous needle leaf forest lwp close 4mpa for other plant functional types pfts lwp close 8mpa for all pfts lwp open 1mpa if the soil is very dry f lwp 0 001 functions of ppfd ta and vpd are given by chen et al 1999 a is obtained from eqs 3 and 12 17 a 1 2 a 1 2 g c 1 2 a g 2 b g c where a is the minimum of a c and a j corresponding to w c and w j if a a c then a k c a 2 b 2 2γ k c a v m 2 c a k r d and c v m r d 2 if a a j then a 2 3γ c a 2 b 0 4 4 3γ c a j 2 c a 2 3γ r d and c 0 2j r d 2 then a is derived after analytical integration over a day considering the nonlinear effects u 1 0 5 π 2 0 π 2 cos θ d θ 4 π 1 27 of meteorological variables on photosynthesis due to their diurnal variability shown as 18 a 1 27 2 g n a 1 2 2 g n 2 c 1 2 g n 2 a g n b 4 a d b 4 a c 1 2 b 2 4 a c 8 a 3 2 ln 2 a g n b 2 a 1 2 d b 2 a 1 2 c 1 2 where gn is the conductance at noon d a g n 2 b g n c 1 2 sunlit and shaded lai as a two leaf model chen et al 2012 bepsd calculates gpp for sunlit leaves and shaded leaves separately lai of sunlit and shaded leaves is calculated as 19 l a i sun 2 cos θ 1 exp 0 5 ω l a i cos θ 20 l a i shade l a i l a i sun 21 θ 1 2 1 2 π 2 θ noon θ noon π 8 3 4 θ noon 22 θ noon π 180 l a t 23 5 sin j u l i a n d a y 81 2 π 365 where θ is the solar zenith angle θ noon is the solar zenith angle at noon lat is the latitude the parameter values used in this study are listed in table 1 2 2 incorporating the new water stress factor into the gs simulation in bepsd g s was calculated using the jarvis model jarvis 1976 liu et al 1999 which included constraints of radiation temperature vpd and lwp a soil water stress factor f lwp as a function of lwp calculated from the swc and the soil water capacity eqs 15 16 was used to scale g smax in bepsd eq 14 in bepsh the jarvis model was replaced by a modified version of the bwb model ball et al 1987 ju et al 2006 both the jarvis and the modified bwb models considered f w however the soil profile was split into multiple layers in the modified bwb model whereas it was only one layer in the jarvis model when simulating f w liu et al 1997 ju et al 2006 in original bepsd f w was calculated from a single soil layer water balance module which did not include the root distribution information the root information and the soil temperature were considered in the modified bwb model in bepsh which performed well ju et al 2006 proving considering multiple layers soil water and root information together produces more accurate f w estimates following the modification of bepsh coupling a model based on multiple soil layers into bepsd instead of the single layer model could give more accurate f w estimation in the g s simulation a penman monteith based model incorporating remote sensing data and a multi layer water balance module rs wbpm was proposed by bai et al 2017 which was parameterised with precipitation vertical root distribution vrd and satellite retrieved vegetation information to estimate the water stress in the mediterranean climate area more accurately in fact it showed satisfactory performance under drought regions bai et al 2017 in rs wbpm the soil profile was divided into multiple soil layers f w was calculated for a bulk surface in rs wbpm using vrd and vis bai et al 2017 f w was the weighted sum of simulated f w of each soil layer and the weight of each layer was determined by the vrd information of that layer and the wettest layer bai et al 2017 the model was validated under mediterranean climate where vegetation suffered from severe drought and it was successful bai et al 2017 in the present study we include a water stress factor eqs 23 26 for the dominating surface species 23 f w l 1 3 s w s f l w l 24 s w s f s w c l θ w l θ m l θ w l 25 w l y l w s t 0 5 1 w l w s t 1 y l w s t y l l l w s t l l w s t 26 y l 1 β d l β d l 1 β d l l 1 l 1 where superscript l denotes a soil layer index l wst denotes the index of the wettest soil layer w swsf and θ w represent the weight the soil water stress factor and the wilting point of the soil layer swc l is the soil water content in soil layer l in mm and it represents the soil water balance in this layer l θ m represents the swc when stomatal conductance becomes maximum y denotes the proportion of root biomass within a soil layer 0 1 d l is the depth of the lower boundary of soil layer l cm and β is the root extinction coefficient we refer to bai et al 2017 and jackson et al 1997 for the value of β of a specified biome 2 3 introducing fndvi into the vm simulation the chlorophyll content is not constrained in the v m estimate in bepsd eq 9 croft et al 2017 proved a close relationship between v m and chlorophyll as previous studies have shown he et al 2014 croft et al 2017 v m 25 has a seasonally variation presenting lower values during early spring and fall than the maximum over the whole growing season thus v m without the chlorophyll constraint in bepsd may be overestimated during early spring and late fall which is probably why gpp is overestimated during these periods to address this issue we try to introduce a factor to reflect the chlorophyll content constrain on v m although the spatially continuous parameterisation of v m on a global scale remains unsolved croft et al 2017 close relationships between vis and chlorophyll are found including relationships between leaf scale vis and lcc leaf scale vis and ccc satellite derived vis and lcc and satellite derived vis and ccc wu et al 2010c dian 2011 dian et al 2016 these provide a solution to the abovementioned problem by computing a stress factor using a vegetation index as the chlorophyll indicator dian 2011 reported that ccc is an exponential function of leaf scale ndvi with r2 of 0 60 and rmse of 0 40g m2 on the canopy level as per wu et al 2010c there is also a strong exponential relationship between ccc and satellite ndvi retrieved from tm and hyperion with r2 0 73 besides the strong relationship between ndvi both leaf scale ndvi and satellite derived ndvi and chlorophyll content there is also a linear relationship between lai and satellite derived ndvi with coarse spatial resolution and high temporal resolution e g modis ndvi wang et al 2005 sun et al 2006 based on these relationships it is deduced that lai ndvi and chlorophyll are positively correlated during the growing season the similar temporal variations of lai and leaf chlorophyll during the growing season shown by houborg et al 2013 is partly consistent with our hypothesis thus it is a reliable and feasible solution to use ndvi instead of chlorophyll to represent the effect of chlorophyll on v m both on the leaf and canopy scales ndvi a classic two band vegetation index among the vegetation indices used in previous works wu et al 2008 dian 2011 dian et al 2016 is chosen to compute the constrain of chlorophyll on v m then eq 9 is modified to eq 27 to eliminate the effect of various ndvi ranges in different pfts the introduced f ndvi is normalised as 27 v m v m 25 2 4 t a 25 10 f t a f n f n d v i 28 f n d v i 0 01 n d v i n d v i min n d v i n d v i min n d v i max n d v i min 2 n d v i min n d v i n d v i max 1 n d v i n d v i max where f ndvi denotes the ndvi constrain factor f ndvi ndvi represents the time series ndvi extracted from remote sensing images according to the location of the flux sites ndvi min is the minimum ndvi value of each pft and is attained using the trial and error method 0 0 0 5 step 0 1 and ndvi max is the 95 percentile of the ndvi time series at each site for each year in this equation ndvi min represents the boundary value of ndvi when photosynthesis is completely limited by chlorophyll for further understanding of ndvi min please see section 4 4 2 4 modelling cases in order to investigate the impacts of the new f w and the introduced f ndvi on bepsd we conduct model simulations using the following four cases table 2 the results of four cases are denoted as gpp1 gpp2 gpp3 and gpp4 by comparing gpp1 with gpp2 we demonstrate the advantage of considering root vertical information and multiple layers swc in f w simulation under drought by comparing gpp1 with gpp3 we investigate the improvements and show the importance of including a chlorophyll constraint on v m when simulating vegetation photosynthesis by comparing the results of the four cases we can determine the best simulation scenario 2 5 data 2 5 1 vegetation indices data vegetation indices including ndvi and lai are employed in this study mod15a2 8days 1km and myd15a2 8days 1km datasets are used to generate lai with a time resolution of 4days and a spatial resolution of 1km we used ndvi with 8days and 250m resolution from mod13q1 16days 250m and myd13q1 16days 250m datasets to calculate the introduced f ndvi the mod and myd datasets used are retrieved from the modis subset data set available on ornl daac http daac ornl gov they are linearly interpolated into the daily step after quality control the quality of ndvi can be examined in a raster layer named pixel reliability we only use data whose pixel reliability value are 1 and 0 ndvi values less than 0 1 are set to 0 1 for lai a raster layer named fparlai qc in bit string format contains the quality of lai data and only the bit string with zero values at both bit no 0 and bit no 2 are used the detailed meanings of pixel reliability in pixel reliability raster layer and bit string in fparlai qc raster layer are available at https lpdaac usgs gov dataset discovery modis modis products table mod13q1 and https lpdaac usgs gov dataset discovery modis modis products table mod15a2 respectively 2 5 2 flux sites data forty one flux sites including 11 enf sites 2 ebf sites 8 dbf sites 2 mf sites 2 csh sites 2 osh sites 9 gra sites and 5 cro sites are used in this study flux data with low quality are removed a total of 339 site years are available the site locations are shown in fig 1 and general information is listed in table 3 among the meteorological inputs of bepsd daily maximum and minimum temperature precipitation and radiation are retrieved from the half hourly temperature observed at the flux sites and relative humidity is calculated from the flux measured vpd missing data of tower observations are filled with the corresponding era interim data as introduced at http fluxnet fluxdata org data fluxnet2015 dataset fullset data product the modelled daily gpp are validated with the flux gpp at the 41 flux sites table 4 retrieved from the fluxnet2015 dataset which is available on http fluxnet fluxdata org data fluxnet2015 dataset the available flux gpp gpp dt vut ref noted as gpp0 is employed in this study it is estimated using the daytime partitioning method lasslop et al 2010 and the variable ustar threshold vut method http fluxnet fluxdata org data fluxnet2015 dataset fullset data product it also has a data quality control qc field with values ranging from 0 to 1 indicating the percentage of good quality half hourly data only data with qc value higher than 0 5 are used to validate our modelled results in this study swc data including the soil water content of the shallowest layer at each flux site are also used to indicate drought in this study the measurement depths at 10 sites are collected and listed in table 4 2 5 3 soil property data the available water capacity awc data are retrieved from the dataset global gridded surfaces of selected soil characteristics of the international geosphere biosphere programme data and information system igbp dis this dataset is available online https daac ornl gov the igbp gridded soil dataset has a spatial resolution of 5 5 arc minutes all measurements in the igbp dis dataset are taken within the depth interval of 0 100cm awc data at each flux site are extracted from this dataset according to their locations 3 results 3 1 ndvimin of each pft the correlation coefficient r and root mean square error rmse values for gpp3 versus gpp0 with different ndvi min in bepsd are shown in fig 2 with high r and low rmse results ndvi min of dbf csh osh and cro are set to 0 5 0 2 0 2 and 0 4 directly the r rmse value of enf and gra remain almost constant with ndvi min increasing from 0 0 to 0 5 thus here ndvi min of enf and gra are set to 0 1 ndvi of 0 0 makes no sense for vegetation the r of ebf decreases with decreasing rmse and the largest decrease rate of rmse occurs when ndvi min varies from 0 2 to 0 3 then ndvi min of ebf is set to 0 3 for mf when ndvi min increases from 0 0 to 0 5 there are no significant variations of r and rmse herein ndvi min of mf is set to 0 2 thus the recommended ndvi min values of each pft are attained table 5 3 2 simulated results for each site the regression statistics number n slope a intercept b r and rmse for each site are summarised in table a1 appendix a the time series of gpp0 and the estimated gpps gpp1 4 of each site are shown in fig a1 c of appendix a along with fig a1 a showing the new f w and swc in the shallowest layer and fig a1 b presenting the original v m and v m parameterised with f ndvi v m ndvi the scatterplots of simulated gpps vs gpp0 for each site are available in appendix b the slope and interception values in table a1 show that the model obviously underestimated gpp at es lju and it noe and obviously overestimated gpp at de lkb and fi jok in the four examined cases the improved model cases 2 4 provides more accurate gpp estimates at es lju and it noe compared with the original model case 1 there are no obvious improvements at de lkb and fi jok in cases 2 4 table a1 shows that r increases obviously increases or decreases only very slightly at most sites excluding it ca1 and it ca3 where r presents an obvious decreasing trend this is discussed in section 4 1 the possible reasons for the unapparent improvements at the de lkb and fi jok sites are discussed in section 4 2 2 the original f w is nearly 1 at all 41 sites and is not drawn in fig a1 it can be seen that the new f w shows consistent trend with swc at most sites and indicates drought satisfactorily fig a1 a the new f w performs successfully particularly at sites located in the subtropical mediterranean climate zone wherein drought usually occurs during the warm summer days e g fr pue it ro1 and it ro2 ryu et al 2012 the outperformances of the new f w at those sites demonstrate its reliability to reflect drought much more convincingly thus it is feasible to use the new f w to replace the old one in bepsd the c maps in fig a1 show that gpp1 is significantly higher than gpp0 at some sites e g es lgs es lju fr pue it noe it ro1 it ro2 and it sro during their summer months mostly during late may to september these overestimates are mostly accompanied by apparent fall of their corresponding f w line fig a1 18 19 23 31 34 36 a which indicates a poor estimate for drought at each site in modelling gpp1 in contrast gpp2 agrees with gpp0 flux gpp much better for example at it noe both new f w and swc show sever water stress during july to september fig a1 31 a when gpp1 is overestimated but gpp2 agrees well with gpp0 compared with r rmse for gpp1 vs gpp0 that for gpp2 vs gpp0 increases decreases from 0 43 1 82gcm 2 d 1 to 0 78 1 23gcm 2 d 1 which demonstrates that the new f w effectively addresses the issue of gpp overestimation under drought conditions these improvements are also obvious at the sites listed above it proves that the new f w can capture droughts and performs better than the original f w fig a1 c also show the overestimates of gpp1 at some sites e g be lon de kli de rus fr gri it ro1 and it ro2 during early spring late fall and winter months such as from september to april or may of the subsequent year however gpp3 effectively eliminates these overestimates during these periods and is closer to gpp0 than gpp1 for example gpp1 at be lon it ro1 and it ro2 fig a1 4 34 35 c show overestimates during spring late fall and winter months resulting in the high rmse values of 4 06 5 97 and 6 04gcm 2 d 1 respectively table a1 gpp3 at the same sites outperforms gpp1 during winter and spring with the decreased rmse values of 3 44 4 49 and 5 06gcm 2 d 1 respectively moreover improvements can also be found at de kli during 2007 2008 2012 de rus spring in 2011 2012 2014 and late fall in 2013 es lgs late summer and fall in 2007 and 2009 es lju fall and winter fi jok early spring in 2001 and fr gri fall and winter fig a1 12 15 18 21 22 c these results show that bepsd with introduced f ndvi yields better results than the original version in addition from fig a1 12 15 18 21 22 b we observe that the periods when gpp is overestimated at these sites are almost consistent with the periods when v m is higher than v m ndvi it is demonstrated that using v m parameterised with introduced f ndvi in bepsd could mitigate the overestimates of gpp during early spring late fall and winter and f ndvi can be employed to quantitatively describe the influence of chlorophyll on v m overall gpp4 i e the estimated result of bepsd which is coupled with the new f w and the introduced f ndvi performs better than gpp2 and gpp3 it combines the advantages of gpp2 and gpp3 mitigating the overestimates under drought conditions and during early spring late fall and winter 3 3 evaluation and comparison of improved bepsd at the biome level we show the original and improved bepsd performances when validated with flux gpp in fig 3 the model performances vary significantly across biomes the original bepsd performs better for pfts of enf mf and gra with r values of 0 85 0 72 and 0 79 respectively and rmse values of 2 39 2 79 and 3 30gcm 2 d 1 respectively the original bepsd does not perform well for other pfts especially for csh with r rmse 0 48 1 83g cm 2 d 1 bepsd with the new f w performs well for ebf dbf csh and osh with apparent improvements r rmse of ebf increases decreases from 0 69 3 17gcm 2 d 1 to 0 75 1 99gcm 2 d 1 and that of csh increases decreases from 0 48 1 83gcm 2 d 1 to 0 76 1 28gcm 2 d 1 bepsd with f ndvi performs better for ebf dbf csh osh and cro compared with the original version rmse values of ebf dbf cro decrease obviously from 3 17 4 01 3 99gcm 2 d 1 to 2 58 3 53 3 15gcm 2 d 1 for csh osh the r and rmse of gpp1 vs gpp0 are 0 48 0 74 and 1 83 0 76gcm 2 d 1 those of the improved gpp3 vs gpp0 increase to 0 52 0 82 and rmse decreases to 1 58 0 51gcm 2 d 1 bepsd coupled with both new f w and f ndvi is found to be better than bepsd with either new f w or f ndvi alone the rmse value decreases obviously in ebf dbf cro by 1 59 1 09 0 83gcm 2 d 1 and r increases most apparently in csh by 0 24 for all sites r of gpp1 vs gpp0 is 0 77 and rmse is 3 11gcm 2 d 1 r rmse of gpp2 3 4 vs gpp0 increases decreases by 0 02 0 03 0 04 0 28 0 26 0 47gcm 2 d 1 than that of gpp1 vs gpp0 the increased r and decreased rmse demonstrate the improvement of cases 2 3 4 relative to case 1 comparison between the improved bepsd and original version is shown in fig 4 using an intuitive taylor diagram fig 4 shows that improvements to bepsd are not apparent in simulating gpp for enf mf and gra the advantages of improved bepsd are apparent at pfts of ebf dbf csh osh and cro with increased r and decreased rmse and standard deviation sd bepsd of case 2 is improved obviously at ebf dbf csh and osh bepsd of case 3 is improved obviously at all ptfs excluding enf mf and gra case 4 is improved obviously at all ptfs excluding enf mf and gra and it performs best in all cases to deeply address the improvement of bepsd in case 3 we compare gpp0 with gpp1 gpp3 at all sites during late fall winter and early spring nov dec jan feb mar fig 5 r rmse of gpp3 vs gpp0 increases decreases by 0 12 0 31gcm 2 d 1 relative to that of gpp1 vs gpp0 at all sites this demonstrates that bepsd coupled with the chlorophyll constraint factor provides a gpp simulation that is more accurate for early spring late fall and winter 4 discussion 4 1 uncertainties of remote sensing data and flux gpp besides the model itself some uncertainties may be introduced due to many reasons such as the quality of the input data e g ndvi and lai the r decrease at it ca1 and it ca3 sites that was mentioned in section 3 2 may be caused by the low quality of remotely sensed ndvi or lai moreover the quality of ndvi and lai is controlled before they are used therefore it is deduced that the inconsistency between remote sensing data and flux data is responsible for the r decrement at it ca1 and it ca3 both ndvi and lai data used are satellite retrieved they have their own spatial resolutions the square shapes of pixel are mostly inconsistent with the irregular footprints of flux sites göckede et al 2005 rebmann et al 2005 göckede et al 2008 thus using the pixel values extracted from satellite retrieved ndvi and lai images according to the locations of the flux sites as the ndvi and lai of each site may introduce uncertainties into the modelled results though there are uncertainties the eddy covariance tower data generally represent fluxes with a footprint of typically 1km 1km papale and valentini 2003 thus the flux gpp is useful for validating our estimates at the site scale in addition to the inconsistent flux footprint with remote sensing data there are also uncertainties in the flux gpp data the eddy covariance technique provides an effective approach to measure co2 flux liu et al 2016 then the measured co2 flux can be converted to gpp nee and ecosystem respiration re using mathematical methods reichstein et al 2005 reichstein et al 2007 lasslop et al 2010 although we performed qc for flux gpp in order to validate our results there exists uncertainty during the process from in situ observed variables to flux gpp although there are uncertainties in the flux gpp as analysed above the performance of flux gpp when they are used to validate models both at the site level wang et al 2003 katherinee et al 2007 and at the pixel level zhang et al 2012a demonstrates that flux gpp are effective and reliable 4 2 reasons for unapparent improvements at some sites and pfts 4 2 1 pfts insensitive to drought and sites with slight water stress as shown in section 3 2 and 3 3 improvements of bepsd with new f w are not obvious at all sites or all pfts because of irrigation cro sites are not likely to be subject to serious drought fig a1 a shows that the enf mf and gra sites do not suffer from severe water stress thus there are no significant improvements when the original f w is replaced with the new one for pfts at cro enf mf and gra the improvements of bepsd with new f w at csh and osh are more obvious compared with those at cro enf mf and gra as most of the csh and osh sites experience dry and hot summers in the mediterranean climate area serrano ortiz et al 2007 carvalhais et al 2010 and present a high water stress factor fig a1 a the improvements at ebf and dbf are also more obvious compared with those at cro enf mf and gra although comparing the simulations of ebf and dbf in case 1 with those in case 2 shows that bepsd with the new f w improves more at ebf than at dbf with r rmse difference between gpp2 vs gpp0 and gpp1 vs gpp0 of 0 06 1 18gcm 2 d 1 for ebf and 0 04 0 71gcm 2 d 1 for dbf it is because ebf is more sensitive to drought than dbf which is consistent with the conclusion of vargas et al 2013 that deciduous sites are more resilient whereas evergreen sites are more sensitive to drought at the site level gpp2 agrees with gpp0 better than gpp1 particularly at sites with great potential of drought as the amount of rainfall during the summer months april september is a good descriptor of the drought intensity rambal et al 2003 the difference between r rmse of gpp2 vs gpp0 and that of gpp1 vs gpp0 at each site with descending order of mean precipitation in the summer months sp including june july and august in this study is shown to demonstrate the improvements of gpp2 fig 6 positive r difference values demonstrate that r of gpp2 vs gpp0 is higher than that of gpp1 vs gpp0 and negative rmse values demonstrate that rmse of gpp2 vs gpp0 is lower than that of gpp1 vs gpp0 it can be seen that the gpp2 improvements perform more obviously along with the descending order of sp at sites with sp more than 200mm the improvement of gpp2 is inconspicuous the improvements of gpp2 perform most obviously at sites with sp less than 200mm which have higher probability to produce drought than sites with abundant rainfall this also helps us understand why there are no obvious improvements for enf mf and gra in case 2 because most of these sites are with relatively abundant precipitation during summer and they suffer from no serious drought 4 2 2 sites with special conditions 1 de lkb v m 25 and g smax of enf are 62 5μmolm 2 s 1and 0 0036ms 1 respectively in this study table 1 although such parameterisation results in satisfactory gpp simulation for most enf sites gpp at de lkb enf is overestimated significantly fig a1 13 c literature survey shows that de lkb site is covered with non cleared wind throw disturbed upland spruce forest lindauer et al 2014 which is different from other enf sites used in this study v m 25 at de lkb is lower than 40 0μmolm 2 s 1 lindauer et al 2014 which is much more lower than the v m 25 value used for most enf sites in this study herein we set v m 25 to 33μmolm 2 s 1 chen et al 1999 to represent the carboxylation rate at a low level fig 7 shows that the simulated gpp with v m 25 of 33μmolm 2 s 1 red line in fig 7 is also higher than the flux gpp black line in fig 7 the g smax used for enf in this study 0 0036ms 1 is close to the approximation by lindauer et al 2014 125mmolm 2 s 1 which is about 0 003ms 1 at 25 c therefore g smax is not responsible for the overestimates of gpp at de lkb thus the over approximation of v m 25 should be partly responsible for the overestimation of gpp at de lkb the reasons for this overestimation should be explored further 2 fi jok fig a1 21 c shows that gpp1 4 at fi jok are obviously overestimated at fi jok in 2001 and 2002 the accumulated flux gpp0 values are 441 and 580gcm 2 d 1 respectively and the estimated accumulated gpp4 values are 1282 and 1770gcm 2 d 1 respectively the accumulated gpp values at fi jok estimated by the method of reichstein et al 2005 are 603 and 782gcm 2 d 1 for 2001 and 2002 katherinee et al 2007 respectively about 1gcm 2 d 1 higher than the average flux gpp during the growing season the difference between gpp0 and result of katherinee et al 2007 is much less than that between gpp0 and our estimate thus it is deduced that gpp0 is reliable here we make following discussions on other aspects it is noted firstly that the soil type at fi jok is special fi jok site is located in a flat square peat field and the deepest peat layer around this site is about 0 5 0 6m lohila et al 2004 more details about the surroundings of fi jok site can be found in previous report lohila et al 2004 as lohila et al 2004 stated this cultivated peat field remained a source of co2 despite a switch from an annual to a perennial crop land it is likely that the released co2 amount here is higher than at other sites which are carbon sinks the high co2 concentration may increase the photosynthesis ability zhao et al 2006 moreover the photosynthesis ability decreases with increasing co2 concentration under temperatures lower than 15 c long 1991 zhao et al 2006 however the peak value at fi jok is also overestimated during july fig a1 21 c when temperature is higher than 15 c fig 8 thus it is necessary to investigate further whether the co2 concentration at fi jok becomes high enough to hamper the photosynthesis of the crop if this is true then the co2 constraint factor in bepsd needs to be modified by further studies then we doubt whether the difference in crop species is responsible for the inaccurate gpp estimates thus the crop species of crop sites used in this study are collected as shown in table 6 we can see that there are various crop species among crop sites and years however there are no conspicuous differences between flux gpp and estimated gpps at cro sites exclude fi jok fig a1 3 12 15 21 22 c thus it is difficult to believe that the crop species difference is related to the obvious gpp overestimate at fi jok another possible reason may be the cold weather at fi jok this site is located at a high latitude place and belongs to the southern boreal climatic zone lohila et al 2004 it has a lower annual air temperature table 6 if the low mean air temperature was responsible for the low gpp estimated at fi jok the temperature constrain factor for a crop at high latitude in bepsd is not estimated accurately as there is no crop site at such high latitude it is difficult to compare and draw a conclusion nevertheless other sites such as fi hyy enf and ru vrk csh which are also located at high latitude also higher than 60 n and have the same temperature set table 1 in the temperature constraint factor calculation in bepsd are not overestimated as fi jok thus it is deduced that there is no problem with the calculation of temperature constrain factor in bepsd an interesting phenomenon attracts our attention that the annual mean air temperature of fi jok is the lowest among the six crops sites table 6 however the mean value of time series modis lai is the highest in addition modis lai is found to be systematically overestimated at high latitude sun et al 2014 and wang et al 2004 shows that there exists overestimation for modis lai at a needle leaf forest site near ruokolahti finland 61 32 n 28 43 e we doubt whether the modis lai at fi jok is overestimated as validation work for modis lai over crop areas in high latitude regions is lacking we make a simple comparison between modis lai and measured lai by lohila et al 2004 for 2000 2001 the peak value of modis lai is about 2 94m2 m 2 in the early july 2001 and that of the measured lai is about 6 0m2 m 2 in the middle july 2001 lohila et al 2004 the result of comparison implies that the modis lai here is not overestimated yet thus the assumption that the modis lai at fi jok is overestimated then results in the overestimation of gpp is not suggested based on the abovementioned analysis the reason for overestimating of gpp using bepsd at fi jok is not clear yet further work and discussion need to be done on this question 4 3 representativeness of ndvi for chlorophyll ndvi retrieved from remote sensing images represents not only leaf chlorophyll but also lai daughtry and jr 2008 juh et al 2008 eitel et al 2009 eraymondjr et al 2011 however the reported good relationship between ndvi and lcc eraymondjr et al 2011 dian et al 2016 and that between ndvi and ccc wu et al 2010c dian 2011 suggest that ndvi is a good indicator of chlorophyll and can be used to quantify the effect of chlorophyll on v m in ecosystem models therefore using satellite retrieved ndvi to reflect the variation of chlorophyll is appropriate there is difference however between the band range of the leaf scale spectral reflectance and that of satellite derived spectral reflectance which is used to compute spectral index in this study ndvi herein we list some band ranges and band centre positions table 7 dian 2011 and wu et al 2010c demonstrated that the relationship between either leaf scale ndvi or satellite derived ndvi and ccc can be described as an exponential function implying that the band ranges used to compute ndvi have no substantial impact on the relationship structure between ndvi and ccc moreover previous studies show strong correlation between ndvi and chlorophyll for various vegetation types and species including maize and winter wheat wu et al 2010c xia et al 2012 as well as other pfts including grass trifolium repens needle leaf platycladus orientalis juniperus fornosana hayata cedrus deodara broadleaf cinnamomum camphora ligustrum lucidum camellia oleifera abel et al and bamboo phyllostachys pubescens mazel ex h de lehaie dian 2011 this suggests that the close relationship between ndvi and chlorophyll is reliable and it is feasible to use ndvi as an indicator of chlorophyll at both leaf and canopy scales previous studies show that ndvi becomes saturated at a high vegetation cover fraction yan et al 2005 xu and meng 2016 in addition high chlorophyll content generally corresponds to high fractional cover however the constraint of chlorophyll on v m mostly occurs at low fractional cover when ndvi is not saturated consequently the chlorophyll constraint factor designed using ndvi in this study avoids the problem of ndvi saturation 4 4 further understanding of ndvimin as stated in section 2 3 ndvi min in eq 28 represents the boundary value of ndvi when chlorophyll exerts the largest limitation on v m to help us understand the meaning of ndvi min we make an analogy between ndvi min and the minimum temperature tmin for vegetation photosynthesis when temperature is lower than the minimum temperature photosynthesis stops or nearly stops yu and wang 2010 ndvi min designed to describe the full limitation of chlorophyll has a similar meaning with tmin for vegetation photosynthesis when ndvi is lower than ndvi min the chlorophyll limitation reaches the highest level minimum f ndvi as ndvi min is designed to describe the complete chlorophyll constraint condition which may not be reached under natural conditions it is a theoretical value rather than the real minimum ndvi among the ndvi time series of each pft table 8 lists the ndvi min of each pft attained by the 5 percentile method the 5 percentile ndvi of each pft is averaged from the 5 percentile ndvi at the sites of the corresponding pft it can be seen that the ndvi min values calculated with the 5 percentile method are higher than those calculated with the trial and error method for most pfts except dbf and cro then if the mean 5 percentile ndvi was taken as ndvi min f ndvi would be underestimated for pfts that cannot reach the full chlorophyll constraint condition under natural conditions 5 conclusions the bepsd model has been improved and evaluated at 41 flux sites across europe a new approach to water stress factor f w calculation is coupled into bepsd to replace the original method and a constrain factor derived from ndvi f ndvi is introduced to reflect the effect of chlorophyll on v m bepsd coupled with the new f w and the introduced f ndvi mitigates the overestimation of gpp under drought and during early spring late fall and winter obviously and performs better than the original bepsd the main conclusions are as follows 1 evaluation of the original bepsd shows that it yields satisfactory results for enf mf and gra whereas it does not performs very well for other pfts the original bepsd overestimates gpp under drought during spring late fall and winter inaccurate estimates of f w in the original bepsd leads to overestimation of gpp under drought neglecting the constraint of chlorophyll on v m is responsible for the overestimates of gpp during spring late fall and winter 2 the new f w calculated using the method of bai et al 2017 captures drought conditions and performs better than the original f w in bepsd particularly for pfts at ebf dbf csh and osh the introduced f ndvi can characterise the constraint of chlorophyll on photosynthesis during the vegetation growth cycle and performs satisfactorily for dbf and cro the improved bepsd with the new f w reduces the overestimation of gpp under drought and bepsd with f ndvi can effectively mitigate the excess estimates of gpp of dbf and cro during spring late fall and winter 3 bepsd coupled with both the new f w and the introduced f ndvi performs best among the improved versions of bepsd for each pft in this study and can be used to simulate gpp across europe acknowledgements we are grateful to anonymous reviewers and the editor for their valuable comments and suggestions which have greatly helped us to improve the original version of the manuscript this work was supported by the national key research and development program of china no 2016yfd0300101 the natural science foundation of china no 31571565 no 31671585 key basic research project of shandong natural science foundation of china no zr2017zb0422 and taishan scholar project of shandong province many thanks to prof jingming chen for sharing the daily beps model we sincerely appreciate dr gang mo dr fangmin zhang prof peijuan wang dr xuegang mao and prof xianfeng feng for their contributions during beps simulation we also would like to thank the language editor of enago www enago cn for the english language review this work used eddy covariance data acquired and shared by the fluxnet community including these networks ameriflux afriflux asiaflux carboafrica carboeuropeip carboitaly carbomont chinaflux fluxnet canada greengrass icos koflux lba necc ozflux tern tcos siberia and usccc the fluxnet eddy covariance data processing and harmonisation was performed by the icos ecosystem thematic centre ameriflux management project and flux data project of fluxnet with the support of cdiac and the ozflux chinaflux and asiaflux offices we greatly offer our profound appreciation to all the providers of the freely available data appendix a appendix b supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j ecolmodel 2017 11 023 appendix b supplementary data the following is supplementary data to this article 
25357,vegetation gross primary productivity gpp is an important component in the global carbon cycle and its accurate estimation is essential in ecosystem monitoring and simulation previous studies show that ecosystem models usually overestimate gpp under drought and during spring late fall and winter in this study these issues are addressed in the daily boreal ecosystem productivity simulator bepsd by introducing a new water stress factor f w to replace the old one and a designed fraction in term of the normalised difference vegetation index ndvi f ndvi to indicate the effect of chlorophyll on photosynthesis gpp simulations are conducted at 41 flux sites across europe to test bepsd with the new f w and f ndvi the new f w captures drought conditions well and f ndvi expresses the chlorophyll constraint on photosynthesis although bepsd with the old f w performs well for some plant function types pfts it is unsatisfactory for others bepsd incorporating both the new f w and f ndvi gives better simulations than the old version particularly for evergreen broadleaf forest deciduous broadleaf forest and closed shrub with r rmse value increasing decreasing from 0 69 3 20gcm 2 d 1 to 0 74 1 65gcm 2 d 1 0 72 4 01gcm 2 d 1 to 0 82 2 91gcm 2 d 1 0 54 1 82gcm 2 d 1 to 0 75 1 59gcm 2 d 1 respectively furthermore the new f w effectively mitigates gpp overestimates under drought and f ndvi counteracts gpp overestimates during spring late fall and winter overall the improved bepsd shows a satisfactory performance at flux sites over europe keywords gross primary productivity gpp daily boreal ecosystem productivity simulator bepsd water stress factor chlorophyll constraint ndvi europe 1 introduction gross primary productivity gpp is defined as the gross carbon fixed by terrestrial ecosystems through photosynthesis per unit time and area beer et al 2010 wu et al 2010b wu et al 2014 gpp is an important component in the terrestrial carbon cycle beer et al 2010 wang et al 2010 yuan et al 2014 li et al 2016 through which carbon dioxide from the atmosphere is fixed into vegetation it is also one of the major fluxes controlling the land atmosphere carbon exchanges raupach et al 2008 li et al 2016 an accurate estimate of gpp is particularly essential for quantifying other parameters in the carbon cycle such as net primary productivity npp and net ecosystem production nee wu et al 2014 as reported numerous ecosystem models are developed to simulate terrestrial gpp such as light use efficiency lue models and process based models li et al 2016 in lue models gpp is estimated through the monteith 1972 equation gpp lue f apar par where lue is the light use efficiency during a period and f apar represents the fraction of absorbed photosynthetically active radiation par in this type of model lue is the key parameter wu et al 2010d modified lue f apar in the monteith equation to vi vi and used the modified lue model to obtain an improved estimate of the gpp of wheat at the national experimental station for precision agriculture 40 10 6 n 116 26 3 e 20kilometres northeast of beijing china the modified lue model was also succussful for maize wu et al 2010b gpp estimation in the vegetation photosynthesis model vpm is also based on lue and lue in vpm is estimated as a function of temperature soil moisture and or vapor pressure deficit vpd xiao et al 2004 lue based models have been embraced for estimating spatial and temporal gpp dynamics on a large spatial scale wu et al 2010a as they are simple and easy to use although they lack strong theoretical basis and sufficient understanding of ecosystem function feng et al 2007 alternatively process based models are based on plant ecological mechanisms liu et al 1997 process based models try to simulate the sophisticated interaction processes between vegetation and atmosphere during plant growth such as photosynthesis respiration and evapotranspiration feng et al 2007 process based models include the boreal ecosystem productivity simulator beps liu et al 1997 and dynamic land ecosystem model dlem tian et al 2010 both beps and dlem can simulate the carbon cycle and the water cycle and their photosynthetic assimilations are based on the farquhar model liu et al 1999 tian et al 2010 as a big model dlem can also simulate the nitrogen cycle and it includes an agriculture module and a city module tian et al 2010 in contrast beps is simple and hence easier to use considering the advantages of process based models and that we only focused on gpp simulation in this study we selected daily beps bepsd this model was developed from the forest biogeochemical cycles forest bgc model liu et al 1997 and has been widely used in gpp npp and evapotranspiration et simulations liu et al 1999 2002 liu et al 2003 zhang et al 2012a there are also researches focusing on parameter optimisation in beps chen et al 2012 he et al 2014 chen et al 2012 studied the effect of the clumping index ω on gpp simulation he et al 2014 used the ensemble kalman filter method to optimise two key parameters the water stress factor f w and the maximum photosynthetic carboxylation rate at 25 c v m 25 in beps and demonstrated their seasonal variations f w and v m 25 are two of the most important parameters in ecosystem models related to carbon uptake by vegetation he et al 2014 the soil water stress factor is included in most ecosystem models e g w ε in carnegie ames stanford approach casa model potter et al 1993 f lwp in bepsd liu et al 1997 f w in half hourly or hourly beps bepsh ju et al 2006 chen et al 2012 and w scalar in vpm xiao et al 2004 these factors are calculated in different ways in lue based models e g casa and vpm the water stress factor is used to scale the maximum lue ε max potter et al 1993 xiao et al 2004 it is computed using evapotranspiration information in casa potter et al 1993 and other factors in vpm xiao et al 2004 in process based models such as bepsd and bepsh the water stress factor parameterised using the soil water content swc liu et al 1997 ju et al 2006 is included in the jarvis stomatal conductance g s model jarvis 1976 liu et al 1997 liu et al 1999 and in the ball woodrow berry bwb type equations ju et al 2006 swc used to calculate f w is computed using soil water balance modules with a single soil layer liu et al 1997 or multiple layers ju et al 2006 it is demonstrated that f w derived from a multi layer model is more effective than that from a single layer model results averaged from five models including bepsd showed that the gpp of evergreen mediterranean oak woodlands was overestimated under drought vargas et al 2013 probably because the stomatal conductance was overestimated under drought xu and baldocchi 2003 vargas et al 2013 the accurate estimation of f w is critical in ecosystem models and is essential for studying carbon and water cycles v m calculated from v m 25 has a significant impact on both vegetation photosynthesis and evapotranspiration in process based ecosystem models he et al 2014 houborg et al 2015 employed a semi mechanistic relationship between chlorophyll and v m 25 based on previous works sage et al 1987 evans 1989 friend 1995 houborg et al 2013 to study the constraint of leaf chlorophyll on gpp simulation in agricultural systems using chlorophyll as a constraint factor in an ecosystem model can improve the accuracy of gpp simulations croft et al 2017 showed that the leaf chlorophyll content lcc and v m 25 are highly correlated with an r2 value of 0 78 for deciduous forest samples including trembling aspen bigtooth aspen red maple and ash the intercomparison from 26 models suggested that gpp is overestimated during winter spring and fall schaefer et al 2012 when the lcc is low however few of the widely used ecosystem models include a chlorophyll constraint factor it is likely that the overestimation of gpp during these periods when chlorophyll stays at a low level early spring late fall and winter may result from the lack or insufficient consideration of chlorophyll constraint on modelling photosynthesis however a temporally and spatially continuous chlorophyll content record cannot be obtained easily previous studies demonstrated a strong relationship between chlorophyll and the vegetation index vi wu et al 2010c showed a close correlation between vegetation indices vis and canopy chlorophyll content ccc dian 2011 also showed a strong relationship between vis and both lcc and ccc thus using a vi e g the normalised difference vegetation index ndvi as the chlorophyll indicator is a good solution to the problem of attaining continuous chlorophyll however few of the widely used ecosystem models include a chlorophyll constraint factor thus in the present study we experiment by introducing a chlorophyll constraint factor into the bepsd ecosystem model the objectives of this paper are 1 to evaluate the performance of the original bepsd over europe 2 to improve the accuracy of gpp simulation using bepsd by incorporating a new f w in the g s calculation formula 3 to use ndvi as an indicator of chlorophyll to quantify the effect of chlorophyll on v m by introducing a designed chlorophyll constraint factor f ndvi into bepsd and 4 to validate and compare the improved versions of bepsd across europe 2 method and data 2 1 overview of beps beps was initially developed at the canada centre for remote sensing to assist in natural resources management liu et al 1997 especially for the boreal forest later most of its applications were in north america liu et al 1999 2002 liu et al 2003 liu et al 2005 ju et al 2006 mo et al 2008 govind et al 2011 zhang et al 2012a gonsamo et al 2013 he et al 2014 sprintsin et al 2015 and also expanded for global applications zheng et al 2015 it has been developed at the daily liu et al 1997 liu et al 1999 the half hourly ju et al 2006 and the hourly step chen et al 2012 in beps gpp is simulated by scaling farquhar s leaf level biochemical model up to the canopy level using a two leaf approach he et al 2014 in bepsd g s is calculated using the jarvis model and v m is parameterised by temperature and nitrogen content some key equations pertaining to the carbon cycle in bepsd are described as follows daily gross primary productivity daily gross primary productivity is calculated in bepsd as 1 a canopy a sun l a i sun a shade l a i shade 2 g p p a canopy d a y l e n g t h f a c t o r gpp where a canopy is the assimilation rate of canopy in μmolm 2 s 1 a sun and a shade are the assimilation rates of sunlit and shaded leaves respectively lai sun and lai shade are the leaf area index lai values of the sunlit and shaded leaves respectively gpp is the gross primary productivity in gcm 2 d 1 daylength is the day of length in second factor gpp converts gpp unit into gcm 2 day 1 assimilation rate a sun and a shade can be solved from the farquhar model and the stomatal conductance model 3 a min w c w j r d 4 w c v m c i γ c i k 5 w j j c i γ 4 5 c i 10 5 γ 6 r d 0 015 v m where a is the net photosynthesis rate in μmolm 2 s 1 w c is the rubisco limited gross photosynthesis rate in μmolm 2 s 1 w j is the light limited gross photosynthesis rate in μmolm 2 s 1 r d is the daytime leaf dark respiration in μmolm 2 s 1 c i is the intercellular co2 concentration in pa г is the co2 compensation point in the absence of dark respiration in pa k is a function of enzyme kinetics in pa j is the electron transport rate in μmolm 2 s 1 and v m is the maximum carboxylation rate in μmolm 2 s 1 components of k and γ are calculated as 7 k k c 1 o 2 k o 8 γ 4 04 1 75 t a 25 10 where k c and k o are the michaelis meton constants for co2 and o2 respectively details in liu et al 1999 o 2 is the intercellular o2 concentration in pa 21 000 and t a is air temperature in c components of v m and j are calculated as 9 v m v m 25 2 4 t a 25 10 f t a f n 10 j j max ppfd ppfd 2 1 j max 11 j max 29 1 1 64 v m where v m 25 is the maximum carboxylation rate at 25 c in μmolm 2 s 1 the functions of t a and n can be found in liu et al 1999 j max is the light saturated rate of electron transport in μmolm 2 s 1 ppfd is the photosynthetically active flux density in μmolm 2 s 1 because c i is not easy to obtain on a regional or global scale a cannot be calculated with the abovementioned formulas according to fluid physics a can also be described as 12 a c a c i g 13 g 10 6 g s r gas t a 273 14 g s g smax f p p f d f t a f v p d f l w p 15 f l w p l w p close l w p l w p close l w p open 16 l w p 0 2 s o i l water s o i l cap where c a is the co2 concentration in the atmosphere in pa g is the total conductance to co2 from cell to air in μmolm 2 s 1 pa 1 g s is the stomatal conductance to co2 in ms 1 and g smax is the maximum stomatal conductance to co2 in ms 1 lwp is the leaf water potential here for the evergreen needle leaf forest and the deciduous needle leaf forest lwp close 4mpa for other plant functional types pfts lwp close 8mpa for all pfts lwp open 1mpa if the soil is very dry f lwp 0 001 functions of ppfd ta and vpd are given by chen et al 1999 a is obtained from eqs 3 and 12 17 a 1 2 a 1 2 g c 1 2 a g 2 b g c where a is the minimum of a c and a j corresponding to w c and w j if a a c then a k c a 2 b 2 2γ k c a v m 2 c a k r d and c v m r d 2 if a a j then a 2 3γ c a 2 b 0 4 4 3γ c a j 2 c a 2 3γ r d and c 0 2j r d 2 then a is derived after analytical integration over a day considering the nonlinear effects u 1 0 5 π 2 0 π 2 cos θ d θ 4 π 1 27 of meteorological variables on photosynthesis due to their diurnal variability shown as 18 a 1 27 2 g n a 1 2 2 g n 2 c 1 2 g n 2 a g n b 4 a d b 4 a c 1 2 b 2 4 a c 8 a 3 2 ln 2 a g n b 2 a 1 2 d b 2 a 1 2 c 1 2 where gn is the conductance at noon d a g n 2 b g n c 1 2 sunlit and shaded lai as a two leaf model chen et al 2012 bepsd calculates gpp for sunlit leaves and shaded leaves separately lai of sunlit and shaded leaves is calculated as 19 l a i sun 2 cos θ 1 exp 0 5 ω l a i cos θ 20 l a i shade l a i l a i sun 21 θ 1 2 1 2 π 2 θ noon θ noon π 8 3 4 θ noon 22 θ noon π 180 l a t 23 5 sin j u l i a n d a y 81 2 π 365 where θ is the solar zenith angle θ noon is the solar zenith angle at noon lat is the latitude the parameter values used in this study are listed in table 1 2 2 incorporating the new water stress factor into the gs simulation in bepsd g s was calculated using the jarvis model jarvis 1976 liu et al 1999 which included constraints of radiation temperature vpd and lwp a soil water stress factor f lwp as a function of lwp calculated from the swc and the soil water capacity eqs 15 16 was used to scale g smax in bepsd eq 14 in bepsh the jarvis model was replaced by a modified version of the bwb model ball et al 1987 ju et al 2006 both the jarvis and the modified bwb models considered f w however the soil profile was split into multiple layers in the modified bwb model whereas it was only one layer in the jarvis model when simulating f w liu et al 1997 ju et al 2006 in original bepsd f w was calculated from a single soil layer water balance module which did not include the root distribution information the root information and the soil temperature were considered in the modified bwb model in bepsh which performed well ju et al 2006 proving considering multiple layers soil water and root information together produces more accurate f w estimates following the modification of bepsh coupling a model based on multiple soil layers into bepsd instead of the single layer model could give more accurate f w estimation in the g s simulation a penman monteith based model incorporating remote sensing data and a multi layer water balance module rs wbpm was proposed by bai et al 2017 which was parameterised with precipitation vertical root distribution vrd and satellite retrieved vegetation information to estimate the water stress in the mediterranean climate area more accurately in fact it showed satisfactory performance under drought regions bai et al 2017 in rs wbpm the soil profile was divided into multiple soil layers f w was calculated for a bulk surface in rs wbpm using vrd and vis bai et al 2017 f w was the weighted sum of simulated f w of each soil layer and the weight of each layer was determined by the vrd information of that layer and the wettest layer bai et al 2017 the model was validated under mediterranean climate where vegetation suffered from severe drought and it was successful bai et al 2017 in the present study we include a water stress factor eqs 23 26 for the dominating surface species 23 f w l 1 3 s w s f l w l 24 s w s f s w c l θ w l θ m l θ w l 25 w l y l w s t 0 5 1 w l w s t 1 y l w s t y l l l w s t l l w s t 26 y l 1 β d l β d l 1 β d l l 1 l 1 where superscript l denotes a soil layer index l wst denotes the index of the wettest soil layer w swsf and θ w represent the weight the soil water stress factor and the wilting point of the soil layer swc l is the soil water content in soil layer l in mm and it represents the soil water balance in this layer l θ m represents the swc when stomatal conductance becomes maximum y denotes the proportion of root biomass within a soil layer 0 1 d l is the depth of the lower boundary of soil layer l cm and β is the root extinction coefficient we refer to bai et al 2017 and jackson et al 1997 for the value of β of a specified biome 2 3 introducing fndvi into the vm simulation the chlorophyll content is not constrained in the v m estimate in bepsd eq 9 croft et al 2017 proved a close relationship between v m and chlorophyll as previous studies have shown he et al 2014 croft et al 2017 v m 25 has a seasonally variation presenting lower values during early spring and fall than the maximum over the whole growing season thus v m without the chlorophyll constraint in bepsd may be overestimated during early spring and late fall which is probably why gpp is overestimated during these periods to address this issue we try to introduce a factor to reflect the chlorophyll content constrain on v m although the spatially continuous parameterisation of v m on a global scale remains unsolved croft et al 2017 close relationships between vis and chlorophyll are found including relationships between leaf scale vis and lcc leaf scale vis and ccc satellite derived vis and lcc and satellite derived vis and ccc wu et al 2010c dian 2011 dian et al 2016 these provide a solution to the abovementioned problem by computing a stress factor using a vegetation index as the chlorophyll indicator dian 2011 reported that ccc is an exponential function of leaf scale ndvi with r2 of 0 60 and rmse of 0 40g m2 on the canopy level as per wu et al 2010c there is also a strong exponential relationship between ccc and satellite ndvi retrieved from tm and hyperion with r2 0 73 besides the strong relationship between ndvi both leaf scale ndvi and satellite derived ndvi and chlorophyll content there is also a linear relationship between lai and satellite derived ndvi with coarse spatial resolution and high temporal resolution e g modis ndvi wang et al 2005 sun et al 2006 based on these relationships it is deduced that lai ndvi and chlorophyll are positively correlated during the growing season the similar temporal variations of lai and leaf chlorophyll during the growing season shown by houborg et al 2013 is partly consistent with our hypothesis thus it is a reliable and feasible solution to use ndvi instead of chlorophyll to represent the effect of chlorophyll on v m both on the leaf and canopy scales ndvi a classic two band vegetation index among the vegetation indices used in previous works wu et al 2008 dian 2011 dian et al 2016 is chosen to compute the constrain of chlorophyll on v m then eq 9 is modified to eq 27 to eliminate the effect of various ndvi ranges in different pfts the introduced f ndvi is normalised as 27 v m v m 25 2 4 t a 25 10 f t a f n f n d v i 28 f n d v i 0 01 n d v i n d v i min n d v i n d v i min n d v i max n d v i min 2 n d v i min n d v i n d v i max 1 n d v i n d v i max where f ndvi denotes the ndvi constrain factor f ndvi ndvi represents the time series ndvi extracted from remote sensing images according to the location of the flux sites ndvi min is the minimum ndvi value of each pft and is attained using the trial and error method 0 0 0 5 step 0 1 and ndvi max is the 95 percentile of the ndvi time series at each site for each year in this equation ndvi min represents the boundary value of ndvi when photosynthesis is completely limited by chlorophyll for further understanding of ndvi min please see section 4 4 2 4 modelling cases in order to investigate the impacts of the new f w and the introduced f ndvi on bepsd we conduct model simulations using the following four cases table 2 the results of four cases are denoted as gpp1 gpp2 gpp3 and gpp4 by comparing gpp1 with gpp2 we demonstrate the advantage of considering root vertical information and multiple layers swc in f w simulation under drought by comparing gpp1 with gpp3 we investigate the improvements and show the importance of including a chlorophyll constraint on v m when simulating vegetation photosynthesis by comparing the results of the four cases we can determine the best simulation scenario 2 5 data 2 5 1 vegetation indices data vegetation indices including ndvi and lai are employed in this study mod15a2 8days 1km and myd15a2 8days 1km datasets are used to generate lai with a time resolution of 4days and a spatial resolution of 1km we used ndvi with 8days and 250m resolution from mod13q1 16days 250m and myd13q1 16days 250m datasets to calculate the introduced f ndvi the mod and myd datasets used are retrieved from the modis subset data set available on ornl daac http daac ornl gov they are linearly interpolated into the daily step after quality control the quality of ndvi can be examined in a raster layer named pixel reliability we only use data whose pixel reliability value are 1 and 0 ndvi values less than 0 1 are set to 0 1 for lai a raster layer named fparlai qc in bit string format contains the quality of lai data and only the bit string with zero values at both bit no 0 and bit no 2 are used the detailed meanings of pixel reliability in pixel reliability raster layer and bit string in fparlai qc raster layer are available at https lpdaac usgs gov dataset discovery modis modis products table mod13q1 and https lpdaac usgs gov dataset discovery modis modis products table mod15a2 respectively 2 5 2 flux sites data forty one flux sites including 11 enf sites 2 ebf sites 8 dbf sites 2 mf sites 2 csh sites 2 osh sites 9 gra sites and 5 cro sites are used in this study flux data with low quality are removed a total of 339 site years are available the site locations are shown in fig 1 and general information is listed in table 3 among the meteorological inputs of bepsd daily maximum and minimum temperature precipitation and radiation are retrieved from the half hourly temperature observed at the flux sites and relative humidity is calculated from the flux measured vpd missing data of tower observations are filled with the corresponding era interim data as introduced at http fluxnet fluxdata org data fluxnet2015 dataset fullset data product the modelled daily gpp are validated with the flux gpp at the 41 flux sites table 4 retrieved from the fluxnet2015 dataset which is available on http fluxnet fluxdata org data fluxnet2015 dataset the available flux gpp gpp dt vut ref noted as gpp0 is employed in this study it is estimated using the daytime partitioning method lasslop et al 2010 and the variable ustar threshold vut method http fluxnet fluxdata org data fluxnet2015 dataset fullset data product it also has a data quality control qc field with values ranging from 0 to 1 indicating the percentage of good quality half hourly data only data with qc value higher than 0 5 are used to validate our modelled results in this study swc data including the soil water content of the shallowest layer at each flux site are also used to indicate drought in this study the measurement depths at 10 sites are collected and listed in table 4 2 5 3 soil property data the available water capacity awc data are retrieved from the dataset global gridded surfaces of selected soil characteristics of the international geosphere biosphere programme data and information system igbp dis this dataset is available online https daac ornl gov the igbp gridded soil dataset has a spatial resolution of 5 5 arc minutes all measurements in the igbp dis dataset are taken within the depth interval of 0 100cm awc data at each flux site are extracted from this dataset according to their locations 3 results 3 1 ndvimin of each pft the correlation coefficient r and root mean square error rmse values for gpp3 versus gpp0 with different ndvi min in bepsd are shown in fig 2 with high r and low rmse results ndvi min of dbf csh osh and cro are set to 0 5 0 2 0 2 and 0 4 directly the r rmse value of enf and gra remain almost constant with ndvi min increasing from 0 0 to 0 5 thus here ndvi min of enf and gra are set to 0 1 ndvi of 0 0 makes no sense for vegetation the r of ebf decreases with decreasing rmse and the largest decrease rate of rmse occurs when ndvi min varies from 0 2 to 0 3 then ndvi min of ebf is set to 0 3 for mf when ndvi min increases from 0 0 to 0 5 there are no significant variations of r and rmse herein ndvi min of mf is set to 0 2 thus the recommended ndvi min values of each pft are attained table 5 3 2 simulated results for each site the regression statistics number n slope a intercept b r and rmse for each site are summarised in table a1 appendix a the time series of gpp0 and the estimated gpps gpp1 4 of each site are shown in fig a1 c of appendix a along with fig a1 a showing the new f w and swc in the shallowest layer and fig a1 b presenting the original v m and v m parameterised with f ndvi v m ndvi the scatterplots of simulated gpps vs gpp0 for each site are available in appendix b the slope and interception values in table a1 show that the model obviously underestimated gpp at es lju and it noe and obviously overestimated gpp at de lkb and fi jok in the four examined cases the improved model cases 2 4 provides more accurate gpp estimates at es lju and it noe compared with the original model case 1 there are no obvious improvements at de lkb and fi jok in cases 2 4 table a1 shows that r increases obviously increases or decreases only very slightly at most sites excluding it ca1 and it ca3 where r presents an obvious decreasing trend this is discussed in section 4 1 the possible reasons for the unapparent improvements at the de lkb and fi jok sites are discussed in section 4 2 2 the original f w is nearly 1 at all 41 sites and is not drawn in fig a1 it can be seen that the new f w shows consistent trend with swc at most sites and indicates drought satisfactorily fig a1 a the new f w performs successfully particularly at sites located in the subtropical mediterranean climate zone wherein drought usually occurs during the warm summer days e g fr pue it ro1 and it ro2 ryu et al 2012 the outperformances of the new f w at those sites demonstrate its reliability to reflect drought much more convincingly thus it is feasible to use the new f w to replace the old one in bepsd the c maps in fig a1 show that gpp1 is significantly higher than gpp0 at some sites e g es lgs es lju fr pue it noe it ro1 it ro2 and it sro during their summer months mostly during late may to september these overestimates are mostly accompanied by apparent fall of their corresponding f w line fig a1 18 19 23 31 34 36 a which indicates a poor estimate for drought at each site in modelling gpp1 in contrast gpp2 agrees with gpp0 flux gpp much better for example at it noe both new f w and swc show sever water stress during july to september fig a1 31 a when gpp1 is overestimated but gpp2 agrees well with gpp0 compared with r rmse for gpp1 vs gpp0 that for gpp2 vs gpp0 increases decreases from 0 43 1 82gcm 2 d 1 to 0 78 1 23gcm 2 d 1 which demonstrates that the new f w effectively addresses the issue of gpp overestimation under drought conditions these improvements are also obvious at the sites listed above it proves that the new f w can capture droughts and performs better than the original f w fig a1 c also show the overestimates of gpp1 at some sites e g be lon de kli de rus fr gri it ro1 and it ro2 during early spring late fall and winter months such as from september to april or may of the subsequent year however gpp3 effectively eliminates these overestimates during these periods and is closer to gpp0 than gpp1 for example gpp1 at be lon it ro1 and it ro2 fig a1 4 34 35 c show overestimates during spring late fall and winter months resulting in the high rmse values of 4 06 5 97 and 6 04gcm 2 d 1 respectively table a1 gpp3 at the same sites outperforms gpp1 during winter and spring with the decreased rmse values of 3 44 4 49 and 5 06gcm 2 d 1 respectively moreover improvements can also be found at de kli during 2007 2008 2012 de rus spring in 2011 2012 2014 and late fall in 2013 es lgs late summer and fall in 2007 and 2009 es lju fall and winter fi jok early spring in 2001 and fr gri fall and winter fig a1 12 15 18 21 22 c these results show that bepsd with introduced f ndvi yields better results than the original version in addition from fig a1 12 15 18 21 22 b we observe that the periods when gpp is overestimated at these sites are almost consistent with the periods when v m is higher than v m ndvi it is demonstrated that using v m parameterised with introduced f ndvi in bepsd could mitigate the overestimates of gpp during early spring late fall and winter and f ndvi can be employed to quantitatively describe the influence of chlorophyll on v m overall gpp4 i e the estimated result of bepsd which is coupled with the new f w and the introduced f ndvi performs better than gpp2 and gpp3 it combines the advantages of gpp2 and gpp3 mitigating the overestimates under drought conditions and during early spring late fall and winter 3 3 evaluation and comparison of improved bepsd at the biome level we show the original and improved bepsd performances when validated with flux gpp in fig 3 the model performances vary significantly across biomes the original bepsd performs better for pfts of enf mf and gra with r values of 0 85 0 72 and 0 79 respectively and rmse values of 2 39 2 79 and 3 30gcm 2 d 1 respectively the original bepsd does not perform well for other pfts especially for csh with r rmse 0 48 1 83g cm 2 d 1 bepsd with the new f w performs well for ebf dbf csh and osh with apparent improvements r rmse of ebf increases decreases from 0 69 3 17gcm 2 d 1 to 0 75 1 99gcm 2 d 1 and that of csh increases decreases from 0 48 1 83gcm 2 d 1 to 0 76 1 28gcm 2 d 1 bepsd with f ndvi performs better for ebf dbf csh osh and cro compared with the original version rmse values of ebf dbf cro decrease obviously from 3 17 4 01 3 99gcm 2 d 1 to 2 58 3 53 3 15gcm 2 d 1 for csh osh the r and rmse of gpp1 vs gpp0 are 0 48 0 74 and 1 83 0 76gcm 2 d 1 those of the improved gpp3 vs gpp0 increase to 0 52 0 82 and rmse decreases to 1 58 0 51gcm 2 d 1 bepsd coupled with both new f w and f ndvi is found to be better than bepsd with either new f w or f ndvi alone the rmse value decreases obviously in ebf dbf cro by 1 59 1 09 0 83gcm 2 d 1 and r increases most apparently in csh by 0 24 for all sites r of gpp1 vs gpp0 is 0 77 and rmse is 3 11gcm 2 d 1 r rmse of gpp2 3 4 vs gpp0 increases decreases by 0 02 0 03 0 04 0 28 0 26 0 47gcm 2 d 1 than that of gpp1 vs gpp0 the increased r and decreased rmse demonstrate the improvement of cases 2 3 4 relative to case 1 comparison between the improved bepsd and original version is shown in fig 4 using an intuitive taylor diagram fig 4 shows that improvements to bepsd are not apparent in simulating gpp for enf mf and gra the advantages of improved bepsd are apparent at pfts of ebf dbf csh osh and cro with increased r and decreased rmse and standard deviation sd bepsd of case 2 is improved obviously at ebf dbf csh and osh bepsd of case 3 is improved obviously at all ptfs excluding enf mf and gra case 4 is improved obviously at all ptfs excluding enf mf and gra and it performs best in all cases to deeply address the improvement of bepsd in case 3 we compare gpp0 with gpp1 gpp3 at all sites during late fall winter and early spring nov dec jan feb mar fig 5 r rmse of gpp3 vs gpp0 increases decreases by 0 12 0 31gcm 2 d 1 relative to that of gpp1 vs gpp0 at all sites this demonstrates that bepsd coupled with the chlorophyll constraint factor provides a gpp simulation that is more accurate for early spring late fall and winter 4 discussion 4 1 uncertainties of remote sensing data and flux gpp besides the model itself some uncertainties may be introduced due to many reasons such as the quality of the input data e g ndvi and lai the r decrease at it ca1 and it ca3 sites that was mentioned in section 3 2 may be caused by the low quality of remotely sensed ndvi or lai moreover the quality of ndvi and lai is controlled before they are used therefore it is deduced that the inconsistency between remote sensing data and flux data is responsible for the r decrement at it ca1 and it ca3 both ndvi and lai data used are satellite retrieved they have their own spatial resolutions the square shapes of pixel are mostly inconsistent with the irregular footprints of flux sites göckede et al 2005 rebmann et al 2005 göckede et al 2008 thus using the pixel values extracted from satellite retrieved ndvi and lai images according to the locations of the flux sites as the ndvi and lai of each site may introduce uncertainties into the modelled results though there are uncertainties the eddy covariance tower data generally represent fluxes with a footprint of typically 1km 1km papale and valentini 2003 thus the flux gpp is useful for validating our estimates at the site scale in addition to the inconsistent flux footprint with remote sensing data there are also uncertainties in the flux gpp data the eddy covariance technique provides an effective approach to measure co2 flux liu et al 2016 then the measured co2 flux can be converted to gpp nee and ecosystem respiration re using mathematical methods reichstein et al 2005 reichstein et al 2007 lasslop et al 2010 although we performed qc for flux gpp in order to validate our results there exists uncertainty during the process from in situ observed variables to flux gpp although there are uncertainties in the flux gpp as analysed above the performance of flux gpp when they are used to validate models both at the site level wang et al 2003 katherinee et al 2007 and at the pixel level zhang et al 2012a demonstrates that flux gpp are effective and reliable 4 2 reasons for unapparent improvements at some sites and pfts 4 2 1 pfts insensitive to drought and sites with slight water stress as shown in section 3 2 and 3 3 improvements of bepsd with new f w are not obvious at all sites or all pfts because of irrigation cro sites are not likely to be subject to serious drought fig a1 a shows that the enf mf and gra sites do not suffer from severe water stress thus there are no significant improvements when the original f w is replaced with the new one for pfts at cro enf mf and gra the improvements of bepsd with new f w at csh and osh are more obvious compared with those at cro enf mf and gra as most of the csh and osh sites experience dry and hot summers in the mediterranean climate area serrano ortiz et al 2007 carvalhais et al 2010 and present a high water stress factor fig a1 a the improvements at ebf and dbf are also more obvious compared with those at cro enf mf and gra although comparing the simulations of ebf and dbf in case 1 with those in case 2 shows that bepsd with the new f w improves more at ebf than at dbf with r rmse difference between gpp2 vs gpp0 and gpp1 vs gpp0 of 0 06 1 18gcm 2 d 1 for ebf and 0 04 0 71gcm 2 d 1 for dbf it is because ebf is more sensitive to drought than dbf which is consistent with the conclusion of vargas et al 2013 that deciduous sites are more resilient whereas evergreen sites are more sensitive to drought at the site level gpp2 agrees with gpp0 better than gpp1 particularly at sites with great potential of drought as the amount of rainfall during the summer months april september is a good descriptor of the drought intensity rambal et al 2003 the difference between r rmse of gpp2 vs gpp0 and that of gpp1 vs gpp0 at each site with descending order of mean precipitation in the summer months sp including june july and august in this study is shown to demonstrate the improvements of gpp2 fig 6 positive r difference values demonstrate that r of gpp2 vs gpp0 is higher than that of gpp1 vs gpp0 and negative rmse values demonstrate that rmse of gpp2 vs gpp0 is lower than that of gpp1 vs gpp0 it can be seen that the gpp2 improvements perform more obviously along with the descending order of sp at sites with sp more than 200mm the improvement of gpp2 is inconspicuous the improvements of gpp2 perform most obviously at sites with sp less than 200mm which have higher probability to produce drought than sites with abundant rainfall this also helps us understand why there are no obvious improvements for enf mf and gra in case 2 because most of these sites are with relatively abundant precipitation during summer and they suffer from no serious drought 4 2 2 sites with special conditions 1 de lkb v m 25 and g smax of enf are 62 5μmolm 2 s 1and 0 0036ms 1 respectively in this study table 1 although such parameterisation results in satisfactory gpp simulation for most enf sites gpp at de lkb enf is overestimated significantly fig a1 13 c literature survey shows that de lkb site is covered with non cleared wind throw disturbed upland spruce forest lindauer et al 2014 which is different from other enf sites used in this study v m 25 at de lkb is lower than 40 0μmolm 2 s 1 lindauer et al 2014 which is much more lower than the v m 25 value used for most enf sites in this study herein we set v m 25 to 33μmolm 2 s 1 chen et al 1999 to represent the carboxylation rate at a low level fig 7 shows that the simulated gpp with v m 25 of 33μmolm 2 s 1 red line in fig 7 is also higher than the flux gpp black line in fig 7 the g smax used for enf in this study 0 0036ms 1 is close to the approximation by lindauer et al 2014 125mmolm 2 s 1 which is about 0 003ms 1 at 25 c therefore g smax is not responsible for the overestimates of gpp at de lkb thus the over approximation of v m 25 should be partly responsible for the overestimation of gpp at de lkb the reasons for this overestimation should be explored further 2 fi jok fig a1 21 c shows that gpp1 4 at fi jok are obviously overestimated at fi jok in 2001 and 2002 the accumulated flux gpp0 values are 441 and 580gcm 2 d 1 respectively and the estimated accumulated gpp4 values are 1282 and 1770gcm 2 d 1 respectively the accumulated gpp values at fi jok estimated by the method of reichstein et al 2005 are 603 and 782gcm 2 d 1 for 2001 and 2002 katherinee et al 2007 respectively about 1gcm 2 d 1 higher than the average flux gpp during the growing season the difference between gpp0 and result of katherinee et al 2007 is much less than that between gpp0 and our estimate thus it is deduced that gpp0 is reliable here we make following discussions on other aspects it is noted firstly that the soil type at fi jok is special fi jok site is located in a flat square peat field and the deepest peat layer around this site is about 0 5 0 6m lohila et al 2004 more details about the surroundings of fi jok site can be found in previous report lohila et al 2004 as lohila et al 2004 stated this cultivated peat field remained a source of co2 despite a switch from an annual to a perennial crop land it is likely that the released co2 amount here is higher than at other sites which are carbon sinks the high co2 concentration may increase the photosynthesis ability zhao et al 2006 moreover the photosynthesis ability decreases with increasing co2 concentration under temperatures lower than 15 c long 1991 zhao et al 2006 however the peak value at fi jok is also overestimated during july fig a1 21 c when temperature is higher than 15 c fig 8 thus it is necessary to investigate further whether the co2 concentration at fi jok becomes high enough to hamper the photosynthesis of the crop if this is true then the co2 constraint factor in bepsd needs to be modified by further studies then we doubt whether the difference in crop species is responsible for the inaccurate gpp estimates thus the crop species of crop sites used in this study are collected as shown in table 6 we can see that there are various crop species among crop sites and years however there are no conspicuous differences between flux gpp and estimated gpps at cro sites exclude fi jok fig a1 3 12 15 21 22 c thus it is difficult to believe that the crop species difference is related to the obvious gpp overestimate at fi jok another possible reason may be the cold weather at fi jok this site is located at a high latitude place and belongs to the southern boreal climatic zone lohila et al 2004 it has a lower annual air temperature table 6 if the low mean air temperature was responsible for the low gpp estimated at fi jok the temperature constrain factor for a crop at high latitude in bepsd is not estimated accurately as there is no crop site at such high latitude it is difficult to compare and draw a conclusion nevertheless other sites such as fi hyy enf and ru vrk csh which are also located at high latitude also higher than 60 n and have the same temperature set table 1 in the temperature constraint factor calculation in bepsd are not overestimated as fi jok thus it is deduced that there is no problem with the calculation of temperature constrain factor in bepsd an interesting phenomenon attracts our attention that the annual mean air temperature of fi jok is the lowest among the six crops sites table 6 however the mean value of time series modis lai is the highest in addition modis lai is found to be systematically overestimated at high latitude sun et al 2014 and wang et al 2004 shows that there exists overestimation for modis lai at a needle leaf forest site near ruokolahti finland 61 32 n 28 43 e we doubt whether the modis lai at fi jok is overestimated as validation work for modis lai over crop areas in high latitude regions is lacking we make a simple comparison between modis lai and measured lai by lohila et al 2004 for 2000 2001 the peak value of modis lai is about 2 94m2 m 2 in the early july 2001 and that of the measured lai is about 6 0m2 m 2 in the middle july 2001 lohila et al 2004 the result of comparison implies that the modis lai here is not overestimated yet thus the assumption that the modis lai at fi jok is overestimated then results in the overestimation of gpp is not suggested based on the abovementioned analysis the reason for overestimating of gpp using bepsd at fi jok is not clear yet further work and discussion need to be done on this question 4 3 representativeness of ndvi for chlorophyll ndvi retrieved from remote sensing images represents not only leaf chlorophyll but also lai daughtry and jr 2008 juh et al 2008 eitel et al 2009 eraymondjr et al 2011 however the reported good relationship between ndvi and lcc eraymondjr et al 2011 dian et al 2016 and that between ndvi and ccc wu et al 2010c dian 2011 suggest that ndvi is a good indicator of chlorophyll and can be used to quantify the effect of chlorophyll on v m in ecosystem models therefore using satellite retrieved ndvi to reflect the variation of chlorophyll is appropriate there is difference however between the band range of the leaf scale spectral reflectance and that of satellite derived spectral reflectance which is used to compute spectral index in this study ndvi herein we list some band ranges and band centre positions table 7 dian 2011 and wu et al 2010c demonstrated that the relationship between either leaf scale ndvi or satellite derived ndvi and ccc can be described as an exponential function implying that the band ranges used to compute ndvi have no substantial impact on the relationship structure between ndvi and ccc moreover previous studies show strong correlation between ndvi and chlorophyll for various vegetation types and species including maize and winter wheat wu et al 2010c xia et al 2012 as well as other pfts including grass trifolium repens needle leaf platycladus orientalis juniperus fornosana hayata cedrus deodara broadleaf cinnamomum camphora ligustrum lucidum camellia oleifera abel et al and bamboo phyllostachys pubescens mazel ex h de lehaie dian 2011 this suggests that the close relationship between ndvi and chlorophyll is reliable and it is feasible to use ndvi as an indicator of chlorophyll at both leaf and canopy scales previous studies show that ndvi becomes saturated at a high vegetation cover fraction yan et al 2005 xu and meng 2016 in addition high chlorophyll content generally corresponds to high fractional cover however the constraint of chlorophyll on v m mostly occurs at low fractional cover when ndvi is not saturated consequently the chlorophyll constraint factor designed using ndvi in this study avoids the problem of ndvi saturation 4 4 further understanding of ndvimin as stated in section 2 3 ndvi min in eq 28 represents the boundary value of ndvi when chlorophyll exerts the largest limitation on v m to help us understand the meaning of ndvi min we make an analogy between ndvi min and the minimum temperature tmin for vegetation photosynthesis when temperature is lower than the minimum temperature photosynthesis stops or nearly stops yu and wang 2010 ndvi min designed to describe the full limitation of chlorophyll has a similar meaning with tmin for vegetation photosynthesis when ndvi is lower than ndvi min the chlorophyll limitation reaches the highest level minimum f ndvi as ndvi min is designed to describe the complete chlorophyll constraint condition which may not be reached under natural conditions it is a theoretical value rather than the real minimum ndvi among the ndvi time series of each pft table 8 lists the ndvi min of each pft attained by the 5 percentile method the 5 percentile ndvi of each pft is averaged from the 5 percentile ndvi at the sites of the corresponding pft it can be seen that the ndvi min values calculated with the 5 percentile method are higher than those calculated with the trial and error method for most pfts except dbf and cro then if the mean 5 percentile ndvi was taken as ndvi min f ndvi would be underestimated for pfts that cannot reach the full chlorophyll constraint condition under natural conditions 5 conclusions the bepsd model has been improved and evaluated at 41 flux sites across europe a new approach to water stress factor f w calculation is coupled into bepsd to replace the original method and a constrain factor derived from ndvi f ndvi is introduced to reflect the effect of chlorophyll on v m bepsd coupled with the new f w and the introduced f ndvi mitigates the overestimation of gpp under drought and during early spring late fall and winter obviously and performs better than the original bepsd the main conclusions are as follows 1 evaluation of the original bepsd shows that it yields satisfactory results for enf mf and gra whereas it does not performs very well for other pfts the original bepsd overestimates gpp under drought during spring late fall and winter inaccurate estimates of f w in the original bepsd leads to overestimation of gpp under drought neglecting the constraint of chlorophyll on v m is responsible for the overestimates of gpp during spring late fall and winter 2 the new f w calculated using the method of bai et al 2017 captures drought conditions and performs better than the original f w in bepsd particularly for pfts at ebf dbf csh and osh the introduced f ndvi can characterise the constraint of chlorophyll on photosynthesis during the vegetation growth cycle and performs satisfactorily for dbf and cro the improved bepsd with the new f w reduces the overestimation of gpp under drought and bepsd with f ndvi can effectively mitigate the excess estimates of gpp of dbf and cro during spring late fall and winter 3 bepsd coupled with both the new f w and the introduced f ndvi performs best among the improved versions of bepsd for each pft in this study and can be used to simulate gpp across europe acknowledgements we are grateful to anonymous reviewers and the editor for their valuable comments and suggestions which have greatly helped us to improve the original version of the manuscript this work was supported by the national key research and development program of china no 2016yfd0300101 the natural science foundation of china no 31571565 no 31671585 key basic research project of shandong natural science foundation of china no zr2017zb0422 and taishan scholar project of shandong province many thanks to prof jingming chen for sharing the daily beps model we sincerely appreciate dr gang mo dr fangmin zhang prof peijuan wang dr xuegang mao and prof xianfeng feng for their contributions during beps simulation we also would like to thank the language editor of enago www enago cn for the english language review this work used eddy covariance data acquired and shared by the fluxnet community including these networks ameriflux afriflux asiaflux carboafrica carboeuropeip carboitaly carbomont chinaflux fluxnet canada greengrass icos koflux lba necc ozflux tern tcos siberia and usccc the fluxnet eddy covariance data processing and harmonisation was performed by the icos ecosystem thematic centre ameriflux management project and flux data project of fluxnet with the support of cdiac and the ozflux chinaflux and asiaflux offices we greatly offer our profound appreciation to all the providers of the freely available data appendix a appendix b supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j ecolmodel 2017 11 023 appendix b supplementary data the following is supplementary data to this article 
25358,radiation is a direct or indirect driver of essentially all biophysical processes in plant systems and is commonly described through the use of models because of its complex distributions in time and space detailed radiation transfer models that represent plant scale heterogeneity have high computational resource requirements thus severely limiting the size of problems that can be feasibly considered while simplified models that can represent entire canopies usually neglect heterogeneity across a wide range of scales this work develops new methods for computing radiation absorption transmission scattering and emission using ray tracing approaches that can explicitly represent scales ranging from leaves to canopies this work focuses on developing a new reverse ray tracing method for describing radiation emission and scattering that ensures all geometric elements e g leaves branches are adequately sampled which guarantees that modelled radiative fluxes are bounded within a reasonable range of values regardless of the number of rays used this is a critical property when complex model geometries are used which can be subject to severe sampling errors even when very large ray counts are used the presented model uses graphics processing units gpus along with highly optimized software to efficiently perform ray object intersection tests in parallel this allowed for the simulation of 500 fully resolved trees on a desktop computer in under five minutes keywords functional structural plant model graphics processing units radiation model ray tracing 1 introduction a fundamental challenge in studying plant systems is understanding how processes of interest translate across the wide range of relevant scales ehleringer and field 1993 plant biophysical processes are often studied locally at the organ level which are coupled with other plants by environmental processes that traverse the range of scales from leaf to canopy or beyond directly measuring physical processes across this wide range of scales is typically not feasible and generally requires the use of a model at some level however representing this range of scales in models is also a considerable challenge and requires significant simplifications in order to make problems tractable this means that models usually seek to represent average or representative behaviour and cannot directly resolve plant scale heterogeneity e g sinclair et al 1976 harley and baldocchi 1995 depury and farquhar 1997 functional structural plant models fspms are a relatively new tool in modelling biophysical processes in plant systems and seek to describe the three dimensional development of plant structure over time as influenced by their local environment and physiological function vos et al 2010 these models consist of a coupled set of sub models that describe various processes involved in plant development such as photosynthesis nutrient water transport carbon allocation and plant architecture fspm development has progressed rapidly and holds great potential to aid in our understanding of complex plant system topologies across scales otherwise inaccessible through traditional experimentation however despite the continued increase in computing power fspms are often limited in terms of the range of scales they can feasibly represent most fspms represent the plant at the leaf and branch scales but are typically only able to represent one to a few plants depending on plant size and model complexity before computational cost becomes prohibitively expensive e g allen et al 2005 pearcy et al 2005 ma et al 2008 vos et al 2010 sarlikioti et al 2011 in many cases this can limit their application in studying plant to plant interactions and competition at the field or ecosystem level a bottleneck in fspm computations is the calculation of radiation fluxes absorbed by plant tissues which directly or indirectly drives nearly all sub models of physiological processes faithfully modelling the transport of radiative energy is complex particularly when accounting for scattering by millions of elements e g leaves branches most physiological processes have a strong temperature dependence johnson and thornley 1984 thus if temperature is to be included in the model radiative emission typically must be considered which adds considerable complexity as each individual element in the domain of interest interacts directly through emission rather than indirectly through scattering because of these challenges models must make compromises in terms of complexity and scale of representation the more complex the radiation model the smaller the problem size can be considered a very wide range of three dimensional methods are available to model the transport of sunlight in plant systems e g ross 1981 myneni 1991 chelle and andrieu 1998 widlowski et al 2013 but relatively few are able to model emission of terrestrial radiation the radiosity method is the standard approach for modelling radiative emission between surfaces or elements and solves a coupled set of equations that represents radiation exchange due to emission and reflection by every element in the domain of interest goel et al 1991 modest 2003 although the radiosity approach is robust it can quickly become prohibitively expensive as the problem size is increased this is due to the fact that the radiosity approach involves solving an n n system of equations n being the total number of elements in the domain which has computational expense that scales as n 3 cf press et al 2007 other radiosity based methods have been developed to improve this scaling by using a multi scale approach such as the nested radiosity approach of chelle and andrieu 1998 which simplifies the contributions due to distant sources of radiation when small elements are present in ray tracing simulations so called reverse ray tracing can be used in which radiation is traced backwards by launching rays from elements toward sources which is common in both computer graphics applications e g shirley and morley 2003 and canopy modelling applications e g lewis and muller 1992 north 1996 lewis 1999 cieslak et al 2008 this method has the advantage that every element is guaranteed to be sampled and thus are more robust when computational cost limits the number of rays that can be afforded the disadvantage of reverse methods is typically most apparent when scattering of radiation is considered when a reflection or transmission event occurs along a ray path the ray traversal becomes irreversible meaning that tracing of the ray in forward or backward directions is no longer equivalent this means that more complicated methods must be devised to deal with scattering when the reverse approach is used which typically leads to increased requirements for memory and run time currently methods are not available to effectively model emission of terrestrial radiation using a reverse ray tracing approach using forward methods to model emission in the case of small geometric elements presents similar problems when modelling shortwave radiation although they may be more severe in the case of emission bailey et al 2016 noted that using a forward tracing method with very small elements resulted in significant sampling errors unless a very large number of rays were used which could result in substantial errors in the modelled net radiative flux when coupled with the energy balance equation it was also noted that very large errors in temperature could result which led to problematic violations of the second law of thermodynamics this paper presents a consistent approach for computing radiative emission in fully resolved plants using what can be considered a reverse ray tracing approach adapted to emission it was hypothesized that using a reverse ray tracing approach for emission would reduce errors in the modeled net longwave radiation flux the method is generalized to devise a means for modelling radiation scattering which is used to produce a complete reverse ray tracing model of radiation transport due to collimated point or terrestrial sources of radiation the ultimate goal was to develop a model for computing the three dimensional net radiative flux distribution in fully resolved canopies that is efficient enough to simulate canopy scale problems over seasonal time scales in a feasible amount of time 2 model description 2 1 element geometry it is assumed that the environment of interest is populated by a large number of discrete planar objects which are termed elements in this work three possible element types will be considered fig 1 which can be combined to form any arbitrary geometry patch a patch is a planar rectangle defined by four vertices the patch normal vector n e is defined using a right hand rule i e following vertices in an anti clockwise pattern yields an upward pointing normal alpha mask an alpha mask also known as a transparency mask is the same as a patch except that a portion of the patch is removed by specifying a two dimensional grid of pixels that determines whether or not material is present the pixel grid is specified using the alpha or transparency channel of a png image file triangle a polygon defined by three arbitrary vertices the triangle normal vector n e is also defined using a right hand rule 2 2 radiative bands arbitrary radiation wavelength bands can be simulated in the model by assigning the appropriate radiative properties to elements for example the photosynthetically active radiation par band 400 700nm wavelengths is important when considering sunlight interception by leaves but typically unimportant when considering radiative emission by leaves since they emit essentially no radiation in this wave band wavelength bands are represented by defining the appropriate radiative properties and emission sources for the band of interest each element is assigned total hemispherical radiative properties of ε emissivity ρ reflectivity τ transmissivity and α absorptivity where ρ τ α 1 modest 2003 here the term total indicates a property that is integrated over the spectral band of interest which is formally defined below e g total reflectivity 1 ρ λ 1 λ 2 ρ λ i λ d λ λ 1 λ 2 i λ d λ where ρ λ is the spectral reflectivity as a function of wavelength λ i λ is the spectral intensity of the radiation source and λ 1 and λ 2 are respectively the lower and upper wavelength limits of the band spectral radiative properties can be obtained by direct measurement using a spectroradiometer or by consulting available spectral databases e g hosgood et al 2005 kotthaus et al 2014 for simplicity the spectral intensity and radiative properties are often assumed to be constant over a certain radiative bands such as the par band 2 3 radiative emission if the emissivity and absolute temperature of an element is nonzero the element acts as a source of radiation through emission the emissive flux of an element can be calculated through the stefan boltzmann law as εσt 4 where σ 5 67 10 8 wm 2 k 4 is the stefan boltzmann constant and t is the absolute temperature of the element ambient emission from the surroundings may also be present which is not directly resolved by elements given these emitted fluxes the following procedure describes the method for calculating the absorbed flux due to emission by other elements and the surroundings illustrated graphically in fig 2 1 sampling element surfaces n rays discrete x y z points are randomly sampled on the surface of each element in the present implementation n rays is specified as a constant for all primitives the sampling of patch surfaces is accomplished by choosing n rays sets of two random numbers from a uniform distribution and mapping them to the surface of the rectangle cf suffern 2007 the same procedure is used for alpha masks except that if points lie in a transparent region they are re sampled until they are no longer in the transparent region there are several possible methods for sampling points on an arbitrary triangle the simplest of which is the rejection method which involves sampling points on a unit rectangle as in patches and discarding points that lie outside of the triangle however the method used here was to uniformly sample points on a right triangle i e half of a patch then apply an affine transform to map them onto the arbitrary triangular element 2 sampling ray directions ray directions are sampled according to a standard cosine weighted distribution 2 θ r sin 1 n t 3 ϕ r 2 π n p where n t and n p are random numbers drawn from a uniform distribution and θ r and ϕ r are respectively the ray zenithal and azimuthal directions for a horizontally oriented element the actual ray directions for an arbitrarily oriented element are found by converting θ r and ϕ r into a cartesian unit vector and rotating it into the direction of the element normal in practice jittered sampling was used for the calculation of n t and n p to improve sampling convergence this involves generating a uniform grid of n t and n p between zero and one then drawing a random number that gives a displacement that lies between the next adjacent point see suffern 2007 for further details a leaf is a thin surface and could be represented by placing two elements back to back to save computational time and memory an option can be enabled to emit rays from both sides of an element for example an element making up the ground or trunk only needs to emit radiation from one side while a leaf emits from both sides 3 path tracing the rays are then launched from the sampled points away from the element and ray object intersection tests are performed suffern 2007 there are one of two possible outcomes for a given ray 1 the ray hits another element in this case the emitted flux of the intersected element is queried q εσt 4 and used to calculate the weighted flux value associated with that ray q n rays for alpha masks the alpha map must be queried to determine if the ray intersection occurred on a transparent portion of the element if so the ray effectively continues propagating as if no intersection had occurred 2 the ray does not intersect any other elements in this case the weighted flux value of the ray is that of the ambient flux q amb wm 2 sr 1 which is the directional hemispherical flux of an unobstructed surface for the wave band of interest if the ambient radiation is isotropic the ambient flux can be assigned a constant hemispherical value q amb wm 2 and the weighted ray flux is simply q amb n rays if the ambient flux is anisotropic a directionally dependent value could be used that varies based on the direction of ray propagation the ambient flux could be measured using a radiometer equipped with a shadowband e g horowitz 1969 or modeled e g harrison and coombes 1988 since the entire hemisphere around the element is sampled there is no explicit need for the treatment of boundary conditions elements near the edge of the domain may intercept some ambient radiation propagating upward which replaces the radiation that would have been intercepted from ground elements were the element near the middle of the domain the weighted flux of the ray is partitioned into absorbed reflected and transmitted components the absorbed component 1 ρ τ q n rays is accumulated summed into an absorbed buffer corresponding to the element from which the ray was launched the reflected component ρq n rays is accumulated in a to be scattered buffer there is one to be scattered for each side of the element and the reflected flux is accumulated in the buffer corresponding to the same side that was hit i e if the ray hit the top side the energy is accumulated in the buffer corresponding to the top side the transmitted component τq n rays is accumulated in a to be scattered buffer in a similar manner except that the transmitted flux is accumulated in the buffer corresponding to the opposite side than was hit i e if the ray hit the top side the energy is accumulated in the buffer corresponding to the bottom side 4 reflection iterations energy in the elements to be scattered buffers needs to be traced as it leaves the elements in the present model implementation it was assumed that reflection and transmission is lambertian measurements indicate that strictly speaking this assumption is nearly always false with specular reflection and scattering of transmitted radiation playing a clear role in creating anisotropy in reflected and transmitted radiation woolley 1971 bousquet et al 2005 combes et al 2007 while this is usually an incorrect assumption in a strict sense chelle 2006 demonstrated that non lambertian effects play a minimal role in overall radiation absorption distributions when considering the incident flux due to scattering by many leaves employing the above assumptions scattering calculations proceed in essentially the same manner as for primary emission the only differences are that the ray strength is assigned based on the to be scattered flux rather than the emissive flux and that the ambient flux is set to zero for scattering iterations the user specifies the maximum allowable to be scattered energy e g 0 1wm 2 and a maximum allowable number of scattering iterations e g 10 scattering iterations proceed until the maximum to be scattered energy among all elements is below the user defined threshold or the number of scattering iterations has exceeded the user defined maximum value in either case any remaining energy in the scattering buffers is absorbed by the element to ensure conservation of energy in cases where ρ and τ are small such as in the par band few iterations are required if ρ or τ are large such as in the near infrared band several iterations may be required however since reflected and transmitted energy decreases exponentially with each iteration the number of required scattering iterations is still usually low e g 5 10 depending on user defined thresholds the required number of scattering iterations will be further explored in section 3 1 1 2 4 external radiation sources 2 4 1 collimated radiation sources standard approaches are used to calculate the primary incident flux i e before scattering due to a collimated source of radiation such as the sun fig 3 collimated radiation is defined as radiation in which all beams of radiation are parallel although emission by the sun is not collimated the large distance between the sun and earth means that radiation emitted by the sun that is intercepted by earth is approximately collimated for collimated radiation points are randomly sampled on the surface of elements in the same way as for emission the ray direction is simply defined by a line connecting the sampled point and the radiation source the rays are then traced from the element toward the source and intersection tests are performed in the same was as for emission rays that do not intersect any other elements are assigned a value of 4 s r q s n e n s n rays where q s is the source s total hemispherical radiative flux w m 2 over the band of interest just above the domain in the direction of n s q s could come from a sensor measurement for example if the measurement of a horizontal sensor was 500w m2 for the direct solar flux q s 500 cosθ s w m2 where θ s is the solar zenith angle or from a model of solar transmission e g liu and jordan 1960 gueymard 2008 absorption and scattering then proceed in the same fashion as with emission if there is a diffuse flux associated with a given source the ambient space can be sampled during the first scattering iteration 2 4 2 point sources if the radiation source can be approximated as emenating from a point e g a lamp in the distance the ray direction is a line that connects the radiation source and a point sampled on the element surface fig 4 the strength of rays that do not intersect any other elements is calculated by integrating incident radiation over all possible directions emitted by the source 5 s r q p n rays n θ n ϕ j 1 n ϕ i 1 n θ n e n r n r n s θ i ϕ j π s 2 δ where q p is the total emitted radiation flux at the source s surface wm 2 over the band of interest n r is a unit vector originating at the point sampled on the element and pointed toward the radiation source s is the distance from the point sampled on the element to the source n s θ i ϕ j is a unit vector originating at the location of the point source and oriented in the spherical direction θ i ϕ j where there are n θ n ϕ total discrete spherical directions θ i is calculated as cos 1 1 2 i 0 5 n θ and ϕ j is calculated as 2 π j 0 5 n ϕ δ is a function that is equal to 1m2 if n r n s 0 and equal to 0 otherwise whose purpose is to discard radiation emitted from the back side of the source 2 5 gpu acceleration in general ray tracing calculations can become costly when performed in serial due to the large number of rays required the computer graphics community has used graphics processing units gpus for decades to accelerate ray tracing calculations a gpu is a specialized piece of hardware with a large number of streaming multiprocessors designed to crunch numbers in parallel several frameworks have been developed to simplify the task of gpu programming such as nvidia s cuda framework and opencl in this work the above ray tracing algorithms were implemented using nvidia s optix ray tracing library which is a highly optimized and generalized gpu ray tracing framework optix does not actually perform any ray tracing per se rather it is a flexible framework for managing data and tasks associated with ray tracing for example the user writes functions that describes how rays are launched what constitutes a ray object intersection and what should happen in the event of an object hit or miss 3 results 3 1 accuracy and consistency tests typically radiation models are tested by comparing results against real world data collected in the field i e validation performing rigorous validation for detailed radiation models presents several considerable challenges first extremely detailed measurements are required to specify geometry within the model since individual leaves and branches are represented in the model it would be necessary to accurately specify the geometry of every leaf and branch in the model if leaf level measurements are to be reproduced another option is to validate using aggregate measurements which have looser requirements for geometry specification however this generally aggregates many different processes and measurement errors making it difficult to separate natural environmental variability measurement errors and model errors another approach that is perhaps of greater value when evaluating these types of detailed models is to verify model predictions for relatively simple cases where the exact answer is known since the exact answer is known all errors in the model predictions must be due to model errors or errors in implementation this verification over validation approach has been favored by some in the remote sensing community and has been used for model intercomparisons pinty et al 2001 widlowski et al 2013 in the majority of their tests the exact answer was not known but rather the aggregate of predictions by different models was used as a relative measure of model performance it should be noted that these tools developed by the remote sensing community focus on the distribution of reflected canopy level radiation and not on absorption by individual elements which is the focus of this work here the model was assessed by ensuring that basic consistency and accuracy conditions were satisfied using problems where the exact answer was known accuracy was verified not only by relative error measures but importantly by ensuring that the error continually decreases as more rays are used i e the solution converges to the exact answer future work will focus on creating new datasets that can be used to more accurately assess real world errors in model predictions 3 1 1 accuracy and convergence to verify the model and make assessments regarding accuracy several sample problems were solved that are simple enough that the exact answer can be calculated fig 5 the test cases were designed to progressively evaluate different aspects of the model test case parameters such as dimensions radiative properties and temperatures were chosen arbitrarily but with the general goal in mind of providing tests of individual model components and demonstrating the associated error convergence as the number of rays was varied each of the test cases are detailed and solved analytically in appendix a these details have been included to provide standard cases against which future models can be verified the performance of the proposed model is contrasted against that of a traditional forward ray tracing model in the forward model rays are launched from the source from which they originated which may be an element a collimated source a point source or an ambient source in which case the rays originate from the surface of a sphere that bounds all elements in the domain a detailed description of the implementation of the forward tracing method is given in appendix b an ensemble of 500 simulations were performed for each case and the average error was quantified using the normalized root mean square error which can be defined mathematically as 6 nrmse 1 r ex i r i r ex 2 1 2 where r i is the modeled value of the absorbed radiation flux for simulation i in the ensemble and r ex is the exact value of the absorbed flux case 1 fig 5 was designed to test collimated radiation absorption of a patch and subsequent reflection to an adjacent patch as well as patch emission the reflectivity for radiation originating from the collimated source chosen to be ρ 1 0 3 and ρ 2 0 for emission parameters were chosen to be ε 1 ε 2 1 t 1 300 and t 2 0 the patch size l is arbitrary and was chosen to be equal to 1 fig 6 reports the normalized error in the absorbed flux for patches 1 and 2 and absorbed flux by patch 2 due to emission by patch 1 using the reverse method the error in absorbed radiation for patch 1 is always zero regardless of the number of direct rays this is a consequence of the fact that an unobstructed element always perfectly samples radiation sources a desirable quality of the model note that the reverse method is not shown in fig 6a because a vale of zero cannot be represented in a logarithmic plot for the forward method the absorbed flux for patch 1 is strongly dependent on the chosen number of rays with the error decreasing exponentially with ray count this exponential decrease in error with ray count is a standard result for forward ray tracing simulations widlowski et al 2013 the radiation flux absorbed by patch 2 was strongly dependent on the chosen number of rays for both the forward and reverse methods and converged to the exact solution at approximately the same exponential rate fig 6b however the error in the forward method flux was consistently about four times larger than that of the reverse method it appears that this is due to the fact that the flux scattered to patch 2 is dependent on proper sampling of the direct radiation by patch 1 since the reverse method is much more accurate than the forward method at predicting the potential amount of radiation available to be scattered from patch 1 it is consistently more accurate in predicting the absorbed flux by patch 2 due to scattering by patch 1 however the diffuse sampling of patch 2 by patch 1 is approximately the same for both methods as indicated by the similar performance by each method in predicting the absorbed radiation flux by patch 2 due to emission by patch 1 fig 6c due to the symmetry of this particular problem using the forward or reverse methods are essentially equivalent in the simulation of radiative emission by the elements case 2 fig 5 was designed to test sampling of an isotropic ambient radiation source radiative properties were chosen to be ρ 1 ρ 2 0 the geometry was chosen such that a 1 b 2 and c 0 5 for both the forward and reverse methods the error in the modelled absorbed flux decreased at approximately the same exponential rate fig 7 however the error in the forward method was consistently about four times larger than that of the reverse method for either method the surface of patch 2 must be adequately sampled by patch 1 in order to calculate the amount of ambient radiation patch 1 intercepts for the reverse method this sampling is being performed from the point of view of patch 1 whereas for the forward method the sampling is being performed from the point of view of the surface of the domain bounding sphere which is on average further away thus the reverse method is more accurate because it better samples the radiation occluded from view of patch 1 by patch 2 but the overall convergence rate is the same case 3 fig 5 was designed to test emission exchange between gray patches and subsequent infinite reflections here both the number of diffuse rays and the maximum number of scattering iterations was varied and only patch emission was considered radiative properties were chosen to be ε 1 ε 2 0 6 and the geometry was specified the same as in case 2 when no scattering computations are allowed both the forward and reverse methods are essentially equivalent because of geometric symmetry as in case 1 in this case there is a small amount of initial decrease in the model error as the number of rays is increased but eventually no improvement is achieved by using more rays given that scattering must be included to correctly predict the absorbed flux fig 8 as the maximum number of scattering events is increased the error decreases at an equivalent exponential rate for both methods note that this is dependent on the particular method used for scattering in the forward ray tracing approach since for every ray object intersection energy is both absorbed and reflected the forward and reverse methods are equivalent many implementations of forward ray tracing use a stochastic approach to determine whether absorption or reflection occurs see appendix b in this stochastic implementation the number of rays available after each scattering event decreases exponentially thus leading to decreased accuracy and a slower rate of convergence when compared to the reverse ray tracing approach not shown case 4 was designed to test emission from a single point radiation source and subsequent absorption by a patch the patch reflectivity was ρ 1 0 3 and the geometry was specified as a 1 b 2 and c 1 n θ and n ϕ were set to a constant value of 30 in the case of the reverse tracing method for both the forward and reverse methods the error decreased approximately exponentially as the number of direct rays was increased fig 9 however the error for the reverse method declined at a faster rate than the forward method as the number of rays was increased on average the error in the forward method was about fifty times larger than in the reverse method the reason for this is similar as in case 1 which is due to the fact that in the reverse method the source is perfectly sampled by the patch whereas in the forward method the source must accurately sample a relatively small fraction of the surrounding spherical space the difference between case 1 is that the source rays have angular dependence and thus even in the reverse method the surface of the patch must be sampled in order to properly calculate the solid angle subtended by the patch with respect to the source therefore the error still has an exponential dependence on ray count even for the reverse method 3 1 2 conservation of energy 1st law of thermodynamics perhaps the most fundamental consistency requirement for a radiation model is that energy is conserved essentially this means that no energy is gained or lost due to transport energy can only be relocated for collimated radiation the total energy rate incident on the domain is equal to q s cosθ s a t where a t is the total footprint area of the domain θ s is the solar zenith angle and we note that units are in energy per time for diffuse radiation the total incident energy rate is q d a t where q d is the diffuse radiation flux on a horizontal surface for emission the total energy rate emitted by all elements is i n ε i σ t i 4 a i in order to evaluate conservation of energy we simply calculate the radiation energy rate absorbed by all elements including energy emitted reflected to the sky and compare this value with the total energy rate we started with if energy is conserved the two should be equal in theory any consistent radiation model e g ray tracing radiosity should be able to satisfy energy conservation as mentioned previously the method described in this work only approximately satisfies conservation of energy this is because the amount of energy that actually leaves an element is dependent on the element being properly sampled by other elements to test conservation of energy a simple domain was created with 140 140 patches of size 0 3 0 3m2 tiled to create a ground surface with four trees spaced a 7m that were 6m tall on average fig 11 three dimensional almond tree geometries were created using the model of weber and penn 1995 and placed into the domain at uniform spacing each tree consisted of about 6000 triangles making up the trunk and branches and about 30 000 alpha masks making up the leaves as detailed in weber and penn 1995 each geometric parameter is randomly perturbed to make each tree geometry unique the shortwave reflectivity and transmissivity of the leaves were arbitrarily set to 0 2 for testing purposes and the emissivity was set to 0 9 all elements were assigned temperatures of 300k for this case it was found that the error in conservation of energy was about 0 01 of the source flux for a collimated source and 0 1 of the ambient flux for emission even if as few as 50 rays per element were used this indicated that conservation of energy was well satisfied with errors only slightly larger than machine precision single precision 3 1 3 equilibrium distribution 2nd law of thermodynamics a requirement of equal importance as conservation of energy is that the radiation distribution must tend toward a uniform equilibrium state or that entropy cannot decrease in the absence of external sources or sinks this is a critically important requirement since if the solution does not revert to the correct equilibrium state we can have little confidence that any observed features of the calculated radiation distribution under non equilibrium have physical meaning testing that this requirement is enforced is relatively simple and can be performed as a precursor check before every simulation the temperature of every element is set to some value t and the ambient flux emanating from the sky is set to σt 4 regardless of geometry and values of ε the net radiative flux for all elements should be exactly zero the orchard test case with four total trees was used to test model adherence to the 2nd law and the total number or rays were varied to illustrate its effect on errors in the net radiative flux fig 10 for the forward approach the total number of rays consisted of both ambient diffuse rays and emission rays emission and ambient rays are the same in the reverse approach for each of the four forward simulations the number of ambient rays was 106 107 108 and 109 and the corresponding number of emission rays per element was 3000 5000 6000 and 10 000 note that some elements are two sided and use twice the number of emission rays the reverse ray tracing approach has the very desirable property that the proper equilibrium distribution will always be reached to within machine precision even with only one ray sample per element this means that in the case where the emitted flux from all elements is equal to the diffuse ambient flux the net modelled radiative flux for all elements is essentially zero about 10 7 on average for this case when the standard forward ray tracing method is used adherence to the 2nd law is strongly dependent on the number of rays in relation to simulated geometry fig 10 for a modest number of rays i e 200 million the standard error was extremely large at about 150wm 2 which is about a third of the ambient emitted flux value σt 4 as the number of rays is increased the error in the 2nd law steadily decreases however even when a very large number of rays was used i e 4 trillion the error in the net radiation flux was still significant with a standard error of about 40wm 2 the spatial distribution of this error is visualized in fig 11 for this ray count the range of errors was still very large with the minimum net flux having a value of 459wm 2 i e no rays sampled the element and the maximum net flux having a value of about 9000wm 2 these extreme values corresponded to triangular elements comprising the woody structure of the trees and occurred at locations of branching splitting or where there were highly skewed elements extreme values could certainly be filtered or the quality of the triangular mesh could be improved however this illustrates a problem with using arbitrary meshes with the forward ray tracing approach that the reverse approach is able to deal with in a straightforward manner the reason that the reverse ray tracing method significantly outperforms the forward method is because it samples the absorbed flux due to both ambient sources and other elements in the exact same way therefore any error in the absorbed ambient flux is exactly offset by the absorbed flux due to other elements for the forward ray tracing method the absorbed ambient flux is sampled from the point of view of the domain bounding sphere and absorbed emission due to other elements is sampled from the point of view of the other elements each of these samplings can contain substantial and unrelated errors which can lead to extremely large errors in the net radiative flux 3 2 visualizations sample visualizations of model outputs are given in fig 12 and are simply meant to provide a visual representation of model capabilities in fig 12 each element is coloured based on a mapping of its absorbed radiation flux to the color scale shown in the figure a flat ground surface was also created by tiling patches of size 0 1 0 1m2 which resulted in roughly 1 1 million patches there were 121 total trees in the orchard which were 6 m tall and spaced at 7m in total the scene depicted in fig 12a contains roughly 5 5 million elements fig 12a shows a pseudocolor plot for the total absorbed direct and diffuse solar radiation flux in the simulated orchard the reflectivity and transmissivity of the leaves were set to 0 05 which is characteristic of the photosynthetically active radiation par band jones and vaughan 2010 and the rest of the surfaces were assumed to be black the collimated radiation flux was set to 800wm 2 and the ratio of the direct to isotropic ambient radiation flux q s q amb was 4 the collimated radiation flux zenithal angle was 45 the number of direct and diffuse rays were set to 100 and 200 respectively fig 12b depicts direct radiation transport from several point radiation sources corresponding to artificial lamps in a greenhouse the three dimensional geometries were read into the code using a standard polygon file that can be generated using nearly any 3d modelling software e g autocad autodesk inc blender www blender org the plants and pots consisted of about 6000 triangles each each of which were identical except that they were given a random azimuthal rotation patches were tiled to make the greenhouse walls and triangles were tessellated to make lamp housings in total the simulation consisted of about 2 million elements 3 3 scaling of model run time a test case was created to demonstrate model scaling up to very large domain sizes the tests were performed on an nvidia gtx titan card which is a consumer grade gaming gpu the same general orchard geometry was used as described in section 3 2 the number of trees was incrementally increased and the execution time required to simulate both collimated and diffuse radiation for a single band at a single time of day was recorded since the purpose of this exercise was simply to demonstrate scaling of run time the reflectivity and transmissivity of the leaves were arbitrarily set to 0 1 and the rest of the surfaces were assumed to be black the number of direct and diffuse rays were chosen to be 100 and 200 respectively the number of scattering iterations was fixed at 0 and 3 to test its effect on execution time and scaling by fixing the number of scattering iterations this removes any effect of the leaf radiative properties on the execution time because scattering will continue in the same manner regardless of the amount of energy to be scattered the execution time scaled linearly with number of elements fig 13 for the range of domain sizes examined above about 22 5 million elements the gpu ran out of memory at this point larger problems could be simulated by utilizing additional ram on the host which comes with an increase in run time due to the low bandwidth between the gpu and host memories once the host runs out of memory larger problems could still be simulated by utilizing swap space on the host hard drive which comes with an even larger increase in run time scattering iterations are expected to incur a similar expense as performing the diffuse radiation calculation without scattering however the first scattering iteration is essentially free when performing diffuse calculations because diffuse calculations and the first scattering iteration can be performed at the same time thus adding three scattering iterations resulted in an increase in execution time of almost exactly 100 one downside with the method used in this work is that the run time is virtually independent of the radiative properties of the elements whereas in traditional forward ray tracing reflection transmission rays are not launched if the surface is black since the ray energy becomes zero see appendix b in the reverse method each element samples the space around it and thus it cannot assume that any neighboring elements are black until it has actually sampled them results showed that for this specific geometry and computational hardware the run time was about 12 5 seconds per million elements with no scattering for additional scattering iterations this run time can be multiplied by n where n is the number of scattering iterations if we were to consider simulating a single tree at hourly timesteps for an entire growing season assume an average of 12h of sunlight per day 200 growing days and no scattering we would find the simulation time to be roughly 15min which is quite feasible if the number of trees were to be increased by 100 the execution time would be roughly one day which is still feasible considering the complexity represented there are of course opportunities to reduce this execution time by not performing calculations every day upgrading the gpu or by using multiple gpus 4 discussion and conclusions complex model geometries with small elements typically cause problems for traditional forward ray tracing approaches e g kimes and kirchner 1982 north 1996 gastellu etchegorry et al 2004 that trace radiation away from its source this is because they require an incredibly large number of rays to adequately sample small elements this forward tracing approach was used to perform simulations of four fully resolved trees to determine whether the model satisfied an equilibrium condition i e if a constant radiative flux is emitted from all objects the net flux should be zero for all objects results showed that there was significant error in the net radiative flux standard error of about 10 even when trillions of rays were used perhaps more concerning was that there were many outlier elements that had extremely large errors 100 it is fairly well known that when elements are small and sampling errors become problematic reverse ray tracing approaches offer a viable solution north 1996 lewis 1999 disney et al 2000 cieslak et al 2008 here radiation is traced from the object back toward the source and therefore all objects are adequately sampled the trade off is that when scattering is considered the methods become more complicated because radiation propagation is not reversible when scattering occurs furthermore reverse tracing models are not readily available to incorporate radiative emission which is needed to calculate the net all wave flux in this work a model was developed that utilizes a reverse tracing approach to calculate the net radiative flux for every element in the simulated domain the forward and reverse tracing methods were shown to be equivalent in the case of symmetric radiative exchange between two objects the difference between the methods becomes evident when there is high asymmetry such as in the case of the sky emitting radiation towards small elements in the case of forward tracing the sky is well sampled by an unobstructed element because the sky occupies a large fraction of the spherical space surrounding the element this same element is relatively poorly sampled by the sky because the element occupies only a small fraction of the total space viewed by the sky for reverse tracing the opposite is true thus if we are concerned with absorption by the elements the reverse method typically offers better sampling of external radiation sources in the case of emission either the forward or reverse method may offer better sampling of some elements while giving worse sampling of other elements depending on the relative element sizes the advantage of the reverse method in all cases is that it guarantees the absorbed flux by any element is bounded within the range of fluxes emitted by all radiation sources even if only one ray sample per element is used this is a considerable advantage when only a limited number rays can be afforded because it guarantees that results are at least reasonable whereas the forward tracing method is likely to produce unreasonable results when modelling the net radiative flux at an element it is advantageous to use a reverse tracing approach both when sampling external sources and emission by elements it was demonstrated that the reverse tracing method used in this work is at the very least consistent with the second law of thermodynamics or equilibrium condition the reason that this method satisfies this consistency condition is because any errors in sampling the ambient radiation flux are exactly offset by errors in sampling incoming emission by other elements which is due to the fact that they are both sampled using the same reverse method the reverse ray tracing method developed in this work bears many similarities with the commonly used radiosity approach e g goral et al 1984 borel et al 1991 chelle and andrieu 1998 both of which satisfy the consistency condition described above in theory the methods are nearly the same with the major difference being in the treatment of scattering the reverse tracing approach presented in this work does not require the solution of a linear system of equations that can become costly to solve when the number of elements is large the present method was shown to scale linearly with the number of geometric elements while the computational effort associated with solving a linear system of equations increases exponentially with the number of elements press et al 2007 results of this work suggest that it is typically preferable to sample the domain from the point of view of the entity of interest that is absorbing radiation if that entity of interest is a leaf or sensor within the canopy it is typically advantageous to use a reverse ray tracing approach that ensures the entity is well sampled in remote sensing applications the entity of interest is often the distribution of radiation scattered to the sky thus it is more common to find forward tracing methods used in these applications because the domain is sampled from the point of view of the sky disney et al 2000 widlowski et al 2006 2013 in addition to performing detailed studies of radiation transport processes independently an important future application of the model is coupling with other biophysical models of plant systems since the model can represent longwave radiation exchange it can be coupled with an energy balance model to predict surface temperature throughout canopies nearly all physiological processes are strongly dependent on temperature e g photosynthesis respiration growth pest pathogen development and such information is needed by physiological models at the organ level this could be applied not only to understand impacts of heterogeneity in natural ecosystems but also help to inform or optimize the management of forestry or agricultural systems acknowledgements financial support of this work by the american vineyard foundation grant 2016 1825 and the usda national institute of food and agriculture hatch project number ca d pls 2401 h appendix a verification test case descriptions and exact solutions for completeness the exact solutions to each of the verification test cases presented in section 3 1 are given below a 1 case 1 this case consists of two orthogonal square lambertian patches that share a common edge fig 5 a collimated radiation source is located directly overhead patch 1 has a nonzero reflectivity which reflects some radiation to patch 2 which receives only this reflected radiation patch 2 is cold and patch 1 is black so the only emission exchange is direct emission from patch 1 to 2 the fraction of energy leaving patch 1 that is intercepted by patch 2 i e viewfactor f 1 2 modest 2003 is f 1 2 0 2 modest 2003 appendix d39 by symmetry f 2 1 0 2 since patch 1 is oriented normal to the radiation source the absorbed radiation flux is simply a 1 q 1 1 ρ 1 q s where q s is the collimated radiation flux normal to the radiation source direction the flux of reflected radiation leaving patch 1 is ρ 1 q s and thus the reflected flux absorbed by patch 2 is a 2 q 2 ρ 1 f 1 2 q s which is only valid for ρ 2 0 note that since patch 2 is parallel to the source direction it receives no direct radiation the radiative flux absorbed by patch 2 due to emission by patch 1 is simply a 3 e 1 f 2 1 ε 2 σ t 2 4 a 2 case 2 this case consists of two aligned parallel lambertian patches fig 5 there is an isotropic ambient radiation source radiation absorption is ignored on the lower side of patch 1 and on the upper side of patch 2 the reflectivity of both patches is zero the fraction of energy leaving patch 1 that is intercepted by patch 2 is given by modest 2003 appendix d38 a 4 f 1 2 2 π xy ln 1 x 2 1 y 2 1 x 2 y 2 1 2 x 1 y 2 tan 1 x 1 y 2 y 1 x 2 tan 1 y 1 x 2 x tan 1 x y tan 1 y where x a c and y b c by conservation of energy the fraction of diffuse radiation incident on patch 1 is 1 f 1 2 it is also noted that f 1 2 f 2 1 by symmetry thus the absorbed ambient radiation flux for patch 1 is a 5 q 1 1 f 1 2 q s by symmetry the absorbed ambient radiation flux for patch 2 is the same as that for patch 1 a 3 case 3 this case is the same as case 2 except that only emission is considered and both patches have emissivities less than unity fig 5 as a result an infinite series of reflections occurs between the two patches the viewfactors f 1 2 and f 2 1 are given by eq a 4 since there is no restriction imposed that ε 1 or ε 2 be equal to unity a radiosity approach can be used to obtain analytical expressions for energy transfer between the two patches using conservation of energy a system of equations describing radiation exchange between the two patches can be written which yields the standard radiosity equations modest 2003 a 6a e 1 ε 1 1 ε 2 1 f 1 2 e 2 σ t 1 4 f 1 2 σ t 2 4 a 6b e 2 ε 2 1 ε 1 1 f 2 1 e 1 σ t 2 4 f 2 1 σ t 1 4 where e i is the net surface flux for patch i by solving for e 2 in eq a 6a and back substituting into eq a 6b an explicit solution can be obtained for e 1 and e 2 a 7a e 1 σ 1 ε 1 f 1 2 t 1 4 f 2 1 t 1 4 t 1 4 t 2 4 1 ε 1 ε 2 1 ε 1 1 1 ε 2 1 f 1 2 f 2 1 a 7b e 2 ε 2 1 ε 1 1 f 2 1 e 1 σ t 2 4 f 2 1 σ t 1 4 finally subtracting the emitted flux gives the absorbed energy flux a 8a e 1 e 1 ε 1 σ t 1 4 a 8b e 2 e 2 ε 2 σ t 2 4 a 4 case 4 this case consists of a single point source of radiation incident on a horizontal patch which has one vertex positioned directly below the source fig 5 the patch may have nonzero reflectivity which reflects some energy from the source to the surroundings the fraction of energy leaving the point source that is absorbed by the patch is given by modest 2003 appendix d47 a 9 f p 1 1 4 π tan 1 1 d 1 2 d 2 2 d 1 2 d 2 2 where d 1 c a and d 2 c b therefore the flux absorbed by the patch is simply a 10 e 1 1 ρ 1 f p 1 q p where q p is the hemispherical flux emitted by the source appendix b standard forward ray tracing methodology the following describes the methodology used to perform forward ray tracing calculations most of what is described below can be found in chapter 11 of mahan 2002 or other standard radiation textbooks modest 2003 howell et al 2010 although some modifications have been made for the treatment of ambient diffuse radiation in traditional monte carlo ray tracing approaches individual bundles or rays of radiation are launched from their sources and intersected with various elements as they travel through the domain of interest ray origins directions and associated energy are chosen differently depending on the type of radiation source from which they originate collimated radiation sources collimated solar radiation is typically modelled by launching a large number of rays from above the domain and toward the domain in the direction of the source in order to ensure that rays cover the entire footprint of the domain a bounding sphere can be constructed that encompasses all modelled objects rays are then cast from a disk with the same diameter as the bounding sphere that is positioned tangentially to the bounding sphere see overby et al 2016 the energy assigned to each ray is q s π r s 2 n rays where q s is the radiative flux normal to the source direction r s is the radius of the domain bounding sphere and n rays is the number of direct rays launched toward the domain point radiation sources emission from a point radiation source is represented by launching rays uniformly in all directions from the location of the source the zenithal direction of each ray is chosen as θ cos 1 1 r t where r t is a number randomly drawn from a uniform distribution the azimuthal direction of the ray can be similarly specified by ϕ 2πr p where r p is an additional uniform random number the energy assigned to each ray is 2q p n rays where q p is the energy flux emitted by the source and n rays is the total number of rays launched from each source ambient diffuse radiation sources ambient diffuse radiation can be modelled by casting rays in random directions toward the domain to define a given ray we must specify both the origin of the ray and a point through which the ray passes or equivalent in the case of an isotropic ambient radiation flux rays can be defined by randomly choosing two points on the surface of the domain bounding sphere through which the ray will pass in a spherical coordinate system the zenithal angle of each point with respect to the bounding sphere center can be given by θ cos 1 1 r t where r t is a number randomly drawn from a uniform distribution the azimuthal angle of the points can be similarly specified by ϕ 2πr p where r p is an additional uniform random number the energy assigned to each ray is q amb 4 π r s 2 n rays where q amb is the isotropic ambient radiation flux and here n rays is the number of diffuse rays if the ambient flux is anisotropic this could be accounted for by varying the ray energy over the bounding sphere terrestrial emission emission by terrestrial objects is modelled by casting rays away from the surface of the objects uniformly distributed points are randomly chosen on the surface of the objects methods for doing so described in suffern 2007 which correspond to ray origins ray directions are also randomly chosen for a horizontal surface the spherical coordinates of the ray can be determined as θ sin 1 r t and ϕ 2πr p where r t and r p are uniform random numbers if the surface is inclined the chosen ray direction can be rotated into a coordinate system normal to the object surface in the case of lambertian emission the energy assigned to each ray is εσt 4 a n rays where ε is the object emissivity σ is the stefan boltzmann constant t is the absolute temperature of the object s surface a is the surface area of the object and here n rays is the number of rays cast from each object ray object intersections rays are intersected with objects to determine the closest ray object intersection point in the event of an intersection some fraction of energy will be absorbed transmitted and reflected one approach to modelling this is to choose a uniformly distributed random number r if r is less than the object reflectivity ρ 100 of the ray s energy is reflected by the object and a new ray is launched originating at the intersection point with a new random direction away from the same side of the object surface the ray intersected if r is greater than ρ but less than ρ τ τ is the object s transmissivity a new ray is launched as in reflection except from the opposite side of the object surface the ray intersected if r ρ τ 100 of the ray s energy is absorbed by the object and the ray is terminated to ensure equivalence with the reverse tracing approach the forward scattering approach used in this work was a deterministic scattering method in this case if a ray intersects an object some fraction of the ray s energy ρ is reflected by launching a new ray whose energy is equal to the product of ρ and the ray energy before scattering similarly some fraction τ is transmitted the final fraction 1 ρ τ is absorbed by the object this process continues until the ray energy falls below some user defined threshold value 
25358,radiation is a direct or indirect driver of essentially all biophysical processes in plant systems and is commonly described through the use of models because of its complex distributions in time and space detailed radiation transfer models that represent plant scale heterogeneity have high computational resource requirements thus severely limiting the size of problems that can be feasibly considered while simplified models that can represent entire canopies usually neglect heterogeneity across a wide range of scales this work develops new methods for computing radiation absorption transmission scattering and emission using ray tracing approaches that can explicitly represent scales ranging from leaves to canopies this work focuses on developing a new reverse ray tracing method for describing radiation emission and scattering that ensures all geometric elements e g leaves branches are adequately sampled which guarantees that modelled radiative fluxes are bounded within a reasonable range of values regardless of the number of rays used this is a critical property when complex model geometries are used which can be subject to severe sampling errors even when very large ray counts are used the presented model uses graphics processing units gpus along with highly optimized software to efficiently perform ray object intersection tests in parallel this allowed for the simulation of 500 fully resolved trees on a desktop computer in under five minutes keywords functional structural plant model graphics processing units radiation model ray tracing 1 introduction a fundamental challenge in studying plant systems is understanding how processes of interest translate across the wide range of relevant scales ehleringer and field 1993 plant biophysical processes are often studied locally at the organ level which are coupled with other plants by environmental processes that traverse the range of scales from leaf to canopy or beyond directly measuring physical processes across this wide range of scales is typically not feasible and generally requires the use of a model at some level however representing this range of scales in models is also a considerable challenge and requires significant simplifications in order to make problems tractable this means that models usually seek to represent average or representative behaviour and cannot directly resolve plant scale heterogeneity e g sinclair et al 1976 harley and baldocchi 1995 depury and farquhar 1997 functional structural plant models fspms are a relatively new tool in modelling biophysical processes in plant systems and seek to describe the three dimensional development of plant structure over time as influenced by their local environment and physiological function vos et al 2010 these models consist of a coupled set of sub models that describe various processes involved in plant development such as photosynthesis nutrient water transport carbon allocation and plant architecture fspm development has progressed rapidly and holds great potential to aid in our understanding of complex plant system topologies across scales otherwise inaccessible through traditional experimentation however despite the continued increase in computing power fspms are often limited in terms of the range of scales they can feasibly represent most fspms represent the plant at the leaf and branch scales but are typically only able to represent one to a few plants depending on plant size and model complexity before computational cost becomes prohibitively expensive e g allen et al 2005 pearcy et al 2005 ma et al 2008 vos et al 2010 sarlikioti et al 2011 in many cases this can limit their application in studying plant to plant interactions and competition at the field or ecosystem level a bottleneck in fspm computations is the calculation of radiation fluxes absorbed by plant tissues which directly or indirectly drives nearly all sub models of physiological processes faithfully modelling the transport of radiative energy is complex particularly when accounting for scattering by millions of elements e g leaves branches most physiological processes have a strong temperature dependence johnson and thornley 1984 thus if temperature is to be included in the model radiative emission typically must be considered which adds considerable complexity as each individual element in the domain of interest interacts directly through emission rather than indirectly through scattering because of these challenges models must make compromises in terms of complexity and scale of representation the more complex the radiation model the smaller the problem size can be considered a very wide range of three dimensional methods are available to model the transport of sunlight in plant systems e g ross 1981 myneni 1991 chelle and andrieu 1998 widlowski et al 2013 but relatively few are able to model emission of terrestrial radiation the radiosity method is the standard approach for modelling radiative emission between surfaces or elements and solves a coupled set of equations that represents radiation exchange due to emission and reflection by every element in the domain of interest goel et al 1991 modest 2003 although the radiosity approach is robust it can quickly become prohibitively expensive as the problem size is increased this is due to the fact that the radiosity approach involves solving an n n system of equations n being the total number of elements in the domain which has computational expense that scales as n 3 cf press et al 2007 other radiosity based methods have been developed to improve this scaling by using a multi scale approach such as the nested radiosity approach of chelle and andrieu 1998 which simplifies the contributions due to distant sources of radiation when small elements are present in ray tracing simulations so called reverse ray tracing can be used in which radiation is traced backwards by launching rays from elements toward sources which is common in both computer graphics applications e g shirley and morley 2003 and canopy modelling applications e g lewis and muller 1992 north 1996 lewis 1999 cieslak et al 2008 this method has the advantage that every element is guaranteed to be sampled and thus are more robust when computational cost limits the number of rays that can be afforded the disadvantage of reverse methods is typically most apparent when scattering of radiation is considered when a reflection or transmission event occurs along a ray path the ray traversal becomes irreversible meaning that tracing of the ray in forward or backward directions is no longer equivalent this means that more complicated methods must be devised to deal with scattering when the reverse approach is used which typically leads to increased requirements for memory and run time currently methods are not available to effectively model emission of terrestrial radiation using a reverse ray tracing approach using forward methods to model emission in the case of small geometric elements presents similar problems when modelling shortwave radiation although they may be more severe in the case of emission bailey et al 2016 noted that using a forward tracing method with very small elements resulted in significant sampling errors unless a very large number of rays were used which could result in substantial errors in the modelled net radiative flux when coupled with the energy balance equation it was also noted that very large errors in temperature could result which led to problematic violations of the second law of thermodynamics this paper presents a consistent approach for computing radiative emission in fully resolved plants using what can be considered a reverse ray tracing approach adapted to emission it was hypothesized that using a reverse ray tracing approach for emission would reduce errors in the modeled net longwave radiation flux the method is generalized to devise a means for modelling radiation scattering which is used to produce a complete reverse ray tracing model of radiation transport due to collimated point or terrestrial sources of radiation the ultimate goal was to develop a model for computing the three dimensional net radiative flux distribution in fully resolved canopies that is efficient enough to simulate canopy scale problems over seasonal time scales in a feasible amount of time 2 model description 2 1 element geometry it is assumed that the environment of interest is populated by a large number of discrete planar objects which are termed elements in this work three possible element types will be considered fig 1 which can be combined to form any arbitrary geometry patch a patch is a planar rectangle defined by four vertices the patch normal vector n e is defined using a right hand rule i e following vertices in an anti clockwise pattern yields an upward pointing normal alpha mask an alpha mask also known as a transparency mask is the same as a patch except that a portion of the patch is removed by specifying a two dimensional grid of pixels that determines whether or not material is present the pixel grid is specified using the alpha or transparency channel of a png image file triangle a polygon defined by three arbitrary vertices the triangle normal vector n e is also defined using a right hand rule 2 2 radiative bands arbitrary radiation wavelength bands can be simulated in the model by assigning the appropriate radiative properties to elements for example the photosynthetically active radiation par band 400 700nm wavelengths is important when considering sunlight interception by leaves but typically unimportant when considering radiative emission by leaves since they emit essentially no radiation in this wave band wavelength bands are represented by defining the appropriate radiative properties and emission sources for the band of interest each element is assigned total hemispherical radiative properties of ε emissivity ρ reflectivity τ transmissivity and α absorptivity where ρ τ α 1 modest 2003 here the term total indicates a property that is integrated over the spectral band of interest which is formally defined below e g total reflectivity 1 ρ λ 1 λ 2 ρ λ i λ d λ λ 1 λ 2 i λ d λ where ρ λ is the spectral reflectivity as a function of wavelength λ i λ is the spectral intensity of the radiation source and λ 1 and λ 2 are respectively the lower and upper wavelength limits of the band spectral radiative properties can be obtained by direct measurement using a spectroradiometer or by consulting available spectral databases e g hosgood et al 2005 kotthaus et al 2014 for simplicity the spectral intensity and radiative properties are often assumed to be constant over a certain radiative bands such as the par band 2 3 radiative emission if the emissivity and absolute temperature of an element is nonzero the element acts as a source of radiation through emission the emissive flux of an element can be calculated through the stefan boltzmann law as εσt 4 where σ 5 67 10 8 wm 2 k 4 is the stefan boltzmann constant and t is the absolute temperature of the element ambient emission from the surroundings may also be present which is not directly resolved by elements given these emitted fluxes the following procedure describes the method for calculating the absorbed flux due to emission by other elements and the surroundings illustrated graphically in fig 2 1 sampling element surfaces n rays discrete x y z points are randomly sampled on the surface of each element in the present implementation n rays is specified as a constant for all primitives the sampling of patch surfaces is accomplished by choosing n rays sets of two random numbers from a uniform distribution and mapping them to the surface of the rectangle cf suffern 2007 the same procedure is used for alpha masks except that if points lie in a transparent region they are re sampled until they are no longer in the transparent region there are several possible methods for sampling points on an arbitrary triangle the simplest of which is the rejection method which involves sampling points on a unit rectangle as in patches and discarding points that lie outside of the triangle however the method used here was to uniformly sample points on a right triangle i e half of a patch then apply an affine transform to map them onto the arbitrary triangular element 2 sampling ray directions ray directions are sampled according to a standard cosine weighted distribution 2 θ r sin 1 n t 3 ϕ r 2 π n p where n t and n p are random numbers drawn from a uniform distribution and θ r and ϕ r are respectively the ray zenithal and azimuthal directions for a horizontally oriented element the actual ray directions for an arbitrarily oriented element are found by converting θ r and ϕ r into a cartesian unit vector and rotating it into the direction of the element normal in practice jittered sampling was used for the calculation of n t and n p to improve sampling convergence this involves generating a uniform grid of n t and n p between zero and one then drawing a random number that gives a displacement that lies between the next adjacent point see suffern 2007 for further details a leaf is a thin surface and could be represented by placing two elements back to back to save computational time and memory an option can be enabled to emit rays from both sides of an element for example an element making up the ground or trunk only needs to emit radiation from one side while a leaf emits from both sides 3 path tracing the rays are then launched from the sampled points away from the element and ray object intersection tests are performed suffern 2007 there are one of two possible outcomes for a given ray 1 the ray hits another element in this case the emitted flux of the intersected element is queried q εσt 4 and used to calculate the weighted flux value associated with that ray q n rays for alpha masks the alpha map must be queried to determine if the ray intersection occurred on a transparent portion of the element if so the ray effectively continues propagating as if no intersection had occurred 2 the ray does not intersect any other elements in this case the weighted flux value of the ray is that of the ambient flux q amb wm 2 sr 1 which is the directional hemispherical flux of an unobstructed surface for the wave band of interest if the ambient radiation is isotropic the ambient flux can be assigned a constant hemispherical value q amb wm 2 and the weighted ray flux is simply q amb n rays if the ambient flux is anisotropic a directionally dependent value could be used that varies based on the direction of ray propagation the ambient flux could be measured using a radiometer equipped with a shadowband e g horowitz 1969 or modeled e g harrison and coombes 1988 since the entire hemisphere around the element is sampled there is no explicit need for the treatment of boundary conditions elements near the edge of the domain may intercept some ambient radiation propagating upward which replaces the radiation that would have been intercepted from ground elements were the element near the middle of the domain the weighted flux of the ray is partitioned into absorbed reflected and transmitted components the absorbed component 1 ρ τ q n rays is accumulated summed into an absorbed buffer corresponding to the element from which the ray was launched the reflected component ρq n rays is accumulated in a to be scattered buffer there is one to be scattered for each side of the element and the reflected flux is accumulated in the buffer corresponding to the same side that was hit i e if the ray hit the top side the energy is accumulated in the buffer corresponding to the top side the transmitted component τq n rays is accumulated in a to be scattered buffer in a similar manner except that the transmitted flux is accumulated in the buffer corresponding to the opposite side than was hit i e if the ray hit the top side the energy is accumulated in the buffer corresponding to the bottom side 4 reflection iterations energy in the elements to be scattered buffers needs to be traced as it leaves the elements in the present model implementation it was assumed that reflection and transmission is lambertian measurements indicate that strictly speaking this assumption is nearly always false with specular reflection and scattering of transmitted radiation playing a clear role in creating anisotropy in reflected and transmitted radiation woolley 1971 bousquet et al 2005 combes et al 2007 while this is usually an incorrect assumption in a strict sense chelle 2006 demonstrated that non lambertian effects play a minimal role in overall radiation absorption distributions when considering the incident flux due to scattering by many leaves employing the above assumptions scattering calculations proceed in essentially the same manner as for primary emission the only differences are that the ray strength is assigned based on the to be scattered flux rather than the emissive flux and that the ambient flux is set to zero for scattering iterations the user specifies the maximum allowable to be scattered energy e g 0 1wm 2 and a maximum allowable number of scattering iterations e g 10 scattering iterations proceed until the maximum to be scattered energy among all elements is below the user defined threshold or the number of scattering iterations has exceeded the user defined maximum value in either case any remaining energy in the scattering buffers is absorbed by the element to ensure conservation of energy in cases where ρ and τ are small such as in the par band few iterations are required if ρ or τ are large such as in the near infrared band several iterations may be required however since reflected and transmitted energy decreases exponentially with each iteration the number of required scattering iterations is still usually low e g 5 10 depending on user defined thresholds the required number of scattering iterations will be further explored in section 3 1 1 2 4 external radiation sources 2 4 1 collimated radiation sources standard approaches are used to calculate the primary incident flux i e before scattering due to a collimated source of radiation such as the sun fig 3 collimated radiation is defined as radiation in which all beams of radiation are parallel although emission by the sun is not collimated the large distance between the sun and earth means that radiation emitted by the sun that is intercepted by earth is approximately collimated for collimated radiation points are randomly sampled on the surface of elements in the same way as for emission the ray direction is simply defined by a line connecting the sampled point and the radiation source the rays are then traced from the element toward the source and intersection tests are performed in the same was as for emission rays that do not intersect any other elements are assigned a value of 4 s r q s n e n s n rays where q s is the source s total hemispherical radiative flux w m 2 over the band of interest just above the domain in the direction of n s q s could come from a sensor measurement for example if the measurement of a horizontal sensor was 500w m2 for the direct solar flux q s 500 cosθ s w m2 where θ s is the solar zenith angle or from a model of solar transmission e g liu and jordan 1960 gueymard 2008 absorption and scattering then proceed in the same fashion as with emission if there is a diffuse flux associated with a given source the ambient space can be sampled during the first scattering iteration 2 4 2 point sources if the radiation source can be approximated as emenating from a point e g a lamp in the distance the ray direction is a line that connects the radiation source and a point sampled on the element surface fig 4 the strength of rays that do not intersect any other elements is calculated by integrating incident radiation over all possible directions emitted by the source 5 s r q p n rays n θ n ϕ j 1 n ϕ i 1 n θ n e n r n r n s θ i ϕ j π s 2 δ where q p is the total emitted radiation flux at the source s surface wm 2 over the band of interest n r is a unit vector originating at the point sampled on the element and pointed toward the radiation source s is the distance from the point sampled on the element to the source n s θ i ϕ j is a unit vector originating at the location of the point source and oriented in the spherical direction θ i ϕ j where there are n θ n ϕ total discrete spherical directions θ i is calculated as cos 1 1 2 i 0 5 n θ and ϕ j is calculated as 2 π j 0 5 n ϕ δ is a function that is equal to 1m2 if n r n s 0 and equal to 0 otherwise whose purpose is to discard radiation emitted from the back side of the source 2 5 gpu acceleration in general ray tracing calculations can become costly when performed in serial due to the large number of rays required the computer graphics community has used graphics processing units gpus for decades to accelerate ray tracing calculations a gpu is a specialized piece of hardware with a large number of streaming multiprocessors designed to crunch numbers in parallel several frameworks have been developed to simplify the task of gpu programming such as nvidia s cuda framework and opencl in this work the above ray tracing algorithms were implemented using nvidia s optix ray tracing library which is a highly optimized and generalized gpu ray tracing framework optix does not actually perform any ray tracing per se rather it is a flexible framework for managing data and tasks associated with ray tracing for example the user writes functions that describes how rays are launched what constitutes a ray object intersection and what should happen in the event of an object hit or miss 3 results 3 1 accuracy and consistency tests typically radiation models are tested by comparing results against real world data collected in the field i e validation performing rigorous validation for detailed radiation models presents several considerable challenges first extremely detailed measurements are required to specify geometry within the model since individual leaves and branches are represented in the model it would be necessary to accurately specify the geometry of every leaf and branch in the model if leaf level measurements are to be reproduced another option is to validate using aggregate measurements which have looser requirements for geometry specification however this generally aggregates many different processes and measurement errors making it difficult to separate natural environmental variability measurement errors and model errors another approach that is perhaps of greater value when evaluating these types of detailed models is to verify model predictions for relatively simple cases where the exact answer is known since the exact answer is known all errors in the model predictions must be due to model errors or errors in implementation this verification over validation approach has been favored by some in the remote sensing community and has been used for model intercomparisons pinty et al 2001 widlowski et al 2013 in the majority of their tests the exact answer was not known but rather the aggregate of predictions by different models was used as a relative measure of model performance it should be noted that these tools developed by the remote sensing community focus on the distribution of reflected canopy level radiation and not on absorption by individual elements which is the focus of this work here the model was assessed by ensuring that basic consistency and accuracy conditions were satisfied using problems where the exact answer was known accuracy was verified not only by relative error measures but importantly by ensuring that the error continually decreases as more rays are used i e the solution converges to the exact answer future work will focus on creating new datasets that can be used to more accurately assess real world errors in model predictions 3 1 1 accuracy and convergence to verify the model and make assessments regarding accuracy several sample problems were solved that are simple enough that the exact answer can be calculated fig 5 the test cases were designed to progressively evaluate different aspects of the model test case parameters such as dimensions radiative properties and temperatures were chosen arbitrarily but with the general goal in mind of providing tests of individual model components and demonstrating the associated error convergence as the number of rays was varied each of the test cases are detailed and solved analytically in appendix a these details have been included to provide standard cases against which future models can be verified the performance of the proposed model is contrasted against that of a traditional forward ray tracing model in the forward model rays are launched from the source from which they originated which may be an element a collimated source a point source or an ambient source in which case the rays originate from the surface of a sphere that bounds all elements in the domain a detailed description of the implementation of the forward tracing method is given in appendix b an ensemble of 500 simulations were performed for each case and the average error was quantified using the normalized root mean square error which can be defined mathematically as 6 nrmse 1 r ex i r i r ex 2 1 2 where r i is the modeled value of the absorbed radiation flux for simulation i in the ensemble and r ex is the exact value of the absorbed flux case 1 fig 5 was designed to test collimated radiation absorption of a patch and subsequent reflection to an adjacent patch as well as patch emission the reflectivity for radiation originating from the collimated source chosen to be ρ 1 0 3 and ρ 2 0 for emission parameters were chosen to be ε 1 ε 2 1 t 1 300 and t 2 0 the patch size l is arbitrary and was chosen to be equal to 1 fig 6 reports the normalized error in the absorbed flux for patches 1 and 2 and absorbed flux by patch 2 due to emission by patch 1 using the reverse method the error in absorbed radiation for patch 1 is always zero regardless of the number of direct rays this is a consequence of the fact that an unobstructed element always perfectly samples radiation sources a desirable quality of the model note that the reverse method is not shown in fig 6a because a vale of zero cannot be represented in a logarithmic plot for the forward method the absorbed flux for patch 1 is strongly dependent on the chosen number of rays with the error decreasing exponentially with ray count this exponential decrease in error with ray count is a standard result for forward ray tracing simulations widlowski et al 2013 the radiation flux absorbed by patch 2 was strongly dependent on the chosen number of rays for both the forward and reverse methods and converged to the exact solution at approximately the same exponential rate fig 6b however the error in the forward method flux was consistently about four times larger than that of the reverse method it appears that this is due to the fact that the flux scattered to patch 2 is dependent on proper sampling of the direct radiation by patch 1 since the reverse method is much more accurate than the forward method at predicting the potential amount of radiation available to be scattered from patch 1 it is consistently more accurate in predicting the absorbed flux by patch 2 due to scattering by patch 1 however the diffuse sampling of patch 2 by patch 1 is approximately the same for both methods as indicated by the similar performance by each method in predicting the absorbed radiation flux by patch 2 due to emission by patch 1 fig 6c due to the symmetry of this particular problem using the forward or reverse methods are essentially equivalent in the simulation of radiative emission by the elements case 2 fig 5 was designed to test sampling of an isotropic ambient radiation source radiative properties were chosen to be ρ 1 ρ 2 0 the geometry was chosen such that a 1 b 2 and c 0 5 for both the forward and reverse methods the error in the modelled absorbed flux decreased at approximately the same exponential rate fig 7 however the error in the forward method was consistently about four times larger than that of the reverse method for either method the surface of patch 2 must be adequately sampled by patch 1 in order to calculate the amount of ambient radiation patch 1 intercepts for the reverse method this sampling is being performed from the point of view of patch 1 whereas for the forward method the sampling is being performed from the point of view of the surface of the domain bounding sphere which is on average further away thus the reverse method is more accurate because it better samples the radiation occluded from view of patch 1 by patch 2 but the overall convergence rate is the same case 3 fig 5 was designed to test emission exchange between gray patches and subsequent infinite reflections here both the number of diffuse rays and the maximum number of scattering iterations was varied and only patch emission was considered radiative properties were chosen to be ε 1 ε 2 0 6 and the geometry was specified the same as in case 2 when no scattering computations are allowed both the forward and reverse methods are essentially equivalent because of geometric symmetry as in case 1 in this case there is a small amount of initial decrease in the model error as the number of rays is increased but eventually no improvement is achieved by using more rays given that scattering must be included to correctly predict the absorbed flux fig 8 as the maximum number of scattering events is increased the error decreases at an equivalent exponential rate for both methods note that this is dependent on the particular method used for scattering in the forward ray tracing approach since for every ray object intersection energy is both absorbed and reflected the forward and reverse methods are equivalent many implementations of forward ray tracing use a stochastic approach to determine whether absorption or reflection occurs see appendix b in this stochastic implementation the number of rays available after each scattering event decreases exponentially thus leading to decreased accuracy and a slower rate of convergence when compared to the reverse ray tracing approach not shown case 4 was designed to test emission from a single point radiation source and subsequent absorption by a patch the patch reflectivity was ρ 1 0 3 and the geometry was specified as a 1 b 2 and c 1 n θ and n ϕ were set to a constant value of 30 in the case of the reverse tracing method for both the forward and reverse methods the error decreased approximately exponentially as the number of direct rays was increased fig 9 however the error for the reverse method declined at a faster rate than the forward method as the number of rays was increased on average the error in the forward method was about fifty times larger than in the reverse method the reason for this is similar as in case 1 which is due to the fact that in the reverse method the source is perfectly sampled by the patch whereas in the forward method the source must accurately sample a relatively small fraction of the surrounding spherical space the difference between case 1 is that the source rays have angular dependence and thus even in the reverse method the surface of the patch must be sampled in order to properly calculate the solid angle subtended by the patch with respect to the source therefore the error still has an exponential dependence on ray count even for the reverse method 3 1 2 conservation of energy 1st law of thermodynamics perhaps the most fundamental consistency requirement for a radiation model is that energy is conserved essentially this means that no energy is gained or lost due to transport energy can only be relocated for collimated radiation the total energy rate incident on the domain is equal to q s cosθ s a t where a t is the total footprint area of the domain θ s is the solar zenith angle and we note that units are in energy per time for diffuse radiation the total incident energy rate is q d a t where q d is the diffuse radiation flux on a horizontal surface for emission the total energy rate emitted by all elements is i n ε i σ t i 4 a i in order to evaluate conservation of energy we simply calculate the radiation energy rate absorbed by all elements including energy emitted reflected to the sky and compare this value with the total energy rate we started with if energy is conserved the two should be equal in theory any consistent radiation model e g ray tracing radiosity should be able to satisfy energy conservation as mentioned previously the method described in this work only approximately satisfies conservation of energy this is because the amount of energy that actually leaves an element is dependent on the element being properly sampled by other elements to test conservation of energy a simple domain was created with 140 140 patches of size 0 3 0 3m2 tiled to create a ground surface with four trees spaced a 7m that were 6m tall on average fig 11 three dimensional almond tree geometries were created using the model of weber and penn 1995 and placed into the domain at uniform spacing each tree consisted of about 6000 triangles making up the trunk and branches and about 30 000 alpha masks making up the leaves as detailed in weber and penn 1995 each geometric parameter is randomly perturbed to make each tree geometry unique the shortwave reflectivity and transmissivity of the leaves were arbitrarily set to 0 2 for testing purposes and the emissivity was set to 0 9 all elements were assigned temperatures of 300k for this case it was found that the error in conservation of energy was about 0 01 of the source flux for a collimated source and 0 1 of the ambient flux for emission even if as few as 50 rays per element were used this indicated that conservation of energy was well satisfied with errors only slightly larger than machine precision single precision 3 1 3 equilibrium distribution 2nd law of thermodynamics a requirement of equal importance as conservation of energy is that the radiation distribution must tend toward a uniform equilibrium state or that entropy cannot decrease in the absence of external sources or sinks this is a critically important requirement since if the solution does not revert to the correct equilibrium state we can have little confidence that any observed features of the calculated radiation distribution under non equilibrium have physical meaning testing that this requirement is enforced is relatively simple and can be performed as a precursor check before every simulation the temperature of every element is set to some value t and the ambient flux emanating from the sky is set to σt 4 regardless of geometry and values of ε the net radiative flux for all elements should be exactly zero the orchard test case with four total trees was used to test model adherence to the 2nd law and the total number or rays were varied to illustrate its effect on errors in the net radiative flux fig 10 for the forward approach the total number of rays consisted of both ambient diffuse rays and emission rays emission and ambient rays are the same in the reverse approach for each of the four forward simulations the number of ambient rays was 106 107 108 and 109 and the corresponding number of emission rays per element was 3000 5000 6000 and 10 000 note that some elements are two sided and use twice the number of emission rays the reverse ray tracing approach has the very desirable property that the proper equilibrium distribution will always be reached to within machine precision even with only one ray sample per element this means that in the case where the emitted flux from all elements is equal to the diffuse ambient flux the net modelled radiative flux for all elements is essentially zero about 10 7 on average for this case when the standard forward ray tracing method is used adherence to the 2nd law is strongly dependent on the number of rays in relation to simulated geometry fig 10 for a modest number of rays i e 200 million the standard error was extremely large at about 150wm 2 which is about a third of the ambient emitted flux value σt 4 as the number of rays is increased the error in the 2nd law steadily decreases however even when a very large number of rays was used i e 4 trillion the error in the net radiation flux was still significant with a standard error of about 40wm 2 the spatial distribution of this error is visualized in fig 11 for this ray count the range of errors was still very large with the minimum net flux having a value of 459wm 2 i e no rays sampled the element and the maximum net flux having a value of about 9000wm 2 these extreme values corresponded to triangular elements comprising the woody structure of the trees and occurred at locations of branching splitting or where there were highly skewed elements extreme values could certainly be filtered or the quality of the triangular mesh could be improved however this illustrates a problem with using arbitrary meshes with the forward ray tracing approach that the reverse approach is able to deal with in a straightforward manner the reason that the reverse ray tracing method significantly outperforms the forward method is because it samples the absorbed flux due to both ambient sources and other elements in the exact same way therefore any error in the absorbed ambient flux is exactly offset by the absorbed flux due to other elements for the forward ray tracing method the absorbed ambient flux is sampled from the point of view of the domain bounding sphere and absorbed emission due to other elements is sampled from the point of view of the other elements each of these samplings can contain substantial and unrelated errors which can lead to extremely large errors in the net radiative flux 3 2 visualizations sample visualizations of model outputs are given in fig 12 and are simply meant to provide a visual representation of model capabilities in fig 12 each element is coloured based on a mapping of its absorbed radiation flux to the color scale shown in the figure a flat ground surface was also created by tiling patches of size 0 1 0 1m2 which resulted in roughly 1 1 million patches there were 121 total trees in the orchard which were 6 m tall and spaced at 7m in total the scene depicted in fig 12a contains roughly 5 5 million elements fig 12a shows a pseudocolor plot for the total absorbed direct and diffuse solar radiation flux in the simulated orchard the reflectivity and transmissivity of the leaves were set to 0 05 which is characteristic of the photosynthetically active radiation par band jones and vaughan 2010 and the rest of the surfaces were assumed to be black the collimated radiation flux was set to 800wm 2 and the ratio of the direct to isotropic ambient radiation flux q s q amb was 4 the collimated radiation flux zenithal angle was 45 the number of direct and diffuse rays were set to 100 and 200 respectively fig 12b depicts direct radiation transport from several point radiation sources corresponding to artificial lamps in a greenhouse the three dimensional geometries were read into the code using a standard polygon file that can be generated using nearly any 3d modelling software e g autocad autodesk inc blender www blender org the plants and pots consisted of about 6000 triangles each each of which were identical except that they were given a random azimuthal rotation patches were tiled to make the greenhouse walls and triangles were tessellated to make lamp housings in total the simulation consisted of about 2 million elements 3 3 scaling of model run time a test case was created to demonstrate model scaling up to very large domain sizes the tests were performed on an nvidia gtx titan card which is a consumer grade gaming gpu the same general orchard geometry was used as described in section 3 2 the number of trees was incrementally increased and the execution time required to simulate both collimated and diffuse radiation for a single band at a single time of day was recorded since the purpose of this exercise was simply to demonstrate scaling of run time the reflectivity and transmissivity of the leaves were arbitrarily set to 0 1 and the rest of the surfaces were assumed to be black the number of direct and diffuse rays were chosen to be 100 and 200 respectively the number of scattering iterations was fixed at 0 and 3 to test its effect on execution time and scaling by fixing the number of scattering iterations this removes any effect of the leaf radiative properties on the execution time because scattering will continue in the same manner regardless of the amount of energy to be scattered the execution time scaled linearly with number of elements fig 13 for the range of domain sizes examined above about 22 5 million elements the gpu ran out of memory at this point larger problems could be simulated by utilizing additional ram on the host which comes with an increase in run time due to the low bandwidth between the gpu and host memories once the host runs out of memory larger problems could still be simulated by utilizing swap space on the host hard drive which comes with an even larger increase in run time scattering iterations are expected to incur a similar expense as performing the diffuse radiation calculation without scattering however the first scattering iteration is essentially free when performing diffuse calculations because diffuse calculations and the first scattering iteration can be performed at the same time thus adding three scattering iterations resulted in an increase in execution time of almost exactly 100 one downside with the method used in this work is that the run time is virtually independent of the radiative properties of the elements whereas in traditional forward ray tracing reflection transmission rays are not launched if the surface is black since the ray energy becomes zero see appendix b in the reverse method each element samples the space around it and thus it cannot assume that any neighboring elements are black until it has actually sampled them results showed that for this specific geometry and computational hardware the run time was about 12 5 seconds per million elements with no scattering for additional scattering iterations this run time can be multiplied by n where n is the number of scattering iterations if we were to consider simulating a single tree at hourly timesteps for an entire growing season assume an average of 12h of sunlight per day 200 growing days and no scattering we would find the simulation time to be roughly 15min which is quite feasible if the number of trees were to be increased by 100 the execution time would be roughly one day which is still feasible considering the complexity represented there are of course opportunities to reduce this execution time by not performing calculations every day upgrading the gpu or by using multiple gpus 4 discussion and conclusions complex model geometries with small elements typically cause problems for traditional forward ray tracing approaches e g kimes and kirchner 1982 north 1996 gastellu etchegorry et al 2004 that trace radiation away from its source this is because they require an incredibly large number of rays to adequately sample small elements this forward tracing approach was used to perform simulations of four fully resolved trees to determine whether the model satisfied an equilibrium condition i e if a constant radiative flux is emitted from all objects the net flux should be zero for all objects results showed that there was significant error in the net radiative flux standard error of about 10 even when trillions of rays were used perhaps more concerning was that there were many outlier elements that had extremely large errors 100 it is fairly well known that when elements are small and sampling errors become problematic reverse ray tracing approaches offer a viable solution north 1996 lewis 1999 disney et al 2000 cieslak et al 2008 here radiation is traced from the object back toward the source and therefore all objects are adequately sampled the trade off is that when scattering is considered the methods become more complicated because radiation propagation is not reversible when scattering occurs furthermore reverse tracing models are not readily available to incorporate radiative emission which is needed to calculate the net all wave flux in this work a model was developed that utilizes a reverse tracing approach to calculate the net radiative flux for every element in the simulated domain the forward and reverse tracing methods were shown to be equivalent in the case of symmetric radiative exchange between two objects the difference between the methods becomes evident when there is high asymmetry such as in the case of the sky emitting radiation towards small elements in the case of forward tracing the sky is well sampled by an unobstructed element because the sky occupies a large fraction of the spherical space surrounding the element this same element is relatively poorly sampled by the sky because the element occupies only a small fraction of the total space viewed by the sky for reverse tracing the opposite is true thus if we are concerned with absorption by the elements the reverse method typically offers better sampling of external radiation sources in the case of emission either the forward or reverse method may offer better sampling of some elements while giving worse sampling of other elements depending on the relative element sizes the advantage of the reverse method in all cases is that it guarantees the absorbed flux by any element is bounded within the range of fluxes emitted by all radiation sources even if only one ray sample per element is used this is a considerable advantage when only a limited number rays can be afforded because it guarantees that results are at least reasonable whereas the forward tracing method is likely to produce unreasonable results when modelling the net radiative flux at an element it is advantageous to use a reverse tracing approach both when sampling external sources and emission by elements it was demonstrated that the reverse tracing method used in this work is at the very least consistent with the second law of thermodynamics or equilibrium condition the reason that this method satisfies this consistency condition is because any errors in sampling the ambient radiation flux are exactly offset by errors in sampling incoming emission by other elements which is due to the fact that they are both sampled using the same reverse method the reverse ray tracing method developed in this work bears many similarities with the commonly used radiosity approach e g goral et al 1984 borel et al 1991 chelle and andrieu 1998 both of which satisfy the consistency condition described above in theory the methods are nearly the same with the major difference being in the treatment of scattering the reverse tracing approach presented in this work does not require the solution of a linear system of equations that can become costly to solve when the number of elements is large the present method was shown to scale linearly with the number of geometric elements while the computational effort associated with solving a linear system of equations increases exponentially with the number of elements press et al 2007 results of this work suggest that it is typically preferable to sample the domain from the point of view of the entity of interest that is absorbing radiation if that entity of interest is a leaf or sensor within the canopy it is typically advantageous to use a reverse ray tracing approach that ensures the entity is well sampled in remote sensing applications the entity of interest is often the distribution of radiation scattered to the sky thus it is more common to find forward tracing methods used in these applications because the domain is sampled from the point of view of the sky disney et al 2000 widlowski et al 2006 2013 in addition to performing detailed studies of radiation transport processes independently an important future application of the model is coupling with other biophysical models of plant systems since the model can represent longwave radiation exchange it can be coupled with an energy balance model to predict surface temperature throughout canopies nearly all physiological processes are strongly dependent on temperature e g photosynthesis respiration growth pest pathogen development and such information is needed by physiological models at the organ level this could be applied not only to understand impacts of heterogeneity in natural ecosystems but also help to inform or optimize the management of forestry or agricultural systems acknowledgements financial support of this work by the american vineyard foundation grant 2016 1825 and the usda national institute of food and agriculture hatch project number ca d pls 2401 h appendix a verification test case descriptions and exact solutions for completeness the exact solutions to each of the verification test cases presented in section 3 1 are given below a 1 case 1 this case consists of two orthogonal square lambertian patches that share a common edge fig 5 a collimated radiation source is located directly overhead patch 1 has a nonzero reflectivity which reflects some radiation to patch 2 which receives only this reflected radiation patch 2 is cold and patch 1 is black so the only emission exchange is direct emission from patch 1 to 2 the fraction of energy leaving patch 1 that is intercepted by patch 2 i e viewfactor f 1 2 modest 2003 is f 1 2 0 2 modest 2003 appendix d39 by symmetry f 2 1 0 2 since patch 1 is oriented normal to the radiation source the absorbed radiation flux is simply a 1 q 1 1 ρ 1 q s where q s is the collimated radiation flux normal to the radiation source direction the flux of reflected radiation leaving patch 1 is ρ 1 q s and thus the reflected flux absorbed by patch 2 is a 2 q 2 ρ 1 f 1 2 q s which is only valid for ρ 2 0 note that since patch 2 is parallel to the source direction it receives no direct radiation the radiative flux absorbed by patch 2 due to emission by patch 1 is simply a 3 e 1 f 2 1 ε 2 σ t 2 4 a 2 case 2 this case consists of two aligned parallel lambertian patches fig 5 there is an isotropic ambient radiation source radiation absorption is ignored on the lower side of patch 1 and on the upper side of patch 2 the reflectivity of both patches is zero the fraction of energy leaving patch 1 that is intercepted by patch 2 is given by modest 2003 appendix d38 a 4 f 1 2 2 π xy ln 1 x 2 1 y 2 1 x 2 y 2 1 2 x 1 y 2 tan 1 x 1 y 2 y 1 x 2 tan 1 y 1 x 2 x tan 1 x y tan 1 y where x a c and y b c by conservation of energy the fraction of diffuse radiation incident on patch 1 is 1 f 1 2 it is also noted that f 1 2 f 2 1 by symmetry thus the absorbed ambient radiation flux for patch 1 is a 5 q 1 1 f 1 2 q s by symmetry the absorbed ambient radiation flux for patch 2 is the same as that for patch 1 a 3 case 3 this case is the same as case 2 except that only emission is considered and both patches have emissivities less than unity fig 5 as a result an infinite series of reflections occurs between the two patches the viewfactors f 1 2 and f 2 1 are given by eq a 4 since there is no restriction imposed that ε 1 or ε 2 be equal to unity a radiosity approach can be used to obtain analytical expressions for energy transfer between the two patches using conservation of energy a system of equations describing radiation exchange between the two patches can be written which yields the standard radiosity equations modest 2003 a 6a e 1 ε 1 1 ε 2 1 f 1 2 e 2 σ t 1 4 f 1 2 σ t 2 4 a 6b e 2 ε 2 1 ε 1 1 f 2 1 e 1 σ t 2 4 f 2 1 σ t 1 4 where e i is the net surface flux for patch i by solving for e 2 in eq a 6a and back substituting into eq a 6b an explicit solution can be obtained for e 1 and e 2 a 7a e 1 σ 1 ε 1 f 1 2 t 1 4 f 2 1 t 1 4 t 1 4 t 2 4 1 ε 1 ε 2 1 ε 1 1 1 ε 2 1 f 1 2 f 2 1 a 7b e 2 ε 2 1 ε 1 1 f 2 1 e 1 σ t 2 4 f 2 1 σ t 1 4 finally subtracting the emitted flux gives the absorbed energy flux a 8a e 1 e 1 ε 1 σ t 1 4 a 8b e 2 e 2 ε 2 σ t 2 4 a 4 case 4 this case consists of a single point source of radiation incident on a horizontal patch which has one vertex positioned directly below the source fig 5 the patch may have nonzero reflectivity which reflects some energy from the source to the surroundings the fraction of energy leaving the point source that is absorbed by the patch is given by modest 2003 appendix d47 a 9 f p 1 1 4 π tan 1 1 d 1 2 d 2 2 d 1 2 d 2 2 where d 1 c a and d 2 c b therefore the flux absorbed by the patch is simply a 10 e 1 1 ρ 1 f p 1 q p where q p is the hemispherical flux emitted by the source appendix b standard forward ray tracing methodology the following describes the methodology used to perform forward ray tracing calculations most of what is described below can be found in chapter 11 of mahan 2002 or other standard radiation textbooks modest 2003 howell et al 2010 although some modifications have been made for the treatment of ambient diffuse radiation in traditional monte carlo ray tracing approaches individual bundles or rays of radiation are launched from their sources and intersected with various elements as they travel through the domain of interest ray origins directions and associated energy are chosen differently depending on the type of radiation source from which they originate collimated radiation sources collimated solar radiation is typically modelled by launching a large number of rays from above the domain and toward the domain in the direction of the source in order to ensure that rays cover the entire footprint of the domain a bounding sphere can be constructed that encompasses all modelled objects rays are then cast from a disk with the same diameter as the bounding sphere that is positioned tangentially to the bounding sphere see overby et al 2016 the energy assigned to each ray is q s π r s 2 n rays where q s is the radiative flux normal to the source direction r s is the radius of the domain bounding sphere and n rays is the number of direct rays launched toward the domain point radiation sources emission from a point radiation source is represented by launching rays uniformly in all directions from the location of the source the zenithal direction of each ray is chosen as θ cos 1 1 r t where r t is a number randomly drawn from a uniform distribution the azimuthal direction of the ray can be similarly specified by ϕ 2πr p where r p is an additional uniform random number the energy assigned to each ray is 2q p n rays where q p is the energy flux emitted by the source and n rays is the total number of rays launched from each source ambient diffuse radiation sources ambient diffuse radiation can be modelled by casting rays in random directions toward the domain to define a given ray we must specify both the origin of the ray and a point through which the ray passes or equivalent in the case of an isotropic ambient radiation flux rays can be defined by randomly choosing two points on the surface of the domain bounding sphere through which the ray will pass in a spherical coordinate system the zenithal angle of each point with respect to the bounding sphere center can be given by θ cos 1 1 r t where r t is a number randomly drawn from a uniform distribution the azimuthal angle of the points can be similarly specified by ϕ 2πr p where r p is an additional uniform random number the energy assigned to each ray is q amb 4 π r s 2 n rays where q amb is the isotropic ambient radiation flux and here n rays is the number of diffuse rays if the ambient flux is anisotropic this could be accounted for by varying the ray energy over the bounding sphere terrestrial emission emission by terrestrial objects is modelled by casting rays away from the surface of the objects uniformly distributed points are randomly chosen on the surface of the objects methods for doing so described in suffern 2007 which correspond to ray origins ray directions are also randomly chosen for a horizontal surface the spherical coordinates of the ray can be determined as θ sin 1 r t and ϕ 2πr p where r t and r p are uniform random numbers if the surface is inclined the chosen ray direction can be rotated into a coordinate system normal to the object surface in the case of lambertian emission the energy assigned to each ray is εσt 4 a n rays where ε is the object emissivity σ is the stefan boltzmann constant t is the absolute temperature of the object s surface a is the surface area of the object and here n rays is the number of rays cast from each object ray object intersections rays are intersected with objects to determine the closest ray object intersection point in the event of an intersection some fraction of energy will be absorbed transmitted and reflected one approach to modelling this is to choose a uniformly distributed random number r if r is less than the object reflectivity ρ 100 of the ray s energy is reflected by the object and a new ray is launched originating at the intersection point with a new random direction away from the same side of the object surface the ray intersected if r is greater than ρ but less than ρ τ τ is the object s transmissivity a new ray is launched as in reflection except from the opposite side of the object surface the ray intersected if r ρ τ 100 of the ray s energy is absorbed by the object and the ray is terminated to ensure equivalence with the reverse tracing approach the forward scattering approach used in this work was a deterministic scattering method in this case if a ray intersects an object some fraction of the ray s energy ρ is reflected by launching a new ray whose energy is equal to the product of ρ and the ray energy before scattering similarly some fraction τ is transmitted the final fraction 1 ρ τ is absorbed by the object this process continues until the ray energy falls below some user defined threshold value 
25359,large predators can significantly impact livestock industries in australia wild dogs canis lupus familiaris canis lupus dingo and hybrids cause economic losses of more than aud 40m annually landscape scale exclusion fencing coupled with lethal techniques is a widely practiced control method in western australia the state barrier fence encompasses approximately 260 000km2 of predominantly agricultural land but its effectiveness in preventing wild dogs from entering the agricultural region is difficult to evaluate we conducted a management strategy evaluation mse based on spatially explicit population models to forecast the effects of upgrades to the western australian state barrier fence and several control scenarios varying in intensity and spatial extent on wild dog populations in southwest western australia the model results indicate that populations of wild dogs on both sides of the state barrier fence are self sustaining and current control practices are not sufficient to effectively reduce their abundance in the agricultural region only when a combination of control techniques is applied on a large scale intensively and continuously are wild dog numbers effectively controlled this study identifies the requirement for addressing extant populations of predators within fenced areas to meet the objective of preventing wild dog expansion this objective is only achieved when control is applied to the whole area where wild dogs are currently present within the fence plus an additional buffer of 20km our modelling focused on the use of baiting trapping and shooting however we acknowledge that additional tools may also be applied finally we recommend that a cost benefit analysis be performed to evaluate the economic viability of an integrated control strategy 1 introduction with the global expansion of production landscapes areas of native habitat are diminishing foley et al 2005 often forcing wildlife and livestock to co exist compete for resources and increasing the frequency of their interactions significant human wildlife conflicts and economic challenges can result when predators impact production systems as occurs through predation on livestock by wolves and bears in north america and europe by felids in asia and south america and a range of species in africa treves and karanth 2003 inskip and zimmermann 2009 treves et al 2009 in australia wild dogs defined as dingoes canis lupus dingo free living domestic dogs canis lupus familiaris and their hybrids are important predators of livestock and can have significant economic impacts these impacts result from direct predation stock harassment and the transmission of disease fleming et al 2014 in australia total economic surplus losses due to wild dogs are estimated at aud 21 9m to sheep industries and aud 26 7m to the cattle industry gong et al 2009 more recent estimates put yearly losses to the western australia wa sheep industry at 14m and to the wa rangeland goat industry at 11m bell 2015 small stock sheep and goats are so vulnerable to predation by wild dogs that it is generally considered that sheep enterprises and wild dogs cannot coexist thomson 1984 newsome 2001 fleming et al 2014 the primary mechanisms for the control of wild dogs in australia are lethal means including trapping shooting and poison baiting fleming et al 2014 in some areas such as the southwest of western australia these control strategies are augmented with landscape scale fences landscape scale fences separating high value agricultural areas from predators have been used for over a century in australia caughley et al 1980 newsome et al 2001 and are being used increasingly in predator control programs binks et al 2015 the state barrier fence sbf runs 1190km in a northwest southeast direction and broadly separates a higher rainfall agricultural region suited to high value stock and cropping from semi arid to arid rangelands fig 1 originally designed to prevent the spread of european rabbits oryctolagus cuniculus from the rangelands into the agricultural region the sbf has also reduced the impacts of larger vertebrates such as emus aves dromaius novaehollandiae and wild dogs in the agricultural region crawford 1967 broomhall 1991 ritchie 1997 however in the last decade wild dog impacts have increased at the interface of the agricultural region and the rangelands dafwa unpublished data wild dog control is becoming a priority for landholder biosecurity groups at the periphery of the agricultural region and in 2014 the entire sbf was upgraded at significant cost to meet wild dog exclusion standards by adding a lap wire protruding from the fence at 45 to prevent tunnelling the upgraded fence s impacts on wild dog abundance and distribution have not been investigated making it difficult to gauge its cost effectiveness the extent to which additional complementary predator controls will be required to protect livestock in the agricultural region has also not been examined population modelling has been recommended to explore the relative effectiveness of different management options starfield 1997 morris and doak 2002 campbell et al 2015 this approach offers the advantage of identifying targets and timeframes for monitoring programs himes boor 2014 here we use spatially explicit demographic models to evaluate whether current or foreseeable management practices along with the sbf are adequate to keep the agricultural region effectively wild dog free scenarios were developed for different levels of baiting trapping and shooting as well as fence permeability we then performed a management strategy evaluation mse based on the examination of wild dog demographic trajectories and social structure in both the agricultural region and the rangelands 2 methods we developed spatially explicit individual based models for predicting abundance and distribution of wild dogs see supplementary material s1 using the hexsim 3 2 8 computer program schumaker 2015 we initially developed a baseline scenario in which no predator control measures were implemented fig 2 we subsequently used modifications of this baseline model to forecast demographic and genetic changes over a 30year time span period resulting from a number of simulated management scenarios table 1 we employed sensitivity analysis to evaluate the consequences of uncertainty in parameters whose values were estimated from sparse or unpublished data 2 1 study area the sbf runs along the interface of the rangelands and the agricultural region of wa the simulation study area incorporates 150km on each side of the northern third of the sbf fig 1 the area on the eastern side of the fence includes livestock grazing properties protected areas and government owned land and is from now on referred to as the rangelands this area comprises occasional low relief ranges to 300m separated by stony slopes and alluvial plains upslope of salt lakes sandy soils on sand plains and granitic country predominate in this region payne et al 1998 within the study area the western side of the sbf has experienced increased wild dog impacts in recent years dafwa unpublished data approximately half of this area is cleared agricultural land used predominantly for crop production and small stock enterprises plus a number of small towns and the city of geraldton population size approx 36 000 although this area includes rangelands for simplicity we refer to it as the agricultural region 2 2 baseline model the study area 118 300km2 was mapped as a raster image comprised of square pixels of 1 3km on each side pixel values were used to represent resource availability a unitless parameter which was determined by the minimum straight line distance to fresh watercourses a maximum resource value of 100 was assigned to pixels falling within the first 500m from watercourses and resource values declined as distance to fresh water increased reaching a lower bound of 60 outside urban areas urban areas themselves were assigned a resource score of 20 hexsim was then used to convert this raster map into a vector array of space filling hexagonal cells each of 1122 4ha and a width of 3 6km measured between parallel sides the hexagons were assigned scores equal to the area weighted mean of the pixel resource availability values falling within their bounds wild dogs are known to be present in the rangelands but are limited to approximately the first 40km to the west of the sbf dafwa unpublished data simulations were initialised with 10 000 individuals randomly distributed in the rangelands and within an area of between 15 and 60km from the sbf within the agricultural region this would generate an initial density of approximately 10 animals per 100km2 as reported in the literature thomson et al 1992a simulation parameters were drawn from published research on biology and ecology of dingo populations in the wa northern rangelands see below for details we assumed that domestic dogs and hybrids could be simulated using the same model parameters as dingoes claridge et al 2014 dingoes are social animals that live in packs and we used a maximum pack size of 13 thomson et al 1992a the maximum pack size parameter limited the joining of packs by lone individuals and forced juveniles to disperse if pack size exceeded 13 after breeding each pack establishes a territory with the maximum allowed territory size being 113km2 thomson et al 1992a from which pack members obtain their required resources pack territories are not overlapping while individual s home ranges are thomson et al 1992a following heinrichs et al 2010 approach we assumed that the smallest observed pack territory i e 44 5km2 or 3 97 hexagons thomson et al 1992a could occur only in the best quality habitats score 100 thus we required territories to contain a minimum cumulative resource availability score of 397 individual resource targets the amount dingoes would consume if resources were unconstrained was set to 40 for adults and juveniles and 5 for yearlings this indirectly imposed a maximum density of approximately 22 animals per 100km2 thomson et al 1992a during the simulations a resource acquisition category low medium high was assigned to each individual based on the percentage of the resource target they were able to obtain each pack was assigned an alpha male and an alpha female and these individuals accounted for the majority of reproduction in the event that one of these dominant individuals died alpha status was assigned to another adult member of the pack at random in roughly 20 of packs a second female was allowed to successfully reproduce with the alpha male as well thomson et al 1992a litter size was drawn from a normal distribution with of mean 5 2 and standard deviation 1 2 individuals that could not join or establish a pack were classified as loners thomson et al 1992a simulated loners were not allowed to reproduce by definition a pair that share a territory would constitute a pack however loners and packs may compete for the same resources data was not available in the literature to estimate the degree of competition between pack members and loners and we set the competition parameter defined in hexsim as pre emption to an arbitrary 10 that is loners were allowed exclusive access to no more than 10 of the resources available in hexagons they shared with a pack given the uncertainty of this parameter we included it in the sensitivity analysis and increased it to 50 which we considered a very high value simulated wild dogs were assigned to age classes including juveniles 12 months yearlings 13 24 months and adults 25 months pack size was maintained through density dependent emigration triggered when membership exceeded 13 see above or when an individual s resources fell below 80 of its target value dispersal priority was stratified by age and resources with non alpha adults being the first to disperse followed by yearlings pack members access to the resources available within a territory was stage stratified with the alpha male s needs being met first then the alpha female and juveniles and finally any non alpha adults and yearlings juveniles were assigned the same resource priority as the alpha female because they derived their resources from the alpha female individuals from a pack dispersed for distances drawn from a lognormal distribution with mean 50 1km and sd 40 3km thomson et al 1992b dispersal distances exceeding 184km thomson et al 1992b were rejected and replaced with another random draw from the distribution loner dispersal distances were drawn from a uniform distribution bound between 11 4 and 42 8km thomson et al 1992b until they joined or establish a pack after dispersal animals explored the landscape in the attempt to establish a home range that met their resource requirements maximum allowed home range size was approximately 134km2 for adult males 78km2 for other pack members that had dispersed and 258km2 for loners thomson 1992 survival rates were stratified by both age and resource class and were modified further to simulate environmental stochasticity age class specific survival rates from thomson et al 1992a were assigned to individuals falling in the medium resource category to include environmental stochasticity in the model a collection of five values defined as the upper and lower 95 confidence interval cis from thomson et al 1992a the mean and the two mid points between the mean and the cis were provided and randomly selected with replacement each simulated year these values were then halved or increased by 50 for the low and high resource class respectively given the lack of data available linking mortality rates to resource intake we performed a sensitivity analysis in which low and high resource individuals survival rates were set to zero and 20 of the value for medium resource individuals respectively we were also interested in evaluating the genetic dynamics associated with possible changes in wild dog demography specifically we wanted to explore whether estimates of genetic differentiation between regions could be used to evaluate the level of demographic separation the sbf imposed on wild dog populations to this end we initialised the simulations with the allele frequencies from 34 microsatellite loci obtained from the same study area stephens et al 2015 and monitored the genetic distance between the wild dog population on the east and west of the sbf over time identical initial allele frequencies were used on both side of the sbf 2 3 management scenarios the sbf was modelled as having four different levels of permeability 100 no fence 5 2 and 0 this range of permeability values reflects the actual uncertainty about wild dogs ability to penetrate the barrier the simulated fence did not impart any mortality individuals were either deflected by it or they passed through it wild dogs are controlled in the study area using a combination of baiting trapping and shooting we simulated three different baiting regimes low medium and high intensity fig 3 based on analysis of western australia department of agriculture and food dafwa landholder permits for the use of poison for wild dog control and expert opinions of dafwa staff and local licenced pest management technicians most of the agricultural region is subjected to a low intensity baiting characterised by occasional deployment of baits after the wild dog breeding season we modelled low intensity baiting by applying additional mortality details below in two randomly selected years within each five year interval we simulated medium intensity baiting as being conducted yearly in the southeast of the rangelands both pre and post breeding high intensity baiting was modelled in the northeast of the study area with one pre and two post breeding baiting rounds being conducted each year all simulated wild dogs were associated to one of the three baiting regimes based on the location of their home range because baiting is usually used in association with another control method no scenarios were developed where baiting occurs without shooting and trapping scenarios with suffix bait shoot in table 1 however within the scenario there might be farms that conduct ground baiting but do not carry out shooting and trapping i e not all control techniques may be applied concurrently in all properties fig 3 we further investigated the potential consequences of two plausible alternative baiting regimes that might realistically be implemented in the agricultural region in the first scenarios with suffix ag low in table 1 all landholders within the agricultural region having current wild dog baiting permits which accounts for 10 of the total agricultural properties within the study area equivalent to 25 of landholders with registered stock brands would shift to a medium level of control the second scenario scenarios with suffix ag hi in table 1 supposes that 22 of properties west of the fence will implement a medium level of control 55 of properties with registered stock brands this 22 figure represents the highest actual density of landholders holding baiting permits within any local government shire in the agricultural region dafwa unpublished data and it includes all the properties with current baiting permits all pastoral properties and a random selection of additional properties in both alternative control strategies shooting and trapping were assumed to also be imposed at all locations were baiting was applied we considered the first of these scenarios as a plausible modest response to wild dog impacts by affected producers in the agricultural region while the second is a plausible but more pronounced response based on our results see below a final strategic high intensity simulated wild dog control assumed that baiting trapping and shooting would be imposed upon the entire area where wild dogs are currently found in the agricultural region plus an additional buffer of approximately 20km in this scenario simulations were conducted in the presence of a completely impermeable fence i e no passage of dogs across the fence which would prevent re invasion in conjunction with a high baiting intensity within the controlled area in the agricultural region and the two levels of additional mortality due to trapping and shooting 5 and 10 scenarios with suffix strat bait shoot see below for details on how shooting and trapping was modelled the mean baiting rate per baiting session in the study area was calculated to be 2 99 baits km based on the number of baits apportioned to properties at bi annual bait preparations meekathara rangeland biosecurity association unpublished data 2009 2015 we conservatively estimated the mortality caused by a baiting event by considering the lowest reported baiting mortality 25 from the lowest baiting rate 8 4km 1 of collared dingoes from thomson 1986 and thomson et al 1992a and calculated the mortality that would be observed using 2 99 baits km assuming a linear relationship i e 0 25 8 4 2 99 0 1 fluctuations in baiting efficiency were simulated by arbitrarily applying a standard deviation of 20 as we do not have enough data to estimate this parameter more accurately this stochasticity was implemented in the models with an approach similar to that used to model stochasticity in natural mortality five values were drawn from the normal distribution parameterised with the mean mortality rate obtained from the fitted exponential function and the 20 standard deviation in each year one of these values is randomly selected and applied after taking into account natural mortality i e removing the proportion of animals that would have already died of natural causes an analogous approach was used to model additional mortality caused by shooting and trapping with the exception that a fixed value rather than several values extracted from a normal distribution was used to simulate the combined effect of these non baiting control methods this is because we only had very limited data on mortality associated with shooting and trapping based on thomson et al 1992a we estimated that up to a 10 of additional mortality can be due to non baiting control methods therefore we used two values 5 and 10 scenarios with suffix shoot05 and shoot10 respectively in table 1 to evaluate what the consequences on the wild dog demography were if a level of control that would cause such additional mortality is regularly applied in the study area the study area was divided in two regions of control fig 3 panel b based on expert opinion of dafwa staff and licenced pest management technicians additional mortality was applied based on the animal s home range 2 4 statistical analysis we ran 200 replicates for each scenario and collated each scenario s results we considered 200 replicates sufficient because no statistical comparisons between mean parameter values were statistically different from estimates obtained from preliminary simulations with only ten replicates data handling for preparation of statistical analysis calculations of descriptive statistics statistical comparisons between scenarios and data plotting was carried out with the r package hexsimr https github com carlopacioni hexsimr in r 3 2 2 r development core team 2015 hexsimr performs pairwise comparisons between scenarios using the strictly standardised mean difference ssmd zhang 2007 which is a convenient statistic in this context because its significance is not influenced by sample size which is large in our case several demographic and genetic parameters were collected s2 for a complete list for each section of the study area i e rangelands and agricultural region each simulated year after the breeding season we then compared mean values at year five and 30 we calculated the mean individual dispersal distance stratified by gender and loner vs pack member mean pack size mean number of packs mean resources obtained by packs and mean territory size using only ten replicates we found that 10 of 200 replicates were sufficient for this analysis because these statistics are derived from each pack data with a total mean population size larger than 10 000 individuals even with only ten replicates these statistics were calculated with more than 300 000 data points because wild dog distribution in the agricultural region is limited to within a 40km wide strip against the sbf we also monitored the progress of the western front of the wild dog distribution by selecting a linear array of patches each of four hexagons 14 4km in length around 4490ha and then determining the occupancy of these patches by at least one animal during the simulations using the function invasion front in hexsimr genetic distance between animals on each side of the sbf was calculated for each replicate and then averaged within each scenario hexsimr quantifies genetic distance using the jost s d jost 2008 as implemented in the r package mmod winter 2012 after having converted the hexsim generated genepop rousset 2008 input file into a genind object with the r package adegenet jombart 2008 3 results 3 1 sensitivity analysis we explored the sensitivity of our model to the competition parameter to evaluate possible demographic and ecological changes associated with this parameter fig s1 increasing the competition parameter did impact the demographic and social structure of wild dog populations in the study area table s1 when the competition parameter was increased to 50 the mean population size was significantly lower p 0 0001 a larger proportion of individuals in each population were loners p 0 0001 and a slightly male biased sex ratio emerged mean 1 1 p 0 033 table s1 these differences were also reflected in the spatial use of the resources simulated wild dog packs were on average smaller p 0 0001 with significantly more resources p 0 006 and territory were marginally smaller p 0 034 sensitivity analysis using less optimistic survival rates generated significantly smaller mean population sizes only at year 5 p 0 034 table s1 in the agricultural region 3 2 fence permeability and the role of wild dog control no statistical differences in demographic projections were found when the same lethal control regime was applied with different levels of sbf permeability due to the presence of wild dogs in the agricultural region the sbf on its own irrespective of the level of permeability applied did not prevent wild dog populations within the agricultural region from reaching carrying capacity in the explored timeframe 30 years further expansion of wild dog distribution into agricultural areas followed similar temporal patterns of approximately 17 3km year sd 27 2 regardless of the permeability of the sbf within each fence permeability scenario wild dog abundance was consistently lowest in scenarios with highest co ordinated baiting and non baiting lethal control regimes in the rangelands fig 4 with a density ranging between 7 and 11 individuals per 100km2 at year 30 as opposed to 15 18 individuals per 100km2 in absence of control when baiting was applied at low intensity in the agricultural region scenarios with suffix bait shoot in table 1 but without the sbf wild dogs were significantly less abundant in this region at year five of the simulations approximately 7 5 individuals per 100km2 rather than 9 5 10 but this was not sustained until year 30 table s2 similarly the two additional scenarios where a higher level of control was modelled in the agricultural region scenarios with suffix ag low and ag hi in table 1 predicted a significantly lower population size only in year five table s2 only when the control was strategically applied across the whole area where wild dogs were present in the agricultural region scenarios with suffix strat bait shoot in table 1 was their abundance significantly reduced for the 30 simulated years approximately 2 5 and 2 5 5 individuals per 100km2 in year 5 and 30 respectively such a level of control effectively prevented the wild dog population from growing for most of the simulated 30 years with the higher mortality from non baiting lethal control resulting in a more marked control in wild dog abundance fig 5 in the absence of baiting non baiting lethal control did not reduce wild dog abundance except at year five when it was modelled to cause a 10 additional mortality table s2 no specific age and sex differences on either side of the fence were found population declines described above were typically a result of a global reduction in abundances the exception to this was the reduction at year 30 in bait shoot 10 which was male biased probably due to a greater propensity for males to be excluded from packs and therefore to become resource deficient when control was carried out the proportion of individuals that were loners was reduced table s2 typically when no control was applied the proportion of loners averaged 18 sd 1 1 range 16 8 19 2 in comparison when extensive control was in place it was reduced to as low as 8 sd 0 5 range 6 9 8 9 table s2 and fig s2 the extent of the reductions seemed inversely proportional to the level of control measures implemented the mean number of pack members pre dispersal was significantly smaller when coordinated non baiting lethal control was carried out table s3 while the total number of packs was not altered accordingly the territory size also tended to be smaller table s3 after 30 years the genetic distance between the wild dogs on the east and west of the fence was small and not significantly different from the baseline model with no fence fig s4 reflecting the relative large population size of each population which was not anticipated and consequent limited genetic drift 4 discussion here we developed models to simulate wild dog population dynamics and used them to determine the effect of different levels of permeability of the sbf and the degree of wild dog control within the agricultural region that would be required to prevent increases in wild dog abundance 4 1 fence permeability and the role of complementary control the main premise for the upgrades to the sbf is that the primary driver for increased wild dog abundance in the agricultural region is immigration from the rangelands however our primary finding is that the presence of an extant population of wild dogs in the agricultural region meant that fence permeability had no significant difference on demographic projections that is invasion from the rangelands is not the only pathway to lead to increases in wild dog abundance and distribution in the agricultural region indeed invasion from the rangelands would appear to have only a minimal role compared to the growth rate that the population in the agricultural region has in the presence of a fully dog proof fence the control of the wild dog population in the agricultural region is a key element for the livestock industry to prevent further increase in wild dog numbers however based on the results we obtained we conclude that to prevent increases of wild dog population in the agricultural region it may be necessary to have a very high participation rate in wild dog control across the agricultural region in fact only when applying combined control with baiting shooting and trapping that strategically encompassed the whole area in the agricultural region where wild dogs are present plus an additional buffer of 20km wild dog population growth was effectively prevented for most of the simulated 30 years on average there was a reduction in the population size of approximately 75 80 and up to 88 when trapping and shooting was applied with a 10 additional mortality overall our results are in agreement with several empirical studies in other areas where intensive and coordinated control effort were recommended to achieve substantial reduction in wild dog populations allen and fleming 2004 allen 2015 on the other hand the modelled control participation rates of 10 and 22 within the agricultural region were insufficient to prevent further increase of wild dog distribution and abundance within the agricultural region and simulations with an optimistic level of participation in wild dog control in the agricultural region 22 of properties only resulted in a short term around 30 reduction in wild dog abundance these 22 of properties were selected partially at random and so they do not address responses to wild dog presence per se additionally the simulated scenarios reflect a static management response to a population of wild dogs which changes in density and distribution over time it was therefore expected that a more targeted response to wild dog presence would have had a more marked effect on wild dog distribution and abundance our simulations demonstrate that substantially reducing and maintaining low wild dog numbers would entail a high level of participation in the agricultural region which would require participation by a large number of cropping enterprises that do not benefit directly from baiting reducing the abundance of wild dogs in the agricultural region should be more achievable than controlling them within the rangelands because wild dog density and distribution is currently more limited in this area however our simulations considered a closed and limited area whereas the agricultural region extends approximately 780km further south to the study area and wild dogs although in very limited numbers have been detected within this region moreover a reduction in wild dog numbers does not guarantee that there will be no livestock losses while there are still benefits for the livestock industries when there is only a partial control of wild dogs binks et al 2015 it is likely that the relationship between wild dog density and impacts is not linear fleming et al 2014 thomson 1986 indicates that prevention of losses for sheep is only achieved when there is 100 removal of resident wild dogs and reinvasion is prevented although limited to cattle enterprises it has also been suggested that incomplete eradication of wild dogs may cause an increase of livestock losses owning to the social structure changes and hunting techniques allen 2015 we acknowledge that the level of mortality that we applied as result of baiting is a conservative value and further research to more precisely estimate this parameter will improve the model prediction accuracy however if the aim is to maintain low wild dog numbers there is a clear need for wild dog management to be undertaken cross tenure to include not only livestock producers but also crop producers and protected areas considering that shooting and trapping did not effectively reduce wild dog abundance when applied without poison baiting effective management of wild dogs in the agricultural region would require a large proportion of landholders to participate in baiting at least at medium intensity based on the results obtained in simulations of the rangelands irrespective of enterprise choice which may be challenging further because of the likely need for sustained high intensity and spatially extensive control with the uncertainty of financial returns when partial removal is achieved there may be a need to consider additional approaches to the protection of small stock industries from wild dog impacts in the agricultural region our modelling focused on the use of baiting trapping and shooting however additional tools may be applied to reduce wild dog impacts and population changes specifically the use of canid pest ejectors is one tool that could assist as demonstrated for other canids fleming et al 2006 hooke et al 2006 non lethal methods such as property and cluster fencing boundary fencing of a number of adjacent properties allen and west 2013 and the use of guardian animals van bommel and johnson 2012 may also be complimentary methods for reducing wild dog impacts to this end trials with alternative techniques should also be promoted in this area to optimise control methods that maximise the benefit cost ratio it should be noted that we deliberately did not include any indirect effects on farming due to wild dog presence e g potential for kangaroo control letnic and crowther 2013 prowse et al 2015 or transmission of diseases king et al 2011 nor considered wild dogs potential additional ecological roles in our analysis because to our knowledge there are not sufficient data to adequately model such aspects in a temperate mediterranean climate such as that of the southwest of western australia and such effects are beyond the scope of this paper however we recognise that wild dog abundance is not the only element determining their impact we also acknowledge that we did not assess the economic viability of an integrated control strategy and recommend that this should be addressed via economic modelling to inform the industry on the cost benefit ratio before such a large investment is carried out 4 2 effects of control on social structure our modelling suggests that wild dog control causes a shift in the social structure with the proportion of loners in the population being inversely proportional to the persecution effort this is likely related to behavioural differences between loners and pack members the former have a tendency to explore larger areas increasing their likelihood of encountering baits or being trapped shot these results reflect a similar pattern in baiting trials in north western australia thomson 1986 but diverge from allen 2015 who found a disproportionate effect of baiting on juvenile animals and social changes resulting from recolonisation of baited areas by yearling animals we recognise that this is an area in need of further empirical investigation 4 3 sensitivity analysis we carried out a sensitivity analysis for the competition and survival rate parameters to assess their effects on the simulations increasing the competition parameter increased the proportion of loners in the population as they were able to acquire more resources when this parameter was increased however because loners do not contribute to reproduction this caused the overall population growth rate to fall reducing the population size at 30 years while we do not have sufficient field data to accurately determine the level of competition between loners and pack members we maintained the selected baseline value of 10 for the competition parameter because it resulted in a social structure that approximates published data that is 10 15 of individuals are loners thomson et al 1992a modifying the survival rates had a significant effect only on the initial growth of the population in the agricultural region in the baseline and management scenarios we used relatively wide survival intervals and allowed the models to fluctuate between these intervals to simulate stochasticity while the data we used were the best available to us it is possible that our models are overestimating the population size of wild dogs especially when the populations are small if their survival is actually lower than we modelled or if environmental changes compromise wild dog survival we inspected a number of secondary predictions to further establish that the models were adequately parameterised for example descriptive statistics of the baseline scenario were in line with general expectations from field data the mean number of pack members was between 10 and 15 the mean territory size was around 80km2 the mean distance covered by loners was about 17km and around 23km by members of a pack that dispersed thomson 1992 thomson rose and kok 1992b moreover the growth rate and progress of the western front of the wild dog distribution were both in line with intervals considered realistic for the species prowse et al 2013 smith 2015 5 conclusions this study is an example of how population modelling can be used to evaluate management options for wildlife populations and has the benefit of quantifying possible outcomes resulting from the implementation of wildlife management actions and as a result the definition of management targets our analysis identified that the sbf and current control practices are not sufficient to reduce the abundance of wild dogs in the agricultural region because 1 a self sustaining wild dog population is already extant in this region and 2 existing control in the agricultural region does not impose sufficient mortality to effectively reduce abundance in order to maintain the agricultural region effectively dog free control effort in this region needs to exceed the current effort and be sustained across the whole area where wild dogs are currently distributed plus an additional buffer of 20km this presents a challenge because of the spatial extent of the control required and because it would require participation in the control operations of not only livestock producers but also other agricultural and non agricultural enterprises acknowledgments we would like to thank j miller c robins and g mcdonald dafwa to provide expert opinion on baiting programs m stadler dafwa for information on fence management and k rose dafwa for clarification and expert opinion on dingo ecology we are also grateful to r jacob dafwa for feedback on sheep industry and s campbell dafwa for comments on early draft of this manuscript this project was funded by the royalties for regions the information in this document has been funded in part by the u s environmental protection agency it has been subjected to review by the national health and environmental effects research laboratory s western ecology division and approved for publication approval does not signify that the contents reflect the views of the agency nor does mention of trade names or commercial products constitute endorsement or recommendation for use appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j ecolmodel 2017 12 001 appendix a supplementary data the following are supplementary data to this article 
25359,large predators can significantly impact livestock industries in australia wild dogs canis lupus familiaris canis lupus dingo and hybrids cause economic losses of more than aud 40m annually landscape scale exclusion fencing coupled with lethal techniques is a widely practiced control method in western australia the state barrier fence encompasses approximately 260 000km2 of predominantly agricultural land but its effectiveness in preventing wild dogs from entering the agricultural region is difficult to evaluate we conducted a management strategy evaluation mse based on spatially explicit population models to forecast the effects of upgrades to the western australian state barrier fence and several control scenarios varying in intensity and spatial extent on wild dog populations in southwest western australia the model results indicate that populations of wild dogs on both sides of the state barrier fence are self sustaining and current control practices are not sufficient to effectively reduce their abundance in the agricultural region only when a combination of control techniques is applied on a large scale intensively and continuously are wild dog numbers effectively controlled this study identifies the requirement for addressing extant populations of predators within fenced areas to meet the objective of preventing wild dog expansion this objective is only achieved when control is applied to the whole area where wild dogs are currently present within the fence plus an additional buffer of 20km our modelling focused on the use of baiting trapping and shooting however we acknowledge that additional tools may also be applied finally we recommend that a cost benefit analysis be performed to evaluate the economic viability of an integrated control strategy 1 introduction with the global expansion of production landscapes areas of native habitat are diminishing foley et al 2005 often forcing wildlife and livestock to co exist compete for resources and increasing the frequency of their interactions significant human wildlife conflicts and economic challenges can result when predators impact production systems as occurs through predation on livestock by wolves and bears in north america and europe by felids in asia and south america and a range of species in africa treves and karanth 2003 inskip and zimmermann 2009 treves et al 2009 in australia wild dogs defined as dingoes canis lupus dingo free living domestic dogs canis lupus familiaris and their hybrids are important predators of livestock and can have significant economic impacts these impacts result from direct predation stock harassment and the transmission of disease fleming et al 2014 in australia total economic surplus losses due to wild dogs are estimated at aud 21 9m to sheep industries and aud 26 7m to the cattle industry gong et al 2009 more recent estimates put yearly losses to the western australia wa sheep industry at 14m and to the wa rangeland goat industry at 11m bell 2015 small stock sheep and goats are so vulnerable to predation by wild dogs that it is generally considered that sheep enterprises and wild dogs cannot coexist thomson 1984 newsome 2001 fleming et al 2014 the primary mechanisms for the control of wild dogs in australia are lethal means including trapping shooting and poison baiting fleming et al 2014 in some areas such as the southwest of western australia these control strategies are augmented with landscape scale fences landscape scale fences separating high value agricultural areas from predators have been used for over a century in australia caughley et al 1980 newsome et al 2001 and are being used increasingly in predator control programs binks et al 2015 the state barrier fence sbf runs 1190km in a northwest southeast direction and broadly separates a higher rainfall agricultural region suited to high value stock and cropping from semi arid to arid rangelands fig 1 originally designed to prevent the spread of european rabbits oryctolagus cuniculus from the rangelands into the agricultural region the sbf has also reduced the impacts of larger vertebrates such as emus aves dromaius novaehollandiae and wild dogs in the agricultural region crawford 1967 broomhall 1991 ritchie 1997 however in the last decade wild dog impacts have increased at the interface of the agricultural region and the rangelands dafwa unpublished data wild dog control is becoming a priority for landholder biosecurity groups at the periphery of the agricultural region and in 2014 the entire sbf was upgraded at significant cost to meet wild dog exclusion standards by adding a lap wire protruding from the fence at 45 to prevent tunnelling the upgraded fence s impacts on wild dog abundance and distribution have not been investigated making it difficult to gauge its cost effectiveness the extent to which additional complementary predator controls will be required to protect livestock in the agricultural region has also not been examined population modelling has been recommended to explore the relative effectiveness of different management options starfield 1997 morris and doak 2002 campbell et al 2015 this approach offers the advantage of identifying targets and timeframes for monitoring programs himes boor 2014 here we use spatially explicit demographic models to evaluate whether current or foreseeable management practices along with the sbf are adequate to keep the agricultural region effectively wild dog free scenarios were developed for different levels of baiting trapping and shooting as well as fence permeability we then performed a management strategy evaluation mse based on the examination of wild dog demographic trajectories and social structure in both the agricultural region and the rangelands 2 methods we developed spatially explicit individual based models for predicting abundance and distribution of wild dogs see supplementary material s1 using the hexsim 3 2 8 computer program schumaker 2015 we initially developed a baseline scenario in which no predator control measures were implemented fig 2 we subsequently used modifications of this baseline model to forecast demographic and genetic changes over a 30year time span period resulting from a number of simulated management scenarios table 1 we employed sensitivity analysis to evaluate the consequences of uncertainty in parameters whose values were estimated from sparse or unpublished data 2 1 study area the sbf runs along the interface of the rangelands and the agricultural region of wa the simulation study area incorporates 150km on each side of the northern third of the sbf fig 1 the area on the eastern side of the fence includes livestock grazing properties protected areas and government owned land and is from now on referred to as the rangelands this area comprises occasional low relief ranges to 300m separated by stony slopes and alluvial plains upslope of salt lakes sandy soils on sand plains and granitic country predominate in this region payne et al 1998 within the study area the western side of the sbf has experienced increased wild dog impacts in recent years dafwa unpublished data approximately half of this area is cleared agricultural land used predominantly for crop production and small stock enterprises plus a number of small towns and the city of geraldton population size approx 36 000 although this area includes rangelands for simplicity we refer to it as the agricultural region 2 2 baseline model the study area 118 300km2 was mapped as a raster image comprised of square pixels of 1 3km on each side pixel values were used to represent resource availability a unitless parameter which was determined by the minimum straight line distance to fresh watercourses a maximum resource value of 100 was assigned to pixels falling within the first 500m from watercourses and resource values declined as distance to fresh water increased reaching a lower bound of 60 outside urban areas urban areas themselves were assigned a resource score of 20 hexsim was then used to convert this raster map into a vector array of space filling hexagonal cells each of 1122 4ha and a width of 3 6km measured between parallel sides the hexagons were assigned scores equal to the area weighted mean of the pixel resource availability values falling within their bounds wild dogs are known to be present in the rangelands but are limited to approximately the first 40km to the west of the sbf dafwa unpublished data simulations were initialised with 10 000 individuals randomly distributed in the rangelands and within an area of between 15 and 60km from the sbf within the agricultural region this would generate an initial density of approximately 10 animals per 100km2 as reported in the literature thomson et al 1992a simulation parameters were drawn from published research on biology and ecology of dingo populations in the wa northern rangelands see below for details we assumed that domestic dogs and hybrids could be simulated using the same model parameters as dingoes claridge et al 2014 dingoes are social animals that live in packs and we used a maximum pack size of 13 thomson et al 1992a the maximum pack size parameter limited the joining of packs by lone individuals and forced juveniles to disperse if pack size exceeded 13 after breeding each pack establishes a territory with the maximum allowed territory size being 113km2 thomson et al 1992a from which pack members obtain their required resources pack territories are not overlapping while individual s home ranges are thomson et al 1992a following heinrichs et al 2010 approach we assumed that the smallest observed pack territory i e 44 5km2 or 3 97 hexagons thomson et al 1992a could occur only in the best quality habitats score 100 thus we required territories to contain a minimum cumulative resource availability score of 397 individual resource targets the amount dingoes would consume if resources were unconstrained was set to 40 for adults and juveniles and 5 for yearlings this indirectly imposed a maximum density of approximately 22 animals per 100km2 thomson et al 1992a during the simulations a resource acquisition category low medium high was assigned to each individual based on the percentage of the resource target they were able to obtain each pack was assigned an alpha male and an alpha female and these individuals accounted for the majority of reproduction in the event that one of these dominant individuals died alpha status was assigned to another adult member of the pack at random in roughly 20 of packs a second female was allowed to successfully reproduce with the alpha male as well thomson et al 1992a litter size was drawn from a normal distribution with of mean 5 2 and standard deviation 1 2 individuals that could not join or establish a pack were classified as loners thomson et al 1992a simulated loners were not allowed to reproduce by definition a pair that share a territory would constitute a pack however loners and packs may compete for the same resources data was not available in the literature to estimate the degree of competition between pack members and loners and we set the competition parameter defined in hexsim as pre emption to an arbitrary 10 that is loners were allowed exclusive access to no more than 10 of the resources available in hexagons they shared with a pack given the uncertainty of this parameter we included it in the sensitivity analysis and increased it to 50 which we considered a very high value simulated wild dogs were assigned to age classes including juveniles 12 months yearlings 13 24 months and adults 25 months pack size was maintained through density dependent emigration triggered when membership exceeded 13 see above or when an individual s resources fell below 80 of its target value dispersal priority was stratified by age and resources with non alpha adults being the first to disperse followed by yearlings pack members access to the resources available within a territory was stage stratified with the alpha male s needs being met first then the alpha female and juveniles and finally any non alpha adults and yearlings juveniles were assigned the same resource priority as the alpha female because they derived their resources from the alpha female individuals from a pack dispersed for distances drawn from a lognormal distribution with mean 50 1km and sd 40 3km thomson et al 1992b dispersal distances exceeding 184km thomson et al 1992b were rejected and replaced with another random draw from the distribution loner dispersal distances were drawn from a uniform distribution bound between 11 4 and 42 8km thomson et al 1992b until they joined or establish a pack after dispersal animals explored the landscape in the attempt to establish a home range that met their resource requirements maximum allowed home range size was approximately 134km2 for adult males 78km2 for other pack members that had dispersed and 258km2 for loners thomson 1992 survival rates were stratified by both age and resource class and were modified further to simulate environmental stochasticity age class specific survival rates from thomson et al 1992a were assigned to individuals falling in the medium resource category to include environmental stochasticity in the model a collection of five values defined as the upper and lower 95 confidence interval cis from thomson et al 1992a the mean and the two mid points between the mean and the cis were provided and randomly selected with replacement each simulated year these values were then halved or increased by 50 for the low and high resource class respectively given the lack of data available linking mortality rates to resource intake we performed a sensitivity analysis in which low and high resource individuals survival rates were set to zero and 20 of the value for medium resource individuals respectively we were also interested in evaluating the genetic dynamics associated with possible changes in wild dog demography specifically we wanted to explore whether estimates of genetic differentiation between regions could be used to evaluate the level of demographic separation the sbf imposed on wild dog populations to this end we initialised the simulations with the allele frequencies from 34 microsatellite loci obtained from the same study area stephens et al 2015 and monitored the genetic distance between the wild dog population on the east and west of the sbf over time identical initial allele frequencies were used on both side of the sbf 2 3 management scenarios the sbf was modelled as having four different levels of permeability 100 no fence 5 2 and 0 this range of permeability values reflects the actual uncertainty about wild dogs ability to penetrate the barrier the simulated fence did not impart any mortality individuals were either deflected by it or they passed through it wild dogs are controlled in the study area using a combination of baiting trapping and shooting we simulated three different baiting regimes low medium and high intensity fig 3 based on analysis of western australia department of agriculture and food dafwa landholder permits for the use of poison for wild dog control and expert opinions of dafwa staff and local licenced pest management technicians most of the agricultural region is subjected to a low intensity baiting characterised by occasional deployment of baits after the wild dog breeding season we modelled low intensity baiting by applying additional mortality details below in two randomly selected years within each five year interval we simulated medium intensity baiting as being conducted yearly in the southeast of the rangelands both pre and post breeding high intensity baiting was modelled in the northeast of the study area with one pre and two post breeding baiting rounds being conducted each year all simulated wild dogs were associated to one of the three baiting regimes based on the location of their home range because baiting is usually used in association with another control method no scenarios were developed where baiting occurs without shooting and trapping scenarios with suffix bait shoot in table 1 however within the scenario there might be farms that conduct ground baiting but do not carry out shooting and trapping i e not all control techniques may be applied concurrently in all properties fig 3 we further investigated the potential consequences of two plausible alternative baiting regimes that might realistically be implemented in the agricultural region in the first scenarios with suffix ag low in table 1 all landholders within the agricultural region having current wild dog baiting permits which accounts for 10 of the total agricultural properties within the study area equivalent to 25 of landholders with registered stock brands would shift to a medium level of control the second scenario scenarios with suffix ag hi in table 1 supposes that 22 of properties west of the fence will implement a medium level of control 55 of properties with registered stock brands this 22 figure represents the highest actual density of landholders holding baiting permits within any local government shire in the agricultural region dafwa unpublished data and it includes all the properties with current baiting permits all pastoral properties and a random selection of additional properties in both alternative control strategies shooting and trapping were assumed to also be imposed at all locations were baiting was applied we considered the first of these scenarios as a plausible modest response to wild dog impacts by affected producers in the agricultural region while the second is a plausible but more pronounced response based on our results see below a final strategic high intensity simulated wild dog control assumed that baiting trapping and shooting would be imposed upon the entire area where wild dogs are currently found in the agricultural region plus an additional buffer of approximately 20km in this scenario simulations were conducted in the presence of a completely impermeable fence i e no passage of dogs across the fence which would prevent re invasion in conjunction with a high baiting intensity within the controlled area in the agricultural region and the two levels of additional mortality due to trapping and shooting 5 and 10 scenarios with suffix strat bait shoot see below for details on how shooting and trapping was modelled the mean baiting rate per baiting session in the study area was calculated to be 2 99 baits km based on the number of baits apportioned to properties at bi annual bait preparations meekathara rangeland biosecurity association unpublished data 2009 2015 we conservatively estimated the mortality caused by a baiting event by considering the lowest reported baiting mortality 25 from the lowest baiting rate 8 4km 1 of collared dingoes from thomson 1986 and thomson et al 1992a and calculated the mortality that would be observed using 2 99 baits km assuming a linear relationship i e 0 25 8 4 2 99 0 1 fluctuations in baiting efficiency were simulated by arbitrarily applying a standard deviation of 20 as we do not have enough data to estimate this parameter more accurately this stochasticity was implemented in the models with an approach similar to that used to model stochasticity in natural mortality five values were drawn from the normal distribution parameterised with the mean mortality rate obtained from the fitted exponential function and the 20 standard deviation in each year one of these values is randomly selected and applied after taking into account natural mortality i e removing the proportion of animals that would have already died of natural causes an analogous approach was used to model additional mortality caused by shooting and trapping with the exception that a fixed value rather than several values extracted from a normal distribution was used to simulate the combined effect of these non baiting control methods this is because we only had very limited data on mortality associated with shooting and trapping based on thomson et al 1992a we estimated that up to a 10 of additional mortality can be due to non baiting control methods therefore we used two values 5 and 10 scenarios with suffix shoot05 and shoot10 respectively in table 1 to evaluate what the consequences on the wild dog demography were if a level of control that would cause such additional mortality is regularly applied in the study area the study area was divided in two regions of control fig 3 panel b based on expert opinion of dafwa staff and licenced pest management technicians additional mortality was applied based on the animal s home range 2 4 statistical analysis we ran 200 replicates for each scenario and collated each scenario s results we considered 200 replicates sufficient because no statistical comparisons between mean parameter values were statistically different from estimates obtained from preliminary simulations with only ten replicates data handling for preparation of statistical analysis calculations of descriptive statistics statistical comparisons between scenarios and data plotting was carried out with the r package hexsimr https github com carlopacioni hexsimr in r 3 2 2 r development core team 2015 hexsimr performs pairwise comparisons between scenarios using the strictly standardised mean difference ssmd zhang 2007 which is a convenient statistic in this context because its significance is not influenced by sample size which is large in our case several demographic and genetic parameters were collected s2 for a complete list for each section of the study area i e rangelands and agricultural region each simulated year after the breeding season we then compared mean values at year five and 30 we calculated the mean individual dispersal distance stratified by gender and loner vs pack member mean pack size mean number of packs mean resources obtained by packs and mean territory size using only ten replicates we found that 10 of 200 replicates were sufficient for this analysis because these statistics are derived from each pack data with a total mean population size larger than 10 000 individuals even with only ten replicates these statistics were calculated with more than 300 000 data points because wild dog distribution in the agricultural region is limited to within a 40km wide strip against the sbf we also monitored the progress of the western front of the wild dog distribution by selecting a linear array of patches each of four hexagons 14 4km in length around 4490ha and then determining the occupancy of these patches by at least one animal during the simulations using the function invasion front in hexsimr genetic distance between animals on each side of the sbf was calculated for each replicate and then averaged within each scenario hexsimr quantifies genetic distance using the jost s d jost 2008 as implemented in the r package mmod winter 2012 after having converted the hexsim generated genepop rousset 2008 input file into a genind object with the r package adegenet jombart 2008 3 results 3 1 sensitivity analysis we explored the sensitivity of our model to the competition parameter to evaluate possible demographic and ecological changes associated with this parameter fig s1 increasing the competition parameter did impact the demographic and social structure of wild dog populations in the study area table s1 when the competition parameter was increased to 50 the mean population size was significantly lower p 0 0001 a larger proportion of individuals in each population were loners p 0 0001 and a slightly male biased sex ratio emerged mean 1 1 p 0 033 table s1 these differences were also reflected in the spatial use of the resources simulated wild dog packs were on average smaller p 0 0001 with significantly more resources p 0 006 and territory were marginally smaller p 0 034 sensitivity analysis using less optimistic survival rates generated significantly smaller mean population sizes only at year 5 p 0 034 table s1 in the agricultural region 3 2 fence permeability and the role of wild dog control no statistical differences in demographic projections were found when the same lethal control regime was applied with different levels of sbf permeability due to the presence of wild dogs in the agricultural region the sbf on its own irrespective of the level of permeability applied did not prevent wild dog populations within the agricultural region from reaching carrying capacity in the explored timeframe 30 years further expansion of wild dog distribution into agricultural areas followed similar temporal patterns of approximately 17 3km year sd 27 2 regardless of the permeability of the sbf within each fence permeability scenario wild dog abundance was consistently lowest in scenarios with highest co ordinated baiting and non baiting lethal control regimes in the rangelands fig 4 with a density ranging between 7 and 11 individuals per 100km2 at year 30 as opposed to 15 18 individuals per 100km2 in absence of control when baiting was applied at low intensity in the agricultural region scenarios with suffix bait shoot in table 1 but without the sbf wild dogs were significantly less abundant in this region at year five of the simulations approximately 7 5 individuals per 100km2 rather than 9 5 10 but this was not sustained until year 30 table s2 similarly the two additional scenarios where a higher level of control was modelled in the agricultural region scenarios with suffix ag low and ag hi in table 1 predicted a significantly lower population size only in year five table s2 only when the control was strategically applied across the whole area where wild dogs were present in the agricultural region scenarios with suffix strat bait shoot in table 1 was their abundance significantly reduced for the 30 simulated years approximately 2 5 and 2 5 5 individuals per 100km2 in year 5 and 30 respectively such a level of control effectively prevented the wild dog population from growing for most of the simulated 30 years with the higher mortality from non baiting lethal control resulting in a more marked control in wild dog abundance fig 5 in the absence of baiting non baiting lethal control did not reduce wild dog abundance except at year five when it was modelled to cause a 10 additional mortality table s2 no specific age and sex differences on either side of the fence were found population declines described above were typically a result of a global reduction in abundances the exception to this was the reduction at year 30 in bait shoot 10 which was male biased probably due to a greater propensity for males to be excluded from packs and therefore to become resource deficient when control was carried out the proportion of individuals that were loners was reduced table s2 typically when no control was applied the proportion of loners averaged 18 sd 1 1 range 16 8 19 2 in comparison when extensive control was in place it was reduced to as low as 8 sd 0 5 range 6 9 8 9 table s2 and fig s2 the extent of the reductions seemed inversely proportional to the level of control measures implemented the mean number of pack members pre dispersal was significantly smaller when coordinated non baiting lethal control was carried out table s3 while the total number of packs was not altered accordingly the territory size also tended to be smaller table s3 after 30 years the genetic distance between the wild dogs on the east and west of the fence was small and not significantly different from the baseline model with no fence fig s4 reflecting the relative large population size of each population which was not anticipated and consequent limited genetic drift 4 discussion here we developed models to simulate wild dog population dynamics and used them to determine the effect of different levels of permeability of the sbf and the degree of wild dog control within the agricultural region that would be required to prevent increases in wild dog abundance 4 1 fence permeability and the role of complementary control the main premise for the upgrades to the sbf is that the primary driver for increased wild dog abundance in the agricultural region is immigration from the rangelands however our primary finding is that the presence of an extant population of wild dogs in the agricultural region meant that fence permeability had no significant difference on demographic projections that is invasion from the rangelands is not the only pathway to lead to increases in wild dog abundance and distribution in the agricultural region indeed invasion from the rangelands would appear to have only a minimal role compared to the growth rate that the population in the agricultural region has in the presence of a fully dog proof fence the control of the wild dog population in the agricultural region is a key element for the livestock industry to prevent further increase in wild dog numbers however based on the results we obtained we conclude that to prevent increases of wild dog population in the agricultural region it may be necessary to have a very high participation rate in wild dog control across the agricultural region in fact only when applying combined control with baiting shooting and trapping that strategically encompassed the whole area in the agricultural region where wild dogs are present plus an additional buffer of 20km wild dog population growth was effectively prevented for most of the simulated 30 years on average there was a reduction in the population size of approximately 75 80 and up to 88 when trapping and shooting was applied with a 10 additional mortality overall our results are in agreement with several empirical studies in other areas where intensive and coordinated control effort were recommended to achieve substantial reduction in wild dog populations allen and fleming 2004 allen 2015 on the other hand the modelled control participation rates of 10 and 22 within the agricultural region were insufficient to prevent further increase of wild dog distribution and abundance within the agricultural region and simulations with an optimistic level of participation in wild dog control in the agricultural region 22 of properties only resulted in a short term around 30 reduction in wild dog abundance these 22 of properties were selected partially at random and so they do not address responses to wild dog presence per se additionally the simulated scenarios reflect a static management response to a population of wild dogs which changes in density and distribution over time it was therefore expected that a more targeted response to wild dog presence would have had a more marked effect on wild dog distribution and abundance our simulations demonstrate that substantially reducing and maintaining low wild dog numbers would entail a high level of participation in the agricultural region which would require participation by a large number of cropping enterprises that do not benefit directly from baiting reducing the abundance of wild dogs in the agricultural region should be more achievable than controlling them within the rangelands because wild dog density and distribution is currently more limited in this area however our simulations considered a closed and limited area whereas the agricultural region extends approximately 780km further south to the study area and wild dogs although in very limited numbers have been detected within this region moreover a reduction in wild dog numbers does not guarantee that there will be no livestock losses while there are still benefits for the livestock industries when there is only a partial control of wild dogs binks et al 2015 it is likely that the relationship between wild dog density and impacts is not linear fleming et al 2014 thomson 1986 indicates that prevention of losses for sheep is only achieved when there is 100 removal of resident wild dogs and reinvasion is prevented although limited to cattle enterprises it has also been suggested that incomplete eradication of wild dogs may cause an increase of livestock losses owning to the social structure changes and hunting techniques allen 2015 we acknowledge that the level of mortality that we applied as result of baiting is a conservative value and further research to more precisely estimate this parameter will improve the model prediction accuracy however if the aim is to maintain low wild dog numbers there is a clear need for wild dog management to be undertaken cross tenure to include not only livestock producers but also crop producers and protected areas considering that shooting and trapping did not effectively reduce wild dog abundance when applied without poison baiting effective management of wild dogs in the agricultural region would require a large proportion of landholders to participate in baiting at least at medium intensity based on the results obtained in simulations of the rangelands irrespective of enterprise choice which may be challenging further because of the likely need for sustained high intensity and spatially extensive control with the uncertainty of financial returns when partial removal is achieved there may be a need to consider additional approaches to the protection of small stock industries from wild dog impacts in the agricultural region our modelling focused on the use of baiting trapping and shooting however additional tools may be applied to reduce wild dog impacts and population changes specifically the use of canid pest ejectors is one tool that could assist as demonstrated for other canids fleming et al 2006 hooke et al 2006 non lethal methods such as property and cluster fencing boundary fencing of a number of adjacent properties allen and west 2013 and the use of guardian animals van bommel and johnson 2012 may also be complimentary methods for reducing wild dog impacts to this end trials with alternative techniques should also be promoted in this area to optimise control methods that maximise the benefit cost ratio it should be noted that we deliberately did not include any indirect effects on farming due to wild dog presence e g potential for kangaroo control letnic and crowther 2013 prowse et al 2015 or transmission of diseases king et al 2011 nor considered wild dogs potential additional ecological roles in our analysis because to our knowledge there are not sufficient data to adequately model such aspects in a temperate mediterranean climate such as that of the southwest of western australia and such effects are beyond the scope of this paper however we recognise that wild dog abundance is not the only element determining their impact we also acknowledge that we did not assess the economic viability of an integrated control strategy and recommend that this should be addressed via economic modelling to inform the industry on the cost benefit ratio before such a large investment is carried out 4 2 effects of control on social structure our modelling suggests that wild dog control causes a shift in the social structure with the proportion of loners in the population being inversely proportional to the persecution effort this is likely related to behavioural differences between loners and pack members the former have a tendency to explore larger areas increasing their likelihood of encountering baits or being trapped shot these results reflect a similar pattern in baiting trials in north western australia thomson 1986 but diverge from allen 2015 who found a disproportionate effect of baiting on juvenile animals and social changes resulting from recolonisation of baited areas by yearling animals we recognise that this is an area in need of further empirical investigation 4 3 sensitivity analysis we carried out a sensitivity analysis for the competition and survival rate parameters to assess their effects on the simulations increasing the competition parameter increased the proportion of loners in the population as they were able to acquire more resources when this parameter was increased however because loners do not contribute to reproduction this caused the overall population growth rate to fall reducing the population size at 30 years while we do not have sufficient field data to accurately determine the level of competition between loners and pack members we maintained the selected baseline value of 10 for the competition parameter because it resulted in a social structure that approximates published data that is 10 15 of individuals are loners thomson et al 1992a modifying the survival rates had a significant effect only on the initial growth of the population in the agricultural region in the baseline and management scenarios we used relatively wide survival intervals and allowed the models to fluctuate between these intervals to simulate stochasticity while the data we used were the best available to us it is possible that our models are overestimating the population size of wild dogs especially when the populations are small if their survival is actually lower than we modelled or if environmental changes compromise wild dog survival we inspected a number of secondary predictions to further establish that the models were adequately parameterised for example descriptive statistics of the baseline scenario were in line with general expectations from field data the mean number of pack members was between 10 and 15 the mean territory size was around 80km2 the mean distance covered by loners was about 17km and around 23km by members of a pack that dispersed thomson 1992 thomson rose and kok 1992b moreover the growth rate and progress of the western front of the wild dog distribution were both in line with intervals considered realistic for the species prowse et al 2013 smith 2015 5 conclusions this study is an example of how population modelling can be used to evaluate management options for wildlife populations and has the benefit of quantifying possible outcomes resulting from the implementation of wildlife management actions and as a result the definition of management targets our analysis identified that the sbf and current control practices are not sufficient to reduce the abundance of wild dogs in the agricultural region because 1 a self sustaining wild dog population is already extant in this region and 2 existing control in the agricultural region does not impose sufficient mortality to effectively reduce abundance in order to maintain the agricultural region effectively dog free control effort in this region needs to exceed the current effort and be sustained across the whole area where wild dogs are currently distributed plus an additional buffer of 20km this presents a challenge because of the spatial extent of the control required and because it would require participation in the control operations of not only livestock producers but also other agricultural and non agricultural enterprises acknowledgments we would like to thank j miller c robins and g mcdonald dafwa to provide expert opinion on baiting programs m stadler dafwa for information on fence management and k rose dafwa for clarification and expert opinion on dingo ecology we are also grateful to r jacob dafwa for feedback on sheep industry and s campbell dafwa for comments on early draft of this manuscript this project was funded by the royalties for regions the information in this document has been funded in part by the u s environmental protection agency it has been subjected to review by the national health and environmental effects research laboratory s western ecology division and approved for publication approval does not signify that the contents reflect the views of the agency nor does mention of trade names or commercial products constitute endorsement or recommendation for use appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j ecolmodel 2017 12 001 appendix a supplementary data the following are supplementary data to this article 
