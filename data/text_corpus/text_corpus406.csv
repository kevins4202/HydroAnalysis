index,text
2030,model calibration is the procedure of finding model settings such that simulated model outputs best match the observed data model calibration is necessary when the model parameters cannot directly be measured as is the case with a wide range of environmental models where parameters are conceptually describing upscaled and effective physical processes model calibration is therefore an important step of environmental modeling as the model might otherwise provide random outputs if never compared to a ground truth model calibration itself is often referred to be an art due to its plenitude of intertwined steps and necessary decisions along the way before a calibration can be carried out or can be regarded successful this work provides a general guide specifying which steps a modeler needs to undertake how to diagnose the success of each step and how to identify the right action to revise steps that were not successful the procedure is formalized into ten iterative steps generally appearing in calibration experiments each step of this calibration life cycle is either illustrated with an exemplary calibration experiment or providing an explicit checklist the modeler can follow these ten strategies are 1 using sensitivity information to guide the calibration 2 handling of parameters with constraints 3 handling of data ranging orders of magnitude 4 choosing the data to base the calibration on 5 presenting various methods to sample model parameters 6 finding appropriate parameter ranges 7 choosing objective functions 8 selecting a calibration algorithm 9 determining the success and quality of a multi objective calibration and 10 providing a checklist to diagnose calibration performance using ideas introduced in the previous steps the formal definition of strategies through the calibration process is providing an overview while shedding a light on connections between these main ingredients to calibrate an environmental model and will therefore enable especially novice modelers to succeed keywords model calibration parameter sampling calibration strategies data availability the data and codes are made available on zenodo https doi org 10 5281 zenodo 7563321 1 introduction environmental models such as hydrologic models water quality models and ecohydrologic models are created to increase the understanding of environmental processes while allowing to study the impact of changes made to the system in computational experiments which might be too costly or impossible to perform in reality the models are approximations of the natural systems in an attempt to reproduce the relevant behavioral characteristics of the true natural system wagener and mcintyre 2007 creating these models in itself is a multi stage process including model study planning data and concepts used model setup calibration and validation as well as simulation and evaluation refsgaard et al 2007 the step of model calibration is necessary for most environmental models as they contain parameters that are either upscaled and or conceptual and can hence not be determined using field measurements kirchner 2006 model calibration is therefore used to infer optimal model parameter settings leading to a good match with those available observations while model calibration seems like a task of secondary importance compared to creating the model itself collecting data and formulating target purpose of the model it is replete with challenges such as a multiple model setups can lead to equally good model outputs equifinality and hence multiple equally likely model realizations all potentially right for the wrong reasons e g beven 1993 beven and freer 2001 khatami et al 2019 b unidentifiable parameters given available data and calibration setup leading to parameters that cannot be transferred as their calibrated values are purely random e g wagener et al 2003 guse et al 2020 c calibrating multiple and or nested basins simultaneously leading to increased model runtimes and the risk that a parameter set that works everywhere might not exist in case parameters are not regionalized properly e g wi et al 2015 rakovec et al 2016 2019 d numerical daemons leading to unstable model runs and jumps in the objective function that hamper the calibration process e g kavetski et al 2006 clark and kavetski 2010 kavetski and clark 2010 e difficulties of defining an objective function measuring the quality of a model fit that is coherent with the purpose the model was built for e g schaefli and gupta 2007 gupta et al 2009 mizukami et al 2019 clark et al 2021 f deciding how to split available data appropriately into sets that are used for model calibration and validation evaluation leading to model settings that might not be robust enough to future changes in case they are only trained with past data or not able to predict responses to extreme events if trained with only a small set of data e g daggupati et al 2015 arsenault et al 2018 zheng et al 2018 2022 shen et al 2022 or g inherent uncertainties associated to model structure input data uncertainties and uncertainties of the observations used for model comparison leading to unrealistic or inappropriate model parameterizations e g kirchner 2006 günther et al 2019 abbaszadeh et al 2019 among many more scientist have been grappling with these challenges advancing the field of environmental model calibration significantly the interested readers are referred to the following literature to learn more about some of these advances for example employing clustering approaches and evolution strategies during calibration e g duan et al 1993 hansen and ostermeier 2001 deb et al 2002 hansen et al 2003 making use of the covariance between parameters during parameter sampling e g hansen and ostermeier 2001 hansen et al 2003 dynamically reducing the search space e g tolson and shoemaker 2007 using soft rules rather than strict calibration objectives to obtain behavioral subsets from random samples of model simulations e g choi and beven 2007 shafii et al 2015 hartmann et al 2017 sarrazin et al 2022 or improving strategies when multiple objectives are to be calibrated e g deb et al 2002 asadzadeh and tolson 2013 asadzadeh et al 2014 several studies exist comparing subsets of these methods showing that some algorithms work better for some problems than others e g yen et al 2015 arsenault et al 2014 wallner et al 2012 zitzler et al 2000 despite these advances model calibration remains somewhat of an art where the latest advances are not always integrated into the workings of someone inexperienced with the calibration literature this is often due to the fact that model calibration is the means to an end to obtain a functional model setup for operational purposes to couple several models or to advance process understanding understandably more time is invested in assembling an advanced model with latest process descriptions and preparing data to setup test the model than for the model calibration itself this work is inspired by the book of hill and tiedeman 2007 which comprises 14 methods and guidelines for effective model calibration while outlining several good modeling practices mainly demonstrated using groundwater models the goal here is to develop a recipe that synthesizes the latest advances in model calibration to create something usable for the low experience modeler while being general enough to be applied to a wide range of environmental models but also provide a full picture of the main parts of each calibration process ten strategies are outlined where each strategies is supported by computational experiments or visualized in form of checklists the work is organized as follows section 2 and its ten subsections will describe in detail each of the ten steps while providing a body of relevant literature and either illustrate the impact of the discussed step with a calibration experiment sections 2 1 to 2 3 and sections 2 5 to 2 9 or define a checklist for the modeler to follow section 2 4 and section 2 10 a summary is provided thereafter section 3 the different experiments presented in this work are using a consistent set of models appendix a objective functions appendix b calibration algorithms appendix c and sampling schemes appendix d 2 strategies this section is laying out ten strategies on how to make decisions throughout the model calibration process and offers ideas to solve challenges arising the section is structured in ten subsections each addressing one challenge given the wide variety of large uncertainties in model inputs outputs and model structures themselves a perfect calibration of environmental models is not possible even when applying all strategies the goal however is to obtain reliable model results in line with our process understanding while making sure the calibration is setup as robust as possible the ten strategies are not meant as a complete list of every issue that can occur during model calibration the strategies and challenges selected are general enough to appear in most calibration exercises using environmental models when using sets of metrics algorithms and models in the ten subsections it is not meant to be an exhaustive list of all options available the sets have been chosen for demonstrative purposes to highlight a range of features and behaviors and to prepare the reader for the wide variety of issues that they might face during a model calibration model calibration is an iterative process which usually requires two or three iterations before the experimental setup is adjusted and can be used reliably for a given model and dataset each iteration consists of roughly three stages illustrated in fig 1 the strategies to resolve challenges in each of these levels however are usually not performed in a strict consecutive order and can easily be swapped with each other the numbering of them in this work is purely due to being able to cross reference them the central questions and topics of each stage will be explained in detail in the following sections this question style approach to model calibration was inspired by hill et al 2015 note that the term strategies rather than steps has been used here to account for the facts that a the items in each stage do not need to be addressed in the specific order presented b additional steps might need to be required for settings and models too specific to be covered here e g multi basin calibration or calibration of coupled models and c some items might not be of interest e g parameter ranges are prescribed and are not allowed to be changed the first stage is the preparation step which deals with general questions about model and data this is where one might apply a sensitivity analysis to identify insensitive unimportant parameters to exclude them from the calibration or at least be aware that these parameters will not be identifiable through this calibration section 2 1 good checks before getting started with the actual calibration are to check if parameters are independent or correlated section 2 2 if the observed data are uniformly spread across time and or space section 2 3 and to make decisions on which data to base the calibration on and which data to safe for a validation evaluation section 2 4 at this point one also needs to decide to use an automatic calibration instead of a manual tuning of individual parameters or a random sampling of parameters section 2 5 the second stage is the execution step where the calibration is setup and all related decisions about defining parameter ranges section 2 6 choosing the objective function that reflects the purpose of the model section 2 7 picking the right calibration algorithm section 2 8 and deciding whether to calibrate a single or multiple objectives section 2 9 the third stage is the checking step where the results of the calibration are thoroughly assessed and decisions about revising the experimental setup with respect to parameter ranges choice of objective function settings of calibration algorithm etc are made section 2 10 the results of the analysis of this stage determines whether to redo the calibration with the revised experimental setup or to use the current calibration results this iterative process is a very important part of model calibration a lot of valuable information about the model is gathered during this process for example model crashes might be caused by too wide parameter ranges one can either reduce the ranges until no crash appears anymore or one can use this opportunity to fix the actual reason in the model implementation causing this issue another example is learning about and fixing parameters that are not independent of each other resolving parameter constraints is not only making the process of finding parameter ranges easier but will also ultimately make the model calibration more successful since most calibration algorithms assume the parameters to be independent while sampling e g kirkpatrick et al 1983 duan et al 1993 kennedy and eberhart 1995 kennedy et al 2001 tolson and shoemaker 2007 and hence suffer from correlated parameters chu et al 2010 wu et al 2017 a third example is finding good and reliable settings for the calibration algorithm itself can usually only be found through testing several settings especially when calibrating a new model at a new location or with new data a robust calibration setup is more likely to easily be transferred to other locations or applications note that there are several tools that are helpful in assisting a successful model calibration model revisions fenicia et al 2016 kavetski and clark 2010 are most helpful when parameters are correlated or the model is crashing under some settings sensitivity analysis and parameter screening borgonovo and plischke 2016 iooss and lemaître 2015 razavi and gupta 2015 saltelli et al 2008 become useful when several parameters are not impacting the model outputs and could be removed from or fixed during the model calibration uncertainty analysis moges et al 2021 mcinerney et al 2017 moradkhani and sorooshian 2009 wu et al 2006 is helpful in incorporating uncertainties of data into the objective function and understanding how these uncertainties propagate to parameter uncertainties and uncertainties of model outputs none of these methods are necessary to perform a model calibration but can be a real asset especially when the goal is to gain a holistic understanding of a model the following ten subsections will describe the ten challenges laid out above in detail 2 1 use of sensitivity analysis to identify parameters to calibrate sensitivity analyses are methods that are used to determine the importance of inputs usually model parameters for the simulation of certain outputs e g streamflow herman et al 2013b a pfannerstill et al 2015 cuntz et al 2016 markstrom et al 2016 mai et al 2022b or performance metrics signatures e g kling gupta efficiency demirel et al 2018 bajracharya et al 2020 lilhare et al 2020 basijokaite and kelleher 2021 this is achieved by generating samples throughout the entire parameter space global methods and evaluating the models for those parameter sets and or combinations of those parameter sets e g use one part from one parameter set and another part from another set the analysis of these model runs e g variance or distribution of model outputs is then used to draw conclusions about the impact of model parameters on model outputs such as identifying parameters leading on average to bigger differences in model outputs while other parameters might not have any effect they can also be more creatively used to detect model deficiencies göhler et al 2013 sheikholeslami et al 2019 help selecting objective functions demirel et al 2018 bajracharya et al 2020 lilhare et al 2020 basijokaite and kelleher 2021 evaluating the importance of alternative model inputs baroni and tarantola 2014 schürz et al 2018 or identifying time period where certain parameters are more likely to be inferable herman et al 2013b pianosi and wagener 2016 like calibration methods there is a wide range of sensitivity analysis methods for reviews of the different methods and background of sensitivity analysis methods in general refer to for example saltelli et al 2008 iooss and lemaître 2015 razavi and gupta 2015 or pianosi et al 2016 the benefits of applying sensitivity analyses before model calibration in order to achieve faster convergence or more robust model setups have been demonstrated before demirel et al 2018 hill et al 2015 cuntz et al 2015 shin et al 2013 foglia et al 2009 hill 2007 white and chaubey 2005 among others here the automatic parameter screening method of efficient elementary effects eee introduced by cuntz et al 2015 is applied to determine the most impactful subset of parameters of a hydrologic model with n p 9 parameters appendix a 3 the parameter sensitivity is derived regarding the kling gupta efficiency appendix b 3 of each model run the screening converged automatically after 80 model evaluations using the standard settings proposed in cuntz et al 2015 the screening is identifying n ˆ p 6 parameters x 1 to x 3 and x 6 to x 8 see table a 2 for definition as informative while the remaining three x 4 x 5 x 9 see table a 2 for definition are regarded noninformative the model setup and the setup of eee used here are available on github mai and cuntz 2020 see examples raven gr4j cemaneige to demonstrate the impact of using only the screened set of parameters a model calibration is applied here using either all n p 9 or the screened subset of parameters n ˆ p 6 the model performance is evaluated using the kling gupta efficiency appendix b 3 of the hydrologic model using three calibration algorithms i e the dynamically dimensioned search dds algorithm appendix c 1 the shuffled complex evolution sce algorithm appendix c 2 and the particle swarm optimization pso method appendix c 3 note that later sections will focus on various performance metrics and calibration algorithms i e section 2 7 and section 2 8 respectively while the aiming to demonstrate which impact a parameter screening could have on later carried out calibration experiments each calibration either using all or only informative parameters is performed in ten independent trials and three different budgets 25 n p 50 n p and 100 n p note that computational budget refers to the maximum number of allowed model runs i e the number of model runs one can afford to obtain a calibration result the calibration results in terms of development of the objective function values over the course of the model calibration are presented in fig 2 the results show that the reduction of parameters to only the informative subset has indeed an impact on calibration results first the calibration trials using the informative subset fig 2d f are generally more consistent compared to using all parameters fig 2a c especially for larger budgets which indicates that the results converge faster second no significant loss can be detected in terms of the best objective function value found across all trials of the calibration compare colored labels in fig 2a vs fig 2d fig 2b vs fig 2e and fig 2c vs fig 2f this is an expected behavior and shows that the parameters screening indeed only identified noninformative parameters to be discarded during calibration third the impact on the calibration performance depends on the calibration algorithm dds and pso behave very similar exhibiting consistent behavior i e spread of trials and fast convergence i e plateauing of objective function values when using all parameters figs 2a and c respectively the dds and pso results using only informative parameters figs 2d and f respectively are more consistent but there is no big difference in general for sce the calibration results are most consistent i e spread of trials for the largest budget fig 2e for the lowest budget the results actually decrease in terms of best objective function found across all trials kge 0 79 calibrating all parameters vs kge 0 74 calibrating only informative parameters this is likely since sce performs best for large budgets and when it can determine when to abort the calibration by itself which was not the case in any of the experiments performed here note that a rather simple hydrologic model was used here for demonstration purposes leading to rather small overall differences between the model with and without screened parameters these differences are expected to be substantial when a more complex model is screened and calibrated the most important point of identifying the informative parameters however is that only calibrated parameter values for those parameters should be analyzed and reported the noninformative parameters are not identifiable given the available data and objective function used informative parameters however are also identifiable unless they are only informative due to correlation and interaction with other parameters the latter can usually be observed through several parameters following similar patterns throughout the model calibration reporting the calibrated values of the noninformative parameters is bearing the risk that they are used in other applications as if they were constrained through this calibration without parameter screening or another sensitivity analysis the information of parameter identifiability would not have been available it is advisable to use a frugal low budget sensitivity analysis such as the morris method morris 1991 or the here used eee method cuntz et al 2015 to identify the informative parameters since only a binary qualitative information if a parameter is informative or not is required to decide whether to calibrate or report a parameter only in very few cases quantitative parameter sensitivities are needed 2 2 considering parameters with constraints to allow for efficient sampling of parameter space model parameters are assumed to be independent by most model calibration algorithms this assumption however is often violated in environmental models hrachowitz et al 2014 for example a parameter is expected to be larger than another one e g vegetation top height needs to be larger than its bottom height or groups of parameters need to sum up to a certain value e g sand soil and clay fractions need to sum up to one these assumptions are easy to be met when doing a manual calibration since the modeler knows that for example the bottom height of the vegetation needs to be smaller than the vegetation top height and is using this understanding when proposing parameter sets during an automatic calibration these constraints need to be translated into the automatic sampling scheme if not some unlucky parameter constellations kavetski et al 2006 might lead to model failures sheikholeslami et al 2019 there are two general possibilities to handle these infeasible parameter sets the first is to simply ignore all constraints sample parameter sets and force the model with them the model might either fail crash or return not a number or no data values the issue is that the calibration algorithm might for example get stuck at those infeasible not a number no data values thinking that this is the optimum as it cannot find any better solution this happens especially if a large proportion of the parameter domain is infeasible filtering these non behavioral model outputs and not considering them in the calibration algorithm is computational least beneficial as the model has already been run with knowing that the parameter set is infeasible and the information of model failure is not even used to guide the search for optimal parameter sets the other possibility is to reject infeasible parameter set right away and resample parameter sets until a feasible one is found this might lead to problems as the calibration algorithm will never learn to avoid these regions the methods of sampling until a feasible parameter combination is found will be referred to as accept reject methods any of the described methods is inefficient and will inevitably lead to problems compared to resolving the constraint or sampling of the parameters avoiding producing infeasible parameter sets besides the above described methods there are some alternative approaches to resolve constraints by either adjusting the model itself through redefinition of parameters using the so called delta method used by e g mai et al 2020 chlumsky et al 2021 or redefining the sampling of the parameters using for example the pie share method mai et al 2022a the delta method is a possibility to revise parameter dependencies where one parameter x 2 needs to be larger than parameter x 1 the idea is to redefine x 2 to be x 1 increased by a positive value δ now δ can be sampled rather than x 2 the model required parameter x 2 can be derived either before the model is provided with a parameter set or the model can be revised to remove x 2 and replace it by x 1 δ throughout the latter is recommended as it will make sure parameters in the model are independent and knowledge about parameters like the information that one needs to be larger than the other are part of the model itself rather than expecting the modeler to know the pie share method introduced by mai et al 2022a is helpful when several parameters need to sum up to a constant c for example when percentages of sand silt and clay must sum up to 100 to determine the texture of a given soil or when sampling biomass compositions by percent lipids carbohydrates and proteins which again need to sum up to 100 the pie share method can also be applied when the sum of parameters is required to not exceed a certain threshold c instead of summing up to this exact value the pie share method essentially describes a strategy on how to transform n uniformly sampled random numbers into parameters x i fulfilling the aforementioned criteria the appendix d describes this method for any threshold c while it was originally introduced for thresholds c 1 mai et al 2022a an incomplete overview of parameters with constraints and attempts to sample them are given in table 1 for constraints that involve two or more parameters two methods each are listed fig 3 visualizes two of the examples given in the table the first example demonstrates the sampling of two parameters x 1 2 6 and x 2 2 8 under the given constraint that x 1 needs to be smaller than x 2 the independent sampling of both parameters using the accept reject method is leading to parameter combinations that are infeasible fig 3a gray colored samples a revised version of the sampling using x 2 x 1 δ with δ 0 6 is also leading to infeasible sets due to the fact that x 2 can now take values beyond the originally specified upper limit of 8 this method might however be favorable if this upper limit of the parameter is defined by the modeler without any physically based constraint and could easily be flexible the second example is demonstrating a situation where the sum of two parameters x 1 0 8 and x 2 0 8 is not allowed to exceed a threshold c 8 the independent sampling of the two parameters accept reject sampling leads to 50 infeasible combinations fig 3c gray colored samples while the use of the pie share sampling fig 3d resolves this issue and only leads to parameter combinations that fulfill the constraint for the pie share sampling n 2 uniformly distributed parameters r 1 u 0 1 and r 2 u 0 1 are sampled and then transformed into x 1 and x 2 using x 1 c 1 1 r 1 and x 2 c x 1 1 1 r 2 c x 1 r 2 as defined in eq d 1 there are certainly other strategies to resolve constraints of parameters for example by revising the model redefining parameters or potentially using copulas this section is to emphasize the importance of this step and to inspire addressing these issues independent parameters and robust model formulations make any model analysis like calibration sensitivity analysis and uncertainty analysis more efficient 2 3 transformation of data ranging orders of magnitude to find optimal fitting model setup environmental observations are usually used to find a best model setup observations of for example time dependent conductivity and transmissivity might range over multiple orders of magnitude with more observations being available for early timesteps and fewer for large timesteps interestingly such kind of data and corresponding model fits are usually presented and assessed in a log transformed or similar space even though this transform might not have been used during the calibration of the model e g madi et al 2018 de rooij et al 2021 fitting the data in their native domain or in the log transformed domain is leading to different optimal models which is illustrated for an experiment presented in fig 4 the first part of the experiment is using the native data points x y x f x and fitting the four parameters of the logistic function defined in eq a 3 fig 4a in the second part of the experiment the log log transformed data x ˆ g x ˆ log 10 x log 10 y are used to fit the corresponding four parameter logistic function in eq a 5 fig 4b the two logistic functions are explained in detail in appendix a 2 parameters and their ranges are defined therein both experiments used ten independent trials of the shuffled complex evolution algorithm appendix c 2 with an upper limit of 500 model evaluations each the root mean square error rmse appendix b 1 is used as a metric to quantify the quality of each model simulation the results show the superior quality of the model fit using the log log transformed data this is because the data and the model fit are assessed in this log log transformed domain both axes in plot are logarithmic the first experiment which is fitting the native data should be evaluated in the native domain that plot would however only show the largest data points and the model fit through these one or two points due to the distribution of the available observations the objective function used here rmse is focusing on fitting only these largest data points as this has the highest impact on any squared error statistic log transforming the data is removing this effect as a rule of thumb data should always be fitted in the transformation the fit is assessed in note that log transformations can also be applied to model parameters spanning multiple orders of magnitude for which is known that small values are more likely than larger parameter values or that perturbations in small parameter values have a larger impact on the model output than perturbations for larger orders of magnitude an example is hydraulic conductivity used as a model parameter to describes water movement through saturated media 2 4 choosing data to use for calibration and validation evaluation an essential step before starting with model calibration is to choose the data the calibration will be based upon two options are to use all available data or splitting the data into two sets i e data that are used for model calibration and an independent set used to estimate how the model performs when facing an independent new set of data this second set of data is commonly referred to as validation or evaluation data the latter term is suggested to be more appropriate for example by oreskes 1998 and wagener et al 2022 as it cannot be formally verified that a model represents the truth and hence validating might be misleading validation however has been used mostly in the environmental modeling community up to now and is hence kept for the sake of inclusiveness this split sample approach is central to the model validation evaluation framework proposed by klemeš 1986 it has since been challenged in several studies arsenault et al 2018 zheng et al 2018 2022 shen et al 2022 demonstrating that split sample approaches have their challenges for example showing that most robust models can be achieved when all data are used for calibration and none are held back for model validation evaluation arsenault et al 2018 shen et al 2022 first option mentioned above or showing that different data splitting approaches lead to large differences in model performance while identifying most robust approaches zheng et al 2018 2022 besides the classic way of validating evaluating models using the split sample method other approaches have been discussed for example the use of sensitivity analysis to complement a comparison of modeling results with observations wagener et al 2022 the model performance obtained during the validation evaluation period is commonly used to derive an understanding of how the model will behave in an independent time period if the state of the system modeled would never change i e all input data such as precipitation and temperature would be stationary the land use would not change and the rivers would not get perturbed by human intervention the validation evaluation performance would likely be the same or very close to the calibration performance since this is not the case in most environmental case studies the validation evaluation is performed to get an idea of how much model prediction skill achieved during calibration would be lost when the model is facing new data two general approaches have been recommended in the literature the first approach is recommended when the calibrated model needs to be put directly into operation i e the model is running in a real time application using data that have not been available during model calibration fig 5a in these cases the data used for calibration should be chosen to be as close as possible to future states of the data reducing impact of non stationary data future states of the model reducing performance loss because model does not transfer well with non stationary inputs and future states of the region the model is applied to avoiding e g large landuse changes hence the calibration data are chosen to be the most recent data more dated data are used for validation evaluation to generate estimates of how much the model performance might drop when the model is facing data that might include large changes in statistical properties of data e g climate change or landuse change the fact that the validation evaluation data period is earlier than the calibration data period might lead to unrealistic estimates this approach is however leading to most robust model results during the deployment period as shown by shen et al 2022 the deployment period is referred to as testing period therein to avoid these unrealistic estimates of model performance it is preferable to use dated data for calibration and the recent data for validation evaluation which is the classic setup in fields like hydrologic modeling this second approach fig 5b where the calibration data period precedes the validation evaluation period leads to more realistic estimates of loss in model skill when the model is facing future data like for example when the model is deployed into its operational application before the model is however put into operation the model is recommended to be calibrated a second time using all available data to enable the most robust results during model deployment this approach is leading to the most robust results when the model is deployed as shown by for example arsenault et al 2018 and shen et al 2022 in large scale studies the expected loss in performance in the deployment period is likely close to the one determined during the foregoing split sample experiment 2 5 choosing appropriate parameter sampling strategy to find optimal model setups the most essential step of each calibration algorithm is the proposal of the next parameter set to evaluate i e the parameter set to run the model with and determine the quality of the fit of the model outputs with observed data one method of proposing the next parameter set is pure random sampling using a random number generator to sample a value for each parameter given its range or distribution this strategy is often referred to as monte carlo method it does not consider information gathered by evaluating previous parameter sets and hence has the advantage to be easily parallelizable as each evaluation is independent of all others another type of sampling are so called stratified sampling methods like latin hypercube sampling lhs mckay et al 1979 or sobol sequences sobol 1976 they sample the parameter domain making sure that the domain is uniformly sampled for any number samples drawn which is not necessarily the case with a purely random sampling these two sampling methods are applicable in a wide range of methods such as sensitivity analysis uncertainty analysis and model calibration a third set of methods are calibration algorithms which major feature is their built in parameter sampling approach specifically tailored to calibrate models this means their proposal of a new candidate parameter set is derived based on the information gathered with preceding parameter sets there is a wide range of strategies how to make use of that information leading to a plenitude of calibration algorithms jahandideh tehrani et al 2021 yen et al 2015 arsenault et al 2014 wallner et al 2012 note that the automatic sampling of parameters through the calibration algorithm requires that parameter constraints are fulfilled for any sample drawn to avoid model crashes during calibration approaches to assure this especially for constrained parameters are discussed above in section 2 2 to demonstrate differences and efficiency of the three methods parameter samples are drawn and evaluated using a benchmark function in replacement of an environmental model benchmark function have the advantage that the optimum is known and they are made to stress test calibration methods in this case the ackley function appendix a 1 with n p 2 and n p 10 parameters is used the optimal parameter set of the ackley function is when each parameter is zero leading to the smallest objective function value of zero latin hypercube sampling is used as stratified sampling method while the shuffled complex evolution sce duan et al 1993 algorithm see appendix c 2 is used as automatic calibration algorithm in total ten independent trials are performed for each method to visualize the random component of each method each method was allowed to draw 100 n p samples in each trial fig 6 shows the distribution of the first two dimensions x 1 and x 2 of the samples panels a c and g i as well as the best function value found after each sample drawn the distribution of the sampled values shows that the random and stratified sampling pick parameter values uniformly distributed across the entire domain figs 6a 6b 6g and 6h the development of the function values demonstrates that both methods work similarly good for low dimensional problems figs 6d and 6e since they can efficiently sample given the allowed budget 200 4 2 12 5 samples per volume unit while it shows that these methods fail to find the optimum for the higher dimensional case figs 6j and 6k in these cases the sampling density is significantly decreased 1000 4 10 00095 samples per volume unit the calibration algorithm however is in most cases successful to identify the optimum figs 6f and 6l or at least get very close to the target function value of zero the sampled parameter values show a clear favor of values around the origin where the optimum is located the algorithm is focusing on beneficial areas and is hence making better use of the budget compared to random and stratified sampling stratified and random sampling will find the minimum eventually it will only take on average much more samples note that random sampling is likely not an approach one would follow when calibrating a model manually i e parameter sets are picked manually the model is run the next set is picked etc occasionally especially at the beginning one would pick a random sample to check if a better solution can be found somewhere else but most of the time one would try to not stray too much from an already good solution but try to successively improve it with smaller changes this natural behavior is what most calibration algorithms try to mimic it has inspired a wide range of optimization algorithms specifying its motivating species in the name jahandideh tehrani et al 2021 such as the ant colony optimization dorigo et al 1991 particle swarm algorithm kennedy and eberhart 1995 shuffled frog leaping algorithm eusuff and lansey 2003 honey bee mating optimization haddad et al 2006 and bat algorithm yang 2010 2 6 adjustment of parameter ranges to make sure optimal values are covered while calibration algorithm is still efficient for every parameter to calibrate a distribution needs to be defined most commonly a uniform distribution is assumed by specifying lower and upper bound for each parameter but technically any kind of distribution like gaussian or log normal distribution can be specified it is more a question of whether the calibration algorithm or calibration toolkit used is supporting those distributions when sampling parameters the ostrich calibration toolbox for example does not allow for distributions other than uniform but it allows to transform parameters or even use derived versions of parameters the parameters of those distributions such as the lower and upper bound of the uniform distribution are most important for a calibration as they defined the search domain for the calibration algorithm in physically based models these ranges might be determined by physics but in a lot of conceptual models the parameters are upscaled and or conceptual entities and a range is rarely known a priori defining wide ranges will make the calibration less efficient as the calibration algorithm will have a larger domain to explore and might get trapped in local optima more easily while defining ranges too narrow might have the effect that the location of the actual global optimum is not contained which will in most cases show in parameters running against their lower or upper bound indicating in which direction it would prefer to go if it would be allowed to the results of an experiment demonstrating these effects are shown in fig 7 for all parts of this experiments the shuffled complex evolution algorithm sce details in appendix c 2 was used allowing for a budget of 500 model evaluations for each of the ten independent trials performed the goal of each calibration trial is to calibrate the four parameters of the four parameter logistic function described in eq a 5 in order to minimize the root mean square error rmse appendix b 1 between the model and log log transformed data points the ranges of the four parameters might not be known beforehand and set to e g l u 4 0 40 0 k u 1 0 1 0 5 10 0 x 0 u 4 0 4 0 s u 1 0 1 0 5 20 0 the independent trials of the calibration result in a large spread of model fits fig 7a the evolution of the parameters over the course of the calibration fig 7d e j and k shows that several trials converge to similar values at the end of the calibration especially for x 0 and most values are within the range and not hitting a bound which is good furthermore several parameters do not take any values in large portions of their specified range i e parameters l x 0 and s hence the ranges might be restricted to l u 10 0 20 0 k u 2 5 5 0 x 0 u 1 0 3 0 s u 1 0 1 0 5 5 0 in order to achieve a smaller spread in the fitted models these narrow ranges however lead to a situation where the parameters are now too restricted the evolution of the parameters throughout the calibration fig 7f g l and m shows that parameter l clearly converges against its lower bound suggesting its optimal value might be below the lower bound the other parameters do not seem to converge at all this is mostly because the parameter l is the most sensitive parameter the fitted functions fig 7b are clearly of lower quality than the previous calibration using the wider ranges using an intermediate set of ranges u 2 0 20 0 k u 1 0 1 0 5 5 0 x 0 u 2 0 2 0 s u 1 0 1 0 5 10 0 is leading to consistent and high quality fits fig 7c and the parameters converge to values placed within the range fig 7h i n and o this is to demonstrate several points first determining ranges is in most cases an iterative process especially when the model is conceptual or unknown to the person calibrating it second parameters are interacting with each other meaning that when one parameter is too restricted other parameters are likely not going to converge either third diagnosing the quality of a calibration and determining reasons for its failure can be achieved by performing multiple trials to use the spread as a measure of quality and analyzing the course of the parameters throughout the calibration diagnosing too wide or narrow ranges 2 7 choose the appropriate objective function for the purpose of the model a main setting that is guiding an automatic model calibration is the objective function as this one makes an objective decision whether a parameter set leads to a preferable model simulation compared to another parameter setting the objective function is expected to reflect the subjective judgement of the modeler comparing two model simulations historically there are objective functions that are more popular in some fields like the nash sutcliffe efficiency nse nash and sutcliffe 1970 see appendix b 2 and the kling gupta efficiency kge gupta et al 2009 see appendix b 3 to evaluate the quality of a hydrologic model simulation these metrics are commonly chosen by default even though there are discussions whether they are the most appropriate metrics lamontagne et al 2020 mizukami et al 2019 schaefli and gupta 2007 moriasi et al 2007 or whether model signatures are more appropriate to be used for calibration gupta et al 2008 mcmillan 2020 user surveys employed to understand the mechanisms behind hydrologic decision making emphasize that metrics often used do not necessarily reflect what a human would have decided gauch et al 2022 crochemore et al 2015 one needs to be aware that metrics are usually focusing on certain features e g high flows low flows and or recession and the modeler might need to revise or create a metric according to their needs this can be achieved by for example weighting more important data points higher or weighting data points with respect to their uncertainty or by transforming or filtering data points the impact of the choice of objective function on the resulting optimal model setting selected by the calibration algorithm is demonstrated in the following the hydrologic model gr4j with cemaneige for snow and a flexible rain snow partitioning is used to fit observed discharge data for the salmon river gauge station near prince george in british columbia canada details about the model and catchment can be found in appendix a 3 ten independent trials of the shuffled complex evolution algorithm sce appendix c 2 are carried out allowing for 100 n p model evaluations in each trial to calibrate the n p 9 parameters of the model the following six objective functions have been used as objectives for model calibration a minimizing the root mean square error rmse q appendix b 1 b maximizing the kling gupta efficiency kge q appendix b 3 c maximizing the nash sutcliffe efficiency nse q appendix b 2 d maximizing the nash sutcliffe efficiency of the log transformed discharge nse logq appendix b 2 e maximizing the squared pearson correlation coefficient r q 2 appendix b 4 and f minimizing the percent bias pbias q appendix b 5 the objective functions have been selected as they are commonly used metrics to calibrate and evaluate hydrologic models the resulting hydrographs of one example year of the 20 year calibration period and the optimal objective function values calculated for the 20 year period are shown in fig 8 in most cases the calibration of an objective function led to the best result for the respective metric across all experiments black highlighted label is best compared to gray colored values in the other five panels for example the least rmse of 24 24 is observed when the rmse is calibrated however the rmse when nse is calibrated is almost equal rmse 24 34 this is due to the fact that the nse and rmse are indeed directly correlated since the nse is the ratio of the mean square error mse and the variance of the observations eq b 2 the latter is a constant while the mse is directly correlated with the rmse this means if the rmse is minimized the nse is implicitly optimized clear differences in the optimal hydrographs can be detected when using different metrics for calibration the calibration using kge fig 8b is yielding better matching simulations during the high flows in april and may compared to the results obtained using nse and rmse using the nse for the log transformed discharge data fig 8d is putting more emphasis on the low flow periods in winter december to march and summer july to october but still achieving a decent magnitude for the freshet high flows this demonstrates again the impact a transformation of data here q vs logq might have on the calibration results as it was discussed in section 2 3 using the squared correlation coefficient fig 8e as a metric is leading to a wide spread of the ten calibration trials with the best trial being the hydrograph consistently underestimating discharge especially during the high flow period in april to june and october to november it shows that the use of the correlation coefficient is focusing on matching general patterns in the data while not actually focusing on the nominal values themselves this is a beneficial feature when knowing that the model is simulating proxies of the available observations but not the nominal values themselves for example most hydrologic models do not explicitly simulate soil moisture but only saturation or soil water storage in a conceptual soil layer where depth is not explicitly modeled hence the model cannot be compared to observed soil moisture directly as their magnitudes are different but it can be expected that their dynamics are similar and hence the squared correlation can be envisaged as objective function the squared correlations calibrating rmse and nse are quite similar both r 2 0 73 to the optimal value achieved by calibrating r 2 directly r 2 0 74 while the former two i e rmse and nse fits would certainly be picked as a better fit by any hydrologist looking for an overall good fit while the latter i e r 2 fit might be chosen as the best fit when low flow values are most important to be modeled calibrating the percent bias fig 8f is exclusively focusing on the overall water budget applications for metrics like this include for example setups for long term hydropower reservoir planning the calibrated hydrograph however would likely not be picked as a good fit of the observed data in most applications the experiment presented here shows that a poor model fit might not be caused by an inappropriate model or calibration but solely be due to the choice of the objective function a critical evaluation of whether a commonly used objective function is emulating the actual objective of the modeling purpose is key in some cases it might be necessary to translate the needs into a user specific objective function or use model signatures to achieve a robust model assessment assuring that the model leads to good results for the right reasons 2 8 choose the right calibration algorithm a wide variety of different algorithms is available all differing in their strategy to suggest the next parameter set to be tested based on previously tested parameter sets and their performance some algorithms focus on achieving best results for a low number of model evaluations while others focus on convergence of results among other features the most used algorithms to calibrate environmental models are heuristic algorithms that do not rely on derivatives of the model being available a lot of them are inspired by nature like movement of amoebas e g nelder mead algorithm nelder and mead 1965 collective swarm behavior e g particle swarm optimization kennedy and eberhart 1995 or optimal formation of atoms during the cooling of metals e g simulated annealing kirkpatrick et al 1983 gradient decent methods on the other hand are much less common for environmental models given that they require derivatives to be defined or determined to guide the calibration derivatives of complex numerical models are however usually not known or not straight forward to derive to demonstrate differences and similarities of calibration algorithms three commonly applied algorithms to calibrate hydrologic and other environmental models have been used to find optimal settings for four exemplary models the four problems are to minimize the root mean square error between the four parameter logistic function eq a 5 and log log transformed data to maximize the kling gupta efficiency of a nine parameter hydrologic model when compared to discharge data appendix a 3 and to minimize ackley functions with 10 and 20 degrees of freedom the three heuristic algorithms selected are the dynamically dimensioned search dds appendix c 1 the shuffled complex evolution sce appendix c 2 and the particle swarm optimization pso appendix c 3 the algorithms are tested using three budgets i e 25 n p 50 n p and 100 n p with n p being the number of parameters in the respective model for each budget algorithm and model ten independent calibration trials are performed to assess the random nature of each of these algorithms the results are presented in fig 9 dds is known to be very budget efficient i e achieving good results for very low budgets the algorithm easily finds the optimum with a low budget in low dimensional problems red lines in figs 9a and d the comparison of the ten independent trials shows a larger spread for more high dimensional problems figs 9g and j independent of the budget used it however reliably identifies good solutions when referring to the best of the ten trials colored labels added to panels increasing the budget seems beneficial especially for higher dimensional problems compare for example red and blue lines in figs 9g and j sce is known to be a good choice especially when the problems allow for a large or even unlimited budget in those cases the algorithm can decide itself when to stop which was not achieved in any of the examples and trials presented here the trials are in general more consistent than for dds the best trial however is not as good as the best trial in dds especially for the lower budget compare red and blue colored values added as labels to figs 9a b d e g h and j k sce even seems to consistently converge to the wrong value for high dimensional problems fig 9k the non convergence of the algorithm is however a good indicator that better results could be achieved when given a larger budget pso is comparable to dds for the largest budget 100 n p in terms of the best overall found objective function value across all trials see gray colored labels added to figs 9c f i and l the trials are more consistent compared to dds especially for higher dimensional problems based on a visual inspection of the objective function values e g compare red and blue lines in figs 9i and 9l with corresponding lines in figs 9g and 9j both sce and pso are population based algorithms meaning that an entire ensemble swarm of parameter sets are evaluated and used to propose new candidate parameter sets in promising directions in case of sce this means that several complexes here numcomplexes 2 see table c 3 containing multiple points parameter sets each here numpointspercomplexes 2 n p 1 see table c 3 need to be evaluated before the calibration algorithm starts suggesting the first true candidate parameter set based on the evaluations of these 2 2 n p 1 points in case of n p 10 figs 9g i and n p 20 figs 9j l parameters it requires 42 and 82 model evaluations respectively to just get started while dds already used all these evaluations to explore the parameter space by itself this leads to the situation that population based algorithms such as sce and pso do not show as steep declines in objective function values at the start of the calibration compared to algorithms that are not population based such as dds dds is known to be very well suited for high dimensional problems especially given a small budget as it does not dissipate limited resources in summary the results show that a running and analyzing multiple trials is helpful to evaluate the quality of a calibration b an increase in budget might lead to significant improvements depending on problem and algorithm other algorithmic parameters not analyzed here might lead to improvements as well and c the quality of calibration algorithms differs in terms of e g spread of trials convergence and overall best trial an overall rule of thumb is to find an algorithm that one finds intuitive such that the adjustments of algorithmic parameters feel natural it seems essential to test the algorithm of choice in a few benchmark examples to find settings of the algorithmic parameters and understand the influence of the various algorithmic parameters a list of algorithmic parameters used here is given in table c 3 as a result of testing these algorithms on a wide range of problems in hydrology it is essential to find good settings for different situations such as models with more less parameters fast slow models consistency of multiple trials the sensitivity of the algorithm to initial guesses general convergence behavior of the algorithm and good settings for budget not in all cases the same algorithm might be best it seems beneficial to run two algorithms in parallel if possible at least in a test setting to pick the one that seems most appropriate 2 9 decide if multiple objective need to be calibrated to find model setups satisfying expectations often a good model setup cannot be determined by only one feature e g dataset information objective but needs two or more aspects of the model to be optimal for example the use of soil moisture performance as a second objective besides streamflow performance might be helpful to obtain realistic patterns in other state variables of the model a preliminary sensitivity analysis can be helpful to identify sets of objectives that help constraining parameters responsible for various aspects of the model demirel et al 2018 in cases multiple objectives need to be evaluated simultaneously one needs to employ a multi objective algorithm there is a plethora of algorithms available for example the multi objective complex evolution mocom ua method yapo et al 1998 the nondominated sorting genetic algorithm ii nsga ii deb et al 2002 the multiobjective shuffled complex evolution metropolis moscem algorithm vrugt et al 2003 the multi objective particle swarm optimization mopso approach reddy and kumar 2007 the multiobjective evolutionary annealing simplex meas method efstratiadis and koutsoyiannis 2009 and the pareto archived dynamically dimensioned search padds algorithm asadzadeh and tolson 2013 among others these algorithms have been applied to a wide variety of environmental models seeking optimal parameter setups for two or more objectives e g jahanpour et al 2018 newland et al 2018 ercan and goodall 2016 gong et al 2015 oraei zare et al 2012 shafii and de smedt 2009 for reviews and comparisons of the algorithms the reader is referred to for example zitzler et al 2000 and efstratiadis and koutsoyiannis 2010 the focus of this work is to summarize how to diagnose that a multi objective algorithm is setup sufficiently and results are reliable i e converged two experiments are setup for demonstration purposes in both cases the padds algorithm appendix c 4 is used to find optimal settings for a hydrologic model with n p 9 parameters appendix a 3 allowing for a budget of 300 n p model evaluations in each of the ten independent trials carried out the first experiment is to find optimal parameter settings for low and high flows using the kge of the log transformed discharge for low and high flow time steps eqs b 7 and b 8 respectively the second experiment is using the bias and correlation component of kge eqs b 5 and b 6 respectively as the two objectives to be maximized note that in both experiments the overall set of non dominated solutions forming the pareto front is derived by identifying the non dominated solutions when the pareto fronts of the ten trials are pooled together hence the final pareto front might contain solutions from different trials rather being the best front of the ten trials both experiments further include as a reference the single objective calibration of each of the two objectives independently i e calibrate only objective 1 and calibrate only objective 2 as well as the single objective calibration of the mean of both objectives i e calibrate 0 5 objective 1 objective 2 these single objective calibration experiments are performed using the dds algorithm appendix c 1 with ten trials and a budget of 100 n p each the second not calibrated objective is simply derived in a post processing using the best model setup with regards to the calibrated objective the results are shown in fig 10 the result of first experiment calibrating low and high flows independently fig 10a shows a pareto front blue line bending over a noticeable range kge of 0 4 to 0 7 for log transformed low flows and kge of 0 6 to 0 9 for log transformed high flows the individual fronts of the ten trials gray lines show some spread but are generally in agreement especially for large objective function values for the high flow objective more importantly the single objective references blue markers for best trial are located on the pareto front and are positioned exactly where one would expect them the calibration using only the first objective blue triangle pointing left is located on the right most edge of the pareto front while the calibration using only the second objective blue triangle pointing downwards is located on the upper edge of the pareto front the single objective calibration using the mean of both objectives blue square marker is positioned in between this highlights two points a any single objective calibration result that is derived as a compromise solution between the involved individual objectives is located on the pareto front its position is determined by the weight of the individual objectives when merging them into a single objective i e compromise objective b single objective calibrations are usually easier to setup require fewer model evaluations and are easier to check for convergence compared to multi objective experiments they can hence be used as an indicator whether a multi objective calibration is converged by checking if the single objective results are positioned on the pareto front if there is a gap between the pareto front line and single objective results markers the multi objective calibration is likely not converged yet the second experiment using the two components of the kge i e kge β and kge r equivalent to calibrating percent bias pbias and correlation r is leading to a significantly different result fig 10b the pareto front does not form a curved shape as the previous experiment i e the range where the bend appears is very narrow the single objective results are located mostly on a straight line parallel to the x axis and y axis the differences in objective function values are of numerical nature i e differences appear only in the second or third digit of kge this is highlighted by a zoom of the original front focusing mostly on the bend of the pareto front fig 10c this highlights two points a the location of the single objective results blue markers on the pareto front blue line demonstrates that the single objective and the multi objective calibrations both converged b more importantly the shape of pareto fronts where the bend of the front is only visible in narrow ranges of the objective functions i e like in figs 10b and 10c are called degenerated as they imply that the two objectives are not contrary the model finds settings that optimize both objectives at the same time i e using the model setup located at the bend of the front is optimal none of the points on the straight limbs would be considered as significant improvements can be achieved for one objective with almost no decrease in performance of the other objective the optimal model setup at the bend can be identified either using the less computationally expensive single objective calibration of the mean of both objectives or using the multi objective calibration and identifying the point located on the pointy bend in cases the shape of the pareto front looks degenerated i e large non bending tails of the front it is recommended to revise the sets of objectives used for calibration by reducing them to a set of non redundant objectives each objective is supposed to add an additional constraint independent of the others constraints while degenerated pareto fronts are forming if objectives are redundant correlated e g nse and rmse see section 2 7 the presented example of calibrating low and high flows independently fig 10a is the superior multi objective setup as the two objectives are truly independent and a bending non degenerated pareto front is forming contrary to the simultaneous calibration of percent bias and correlation figs 10b and 10c 2 10 diagnosing calibration performance this last step aims to summarize the previous steps providing a checklist that should be examined after each iteration of a calibration experiment i e one loop of the calibration life cycle shown in fig 1 the checklist contains potential issues that might be identified when analyzing a the convergence of the parameter values over the course of the calibration b the development of the objective function values over the course of the calibration and c the actual fit of the calibrated modeled outputs to the data action items are specified that will help resolving detected issue most issues and ideas to resolve them have been described in the previous sections the individual check points are explained briefly in the following a visual summary of the checklist including brief descriptions of problems and solutions is shown in fig 11 at first the development of parameter values over the course of model evaluations performed during the model calibration can be analyzed as done for example in section 2 6 it is helpful to analyze multiple independent calibration trials as they provide a clearer picture a large spread of the trials fig 11a might indicate either a too wide range of the parameter or a too low budget used for calibration in case other parameters converge to consistent values a large spread between the trials for a parameter indicates however that the parameter might be not inferable given the used data or objective function or the parameter is dependent on the values of several other parameters i e parameter interaction this might be checked with a sensitivity analysis convergence patterns that look very similar for pairs or more parameters indicate that parameters might be correlated not shown in this work parameters that consistently converge to values close to the parameter s boundary fig 11b indicate the necessity to increase the range of the parameter the consistent convergence of all trials towards a value that is not the upper or lower limit fig 11c is the desired behavior as it suggests the identifiability of the parameter and a robust calibration setup another item to check is the development of objective function values over the course of the model evaluations during calibration this is best done also using multiple independent trials for single objective calibration tasks the steepness of curve towards the end of the calibration fig 11d might reveal the necessity to increase the calibration budget if the curve has a clear downward trend instead of plateauing at the end of the calibration the spread of the trials fig 11e indicates the convergence and reliability of the setup a wide spread might be resolved using a larger budget or other algorithmic parameters or revising the parameter ranges the target is to achieve consistent trials that do not spread too much especially at the end of the calibration as well as converging plateauing for iterations at the end of the calibration fig 11f to avoid that this behavior only looks consistent because the algorithm consistently fails a second calibration algorithm could be employed in parallel to check whether both converge to the same value as shown in section 2 8 for multi objective calibration experiments the convergence and consistency of the objective function values is harder to check based on the spread of multiple trials or the plateauing of the objective function one possibility is to check whether the multi objective pareto front is consistent with corresponding single objective calibrations instead as shown in section 2 9 a first check is if the pareto front is degenerated fig 11g which usually indicates that some of the chosen objectives are redundant and one or more can be removed without any loss of information less objectives make any algorithm faster in any case and more reliable if redundant objectives are removed single objective results that are not placed on the pareto front fig 11h indicate that usually the multi objective calibration is not converged yet and the budget should be increased the best case scenario is that the pareto front shows a bend over a significant range of objective function values and corresponding single objective calibration results are placed on the front itself fig 11i showing that the objectives are indeed contrary helping different features of the model to be calibrated and the calibration converged reliably lastly the actual fit of the data with the calibrated model simulation needs to be checked as shown for example in sections 2 6 and 2 7 the task for the modeler is to check objectively if the features of the data are matched by the model as expected figs 11j l if this is not the case the choice of the objective function should be revisited another check is the quality of the fit as well as the spread of independent calibration trials failures to fit the data entirely fig 11m is the worst case scenario and can have multiple reasons it most likely will need to be further diagnosed further using the behavior of parameters and objective function values over the calibration iterations figs 11a c and figs 11d i respectively possible reasons for a failure are that a parameter ranges that are too narrow such that the optimum is not contained b parameter ranges are too wide such that the algorithm does not find the global optimum c the calibration algorithm itself is not properly setup d the input data or output observations have large uncertainties or e the model is just not appropriate to describe the data and needs to be revised independent trials that generally fit well but show a relatively wide spread fig 11n usually indicate that parameter ranges might be too wide the calibration algorithm might therefore have trouble to converge to the same parameter set in all trails increasing the budget or narrowing the ranges depending on how the convergence of objective function and parameters look like might solve this issue a good fit fig 11o is consistent across the trials small spread and fits the data as expected all this of course in combination with parameters converging to consistent values within range and objective function converging consistently this successful verification of i the model fit ii consistently converging parameters and iii consistently converging objective function values could be called the hat trick of model calibration and successfully concludes the endeavor of calibrating an environmental model once the model is successfully calibrated the model setup needs to be tested through validation and evaluation experiments using data and outputs that have not been used during calibration making sure the model gives right answers for the right reasons 3 summary this work describes and illustrates ten strategies to guide model calibration of environmental models the strategies are general enough to cover a wide range of problems and model types the list could certainly be extended but would sacrifice generality the strategies are part of the calibration life cycle and need to be understood as an iterative process to arrive at optimal settings to guarantee a successful model calibration the order they appear in the following list is as they appeared in this work during a true model calibration they will likely not appear in this consecutive order but being swapped augmented by other steps or applied several times before moving on to solving the next challenge the strategies presented in detail in the previous sections can be summarized as follows 1 use of sensitivity analysis to identify parameters to calibrate identifying parameters that are important for a model output i e parameter screening and only calibrating these might make the calibration more efficient especially when the algorithm is not internally using sensitivity information the more important benefit of knowing about the sensitivity importance of parameters is that it identifies parameters that can or cannot be inferred through a calibration using the given model output and data at hand if an insensitive parameter is calibrated the inferred values are purely random and hence should not be transferred and only used with care 2 considering parameters with constraints to allow for efficient sampling of parameter space parameter constraints might lead to model crashes and should be resolved by adjusting sampling strategies wise choices of parameter ranges redefinition of parameters or a revision of the model 3 transformation of data ranging orders of magnitude to find optimal fitting model setup when data range across multiple orders of magnitude e g conductivity transmissivity it might be a wise choice to transform them e g log transform to convert them in a range where data are more equally distributed a good guideline is to pick the transform you would use to check the fit of the model and the data 4 choosing data to use for calibration and validation evaluation choose the data for calibration and validation evaluation in order to make sure the most robust model can be deployed and used for operational purposes using future data 5 choosing appropriate parameter sampling strategy to find optimal model setups random sampling is the most inefficient form of model calibration calibration algorithms are setup to learn from past model evaluations and make an informed choice for the next sample parameter set 6 adjustment of parameter ranges to make sure optimal values are covered while calibration algorithm is still efficient the goal is to find a range for each parameter that is as narrow as possible such that it is easiest for the calibration algorithm to find the optimum but wide enough such that the optimum is contained and calibrated parameters are not located at the bound finding a balance for each parameter range is an iterative process 7 choose the appropriate objective function for the purpose of the model the objective function is summarizing the features you expect your model to fit best regarding the given data if a calibrated model output is not satisfactorily fitting your data it may not be due to the inappropriateness of the model but an insufficient choice of the objective function objective functions are not set in stone you can create your own specifying the feature you want to be fitted best 8 choose the right calibration algorithm the various algorithms show their strengths for different problems ultimately it is more important to find an algorithm you understand and know how to tweak running two algorithms in parallel is a wise choice to gain confidence in results therefore picking a calibration toolbox allowing to easily switch between different algorithms is key 9 decide if multiple objective need to be calibrated to find model setups satisfying expectations technically multiple objectives can be calibrated at the same time however it should be checked that additional objectives add non redundant information i e information that is not already represented by another objective in any case since multi objective calibration problems are harder they should be checked for consistency with the easier problems of calibrating each objective individually 10 diagnosing calibration performance a major part of model calibration is diagnosing the quality of calibration results with respect to parameter values the objective function and the fit of the model to the data the analysis of these three components can be used to guide the revision of the experimental setup of the calibration e g adjust ranges increase calibration budget modify objective function this work is attempting to formalize the process of successfully calibrating an environmental model by providing clear guidance and checklists for people who like checklists like myself the study is not meant as a benchmark of objective functions or calibration algorithms even though model calibration remains to be an art this work attempts to make the process more accessible especially to students and researchers whose primary focus is not model calibration itself but are using model calibration as a tool credit authorship contribution statement juliane mai conceptualization methodology software validation formal analysis writing visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the author would like to thank bryan tolson university of waterloo and luis samaniego helmholtz centre for environmental research for the inspiring discussions about the art of model calibration throughout the years many thanks to the special issue women in hydrology for the opportunity and its focus on amazing female mentors the author wishes to recognize the two anonymous reviewers for their most helpful and encouraging evaluations of the manuscript this research was undertaken thanks in part to the global water futures program funded by the canada first research excellence fund cfref the data and codes are made available on zenodo https doi org 10 5281 zenodo 7563321 appendix a models used three models are used for demonstration purposes in this work two of them are mathematical functions i e the ackley function appendix a 1 and the logistic function appendix a 2 the third model is a hydrologic model called gr4j for one example watershed in british columbia appendix a 3 a 1 ackley test function the ackley function introduced by ackley 1987 in his phd thesis is a common benchmark function in model calibration to test the performance of calibration algorithms tolson and shoemaker 2007 behrangi et al 2008 regis and shoemaker 2009 müller and shoemaker 2014 the global optimum is known to be located at the origin x 0 with a minimal function value of 0 0 the ackley function exists for a variable number of dimensions n which makes it very convenient to test algorithms for low dimensional problems n typically 2 to 5 and high dimensional problems n larger than 10 the function has many local minima which make it difficult for some algorithms to find the global optimum at the origin when calibrating this function no objective function is required as the function value f a c k l e y x 1 x 2 x n itself is used to find the minimum the function is defined as follows a 1 f a c k l e y x 1 x 2 x n a exp b s 1 n exp s 2 n a exp 1 0 a 20 0 b 0 2 c 2 0 π s 1 i 1 n x i 2 s 2 i 1 n cos c x i the range of the n parameters x i i 1 n are set to be the following if not otherwise stated a 2 x i u 2 0 2 0 a 2 logistic function the logistic function is used in this study to demonstrate fitting of data points in this study these data points span multiple orders of magnitude but their general trend is that of a logistic function after its point of inflection the logistic function here used with four parameters is an s shaped function with a wide range of application fields such as biology economics physics social sciences and even sensitivity analysis berkson 1944 long et al 1974 garrick 1995 kooi et al 1998 zimm 2005 cuntz et al 2015 a 3 f x 1 0 l 1 exp k x x 0 s the parameters are the curve s maximum value when not shifted i e s 0 the logistic growth rate or steepness of the curve k the value of the sigmoid s midpoint x 0 and the shift curve along direction of y axis s the ranges of the parameters in the examples presented in this work are set as follows if not otherwise stated a 4 l u 2 0 20 0 k u 1 0 1 0 5 5 0 x 0 u 2 0 2 0 s u 1 0 1 0 5 10 0 this function is also used with log transformed independent variables x ˆ log 10 x the function should then be adjusted to account for this transformation but keeping the ranges of the parameters consistent with the above logistic function a 5 g x ˆ log 10 f x l 1 exp k x ˆ x 0 s with x ˆ log 10 x in this work the root mean square error appendix b 1 is used to find the best fit of the data points using a logistic function a 3 hydrologic model gr4j with cemaneige the gr4j model perrin et al 2003 is a lumped water balance model that relates runoff to rainfall and evapotranspiration using daily data while the model contains two stores and originally has four parameters x 1 to x 4 the gr4j parameter x 1 denotes the production store capacity parameter x 2 is the inter catchment exchange coefficient x 3 is the routing store capacity and x 4 is the unit hydrograph time constant gr4j is used in concert with cemaneige valéry et al 2014 for the handling of snow gr4j and cemaneige are fully emulated within the raven modeling framework craig et al 2020 and used as such in this study the parameters their ranges and where these parameters are located in a raven setup is listed in table a 2 please note that parameter x 5 for an estimate of annual average snow is usually not part of the cemaneige model but was added here as raven requires an estimate and none was at hand two additional parameters for rain snow partitioning were added as only precipitation forcings instead of precipitation separated into rain and snow were available the model is setup to simulate streamflow for the salmon river basin located in the canadian rocky mountains in british columbia draining towards the streamflow gauge station near prince george water survey canada id 08kc001 the catchment is about 4200 km 2 large and almost not impacted by humans a lumped model was setup for the simulation period from january 1 1989 to december 31 2010 while the first two years were discarded as warm up hence 20 years of daily streamflow simulations were used for this study more details about the catchment can be found in mai et al 2020 section 2 1 3 case study domain therein in this work a range of metrics is used to find an optimal setup for this model to match the streamflow observations for the salmon river basin the objective function used primarily is the kling gupta efficiency appendix b 3 but also other metrics to study differences in calibration results due to this choice in single objective as well as multi objective calibration exercises appendix b objective functions used objective functions or metrics are used to objectively quantify the quality of a simulated model outputs y sim by comparing them with observed data y obs the various objective functions used throughout this study are the root mean square error appendix b 1 nash sutcliffe efficiency and some of its derivatives appendix b 2 the kling gupta efficiency and some of its derivatives appendix b 3 the squared pearson correlation appendix b 4 and the percent bias appendix b 5 b 1 root mean square error rmse the root of the mean squared error rmse is defined as b 1 rmse 1 n i 1 n y obs i y sim i 2 0 with n being the number of time points i where both the observations and simulations exist the rmse is usually chosen over the mean square error mse since it has the same unit as the data points themselves the error is hence easier to interpret and set in comparison with the actual measured and modeled variables the optimal value of the rmse and mse is 0 the goal of calibrations using rmse and mse is therefore to minimize the objective function values b 2 nash sutcliffe efficiency nse and some of its derivatives the nash sutcliffe efficiency nse nash and sutcliffe 1970 was likely the most popular metric used in hydrologic applications before the kling gupta efficiency next section was introduced the nse is calculated as one minus the ratio of the mean square error mse divided by the variance of the observed time series σ obs 2 b 2 nse m s e σ obs 2 y obs i y sim i 2 y obs i y obs 2 1 with y obs being the mean of the observed time series all sums are derived over all data points i where both the observation and simulation are available the nse is optimal for its maximum value of 1 and suboptimal for values less than 1 the goal of calibrations using nse or any of its derivatives is to maximize the objective function values b 3 kling gupta efficiency kge and some of its derivatives in this study the kling gupta efficiency kge gupta et al 2009 and its three components α β and r gupta et al 2009 are used for streamflow calibration the component α measures the relative variability of a simulated versus an observed time series e g q t the component β measures the bias of a simulated versus an observed time series while the component r measures the pearson correlation of a simulated versus an observed time series the overall kge is then based on the euclidean distance from its ideal point in the untransformed criteria space and converting the metric to the range of the nash sutcliffe efficiency i e optimal performance results in a kge and nse of 1 and suboptimal behavior is lower than 1 the kling gupta efficiency is defined as follows b 3 kge 1 1 α 2 1 β 2 1 r 2 1 with α σ y sim σ y obs and β y sim y obs while y and σ y are the mean and standard deviation of a time series y either simulated s i m or observed o b s and r being the pearson correlation coefficient to make comparisons easier the range of the kge components is transformed here such that they match the range of the kge this leads to the following component metrics as introduced by mai et al 2022c b 4 kge α 1 1 α 2 1 1 σ y sim σ y obs 2 1 b 5 kge β 1 1 β 2 1 1 y sim y obs 2 1 b 6 kge r 1 1 r 2 1 the kling gupta efficiency can also be derived when the simulated and observed time series are log transformed y ˆ ln y the kge will then be denoted as kge l o g another derivative of the kge is defined by focusing on high flow or low flow values the two metrics are denoted as kge l o g q high and kge l o g q low respectively a time point t is considered in the derivation of kge l o g q low if the observed streamflow for that time step q o b s t satisfies the following conditions b 7 0 0 q o b s t min q o b s 0 05 max q o b s min q o b s a time point t is considered in the derivation of kge l o g q high if the observed streamflow for that time step q o b s t satisfies the following condition b 8 q o b s t min q o b s 0 05 max q o b s min q o b s the metrics kge l o g q low and kge l o g q high are then the kling gupta efficiencies of the log transformed observed and simulated streamflow considering only low flow and high flow time steps respectively please note that the decision if a time step is a low flow or high flow time step is solely based on the observations which means it is always the same time steps for a given basin and time period while being independent of the simulation the goal of calibrations using kge or any of its derivatives is to maximize the objective function values as all range from infinity to 1 with the latter being optimal b 4 squared pearson correlation r 2 the pearson correlation r lies between 1 and 1 with the upper limit indicating a strong positive and the lower limit indicating a strong negative correlation between two time series y sim and y obs the pearson correlation is often squared to find model fits that strongly correlate irrespective of a positive or negative connection in hydrologic applications it is usually hard to obtain a strong negative correlation with an underlying physically based model hence the squared pearson correlation r 2 is more popular the squared pearson correlation is defined as follows b 9 r 2 y sim i y sim y obs i y obs y sim i y sim 2 y obs i y obs 2 2 0 1 where y is the mean of a time series all sums are derived over all data points i where both the observation and simulation are available the optimal value for the squared pearson correlation is 1 the goal of calibrations using r 2 is to maximize the objective function values b 5 percent bias pbias the percent bias is the relative error of the simulation compared to the observations expressed in percent the optimal value is 0 while positive values indicate an average overestimation of the simulation compared to the observed data while a negative percent bias indicates an average underestimation of the simulations compared to observations the percent bias is defined as follows b 10 pbias y sim y obs y obs 100 since the percent bias is optimal at zero and values below or above are not desired usually the absolute or squared percent bias both optimal at zero and non optimal for positive values is optimized for applications in this study the squared percent bias pbias 2 was minimized but the percent bias pbias reported due to its more insightful interpretation of over and underestimation of the simulation appendix c calibration algorithms used the calibration algorithms used in this study are not by any means an exhaustive list of available algorithms they have been chosen because they are popular in hydrologic modeling and they are available in the ostrich calibration toolbox matott 2017 and were hence easy to setup and switch between the different methods the main algorithmic parameters for each algorithm are summarized in table c 3 some of the parameters for example the budget of allowed model evaluations vary between experiments of this study or are dependent on the number of model parameters to be calibrated the rules used to derive the algorithmic parameters are specified in the table three algorithms that calibrated one objective at a time single objective algorithm have been used in this study i e the dynamically dimensioned search algorithm dds appendix c 1 the shuffled complex evolution algorithm sce appendix c 2 and the particle swarm optimization algorithm pso appendix c 3 one algorithm is used to demonstrate aspects of a calibration of multiple objectives simultaneously multi objective algorithm the pareto archived dynamically dimensioned search algorithm padds appendix c 4 is used for this purpose the algorithms are briefly described in the following subsections for further details on implementation the reader is referred to matott 2017 and for algorithmic details and functioning to the publications introducing the individual algorithms c 1 dynamically dimension search dds algorithm the dynamically dimension search dds algorithm introduced by tolson and shoemaker 2007 is a single objective algorithm mostly popular in hydrologic and water resource engineering applications it has been cited 691 435 times based on google scholar web of science as of july 2022 the algorithm is known to be budget efficient meaning it finds good solutions with a comparatively small number of model evaluations required the algorithm is hence often applied for models that are computationally expensive dds has only two algorithmic parameters see table c 3 the first is the perturbation value which is suggested to be fixed at 0 2 in the original publication and set to this value for all experiments the second algorithmic parameter is the allowed budget i e the number of times dds is allowed to evaluate the model using a parameter set it had sampled the algorithm will use all the allowed model evaluations and will not terminate before because the algorithm for example is detecting that it does not find any better solutions and hence determines to be converged the only possibility to check the quality e g convergence of the dds results is by performing multiple independent calibration trials and analyze their spread the budgets used for the various experiments in this study are derived mostly as multiples of the number of parameters e g 25 n p 50 n p and 100 n p with n p being the number of model parameters to calibrate for each experiment ten independent trials were performed c 2 shuffled complex evolution sce algorithm the shuffled complex evolution sce algorithm introduced by duan et al 1993 is another very popular calibration method in hydrology it has been cited 1828 1041 times based on google scholar web of science as of july 2022 the algorithm is available in the ostrich calibration toolkit the algorithm generally requires a larger budget than for example dds it however has an internal measure of determining convergence which means a huge budget can be specified and the algorithm would terminate when necessary if the convergence criterion is not fulfilled by the end of the maximum number of allowed model evaluations and the algorithm aborts because of this upper limit being reached a possibility to check for the quality convergence of the results is to analyze the spread of the results of independent trials as done for dds for most examples shown in this study sce did not converge by itself the budgets were chosen depending on the number of model parameters n p to calibrate in the various experiments usually 25 n p 50 n p and 100 n p for each trial in total ten independent trials were performed for each experiment the algorithmic parameters used for sce are listed in table c 3 several of them are chosen depending on the number of model parameters n p the parameters are set to those values as they were suggested in the original publication duan et al 1993 and follow up publications duan et al 1994 behrangi et al 2008 c 3 particle swarm optimization pso algorithm the algorithm of particle swarm optimization pso was introduced by kennedy and eberhart 1995 in a conference paper and later in more detail in kennedy et al 2001 it is likely one of the most cited algorithms with 73 328 on google scholar the algorithm is available in the ostrich calibration toolkit and the algorithmic parameters used in this study are listed in table c 3 several of those parameters again are chosen depending on the number of model parameters n p to be calibrated the budget for the pso algorithm is not explicitly specified in ostrich but is the number of generations increased by 1 times the swarm size the number of generations was chosen such that the budget was as close as possible to pre defined budgets such as 25 n p 50 n p and 100 n p for each trial in total ten independent trials were performed for each experiment the algorithm has an internal convergence criterion it would use to determine to abort the calibration even if the maximal number of model evaluations has not been reached yet like sce almost no experiment performed in this study however reached that stage almost all experiments were determined because the budget was reached like dds and sce independent calibration trials were used to determine the quality convergence of the algorithm c 4 pareto archived dynamically dimensioned search padds algorithm the pareto archived dynamically dimensioned search padds algorithm is used in this study as an example for a multi objective calibration algorithm it was introduced by asadzadeh and tolson 2013 google scholar cited by 65 web of science 43 as of july 2022 using the method of hypervolume contributions to determine the next candidate parameter set to evaluate the model with while asadzadeh et al 2014 google scholar cited by 37 web of science 23 as of july 2022 proposed convex hull contributions as a superior but computationally more expensive method to determine candidate parameter sets the first method is available in ostrich and has been used in this study the other algorithmic parameters are the same as for dds all can be found in table c 3 like dds padds is known to be budget efficient making efficient use of the specified budget the budget of allowed model evaluations is set to be 300 n p which will be used completely by padds no pre emption of the algorithm through convergence criteria is implemented the quality convergence of the calibration experiments is determined using ten independent calibration trials in all experiments the overall best solution pareto front is determined by merging the pareto fronts of all trials to one final front for a more detailed and visual explanation of pareto fronts the reader is referred to the supplementary material of mai et al 2022c sect s2 and fig s2 therein appendix d pie share sampling the pie share sampling method and some applications were introduced in mai et al 2022a therein the parameters x i are assumed to sum up to 1 with noting that this assumption can be relaxed but not providing the generalized sampling scheme for this the more general description is x 1 c s n r 1 x 2 c x 1 s n 1 r 2 x 3 c x 1 x 2 s n 2 r 3 d 1 x j c i 1 j 1 x i s n j 1 r j x n 1 c i 1 n x i with s n r 1 1 r 1 n r i u 0 1 i 1 n with r i i 1 n being n uniformly distributed parameters that are transformed into model parameters x i the method can be used a when n 1 parameters x i must sum up to a certain value c or b when the sum of n parameters x i is not allowed to exceed a threshold c 
2030,model calibration is the procedure of finding model settings such that simulated model outputs best match the observed data model calibration is necessary when the model parameters cannot directly be measured as is the case with a wide range of environmental models where parameters are conceptually describing upscaled and effective physical processes model calibration is therefore an important step of environmental modeling as the model might otherwise provide random outputs if never compared to a ground truth model calibration itself is often referred to be an art due to its plenitude of intertwined steps and necessary decisions along the way before a calibration can be carried out or can be regarded successful this work provides a general guide specifying which steps a modeler needs to undertake how to diagnose the success of each step and how to identify the right action to revise steps that were not successful the procedure is formalized into ten iterative steps generally appearing in calibration experiments each step of this calibration life cycle is either illustrated with an exemplary calibration experiment or providing an explicit checklist the modeler can follow these ten strategies are 1 using sensitivity information to guide the calibration 2 handling of parameters with constraints 3 handling of data ranging orders of magnitude 4 choosing the data to base the calibration on 5 presenting various methods to sample model parameters 6 finding appropriate parameter ranges 7 choosing objective functions 8 selecting a calibration algorithm 9 determining the success and quality of a multi objective calibration and 10 providing a checklist to diagnose calibration performance using ideas introduced in the previous steps the formal definition of strategies through the calibration process is providing an overview while shedding a light on connections between these main ingredients to calibrate an environmental model and will therefore enable especially novice modelers to succeed keywords model calibration parameter sampling calibration strategies data availability the data and codes are made available on zenodo https doi org 10 5281 zenodo 7563321 1 introduction environmental models such as hydrologic models water quality models and ecohydrologic models are created to increase the understanding of environmental processes while allowing to study the impact of changes made to the system in computational experiments which might be too costly or impossible to perform in reality the models are approximations of the natural systems in an attempt to reproduce the relevant behavioral characteristics of the true natural system wagener and mcintyre 2007 creating these models in itself is a multi stage process including model study planning data and concepts used model setup calibration and validation as well as simulation and evaluation refsgaard et al 2007 the step of model calibration is necessary for most environmental models as they contain parameters that are either upscaled and or conceptual and can hence not be determined using field measurements kirchner 2006 model calibration is therefore used to infer optimal model parameter settings leading to a good match with those available observations while model calibration seems like a task of secondary importance compared to creating the model itself collecting data and formulating target purpose of the model it is replete with challenges such as a multiple model setups can lead to equally good model outputs equifinality and hence multiple equally likely model realizations all potentially right for the wrong reasons e g beven 1993 beven and freer 2001 khatami et al 2019 b unidentifiable parameters given available data and calibration setup leading to parameters that cannot be transferred as their calibrated values are purely random e g wagener et al 2003 guse et al 2020 c calibrating multiple and or nested basins simultaneously leading to increased model runtimes and the risk that a parameter set that works everywhere might not exist in case parameters are not regionalized properly e g wi et al 2015 rakovec et al 2016 2019 d numerical daemons leading to unstable model runs and jumps in the objective function that hamper the calibration process e g kavetski et al 2006 clark and kavetski 2010 kavetski and clark 2010 e difficulties of defining an objective function measuring the quality of a model fit that is coherent with the purpose the model was built for e g schaefli and gupta 2007 gupta et al 2009 mizukami et al 2019 clark et al 2021 f deciding how to split available data appropriately into sets that are used for model calibration and validation evaluation leading to model settings that might not be robust enough to future changes in case they are only trained with past data or not able to predict responses to extreme events if trained with only a small set of data e g daggupati et al 2015 arsenault et al 2018 zheng et al 2018 2022 shen et al 2022 or g inherent uncertainties associated to model structure input data uncertainties and uncertainties of the observations used for model comparison leading to unrealistic or inappropriate model parameterizations e g kirchner 2006 günther et al 2019 abbaszadeh et al 2019 among many more scientist have been grappling with these challenges advancing the field of environmental model calibration significantly the interested readers are referred to the following literature to learn more about some of these advances for example employing clustering approaches and evolution strategies during calibration e g duan et al 1993 hansen and ostermeier 2001 deb et al 2002 hansen et al 2003 making use of the covariance between parameters during parameter sampling e g hansen and ostermeier 2001 hansen et al 2003 dynamically reducing the search space e g tolson and shoemaker 2007 using soft rules rather than strict calibration objectives to obtain behavioral subsets from random samples of model simulations e g choi and beven 2007 shafii et al 2015 hartmann et al 2017 sarrazin et al 2022 or improving strategies when multiple objectives are to be calibrated e g deb et al 2002 asadzadeh and tolson 2013 asadzadeh et al 2014 several studies exist comparing subsets of these methods showing that some algorithms work better for some problems than others e g yen et al 2015 arsenault et al 2014 wallner et al 2012 zitzler et al 2000 despite these advances model calibration remains somewhat of an art where the latest advances are not always integrated into the workings of someone inexperienced with the calibration literature this is often due to the fact that model calibration is the means to an end to obtain a functional model setup for operational purposes to couple several models or to advance process understanding understandably more time is invested in assembling an advanced model with latest process descriptions and preparing data to setup test the model than for the model calibration itself this work is inspired by the book of hill and tiedeman 2007 which comprises 14 methods and guidelines for effective model calibration while outlining several good modeling practices mainly demonstrated using groundwater models the goal here is to develop a recipe that synthesizes the latest advances in model calibration to create something usable for the low experience modeler while being general enough to be applied to a wide range of environmental models but also provide a full picture of the main parts of each calibration process ten strategies are outlined where each strategies is supported by computational experiments or visualized in form of checklists the work is organized as follows section 2 and its ten subsections will describe in detail each of the ten steps while providing a body of relevant literature and either illustrate the impact of the discussed step with a calibration experiment sections 2 1 to 2 3 and sections 2 5 to 2 9 or define a checklist for the modeler to follow section 2 4 and section 2 10 a summary is provided thereafter section 3 the different experiments presented in this work are using a consistent set of models appendix a objective functions appendix b calibration algorithms appendix c and sampling schemes appendix d 2 strategies this section is laying out ten strategies on how to make decisions throughout the model calibration process and offers ideas to solve challenges arising the section is structured in ten subsections each addressing one challenge given the wide variety of large uncertainties in model inputs outputs and model structures themselves a perfect calibration of environmental models is not possible even when applying all strategies the goal however is to obtain reliable model results in line with our process understanding while making sure the calibration is setup as robust as possible the ten strategies are not meant as a complete list of every issue that can occur during model calibration the strategies and challenges selected are general enough to appear in most calibration exercises using environmental models when using sets of metrics algorithms and models in the ten subsections it is not meant to be an exhaustive list of all options available the sets have been chosen for demonstrative purposes to highlight a range of features and behaviors and to prepare the reader for the wide variety of issues that they might face during a model calibration model calibration is an iterative process which usually requires two or three iterations before the experimental setup is adjusted and can be used reliably for a given model and dataset each iteration consists of roughly three stages illustrated in fig 1 the strategies to resolve challenges in each of these levels however are usually not performed in a strict consecutive order and can easily be swapped with each other the numbering of them in this work is purely due to being able to cross reference them the central questions and topics of each stage will be explained in detail in the following sections this question style approach to model calibration was inspired by hill et al 2015 note that the term strategies rather than steps has been used here to account for the facts that a the items in each stage do not need to be addressed in the specific order presented b additional steps might need to be required for settings and models too specific to be covered here e g multi basin calibration or calibration of coupled models and c some items might not be of interest e g parameter ranges are prescribed and are not allowed to be changed the first stage is the preparation step which deals with general questions about model and data this is where one might apply a sensitivity analysis to identify insensitive unimportant parameters to exclude them from the calibration or at least be aware that these parameters will not be identifiable through this calibration section 2 1 good checks before getting started with the actual calibration are to check if parameters are independent or correlated section 2 2 if the observed data are uniformly spread across time and or space section 2 3 and to make decisions on which data to base the calibration on and which data to safe for a validation evaluation section 2 4 at this point one also needs to decide to use an automatic calibration instead of a manual tuning of individual parameters or a random sampling of parameters section 2 5 the second stage is the execution step where the calibration is setup and all related decisions about defining parameter ranges section 2 6 choosing the objective function that reflects the purpose of the model section 2 7 picking the right calibration algorithm section 2 8 and deciding whether to calibrate a single or multiple objectives section 2 9 the third stage is the checking step where the results of the calibration are thoroughly assessed and decisions about revising the experimental setup with respect to parameter ranges choice of objective function settings of calibration algorithm etc are made section 2 10 the results of the analysis of this stage determines whether to redo the calibration with the revised experimental setup or to use the current calibration results this iterative process is a very important part of model calibration a lot of valuable information about the model is gathered during this process for example model crashes might be caused by too wide parameter ranges one can either reduce the ranges until no crash appears anymore or one can use this opportunity to fix the actual reason in the model implementation causing this issue another example is learning about and fixing parameters that are not independent of each other resolving parameter constraints is not only making the process of finding parameter ranges easier but will also ultimately make the model calibration more successful since most calibration algorithms assume the parameters to be independent while sampling e g kirkpatrick et al 1983 duan et al 1993 kennedy and eberhart 1995 kennedy et al 2001 tolson and shoemaker 2007 and hence suffer from correlated parameters chu et al 2010 wu et al 2017 a third example is finding good and reliable settings for the calibration algorithm itself can usually only be found through testing several settings especially when calibrating a new model at a new location or with new data a robust calibration setup is more likely to easily be transferred to other locations or applications note that there are several tools that are helpful in assisting a successful model calibration model revisions fenicia et al 2016 kavetski and clark 2010 are most helpful when parameters are correlated or the model is crashing under some settings sensitivity analysis and parameter screening borgonovo and plischke 2016 iooss and lemaître 2015 razavi and gupta 2015 saltelli et al 2008 become useful when several parameters are not impacting the model outputs and could be removed from or fixed during the model calibration uncertainty analysis moges et al 2021 mcinerney et al 2017 moradkhani and sorooshian 2009 wu et al 2006 is helpful in incorporating uncertainties of data into the objective function and understanding how these uncertainties propagate to parameter uncertainties and uncertainties of model outputs none of these methods are necessary to perform a model calibration but can be a real asset especially when the goal is to gain a holistic understanding of a model the following ten subsections will describe the ten challenges laid out above in detail 2 1 use of sensitivity analysis to identify parameters to calibrate sensitivity analyses are methods that are used to determine the importance of inputs usually model parameters for the simulation of certain outputs e g streamflow herman et al 2013b a pfannerstill et al 2015 cuntz et al 2016 markstrom et al 2016 mai et al 2022b or performance metrics signatures e g kling gupta efficiency demirel et al 2018 bajracharya et al 2020 lilhare et al 2020 basijokaite and kelleher 2021 this is achieved by generating samples throughout the entire parameter space global methods and evaluating the models for those parameter sets and or combinations of those parameter sets e g use one part from one parameter set and another part from another set the analysis of these model runs e g variance or distribution of model outputs is then used to draw conclusions about the impact of model parameters on model outputs such as identifying parameters leading on average to bigger differences in model outputs while other parameters might not have any effect they can also be more creatively used to detect model deficiencies göhler et al 2013 sheikholeslami et al 2019 help selecting objective functions demirel et al 2018 bajracharya et al 2020 lilhare et al 2020 basijokaite and kelleher 2021 evaluating the importance of alternative model inputs baroni and tarantola 2014 schürz et al 2018 or identifying time period where certain parameters are more likely to be inferable herman et al 2013b pianosi and wagener 2016 like calibration methods there is a wide range of sensitivity analysis methods for reviews of the different methods and background of sensitivity analysis methods in general refer to for example saltelli et al 2008 iooss and lemaître 2015 razavi and gupta 2015 or pianosi et al 2016 the benefits of applying sensitivity analyses before model calibration in order to achieve faster convergence or more robust model setups have been demonstrated before demirel et al 2018 hill et al 2015 cuntz et al 2015 shin et al 2013 foglia et al 2009 hill 2007 white and chaubey 2005 among others here the automatic parameter screening method of efficient elementary effects eee introduced by cuntz et al 2015 is applied to determine the most impactful subset of parameters of a hydrologic model with n p 9 parameters appendix a 3 the parameter sensitivity is derived regarding the kling gupta efficiency appendix b 3 of each model run the screening converged automatically after 80 model evaluations using the standard settings proposed in cuntz et al 2015 the screening is identifying n ˆ p 6 parameters x 1 to x 3 and x 6 to x 8 see table a 2 for definition as informative while the remaining three x 4 x 5 x 9 see table a 2 for definition are regarded noninformative the model setup and the setup of eee used here are available on github mai and cuntz 2020 see examples raven gr4j cemaneige to demonstrate the impact of using only the screened set of parameters a model calibration is applied here using either all n p 9 or the screened subset of parameters n ˆ p 6 the model performance is evaluated using the kling gupta efficiency appendix b 3 of the hydrologic model using three calibration algorithms i e the dynamically dimensioned search dds algorithm appendix c 1 the shuffled complex evolution sce algorithm appendix c 2 and the particle swarm optimization pso method appendix c 3 note that later sections will focus on various performance metrics and calibration algorithms i e section 2 7 and section 2 8 respectively while the aiming to demonstrate which impact a parameter screening could have on later carried out calibration experiments each calibration either using all or only informative parameters is performed in ten independent trials and three different budgets 25 n p 50 n p and 100 n p note that computational budget refers to the maximum number of allowed model runs i e the number of model runs one can afford to obtain a calibration result the calibration results in terms of development of the objective function values over the course of the model calibration are presented in fig 2 the results show that the reduction of parameters to only the informative subset has indeed an impact on calibration results first the calibration trials using the informative subset fig 2d f are generally more consistent compared to using all parameters fig 2a c especially for larger budgets which indicates that the results converge faster second no significant loss can be detected in terms of the best objective function value found across all trials of the calibration compare colored labels in fig 2a vs fig 2d fig 2b vs fig 2e and fig 2c vs fig 2f this is an expected behavior and shows that the parameters screening indeed only identified noninformative parameters to be discarded during calibration third the impact on the calibration performance depends on the calibration algorithm dds and pso behave very similar exhibiting consistent behavior i e spread of trials and fast convergence i e plateauing of objective function values when using all parameters figs 2a and c respectively the dds and pso results using only informative parameters figs 2d and f respectively are more consistent but there is no big difference in general for sce the calibration results are most consistent i e spread of trials for the largest budget fig 2e for the lowest budget the results actually decrease in terms of best objective function found across all trials kge 0 79 calibrating all parameters vs kge 0 74 calibrating only informative parameters this is likely since sce performs best for large budgets and when it can determine when to abort the calibration by itself which was not the case in any of the experiments performed here note that a rather simple hydrologic model was used here for demonstration purposes leading to rather small overall differences between the model with and without screened parameters these differences are expected to be substantial when a more complex model is screened and calibrated the most important point of identifying the informative parameters however is that only calibrated parameter values for those parameters should be analyzed and reported the noninformative parameters are not identifiable given the available data and objective function used informative parameters however are also identifiable unless they are only informative due to correlation and interaction with other parameters the latter can usually be observed through several parameters following similar patterns throughout the model calibration reporting the calibrated values of the noninformative parameters is bearing the risk that they are used in other applications as if they were constrained through this calibration without parameter screening or another sensitivity analysis the information of parameter identifiability would not have been available it is advisable to use a frugal low budget sensitivity analysis such as the morris method morris 1991 or the here used eee method cuntz et al 2015 to identify the informative parameters since only a binary qualitative information if a parameter is informative or not is required to decide whether to calibrate or report a parameter only in very few cases quantitative parameter sensitivities are needed 2 2 considering parameters with constraints to allow for efficient sampling of parameter space model parameters are assumed to be independent by most model calibration algorithms this assumption however is often violated in environmental models hrachowitz et al 2014 for example a parameter is expected to be larger than another one e g vegetation top height needs to be larger than its bottom height or groups of parameters need to sum up to a certain value e g sand soil and clay fractions need to sum up to one these assumptions are easy to be met when doing a manual calibration since the modeler knows that for example the bottom height of the vegetation needs to be smaller than the vegetation top height and is using this understanding when proposing parameter sets during an automatic calibration these constraints need to be translated into the automatic sampling scheme if not some unlucky parameter constellations kavetski et al 2006 might lead to model failures sheikholeslami et al 2019 there are two general possibilities to handle these infeasible parameter sets the first is to simply ignore all constraints sample parameter sets and force the model with them the model might either fail crash or return not a number or no data values the issue is that the calibration algorithm might for example get stuck at those infeasible not a number no data values thinking that this is the optimum as it cannot find any better solution this happens especially if a large proportion of the parameter domain is infeasible filtering these non behavioral model outputs and not considering them in the calibration algorithm is computational least beneficial as the model has already been run with knowing that the parameter set is infeasible and the information of model failure is not even used to guide the search for optimal parameter sets the other possibility is to reject infeasible parameter set right away and resample parameter sets until a feasible one is found this might lead to problems as the calibration algorithm will never learn to avoid these regions the methods of sampling until a feasible parameter combination is found will be referred to as accept reject methods any of the described methods is inefficient and will inevitably lead to problems compared to resolving the constraint or sampling of the parameters avoiding producing infeasible parameter sets besides the above described methods there are some alternative approaches to resolve constraints by either adjusting the model itself through redefinition of parameters using the so called delta method used by e g mai et al 2020 chlumsky et al 2021 or redefining the sampling of the parameters using for example the pie share method mai et al 2022a the delta method is a possibility to revise parameter dependencies where one parameter x 2 needs to be larger than parameter x 1 the idea is to redefine x 2 to be x 1 increased by a positive value δ now δ can be sampled rather than x 2 the model required parameter x 2 can be derived either before the model is provided with a parameter set or the model can be revised to remove x 2 and replace it by x 1 δ throughout the latter is recommended as it will make sure parameters in the model are independent and knowledge about parameters like the information that one needs to be larger than the other are part of the model itself rather than expecting the modeler to know the pie share method introduced by mai et al 2022a is helpful when several parameters need to sum up to a constant c for example when percentages of sand silt and clay must sum up to 100 to determine the texture of a given soil or when sampling biomass compositions by percent lipids carbohydrates and proteins which again need to sum up to 100 the pie share method can also be applied when the sum of parameters is required to not exceed a certain threshold c instead of summing up to this exact value the pie share method essentially describes a strategy on how to transform n uniformly sampled random numbers into parameters x i fulfilling the aforementioned criteria the appendix d describes this method for any threshold c while it was originally introduced for thresholds c 1 mai et al 2022a an incomplete overview of parameters with constraints and attempts to sample them are given in table 1 for constraints that involve two or more parameters two methods each are listed fig 3 visualizes two of the examples given in the table the first example demonstrates the sampling of two parameters x 1 2 6 and x 2 2 8 under the given constraint that x 1 needs to be smaller than x 2 the independent sampling of both parameters using the accept reject method is leading to parameter combinations that are infeasible fig 3a gray colored samples a revised version of the sampling using x 2 x 1 δ with δ 0 6 is also leading to infeasible sets due to the fact that x 2 can now take values beyond the originally specified upper limit of 8 this method might however be favorable if this upper limit of the parameter is defined by the modeler without any physically based constraint and could easily be flexible the second example is demonstrating a situation where the sum of two parameters x 1 0 8 and x 2 0 8 is not allowed to exceed a threshold c 8 the independent sampling of the two parameters accept reject sampling leads to 50 infeasible combinations fig 3c gray colored samples while the use of the pie share sampling fig 3d resolves this issue and only leads to parameter combinations that fulfill the constraint for the pie share sampling n 2 uniformly distributed parameters r 1 u 0 1 and r 2 u 0 1 are sampled and then transformed into x 1 and x 2 using x 1 c 1 1 r 1 and x 2 c x 1 1 1 r 2 c x 1 r 2 as defined in eq d 1 there are certainly other strategies to resolve constraints of parameters for example by revising the model redefining parameters or potentially using copulas this section is to emphasize the importance of this step and to inspire addressing these issues independent parameters and robust model formulations make any model analysis like calibration sensitivity analysis and uncertainty analysis more efficient 2 3 transformation of data ranging orders of magnitude to find optimal fitting model setup environmental observations are usually used to find a best model setup observations of for example time dependent conductivity and transmissivity might range over multiple orders of magnitude with more observations being available for early timesteps and fewer for large timesteps interestingly such kind of data and corresponding model fits are usually presented and assessed in a log transformed or similar space even though this transform might not have been used during the calibration of the model e g madi et al 2018 de rooij et al 2021 fitting the data in their native domain or in the log transformed domain is leading to different optimal models which is illustrated for an experiment presented in fig 4 the first part of the experiment is using the native data points x y x f x and fitting the four parameters of the logistic function defined in eq a 3 fig 4a in the second part of the experiment the log log transformed data x ˆ g x ˆ log 10 x log 10 y are used to fit the corresponding four parameter logistic function in eq a 5 fig 4b the two logistic functions are explained in detail in appendix a 2 parameters and their ranges are defined therein both experiments used ten independent trials of the shuffled complex evolution algorithm appendix c 2 with an upper limit of 500 model evaluations each the root mean square error rmse appendix b 1 is used as a metric to quantify the quality of each model simulation the results show the superior quality of the model fit using the log log transformed data this is because the data and the model fit are assessed in this log log transformed domain both axes in plot are logarithmic the first experiment which is fitting the native data should be evaluated in the native domain that plot would however only show the largest data points and the model fit through these one or two points due to the distribution of the available observations the objective function used here rmse is focusing on fitting only these largest data points as this has the highest impact on any squared error statistic log transforming the data is removing this effect as a rule of thumb data should always be fitted in the transformation the fit is assessed in note that log transformations can also be applied to model parameters spanning multiple orders of magnitude for which is known that small values are more likely than larger parameter values or that perturbations in small parameter values have a larger impact on the model output than perturbations for larger orders of magnitude an example is hydraulic conductivity used as a model parameter to describes water movement through saturated media 2 4 choosing data to use for calibration and validation evaluation an essential step before starting with model calibration is to choose the data the calibration will be based upon two options are to use all available data or splitting the data into two sets i e data that are used for model calibration and an independent set used to estimate how the model performs when facing an independent new set of data this second set of data is commonly referred to as validation or evaluation data the latter term is suggested to be more appropriate for example by oreskes 1998 and wagener et al 2022 as it cannot be formally verified that a model represents the truth and hence validating might be misleading validation however has been used mostly in the environmental modeling community up to now and is hence kept for the sake of inclusiveness this split sample approach is central to the model validation evaluation framework proposed by klemeš 1986 it has since been challenged in several studies arsenault et al 2018 zheng et al 2018 2022 shen et al 2022 demonstrating that split sample approaches have their challenges for example showing that most robust models can be achieved when all data are used for calibration and none are held back for model validation evaluation arsenault et al 2018 shen et al 2022 first option mentioned above or showing that different data splitting approaches lead to large differences in model performance while identifying most robust approaches zheng et al 2018 2022 besides the classic way of validating evaluating models using the split sample method other approaches have been discussed for example the use of sensitivity analysis to complement a comparison of modeling results with observations wagener et al 2022 the model performance obtained during the validation evaluation period is commonly used to derive an understanding of how the model will behave in an independent time period if the state of the system modeled would never change i e all input data such as precipitation and temperature would be stationary the land use would not change and the rivers would not get perturbed by human intervention the validation evaluation performance would likely be the same or very close to the calibration performance since this is not the case in most environmental case studies the validation evaluation is performed to get an idea of how much model prediction skill achieved during calibration would be lost when the model is facing new data two general approaches have been recommended in the literature the first approach is recommended when the calibrated model needs to be put directly into operation i e the model is running in a real time application using data that have not been available during model calibration fig 5a in these cases the data used for calibration should be chosen to be as close as possible to future states of the data reducing impact of non stationary data future states of the model reducing performance loss because model does not transfer well with non stationary inputs and future states of the region the model is applied to avoiding e g large landuse changes hence the calibration data are chosen to be the most recent data more dated data are used for validation evaluation to generate estimates of how much the model performance might drop when the model is facing data that might include large changes in statistical properties of data e g climate change or landuse change the fact that the validation evaluation data period is earlier than the calibration data period might lead to unrealistic estimates this approach is however leading to most robust model results during the deployment period as shown by shen et al 2022 the deployment period is referred to as testing period therein to avoid these unrealistic estimates of model performance it is preferable to use dated data for calibration and the recent data for validation evaluation which is the classic setup in fields like hydrologic modeling this second approach fig 5b where the calibration data period precedes the validation evaluation period leads to more realistic estimates of loss in model skill when the model is facing future data like for example when the model is deployed into its operational application before the model is however put into operation the model is recommended to be calibrated a second time using all available data to enable the most robust results during model deployment this approach is leading to the most robust results when the model is deployed as shown by for example arsenault et al 2018 and shen et al 2022 in large scale studies the expected loss in performance in the deployment period is likely close to the one determined during the foregoing split sample experiment 2 5 choosing appropriate parameter sampling strategy to find optimal model setups the most essential step of each calibration algorithm is the proposal of the next parameter set to evaluate i e the parameter set to run the model with and determine the quality of the fit of the model outputs with observed data one method of proposing the next parameter set is pure random sampling using a random number generator to sample a value for each parameter given its range or distribution this strategy is often referred to as monte carlo method it does not consider information gathered by evaluating previous parameter sets and hence has the advantage to be easily parallelizable as each evaluation is independent of all others another type of sampling are so called stratified sampling methods like latin hypercube sampling lhs mckay et al 1979 or sobol sequences sobol 1976 they sample the parameter domain making sure that the domain is uniformly sampled for any number samples drawn which is not necessarily the case with a purely random sampling these two sampling methods are applicable in a wide range of methods such as sensitivity analysis uncertainty analysis and model calibration a third set of methods are calibration algorithms which major feature is their built in parameter sampling approach specifically tailored to calibrate models this means their proposal of a new candidate parameter set is derived based on the information gathered with preceding parameter sets there is a wide range of strategies how to make use of that information leading to a plenitude of calibration algorithms jahandideh tehrani et al 2021 yen et al 2015 arsenault et al 2014 wallner et al 2012 note that the automatic sampling of parameters through the calibration algorithm requires that parameter constraints are fulfilled for any sample drawn to avoid model crashes during calibration approaches to assure this especially for constrained parameters are discussed above in section 2 2 to demonstrate differences and efficiency of the three methods parameter samples are drawn and evaluated using a benchmark function in replacement of an environmental model benchmark function have the advantage that the optimum is known and they are made to stress test calibration methods in this case the ackley function appendix a 1 with n p 2 and n p 10 parameters is used the optimal parameter set of the ackley function is when each parameter is zero leading to the smallest objective function value of zero latin hypercube sampling is used as stratified sampling method while the shuffled complex evolution sce duan et al 1993 algorithm see appendix c 2 is used as automatic calibration algorithm in total ten independent trials are performed for each method to visualize the random component of each method each method was allowed to draw 100 n p samples in each trial fig 6 shows the distribution of the first two dimensions x 1 and x 2 of the samples panels a c and g i as well as the best function value found after each sample drawn the distribution of the sampled values shows that the random and stratified sampling pick parameter values uniformly distributed across the entire domain figs 6a 6b 6g and 6h the development of the function values demonstrates that both methods work similarly good for low dimensional problems figs 6d and 6e since they can efficiently sample given the allowed budget 200 4 2 12 5 samples per volume unit while it shows that these methods fail to find the optimum for the higher dimensional case figs 6j and 6k in these cases the sampling density is significantly decreased 1000 4 10 00095 samples per volume unit the calibration algorithm however is in most cases successful to identify the optimum figs 6f and 6l or at least get very close to the target function value of zero the sampled parameter values show a clear favor of values around the origin where the optimum is located the algorithm is focusing on beneficial areas and is hence making better use of the budget compared to random and stratified sampling stratified and random sampling will find the minimum eventually it will only take on average much more samples note that random sampling is likely not an approach one would follow when calibrating a model manually i e parameter sets are picked manually the model is run the next set is picked etc occasionally especially at the beginning one would pick a random sample to check if a better solution can be found somewhere else but most of the time one would try to not stray too much from an already good solution but try to successively improve it with smaller changes this natural behavior is what most calibration algorithms try to mimic it has inspired a wide range of optimization algorithms specifying its motivating species in the name jahandideh tehrani et al 2021 such as the ant colony optimization dorigo et al 1991 particle swarm algorithm kennedy and eberhart 1995 shuffled frog leaping algorithm eusuff and lansey 2003 honey bee mating optimization haddad et al 2006 and bat algorithm yang 2010 2 6 adjustment of parameter ranges to make sure optimal values are covered while calibration algorithm is still efficient for every parameter to calibrate a distribution needs to be defined most commonly a uniform distribution is assumed by specifying lower and upper bound for each parameter but technically any kind of distribution like gaussian or log normal distribution can be specified it is more a question of whether the calibration algorithm or calibration toolkit used is supporting those distributions when sampling parameters the ostrich calibration toolbox for example does not allow for distributions other than uniform but it allows to transform parameters or even use derived versions of parameters the parameters of those distributions such as the lower and upper bound of the uniform distribution are most important for a calibration as they defined the search domain for the calibration algorithm in physically based models these ranges might be determined by physics but in a lot of conceptual models the parameters are upscaled and or conceptual entities and a range is rarely known a priori defining wide ranges will make the calibration less efficient as the calibration algorithm will have a larger domain to explore and might get trapped in local optima more easily while defining ranges too narrow might have the effect that the location of the actual global optimum is not contained which will in most cases show in parameters running against their lower or upper bound indicating in which direction it would prefer to go if it would be allowed to the results of an experiment demonstrating these effects are shown in fig 7 for all parts of this experiments the shuffled complex evolution algorithm sce details in appendix c 2 was used allowing for a budget of 500 model evaluations for each of the ten independent trials performed the goal of each calibration trial is to calibrate the four parameters of the four parameter logistic function described in eq a 5 in order to minimize the root mean square error rmse appendix b 1 between the model and log log transformed data points the ranges of the four parameters might not be known beforehand and set to e g l u 4 0 40 0 k u 1 0 1 0 5 10 0 x 0 u 4 0 4 0 s u 1 0 1 0 5 20 0 the independent trials of the calibration result in a large spread of model fits fig 7a the evolution of the parameters over the course of the calibration fig 7d e j and k shows that several trials converge to similar values at the end of the calibration especially for x 0 and most values are within the range and not hitting a bound which is good furthermore several parameters do not take any values in large portions of their specified range i e parameters l x 0 and s hence the ranges might be restricted to l u 10 0 20 0 k u 2 5 5 0 x 0 u 1 0 3 0 s u 1 0 1 0 5 5 0 in order to achieve a smaller spread in the fitted models these narrow ranges however lead to a situation where the parameters are now too restricted the evolution of the parameters throughout the calibration fig 7f g l and m shows that parameter l clearly converges against its lower bound suggesting its optimal value might be below the lower bound the other parameters do not seem to converge at all this is mostly because the parameter l is the most sensitive parameter the fitted functions fig 7b are clearly of lower quality than the previous calibration using the wider ranges using an intermediate set of ranges u 2 0 20 0 k u 1 0 1 0 5 5 0 x 0 u 2 0 2 0 s u 1 0 1 0 5 10 0 is leading to consistent and high quality fits fig 7c and the parameters converge to values placed within the range fig 7h i n and o this is to demonstrate several points first determining ranges is in most cases an iterative process especially when the model is conceptual or unknown to the person calibrating it second parameters are interacting with each other meaning that when one parameter is too restricted other parameters are likely not going to converge either third diagnosing the quality of a calibration and determining reasons for its failure can be achieved by performing multiple trials to use the spread as a measure of quality and analyzing the course of the parameters throughout the calibration diagnosing too wide or narrow ranges 2 7 choose the appropriate objective function for the purpose of the model a main setting that is guiding an automatic model calibration is the objective function as this one makes an objective decision whether a parameter set leads to a preferable model simulation compared to another parameter setting the objective function is expected to reflect the subjective judgement of the modeler comparing two model simulations historically there are objective functions that are more popular in some fields like the nash sutcliffe efficiency nse nash and sutcliffe 1970 see appendix b 2 and the kling gupta efficiency kge gupta et al 2009 see appendix b 3 to evaluate the quality of a hydrologic model simulation these metrics are commonly chosen by default even though there are discussions whether they are the most appropriate metrics lamontagne et al 2020 mizukami et al 2019 schaefli and gupta 2007 moriasi et al 2007 or whether model signatures are more appropriate to be used for calibration gupta et al 2008 mcmillan 2020 user surveys employed to understand the mechanisms behind hydrologic decision making emphasize that metrics often used do not necessarily reflect what a human would have decided gauch et al 2022 crochemore et al 2015 one needs to be aware that metrics are usually focusing on certain features e g high flows low flows and or recession and the modeler might need to revise or create a metric according to their needs this can be achieved by for example weighting more important data points higher or weighting data points with respect to their uncertainty or by transforming or filtering data points the impact of the choice of objective function on the resulting optimal model setting selected by the calibration algorithm is demonstrated in the following the hydrologic model gr4j with cemaneige for snow and a flexible rain snow partitioning is used to fit observed discharge data for the salmon river gauge station near prince george in british columbia canada details about the model and catchment can be found in appendix a 3 ten independent trials of the shuffled complex evolution algorithm sce appendix c 2 are carried out allowing for 100 n p model evaluations in each trial to calibrate the n p 9 parameters of the model the following six objective functions have been used as objectives for model calibration a minimizing the root mean square error rmse q appendix b 1 b maximizing the kling gupta efficiency kge q appendix b 3 c maximizing the nash sutcliffe efficiency nse q appendix b 2 d maximizing the nash sutcliffe efficiency of the log transformed discharge nse logq appendix b 2 e maximizing the squared pearson correlation coefficient r q 2 appendix b 4 and f minimizing the percent bias pbias q appendix b 5 the objective functions have been selected as they are commonly used metrics to calibrate and evaluate hydrologic models the resulting hydrographs of one example year of the 20 year calibration period and the optimal objective function values calculated for the 20 year period are shown in fig 8 in most cases the calibration of an objective function led to the best result for the respective metric across all experiments black highlighted label is best compared to gray colored values in the other five panels for example the least rmse of 24 24 is observed when the rmse is calibrated however the rmse when nse is calibrated is almost equal rmse 24 34 this is due to the fact that the nse and rmse are indeed directly correlated since the nse is the ratio of the mean square error mse and the variance of the observations eq b 2 the latter is a constant while the mse is directly correlated with the rmse this means if the rmse is minimized the nse is implicitly optimized clear differences in the optimal hydrographs can be detected when using different metrics for calibration the calibration using kge fig 8b is yielding better matching simulations during the high flows in april and may compared to the results obtained using nse and rmse using the nse for the log transformed discharge data fig 8d is putting more emphasis on the low flow periods in winter december to march and summer july to october but still achieving a decent magnitude for the freshet high flows this demonstrates again the impact a transformation of data here q vs logq might have on the calibration results as it was discussed in section 2 3 using the squared correlation coefficient fig 8e as a metric is leading to a wide spread of the ten calibration trials with the best trial being the hydrograph consistently underestimating discharge especially during the high flow period in april to june and october to november it shows that the use of the correlation coefficient is focusing on matching general patterns in the data while not actually focusing on the nominal values themselves this is a beneficial feature when knowing that the model is simulating proxies of the available observations but not the nominal values themselves for example most hydrologic models do not explicitly simulate soil moisture but only saturation or soil water storage in a conceptual soil layer where depth is not explicitly modeled hence the model cannot be compared to observed soil moisture directly as their magnitudes are different but it can be expected that their dynamics are similar and hence the squared correlation can be envisaged as objective function the squared correlations calibrating rmse and nse are quite similar both r 2 0 73 to the optimal value achieved by calibrating r 2 directly r 2 0 74 while the former two i e rmse and nse fits would certainly be picked as a better fit by any hydrologist looking for an overall good fit while the latter i e r 2 fit might be chosen as the best fit when low flow values are most important to be modeled calibrating the percent bias fig 8f is exclusively focusing on the overall water budget applications for metrics like this include for example setups for long term hydropower reservoir planning the calibrated hydrograph however would likely not be picked as a good fit of the observed data in most applications the experiment presented here shows that a poor model fit might not be caused by an inappropriate model or calibration but solely be due to the choice of the objective function a critical evaluation of whether a commonly used objective function is emulating the actual objective of the modeling purpose is key in some cases it might be necessary to translate the needs into a user specific objective function or use model signatures to achieve a robust model assessment assuring that the model leads to good results for the right reasons 2 8 choose the right calibration algorithm a wide variety of different algorithms is available all differing in their strategy to suggest the next parameter set to be tested based on previously tested parameter sets and their performance some algorithms focus on achieving best results for a low number of model evaluations while others focus on convergence of results among other features the most used algorithms to calibrate environmental models are heuristic algorithms that do not rely on derivatives of the model being available a lot of them are inspired by nature like movement of amoebas e g nelder mead algorithm nelder and mead 1965 collective swarm behavior e g particle swarm optimization kennedy and eberhart 1995 or optimal formation of atoms during the cooling of metals e g simulated annealing kirkpatrick et al 1983 gradient decent methods on the other hand are much less common for environmental models given that they require derivatives to be defined or determined to guide the calibration derivatives of complex numerical models are however usually not known or not straight forward to derive to demonstrate differences and similarities of calibration algorithms three commonly applied algorithms to calibrate hydrologic and other environmental models have been used to find optimal settings for four exemplary models the four problems are to minimize the root mean square error between the four parameter logistic function eq a 5 and log log transformed data to maximize the kling gupta efficiency of a nine parameter hydrologic model when compared to discharge data appendix a 3 and to minimize ackley functions with 10 and 20 degrees of freedom the three heuristic algorithms selected are the dynamically dimensioned search dds appendix c 1 the shuffled complex evolution sce appendix c 2 and the particle swarm optimization pso appendix c 3 the algorithms are tested using three budgets i e 25 n p 50 n p and 100 n p with n p being the number of parameters in the respective model for each budget algorithm and model ten independent calibration trials are performed to assess the random nature of each of these algorithms the results are presented in fig 9 dds is known to be very budget efficient i e achieving good results for very low budgets the algorithm easily finds the optimum with a low budget in low dimensional problems red lines in figs 9a and d the comparison of the ten independent trials shows a larger spread for more high dimensional problems figs 9g and j independent of the budget used it however reliably identifies good solutions when referring to the best of the ten trials colored labels added to panels increasing the budget seems beneficial especially for higher dimensional problems compare for example red and blue lines in figs 9g and j sce is known to be a good choice especially when the problems allow for a large or even unlimited budget in those cases the algorithm can decide itself when to stop which was not achieved in any of the examples and trials presented here the trials are in general more consistent than for dds the best trial however is not as good as the best trial in dds especially for the lower budget compare red and blue colored values added as labels to figs 9a b d e g h and j k sce even seems to consistently converge to the wrong value for high dimensional problems fig 9k the non convergence of the algorithm is however a good indicator that better results could be achieved when given a larger budget pso is comparable to dds for the largest budget 100 n p in terms of the best overall found objective function value across all trials see gray colored labels added to figs 9c f i and l the trials are more consistent compared to dds especially for higher dimensional problems based on a visual inspection of the objective function values e g compare red and blue lines in figs 9i and 9l with corresponding lines in figs 9g and 9j both sce and pso are population based algorithms meaning that an entire ensemble swarm of parameter sets are evaluated and used to propose new candidate parameter sets in promising directions in case of sce this means that several complexes here numcomplexes 2 see table c 3 containing multiple points parameter sets each here numpointspercomplexes 2 n p 1 see table c 3 need to be evaluated before the calibration algorithm starts suggesting the first true candidate parameter set based on the evaluations of these 2 2 n p 1 points in case of n p 10 figs 9g i and n p 20 figs 9j l parameters it requires 42 and 82 model evaluations respectively to just get started while dds already used all these evaluations to explore the parameter space by itself this leads to the situation that population based algorithms such as sce and pso do not show as steep declines in objective function values at the start of the calibration compared to algorithms that are not population based such as dds dds is known to be very well suited for high dimensional problems especially given a small budget as it does not dissipate limited resources in summary the results show that a running and analyzing multiple trials is helpful to evaluate the quality of a calibration b an increase in budget might lead to significant improvements depending on problem and algorithm other algorithmic parameters not analyzed here might lead to improvements as well and c the quality of calibration algorithms differs in terms of e g spread of trials convergence and overall best trial an overall rule of thumb is to find an algorithm that one finds intuitive such that the adjustments of algorithmic parameters feel natural it seems essential to test the algorithm of choice in a few benchmark examples to find settings of the algorithmic parameters and understand the influence of the various algorithmic parameters a list of algorithmic parameters used here is given in table c 3 as a result of testing these algorithms on a wide range of problems in hydrology it is essential to find good settings for different situations such as models with more less parameters fast slow models consistency of multiple trials the sensitivity of the algorithm to initial guesses general convergence behavior of the algorithm and good settings for budget not in all cases the same algorithm might be best it seems beneficial to run two algorithms in parallel if possible at least in a test setting to pick the one that seems most appropriate 2 9 decide if multiple objective need to be calibrated to find model setups satisfying expectations often a good model setup cannot be determined by only one feature e g dataset information objective but needs two or more aspects of the model to be optimal for example the use of soil moisture performance as a second objective besides streamflow performance might be helpful to obtain realistic patterns in other state variables of the model a preliminary sensitivity analysis can be helpful to identify sets of objectives that help constraining parameters responsible for various aspects of the model demirel et al 2018 in cases multiple objectives need to be evaluated simultaneously one needs to employ a multi objective algorithm there is a plethora of algorithms available for example the multi objective complex evolution mocom ua method yapo et al 1998 the nondominated sorting genetic algorithm ii nsga ii deb et al 2002 the multiobjective shuffled complex evolution metropolis moscem algorithm vrugt et al 2003 the multi objective particle swarm optimization mopso approach reddy and kumar 2007 the multiobjective evolutionary annealing simplex meas method efstratiadis and koutsoyiannis 2009 and the pareto archived dynamically dimensioned search padds algorithm asadzadeh and tolson 2013 among others these algorithms have been applied to a wide variety of environmental models seeking optimal parameter setups for two or more objectives e g jahanpour et al 2018 newland et al 2018 ercan and goodall 2016 gong et al 2015 oraei zare et al 2012 shafii and de smedt 2009 for reviews and comparisons of the algorithms the reader is referred to for example zitzler et al 2000 and efstratiadis and koutsoyiannis 2010 the focus of this work is to summarize how to diagnose that a multi objective algorithm is setup sufficiently and results are reliable i e converged two experiments are setup for demonstration purposes in both cases the padds algorithm appendix c 4 is used to find optimal settings for a hydrologic model with n p 9 parameters appendix a 3 allowing for a budget of 300 n p model evaluations in each of the ten independent trials carried out the first experiment is to find optimal parameter settings for low and high flows using the kge of the log transformed discharge for low and high flow time steps eqs b 7 and b 8 respectively the second experiment is using the bias and correlation component of kge eqs b 5 and b 6 respectively as the two objectives to be maximized note that in both experiments the overall set of non dominated solutions forming the pareto front is derived by identifying the non dominated solutions when the pareto fronts of the ten trials are pooled together hence the final pareto front might contain solutions from different trials rather being the best front of the ten trials both experiments further include as a reference the single objective calibration of each of the two objectives independently i e calibrate only objective 1 and calibrate only objective 2 as well as the single objective calibration of the mean of both objectives i e calibrate 0 5 objective 1 objective 2 these single objective calibration experiments are performed using the dds algorithm appendix c 1 with ten trials and a budget of 100 n p each the second not calibrated objective is simply derived in a post processing using the best model setup with regards to the calibrated objective the results are shown in fig 10 the result of first experiment calibrating low and high flows independently fig 10a shows a pareto front blue line bending over a noticeable range kge of 0 4 to 0 7 for log transformed low flows and kge of 0 6 to 0 9 for log transformed high flows the individual fronts of the ten trials gray lines show some spread but are generally in agreement especially for large objective function values for the high flow objective more importantly the single objective references blue markers for best trial are located on the pareto front and are positioned exactly where one would expect them the calibration using only the first objective blue triangle pointing left is located on the right most edge of the pareto front while the calibration using only the second objective blue triangle pointing downwards is located on the upper edge of the pareto front the single objective calibration using the mean of both objectives blue square marker is positioned in between this highlights two points a any single objective calibration result that is derived as a compromise solution between the involved individual objectives is located on the pareto front its position is determined by the weight of the individual objectives when merging them into a single objective i e compromise objective b single objective calibrations are usually easier to setup require fewer model evaluations and are easier to check for convergence compared to multi objective experiments they can hence be used as an indicator whether a multi objective calibration is converged by checking if the single objective results are positioned on the pareto front if there is a gap between the pareto front line and single objective results markers the multi objective calibration is likely not converged yet the second experiment using the two components of the kge i e kge β and kge r equivalent to calibrating percent bias pbias and correlation r is leading to a significantly different result fig 10b the pareto front does not form a curved shape as the previous experiment i e the range where the bend appears is very narrow the single objective results are located mostly on a straight line parallel to the x axis and y axis the differences in objective function values are of numerical nature i e differences appear only in the second or third digit of kge this is highlighted by a zoom of the original front focusing mostly on the bend of the pareto front fig 10c this highlights two points a the location of the single objective results blue markers on the pareto front blue line demonstrates that the single objective and the multi objective calibrations both converged b more importantly the shape of pareto fronts where the bend of the front is only visible in narrow ranges of the objective functions i e like in figs 10b and 10c are called degenerated as they imply that the two objectives are not contrary the model finds settings that optimize both objectives at the same time i e using the model setup located at the bend of the front is optimal none of the points on the straight limbs would be considered as significant improvements can be achieved for one objective with almost no decrease in performance of the other objective the optimal model setup at the bend can be identified either using the less computationally expensive single objective calibration of the mean of both objectives or using the multi objective calibration and identifying the point located on the pointy bend in cases the shape of the pareto front looks degenerated i e large non bending tails of the front it is recommended to revise the sets of objectives used for calibration by reducing them to a set of non redundant objectives each objective is supposed to add an additional constraint independent of the others constraints while degenerated pareto fronts are forming if objectives are redundant correlated e g nse and rmse see section 2 7 the presented example of calibrating low and high flows independently fig 10a is the superior multi objective setup as the two objectives are truly independent and a bending non degenerated pareto front is forming contrary to the simultaneous calibration of percent bias and correlation figs 10b and 10c 2 10 diagnosing calibration performance this last step aims to summarize the previous steps providing a checklist that should be examined after each iteration of a calibration experiment i e one loop of the calibration life cycle shown in fig 1 the checklist contains potential issues that might be identified when analyzing a the convergence of the parameter values over the course of the calibration b the development of the objective function values over the course of the calibration and c the actual fit of the calibrated modeled outputs to the data action items are specified that will help resolving detected issue most issues and ideas to resolve them have been described in the previous sections the individual check points are explained briefly in the following a visual summary of the checklist including brief descriptions of problems and solutions is shown in fig 11 at first the development of parameter values over the course of model evaluations performed during the model calibration can be analyzed as done for example in section 2 6 it is helpful to analyze multiple independent calibration trials as they provide a clearer picture a large spread of the trials fig 11a might indicate either a too wide range of the parameter or a too low budget used for calibration in case other parameters converge to consistent values a large spread between the trials for a parameter indicates however that the parameter might be not inferable given the used data or objective function or the parameter is dependent on the values of several other parameters i e parameter interaction this might be checked with a sensitivity analysis convergence patterns that look very similar for pairs or more parameters indicate that parameters might be correlated not shown in this work parameters that consistently converge to values close to the parameter s boundary fig 11b indicate the necessity to increase the range of the parameter the consistent convergence of all trials towards a value that is not the upper or lower limit fig 11c is the desired behavior as it suggests the identifiability of the parameter and a robust calibration setup another item to check is the development of objective function values over the course of the model evaluations during calibration this is best done also using multiple independent trials for single objective calibration tasks the steepness of curve towards the end of the calibration fig 11d might reveal the necessity to increase the calibration budget if the curve has a clear downward trend instead of plateauing at the end of the calibration the spread of the trials fig 11e indicates the convergence and reliability of the setup a wide spread might be resolved using a larger budget or other algorithmic parameters or revising the parameter ranges the target is to achieve consistent trials that do not spread too much especially at the end of the calibration as well as converging plateauing for iterations at the end of the calibration fig 11f to avoid that this behavior only looks consistent because the algorithm consistently fails a second calibration algorithm could be employed in parallel to check whether both converge to the same value as shown in section 2 8 for multi objective calibration experiments the convergence and consistency of the objective function values is harder to check based on the spread of multiple trials or the plateauing of the objective function one possibility is to check whether the multi objective pareto front is consistent with corresponding single objective calibrations instead as shown in section 2 9 a first check is if the pareto front is degenerated fig 11g which usually indicates that some of the chosen objectives are redundant and one or more can be removed without any loss of information less objectives make any algorithm faster in any case and more reliable if redundant objectives are removed single objective results that are not placed on the pareto front fig 11h indicate that usually the multi objective calibration is not converged yet and the budget should be increased the best case scenario is that the pareto front shows a bend over a significant range of objective function values and corresponding single objective calibration results are placed on the front itself fig 11i showing that the objectives are indeed contrary helping different features of the model to be calibrated and the calibration converged reliably lastly the actual fit of the data with the calibrated model simulation needs to be checked as shown for example in sections 2 6 and 2 7 the task for the modeler is to check objectively if the features of the data are matched by the model as expected figs 11j l if this is not the case the choice of the objective function should be revisited another check is the quality of the fit as well as the spread of independent calibration trials failures to fit the data entirely fig 11m is the worst case scenario and can have multiple reasons it most likely will need to be further diagnosed further using the behavior of parameters and objective function values over the calibration iterations figs 11a c and figs 11d i respectively possible reasons for a failure are that a parameter ranges that are too narrow such that the optimum is not contained b parameter ranges are too wide such that the algorithm does not find the global optimum c the calibration algorithm itself is not properly setup d the input data or output observations have large uncertainties or e the model is just not appropriate to describe the data and needs to be revised independent trials that generally fit well but show a relatively wide spread fig 11n usually indicate that parameter ranges might be too wide the calibration algorithm might therefore have trouble to converge to the same parameter set in all trails increasing the budget or narrowing the ranges depending on how the convergence of objective function and parameters look like might solve this issue a good fit fig 11o is consistent across the trials small spread and fits the data as expected all this of course in combination with parameters converging to consistent values within range and objective function converging consistently this successful verification of i the model fit ii consistently converging parameters and iii consistently converging objective function values could be called the hat trick of model calibration and successfully concludes the endeavor of calibrating an environmental model once the model is successfully calibrated the model setup needs to be tested through validation and evaluation experiments using data and outputs that have not been used during calibration making sure the model gives right answers for the right reasons 3 summary this work describes and illustrates ten strategies to guide model calibration of environmental models the strategies are general enough to cover a wide range of problems and model types the list could certainly be extended but would sacrifice generality the strategies are part of the calibration life cycle and need to be understood as an iterative process to arrive at optimal settings to guarantee a successful model calibration the order they appear in the following list is as they appeared in this work during a true model calibration they will likely not appear in this consecutive order but being swapped augmented by other steps or applied several times before moving on to solving the next challenge the strategies presented in detail in the previous sections can be summarized as follows 1 use of sensitivity analysis to identify parameters to calibrate identifying parameters that are important for a model output i e parameter screening and only calibrating these might make the calibration more efficient especially when the algorithm is not internally using sensitivity information the more important benefit of knowing about the sensitivity importance of parameters is that it identifies parameters that can or cannot be inferred through a calibration using the given model output and data at hand if an insensitive parameter is calibrated the inferred values are purely random and hence should not be transferred and only used with care 2 considering parameters with constraints to allow for efficient sampling of parameter space parameter constraints might lead to model crashes and should be resolved by adjusting sampling strategies wise choices of parameter ranges redefinition of parameters or a revision of the model 3 transformation of data ranging orders of magnitude to find optimal fitting model setup when data range across multiple orders of magnitude e g conductivity transmissivity it might be a wise choice to transform them e g log transform to convert them in a range where data are more equally distributed a good guideline is to pick the transform you would use to check the fit of the model and the data 4 choosing data to use for calibration and validation evaluation choose the data for calibration and validation evaluation in order to make sure the most robust model can be deployed and used for operational purposes using future data 5 choosing appropriate parameter sampling strategy to find optimal model setups random sampling is the most inefficient form of model calibration calibration algorithms are setup to learn from past model evaluations and make an informed choice for the next sample parameter set 6 adjustment of parameter ranges to make sure optimal values are covered while calibration algorithm is still efficient the goal is to find a range for each parameter that is as narrow as possible such that it is easiest for the calibration algorithm to find the optimum but wide enough such that the optimum is contained and calibrated parameters are not located at the bound finding a balance for each parameter range is an iterative process 7 choose the appropriate objective function for the purpose of the model the objective function is summarizing the features you expect your model to fit best regarding the given data if a calibrated model output is not satisfactorily fitting your data it may not be due to the inappropriateness of the model but an insufficient choice of the objective function objective functions are not set in stone you can create your own specifying the feature you want to be fitted best 8 choose the right calibration algorithm the various algorithms show their strengths for different problems ultimately it is more important to find an algorithm you understand and know how to tweak running two algorithms in parallel is a wise choice to gain confidence in results therefore picking a calibration toolbox allowing to easily switch between different algorithms is key 9 decide if multiple objective need to be calibrated to find model setups satisfying expectations technically multiple objectives can be calibrated at the same time however it should be checked that additional objectives add non redundant information i e information that is not already represented by another objective in any case since multi objective calibration problems are harder they should be checked for consistency with the easier problems of calibrating each objective individually 10 diagnosing calibration performance a major part of model calibration is diagnosing the quality of calibration results with respect to parameter values the objective function and the fit of the model to the data the analysis of these three components can be used to guide the revision of the experimental setup of the calibration e g adjust ranges increase calibration budget modify objective function this work is attempting to formalize the process of successfully calibrating an environmental model by providing clear guidance and checklists for people who like checklists like myself the study is not meant as a benchmark of objective functions or calibration algorithms even though model calibration remains to be an art this work attempts to make the process more accessible especially to students and researchers whose primary focus is not model calibration itself but are using model calibration as a tool credit authorship contribution statement juliane mai conceptualization methodology software validation formal analysis writing visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the author would like to thank bryan tolson university of waterloo and luis samaniego helmholtz centre for environmental research for the inspiring discussions about the art of model calibration throughout the years many thanks to the special issue women in hydrology for the opportunity and its focus on amazing female mentors the author wishes to recognize the two anonymous reviewers for their most helpful and encouraging evaluations of the manuscript this research was undertaken thanks in part to the global water futures program funded by the canada first research excellence fund cfref the data and codes are made available on zenodo https doi org 10 5281 zenodo 7563321 appendix a models used three models are used for demonstration purposes in this work two of them are mathematical functions i e the ackley function appendix a 1 and the logistic function appendix a 2 the third model is a hydrologic model called gr4j for one example watershed in british columbia appendix a 3 a 1 ackley test function the ackley function introduced by ackley 1987 in his phd thesis is a common benchmark function in model calibration to test the performance of calibration algorithms tolson and shoemaker 2007 behrangi et al 2008 regis and shoemaker 2009 müller and shoemaker 2014 the global optimum is known to be located at the origin x 0 with a minimal function value of 0 0 the ackley function exists for a variable number of dimensions n which makes it very convenient to test algorithms for low dimensional problems n typically 2 to 5 and high dimensional problems n larger than 10 the function has many local minima which make it difficult for some algorithms to find the global optimum at the origin when calibrating this function no objective function is required as the function value f a c k l e y x 1 x 2 x n itself is used to find the minimum the function is defined as follows a 1 f a c k l e y x 1 x 2 x n a exp b s 1 n exp s 2 n a exp 1 0 a 20 0 b 0 2 c 2 0 π s 1 i 1 n x i 2 s 2 i 1 n cos c x i the range of the n parameters x i i 1 n are set to be the following if not otherwise stated a 2 x i u 2 0 2 0 a 2 logistic function the logistic function is used in this study to demonstrate fitting of data points in this study these data points span multiple orders of magnitude but their general trend is that of a logistic function after its point of inflection the logistic function here used with four parameters is an s shaped function with a wide range of application fields such as biology economics physics social sciences and even sensitivity analysis berkson 1944 long et al 1974 garrick 1995 kooi et al 1998 zimm 2005 cuntz et al 2015 a 3 f x 1 0 l 1 exp k x x 0 s the parameters are the curve s maximum value when not shifted i e s 0 the logistic growth rate or steepness of the curve k the value of the sigmoid s midpoint x 0 and the shift curve along direction of y axis s the ranges of the parameters in the examples presented in this work are set as follows if not otherwise stated a 4 l u 2 0 20 0 k u 1 0 1 0 5 5 0 x 0 u 2 0 2 0 s u 1 0 1 0 5 10 0 this function is also used with log transformed independent variables x ˆ log 10 x the function should then be adjusted to account for this transformation but keeping the ranges of the parameters consistent with the above logistic function a 5 g x ˆ log 10 f x l 1 exp k x ˆ x 0 s with x ˆ log 10 x in this work the root mean square error appendix b 1 is used to find the best fit of the data points using a logistic function a 3 hydrologic model gr4j with cemaneige the gr4j model perrin et al 2003 is a lumped water balance model that relates runoff to rainfall and evapotranspiration using daily data while the model contains two stores and originally has four parameters x 1 to x 4 the gr4j parameter x 1 denotes the production store capacity parameter x 2 is the inter catchment exchange coefficient x 3 is the routing store capacity and x 4 is the unit hydrograph time constant gr4j is used in concert with cemaneige valéry et al 2014 for the handling of snow gr4j and cemaneige are fully emulated within the raven modeling framework craig et al 2020 and used as such in this study the parameters their ranges and where these parameters are located in a raven setup is listed in table a 2 please note that parameter x 5 for an estimate of annual average snow is usually not part of the cemaneige model but was added here as raven requires an estimate and none was at hand two additional parameters for rain snow partitioning were added as only precipitation forcings instead of precipitation separated into rain and snow were available the model is setup to simulate streamflow for the salmon river basin located in the canadian rocky mountains in british columbia draining towards the streamflow gauge station near prince george water survey canada id 08kc001 the catchment is about 4200 km 2 large and almost not impacted by humans a lumped model was setup for the simulation period from january 1 1989 to december 31 2010 while the first two years were discarded as warm up hence 20 years of daily streamflow simulations were used for this study more details about the catchment can be found in mai et al 2020 section 2 1 3 case study domain therein in this work a range of metrics is used to find an optimal setup for this model to match the streamflow observations for the salmon river basin the objective function used primarily is the kling gupta efficiency appendix b 3 but also other metrics to study differences in calibration results due to this choice in single objective as well as multi objective calibration exercises appendix b objective functions used objective functions or metrics are used to objectively quantify the quality of a simulated model outputs y sim by comparing them with observed data y obs the various objective functions used throughout this study are the root mean square error appendix b 1 nash sutcliffe efficiency and some of its derivatives appendix b 2 the kling gupta efficiency and some of its derivatives appendix b 3 the squared pearson correlation appendix b 4 and the percent bias appendix b 5 b 1 root mean square error rmse the root of the mean squared error rmse is defined as b 1 rmse 1 n i 1 n y obs i y sim i 2 0 with n being the number of time points i where both the observations and simulations exist the rmse is usually chosen over the mean square error mse since it has the same unit as the data points themselves the error is hence easier to interpret and set in comparison with the actual measured and modeled variables the optimal value of the rmse and mse is 0 the goal of calibrations using rmse and mse is therefore to minimize the objective function values b 2 nash sutcliffe efficiency nse and some of its derivatives the nash sutcliffe efficiency nse nash and sutcliffe 1970 was likely the most popular metric used in hydrologic applications before the kling gupta efficiency next section was introduced the nse is calculated as one minus the ratio of the mean square error mse divided by the variance of the observed time series σ obs 2 b 2 nse m s e σ obs 2 y obs i y sim i 2 y obs i y obs 2 1 with y obs being the mean of the observed time series all sums are derived over all data points i where both the observation and simulation are available the nse is optimal for its maximum value of 1 and suboptimal for values less than 1 the goal of calibrations using nse or any of its derivatives is to maximize the objective function values b 3 kling gupta efficiency kge and some of its derivatives in this study the kling gupta efficiency kge gupta et al 2009 and its three components α β and r gupta et al 2009 are used for streamflow calibration the component α measures the relative variability of a simulated versus an observed time series e g q t the component β measures the bias of a simulated versus an observed time series while the component r measures the pearson correlation of a simulated versus an observed time series the overall kge is then based on the euclidean distance from its ideal point in the untransformed criteria space and converting the metric to the range of the nash sutcliffe efficiency i e optimal performance results in a kge and nse of 1 and suboptimal behavior is lower than 1 the kling gupta efficiency is defined as follows b 3 kge 1 1 α 2 1 β 2 1 r 2 1 with α σ y sim σ y obs and β y sim y obs while y and σ y are the mean and standard deviation of a time series y either simulated s i m or observed o b s and r being the pearson correlation coefficient to make comparisons easier the range of the kge components is transformed here such that they match the range of the kge this leads to the following component metrics as introduced by mai et al 2022c b 4 kge α 1 1 α 2 1 1 σ y sim σ y obs 2 1 b 5 kge β 1 1 β 2 1 1 y sim y obs 2 1 b 6 kge r 1 1 r 2 1 the kling gupta efficiency can also be derived when the simulated and observed time series are log transformed y ˆ ln y the kge will then be denoted as kge l o g another derivative of the kge is defined by focusing on high flow or low flow values the two metrics are denoted as kge l o g q high and kge l o g q low respectively a time point t is considered in the derivation of kge l o g q low if the observed streamflow for that time step q o b s t satisfies the following conditions b 7 0 0 q o b s t min q o b s 0 05 max q o b s min q o b s a time point t is considered in the derivation of kge l o g q high if the observed streamflow for that time step q o b s t satisfies the following condition b 8 q o b s t min q o b s 0 05 max q o b s min q o b s the metrics kge l o g q low and kge l o g q high are then the kling gupta efficiencies of the log transformed observed and simulated streamflow considering only low flow and high flow time steps respectively please note that the decision if a time step is a low flow or high flow time step is solely based on the observations which means it is always the same time steps for a given basin and time period while being independent of the simulation the goal of calibrations using kge or any of its derivatives is to maximize the objective function values as all range from infinity to 1 with the latter being optimal b 4 squared pearson correlation r 2 the pearson correlation r lies between 1 and 1 with the upper limit indicating a strong positive and the lower limit indicating a strong negative correlation between two time series y sim and y obs the pearson correlation is often squared to find model fits that strongly correlate irrespective of a positive or negative connection in hydrologic applications it is usually hard to obtain a strong negative correlation with an underlying physically based model hence the squared pearson correlation r 2 is more popular the squared pearson correlation is defined as follows b 9 r 2 y sim i y sim y obs i y obs y sim i y sim 2 y obs i y obs 2 2 0 1 where y is the mean of a time series all sums are derived over all data points i where both the observation and simulation are available the optimal value for the squared pearson correlation is 1 the goal of calibrations using r 2 is to maximize the objective function values b 5 percent bias pbias the percent bias is the relative error of the simulation compared to the observations expressed in percent the optimal value is 0 while positive values indicate an average overestimation of the simulation compared to the observed data while a negative percent bias indicates an average underestimation of the simulations compared to observations the percent bias is defined as follows b 10 pbias y sim y obs y obs 100 since the percent bias is optimal at zero and values below or above are not desired usually the absolute or squared percent bias both optimal at zero and non optimal for positive values is optimized for applications in this study the squared percent bias pbias 2 was minimized but the percent bias pbias reported due to its more insightful interpretation of over and underestimation of the simulation appendix c calibration algorithms used the calibration algorithms used in this study are not by any means an exhaustive list of available algorithms they have been chosen because they are popular in hydrologic modeling and they are available in the ostrich calibration toolbox matott 2017 and were hence easy to setup and switch between the different methods the main algorithmic parameters for each algorithm are summarized in table c 3 some of the parameters for example the budget of allowed model evaluations vary between experiments of this study or are dependent on the number of model parameters to be calibrated the rules used to derive the algorithmic parameters are specified in the table three algorithms that calibrated one objective at a time single objective algorithm have been used in this study i e the dynamically dimensioned search algorithm dds appendix c 1 the shuffled complex evolution algorithm sce appendix c 2 and the particle swarm optimization algorithm pso appendix c 3 one algorithm is used to demonstrate aspects of a calibration of multiple objectives simultaneously multi objective algorithm the pareto archived dynamically dimensioned search algorithm padds appendix c 4 is used for this purpose the algorithms are briefly described in the following subsections for further details on implementation the reader is referred to matott 2017 and for algorithmic details and functioning to the publications introducing the individual algorithms c 1 dynamically dimension search dds algorithm the dynamically dimension search dds algorithm introduced by tolson and shoemaker 2007 is a single objective algorithm mostly popular in hydrologic and water resource engineering applications it has been cited 691 435 times based on google scholar web of science as of july 2022 the algorithm is known to be budget efficient meaning it finds good solutions with a comparatively small number of model evaluations required the algorithm is hence often applied for models that are computationally expensive dds has only two algorithmic parameters see table c 3 the first is the perturbation value which is suggested to be fixed at 0 2 in the original publication and set to this value for all experiments the second algorithmic parameter is the allowed budget i e the number of times dds is allowed to evaluate the model using a parameter set it had sampled the algorithm will use all the allowed model evaluations and will not terminate before because the algorithm for example is detecting that it does not find any better solutions and hence determines to be converged the only possibility to check the quality e g convergence of the dds results is by performing multiple independent calibration trials and analyze their spread the budgets used for the various experiments in this study are derived mostly as multiples of the number of parameters e g 25 n p 50 n p and 100 n p with n p being the number of model parameters to calibrate for each experiment ten independent trials were performed c 2 shuffled complex evolution sce algorithm the shuffled complex evolution sce algorithm introduced by duan et al 1993 is another very popular calibration method in hydrology it has been cited 1828 1041 times based on google scholar web of science as of july 2022 the algorithm is available in the ostrich calibration toolkit the algorithm generally requires a larger budget than for example dds it however has an internal measure of determining convergence which means a huge budget can be specified and the algorithm would terminate when necessary if the convergence criterion is not fulfilled by the end of the maximum number of allowed model evaluations and the algorithm aborts because of this upper limit being reached a possibility to check for the quality convergence of the results is to analyze the spread of the results of independent trials as done for dds for most examples shown in this study sce did not converge by itself the budgets were chosen depending on the number of model parameters n p to calibrate in the various experiments usually 25 n p 50 n p and 100 n p for each trial in total ten independent trials were performed for each experiment the algorithmic parameters used for sce are listed in table c 3 several of them are chosen depending on the number of model parameters n p the parameters are set to those values as they were suggested in the original publication duan et al 1993 and follow up publications duan et al 1994 behrangi et al 2008 c 3 particle swarm optimization pso algorithm the algorithm of particle swarm optimization pso was introduced by kennedy and eberhart 1995 in a conference paper and later in more detail in kennedy et al 2001 it is likely one of the most cited algorithms with 73 328 on google scholar the algorithm is available in the ostrich calibration toolkit and the algorithmic parameters used in this study are listed in table c 3 several of those parameters again are chosen depending on the number of model parameters n p to be calibrated the budget for the pso algorithm is not explicitly specified in ostrich but is the number of generations increased by 1 times the swarm size the number of generations was chosen such that the budget was as close as possible to pre defined budgets such as 25 n p 50 n p and 100 n p for each trial in total ten independent trials were performed for each experiment the algorithm has an internal convergence criterion it would use to determine to abort the calibration even if the maximal number of model evaluations has not been reached yet like sce almost no experiment performed in this study however reached that stage almost all experiments were determined because the budget was reached like dds and sce independent calibration trials were used to determine the quality convergence of the algorithm c 4 pareto archived dynamically dimensioned search padds algorithm the pareto archived dynamically dimensioned search padds algorithm is used in this study as an example for a multi objective calibration algorithm it was introduced by asadzadeh and tolson 2013 google scholar cited by 65 web of science 43 as of july 2022 using the method of hypervolume contributions to determine the next candidate parameter set to evaluate the model with while asadzadeh et al 2014 google scholar cited by 37 web of science 23 as of july 2022 proposed convex hull contributions as a superior but computationally more expensive method to determine candidate parameter sets the first method is available in ostrich and has been used in this study the other algorithmic parameters are the same as for dds all can be found in table c 3 like dds padds is known to be budget efficient making efficient use of the specified budget the budget of allowed model evaluations is set to be 300 n p which will be used completely by padds no pre emption of the algorithm through convergence criteria is implemented the quality convergence of the calibration experiments is determined using ten independent calibration trials in all experiments the overall best solution pareto front is determined by merging the pareto fronts of all trials to one final front for a more detailed and visual explanation of pareto fronts the reader is referred to the supplementary material of mai et al 2022c sect s2 and fig s2 therein appendix d pie share sampling the pie share sampling method and some applications were introduced in mai et al 2022a therein the parameters x i are assumed to sum up to 1 with noting that this assumption can be relaxed but not providing the generalized sampling scheme for this the more general description is x 1 c s n r 1 x 2 c x 1 s n 1 r 2 x 3 c x 1 x 2 s n 2 r 3 d 1 x j c i 1 j 1 x i s n j 1 r j x n 1 c i 1 n x i with s n r 1 1 r 1 n r i u 0 1 i 1 n with r i i 1 n being n uniformly distributed parameters that are transformed into model parameters x i the method can be used a when n 1 parameters x i must sum up to a certain value c or b when the sum of n parameters x i is not allowed to exceed a threshold c 
2031,understanding the change in sediment load at a flood event scale helps to enhance channel regulation and optimize reservoir operation this is particularly true in the upper yangtze river basin where a few flood events contribute most of the annual sediment load this paper collected field data from representative hydrological stations and applied statistical analysis to investigate the change in runoff and sediment load during flood events in the upper yangtze river including both the mainstream jinsha river and its major tributaries min tuo jialing and wu rivers the results show that flood events contribute more sediment load than runoff volume to the annual statistics a general decreasing trend occurs in sediment load in all river basins which is closely related to soil conservation efforts and dam construction the series of sediment load modulus of each stations present abrupt change the variations in sediment load modulus in low flow are more significantly than those in high flow at different abrupt change period indicating high flows during flood event have been found to still carry the same amount of sediment as their predecessors in the upper yangtze river an abnormal increase in sediment load in recent years 2013 2020 has been observed at the fushun station in tuo river and three factors i e earthquake high intensity rainstorm and local scouring were identified to be responsible for such increase the findings of our study are helpful for better management of basin scale sediment resources and regulation of reservoirs in the upper yangtze river keywords sediment load flood event wenchuan earthquake sediment carrying capacity three gorges reservoir data availability the authors do not have permission to share data 1 introduction rivers link continents and oceans with ceaseless transport of sediment and other nutrients to sustain various dynamic geomorphological processes yang et al 2014 vercruysse et al 2017 dai et al 2018 tsyplenkov et al 2021 in recent years the sediment load has changed significantly in various rivers under the influence of climate change and human activities misset et al 2019 binh et al 2020 potemkina and potemkin 2021 climate change has been reported to affect hydrological cycle in snow precipitation dominated regions das et al 2022 zhang et al 2022 intensified precipitations have triggered more frequent floods and increased sediment load in rivers li et al 2018 ran et al 2020 human activities such as dam construction best 2019 syvitski et al 2022 and land use change li et al 2020 sam and khoi 2022 also have profoundly impacted runoff and sediment load in particular dam building accounts for most of the sediment reduction in large rivers disrupting the natural process in riverine environment walling and fang 2003 guo et al 2020 the influence of land use on sediment yield is diverse e g increased vegetation coverage reduces sediment yield wang et al 2016 zhao et al 2022 while road construction and mineral mining accelerates soil erosion lu and higgitt 1998 liu et al 2019 understanding the variation in runoff and sediment load is of great significance for better management of river basins shao et al 2021 li et al 2021 das et al 2022 quantification of the changes in runoff and sediment load generally involves average over an annual monthly or flood event scale ran et al 2020 gao et al 2021 the use of flood event scale proves to be particularly important for river basins where rainstorms dominate precipitation and runoff e g the thames river bussi et al 2017 the celone river de girolamo et al 2015 the xihe river liu et al 2021 the carapelle river fortesa et al 2021 the yanhe river gu et al 2021 the kortuter river cakmak et al 2021 the yellow river zheng et al 2020 three major characteristic features of flood events have been recognized by previous studies firstly flood events account for most of the annual sediment load e g high flows have been observed to convey 94 of the total suspended load in the celone river basin de girolamo et al 2015 secondly flood events within a river basin may exhibit great fluctuations under similar discharges e g the first flood carries up to twice the sediment compared with a subsequent flood event in thames river bussi et al 2017 thirdly sediment transport in a flood event is closely related to land use fortesa et al 2021 and soil conservation efforts gu et al 2021 the mainstream of the yangtze river originates from the qinghai tibet plateau known as the third pole of earth and the water tower of asia li et al 2018 li et al 2021 recent studies have revealed increased sediment loads in the headwater region of yangtze river mainly due to climate change and glacier permafrost disturbances li et al 2022 and reduced sediment in the upper middle lower reaches of yangtze largely driven by reservoirs guo et al 2018 guo et al 2020 change in sediment transport in the upper yangtze river also have drawn much attention in the past decades due partly to the construction and operation of the three gorges reservoir tgr as well as other upstream water conservancy projects from its headwater to the city of yichang the upper yangtze river constitutes a major source of sediment yield for the entire river basin chen and wang 2019 sun et al 2022a b and temporal and spatial complexities in hydrology meteorology and geology result in extremely uneven runoff and sediment load in both the mainstream and tributaries wei et al 2014 cheng et al 2019 in recent years a drastic drop has been identified in sediment load from the upper yangtze river yang et al 2018 guo et al 2020 the decrease is attributed to sediment trapping by reservoirs tan et al 2019 peng et al 2020 especially large sized reservoirs as xiangjiaba and xiluodu reservoirs yang et al 2014 li et al 2018 the temporal and spatial variations of runoff and sediment load affect not only flowing condition of runoff and sediment into the tgr but the stability of river channel in the middle and lower yangtze river yang et al 2014 ren et al 2021 yan et al 2021 reported that reservoirs in the jinsha and jialing rivers are responsible for 70 of the reduction in sediment load the flood season from july to september contributes more than 90 of the annual sediment load in the upper yangtze river liu et al 2022 and human activities have been found to exert more influence on sediment load in flood seasons than in dry seasons xu et al 2021 previous studies have enhanced our understanding of the characteristics of sediment transport in the upper yangtze river on an annual or monthly basis global warming accelerates the hydrological cycle which changes the temporal and spatial characteristics of precipitation thereby leading to frequent extreme flood events and affecting the sediment transport capacity in rivers huntington 2006 in recent years therefore the use of flood event scale to investigate sediment load becomes more and more urgent on the one hand extreme flood events have been seen in recent years and their role in sediment transport awaits in depth explanation for instance a flood with record high concentration occurred in mid july 2018 in the tuo river accounting for 71 of the annual sediment load and a surprising 21 of all the annual sediment into tgr in previous studies the contribution by the tuo river was often ignored the reason for the abrupt increase in sediment load of the tuo river deserves to be studied in detail on the other hand the high intensity sediment transport during flood event influences in sediment deposition along the tgr liu et al 2022 the optimal operation of the tgr relies on detailed information of the flood itself e g the asynchronous propagation of sand peaks during a flooding process wang et al 2011 ren et al 2020 this paper aims to provide new insight into the characteristics of runoff and sediment load in the upper yangtze river by adopting a flood event scale based on measured data from representative hydrological stations a correlation is established between sediment load modulus and runoff erosion power then the variation in the power function is quantified and compared among the mainstream upper yangtze river and its major tributaries finally the abrupt increase reasons in sediment load in tuo and fu rivers in recent years are examined 2 study region the study region includes the mainstream of the upper yangtze river and its four major tributaries fig 1 a the yangtze river originates in melt snow on the slopes of the tanggula mountains flows across the tibet plateau highlands and then descends to a winding course with gorges and deep valleys this winding stretch is called the jinsha river until it receives the water from the min river and gets its official name of yangtze river the upper yangtze river is then joined by the other three major tributaries i e the tuo river the jialing river and the wu river before dumping into the tgr with the dam at the city of yichang the dividing point between the upper and middle reaches of the yangtze river from its headwater to yichang the upper yangtze river stretches approximately 4300 km and drains an area about 100 104 km2 the watershed areas are 458800 132926 23283 156142 and 83035 km2 for jinsha min tuo jialing and wu rivers respectively in history approximately 35 of the upper yangtze river basin suffered from high intensity soil loss the most affected regions with sediment modulus greater than 2000 t km2 included the lower reach of jinsha river and the upper reach of jialing river basins covering a total area of 5 104 km2 xu et al 2004 the upper yangtze river basin witnessed an average annual surface soil erosion of 16 108 t and a ratio of sediment load to sediment yield ranging between 0 1 and 0 6 large scale conservation efforts have been carried out since the 1980 s and an overall 35 reduction of area was achieved in region with high intensity soil erosion cwrc 2019 at the same time increase in sediment load has been observed in some regions which were hit by recent earthquakes e g the wenchuan earthquake may 12 2008 the ya an earthquake april 20 2013 and the jiuzhaigou earthquake august 8 2017 these regions are centered at the longmen mountain fault zone wang et al 2015 rapid economic growth has prompted comprehensive watershed development in the upper yangtze river in the last decades yang et al 2014 liu et al 2022 a total number of 14 594 reservoirs were built in 1950 2018 reaching a combined storage capacity of 1688 3 108 m3 83 of which is from 98 large sized reservoirs more information of the reservoirs is given in table 1 the distributions of typical large reservoirs in the jinsha min jialing and wu rivers are shown in fig 1b 3 data sources daily flow discharge and sediment concentration were recorded at hydrological stations that monitor the mainstream of the upper yangtze and its major tributaries the hydrological stations at the mainstream are the xiangjiaba station controls the jinsha river the zhutuo station controls jinsha min and tuo rivers and the cuntan station controls jinsha min tuo and jialing rivers the stations for four tributaries include gaochang station controls the min river fushun station controls the tuo river beibei station controls the jialing river and wulong station controls the wu river to further explore the reasons for recent increase in sediment yield in tuo and jialing river basins we also collected data from the zhenjiangguan station at the upper min river and three more stations at jialing river i e luoduxi station at the qu river a tributary on the left part of jialing river basin xiaoheba station at the fu river a tributary on the right part of jialing river basin and wusheng station at the mainstream of jialing river before the qu and fu rivers join the mainstream jialing river in addition suspended sediment grain size distribution and cross sectional topography in 2018 at fushun and xiaoheba stations were collected the data span a relatively long period of time i e 68 years at xiangjiaba and fushun stations 70 years at gaochang station 66 years at beibei and cuntan stations 62 years at wulong and zhutuo stations covering both wet years e g 1981 2013 2018 and 2020 and dry years e g 2002 and 2012 all the data were provided by the changjiang water resource commission cwrc except those from the zhenjiangguan station which is run by the hydrological bureau of sichuan province hbsp basic information of the collected data is shown in table 2 and the locations of each hydrological station are shown in fig 1a 4 methods 4 1 identification of flood event and calculation of its runoff volume and sediment load we used two criteria to identify a flood event i e i the peak flow discharge of a flood event should be greater than the average flow in flood season and ii flow discharge and sediment concentration in a flood event has a complete rising and receding process wang et al 2022 the runoff and sediment load in each flood event can be calculated with eqs 1 and 2 1 rv δ t i 1 n q i 10 8 2 sl δ t i 1 n q i ssc i 10 7 where rv is the runoff volume 108 m3 qi is the average flow discharge on the ith day m3 s sl is the sediment load 104 t ssci is the average sediment concentration in the ith day kg m3 n is the duration in days of the flood event and δt is the length of the day in seconds i e δt 24 3600 s 86400 s to facilitate comparison the sediment load modulus is scaled with the catchment area a in km2 as follows 3 w m sl a 10 4 where wm is sediment load modulus t km2 in the following sections we will use wm to refer to sediment load modulus unless specified otherwise 4 2 calculation of runoff erosion power the erosion power of a given flood event e is defined as the product of runoff depth and peak flow discharge divided by the catchment area 4 e h q max a where e is the flow erosion power mm m3 s km2 qmax is the peak discharge m3 s h is runoff depth mm a is catchment area km2 on an annual scale the average h proves to be a good parameter to represent the driving force of the flow liu et al 2008 yang et al 2014 on a flood event scale however the influence of rainfall intensity should also be taken into consideration wischmeier 1959 lu et al 2008 polyakov et al 2010 the runoff erosion power which accounts for the effect of raindrop splashing on soil erosion and transport has been recognized by previous studies to be a better parameter for correlating sediment and flow lu et al 2009 zhang et al 2016 wang et al 2018 cheng et al 2021 4 3 trend analysis we used the non parametric mann kendall m k test mann 1945 kendall 1975 for detecting a general trend within a time series of hydrological variables for a time series x x1 x2 xn the standardized test statistic z is given as 5 z s 1 v a r s s 0 0 s 0 s 1 var s s 0 in which 6 s i 1 n 1 j i 1 n sgn x j x i 1 x i x j 0 x i x j 1 x i x j 7 v a r s n n 1 2 n 5 i 1 q t i t i 1 2 t i 5 18 where s is the test statistic value xi and xj are the data values in time series i and j j i respectively n is the number of data points ti is the number of time points i and q is the number of groups a positive negative z value indicates an upward downward trend at 5 significance level the null hypothesis of no presence of trend was rejected if z greater than1 96 4 4 change point analysis the change point analysis involves identification of an abrupt change point followed by a significance test the sequential cluster method has been widely applied to identify change points in a hydrological series yang et al 2019 peng et al 2020 for a given series xi i 1 2 n with a possible abrupt change at τ this method examines the sum of the squares of deviations sn τ which is calculated as follows 8 sn τ i 1 τ x i x τ 2 i τ 1 n x i x n τ 2 where x τ and x n τ are the mean values of the data series before and after τ respectively a change point τ is determined when sn τ reaches minimum the rank sum test wilcoxon 1945 is one of the most common methods to test the significance of an abrupt change first proposed by wilcoxon in 1945 it is now widely used in hydrology wang et al 1997 fulton et al 2010 hao et al 2016 shi et al 2017 liu et al 2019 the detailed principle and application of the method can be found in rosner et al 2003 and ott and longnecker 2008 for a given series xi i 1 2 n with a change point of xτ this method introduces ranking of data from small to large within the entire sequence i e the smallest point ranks first and the largest point ranks the nth then the ranked series is divided into two groups by xτ with n 1 samples less than xτ and n 2 samples greater than xτ the sum of rank for the group with smaller capacity is denoted as w then a statistical variable is defined as follows 9 u w n 1 n 1 n 2 1 2 n 1 n 2 n 1 n 2 1 12 for a given significance level of 0 05 u greater than1 96 indicates a significant change point in the present study we used the sequential cluster method and the rank sum test for change point analysis 4 5 double mass curve analysis the change in the relation between runoff erosion power e and sediment load modulus wm was investigated with the double mass curve dmc method zhou et al 2020 kar and sarkar 2021 in an e abscissa wm ordinate curve a downward upward shift indicates a systematic decrease increase in wm under the same value of e the dmc analysis can also be used to further examine the rationality of change points 4 6 relation between e and wm for a specific station the relation between annual e and sediment load generally follows a power function walling 1977 lu et al 2009 zhang et al 2016 similarly we used a power function to correlate e and wm as follows 10 w m α e β where α and β are parameters to be determined specifically α indicates sufficiency of sediment supply while β reflects the flow capacity to carry sediment a river basin with loess produces a larger α than a region covered mostly by rock a reduced α will be observed after dam construction and soil conservation efforts the range of β grows wider when sediment trapping efforts are in operation for a specific watershed both α and β reflect the influence of human activities on sediment load 5 results 5 1 basic characteristics of runoff and sediment load during flood events some basic statistics of the flood events are given in table 3 on average the annual number of occurrence of flood events varies from three xiangjiaba station at the jinsha river to seven beibei station at the jialing river most of the floods occur in the flood season from june to september the occurrence of flood is closely related to or even dominated by rainstorms wang et al 2011 in the wu river basin wulong station for instance high rainfall comes much earlier than in other areas ye et al 2020 so june ranks the first in terms of annual maximum flood occurrence 40 while other areas witness most of the flood in july august or september with a percentage of 91 96 97 87 98 and 97 in above period for xiangjiaba gaochang fushun beibei zhutuo and cuntan stations respectively the difference in annual number of flood occurrence among tributaries can also be attributed to the spatial heterogeneity in frequency and intensity of rainstorm in the upper yangtze river according to wang and xing 2019 the wu river and lower jinsha river basins are characterized by low frequency rainstorm events while the min tuo and jialing rivers see more high frequency and high intensity rainstorms the floods play a vital role in contribution to annual runoff and sediment load the flood events account for 27 64 of the total annual runoff and the contribution to sediment load is more pronounced ranging from 64 to 99 5 2 trend and change point in runoff and sediment load 5 2 1 trend analysis as shown in fig 2 the annual variation of wm rv and e present decreasing trend of each station and the results of the m k trend analysis for wm rv and e are summarized in table 4 at a significance level of 0 05 neither the rv nor the e shows obvious change in trend in contrast a significant decreasing trend is recognized for wm such trend for flood events is consistent with the results based on the annual scale chen and wang 2019 peng et al 2020 yan et al 2021 5 2 2 change point analysis no significant change points present in the variations of runoff volume and flow erosion power however significant change points in sediment load have been identified for every station table 4 i e in 1999 and 2013 at xiangjiaba 2008 at gaochang 1985 and 2013 at fushun 1984 and 1998 at beibei 1992 and 2008 at wulong and 1999 and 2013 at both zhutuo and cuntan stations it is worth noting that two abrupt change points identified through the sequential cluster method have been deemed to be insignificant by the rank sum test decrease in 1995 at gaochang and increase in 2013 at beibei station the dmc results of e wm are depicted in fig 3 also included in fig 3 are the change points which highlight different periods generally it s believed that the abrupt changes of sediment load are caused by the operation of reservoir li et al 2018 wei et al 2020 in all stations except fushun station the abrupt change of wm during flood event in the upper yangtze river coincides with the operation of large dams in the upstream reach two break points are noticed for the xiangjiaba zhutuo and cuntan stations which are on the mainstream of the yangtze river the first in 1999 and the second in 2013 the first break is believed to be associated with the impoundment of the ertan reservoir in 1998 the dam traps most of sediment load from the yalong river a tributary of the jinsha river and a major contributor of sediment to the upper yangtze river the second break follows the damming of the jinsha river in 2012 by two giant projects the xiluodu and xiangjiaba reservoirs the break points downward for other stations are also attributed to dam construction for instance the decrease in 2008 at the gaochang station is due to the operation of the zipingpu reservoir in the min river lyv et al 2020 at the beibei station the two break points 1984 and 1998 are caused by the operation of the shengzhong reservoir in 1983 and the baozhusi reservoir in 1998 respectively wei et al 2014 zhou et al 2020 the two significant change points at the wulong station are associated with the impoundment of the wujiangdu reservoir in 1984 and the pengshui reservoir in 2008 respectively chen et al 2019 ye et al 2020 when runoff remains relatively stable dam construction is believed to be the major factor responsible for abrupt change in sediment load yang et al 2014 the lower jinsha and upper wu and jialing river basins which are notoriously prone to erosion have been enjoying large scale water and soil conservation efforts i e the changzhi project and the tianbao project since 1989 zhang and wen 2004 yang et al 2018 as a result an overall of 34 decrease in area subject to severe soil erosion was achieved from 35 104 m2 in the 1980s down to 23 104 m2 in 2018 cwrc 2019 however the effect of soil conservation efforts on sediment yield tends to be gradual insufficient to cause an abrupt change in the sediment load yang et al 2014 in contrast the construction of dams especially large sized ones leads to immediate trapping of most sediment in the reservoir huang et al 2013 binh et al 2020 yang et al 2014 reported that compared to soil and water conservation projects dams play a far more important role in sediment load reduction it is fact that the fushun station tuo river with a downward turn in 1985 experienced a unique rebound in 2013 while the 1985 downward break is attributed to impoundment of reservoirs the surprising upward break in 2013 still awaits further explanation this will be addressed in the discussion section 5 3 relation between e and wm 5 3 1 comparison of change among different periods fig 4 depicts the e wm relations of each flood event for all the stations the data were grouped into different periods by the change points at each station strong positive correlation between e and wm are obtained for all stations the fushun station tuo river shows the maximum value of r2 as 0 90 for the period of 2013 2020 while the beibei station jialing river presents the minimum r2 as 0 71 for the period of 1955 1983 thus on flood event scale the runoff erosion power proves to be a good variable closely related to sediment load in addition two prominent features can be seen from fig 4 and table 5 firstly the e wm plot exhibits a persistent drop at all stations except the fushun station this is illustrated by a closer look at the xiangjiaba station the first period 1953 1998 p1 witnesses the highest e wm profile the third period 2013 2020 p3 has the lowest profile while the second period 1999 2012 p2 lies in between though the drop from p2 to p3 is far more drastic than that from p1 to p2 the feature is accompanied by a decrease in α e g from 70 01 p1 to 56 32 p2 and to 0 56 p3 the drop in e wm indicates a reduction in sediment load on a flood event basis secondly the e wm plots of various periods tend to converge as e grows higher this is further illustrated by taking the wulong station as an example the plots separate at low e and converge at about e 90 mm m3 s km2 the steepness of the plot indicator of the magnitude of β increases from 0 79 p1 to 0 99 p2 and 1 34 p3 the converging characteristics can be seen for all the stations the lines at xiangjiaba station seems parallel at first look but a closer scrutiny of β reveals that they do converge i e β 0 72 p1 0 78 p2 and 1 02 p3 this new finding is of practical importance in reminding the sediment research community that though a general reduction in sediment load is established high flows have been found to still carry the same amount of sediment as their predecessors as mentioned earlier α and β reflect sediment supply to the stream and carrying capacity of the flow within the stream both parameters are sensitive to soil conservation efforts and dam construction although soil and water conservation projects are incapable of leading to abrupt change in sediment load they do exert a long term impact by enhancing vegetation coverage and reducing sediment yield e g responsible for 23 of the reduction in sediment load in the yangtze river basin during the period of 1989 2015 peng et al 2020 large dams play a more important role in sediment trapping for instance the xiluodu and xiangjiaba reservoirs with respective storage capacities of 13 7 108 m3 and 5 2 108 m3 successfully intercepted most of the sediment from the jinsha river lu et al 2019 yan et al 2021 the drastic reduction in sediment load at the xiangjiaba station will continue with recent operation of the wudongde and baihetan reservoirs compared with large and medium sized dams small dams exert multifaceted influence on sediment load on the one hand they intercept sediment at low to medium flows thus reducing the sediment flux to downstream stations on the other hand due to limited storage capacity they have to release previously deposited sediment out of the dam during high flows either passively or planned and as a result the downstream station receives increased sediment load due to the combined effects of soil conservation and dam construction a general decrease in α and an increase in β are observed for all stations except fushun station however the variation of α and β displays marked difference among these stations addressing such difference requires future study based on more detailed information which is out the scope of the present study the fushun station tuo river presents itself as an outlier in that an increase in sediment load is observed in recent years 2013 2020 it is the only station that witnesses such an abnormal increase in the first and second periods α 11 21 p1 2 91 p2 and β 0 89 p1 1 21 p2 in the third period α increases to 5 88 and β drops to 1 07 this unusual change in recent years is plausibly attributed to the influence of earthquakes as will be established in the discussion 5 4 2 comparison of the e wm relation in the first period p1 the first period p1 is used as benchmark when human activities exerted relatively light impact on sediment load miao et al 2011 shi et al 2017 fig 5 shows the e wm relation for all the stations except cuntan and zhutuo stations it is seen that the xiangjiaba station and the beibei station are higher in their plots of e wm than other stations this points to the fact the jinsha and jialing river basins are two major sources of sediment yield in the upper yangtze river yang et al 2014 liu et al 2022 the middle and lower reaches of jinsha river are characterized by hot and dry valleys which are susceptible to severe soil erosion liu et al 2019 the upper jialing river located in the south of the qinling mountain is also prone to strong erosion due to lack of vegetation shao et al 2021 in contrast the min and tuo rivers enjoy favorable land cover so the plots of e wm at gaochang and fushun stations are lower the wu river basin abounds in karst area resulting in the lowest e wm profile at the wulong station 6 discussions 6 1 comparison of change in wm while a qualitative description of the change in wm already has been given in fig 4 it is of practical importance to get a quantitative comparison of p3 p2 for fushun station against its benchmark p1 this is achieved by averaging the difference of wm between the two lines over the entire e interval see fig 4 to facilitate comparison the data are normalized by the benchmark values taking xiangjiaba station as an example the variation range of e is 0 1 4 6 mm m3 s km2 and the whole reduction amplitude δa can be calculated as follows 11 δ a 0 1 4 6 70 01 e 0 72 0 56 e 1 02 70 01 e 0 72 d e 100 then the average amplitude x at xiangjiaba station is as follows 12 x δ a 4 6 0 1 99 the results show that a 99 reduction is observed at the xiangjiaba station the reduction at gaochang fushun beibei and wulong stations are 46 23 69 59 respectively the strikingly high reduction in wm at xiangjiaba station is attributed to large scale dam construction in the upstream reaches the fushun station tuo river witnesses a much lower reduction 23 as only low head dams are in operation in the river basin 6 2 possible reasons for the recent increase in sediment load in the tuo and jialing rivers the fushun station tuo river witnessed an abrupt increase in sediment load in 2013 the jialing river also experienced increase in sediment load in recent years though no significant change points have been identified as the reported by li et al 2022 the increasing sediment loads in the headwater basin of yangtze river are mainly due to climate change and glacier permafrost disturbances nevertheless the increase in sediment in tuo and jialing river can be attributed to three possible factors i e earthquakes high intensity rainfall and local scouring during flood events 6 2 1 earthquake large earthquake could cause substantial landslides which are the predominate source of basin sediment yield chousianitis et al 2016 xie et al 2022 for example lin et al 2012 found that taitung earthquke caused the sharp increase in sediment load in luye catchment the tuo min and fu rivers run through the longmen mountain fault zone which was hit by frequent earthquakes in recent years e g may 12 2008 wenchuan earthquake april 20 2013 ya an earthquake and august 8 2017 jiuzhaigou earthquake according to china seismological bureau the wenchuan earthquake shook an irregular elliptical area of 440442 km2 with vi intensity or higher the areas of xi intensity x intensity ix intensity viii intensity and vii intensity are 2419 km2 3144 km2 7738 km2 27786 km2 and 84449 km2 triggering more than 56 000 geological hazards fig 6 a chen et al 2011 reported that landslides and collapses caused by the wenchuan earthquake were mainly distributed in the vii intensity region i e the upper reaches of the min tuo and fu rivers fig 6a landslides and debris flow fig 6b and fig 6c provide abundant sediment supplements ding et al 2014 li et al 2016 amounting to approximately 83 108 m3 in the upper basins of min tuo and fu rivers cheng et al 2010 it is estimated that transporting the sediment requires more than thirty years for fine particles less than 25 mm and up to 1000 years for coarse particles wang et al 2015 rainstorms hit the upper reaches of the min tuo and fu rivers in 2013 2018 and 2020 causing loose sediment from the hillside to pour into the river stream and thus enhancing downstream sediment concentration this increase in sediment load can be partly illustrated by a comparison of two typical flood events one in 2003 august 29 to september 3 and the other in 2018 june 25 to june 30 the two flood events are similar in duration about 6 days peak discharge 6080 versus 6230 in m3 s and runoff volume 15 versus 13 in 108 m3 but the 2018 flood carries two times more sediment than its 2003 counterpart i e 451 versus 239 in 104t this sharp difference can only be attributed to the influence of the earthquake as other conditions remain similar table 6 shows statistics of annual runoff and sediment load for two periods i e 1999 2012 and 2013 2020 in addition to fushun beibei and gaochang stations four more stations are included i e the xiaoheba station at the fu river major tributary of jialing river the luoduxi station at the qu river major tributary of jialing river and the wusheng station at the mainstream of jialing river in the upstream of beibei station and the zhenjiangguan station at the upper reaches of min river among all the seven stations three of them fushun xiaoheba and zhenjiangguan stations drain water directly from the earthquake hit regions as expected for these three stations the comparison of the two periods indicates a sharp rise in sediment load with fushun xiaoheba and zhenjiangguan stations witnessing an increase of 404 373 and 87 respectively it is not surprising that an increase of 88 at zhenjiangguan station in the upper min river is companied by a 19 decrease at gaochang station in the lower min river the zipingpu reservoir which lies in between of the two stations is believed to be responsible for such sharp contrast with a storage capacity of 11 1 108 m3 the reservoir has been reported to devour most of the sediment from landslides induced by the wenchuan earthquake you et al 2021 the increase at xiaoheba station is compensated by the simultaneous decrease at luoduxi and wusheng sations so at beibei station a much milder increase in sediment load is observed i e a 25 up from 2716 to 3392 104 t 6 2 2 high intensity precipitation in general higher precipitation would increase runoff and sediment load sun et al 2022a b especially for rivers flood by rainstorm the maximum sediment concentration in rivers is often synchronized with rainstorm ran et al 2020 high intensity rainstorms in 2013 2018 and 2020 act as direct driving agents to increase sediment load fig 7 for instance fig 8 the 2018 storm july 8 15 centered right at the upper tuo and fu rivers respective brought a total precipitation of 300 mm and 210 mm and resulted in a peak discharge of 8800 m3 s at the fushun station and a peak discharge of 16000 m3 s at the xiaoheba station a peak discharge of 28600 m3 s was observed at the beibei station high sediment concentrations were also registered during the rainstorms e g 18 kg m3 return period of 33 years at fushun station and 22 kg m3 at xiaoheba station record high at these two stations the return periods for sediment concentration are greater than those for peak discharge 6 2 3 local scouring local scouring within the rive stream during high flows also plays an important role in sediment transport on the one hand the gates of low head power stations are open during flood to facilitate scour of the sediment previously deposited in the reservoir on the other hand the power stations could be severely damaged or even completely destroyed resulting in large scale bank collapse dong et al 2019 fig 9 shows the cross sectional topographic changes at the fushun and xiaoheba stations before and after the flood season in 2018 fig 10 shows the actual scouring of the riverbed in fu river in 2018 clearly erosion is observed in some parts of the river bed implying that the flow had flushed local sediment to downstream it is significant to know if the stream got saturated in terms of sediment carrying capacity during the 2018 flood this is achieved by a comparison of the measured concentration with the carrying capacity of the flow the ruijin zhang s formula is selected for calculating sediment carrying capacity this formula has been tested repeatedly against abundant data zhang 1998 tan et al 2018 and found wide application in the upper reaches of the yangtze river lu 1998 chen et al 2021 the formula is as follows 13 s k u 3 gr ω m m 14 ω m j 1 n p j ω j where ω m is the average setting velocity of sediment m s n is the group number of sediment particles ω j is the average setting velocity of the jth group m s which can be calculated with the gonzalov formula zhang 1998 p j is the gradation of the jth group r is the hydraulic radius u is the average flow velocity m s and g is the acceleration of gravity equal to 9 8 m s2 here k and m are calibrated with the results by ruijing zhang zhang 1998 tan et al 2018 fig 11 shows a comparison of the sediment carrying capacity with the measured sediment concentration at the fushun and xiaoheba stations during the flood from july 8 to july 15 2018 it is seen that the actual sediment concentration did not arrive at the saturation state at both stations with ratio of the measured sediment concentration to the carrying capacity reaching 0 8 and 0 7 at fushun and xiaoheba stations respectively so it is possible that new record high sediment concentration will be recorded in the future with abundant supply of sediment in the upstream reaches 6 3 influence of flood events on sediment load flowing into the tgr while the runoff remains relatively stable a consistent decrease in sediment load into the tgr has been observed in recent years yang et al 2014 liu et al 2022 yan et al 2022 fig 12 shows the variation in the percentage of annual runoff and sediment load at the xiangjia gaochang fushun beibei and wulong stations relative to the total runoff and sediment load flowing into the tgr in 1960 2020 we adopted the most common method for calculating the runoff and sediment load flowing into the tgr in this paper zhou et al 2016 liu et al 2022 by summing up the three representative stations i e zhutuo beibei and wulong stations before the impoundment of the tgr 1960 2002 the annual runoff at xiangjiaba gaochang fushun beibei and wulong stations accounts for 37 22 3 17 and 13 of the total runoff discharge to tgr while the contributions of sediment load at these stations are 55 10 2 24 and 6 respectively after the impoundment of the tgr 2003 2012 the annual average runoff at these five stations accounts for 38 22 3 18 and 12 of the total runoff into the tgr while the sediment load accounts for 70 15 1 14 and 3 respectively however after the operation of xiangjiaba and xiluodu reservoirs in 2013 the jialing river replaced the jinsha river as the main source of sediment load flowing into the tgr the proportion of sediment load at the xiangjiaba station to that flowing into the tgr decreases to 2 in 2013 2020 while that at beibei and fushun stations increases to 39 and 13 respectively the proportion of runoff at the beibei and fushun stations increases to 17 and 4 respectively and the rate of increase are much lower than that of sediment due to high intensity rainstorms in the tuo and fu river basins in 2013 2018 and 2020 sediment load increased significantly in these years the annual sediment loads at the fushun station in these three years amount to 3600 104 t 2330 104 t and 2100 104 t accounting for 28 16 and 11 of the total sediment load flowing into the tgr respectively and those at the xiaoheba station are 3810 104 t 30 5170 104 t 36 and 7026 104 t 36 respectively during three typical flood events i e from july 8 to july 17 2013 from july 8 to july 15 2018 and from august 15 to august 21 2020 the fushun station receives sediment load of 2750 104 t 41 1436 104 t 20 and 1295 104 t 14 respectively and those at xiaoheba are 2943 104 t 43 3821 104 t 58 and 4754 104 t 49 flood events in the upper reaches of yangtze river exerted a notable impact on the sediment load flowing into the tgr 6 4 application and limitation of this study based on the measured data of runoff discharge and sediment load from hydrological stations in the upper yangtze river this paper systematically analyzed the temporal and spatial variation of sediment load during flood event and possible reason for recent increase of sediment load in tuo and fu rivers this paper presented three important finding firstly recent high flows of each station have been found to still carry the same amount of sediment as their predecessors secondly the occurrence of some special events such as earthquake rainstorm and river channel scouring has resulted in a significant increase in sediment load in tuo and fu rivers which were important reasons for recent increase in sediment load flowing into the three gorges reservoir thirdly the measured sediment concentration of classical flood event in tuo and fu rivers were far less than carrying capacity of flow implying new record high sediment concentration will be recorded in the future with abundant supply of loosen material induced by earthquake the results in this paper provide new insights into changes in sediment transport during the flood event in the upper yangtze river at present the studies concerning sediment transport law during flood event become more and more extensive under the background of global warming ran et al 2020 fortesa et al 2021 the study ideas in this paper also provide reference value for the relevant research of other rivers in the world at present abundant flow sediment data measured in the erosion and deposition balance of riverbed is the basis to calibrate parameters of sediment carrying capacity formula however for most of mountainous rivers such as tuo and fu rivers it is difficult to seek rationality data to determine parameter in sediment carrying capacity formula so this paper used the ruijin zhang s formula zhang 1998 tan et al 2018 a widely used in the upper yangtze river lu 1998 chen et al 2021 to calculate the change of sediment carrying capacity of the flood event in july 2018 in fushun and xiaoheba stations the more suitable method of sediment carrying capacity for the upper yangtze river needs to be further studied 7 conclusions measured data were collected and statistical analysis was conducted to investigate the variation in runoff volume and sediment load during flood events in the upper reaches of the yangtze river the findings in this paper provide new insights concerning the variation of runoff and sediment load in rivers and the results help to better manage flood events in operation of large reservoirs the following conclusions were reached 1 the flood events account for 27 to 64 of the total annual runoff the contributions to sediment load are more pronounced ranging from 64 to 99 2 a general decreasing trend is observed in sediment load during flood events at all river basins due to human activities 3 the relation between flow erosion power e and sediment load modulus wm follows a power function i e wm α eβ and all the stations except fushun witness a consistent decrease in α and increase in β the variation of α and β are closely related to soil conservation efforts and dam construction 4 a significant increase in sediment load has been observed in recent years in the tuo and fu rivers this abnormal increase is attributed to three possible factors i e earthquakes high intensity rainstorms and the local scour of sediment 5 flood events in the upper reaches of yangtze river exert a notable impact on the sediment load flowing into the tgr 6 as the peak sediment concentration at xiaoheba and fushun stations are still lower than sediment carrying capacity of flow it is possible that new record high sediment concentration will be recorded in the future with abundant supply of loosen material induced by earthquake credit authorship contribution statement shangwu liu conceptualization methodology investigation writing original draft dayu wang writing review editing formal analysis wei miao writing review editing formal analysis zhili wang writing review editing formal analysis peng zhang writing original draft danxun li conceptualization validation writing review editing funding acquisition visualization project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china grant no 51879138 and the national key hydraulic engineering construction funds 12630100100020j005 we express our deepest gratitude to the editor and reviewers who have provided valuable insights for revision of this paper 
2031,understanding the change in sediment load at a flood event scale helps to enhance channel regulation and optimize reservoir operation this is particularly true in the upper yangtze river basin where a few flood events contribute most of the annual sediment load this paper collected field data from representative hydrological stations and applied statistical analysis to investigate the change in runoff and sediment load during flood events in the upper yangtze river including both the mainstream jinsha river and its major tributaries min tuo jialing and wu rivers the results show that flood events contribute more sediment load than runoff volume to the annual statistics a general decreasing trend occurs in sediment load in all river basins which is closely related to soil conservation efforts and dam construction the series of sediment load modulus of each stations present abrupt change the variations in sediment load modulus in low flow are more significantly than those in high flow at different abrupt change period indicating high flows during flood event have been found to still carry the same amount of sediment as their predecessors in the upper yangtze river an abnormal increase in sediment load in recent years 2013 2020 has been observed at the fushun station in tuo river and three factors i e earthquake high intensity rainstorm and local scouring were identified to be responsible for such increase the findings of our study are helpful for better management of basin scale sediment resources and regulation of reservoirs in the upper yangtze river keywords sediment load flood event wenchuan earthquake sediment carrying capacity three gorges reservoir data availability the authors do not have permission to share data 1 introduction rivers link continents and oceans with ceaseless transport of sediment and other nutrients to sustain various dynamic geomorphological processes yang et al 2014 vercruysse et al 2017 dai et al 2018 tsyplenkov et al 2021 in recent years the sediment load has changed significantly in various rivers under the influence of climate change and human activities misset et al 2019 binh et al 2020 potemkina and potemkin 2021 climate change has been reported to affect hydrological cycle in snow precipitation dominated regions das et al 2022 zhang et al 2022 intensified precipitations have triggered more frequent floods and increased sediment load in rivers li et al 2018 ran et al 2020 human activities such as dam construction best 2019 syvitski et al 2022 and land use change li et al 2020 sam and khoi 2022 also have profoundly impacted runoff and sediment load in particular dam building accounts for most of the sediment reduction in large rivers disrupting the natural process in riverine environment walling and fang 2003 guo et al 2020 the influence of land use on sediment yield is diverse e g increased vegetation coverage reduces sediment yield wang et al 2016 zhao et al 2022 while road construction and mineral mining accelerates soil erosion lu and higgitt 1998 liu et al 2019 understanding the variation in runoff and sediment load is of great significance for better management of river basins shao et al 2021 li et al 2021 das et al 2022 quantification of the changes in runoff and sediment load generally involves average over an annual monthly or flood event scale ran et al 2020 gao et al 2021 the use of flood event scale proves to be particularly important for river basins where rainstorms dominate precipitation and runoff e g the thames river bussi et al 2017 the celone river de girolamo et al 2015 the xihe river liu et al 2021 the carapelle river fortesa et al 2021 the yanhe river gu et al 2021 the kortuter river cakmak et al 2021 the yellow river zheng et al 2020 three major characteristic features of flood events have been recognized by previous studies firstly flood events account for most of the annual sediment load e g high flows have been observed to convey 94 of the total suspended load in the celone river basin de girolamo et al 2015 secondly flood events within a river basin may exhibit great fluctuations under similar discharges e g the first flood carries up to twice the sediment compared with a subsequent flood event in thames river bussi et al 2017 thirdly sediment transport in a flood event is closely related to land use fortesa et al 2021 and soil conservation efforts gu et al 2021 the mainstream of the yangtze river originates from the qinghai tibet plateau known as the third pole of earth and the water tower of asia li et al 2018 li et al 2021 recent studies have revealed increased sediment loads in the headwater region of yangtze river mainly due to climate change and glacier permafrost disturbances li et al 2022 and reduced sediment in the upper middle lower reaches of yangtze largely driven by reservoirs guo et al 2018 guo et al 2020 change in sediment transport in the upper yangtze river also have drawn much attention in the past decades due partly to the construction and operation of the three gorges reservoir tgr as well as other upstream water conservancy projects from its headwater to the city of yichang the upper yangtze river constitutes a major source of sediment yield for the entire river basin chen and wang 2019 sun et al 2022a b and temporal and spatial complexities in hydrology meteorology and geology result in extremely uneven runoff and sediment load in both the mainstream and tributaries wei et al 2014 cheng et al 2019 in recent years a drastic drop has been identified in sediment load from the upper yangtze river yang et al 2018 guo et al 2020 the decrease is attributed to sediment trapping by reservoirs tan et al 2019 peng et al 2020 especially large sized reservoirs as xiangjiaba and xiluodu reservoirs yang et al 2014 li et al 2018 the temporal and spatial variations of runoff and sediment load affect not only flowing condition of runoff and sediment into the tgr but the stability of river channel in the middle and lower yangtze river yang et al 2014 ren et al 2021 yan et al 2021 reported that reservoirs in the jinsha and jialing rivers are responsible for 70 of the reduction in sediment load the flood season from july to september contributes more than 90 of the annual sediment load in the upper yangtze river liu et al 2022 and human activities have been found to exert more influence on sediment load in flood seasons than in dry seasons xu et al 2021 previous studies have enhanced our understanding of the characteristics of sediment transport in the upper yangtze river on an annual or monthly basis global warming accelerates the hydrological cycle which changes the temporal and spatial characteristics of precipitation thereby leading to frequent extreme flood events and affecting the sediment transport capacity in rivers huntington 2006 in recent years therefore the use of flood event scale to investigate sediment load becomes more and more urgent on the one hand extreme flood events have been seen in recent years and their role in sediment transport awaits in depth explanation for instance a flood with record high concentration occurred in mid july 2018 in the tuo river accounting for 71 of the annual sediment load and a surprising 21 of all the annual sediment into tgr in previous studies the contribution by the tuo river was often ignored the reason for the abrupt increase in sediment load of the tuo river deserves to be studied in detail on the other hand the high intensity sediment transport during flood event influences in sediment deposition along the tgr liu et al 2022 the optimal operation of the tgr relies on detailed information of the flood itself e g the asynchronous propagation of sand peaks during a flooding process wang et al 2011 ren et al 2020 this paper aims to provide new insight into the characteristics of runoff and sediment load in the upper yangtze river by adopting a flood event scale based on measured data from representative hydrological stations a correlation is established between sediment load modulus and runoff erosion power then the variation in the power function is quantified and compared among the mainstream upper yangtze river and its major tributaries finally the abrupt increase reasons in sediment load in tuo and fu rivers in recent years are examined 2 study region the study region includes the mainstream of the upper yangtze river and its four major tributaries fig 1 a the yangtze river originates in melt snow on the slopes of the tanggula mountains flows across the tibet plateau highlands and then descends to a winding course with gorges and deep valleys this winding stretch is called the jinsha river until it receives the water from the min river and gets its official name of yangtze river the upper yangtze river is then joined by the other three major tributaries i e the tuo river the jialing river and the wu river before dumping into the tgr with the dam at the city of yichang the dividing point between the upper and middle reaches of the yangtze river from its headwater to yichang the upper yangtze river stretches approximately 4300 km and drains an area about 100 104 km2 the watershed areas are 458800 132926 23283 156142 and 83035 km2 for jinsha min tuo jialing and wu rivers respectively in history approximately 35 of the upper yangtze river basin suffered from high intensity soil loss the most affected regions with sediment modulus greater than 2000 t km2 included the lower reach of jinsha river and the upper reach of jialing river basins covering a total area of 5 104 km2 xu et al 2004 the upper yangtze river basin witnessed an average annual surface soil erosion of 16 108 t and a ratio of sediment load to sediment yield ranging between 0 1 and 0 6 large scale conservation efforts have been carried out since the 1980 s and an overall 35 reduction of area was achieved in region with high intensity soil erosion cwrc 2019 at the same time increase in sediment load has been observed in some regions which were hit by recent earthquakes e g the wenchuan earthquake may 12 2008 the ya an earthquake april 20 2013 and the jiuzhaigou earthquake august 8 2017 these regions are centered at the longmen mountain fault zone wang et al 2015 rapid economic growth has prompted comprehensive watershed development in the upper yangtze river in the last decades yang et al 2014 liu et al 2022 a total number of 14 594 reservoirs were built in 1950 2018 reaching a combined storage capacity of 1688 3 108 m3 83 of which is from 98 large sized reservoirs more information of the reservoirs is given in table 1 the distributions of typical large reservoirs in the jinsha min jialing and wu rivers are shown in fig 1b 3 data sources daily flow discharge and sediment concentration were recorded at hydrological stations that monitor the mainstream of the upper yangtze and its major tributaries the hydrological stations at the mainstream are the xiangjiaba station controls the jinsha river the zhutuo station controls jinsha min and tuo rivers and the cuntan station controls jinsha min tuo and jialing rivers the stations for four tributaries include gaochang station controls the min river fushun station controls the tuo river beibei station controls the jialing river and wulong station controls the wu river to further explore the reasons for recent increase in sediment yield in tuo and jialing river basins we also collected data from the zhenjiangguan station at the upper min river and three more stations at jialing river i e luoduxi station at the qu river a tributary on the left part of jialing river basin xiaoheba station at the fu river a tributary on the right part of jialing river basin and wusheng station at the mainstream of jialing river before the qu and fu rivers join the mainstream jialing river in addition suspended sediment grain size distribution and cross sectional topography in 2018 at fushun and xiaoheba stations were collected the data span a relatively long period of time i e 68 years at xiangjiaba and fushun stations 70 years at gaochang station 66 years at beibei and cuntan stations 62 years at wulong and zhutuo stations covering both wet years e g 1981 2013 2018 and 2020 and dry years e g 2002 and 2012 all the data were provided by the changjiang water resource commission cwrc except those from the zhenjiangguan station which is run by the hydrological bureau of sichuan province hbsp basic information of the collected data is shown in table 2 and the locations of each hydrological station are shown in fig 1a 4 methods 4 1 identification of flood event and calculation of its runoff volume and sediment load we used two criteria to identify a flood event i e i the peak flow discharge of a flood event should be greater than the average flow in flood season and ii flow discharge and sediment concentration in a flood event has a complete rising and receding process wang et al 2022 the runoff and sediment load in each flood event can be calculated with eqs 1 and 2 1 rv δ t i 1 n q i 10 8 2 sl δ t i 1 n q i ssc i 10 7 where rv is the runoff volume 108 m3 qi is the average flow discharge on the ith day m3 s sl is the sediment load 104 t ssci is the average sediment concentration in the ith day kg m3 n is the duration in days of the flood event and δt is the length of the day in seconds i e δt 24 3600 s 86400 s to facilitate comparison the sediment load modulus is scaled with the catchment area a in km2 as follows 3 w m sl a 10 4 where wm is sediment load modulus t km2 in the following sections we will use wm to refer to sediment load modulus unless specified otherwise 4 2 calculation of runoff erosion power the erosion power of a given flood event e is defined as the product of runoff depth and peak flow discharge divided by the catchment area 4 e h q max a where e is the flow erosion power mm m3 s km2 qmax is the peak discharge m3 s h is runoff depth mm a is catchment area km2 on an annual scale the average h proves to be a good parameter to represent the driving force of the flow liu et al 2008 yang et al 2014 on a flood event scale however the influence of rainfall intensity should also be taken into consideration wischmeier 1959 lu et al 2008 polyakov et al 2010 the runoff erosion power which accounts for the effect of raindrop splashing on soil erosion and transport has been recognized by previous studies to be a better parameter for correlating sediment and flow lu et al 2009 zhang et al 2016 wang et al 2018 cheng et al 2021 4 3 trend analysis we used the non parametric mann kendall m k test mann 1945 kendall 1975 for detecting a general trend within a time series of hydrological variables for a time series x x1 x2 xn the standardized test statistic z is given as 5 z s 1 v a r s s 0 0 s 0 s 1 var s s 0 in which 6 s i 1 n 1 j i 1 n sgn x j x i 1 x i x j 0 x i x j 1 x i x j 7 v a r s n n 1 2 n 5 i 1 q t i t i 1 2 t i 5 18 where s is the test statistic value xi and xj are the data values in time series i and j j i respectively n is the number of data points ti is the number of time points i and q is the number of groups a positive negative z value indicates an upward downward trend at 5 significance level the null hypothesis of no presence of trend was rejected if z greater than1 96 4 4 change point analysis the change point analysis involves identification of an abrupt change point followed by a significance test the sequential cluster method has been widely applied to identify change points in a hydrological series yang et al 2019 peng et al 2020 for a given series xi i 1 2 n with a possible abrupt change at τ this method examines the sum of the squares of deviations sn τ which is calculated as follows 8 sn τ i 1 τ x i x τ 2 i τ 1 n x i x n τ 2 where x τ and x n τ are the mean values of the data series before and after τ respectively a change point τ is determined when sn τ reaches minimum the rank sum test wilcoxon 1945 is one of the most common methods to test the significance of an abrupt change first proposed by wilcoxon in 1945 it is now widely used in hydrology wang et al 1997 fulton et al 2010 hao et al 2016 shi et al 2017 liu et al 2019 the detailed principle and application of the method can be found in rosner et al 2003 and ott and longnecker 2008 for a given series xi i 1 2 n with a change point of xτ this method introduces ranking of data from small to large within the entire sequence i e the smallest point ranks first and the largest point ranks the nth then the ranked series is divided into two groups by xτ with n 1 samples less than xτ and n 2 samples greater than xτ the sum of rank for the group with smaller capacity is denoted as w then a statistical variable is defined as follows 9 u w n 1 n 1 n 2 1 2 n 1 n 2 n 1 n 2 1 12 for a given significance level of 0 05 u greater than1 96 indicates a significant change point in the present study we used the sequential cluster method and the rank sum test for change point analysis 4 5 double mass curve analysis the change in the relation between runoff erosion power e and sediment load modulus wm was investigated with the double mass curve dmc method zhou et al 2020 kar and sarkar 2021 in an e abscissa wm ordinate curve a downward upward shift indicates a systematic decrease increase in wm under the same value of e the dmc analysis can also be used to further examine the rationality of change points 4 6 relation between e and wm for a specific station the relation between annual e and sediment load generally follows a power function walling 1977 lu et al 2009 zhang et al 2016 similarly we used a power function to correlate e and wm as follows 10 w m α e β where α and β are parameters to be determined specifically α indicates sufficiency of sediment supply while β reflects the flow capacity to carry sediment a river basin with loess produces a larger α than a region covered mostly by rock a reduced α will be observed after dam construction and soil conservation efforts the range of β grows wider when sediment trapping efforts are in operation for a specific watershed both α and β reflect the influence of human activities on sediment load 5 results 5 1 basic characteristics of runoff and sediment load during flood events some basic statistics of the flood events are given in table 3 on average the annual number of occurrence of flood events varies from three xiangjiaba station at the jinsha river to seven beibei station at the jialing river most of the floods occur in the flood season from june to september the occurrence of flood is closely related to or even dominated by rainstorms wang et al 2011 in the wu river basin wulong station for instance high rainfall comes much earlier than in other areas ye et al 2020 so june ranks the first in terms of annual maximum flood occurrence 40 while other areas witness most of the flood in july august or september with a percentage of 91 96 97 87 98 and 97 in above period for xiangjiaba gaochang fushun beibei zhutuo and cuntan stations respectively the difference in annual number of flood occurrence among tributaries can also be attributed to the spatial heterogeneity in frequency and intensity of rainstorm in the upper yangtze river according to wang and xing 2019 the wu river and lower jinsha river basins are characterized by low frequency rainstorm events while the min tuo and jialing rivers see more high frequency and high intensity rainstorms the floods play a vital role in contribution to annual runoff and sediment load the flood events account for 27 64 of the total annual runoff and the contribution to sediment load is more pronounced ranging from 64 to 99 5 2 trend and change point in runoff and sediment load 5 2 1 trend analysis as shown in fig 2 the annual variation of wm rv and e present decreasing trend of each station and the results of the m k trend analysis for wm rv and e are summarized in table 4 at a significance level of 0 05 neither the rv nor the e shows obvious change in trend in contrast a significant decreasing trend is recognized for wm such trend for flood events is consistent with the results based on the annual scale chen and wang 2019 peng et al 2020 yan et al 2021 5 2 2 change point analysis no significant change points present in the variations of runoff volume and flow erosion power however significant change points in sediment load have been identified for every station table 4 i e in 1999 and 2013 at xiangjiaba 2008 at gaochang 1985 and 2013 at fushun 1984 and 1998 at beibei 1992 and 2008 at wulong and 1999 and 2013 at both zhutuo and cuntan stations it is worth noting that two abrupt change points identified through the sequential cluster method have been deemed to be insignificant by the rank sum test decrease in 1995 at gaochang and increase in 2013 at beibei station the dmc results of e wm are depicted in fig 3 also included in fig 3 are the change points which highlight different periods generally it s believed that the abrupt changes of sediment load are caused by the operation of reservoir li et al 2018 wei et al 2020 in all stations except fushun station the abrupt change of wm during flood event in the upper yangtze river coincides with the operation of large dams in the upstream reach two break points are noticed for the xiangjiaba zhutuo and cuntan stations which are on the mainstream of the yangtze river the first in 1999 and the second in 2013 the first break is believed to be associated with the impoundment of the ertan reservoir in 1998 the dam traps most of sediment load from the yalong river a tributary of the jinsha river and a major contributor of sediment to the upper yangtze river the second break follows the damming of the jinsha river in 2012 by two giant projects the xiluodu and xiangjiaba reservoirs the break points downward for other stations are also attributed to dam construction for instance the decrease in 2008 at the gaochang station is due to the operation of the zipingpu reservoir in the min river lyv et al 2020 at the beibei station the two break points 1984 and 1998 are caused by the operation of the shengzhong reservoir in 1983 and the baozhusi reservoir in 1998 respectively wei et al 2014 zhou et al 2020 the two significant change points at the wulong station are associated with the impoundment of the wujiangdu reservoir in 1984 and the pengshui reservoir in 2008 respectively chen et al 2019 ye et al 2020 when runoff remains relatively stable dam construction is believed to be the major factor responsible for abrupt change in sediment load yang et al 2014 the lower jinsha and upper wu and jialing river basins which are notoriously prone to erosion have been enjoying large scale water and soil conservation efforts i e the changzhi project and the tianbao project since 1989 zhang and wen 2004 yang et al 2018 as a result an overall of 34 decrease in area subject to severe soil erosion was achieved from 35 104 m2 in the 1980s down to 23 104 m2 in 2018 cwrc 2019 however the effect of soil conservation efforts on sediment yield tends to be gradual insufficient to cause an abrupt change in the sediment load yang et al 2014 in contrast the construction of dams especially large sized ones leads to immediate trapping of most sediment in the reservoir huang et al 2013 binh et al 2020 yang et al 2014 reported that compared to soil and water conservation projects dams play a far more important role in sediment load reduction it is fact that the fushun station tuo river with a downward turn in 1985 experienced a unique rebound in 2013 while the 1985 downward break is attributed to impoundment of reservoirs the surprising upward break in 2013 still awaits further explanation this will be addressed in the discussion section 5 3 relation between e and wm 5 3 1 comparison of change among different periods fig 4 depicts the e wm relations of each flood event for all the stations the data were grouped into different periods by the change points at each station strong positive correlation between e and wm are obtained for all stations the fushun station tuo river shows the maximum value of r2 as 0 90 for the period of 2013 2020 while the beibei station jialing river presents the minimum r2 as 0 71 for the period of 1955 1983 thus on flood event scale the runoff erosion power proves to be a good variable closely related to sediment load in addition two prominent features can be seen from fig 4 and table 5 firstly the e wm plot exhibits a persistent drop at all stations except the fushun station this is illustrated by a closer look at the xiangjiaba station the first period 1953 1998 p1 witnesses the highest e wm profile the third period 2013 2020 p3 has the lowest profile while the second period 1999 2012 p2 lies in between though the drop from p2 to p3 is far more drastic than that from p1 to p2 the feature is accompanied by a decrease in α e g from 70 01 p1 to 56 32 p2 and to 0 56 p3 the drop in e wm indicates a reduction in sediment load on a flood event basis secondly the e wm plots of various periods tend to converge as e grows higher this is further illustrated by taking the wulong station as an example the plots separate at low e and converge at about e 90 mm m3 s km2 the steepness of the plot indicator of the magnitude of β increases from 0 79 p1 to 0 99 p2 and 1 34 p3 the converging characteristics can be seen for all the stations the lines at xiangjiaba station seems parallel at first look but a closer scrutiny of β reveals that they do converge i e β 0 72 p1 0 78 p2 and 1 02 p3 this new finding is of practical importance in reminding the sediment research community that though a general reduction in sediment load is established high flows have been found to still carry the same amount of sediment as their predecessors as mentioned earlier α and β reflect sediment supply to the stream and carrying capacity of the flow within the stream both parameters are sensitive to soil conservation efforts and dam construction although soil and water conservation projects are incapable of leading to abrupt change in sediment load they do exert a long term impact by enhancing vegetation coverage and reducing sediment yield e g responsible for 23 of the reduction in sediment load in the yangtze river basin during the period of 1989 2015 peng et al 2020 large dams play a more important role in sediment trapping for instance the xiluodu and xiangjiaba reservoirs with respective storage capacities of 13 7 108 m3 and 5 2 108 m3 successfully intercepted most of the sediment from the jinsha river lu et al 2019 yan et al 2021 the drastic reduction in sediment load at the xiangjiaba station will continue with recent operation of the wudongde and baihetan reservoirs compared with large and medium sized dams small dams exert multifaceted influence on sediment load on the one hand they intercept sediment at low to medium flows thus reducing the sediment flux to downstream stations on the other hand due to limited storage capacity they have to release previously deposited sediment out of the dam during high flows either passively or planned and as a result the downstream station receives increased sediment load due to the combined effects of soil conservation and dam construction a general decrease in α and an increase in β are observed for all stations except fushun station however the variation of α and β displays marked difference among these stations addressing such difference requires future study based on more detailed information which is out the scope of the present study the fushun station tuo river presents itself as an outlier in that an increase in sediment load is observed in recent years 2013 2020 it is the only station that witnesses such an abnormal increase in the first and second periods α 11 21 p1 2 91 p2 and β 0 89 p1 1 21 p2 in the third period α increases to 5 88 and β drops to 1 07 this unusual change in recent years is plausibly attributed to the influence of earthquakes as will be established in the discussion 5 4 2 comparison of the e wm relation in the first period p1 the first period p1 is used as benchmark when human activities exerted relatively light impact on sediment load miao et al 2011 shi et al 2017 fig 5 shows the e wm relation for all the stations except cuntan and zhutuo stations it is seen that the xiangjiaba station and the beibei station are higher in their plots of e wm than other stations this points to the fact the jinsha and jialing river basins are two major sources of sediment yield in the upper yangtze river yang et al 2014 liu et al 2022 the middle and lower reaches of jinsha river are characterized by hot and dry valleys which are susceptible to severe soil erosion liu et al 2019 the upper jialing river located in the south of the qinling mountain is also prone to strong erosion due to lack of vegetation shao et al 2021 in contrast the min and tuo rivers enjoy favorable land cover so the plots of e wm at gaochang and fushun stations are lower the wu river basin abounds in karst area resulting in the lowest e wm profile at the wulong station 6 discussions 6 1 comparison of change in wm while a qualitative description of the change in wm already has been given in fig 4 it is of practical importance to get a quantitative comparison of p3 p2 for fushun station against its benchmark p1 this is achieved by averaging the difference of wm between the two lines over the entire e interval see fig 4 to facilitate comparison the data are normalized by the benchmark values taking xiangjiaba station as an example the variation range of e is 0 1 4 6 mm m3 s km2 and the whole reduction amplitude δa can be calculated as follows 11 δ a 0 1 4 6 70 01 e 0 72 0 56 e 1 02 70 01 e 0 72 d e 100 then the average amplitude x at xiangjiaba station is as follows 12 x δ a 4 6 0 1 99 the results show that a 99 reduction is observed at the xiangjiaba station the reduction at gaochang fushun beibei and wulong stations are 46 23 69 59 respectively the strikingly high reduction in wm at xiangjiaba station is attributed to large scale dam construction in the upstream reaches the fushun station tuo river witnesses a much lower reduction 23 as only low head dams are in operation in the river basin 6 2 possible reasons for the recent increase in sediment load in the tuo and jialing rivers the fushun station tuo river witnessed an abrupt increase in sediment load in 2013 the jialing river also experienced increase in sediment load in recent years though no significant change points have been identified as the reported by li et al 2022 the increasing sediment loads in the headwater basin of yangtze river are mainly due to climate change and glacier permafrost disturbances nevertheless the increase in sediment in tuo and jialing river can be attributed to three possible factors i e earthquakes high intensity rainfall and local scouring during flood events 6 2 1 earthquake large earthquake could cause substantial landslides which are the predominate source of basin sediment yield chousianitis et al 2016 xie et al 2022 for example lin et al 2012 found that taitung earthquke caused the sharp increase in sediment load in luye catchment the tuo min and fu rivers run through the longmen mountain fault zone which was hit by frequent earthquakes in recent years e g may 12 2008 wenchuan earthquake april 20 2013 ya an earthquake and august 8 2017 jiuzhaigou earthquake according to china seismological bureau the wenchuan earthquake shook an irregular elliptical area of 440442 km2 with vi intensity or higher the areas of xi intensity x intensity ix intensity viii intensity and vii intensity are 2419 km2 3144 km2 7738 km2 27786 km2 and 84449 km2 triggering more than 56 000 geological hazards fig 6 a chen et al 2011 reported that landslides and collapses caused by the wenchuan earthquake were mainly distributed in the vii intensity region i e the upper reaches of the min tuo and fu rivers fig 6a landslides and debris flow fig 6b and fig 6c provide abundant sediment supplements ding et al 2014 li et al 2016 amounting to approximately 83 108 m3 in the upper basins of min tuo and fu rivers cheng et al 2010 it is estimated that transporting the sediment requires more than thirty years for fine particles less than 25 mm and up to 1000 years for coarse particles wang et al 2015 rainstorms hit the upper reaches of the min tuo and fu rivers in 2013 2018 and 2020 causing loose sediment from the hillside to pour into the river stream and thus enhancing downstream sediment concentration this increase in sediment load can be partly illustrated by a comparison of two typical flood events one in 2003 august 29 to september 3 and the other in 2018 june 25 to june 30 the two flood events are similar in duration about 6 days peak discharge 6080 versus 6230 in m3 s and runoff volume 15 versus 13 in 108 m3 but the 2018 flood carries two times more sediment than its 2003 counterpart i e 451 versus 239 in 104t this sharp difference can only be attributed to the influence of the earthquake as other conditions remain similar table 6 shows statistics of annual runoff and sediment load for two periods i e 1999 2012 and 2013 2020 in addition to fushun beibei and gaochang stations four more stations are included i e the xiaoheba station at the fu river major tributary of jialing river the luoduxi station at the qu river major tributary of jialing river and the wusheng station at the mainstream of jialing river in the upstream of beibei station and the zhenjiangguan station at the upper reaches of min river among all the seven stations three of them fushun xiaoheba and zhenjiangguan stations drain water directly from the earthquake hit regions as expected for these three stations the comparison of the two periods indicates a sharp rise in sediment load with fushun xiaoheba and zhenjiangguan stations witnessing an increase of 404 373 and 87 respectively it is not surprising that an increase of 88 at zhenjiangguan station in the upper min river is companied by a 19 decrease at gaochang station in the lower min river the zipingpu reservoir which lies in between of the two stations is believed to be responsible for such sharp contrast with a storage capacity of 11 1 108 m3 the reservoir has been reported to devour most of the sediment from landslides induced by the wenchuan earthquake you et al 2021 the increase at xiaoheba station is compensated by the simultaneous decrease at luoduxi and wusheng sations so at beibei station a much milder increase in sediment load is observed i e a 25 up from 2716 to 3392 104 t 6 2 2 high intensity precipitation in general higher precipitation would increase runoff and sediment load sun et al 2022a b especially for rivers flood by rainstorm the maximum sediment concentration in rivers is often synchronized with rainstorm ran et al 2020 high intensity rainstorms in 2013 2018 and 2020 act as direct driving agents to increase sediment load fig 7 for instance fig 8 the 2018 storm july 8 15 centered right at the upper tuo and fu rivers respective brought a total precipitation of 300 mm and 210 mm and resulted in a peak discharge of 8800 m3 s at the fushun station and a peak discharge of 16000 m3 s at the xiaoheba station a peak discharge of 28600 m3 s was observed at the beibei station high sediment concentrations were also registered during the rainstorms e g 18 kg m3 return period of 33 years at fushun station and 22 kg m3 at xiaoheba station record high at these two stations the return periods for sediment concentration are greater than those for peak discharge 6 2 3 local scouring local scouring within the rive stream during high flows also plays an important role in sediment transport on the one hand the gates of low head power stations are open during flood to facilitate scour of the sediment previously deposited in the reservoir on the other hand the power stations could be severely damaged or even completely destroyed resulting in large scale bank collapse dong et al 2019 fig 9 shows the cross sectional topographic changes at the fushun and xiaoheba stations before and after the flood season in 2018 fig 10 shows the actual scouring of the riverbed in fu river in 2018 clearly erosion is observed in some parts of the river bed implying that the flow had flushed local sediment to downstream it is significant to know if the stream got saturated in terms of sediment carrying capacity during the 2018 flood this is achieved by a comparison of the measured concentration with the carrying capacity of the flow the ruijin zhang s formula is selected for calculating sediment carrying capacity this formula has been tested repeatedly against abundant data zhang 1998 tan et al 2018 and found wide application in the upper reaches of the yangtze river lu 1998 chen et al 2021 the formula is as follows 13 s k u 3 gr ω m m 14 ω m j 1 n p j ω j where ω m is the average setting velocity of sediment m s n is the group number of sediment particles ω j is the average setting velocity of the jth group m s which can be calculated with the gonzalov formula zhang 1998 p j is the gradation of the jth group r is the hydraulic radius u is the average flow velocity m s and g is the acceleration of gravity equal to 9 8 m s2 here k and m are calibrated with the results by ruijing zhang zhang 1998 tan et al 2018 fig 11 shows a comparison of the sediment carrying capacity with the measured sediment concentration at the fushun and xiaoheba stations during the flood from july 8 to july 15 2018 it is seen that the actual sediment concentration did not arrive at the saturation state at both stations with ratio of the measured sediment concentration to the carrying capacity reaching 0 8 and 0 7 at fushun and xiaoheba stations respectively so it is possible that new record high sediment concentration will be recorded in the future with abundant supply of sediment in the upstream reaches 6 3 influence of flood events on sediment load flowing into the tgr while the runoff remains relatively stable a consistent decrease in sediment load into the tgr has been observed in recent years yang et al 2014 liu et al 2022 yan et al 2022 fig 12 shows the variation in the percentage of annual runoff and sediment load at the xiangjia gaochang fushun beibei and wulong stations relative to the total runoff and sediment load flowing into the tgr in 1960 2020 we adopted the most common method for calculating the runoff and sediment load flowing into the tgr in this paper zhou et al 2016 liu et al 2022 by summing up the three representative stations i e zhutuo beibei and wulong stations before the impoundment of the tgr 1960 2002 the annual runoff at xiangjiaba gaochang fushun beibei and wulong stations accounts for 37 22 3 17 and 13 of the total runoff discharge to tgr while the contributions of sediment load at these stations are 55 10 2 24 and 6 respectively after the impoundment of the tgr 2003 2012 the annual average runoff at these five stations accounts for 38 22 3 18 and 12 of the total runoff into the tgr while the sediment load accounts for 70 15 1 14 and 3 respectively however after the operation of xiangjiaba and xiluodu reservoirs in 2013 the jialing river replaced the jinsha river as the main source of sediment load flowing into the tgr the proportion of sediment load at the xiangjiaba station to that flowing into the tgr decreases to 2 in 2013 2020 while that at beibei and fushun stations increases to 39 and 13 respectively the proportion of runoff at the beibei and fushun stations increases to 17 and 4 respectively and the rate of increase are much lower than that of sediment due to high intensity rainstorms in the tuo and fu river basins in 2013 2018 and 2020 sediment load increased significantly in these years the annual sediment loads at the fushun station in these three years amount to 3600 104 t 2330 104 t and 2100 104 t accounting for 28 16 and 11 of the total sediment load flowing into the tgr respectively and those at the xiaoheba station are 3810 104 t 30 5170 104 t 36 and 7026 104 t 36 respectively during three typical flood events i e from july 8 to july 17 2013 from july 8 to july 15 2018 and from august 15 to august 21 2020 the fushun station receives sediment load of 2750 104 t 41 1436 104 t 20 and 1295 104 t 14 respectively and those at xiaoheba are 2943 104 t 43 3821 104 t 58 and 4754 104 t 49 flood events in the upper reaches of yangtze river exerted a notable impact on the sediment load flowing into the tgr 6 4 application and limitation of this study based on the measured data of runoff discharge and sediment load from hydrological stations in the upper yangtze river this paper systematically analyzed the temporal and spatial variation of sediment load during flood event and possible reason for recent increase of sediment load in tuo and fu rivers this paper presented three important finding firstly recent high flows of each station have been found to still carry the same amount of sediment as their predecessors secondly the occurrence of some special events such as earthquake rainstorm and river channel scouring has resulted in a significant increase in sediment load in tuo and fu rivers which were important reasons for recent increase in sediment load flowing into the three gorges reservoir thirdly the measured sediment concentration of classical flood event in tuo and fu rivers were far less than carrying capacity of flow implying new record high sediment concentration will be recorded in the future with abundant supply of loosen material induced by earthquake the results in this paper provide new insights into changes in sediment transport during the flood event in the upper yangtze river at present the studies concerning sediment transport law during flood event become more and more extensive under the background of global warming ran et al 2020 fortesa et al 2021 the study ideas in this paper also provide reference value for the relevant research of other rivers in the world at present abundant flow sediment data measured in the erosion and deposition balance of riverbed is the basis to calibrate parameters of sediment carrying capacity formula however for most of mountainous rivers such as tuo and fu rivers it is difficult to seek rationality data to determine parameter in sediment carrying capacity formula so this paper used the ruijin zhang s formula zhang 1998 tan et al 2018 a widely used in the upper yangtze river lu 1998 chen et al 2021 to calculate the change of sediment carrying capacity of the flood event in july 2018 in fushun and xiaoheba stations the more suitable method of sediment carrying capacity for the upper yangtze river needs to be further studied 7 conclusions measured data were collected and statistical analysis was conducted to investigate the variation in runoff volume and sediment load during flood events in the upper reaches of the yangtze river the findings in this paper provide new insights concerning the variation of runoff and sediment load in rivers and the results help to better manage flood events in operation of large reservoirs the following conclusions were reached 1 the flood events account for 27 to 64 of the total annual runoff the contributions to sediment load are more pronounced ranging from 64 to 99 2 a general decreasing trend is observed in sediment load during flood events at all river basins due to human activities 3 the relation between flow erosion power e and sediment load modulus wm follows a power function i e wm α eβ and all the stations except fushun witness a consistent decrease in α and increase in β the variation of α and β are closely related to soil conservation efforts and dam construction 4 a significant increase in sediment load has been observed in recent years in the tuo and fu rivers this abnormal increase is attributed to three possible factors i e earthquakes high intensity rainstorms and the local scour of sediment 5 flood events in the upper reaches of yangtze river exert a notable impact on the sediment load flowing into the tgr 6 as the peak sediment concentration at xiaoheba and fushun stations are still lower than sediment carrying capacity of flow it is possible that new record high sediment concentration will be recorded in the future with abundant supply of loosen material induced by earthquake credit authorship contribution statement shangwu liu conceptualization methodology investigation writing original draft dayu wang writing review editing formal analysis wei miao writing review editing formal analysis zhili wang writing review editing formal analysis peng zhang writing original draft danxun li conceptualization validation writing review editing funding acquisition visualization project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china grant no 51879138 and the national key hydraulic engineering construction funds 12630100100020j005 we express our deepest gratitude to the editor and reviewers who have provided valuable insights for revision of this paper 
2032,owing to the lack of hydrological monitoring stations and groundwater observations particularly for natural habitats the distribution of groundwater along the mountainous alluvial fan oasis desert system mods in the middle reaches of the tarim river has not yet been systematically investigated the performance of ensemble algorithms with multi source multi scale 30 m 90 m 250 m 500 m and 1000 m remote sensing data to predict the groundwater levels of the mods of the tarim basin should be investigated to improve the current knowledge on the spatial distribution of groundwater in this region we conducted a study on the applicability of ensemble learning algorithms for groundwater assessments in typical regions of dryland namely the oasis taklimakan desert otd nature land in oasis system nlos irrigation area ia and oasis system os the results showed that the r2 values of the groundwater level prediction accuracy for the four ecosystems were 0 92 0 96 0 86 and 0 50 respectively and their corresponding optimal ensemble algorithms were ar grbfn ar rf rotationforest mlp and ar grbfn the scales corresponding to the optimal prediction accuracy of otd nlos ia and os were 250 m 30 m 250 m and 90 m respectively and the effect of the scale on their respective groundwater level prediction accuracies maximum value compared to minimum value were 11 48 21 63 26 73 and 10 59 random subspace rotation forest and additive regression improved the prediction accuracy of the base learning method by 76 72 and 64 respectively followed by bagging whereas dagging greatly increased prediction errors rotation forest and random subspace showed stable performances and guaranteed relatively low prediction errors among all ensemble algorithms additive regression helped the base learners obtain relatively optimal prediction accuracies with high probabilities except for the os system groundwater level could be predicted with much greater accuracy at depths of 8 m than at 8 m the contribution analysis showed that topography and land use patterns controlled the spatial distribution of groundwater across mods the ensemble learning algorithm showed good performance using multi source and multi scale data keywords groundwater level tarim basin ensemble algorithms scale effect remote sensing data availability the data that has been used is confidential 1 introduction groundwater which comprises nearly one third of the world s freshwater supply is an essential part of the distribution of water resources and the largest freshwater reservoir in the world especially in arid areas wang et al 2021b yang et al 2019 groundwater reduces the gap between water supply and demand owing to its widespread distribution and ease of exploitation however the disparity between the availability and demand of water resources has worsened due to rapid economic growth population growth and agricultural land expansion groundwater is an important water source for residential use and farming as well as for natural dryland ecosystems such as the tarim river basin in central asia li et al 2007 zuo et al 2021 substantial hydrological and ecological issues have become apparent in the middle and lower reaches of the tarim river basin due to anthropogenic impacts and climate change including precipitous groundwater level decline bai et al 2020 natural vegetation degradation especially riparian ecosystems soil salinisation and desertification and river drying wei et al 2020 bai et al 2020 proposed that increased water usage and evaporation in the upstream regions are responsible for the declining groundwater level in the tarim basin chen et al 2019 suggested that groundwater depletion at the current rate would not sustain the normal function of the existing ecosystem in response the chinese government initiated the ecological water diversion project ewdp in 2000 spending 10 7 billion cny to restore the lower tarim river s ecological environment several research projects have been conducted to explore the intricate connection between the ewdp and ecohydrological processes these studies have primarily focused on the characteristics of changes in groundwater depth and quality in the lower tarim river recharge zone ecological responses such as plant community structure and diversity minimal ecological water requirements of natural vegetation and appropriate groundwater levels for vegetation bao et al 2017 huang and pang 2010 li et al 2007 liu et al 2022 according to the findings of previous studies land use temperature and precipitation in tarim river basin have changed significantly over the past few decades liu et al 2014 yao et al 2019 yao et al 2018 thus the spatial distribution of groundwater level in the middle reaches of the tarim river would also change however owing to the lack of hydrological monitoring stations and groundwater observations particularly for natural habitats a systematic understanding of the distribution of groundwater along the mountainous alluvial fan oasis desert system mods in the middle reaches of the tarim river has not yet been achieved chen et al 2019 this is detrimental to the proper scheduling and use of groundwater the advent of ensemble machine learning and multi source multi scale remote sensing data may provide the opportunity to reproduce the spatial distribution of groundwater in recent years data driven models related to machine learning and multi source multi scale remote sensing for groundwater prediction have been increasingly used due to their inexpensiveness and ability to rapidly produce reliable findings in various complicated groundwater systems sahoo et al 2017 zounemat kermani et al 2021 boreholes and geophysical geological hydrogeological and methodological approaches are standard traditional groundwater research techniques however they are typically expensive time consuming and unprofitable groundwater mapping has benefited from applying geographic information system gis remote sensing and machine learning approaches panahi et al 2020 groundwater grade zoning and water resource management are two applications in which the use of gis is efficient for processing vast volumes of spatial data thus remote sensing has emerged as an alternative method for acquiring surface water soil biological and gas characteristics konduri et al 2020 ensemble techniques have received considerable attention owing to their proficiency in managing complex and high dimensional data zounemat kermani et al 2021 according to recent findings ensemble machine learning approaches better decreased simulation errors than individual machine learning techniques shin et al 2017 sun et al 2020 since the 1990s ensemble techniques have emerged as a research hotspot in statistics data mining machine learning and pattern recognition many researchers have used these methods to solve various classification and regression issues singh et al 2014 modelled the hydrochemistry of groundwater in northern india based on bagging and boosting the findings demonstrated that ensemble models had higher accuracy than single decision tree models and were more successful in describing the impacts of human activity and seasonal fluctuations on groundwater hydrochemistry avand et al 2020 examined ensemble learning such as multiboosting best first tree bagging and adaboost to discover potential groundwater zones with best first tree providing the best prediction accuracy chen et al 2020 proposed several ensemble learning methodologies to map groundwater spring inventories in wuqi county shaanxi province china all ensemble learning methods showed superior prediction accuracy compared to the base classifier with rotation forest having the highest output accuracy to estimate groundwater potential mapping in the vietnamese province of dak lak nguyen et al 2020 developed five ensemble learning approaches including bagging dagging decorating multiboosting and random subspace the outcomes demonstrated that the prediction accuracy of the ensemble learning method was significantly increased compared with that of the base learner groundwater contamination potential mapping groundwater table and groundwater balancing are the primary focus of the present study groundwater prediction mainly involves forecasting groundwater levels in a limited scale or in coastal zones or the numerical modelling of longer time series of individual observation locations additive regression friedman 2002 bagging breiman 1996 dagging ting and witten 1997 random subspace tin kam 1998 and rotation forest are the ensemble approaches which have become the most popular in recent years rodriguez et al 2006 the performance of these ensemble algorithms have not been examined to predict the groundwater levels of the tarim basin mods therefore the objectives of this study were to 1 quantify the impact of four basic learners and five ensemble algorithms on the groundwater prediction accuracy in four ecosystems oasis taklimakan desert otd nature land in oasis system nlos oasis system os and irrigation area ia 2 evaluate the effects of multi scale remote sensing using five spatial resolutions and topographic variables on predicting groundwater level errors and 3 map the spatial distribution of groundwater levels using optimal models for the four ecosystems 2 study area the study area is located in the tarim basin in southern xinjiang the elevation decreases from north to south and west to east fig 1 with a range between 900 and 1300 m quaternary sediments of various hydrogeological units cover the study area including fine sandstones pebble sandstones chalky fine sandstones fine siltstones sand gravels and sandstones runoff is caused by glaciers snowmelt and precipitation from the surrounding mountains in the tarim basin the weigan and kuche rivers in kuqa muzal and kizil rivers in baicheng county kizil river in luntai county and tarim river in southern kuqa are the principal rivers in the research region the annual surface runoff to the oasis is 31 2 108 m3 and 90 of the river water is channelled for agriculture wang et al 2021b which has caused natural rivers to dry up in recent decades a moderate continental desert climate dominates southern xinjiang the mean annual precipitation ranges between 17 4 and 42 0 mm while pan evaporation ranges between 2500 and 3000 mm the mean annual temperature ranges between 5 1 c and 6 1 c and the accumulated temperature over ten c ranges from 4100 c to 4300 c per year under extreme circumstances cotton wheat corn and several other varieties of economic crops are the principal crops grown in oasis irrigation areas in contrast the natural environment comprises drought salt tolerant trees shrubs and grasses with the most common species being tamarisk poplar reed early sedge and camel thorns groundwater from oasis areas is critical to the natural desert ecosystems and agricultural economic development the area is also densely populated with oil fields and has a well developed road network with diverse landscape types the tarim river basin s hydrological characteristics in northwest china s interior desert region have changed significantly due to human activity and climate change chen et al 2015 yang et al 2022 with the expansion of oasis areas in arid regions over the past 30 years the centre of gravity for new arable land has gradually shifted from northeast to northwest in china especially in the tarim basin liu et al 2014 creating a water use conflict between ecology and agriculture wang et al 2021b the tarim river basin in southern xinjiang is the largest inland river basin in china and is a socially and economically important region for ethnic minorities during the past 60 years snowmelt runoff from three source rivers aksu hotan and yarkant of the tarim river basin has increased however net runoff from the main streams has been declining zhou et al 2018 the analysis showed that the total evapotranspiration of major crops in xinjiang increased by 47 85 causing a trend of decreasing ecological water supply to desert vegetation lv et al 2017 surface soil moisture 0 50 cm in the oasis region of the tarim basin has also significantly decreased during the past three decades because of higher evapotranspiration caused by global warming yao et al 2018 climate change and human activities have already caused dramatic changes in the hydrological environment changes in the groundwater levels in recent decades due to the hydrological characteristics of the study area are of great concerns to stakeholders as groundwater is an essential source of irrigation in dryland oases li et al 2007 wang et al 2021b zuo et al 2021 3 materials and methods 3 1 observations of groundwater level groundwater data obtained from the tarim oilfield construction project which has a capacity of 30 million tons were used for the preliminary environmental assessment of the groundwater prior to project development and collected from the baicheng kuqa and luntai oases taklamakan desert and the desert oasis ecotone between the oases these data were derived from two types of surveys fig 2 existing observation wells eow and drilling boreholes db the sampling sites were near the oil and gas fields and groundwater observation wells specifically the kuqa oasis eow baicheng oasis eow donghetang db halahatang db keshen db lungu db qianshan in yingmaili eow dina eow clastic rocks in the western part of tabei eow and tazhong db a total of 436 groundwater level readings were obtained throughout april to june 2018 to the best of our knowledge these observations were applied to regional scale groundwater prediction research for the typical mods of the tarim basin in central asia or the first time 3 2 environment variables google earth engine gee and other publicly available datasets were used to collect variables for the experiment all variables are shown in table 1 geographic information system gis data include land use and landform categories remote sensing and dem derived variables include surface reflectance vegetation biomass parameters such as normalised difference vegetation index ndvi and enhanced vegetation index evi surface temperature and topographic indicators the bioclimatic variables from worldclim https www worldclim org data index html include temperature and precipitation data the soil properties database was derived from soilgrid all variables were normalized prior to modelling digital elevation model data was obtained from gee the dataset nasa dem 30 m was created by reprocessing srtm data and adding supplementary information from the aster gdem icesat glas and prism datasets to increase accuracy the system for automated geoscientific analyses saga gis platform was then used to calculate several morphometry and hydrology related topographic indices namely topographic position index tpi multiresolution index of valley bottom flatness mrvbf mrrtf terrain ruggedness index tri topographic wetness index twi slope and aspect these variables were then resampled into five spatial resolutions 30 90 250 500 and 1000 m in the arid zone the change of land use and the fluctuation of groundwater level are closely correlated and are eagerly noticed scanlon et al 2005 therefore a global 30 m surface land use cover product finer resolution observation and monitoring from made by tsinghua university and published in 2017 gong et al 2019 was selected and downloaded from https data ess tsinghua edu cn this product provides seven land use categories irrigation field shrub grass lake salt affected soil rock and gravel wetland bare land gobi city and desert chai et al 2009 provided a technique for the geomorphologic surface regionalisation of xinjiang using srtm dem 90 m and landsat tm 30 m data this dataset 30 m resolution was downloaded from the national earth system science data center 2009 the final product divided the xinjiang topography into six geomorphic zones 23 geomorphic subzones and 195 geomorphic plots the study used the gee platform to calculate surface reflectance and the vegetation variables that reflected surface hydrology based on long time series data from landsat 8 oli the variables involved in b1 b2 b3 b4 b5 and b7 tcw tcg and tcb represent wetness greenness and brightness respectively derived from the tasselled cap transformation evi ndvi and generalised difference vegetation index gdvi the time period covered by these variables was the entire growing season april to september in 2018 each index involved in the calculation included the minimum 5 maximum 95 and mean values from five spatial resolutions 30 90 250 500 and 1000 m several surface temperature time series characteristic parameters were computed based on the moderate resolution imaging spectroradiometer modis surface temperature day and night dataset from the gee platform they were employed to describe the feedback of various surface changes for hydrothermal feature the parameters included the annual mean surface temperatures during the day lst day and night lst night the average day surface temperature in march subtracted from the surface temperature in april dlst3 4 dlst4 5 dlst5 6 dlst7 8 dlst8 9 and the average night surface temperature in march subtracted from the surface temperature in april lst3 4 lst4 5 lst5 6 lst7 8 and lst8 9 instead of using the mean annual temperature and rainfall as climate proxies bioclimatic variables that had greater environmental relevance were used to characterize hydrological changes nineteen bioclimatic variables from worldclim bio1 bio2 bio3 bio19 hijmans et al 2005 were also used as potential predictors and were calculated using monthly temperature and rainfall data from 1970 to 2000 these yearly trends such as mean annual temperature and precipitation and extreme or limiting environmental circumstances are represented by biologically significant variables such as temperature during the coldest and hottest months and precipitation during the rainy and dry seasons please visit https www worldclim for further detail soilgrid provided eight soil products with global coverage organic carbon soil bulk density cation exchange ph sand content clay content and available water content each product contained seven standard depths 0 5 15 30 60 100 and 200 cm hengl et al 2017 with a spatial resolution of 250 m the value of a specific product participating in the modelling was calculated by averaging seven depths at that location 3 3 methods 3 3 1 base learner the most common base learners in studies on ensemble learning in hydrology related domains are tree model structures 42 followed by artificial neural networks 30 according to zounemat kermani et al 2021 therefore we employed two algorithms for tree model structures m5and random forest rf and two techniques for neural network types gaussian radial basis function networks grbfn and multilayer perceptron neural network mlp m5 depending on the input data the m5 tree regression technique can be used to develop useful regression models quinlan 1992 it can predict the nonlinear characteristics of data more accurately than conventional linear regression holmes et al 1999 this white box method makes it easier to describe the contribution of variables to the research outcomes compared with black box algorithms such as neural nets the crucial parameter of this algorithm minnuminstances was set to 4 witten et al 1999 rf rf is a powerful and versatile supervised machine learning algorithm that grows and combines multiple decision trees to create a forest breiman 2001 it can solve both classification and regression issues the forest chooses the mean of all tree outputs when performing regression its main practical advantage is that it automatically corrects decision trees propensity to overfit their training set the problem of overfitting is almost eliminated when using the bagging approach with random feature selection which is excellent because overfitting results in erroneous results furthermore rfs often retain accuracy even without certain data maxdepth unlimited numfeatures sets the number of randomly chosen attributes when int log 2 predictors 1 is used for this study we set numiterations 100 grbfn grbfns are an effective method for learning detailed input output mappings a grbfn is typically described as a monotonic function of the euclidean distance between every point in space and a specific centroid frank 2014 this is a scalar function that is symmetric along the radial direction its function assumes a small value when a point in space is far from the centre complex functions can be fitted by modifying the local function value and range of action the most popular grbfn which has numerous applications in computer vision artificial intelligence image compression and data fitting is the one we employed in this research the main parameters were set by default the scale optimisation option was set to use scale per unit and attribute rige 0 01 seed one tolerance 1 0 e 6 frank 2014 mlp an mlp is a feed forward neural network augmentation hornik et al 1989 that comprises three layers input output and hidden the input layer receives the input signal to be processed prediction and categorisation are examples of operations that fall under the purview of an output layer an arbitrary number of hidden layers between the input and output constitute the mlp s true computational engine like a feed forward network the data moves forward from the input to the output layer of an mlp the mlp neurones were trained using the back propagation learning method mlp is used to address problems that cannot be resolved linearly and approximate any continuous function the main parameters were set by default as follows hiddenlayers attribs 2 learning rate 0 3 and momentum 0 2 taud and mas 2018 3 3 2 ensemble algorithm 3 3 2 1 additive regression gradient boosting constructs additive regression models which is accomplished by sequentially fitting a simple parameterised function base learner to current pseudo residuals using least squares at each iteration the pseudo residuals are the gradient of the loss function that is minimised for the model values at each training data point evaluated at the current step adding randomness to the process can significantly increase the approximation accuracy and execution speed of gradient boosting a subsample of the training data is randomly selected without replacement from the entire training dataset at each cycle the base learner is then fitted to this randomly chosen subsample and the model update for the current iteration is computed instead of using the entire sample this randomised approach also increases robustness against the overcapacity of the base learner friedman 2002 the main parameters were set by default the number of iterations 100 and shrinkage 1 0 3 3 2 2 bagging bagging predictors are methods that generate multiple versions of a predictor and use them to obtain an aggregated predictor when forecasting a numerical outcome aggregation averages all variants and when predicting a class it performs a plurality vote many versions are generated by creating bootstrap copies of the learning set and using these as new learning sets bagging can result in significant improvements in accuracy according to tests on actual and simulated datasets utilising classification and regression trees as well as subset selection in linear regression bagging can increase accuracy if perturbing the learning set can result in significant changes in the built predictor breiman 1996 the main parameters were set by default as follows numexecutionslot 1 numiterations 100 3 3 2 3 dagging ting and witten 1997 examined the technique of stacked generalisation in mixing models produced by a single learning algorithm from several subsets of a training dataset and other methods the influence of the sampling regime used to produce training subsets has already been investigated in this context when bootstrap samples are used the approach is termed bagging and dagging for disjunct samples the simplest way to integrate predictions from competing models is through the majority vote in this study we expanded these investigations to stacked generalisation combining the models using a learning technique this results in two new techniques known as bag stacking and dag stacking 3 3 2 4 random subspace splitting criteria and tree size optimisation have received considerable attention when discussing decision trees ho 1998 the dilemma between overfitting and achieving maximum accuracy has seldom been resolved a proposed method to construct a decision tree based classifier that maintains the highest accuracy on training data and improves the generalisation accuracy as it grows in complexity the classifier is composed of many trees that were built systematically by pseudo randomly choosing subsets of the feature vector component or trees built in randomly selected subspaces experiments on publicly accessible datasets were used to compare the subspace approach against single tree classifiers and other forest construction techniques proving the method s superiority the main parameters were set by default as follows numexecutionslots 1 numiterations 100 seed 1 and subspacesize 0 5 3 3 2 5 rotation forest rodriguez et al 2006 presented a feature extraction based strategy for constructing classifier ensembles the feature set was randomly divided into k subsets k is an algorithm parameter and principal component analysis was performed on each subset to generate training data for a base classifier all primary components were kept to maintain the variability information in the data consequently k axis rotations occur to produce additional features for a base classifier the rotation strategy was intended to promote individual accuracy and variety within the ensemble feature extraction for each base classifier promotes diversity decision trees were chosen because they are sensitive to the rotation of the feature axes hence the name forest the main parameters were set by default as follows maxgroup 3 mingroup 3 numberofgroups 1 numiterations 100 removedpercentage 50 and seed 1 recursive feature elimination rfe by genuer et al 2010 was used to find the smallest dataset from all covariates that is the dataset with a minor relative error in the variable combination rfe determines the smallest dataset by employing all variables at the start of the modelling and then eliminating the last variable in the order of contribution from each iteration this modelling method was repeated until the final covariate was reached this technique employs coefficient of determination r2 and root mean square error rmse to assess the ideal small dataset which has the highest r2 and lowest rmse values camera et al 2017 advocated combining the rfe with rf to produce more consistent and dependable forecasts therefore this study used the same strategy the modelling strategies listed below were used for the four ecosystems to predict groundwater level previous studies have demonstrated that the application of scale dependency has been shown to significantly increase the mapping accuracy of soil salinity in regions with considerable spatial variability huang et al 2015 pachepsky and hill 2017 wei et al 2021 the groundwater depth in the study region determines how severely the soil is salinized li et al 2007 thus the scale impact may improve the prediction accuracy of the groundwater level in the study area furthermore we also investigated the ensemble algorithm s potential to simultaneously increase the prediction accuracy for this purpose we developed the following procedure fig 4 first four datasets ecosystems were established based on the geographical location of the samples and land use characteristics namely the otd nlos os and ia fig 3 this was done to explore the main factors of the different ecosystems that controlled groundwater levels and to examine the effectiveness of ensemble algorithms at various scales five spatial resolution datasets in improving prediction accuracy second we analysed the influence of ensemble algorithms and scales on groundwater level prediction in different ecosystems for this purpose 24 algorithms were used for modelling including four base algorithms mlp grbfn m5p and rf and five combinations of ensemble and base learners 20 groups by mode of one to one integration such as additive regression rf additive regression mlp additive regression grbfn and additive regression m5p the five datasets used in the modelling corresponded to five spatial resolutions 30 90 250 500 and 1000 m climate variables modis temperature products land use and geomorphology were maintained at their original scales the variability between the datasets was dominated by landsat and dem derived variables with different resolutions finally the study selected the best combination base learner ensemble algorithm specific spatial resolution dataset based on the results of error analysis to map the spatial distribution of groundwater level in the different ecosystems 3 4 validation owing to the limited amount of data provided the predictive abilities of the ensemble algorithms and base learners were examined using the k fold cross validation method in this method the training dataset was randomly partitioned into five subsets with four partitions containing 80 of the data used to train the learner and the remaining 20 for validation because the approach involves significantly more computing effort than a simple train and test hold out procedure this validation test was reliable and impartial for smaller datasets heung et al 2016 this approach was deemed superior to the standard leave one out cross validation technique by assessing the predictive capabilities of the regression models over a larger number of samples the rmse and r2 values were used to evaluate the model s performance these statistical parameters and related calculations were performed in graphpad prism 7 0 and origin 8 5 r 2 i 1 n o i o p i p i 1 n o i o 2 0 5 i 1 n p i p 2 0 5 2 rmse i 1 n o i p i 2 n 1 where pi refers to the model predicted value at sampling site i and p is the mean value r2 values close to 1 indicated that the model effectively explained the variance of the observations rmse values close to 0 indicated accurate model predictions 4 result and discussion 4 1 statistical characteristics and spatial variability of groundwater levels in typical ecosystems the mean values of the corresponding groundwater level for each artificial and natural ecosystem were calculated table 2 for all samples the minimum maximum and mean values were 0 79 150 26 and 12 25 m respectively with an extreme spatial variability coefficient of variability equal to 1 87 subsequently we divided the samples into otd nlos ia os crop highland crop plain saline soil saline desert and desert based on google maps apparent electrical conductivity maps wang et al 2021a land use and landsat oli actual colour synthetic images and our local knowledge over many years the mean value of each dataset showed that the crop highland subsurface level was the largest 18 84 m followed by nlos and the minimum value occurs in saline soil 3 89 m saline desert ecotone and desert had mean groundwater levels of 7 69 and 7 70 m respectively in a study of the daryaboyi oasis in the centre of the taklamakan desert imin et al 2021 found that a groundwater level of 2 1 4 3 m may guarantee more active photosynthesis of plants groundwater level cannot be 7 m to guarantee the normal development of riparian woody plants according to li et al 2007 groundwater levels 5 59 m and soil salinity 1 61 g l would significantly impede the expected growth of t ramosissima groundwater levels in os cv 1 88 and nlos cv 1 26 exhibited very high spatial variability whereas the remaining geographic areas showed moderate spatial variability considering the importance of groundwater for oases natural ecosystems and irrigated areas and the number of samples we then used otd os nlos and ia as the main geographic areas for testing the effect of ensemble algorithms and scale on the prediction accuracy of groundwater level groundwater level influences the fate of ecosystems in dry zones and fluctuates with it to produce different landscape characteristics in this study the mean value of groundwater level in the irrigated agricultural region 4 66 m in plain and 18 84 m in baicheng oasis plain was greater than the results of wang et al 2021b who assessed the mean value of groundwater level from 2000 to 2018 as 4 10 m research showed that irrigated agriculture that is heavily reliant on groundwater leads to overexploitation wang et al 2021b which in turn depletes the groundwater table this supports that the main causal factor for the continuous decline of the groundwater level is the large amount of groundwater consumed by the expansion of the oasis in recent decades sun et al 2011 additionally the groundwater in the tarim basin also showed a decreasing trend between 2003 and 2016 with a significant decline in south tianshan zuo et al 2021 the same phenomenon was observed in the hexi corridor of china chen 2010 and western australia hu et al 2019 data for the study were collected during cropping from april to june when groundwater was required for salt washing and irrigation which may have further lowered the groundwater level fig 5 shows the statistical characteristics of evi at different groundwater levels revealing that the interval of 6 8 m was the extreme range for vegetation to survive whereas only specific deep rooted vegetation can survive at 8 10 m the localisation found that these observations were mainly in the desert area and adjacent to the oasis desert transition zone the areas with higher vegetation cover are located in the areas with groundwater levels 6 m and 10 m respectively attributed to irrigation and rainfall at higher elevations such as the baicheng oasis respectively historical studies in this region indicated that the basic groundwater table range for plants in the lower tarim river basin was 0 6 m and the diversity of species was greatest between 2 and 4 m drastically declining below 6 m hao et al 2010 this is consistent with our findings demonstrating the reliability of the sampling results regional fluctuations of the groundwater level was present in otd nlos and ia consistent with the strong spatial variability of soil salinity in the study area this is because the groundwater level is a crucial determinant of the occurrence of salinisation within the region wang et al 2021a 4 2 groundwater level prediction for different ecosystems fig 6 shows the prediction error rmse of groundwater level under different predefined conditions the nlos and otd ecosystems had the highest prediction accuracies with r2 and rmse values of 0 96 and 0 92 and 5 66 m and 6 09 m respectively this was followed by ia with r2 and rmse values of 0 86 and 3 78 m respectively meanwhile os had a 50 explanatory power of spatial variability with an rmse value of 3 49 m the lowest rmse for otd was obtained using a dataset with a spatial resolution of 30 m and a mean value of 10 86 m the minimum rmse produced without the ensemble algorithm by the base learner was 7 00 m with r2 0 94 this result was obtained from the grbfn algorithm with a 1000 m spatial resolution dataset the ensemble algorithm performance of the five machine learning methods at different spatial resolutions revealed that rotation forest computed the comparatively least rmse with a mean value of 10 02 m followed by random subspace and dagging with mean values of 10 20 and 19 28 m respectively ratationforest mlp which was supported by a 30 m spatial resolution dataset was the top performing ensemble algorithm for nlos this algorithm predicted an rmse of 5 66 m with an r2 of 0 98 comparatively randomsubspace mlp achieved a lower rmse of 5 91 m with r2 0 96 supported by same dataset the best performing ensemble algorithm for ia was the additive regression grbfn supported by a 250 m spatial resolution dataset with an rmse of 3 78 m and r2 0 93 the best performing base learner learning algorithm was rf 250 m with a predicted rmse of 4 43 and r2 0 91 under different conditions random subspace was generally the best partner for all base learners in the ia region with a mean rmse of 5 69 m followed by rotation forest with the worst being dagging for os additive regression rf was the best ensemble technique for forecasting groundwater levels which performed slightly better than rf using a 90 m spatial resolution dataset additive regression rf 90 m predicted an rmse of 3 49 with r2 0 50 whereas rf 90 m had an rmse of 3 59 m however comparing the performance of the five ensemble algorithms under the background of five spatial resolution datasets revealed that random subspace had the best ensemble performance with a mean rmse of 4 26 m followed by bagging and the worst performer was additive regression without the assistance of ensemble algorithms the mean rmse of the groundwater level predicted by the four base learners and supported by five spatial resolution datasets was 4 87 m after all ecosystems were evaluated the os region had the lowest prediction accuracy this could be explained by the poor link between environmental factors and groundwater levels in the area the sustainability of the artificial oasis in the os ecosystem relies mainly on irrigation shallow rooted vegetation which differs significantly from the survival strategy of plants under the natural environment we speculated that the combined data from the two regions artificial oasis and natural ecosystems lowered the groundwater aboveground landscape relationship 4 3 impact of ensemble algorithms on prediction accuracy the data statistics showed that the most optimal ensemble algorithm was influential in predicting the groundwater level in the study area fig 6 first we used the mean rmse value as a benchmark to evaluate the performance of the four base learners the best performing algorithm according to the 20 rmse statistics scenarios for the five spatial resolution datasets from four regions was rf rmse 6 49 m followed by grbfn rmse 6 75 m mlp rmse 8 31 m and m5 rmse 10 26 m this is because rf itself is an ensemble algorithm however in many instances in our investigation grbfn performed similarly and had superior computational accuracy however the advantage of rf over grbfn is that it requires fewer parameters to be set subsequently a total of 84 rmse statistics from the five spatial resolutions datasets of the four regions confirmed that random subspace was the best ensemble algorithm compared with the base learner with an average reduction in rmse percentage of 9 22 followed by rotation forest 6 74 additive regression 2 94 and bagging 2 78 and the worst one was dagging 42 55 fig 7 a among them random subspace rotation forest and additive regression improved the prediction accuracy of the base learner by 76 72 and 64 followed by bagging meanwhile dagging increased the possibility of prediction errors by comparing the four ecosystems we found that random subspace was more stable and improved the rmse of all systems followed by rotation forest and additive regression bagging s overall performance was average however in terms of error reduction it was comparable to that of rotation forest and random subspace the largest error reduced by bagging in nlos os and ia were 28 21 36 63 and 26 67 respectively whereas rotation forest and random subspace reduced the errors by 34 60 and 37 11 37 06 and 29 80 and 25 85 and 23 69 respectively finally we analysed the efficiency of cooperation between the base learners and ensemble algorithms the results showed that additive regression was best paired with rf rmse 6 10 random subspace with grbfn rmse 6 55 m bagging with mlp rmse 6 90 m and rotation forest with m5 rmse 8 21 m additive regression rf had the best predictive power among the ensemble algorithms furthermore in terms of mean rmse additive regression was the only ensemble algorithm that improved the prediction error of rf rmse 6 48 m whereas all other ensemble algorithms such as bagging rmse 7 06 m dagging rmse 11 97 m random subspace rmse 6 54 m and rotation forest rmse 6 61 m failed to improve the prediction error of rf with all base learners additive regression had the highest probability 75 of obtaining optimal prediction accuracy within the four ecosystems in summary rotation forest and random subspace provided relatively low prediction errors with high probability and were distinguished by their steady performance in various ecosystems additive regression helped base learners obtain relatively optimal solutions with high probability however no single algorithm perfectly optimised all the error problems multiple algorithms must be applied and validated for testing to determine the most efficient one khosravi et al 2018a 4 4 effect of different scale data sets on prediction accuracy the accuracy of groundwater level estimation in various ecosystems was influenced by five datasets with different spatial resolutions fig 7b we calculated the average values of 24 rmses at five resolutions in four ecosystems which included the base learner four calculation scenarios and ensemble learning algorithms 20 calculation scenarios the magnitudes of the effect of five datasets with different spatial resolutions on the accuracy of groundwater level prediction maximum value of rmse compared with the minimum value were 11 48 21 63 10 59 and 26 73 for otd nlos os and ia respectively and their respective optimal spatial resolutions were 500 30 90 and 250 m however the above statistical results did not represent the spatial resolution used for the highest prediction accuracy nor indicated that the optimal solution can be obtained with the support of the spatial resolution dataset the probability of the optimal solution was higher with the support of the spatial resolution dataset for example the optimal scales for obtaining optimal precision for different ecosystems were 250 m otd 30 m nlos 90 m os and 250 m ia whereas the optimal scale for otd was not from the dataset with spatial resolution of 500 m however the results demonstrated that scale factors can improve the accuracy of groundwater level predictions in different ecosystems the findings from the four ecosystems showed the effect of the scale on the groundwater level and surface environment consistent with many other studies that have found that scale affects the relationship between soil properties and the environment behrens et al 2010 guo et al 2018 miller et al 2015 this may be because groundwater is the precondition of oasis existence and its spatial and temporal variability is essential in determining the spatial heterogeneity between soil properties and environmental characteristics notably the optimal interpretation scale adapted to the mods system such as the otd is not the optimal scale for other ecosystems the optimal scale of groundwater level prediction changes depending on the ecosystem we correlated the scale and prediction accuracy with the spatial variability of the groundwater level and found no significant correlations between them implying that numerous attempts should be performed to obtain an ideal scale 4 5 prediction error of the optimal ensemble algorithm at the threshold depth of the groundwater table fig 8 and table 3 show the accuracy of the predicted groundwater levels at various depths in the four ecosystems according to the literature the groundwater levels at 4 and 8 m are the fuzzy threshold values for salinisation and desertification respectively hao et al 2010 sun et al 2011 using this as a guideline this study investigated the performance of the optimum model at 0 4 4 8 0 8 and 8 m except for the os system the results revealed that groundwater levels can be forecasted with higher accuracy at depths 8 m than at depths 8 m we discovered that these observations were mostly located in natural habitats outside the irrigation region and were tightly connected to topography and geomorphology the mean predicted values of all ecosystems were higher than the observed values for groundwater depths 4 m table 3 the randomness of the scatter between the predicted and observed values within this range is shown in fig 8 the reason for this might be that this depth 4 m occurs in the saline zone of the nlos the oasis desert transition zone and irrigated areas the vegetation types within the saline zone are homogeneous and have low coverage while the vegetation within the oasis desert transition zone which can reflect the groundwater level is composed of multiple communities and has a low remote sensing spectral separation in addition not all vegetation is groundwater dependent the utilisation of various water sources by desert vegetation during various growing seasons suggests that roots are selective in their uptake of water sources during different periods bahejiayinaer et al 2018 in summer phragmites in dune habitats mainly use groundwater salt spiraea in salt marsh habitats use soil water and white spiraea in desert habitats shift their water use strategy from surface to deep soil water hao and li 2021 in contrast in agricultural cultivation areas where irrigation is routine the groundwater level does not influence the geographical variance of economic crop greenness we also attempted to forecast the groundwater level in the desert however because of the uniform texture undulating dunes and lack of vegetation determining the groundwater level using the current dataset was more challenging therefore the results were excluded from the study in these locations the combined impact of environmental factors on groundwater is complicated bekele et al 2019 the means of the predicted and observed values were similar when the observed groundwater depth was between 4 and 8 m however the r2 values were not high similar to the predictions within 0 8 m all forecasted values fell within the threshold range in summary the use of data with various spatial resolutions and sub regional modelling is recommended to boost the groundwater prediction accuracy according to the comparison of the prediction errors at various depths especially when the scale of the research area is large and its land cover type is more diverse 4 6 influencing factors in diverse habitats the combination of topography land use and climate control the groundwater fluctuations that vary depending on ecosystem features fig 9 geomorphology dem 250 m and climatic variables wc01 wc13 wc15 had the most significant influence on the overall trend of the groundwater level in the otd although temperature and evaporation significantly impacted variation of groundwater level precipitation has a comparatively limited impact on changes in groundwater levels zuo et al 2021 for the os area the spatial variation in the groundwater level resulted from the interaction between geomorphology land use and mrvbf 90 m in which geomorphological features dominated among all variables climate variables such as the maximum temperature of the warmest month and other meteorological parameters had a greater influence than other factors in the nlos region followed by elevation dem30m the dem at 250 m dominated the entire groundwater level in the irrigated region groundwater level changes in the studied region were generally driven by geomorphology and the dem therefore we used otd as an example to investigate the response relation between these two and the partial variable dependent groundwater level in fig 10 a when the altitude is 1025 m the groundwater table is relatively shallow and does not exhibit a clear trend however when the altitude is between 1025 and 1200 m the groundwater depth exhibited an increasing trend and raised quickly between 1200 and 1240 m mid elevation alluvial floodplain after which 1240 m the groundwater table did not change this may be the result of insufficient observations fig 10b illustrates that the middle elevation alluvial floodplain 121341 middle elevation river valley plain 121360 middle elevation flowing gently undulating sandy area 123132 middle elevation dry floodplain 123220 and middle elevation dry floodplain 123221 had deeper groundwater levels in contrast the low elevation alluvial fan plain 111334 middle elevation alluvial depressions 121336 and erosion and deposition have relatively shallow groundwater levels the pearson correlation between the preferred variables and the groundwater level is presented in table 4 for otd dem250m showed the highest correlation value with the groundwater level r 0 71 followed by slope250m r 0 52 and wc12 r 0 36 the mean annual temperature wc01 showed the highest negative correlation r 0 60 followed by mrvbf250m r 0 52 the groundwater level decreased significantly with decreasing elevation bekele et al 2019 the correlations between the preferred environmental variables and groundwater levels were low within the os system regarding preferred environmental characteristics inside nlos wc11 r 0 40 and tcw90m r 0 32 had the strongest correlations the groundwater level and dem showed the greatest positive correlation with r 0 90 followed by wc12 and the greatest negative correlation indicated by wc01 with r 0 88 and wc05 with r 0 83 within the ia system the largest positive correlation value was dem250m r 0 63 followed by slope250m and the maximum negative correlation was mrvbf250m with r 0 53 4 7 spatial distribution characteristics of groundwater levels the spatial distribution characteristics of the groundwater levels matched the land use and geomorphological patterns of the study area fig 11 the groundwater level of the baicheng oasis was deeper compared with other regions due to its high altitude in contrast the groundwater level of the irrigation area was relatively shallow maintaining a groundwater level of 4 m groundwater levels in the ecotone between the desert and the oasis ranged from 6 to 8 m but increased with elevation as seen in the eastern half of the kuqa oasis and its western border with the aksu salt affected land the actual predictions were in the range of 6 8 m whereas the mean value of the groundwater level in the surroundings of the kuqa oasis in the northern end of the southern taklamakan desert was approximately 8 m the predicted and observed values were within comparable intervals in the region where the sample sites were located in the southern taklamakan desert the link between the observed and predicted values was further examined and the prediction accuracy was found to vary depending on the depth range the probability that the predicted and actual values fell within the 4 8 m depth range was 87 57 93 47 92 91 and 92 10 for otd nlos os and ia respectively in contrast using the predicted values in the range of 4 8 m as a reference the probability of the actual values that fell in this range were 57 37 for otd and 62 07 for os here only these two ecosystems were used as examples the probability of the predicted values between 0 and 8 m was 92 39 95 45 91 81 and 99 01 for otd nlos os and ia respectively in contrast the probability that the predicted values were set within 0 8 m and the actual values fell within this range were 85 35 for otd and 91 80 for os finally for predictions deeper than 8 m the predicted values indicated a confidence level of 76 42 for otd and 83 87 for os in conclusion the confidence level of the predicted groundwater depths was greater in the ranges of 0 8 m and 8 m than at the other depths when utilising the observed and forecasted values as references the findings of these respective depth ranges assisted us in recognising differences in the actual scenario 4 8 uncertainty and limitations the unsatisfactory precision of the available data and a lack of important soil vegetation factors that were directly connected to groundwater may restrict the predicted accuracy of the groundwater level groundwater samples were collected over two months for this investigation possibly affecting the time dependence of the groundwater level subsurface relationship because the sample could not be promptly collected we examined data from observation wells in the kuqa oasis from 1997 to 2003 and discovered that intra annual groundwater level changes varied from 0 44 to 2 33 m with a mean value of 1 17 m however because the range of intra annual groundwater level changes in the desert oasis transition zone was unknown the precise degree of the impacts above cannot be determined additionally land use data were mapped for 2015 which may have reflected the land use pattern of an earlier period furthermore the prediction accuracy of 0 4 and 0 8 m could have been better enhanced if soil related data with greater spatial resolution were available such as soil moisture and hydraulic characteristics the analysis of variations in groundwater levels benefits significantly from the more specific categories of soil properties fu et al 2019 existing soil moisture data with a poor spatial resolution 25 km are of limited use in the prediction aims of the current study high precision soil hydraulic properties such as bulk weight soil texture organic carbon wilting point saturated water content field water holding capacity and agricultural drainage data are in short supply fu et al 2019 kim and jackson 2012 zhang et al 2014 these characteristics provide clear evidence of the hydrological relationship between groundwater and the surrounding environment in anthropogenic and natural oases jiang et al 2015 wang et al 2014 a hydrogeological map considerably impacts groundwater level although it can be challenging to collect erler et al 2019 in contrast information regarding the development of the groundwater cycle and hydrogeochemistry can be obtained from the geographical and temporal distribution of groundwater levels 5 conclusion the spatial variability of groundwater level in the typical mods of arid zones is unknown due to the extreme lack of observational data to this end the study used explored the spatial distribution patterns of groundwater levels in mods in the tarim basin in central asia using limited observational data the study simultaneously examined the effects of ecosystem characteristics of four typical regions otd ia os and nlos ensemble algorithms and five spatial resolution datasets to obtain the best groundwater level prediction accuracy in otd ia and nlos the best model exhibited 90 explanatory power whereas os only explained 50 of the spatial variability random subspace rotation forest and additive regression improved the prediction accuracy of the base learner by 76 72 and 64 respectively dagging with other base learners indicated a high probability of increasing the error rotation forest and random subspace combined with base learners generated relatively low prediction errors with a high probability and exhibited steady performances in the four ecosystems base learners produced nearly optimum outcomes with high probability using additive regression in the four ecosystems the scale factor increased the prediction accuracy of groundwater level by 10 59 26 73 compared with the range of 8 m the optimum model performed better in the range of 8 m future research should address the limited prediction accuracy of shallow groundwater level 8 m by adding multiple data sources such as geologic data multi period runoff data multi period microwave retrieved soil moisture datasets with high spatial resolution soil hydrologic conductivity spatial interpolation based on field collected data and fine scale vegetation types crops and natural vegetation communities as vegetation is an essential prerequisite for maintaining the survival of oases in dryland the correlation between groundwater level and above ground vegetation should also be explored covering the influence of vegetation type biomass vegetation growth fluctuations and vegetation phenology on groundwater levels credit authorship contribution statement yang wei investigation methodology writing original draft fei wang supervision formal analysis conceptualization writing review editing bo hong investigation formal analysis shengtian yang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the national natural science foundation of china 42101363 and u1603241 we are very proud to have been invited to participate in the environmental assessment project and we also thank the truck mounted drilling operators and samplers who worked in the difficult conditions finally we thank the reviewers for their comments and suggestions 
2032,owing to the lack of hydrological monitoring stations and groundwater observations particularly for natural habitats the distribution of groundwater along the mountainous alluvial fan oasis desert system mods in the middle reaches of the tarim river has not yet been systematically investigated the performance of ensemble algorithms with multi source multi scale 30 m 90 m 250 m 500 m and 1000 m remote sensing data to predict the groundwater levels of the mods of the tarim basin should be investigated to improve the current knowledge on the spatial distribution of groundwater in this region we conducted a study on the applicability of ensemble learning algorithms for groundwater assessments in typical regions of dryland namely the oasis taklimakan desert otd nature land in oasis system nlos irrigation area ia and oasis system os the results showed that the r2 values of the groundwater level prediction accuracy for the four ecosystems were 0 92 0 96 0 86 and 0 50 respectively and their corresponding optimal ensemble algorithms were ar grbfn ar rf rotationforest mlp and ar grbfn the scales corresponding to the optimal prediction accuracy of otd nlos ia and os were 250 m 30 m 250 m and 90 m respectively and the effect of the scale on their respective groundwater level prediction accuracies maximum value compared to minimum value were 11 48 21 63 26 73 and 10 59 random subspace rotation forest and additive regression improved the prediction accuracy of the base learning method by 76 72 and 64 respectively followed by bagging whereas dagging greatly increased prediction errors rotation forest and random subspace showed stable performances and guaranteed relatively low prediction errors among all ensemble algorithms additive regression helped the base learners obtain relatively optimal prediction accuracies with high probabilities except for the os system groundwater level could be predicted with much greater accuracy at depths of 8 m than at 8 m the contribution analysis showed that topography and land use patterns controlled the spatial distribution of groundwater across mods the ensemble learning algorithm showed good performance using multi source and multi scale data keywords groundwater level tarim basin ensemble algorithms scale effect remote sensing data availability the data that has been used is confidential 1 introduction groundwater which comprises nearly one third of the world s freshwater supply is an essential part of the distribution of water resources and the largest freshwater reservoir in the world especially in arid areas wang et al 2021b yang et al 2019 groundwater reduces the gap between water supply and demand owing to its widespread distribution and ease of exploitation however the disparity between the availability and demand of water resources has worsened due to rapid economic growth population growth and agricultural land expansion groundwater is an important water source for residential use and farming as well as for natural dryland ecosystems such as the tarim river basin in central asia li et al 2007 zuo et al 2021 substantial hydrological and ecological issues have become apparent in the middle and lower reaches of the tarim river basin due to anthropogenic impacts and climate change including precipitous groundwater level decline bai et al 2020 natural vegetation degradation especially riparian ecosystems soil salinisation and desertification and river drying wei et al 2020 bai et al 2020 proposed that increased water usage and evaporation in the upstream regions are responsible for the declining groundwater level in the tarim basin chen et al 2019 suggested that groundwater depletion at the current rate would not sustain the normal function of the existing ecosystem in response the chinese government initiated the ecological water diversion project ewdp in 2000 spending 10 7 billion cny to restore the lower tarim river s ecological environment several research projects have been conducted to explore the intricate connection between the ewdp and ecohydrological processes these studies have primarily focused on the characteristics of changes in groundwater depth and quality in the lower tarim river recharge zone ecological responses such as plant community structure and diversity minimal ecological water requirements of natural vegetation and appropriate groundwater levels for vegetation bao et al 2017 huang and pang 2010 li et al 2007 liu et al 2022 according to the findings of previous studies land use temperature and precipitation in tarim river basin have changed significantly over the past few decades liu et al 2014 yao et al 2019 yao et al 2018 thus the spatial distribution of groundwater level in the middle reaches of the tarim river would also change however owing to the lack of hydrological monitoring stations and groundwater observations particularly for natural habitats a systematic understanding of the distribution of groundwater along the mountainous alluvial fan oasis desert system mods in the middle reaches of the tarim river has not yet been achieved chen et al 2019 this is detrimental to the proper scheduling and use of groundwater the advent of ensemble machine learning and multi source multi scale remote sensing data may provide the opportunity to reproduce the spatial distribution of groundwater in recent years data driven models related to machine learning and multi source multi scale remote sensing for groundwater prediction have been increasingly used due to their inexpensiveness and ability to rapidly produce reliable findings in various complicated groundwater systems sahoo et al 2017 zounemat kermani et al 2021 boreholes and geophysical geological hydrogeological and methodological approaches are standard traditional groundwater research techniques however they are typically expensive time consuming and unprofitable groundwater mapping has benefited from applying geographic information system gis remote sensing and machine learning approaches panahi et al 2020 groundwater grade zoning and water resource management are two applications in which the use of gis is efficient for processing vast volumes of spatial data thus remote sensing has emerged as an alternative method for acquiring surface water soil biological and gas characteristics konduri et al 2020 ensemble techniques have received considerable attention owing to their proficiency in managing complex and high dimensional data zounemat kermani et al 2021 according to recent findings ensemble machine learning approaches better decreased simulation errors than individual machine learning techniques shin et al 2017 sun et al 2020 since the 1990s ensemble techniques have emerged as a research hotspot in statistics data mining machine learning and pattern recognition many researchers have used these methods to solve various classification and regression issues singh et al 2014 modelled the hydrochemistry of groundwater in northern india based on bagging and boosting the findings demonstrated that ensemble models had higher accuracy than single decision tree models and were more successful in describing the impacts of human activity and seasonal fluctuations on groundwater hydrochemistry avand et al 2020 examined ensemble learning such as multiboosting best first tree bagging and adaboost to discover potential groundwater zones with best first tree providing the best prediction accuracy chen et al 2020 proposed several ensemble learning methodologies to map groundwater spring inventories in wuqi county shaanxi province china all ensemble learning methods showed superior prediction accuracy compared to the base classifier with rotation forest having the highest output accuracy to estimate groundwater potential mapping in the vietnamese province of dak lak nguyen et al 2020 developed five ensemble learning approaches including bagging dagging decorating multiboosting and random subspace the outcomes demonstrated that the prediction accuracy of the ensemble learning method was significantly increased compared with that of the base learner groundwater contamination potential mapping groundwater table and groundwater balancing are the primary focus of the present study groundwater prediction mainly involves forecasting groundwater levels in a limited scale or in coastal zones or the numerical modelling of longer time series of individual observation locations additive regression friedman 2002 bagging breiman 1996 dagging ting and witten 1997 random subspace tin kam 1998 and rotation forest are the ensemble approaches which have become the most popular in recent years rodriguez et al 2006 the performance of these ensemble algorithms have not been examined to predict the groundwater levels of the tarim basin mods therefore the objectives of this study were to 1 quantify the impact of four basic learners and five ensemble algorithms on the groundwater prediction accuracy in four ecosystems oasis taklimakan desert otd nature land in oasis system nlos oasis system os and irrigation area ia 2 evaluate the effects of multi scale remote sensing using five spatial resolutions and topographic variables on predicting groundwater level errors and 3 map the spatial distribution of groundwater levels using optimal models for the four ecosystems 2 study area the study area is located in the tarim basin in southern xinjiang the elevation decreases from north to south and west to east fig 1 with a range between 900 and 1300 m quaternary sediments of various hydrogeological units cover the study area including fine sandstones pebble sandstones chalky fine sandstones fine siltstones sand gravels and sandstones runoff is caused by glaciers snowmelt and precipitation from the surrounding mountains in the tarim basin the weigan and kuche rivers in kuqa muzal and kizil rivers in baicheng county kizil river in luntai county and tarim river in southern kuqa are the principal rivers in the research region the annual surface runoff to the oasis is 31 2 108 m3 and 90 of the river water is channelled for agriculture wang et al 2021b which has caused natural rivers to dry up in recent decades a moderate continental desert climate dominates southern xinjiang the mean annual precipitation ranges between 17 4 and 42 0 mm while pan evaporation ranges between 2500 and 3000 mm the mean annual temperature ranges between 5 1 c and 6 1 c and the accumulated temperature over ten c ranges from 4100 c to 4300 c per year under extreme circumstances cotton wheat corn and several other varieties of economic crops are the principal crops grown in oasis irrigation areas in contrast the natural environment comprises drought salt tolerant trees shrubs and grasses with the most common species being tamarisk poplar reed early sedge and camel thorns groundwater from oasis areas is critical to the natural desert ecosystems and agricultural economic development the area is also densely populated with oil fields and has a well developed road network with diverse landscape types the tarim river basin s hydrological characteristics in northwest china s interior desert region have changed significantly due to human activity and climate change chen et al 2015 yang et al 2022 with the expansion of oasis areas in arid regions over the past 30 years the centre of gravity for new arable land has gradually shifted from northeast to northwest in china especially in the tarim basin liu et al 2014 creating a water use conflict between ecology and agriculture wang et al 2021b the tarim river basin in southern xinjiang is the largest inland river basin in china and is a socially and economically important region for ethnic minorities during the past 60 years snowmelt runoff from three source rivers aksu hotan and yarkant of the tarim river basin has increased however net runoff from the main streams has been declining zhou et al 2018 the analysis showed that the total evapotranspiration of major crops in xinjiang increased by 47 85 causing a trend of decreasing ecological water supply to desert vegetation lv et al 2017 surface soil moisture 0 50 cm in the oasis region of the tarim basin has also significantly decreased during the past three decades because of higher evapotranspiration caused by global warming yao et al 2018 climate change and human activities have already caused dramatic changes in the hydrological environment changes in the groundwater levels in recent decades due to the hydrological characteristics of the study area are of great concerns to stakeholders as groundwater is an essential source of irrigation in dryland oases li et al 2007 wang et al 2021b zuo et al 2021 3 materials and methods 3 1 observations of groundwater level groundwater data obtained from the tarim oilfield construction project which has a capacity of 30 million tons were used for the preliminary environmental assessment of the groundwater prior to project development and collected from the baicheng kuqa and luntai oases taklamakan desert and the desert oasis ecotone between the oases these data were derived from two types of surveys fig 2 existing observation wells eow and drilling boreholes db the sampling sites were near the oil and gas fields and groundwater observation wells specifically the kuqa oasis eow baicheng oasis eow donghetang db halahatang db keshen db lungu db qianshan in yingmaili eow dina eow clastic rocks in the western part of tabei eow and tazhong db a total of 436 groundwater level readings were obtained throughout april to june 2018 to the best of our knowledge these observations were applied to regional scale groundwater prediction research for the typical mods of the tarim basin in central asia or the first time 3 2 environment variables google earth engine gee and other publicly available datasets were used to collect variables for the experiment all variables are shown in table 1 geographic information system gis data include land use and landform categories remote sensing and dem derived variables include surface reflectance vegetation biomass parameters such as normalised difference vegetation index ndvi and enhanced vegetation index evi surface temperature and topographic indicators the bioclimatic variables from worldclim https www worldclim org data index html include temperature and precipitation data the soil properties database was derived from soilgrid all variables were normalized prior to modelling digital elevation model data was obtained from gee the dataset nasa dem 30 m was created by reprocessing srtm data and adding supplementary information from the aster gdem icesat glas and prism datasets to increase accuracy the system for automated geoscientific analyses saga gis platform was then used to calculate several morphometry and hydrology related topographic indices namely topographic position index tpi multiresolution index of valley bottom flatness mrvbf mrrtf terrain ruggedness index tri topographic wetness index twi slope and aspect these variables were then resampled into five spatial resolutions 30 90 250 500 and 1000 m in the arid zone the change of land use and the fluctuation of groundwater level are closely correlated and are eagerly noticed scanlon et al 2005 therefore a global 30 m surface land use cover product finer resolution observation and monitoring from made by tsinghua university and published in 2017 gong et al 2019 was selected and downloaded from https data ess tsinghua edu cn this product provides seven land use categories irrigation field shrub grass lake salt affected soil rock and gravel wetland bare land gobi city and desert chai et al 2009 provided a technique for the geomorphologic surface regionalisation of xinjiang using srtm dem 90 m and landsat tm 30 m data this dataset 30 m resolution was downloaded from the national earth system science data center 2009 the final product divided the xinjiang topography into six geomorphic zones 23 geomorphic subzones and 195 geomorphic plots the study used the gee platform to calculate surface reflectance and the vegetation variables that reflected surface hydrology based on long time series data from landsat 8 oli the variables involved in b1 b2 b3 b4 b5 and b7 tcw tcg and tcb represent wetness greenness and brightness respectively derived from the tasselled cap transformation evi ndvi and generalised difference vegetation index gdvi the time period covered by these variables was the entire growing season april to september in 2018 each index involved in the calculation included the minimum 5 maximum 95 and mean values from five spatial resolutions 30 90 250 500 and 1000 m several surface temperature time series characteristic parameters were computed based on the moderate resolution imaging spectroradiometer modis surface temperature day and night dataset from the gee platform they were employed to describe the feedback of various surface changes for hydrothermal feature the parameters included the annual mean surface temperatures during the day lst day and night lst night the average day surface temperature in march subtracted from the surface temperature in april dlst3 4 dlst4 5 dlst5 6 dlst7 8 dlst8 9 and the average night surface temperature in march subtracted from the surface temperature in april lst3 4 lst4 5 lst5 6 lst7 8 and lst8 9 instead of using the mean annual temperature and rainfall as climate proxies bioclimatic variables that had greater environmental relevance were used to characterize hydrological changes nineteen bioclimatic variables from worldclim bio1 bio2 bio3 bio19 hijmans et al 2005 were also used as potential predictors and were calculated using monthly temperature and rainfall data from 1970 to 2000 these yearly trends such as mean annual temperature and precipitation and extreme or limiting environmental circumstances are represented by biologically significant variables such as temperature during the coldest and hottest months and precipitation during the rainy and dry seasons please visit https www worldclim for further detail soilgrid provided eight soil products with global coverage organic carbon soil bulk density cation exchange ph sand content clay content and available water content each product contained seven standard depths 0 5 15 30 60 100 and 200 cm hengl et al 2017 with a spatial resolution of 250 m the value of a specific product participating in the modelling was calculated by averaging seven depths at that location 3 3 methods 3 3 1 base learner the most common base learners in studies on ensemble learning in hydrology related domains are tree model structures 42 followed by artificial neural networks 30 according to zounemat kermani et al 2021 therefore we employed two algorithms for tree model structures m5and random forest rf and two techniques for neural network types gaussian radial basis function networks grbfn and multilayer perceptron neural network mlp m5 depending on the input data the m5 tree regression technique can be used to develop useful regression models quinlan 1992 it can predict the nonlinear characteristics of data more accurately than conventional linear regression holmes et al 1999 this white box method makes it easier to describe the contribution of variables to the research outcomes compared with black box algorithms such as neural nets the crucial parameter of this algorithm minnuminstances was set to 4 witten et al 1999 rf rf is a powerful and versatile supervised machine learning algorithm that grows and combines multiple decision trees to create a forest breiman 2001 it can solve both classification and regression issues the forest chooses the mean of all tree outputs when performing regression its main practical advantage is that it automatically corrects decision trees propensity to overfit their training set the problem of overfitting is almost eliminated when using the bagging approach with random feature selection which is excellent because overfitting results in erroneous results furthermore rfs often retain accuracy even without certain data maxdepth unlimited numfeatures sets the number of randomly chosen attributes when int log 2 predictors 1 is used for this study we set numiterations 100 grbfn grbfns are an effective method for learning detailed input output mappings a grbfn is typically described as a monotonic function of the euclidean distance between every point in space and a specific centroid frank 2014 this is a scalar function that is symmetric along the radial direction its function assumes a small value when a point in space is far from the centre complex functions can be fitted by modifying the local function value and range of action the most popular grbfn which has numerous applications in computer vision artificial intelligence image compression and data fitting is the one we employed in this research the main parameters were set by default the scale optimisation option was set to use scale per unit and attribute rige 0 01 seed one tolerance 1 0 e 6 frank 2014 mlp an mlp is a feed forward neural network augmentation hornik et al 1989 that comprises three layers input output and hidden the input layer receives the input signal to be processed prediction and categorisation are examples of operations that fall under the purview of an output layer an arbitrary number of hidden layers between the input and output constitute the mlp s true computational engine like a feed forward network the data moves forward from the input to the output layer of an mlp the mlp neurones were trained using the back propagation learning method mlp is used to address problems that cannot be resolved linearly and approximate any continuous function the main parameters were set by default as follows hiddenlayers attribs 2 learning rate 0 3 and momentum 0 2 taud and mas 2018 3 3 2 ensemble algorithm 3 3 2 1 additive regression gradient boosting constructs additive regression models which is accomplished by sequentially fitting a simple parameterised function base learner to current pseudo residuals using least squares at each iteration the pseudo residuals are the gradient of the loss function that is minimised for the model values at each training data point evaluated at the current step adding randomness to the process can significantly increase the approximation accuracy and execution speed of gradient boosting a subsample of the training data is randomly selected without replacement from the entire training dataset at each cycle the base learner is then fitted to this randomly chosen subsample and the model update for the current iteration is computed instead of using the entire sample this randomised approach also increases robustness against the overcapacity of the base learner friedman 2002 the main parameters were set by default the number of iterations 100 and shrinkage 1 0 3 3 2 2 bagging bagging predictors are methods that generate multiple versions of a predictor and use them to obtain an aggregated predictor when forecasting a numerical outcome aggregation averages all variants and when predicting a class it performs a plurality vote many versions are generated by creating bootstrap copies of the learning set and using these as new learning sets bagging can result in significant improvements in accuracy according to tests on actual and simulated datasets utilising classification and regression trees as well as subset selection in linear regression bagging can increase accuracy if perturbing the learning set can result in significant changes in the built predictor breiman 1996 the main parameters were set by default as follows numexecutionslot 1 numiterations 100 3 3 2 3 dagging ting and witten 1997 examined the technique of stacked generalisation in mixing models produced by a single learning algorithm from several subsets of a training dataset and other methods the influence of the sampling regime used to produce training subsets has already been investigated in this context when bootstrap samples are used the approach is termed bagging and dagging for disjunct samples the simplest way to integrate predictions from competing models is through the majority vote in this study we expanded these investigations to stacked generalisation combining the models using a learning technique this results in two new techniques known as bag stacking and dag stacking 3 3 2 4 random subspace splitting criteria and tree size optimisation have received considerable attention when discussing decision trees ho 1998 the dilemma between overfitting and achieving maximum accuracy has seldom been resolved a proposed method to construct a decision tree based classifier that maintains the highest accuracy on training data and improves the generalisation accuracy as it grows in complexity the classifier is composed of many trees that were built systematically by pseudo randomly choosing subsets of the feature vector component or trees built in randomly selected subspaces experiments on publicly accessible datasets were used to compare the subspace approach against single tree classifiers and other forest construction techniques proving the method s superiority the main parameters were set by default as follows numexecutionslots 1 numiterations 100 seed 1 and subspacesize 0 5 3 3 2 5 rotation forest rodriguez et al 2006 presented a feature extraction based strategy for constructing classifier ensembles the feature set was randomly divided into k subsets k is an algorithm parameter and principal component analysis was performed on each subset to generate training data for a base classifier all primary components were kept to maintain the variability information in the data consequently k axis rotations occur to produce additional features for a base classifier the rotation strategy was intended to promote individual accuracy and variety within the ensemble feature extraction for each base classifier promotes diversity decision trees were chosen because they are sensitive to the rotation of the feature axes hence the name forest the main parameters were set by default as follows maxgroup 3 mingroup 3 numberofgroups 1 numiterations 100 removedpercentage 50 and seed 1 recursive feature elimination rfe by genuer et al 2010 was used to find the smallest dataset from all covariates that is the dataset with a minor relative error in the variable combination rfe determines the smallest dataset by employing all variables at the start of the modelling and then eliminating the last variable in the order of contribution from each iteration this modelling method was repeated until the final covariate was reached this technique employs coefficient of determination r2 and root mean square error rmse to assess the ideal small dataset which has the highest r2 and lowest rmse values camera et al 2017 advocated combining the rfe with rf to produce more consistent and dependable forecasts therefore this study used the same strategy the modelling strategies listed below were used for the four ecosystems to predict groundwater level previous studies have demonstrated that the application of scale dependency has been shown to significantly increase the mapping accuracy of soil salinity in regions with considerable spatial variability huang et al 2015 pachepsky and hill 2017 wei et al 2021 the groundwater depth in the study region determines how severely the soil is salinized li et al 2007 thus the scale impact may improve the prediction accuracy of the groundwater level in the study area furthermore we also investigated the ensemble algorithm s potential to simultaneously increase the prediction accuracy for this purpose we developed the following procedure fig 4 first four datasets ecosystems were established based on the geographical location of the samples and land use characteristics namely the otd nlos os and ia fig 3 this was done to explore the main factors of the different ecosystems that controlled groundwater levels and to examine the effectiveness of ensemble algorithms at various scales five spatial resolution datasets in improving prediction accuracy second we analysed the influence of ensemble algorithms and scales on groundwater level prediction in different ecosystems for this purpose 24 algorithms were used for modelling including four base algorithms mlp grbfn m5p and rf and five combinations of ensemble and base learners 20 groups by mode of one to one integration such as additive regression rf additive regression mlp additive regression grbfn and additive regression m5p the five datasets used in the modelling corresponded to five spatial resolutions 30 90 250 500 and 1000 m climate variables modis temperature products land use and geomorphology were maintained at their original scales the variability between the datasets was dominated by landsat and dem derived variables with different resolutions finally the study selected the best combination base learner ensemble algorithm specific spatial resolution dataset based on the results of error analysis to map the spatial distribution of groundwater level in the different ecosystems 3 4 validation owing to the limited amount of data provided the predictive abilities of the ensemble algorithms and base learners were examined using the k fold cross validation method in this method the training dataset was randomly partitioned into five subsets with four partitions containing 80 of the data used to train the learner and the remaining 20 for validation because the approach involves significantly more computing effort than a simple train and test hold out procedure this validation test was reliable and impartial for smaller datasets heung et al 2016 this approach was deemed superior to the standard leave one out cross validation technique by assessing the predictive capabilities of the regression models over a larger number of samples the rmse and r2 values were used to evaluate the model s performance these statistical parameters and related calculations were performed in graphpad prism 7 0 and origin 8 5 r 2 i 1 n o i o p i p i 1 n o i o 2 0 5 i 1 n p i p 2 0 5 2 rmse i 1 n o i p i 2 n 1 where pi refers to the model predicted value at sampling site i and p is the mean value r2 values close to 1 indicated that the model effectively explained the variance of the observations rmse values close to 0 indicated accurate model predictions 4 result and discussion 4 1 statistical characteristics and spatial variability of groundwater levels in typical ecosystems the mean values of the corresponding groundwater level for each artificial and natural ecosystem were calculated table 2 for all samples the minimum maximum and mean values were 0 79 150 26 and 12 25 m respectively with an extreme spatial variability coefficient of variability equal to 1 87 subsequently we divided the samples into otd nlos ia os crop highland crop plain saline soil saline desert and desert based on google maps apparent electrical conductivity maps wang et al 2021a land use and landsat oli actual colour synthetic images and our local knowledge over many years the mean value of each dataset showed that the crop highland subsurface level was the largest 18 84 m followed by nlos and the minimum value occurs in saline soil 3 89 m saline desert ecotone and desert had mean groundwater levels of 7 69 and 7 70 m respectively in a study of the daryaboyi oasis in the centre of the taklamakan desert imin et al 2021 found that a groundwater level of 2 1 4 3 m may guarantee more active photosynthesis of plants groundwater level cannot be 7 m to guarantee the normal development of riparian woody plants according to li et al 2007 groundwater levels 5 59 m and soil salinity 1 61 g l would significantly impede the expected growth of t ramosissima groundwater levels in os cv 1 88 and nlos cv 1 26 exhibited very high spatial variability whereas the remaining geographic areas showed moderate spatial variability considering the importance of groundwater for oases natural ecosystems and irrigated areas and the number of samples we then used otd os nlos and ia as the main geographic areas for testing the effect of ensemble algorithms and scale on the prediction accuracy of groundwater level groundwater level influences the fate of ecosystems in dry zones and fluctuates with it to produce different landscape characteristics in this study the mean value of groundwater level in the irrigated agricultural region 4 66 m in plain and 18 84 m in baicheng oasis plain was greater than the results of wang et al 2021b who assessed the mean value of groundwater level from 2000 to 2018 as 4 10 m research showed that irrigated agriculture that is heavily reliant on groundwater leads to overexploitation wang et al 2021b which in turn depletes the groundwater table this supports that the main causal factor for the continuous decline of the groundwater level is the large amount of groundwater consumed by the expansion of the oasis in recent decades sun et al 2011 additionally the groundwater in the tarim basin also showed a decreasing trend between 2003 and 2016 with a significant decline in south tianshan zuo et al 2021 the same phenomenon was observed in the hexi corridor of china chen 2010 and western australia hu et al 2019 data for the study were collected during cropping from april to june when groundwater was required for salt washing and irrigation which may have further lowered the groundwater level fig 5 shows the statistical characteristics of evi at different groundwater levels revealing that the interval of 6 8 m was the extreme range for vegetation to survive whereas only specific deep rooted vegetation can survive at 8 10 m the localisation found that these observations were mainly in the desert area and adjacent to the oasis desert transition zone the areas with higher vegetation cover are located in the areas with groundwater levels 6 m and 10 m respectively attributed to irrigation and rainfall at higher elevations such as the baicheng oasis respectively historical studies in this region indicated that the basic groundwater table range for plants in the lower tarim river basin was 0 6 m and the diversity of species was greatest between 2 and 4 m drastically declining below 6 m hao et al 2010 this is consistent with our findings demonstrating the reliability of the sampling results regional fluctuations of the groundwater level was present in otd nlos and ia consistent with the strong spatial variability of soil salinity in the study area this is because the groundwater level is a crucial determinant of the occurrence of salinisation within the region wang et al 2021a 4 2 groundwater level prediction for different ecosystems fig 6 shows the prediction error rmse of groundwater level under different predefined conditions the nlos and otd ecosystems had the highest prediction accuracies with r2 and rmse values of 0 96 and 0 92 and 5 66 m and 6 09 m respectively this was followed by ia with r2 and rmse values of 0 86 and 3 78 m respectively meanwhile os had a 50 explanatory power of spatial variability with an rmse value of 3 49 m the lowest rmse for otd was obtained using a dataset with a spatial resolution of 30 m and a mean value of 10 86 m the minimum rmse produced without the ensemble algorithm by the base learner was 7 00 m with r2 0 94 this result was obtained from the grbfn algorithm with a 1000 m spatial resolution dataset the ensemble algorithm performance of the five machine learning methods at different spatial resolutions revealed that rotation forest computed the comparatively least rmse with a mean value of 10 02 m followed by random subspace and dagging with mean values of 10 20 and 19 28 m respectively ratationforest mlp which was supported by a 30 m spatial resolution dataset was the top performing ensemble algorithm for nlos this algorithm predicted an rmse of 5 66 m with an r2 of 0 98 comparatively randomsubspace mlp achieved a lower rmse of 5 91 m with r2 0 96 supported by same dataset the best performing ensemble algorithm for ia was the additive regression grbfn supported by a 250 m spatial resolution dataset with an rmse of 3 78 m and r2 0 93 the best performing base learner learning algorithm was rf 250 m with a predicted rmse of 4 43 and r2 0 91 under different conditions random subspace was generally the best partner for all base learners in the ia region with a mean rmse of 5 69 m followed by rotation forest with the worst being dagging for os additive regression rf was the best ensemble technique for forecasting groundwater levels which performed slightly better than rf using a 90 m spatial resolution dataset additive regression rf 90 m predicted an rmse of 3 49 with r2 0 50 whereas rf 90 m had an rmse of 3 59 m however comparing the performance of the five ensemble algorithms under the background of five spatial resolution datasets revealed that random subspace had the best ensemble performance with a mean rmse of 4 26 m followed by bagging and the worst performer was additive regression without the assistance of ensemble algorithms the mean rmse of the groundwater level predicted by the four base learners and supported by five spatial resolution datasets was 4 87 m after all ecosystems were evaluated the os region had the lowest prediction accuracy this could be explained by the poor link between environmental factors and groundwater levels in the area the sustainability of the artificial oasis in the os ecosystem relies mainly on irrigation shallow rooted vegetation which differs significantly from the survival strategy of plants under the natural environment we speculated that the combined data from the two regions artificial oasis and natural ecosystems lowered the groundwater aboveground landscape relationship 4 3 impact of ensemble algorithms on prediction accuracy the data statistics showed that the most optimal ensemble algorithm was influential in predicting the groundwater level in the study area fig 6 first we used the mean rmse value as a benchmark to evaluate the performance of the four base learners the best performing algorithm according to the 20 rmse statistics scenarios for the five spatial resolution datasets from four regions was rf rmse 6 49 m followed by grbfn rmse 6 75 m mlp rmse 8 31 m and m5 rmse 10 26 m this is because rf itself is an ensemble algorithm however in many instances in our investigation grbfn performed similarly and had superior computational accuracy however the advantage of rf over grbfn is that it requires fewer parameters to be set subsequently a total of 84 rmse statistics from the five spatial resolutions datasets of the four regions confirmed that random subspace was the best ensemble algorithm compared with the base learner with an average reduction in rmse percentage of 9 22 followed by rotation forest 6 74 additive regression 2 94 and bagging 2 78 and the worst one was dagging 42 55 fig 7 a among them random subspace rotation forest and additive regression improved the prediction accuracy of the base learner by 76 72 and 64 followed by bagging meanwhile dagging increased the possibility of prediction errors by comparing the four ecosystems we found that random subspace was more stable and improved the rmse of all systems followed by rotation forest and additive regression bagging s overall performance was average however in terms of error reduction it was comparable to that of rotation forest and random subspace the largest error reduced by bagging in nlos os and ia were 28 21 36 63 and 26 67 respectively whereas rotation forest and random subspace reduced the errors by 34 60 and 37 11 37 06 and 29 80 and 25 85 and 23 69 respectively finally we analysed the efficiency of cooperation between the base learners and ensemble algorithms the results showed that additive regression was best paired with rf rmse 6 10 random subspace with grbfn rmse 6 55 m bagging with mlp rmse 6 90 m and rotation forest with m5 rmse 8 21 m additive regression rf had the best predictive power among the ensemble algorithms furthermore in terms of mean rmse additive regression was the only ensemble algorithm that improved the prediction error of rf rmse 6 48 m whereas all other ensemble algorithms such as bagging rmse 7 06 m dagging rmse 11 97 m random subspace rmse 6 54 m and rotation forest rmse 6 61 m failed to improve the prediction error of rf with all base learners additive regression had the highest probability 75 of obtaining optimal prediction accuracy within the four ecosystems in summary rotation forest and random subspace provided relatively low prediction errors with high probability and were distinguished by their steady performance in various ecosystems additive regression helped base learners obtain relatively optimal solutions with high probability however no single algorithm perfectly optimised all the error problems multiple algorithms must be applied and validated for testing to determine the most efficient one khosravi et al 2018a 4 4 effect of different scale data sets on prediction accuracy the accuracy of groundwater level estimation in various ecosystems was influenced by five datasets with different spatial resolutions fig 7b we calculated the average values of 24 rmses at five resolutions in four ecosystems which included the base learner four calculation scenarios and ensemble learning algorithms 20 calculation scenarios the magnitudes of the effect of five datasets with different spatial resolutions on the accuracy of groundwater level prediction maximum value of rmse compared with the minimum value were 11 48 21 63 10 59 and 26 73 for otd nlos os and ia respectively and their respective optimal spatial resolutions were 500 30 90 and 250 m however the above statistical results did not represent the spatial resolution used for the highest prediction accuracy nor indicated that the optimal solution can be obtained with the support of the spatial resolution dataset the probability of the optimal solution was higher with the support of the spatial resolution dataset for example the optimal scales for obtaining optimal precision for different ecosystems were 250 m otd 30 m nlos 90 m os and 250 m ia whereas the optimal scale for otd was not from the dataset with spatial resolution of 500 m however the results demonstrated that scale factors can improve the accuracy of groundwater level predictions in different ecosystems the findings from the four ecosystems showed the effect of the scale on the groundwater level and surface environment consistent with many other studies that have found that scale affects the relationship between soil properties and the environment behrens et al 2010 guo et al 2018 miller et al 2015 this may be because groundwater is the precondition of oasis existence and its spatial and temporal variability is essential in determining the spatial heterogeneity between soil properties and environmental characteristics notably the optimal interpretation scale adapted to the mods system such as the otd is not the optimal scale for other ecosystems the optimal scale of groundwater level prediction changes depending on the ecosystem we correlated the scale and prediction accuracy with the spatial variability of the groundwater level and found no significant correlations between them implying that numerous attempts should be performed to obtain an ideal scale 4 5 prediction error of the optimal ensemble algorithm at the threshold depth of the groundwater table fig 8 and table 3 show the accuracy of the predicted groundwater levels at various depths in the four ecosystems according to the literature the groundwater levels at 4 and 8 m are the fuzzy threshold values for salinisation and desertification respectively hao et al 2010 sun et al 2011 using this as a guideline this study investigated the performance of the optimum model at 0 4 4 8 0 8 and 8 m except for the os system the results revealed that groundwater levels can be forecasted with higher accuracy at depths 8 m than at depths 8 m we discovered that these observations were mostly located in natural habitats outside the irrigation region and were tightly connected to topography and geomorphology the mean predicted values of all ecosystems were higher than the observed values for groundwater depths 4 m table 3 the randomness of the scatter between the predicted and observed values within this range is shown in fig 8 the reason for this might be that this depth 4 m occurs in the saline zone of the nlos the oasis desert transition zone and irrigated areas the vegetation types within the saline zone are homogeneous and have low coverage while the vegetation within the oasis desert transition zone which can reflect the groundwater level is composed of multiple communities and has a low remote sensing spectral separation in addition not all vegetation is groundwater dependent the utilisation of various water sources by desert vegetation during various growing seasons suggests that roots are selective in their uptake of water sources during different periods bahejiayinaer et al 2018 in summer phragmites in dune habitats mainly use groundwater salt spiraea in salt marsh habitats use soil water and white spiraea in desert habitats shift their water use strategy from surface to deep soil water hao and li 2021 in contrast in agricultural cultivation areas where irrigation is routine the groundwater level does not influence the geographical variance of economic crop greenness we also attempted to forecast the groundwater level in the desert however because of the uniform texture undulating dunes and lack of vegetation determining the groundwater level using the current dataset was more challenging therefore the results were excluded from the study in these locations the combined impact of environmental factors on groundwater is complicated bekele et al 2019 the means of the predicted and observed values were similar when the observed groundwater depth was between 4 and 8 m however the r2 values were not high similar to the predictions within 0 8 m all forecasted values fell within the threshold range in summary the use of data with various spatial resolutions and sub regional modelling is recommended to boost the groundwater prediction accuracy according to the comparison of the prediction errors at various depths especially when the scale of the research area is large and its land cover type is more diverse 4 6 influencing factors in diverse habitats the combination of topography land use and climate control the groundwater fluctuations that vary depending on ecosystem features fig 9 geomorphology dem 250 m and climatic variables wc01 wc13 wc15 had the most significant influence on the overall trend of the groundwater level in the otd although temperature and evaporation significantly impacted variation of groundwater level precipitation has a comparatively limited impact on changes in groundwater levels zuo et al 2021 for the os area the spatial variation in the groundwater level resulted from the interaction between geomorphology land use and mrvbf 90 m in which geomorphological features dominated among all variables climate variables such as the maximum temperature of the warmest month and other meteorological parameters had a greater influence than other factors in the nlos region followed by elevation dem30m the dem at 250 m dominated the entire groundwater level in the irrigated region groundwater level changes in the studied region were generally driven by geomorphology and the dem therefore we used otd as an example to investigate the response relation between these two and the partial variable dependent groundwater level in fig 10 a when the altitude is 1025 m the groundwater table is relatively shallow and does not exhibit a clear trend however when the altitude is between 1025 and 1200 m the groundwater depth exhibited an increasing trend and raised quickly between 1200 and 1240 m mid elevation alluvial floodplain after which 1240 m the groundwater table did not change this may be the result of insufficient observations fig 10b illustrates that the middle elevation alluvial floodplain 121341 middle elevation river valley plain 121360 middle elevation flowing gently undulating sandy area 123132 middle elevation dry floodplain 123220 and middle elevation dry floodplain 123221 had deeper groundwater levels in contrast the low elevation alluvial fan plain 111334 middle elevation alluvial depressions 121336 and erosion and deposition have relatively shallow groundwater levels the pearson correlation between the preferred variables and the groundwater level is presented in table 4 for otd dem250m showed the highest correlation value with the groundwater level r 0 71 followed by slope250m r 0 52 and wc12 r 0 36 the mean annual temperature wc01 showed the highest negative correlation r 0 60 followed by mrvbf250m r 0 52 the groundwater level decreased significantly with decreasing elevation bekele et al 2019 the correlations between the preferred environmental variables and groundwater levels were low within the os system regarding preferred environmental characteristics inside nlos wc11 r 0 40 and tcw90m r 0 32 had the strongest correlations the groundwater level and dem showed the greatest positive correlation with r 0 90 followed by wc12 and the greatest negative correlation indicated by wc01 with r 0 88 and wc05 with r 0 83 within the ia system the largest positive correlation value was dem250m r 0 63 followed by slope250m and the maximum negative correlation was mrvbf250m with r 0 53 4 7 spatial distribution characteristics of groundwater levels the spatial distribution characteristics of the groundwater levels matched the land use and geomorphological patterns of the study area fig 11 the groundwater level of the baicheng oasis was deeper compared with other regions due to its high altitude in contrast the groundwater level of the irrigation area was relatively shallow maintaining a groundwater level of 4 m groundwater levels in the ecotone between the desert and the oasis ranged from 6 to 8 m but increased with elevation as seen in the eastern half of the kuqa oasis and its western border with the aksu salt affected land the actual predictions were in the range of 6 8 m whereas the mean value of the groundwater level in the surroundings of the kuqa oasis in the northern end of the southern taklamakan desert was approximately 8 m the predicted and observed values were within comparable intervals in the region where the sample sites were located in the southern taklamakan desert the link between the observed and predicted values was further examined and the prediction accuracy was found to vary depending on the depth range the probability that the predicted and actual values fell within the 4 8 m depth range was 87 57 93 47 92 91 and 92 10 for otd nlos os and ia respectively in contrast using the predicted values in the range of 4 8 m as a reference the probability of the actual values that fell in this range were 57 37 for otd and 62 07 for os here only these two ecosystems were used as examples the probability of the predicted values between 0 and 8 m was 92 39 95 45 91 81 and 99 01 for otd nlos os and ia respectively in contrast the probability that the predicted values were set within 0 8 m and the actual values fell within this range were 85 35 for otd and 91 80 for os finally for predictions deeper than 8 m the predicted values indicated a confidence level of 76 42 for otd and 83 87 for os in conclusion the confidence level of the predicted groundwater depths was greater in the ranges of 0 8 m and 8 m than at the other depths when utilising the observed and forecasted values as references the findings of these respective depth ranges assisted us in recognising differences in the actual scenario 4 8 uncertainty and limitations the unsatisfactory precision of the available data and a lack of important soil vegetation factors that were directly connected to groundwater may restrict the predicted accuracy of the groundwater level groundwater samples were collected over two months for this investigation possibly affecting the time dependence of the groundwater level subsurface relationship because the sample could not be promptly collected we examined data from observation wells in the kuqa oasis from 1997 to 2003 and discovered that intra annual groundwater level changes varied from 0 44 to 2 33 m with a mean value of 1 17 m however because the range of intra annual groundwater level changes in the desert oasis transition zone was unknown the precise degree of the impacts above cannot be determined additionally land use data were mapped for 2015 which may have reflected the land use pattern of an earlier period furthermore the prediction accuracy of 0 4 and 0 8 m could have been better enhanced if soil related data with greater spatial resolution were available such as soil moisture and hydraulic characteristics the analysis of variations in groundwater levels benefits significantly from the more specific categories of soil properties fu et al 2019 existing soil moisture data with a poor spatial resolution 25 km are of limited use in the prediction aims of the current study high precision soil hydraulic properties such as bulk weight soil texture organic carbon wilting point saturated water content field water holding capacity and agricultural drainage data are in short supply fu et al 2019 kim and jackson 2012 zhang et al 2014 these characteristics provide clear evidence of the hydrological relationship between groundwater and the surrounding environment in anthropogenic and natural oases jiang et al 2015 wang et al 2014 a hydrogeological map considerably impacts groundwater level although it can be challenging to collect erler et al 2019 in contrast information regarding the development of the groundwater cycle and hydrogeochemistry can be obtained from the geographical and temporal distribution of groundwater levels 5 conclusion the spatial variability of groundwater level in the typical mods of arid zones is unknown due to the extreme lack of observational data to this end the study used explored the spatial distribution patterns of groundwater levels in mods in the tarim basin in central asia using limited observational data the study simultaneously examined the effects of ecosystem characteristics of four typical regions otd ia os and nlos ensemble algorithms and five spatial resolution datasets to obtain the best groundwater level prediction accuracy in otd ia and nlos the best model exhibited 90 explanatory power whereas os only explained 50 of the spatial variability random subspace rotation forest and additive regression improved the prediction accuracy of the base learner by 76 72 and 64 respectively dagging with other base learners indicated a high probability of increasing the error rotation forest and random subspace combined with base learners generated relatively low prediction errors with a high probability and exhibited steady performances in the four ecosystems base learners produced nearly optimum outcomes with high probability using additive regression in the four ecosystems the scale factor increased the prediction accuracy of groundwater level by 10 59 26 73 compared with the range of 8 m the optimum model performed better in the range of 8 m future research should address the limited prediction accuracy of shallow groundwater level 8 m by adding multiple data sources such as geologic data multi period runoff data multi period microwave retrieved soil moisture datasets with high spatial resolution soil hydrologic conductivity spatial interpolation based on field collected data and fine scale vegetation types crops and natural vegetation communities as vegetation is an essential prerequisite for maintaining the survival of oases in dryland the correlation between groundwater level and above ground vegetation should also be explored covering the influence of vegetation type biomass vegetation growth fluctuations and vegetation phenology on groundwater levels credit authorship contribution statement yang wei investigation methodology writing original draft fei wang supervision formal analysis conceptualization writing review editing bo hong investigation formal analysis shengtian yang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the national natural science foundation of china 42101363 and u1603241 we are very proud to have been invited to participate in the environmental assessment project and we also thank the truck mounted drilling operators and samplers who worked in the difficult conditions finally we thank the reviewers for their comments and suggestions 
2033,glaciers are one of the main sources of freshwater in cold regions the glacier melting process can significantly impact the glacier mass balance gmb and contribute a large amount of runoff in cold regions this study applied the recently developed semi distributed glacio hydrological conceptual model flexg to understand the glacier melting process and the effect of topography on gmb in the torne river basin northern sweden the study simulated glacier and snow accumulation and ablation as well as runoff from the glacier and non glacier areas of the basin using the flexg model for the time period 1989 2018 the flexg model considers the influence of topography on runoff generation and in this study the basin was classified into 143 zones depending on elevation and aspect in order to gain a comprehensive view of the performance of the flexg model the classical lumped hydrological model hbv was used and compared with the flexg model in simulating total streamflow and peak runoff at the outlet of the basin our results revealed that the flexg model performed well in reproducing the streamflow also better than the hbv model with metric kling gupta efficiency kge of 0 80 and 0 71 for the calibration and validation periods respectively we also found that the flexg model performs better in peak runoff simulation than the hbv model the flexg simulated snow cover area proportion agreed well with the modis satellite snow cover product r2 0 60 and rmse 28 the gmb in different elevation zones was simulated and a downward trend was found for gmb changes during the study period because of climate change keywords arctic region catchment hydrology glacier hydrology hydrological model runoff simulation satellite data data availability data will be made available on request 1 introduction glaciers as a part of important water resources in cold regions play a remarkable role in various research areas such as hydrological modeling glaciological studies agricultural climatological and downstream ecosystem studies gao et al 2021b hock 2005 yang et al 2012 glaciers as natural water reservoirs reduce streamflow variability stabilize water resources and protect large populations from drought stress especially in arid regions pritchard 2019 gao et al 2021a the meltwater of glaciers plays a key role in streamflow quality and quantity singh et al 2021 therefore understanding the spatial and temporal dynamics of glaciers and the contribution of meltwater to runoff generation are essential for many application such as hydrological modeling and prediction water resources management and ecosystem assessment in cold regions cooper et al 2011 verbunt et al 2003 liu et al 2020 in this regard ideally long term measurements of glacier extent glacier meltwater and total streamflow are needed however unfortunately in situ measurements are often limited in many cold regions across the world mainly due to inaccessibility and harsh environments duethmann et al 2015 hydrological models are based on the simplified representation of the complex water cycle and hydrological processes hydrological models can be used to extrapolate limited available measurements in both space and time to improve our understanding of hydrological processes beven 2011 these models provide one of the popular ways to predict streamflow under environmental and climate changes in different regions rintis and setyoasri 2016 abebe et al 2010 duan et al 2019 hydrological models have been widely recognized as an essential tool to support water resources management controlling water structures designing urban water systems irrigation planning hydropower designing and flood risk assessment etc gichamo and tarboton 2019 uhlmann et al 2013 therefore development of hydrological models is vital for hydrological agricultural ecological and climatological studies song et al 2020 munzimi et al 2019 given the important role of glaciers and extensive snow cover in cold regions ayala et al 2020 gao et al 2020 dynamics of glaciers and snow and their hydrological processes should be sufficiently represented in hydrological models in order to achieve reasonable streamflow simulation in recent years many efforts have been made to test and develop various hydrological models for streamflow simulation in different glacierized regions for example the hbv hydrologiska byråns vattenbalansavdelning model was employed for analyzing the effect of climate change on the runoff and glacier changes in yushugou basin china zhou et al 2022 the hbv model was also used to estimate the snowmelt and its contribution to the total streamflow in the panchachuli glacier region in central himalaya rautela et al 2022 wang et al 2020 coupled a modified version of the hbv hbv d by an improved temperature index approach for gmb and runoff simulation in the tianshan mountains china the hbv model has also been successfully used to quantify glacio hydrological variables such as gmb kong et al 2021 rainfall runoff glacier runoff and snow runoff su et al 2021 in different regions zhao et al 2019 used the variable infiltration capacity macroscale hydrological model vic cas for the monthly streamflow simulation in a glacierized basin in the tibetan plateau van beusekom and viger 2016 integrated the glacier module into the precipitation runoff modeling system prms to develop the prms glacier model for streamflow simulation this model was successfully tested for streamflow simulation in two glacierized basins in alaska gao et al 2018 developed a semi distributed hydrological model flexg for glacierized basins this model uses the temperature index method distributed by topographical data to divide the entire basin into glacial and non glacial areas for calculating runoff in the glacier areas the xinanjiang storage capacity curve was employed in the flexg model for calculating runoff formation in the non glacier area gao et al 2017 the flexg model has been successfully tested and found to perform well in streamflow simulation in many mid latitude and high altitude glacierized regions in china gao et al 2017 2018 many other models were successfully applied in a number of glacierized basins in different parts of the world brown et al 2014 burger et al 2019 chen et al 2019 certainly each hydrological model has its own strengths and weaknesses the most important feature of a hydrological model for glacierized basins is its ability in 1 reasonable representation of various hydrological processes particularly from glaciers and snow 2 the separation of runoff contribution from different sources e g the glacier and non glacier parts and 3 the validity of simulated variables by observations after literature reviews and initial comparison of the features of different hydrological models the flexg model stands out because it integrates snow and glacier processes and can generate many outputs including runoff simulation in glacier and non glacier parts snow water equivalent swe snow cover area proportion sca gmb etc gao et al 2017 2021b previous studies reported the good performance of the flexg model but mostly in mid latitude cold regions mainly in china the flexg model has not been tested in other glacier areas with different climate and environmental conditions and especially not on glaciers in high latitude zones such as the arctic region therefore it is interesting and scientifically relevant to evaluate the flexg model in other regions this study aims to do so in the torne river basin in northern sweden for the first time the objectives of this study are to 1 evaluate the performance of the flexg model for streamflow simulation and gmb simulation for a 30 year long period 1989 2018 for each elevation zone 2 assess the role of glaciers in runoff generation in the study basin by separating runoff from the glacier and non glacier areas 3 investigate the effects of altitude on gmb spatial distribution 4 simulate swe and sca at a daily scale by the flexg model 5 examine and compare the ability of the flexg model in simulating peak runoff with the classical hydrological model hbv 6 perform the trend analysis of simulated variables over the entire period 2 study area this study focused on the torne river basin which is a glacierized basin in north sweden the basin is located in the latitude 68 8 68 24 north and the longitude 18 3 18 56 east as shown in fig 1 the basin area is 547 6 km2 it is one of the few high latitude research stations with a long time series of measured streamflow data and meteorological variables the digital elevation model dem data shows that the elevation ranges from 342 to 1781 m above the mean sea level the basin is located in the polar climate zone according to the köppen geiger climate classification kottek et al 2006 the basin is located in the permafrost zone and snow and glaciers provide essential water resources in this basin dahlke et al 2012 ploum et al 2019 3 data and methods 3 1 data and data processing the required daily climatic air temperature t and precipitation p and hydrologic data streamflow for the time period 1985 2018 were obtained from the swedish meteorological and hydrological institute smhi https www smhi se the location of the meteorological station and the streamflow station is show in fig 1 the dem data were obtained from european union s earth observation program copernicus https land copernicus eu the european digital elevation model eu dem version 1 1 was used in this study and it is at the spatial resolution of 25 m with vertical accuracy root mean square error equal to 7 m the ice thickness information was obtained from eth zurich s research collection farinotti et al 2019 snow cover area dataset related to modis terra cloud gap filled cgf mod10a1f version 61 remotely sensed data was obtained by national snow and ice data center https nsidc org the torne river basin is located in the arctic region and the minimum maximum and daily average temperatures during the study period are 32 1 c 23 1 c and 0 29 c respectively the amount of minimum maximum and daily average precipitation are 0 mm 61 9 mm and 0 93 mm respectively the minimum maximum and the daily average streamflow in the outlet of the basin are 0 13 m3 s 1 219 m3 s 1 and 14 42 m3 s 1 respectively the entire time series of streamflow data were split into the 3 75 year warming up 1985 1988 15 year calibration 1989 2003 and 15 year validation 2004 2018 there is only one weather station in the studied area and a distributed input force to drive the flexg model is required to achieve this a linear distribution method was used for generating distributed points in the studied basin the precipitation and temperature distribution for each altitude range was determined by interpolating the observation data from the abisko meteorological station the distribution of aspects and elevations in the glacier and non glacier areas in the study basin is shown in fig 2 there are 22 glaciers in the study area fig 3 shows their geographic locations and their ice thickness distribution the required dataset for ice thickness analysis was obtained from farinotti et al 2019 the randolph glacier inventory 6 0 was used for obtaining glacier location area and height rgi 2017 three glaciers rgi60 08 00261 rgi60 08 00460 and rgi60 08 00468 were clipped based on the studied basin boundary the ice thickness map shows that the rgi60 08 00463 is the thickest glacier in the studied basin with ice larger than 70 m flexg considers the lapse rate for precipitation and air temperature changes during the modeling process gao et al 2017 2018 after initial testing and literature review the rates of 5 100 m and 0 4 c 100 m were used for increasing precipitation and decreasing air temperature with elevation change respectively in addition the limitation of using one climate station the only available station may add some uncertainties to hydrological modeling renard et al 2010 including i uncertainty in the interpolation to distributed data via lapse rate in the flexg model input and ii uncertainty in the flexg model calibration and uncertainty in precipitation and temperature may further have an effect on the model parameter setting during the calibration however previous studies reported that using one climate station can have an acceptable result for running distributed hydrological models in small catchments koch et al 2016 maneta et al 2007 due to the influence of altitude and shadows a regional representation based on meteorological point observations is a common problem in mountainous areas hrachowitz and weiler 2011 most meteorological stations are located in low altitudes to ensure suitable access to services stahl et al 2006 which usually results in high distortion of the observed variables because terrain has a significant impact on the distribution and energy balance of spatial forcing data see e g euser et al 2015 it also affects the temporal and spatial changes of glacier and snow dynamics verbunt et al 2003 solar radiation is the primary control for snow ice melting hock 2005 in order to consider the influence of altitude on temperature and runoff generation the basin was divided into 143 elevation zones at the interval of 10 m each elevation zone was further segregated by three azimuth zones in terms of aspects north 315 45 south 135 225 and east west line azimuth 45 135 and 225 315 to account for the effects of aspects on glacier and snow melting and runoff generation considering the different elevations aspects and glacier and non glacial areas the studied basin with 22 glaciers was classified into the glacier and non glacier classes fig 2 3 2 the flexg model 3 2 1 snowfall and snowmelt simulation in the flexg model precipitation is converted to solid precipitation particularly snow ps mm and liquid precipitation particularly rain pl mm according to air temperature t c and the threshold air temperature tt c with equations 1 and 2 1 p s p t t t 0 t t t 2 p i p t t t 0 t t t where tt is an essential parameter to calibrate in the flexg model berghuijs et al 2016 wen et al 2013 kienzle 2008 previous studies showed that tt could be even as low as 5 c in nordic regions seibert 1997 thus we used a wide prior range 6 c to 4 c to find optimal values of tt for testing its influence on the flexg model ability in simulation of hydrograph as well as glacier and snow dynamics hock 2003 singh et al 2000 the snowpack is a porous medium and liquid water can be held by the snowpack fujita and sakai 2014 gao et al 2021a the water holding capacity of the snowpack relies on its physical characteristics particularly the snow depth and density patil et al 2020 mahmoodzada et al 2022 for shallow snowpacks the snowpack may not necessarily retain water but it may release it to the bottom layer which will ultimately have sufficient lubrication to cause sliding down creep similar to the movement of the whole snowpack over the glacier ice beria et al 2018 singh et al 1997 the flexg model uses equations 3 to 7 for modeling the snowmelt process 3 d s w dt p s r rf m s 4 d s wl dt p l m s r rf p e 5 m s f dd t t t t t t 0 t t t 6 p e s wl c wh s w s wl c wh s w 0 s wl c wh s w 7 r rf f dd f rr t t t t t t 0 t t t where sw refers to the solid snowpack swl refers to the liquid water inside the snowpack ms is the amount of melted snow rrf indicates the refreezing water from liquid storage pe refers to the generated runoff from ice soil surface fdd indicates the factor of degree day mm c day 1 tt is the threshold temperature zhang et al 2006 ca is a factor for the effect of aspect on the melting process and cwh is a certain fraction which snowpack can hold the liquid melting water without runoff generation fr r is a correction coefficient used for simulating liquid water refreezing when the air temperature is below tt and the snowmelt was calculated for each elevation zones individually konz et al 2007 seibert 1997 3 2 2 glacier melt simulation the mechanism for simulating glacier melt is based on the temperature index approach which is the same as snowmelt but with a different degree day factor if there is no snow cover then ice begins to melt and there will be a generation process for glacier runoff mg mm day 1 and also qg and qf g show streamflow for the glacier part of the basin l t and subsurface glacier stormflow l t respectively because the ice surface is mostly covered by impurity substance with less albedo the glaciers degree day factor is considered to be larger than snow cover braithwaite and olesen 1989 for calculating the released runoff from glacier areas equation 8 10 the flexg model also needs an assumed linear reservoir sf g and a parameter for recession kf g day zhang et al 2006 gao et al 2017 8 m g f dd c g t t t t t t a n d s w 0 0 t t t o r s w 0 9 d s f g dt p l m g q f g 10 q g s f g k f g 3 2 3 non glacier area rainfall runoff simulation for runoff simulation in non glacier areas rainfall and snowmelt were considered as unsaturated rootzone reservoirs su the amount of runoff is simulated based on the relative water content of the soil su su max and the amount of water entering the soil pe and the actual evaporation ea is estimated based on the relative water content seibert 1997 so snowmelt and rainfall in non glacier areas are simulated by equation 11 gao et al 2017 11 d s u dt p e e a r u pe mm day refers to the effective rainfall the actual evaporation is formulated by ea mm day 1 the relative soil moisture su su max mm ce a free parameter and potential evaporation e0 mm day 1 are used for calculating ea the storage capacity curve is determined by the xinanjiang approach zhao 1992 ru is the amount of runoff generated from unsaturated reservoirs su zhao and liu 1995 12 r u p e 1 1 s u 1 β s u m a x β 13 e a e 0 s u c e s u m a x 14 d s f dt r f q f 15 d s s dt r s q s 16 q f s f k f 17 q s s s k s where the rootzone storage capacity is shown by su max mm β represents a parameter of shape and sf and ss are linear reservoirs also the amount of recharge related to the reservoir s fast response was introduced by rf mm day 1 and it was calculated by ru day the amount of recharge related to the reservoir s slow response was presented by rs mm day 1 and it was calculated by ru qf mm day 1 is subsurface fast runoff qs mm day 1 is groundwater slow runoff kf day is the fast response parameter and ks day is the slow response parameter 3 2 4 implementation of the flexg model the flexg model has 13 parameters and we used the generalized likelihood uncertainty estimation glue beven and binley 1992 for calibrating the flexg model parameters gao et al 2017 a monte carlo sampling strategy was implemented with 10 000 homogeneous distribution parameters and then the best 1 of parameter sets were used for further analysis the best parameter sets are shown in fig 4 the kling gupta efficiency kge gupta et al 2009 was employed as the objective function during the model calibration the result of the monte carlo strategy is provided in section 4 1 since the current study is the first application of flexg in the high altitude domain literature review gao et al 2017 2018 and our preliminary analysis were performed to obtain a reliable range for flexg parameters during the calibration period table 1 presents the used range for the flexg model parameters during the calibration 3 3 the hbv model the hydrologiska byråns vattenbalansavdelning hbv model is a classic lumped hydrological model and it has been widely used for runoff simulation in different areas the hbv was developed at the smhi in the early 1970s bergström 1976 this model has been used to predict runoff and evaluate water resources in scandinavian countries by considering the effect of rainfall and snowfall separately huang et al 2019 li et al 2015 the hbv model is a conceptual model for field scale hydrological processes and it is applicable as a lumped model by dividing the field into several sub fields bergström and lindström 2015 the hbv model works by considering precipitation temperature evapotranspiration snowfall soil storage and the conceptual stage of basin reaction for runoff generation abebe et al 2010 the model has been widely employed to simulate streamflow in the nordic region e g lindström et al 1997 the hbv model requires evapotranspiration air temperature and precipitation as the essential inputs for runoff simulation this model considers three sub routine levels for runoff modeling i the soil moisture stage ii the snow stage iii the runoff response stage more details and descriptions of the hbv model structure can be found in bergström 1995 lindström et al 1997 and seibert and vis 2012 the current study used the hbv model that includes the glacier module the monte carlo calibration method was used with the same kge as the objective function during the calibration of the hbv model the parameter ranges and their optimal values for the hbv model in our study are listed in table 2 3 4 metrics for evaluating the model performance four widely used statistical metrics namely coefficient of determination r2 kling gupta efficiency kge gupta et al 2009 nash sutcliffe efficiency nse nash and sutcliffe 1970 and root mean square error rmse were used in the current study to evaluate the model performance table 3 presents the formulas of the used metrics 3 5 mann kendall trend test trend analysis of the flexg model simulated output components was performed by employing the mann kendall test of prewhitened time series data mk test based on the storch approach von storch 1999 for a better understanding of changes in different hydrological variables the mann kendall trend test was first introduced by mann in 1945 mann 1945 and then developed by kendall in 1966 like other statistical tests the test is based on comparing hypothesis zero h0 and hypothesis one h1 and ultimately decides whether to accept h0 yue and wang 2004 the h0 test is based on randomness and the absence of trends in the data series and acceptance of h1 rejection of h0 indicates the existence of trends in the data series if the z statistic is positive there is an uptrend and if it is negative it is considered a downtrend hamed 2009 4 results and discussion 4 1 calibration of the flexg model parameters the flexg model has 13 parameters to calibrate the initial range of the parameter set is generally consistent with the previous study gao et al 2017 2020 2021a b the model parameters behavior is demonstrated by dotty plots as shown in fig 4 after running the monte carlo approach and after evaluating the performance of the flexg model the optimal values for each parameter are presented in table 1 the calibration process by previous studies showed that the t t tended to be 0 c in urumqi glacier no 1 catchment northwest china gao et al 2017 and 1 c to 0 5 c in dongkemadi glacier tibetan plateau gao et al 2021a while in the current study area the t t tended to be between 4 c to 4 c the coefficient for the recession of glacier runoff k fg was considered as the range of 1 3 in urumqi glacier no 1 catchment and the range of 5 10 in dongkemadi glacier while the k fg of 0 8 was found in the studied area in northern sweden the degree day factor for snow f dd was found between 2 and 10 as suitable range in the current study while f dd was set in the range of 2 7 in yigong zangbu river basin gao et al 2021a 4 2 flexg model performance in runoff simulation streamflow simulation using the process based model may be significantly disputing due to complex physical hydrological processes that may not be comprehended in some terms of hydrological phenomena zhu et al 2020 li et al 2020 swift et al 2005 regarding the importance of glacier regions as stable water resources for available water resources systems simulating streamflow values at the snow covered regions via suitable approaches is an essential requirement therefore this study investigated the ability of the flexg model in runoff simulation related to the glacier and non glacier parts of the studied basin fig 5 shows a comparison of simulated daily runoff by the flexg model and observed total runoff at the basin for the calibration period 1989 2003 against the daily precipitation p and temperature t the model gives satisfactory results for the calibration period with the metrics of kge 0 80 nse 0 64 and r2 0 65 the flexg model has been successfully used in many different glacierized basins in china e g the dongkemadi glacier river basin including a 15 km2 glacier coverage in the tibetan plateau by gao et al 2021a who reported kge values around 0 88 and 0 70 for the calibration and validation periods respectively the flexg employed in the urumqi glacier northwest china by gao et al 2018 showed a different range of kge in which the best result of kge was reported as 0 75 and 0 73 for the calibration and validation periods respectively the measured and simulated runoff values during the validation period 2004 2018 are shown in fig 6 in the validation period the flexg model is able to produce a better simulation of the extremum streamflow values minima and maxima also the model has been able to satisfactorily simulate runoff behavior during the validation period which has provided acceptable performance as a model in glacial regions this is consistent with previous results by gao et al 2017 however the statistical metrics showed a slightly weaker performance during the validation period with kge 0 71 nse 0 47 and r2 0 58 according to the criteria mentioned in previous studies the performance of the flexg model can be classified as satisfactory which proves the robustness of the flexg model in the glacierized basin and allows additional research on future predictions also examining the behavior of precipitation and temperature time series at the same time as measured streamflow we concluded that this model is consistent with streamflow precipitation and temperature behavior in the catchment in general the flexg model can reproduce historical hydrography which allows us to make predictions on the impacts of climate change on runoff fig 7 shows the time series of the simulated runoff from the glacierized area qglacier and the simulated runoff from the non glacier region of the basin qnon glacier the flexg model has been able to reasonably simulate the peak points of the runoff during the calibration and validation period another important result is that in the study basin generated runoff from glacier areas and non glacier areas have the same contribution to total runoff generation the results proved that a large ratio of runoff comes from glaciers in the studied basin indicating the important role of glaciers and their changes on runoff generation in the torne river basin this conclusion is consistent with the results from gao et al 2020 they also found an upward trend in the simulated runoff and that result is in the same direction as the results of the current study 4 3 simulation of glacier mass balance gmb fig 8 shows the simulated gmb in the glacierized basin from the flexg model during the entire studied time period the highest amount of gmb changes occurred in 2013 with an amount of 204 06 mm w e while in 1993 the lowest amount of gmb changes occurred with a value of 113 05 mm w e the overall simulated gmb over 30 years showed that the rate of gmb changes is decreasing as shown by the red trend line in fig 8 gao et al 2018 reported similar results in other regions in china they reproduced gmb by the flexg model in urumqi glacier china and their result showed a decrease in gmb from 1959 to 2007 and our study showed a negative rate for the glacier mass volume in northern sweden our results are also consistent with hugonnet et al 2021 that reported negative glacier mass changes in scandinavia 4 4 the role of topography on glacier mass balance another feature of the flexg model is that it can separate the gmb in relation to each elevation zone in order to spatially separate the gmb 143 elevation zones were selected as study zones with an interval of 10 m fig 9 shows some of these elevation zones as representative of changes this figure shows the gmb millimeter water equivalent per year mm w e in each elevation zone relative to the daily average in each month all elevation zones have more gmbs loss in july and august which can be reasonable due to the higher temperature in these months the gmb loss has been occurring more intensely at lower altitudes so that at lower altitudes the thinning rate is 2450 32 mm w e to 37 77 mm w e while the rate of gmb changes at higher altitudes reach values of 555 94 mm w e to 50 75 mm w e figs 9 and 10 show that there is a wider more elongated chart of the gmb at low altitudes per month which indicates that there is a wider range for gmb changing at low altitudes per month the daily average of gmb changes at an altitude of 1007 49 m in july starts from less than 2400 mm w e and reaches beyond 412 mm w e the range of the glacier retraction rate at higher altitudes is less than at low altitudes so at 1647 14 m the glacier change rate is around 555 94 mm w e to 50 75 mm w e by fewer fluctuation 4 5 simulation of snowmelt rate fig 11 shows the box plot of the snow water equivalent swe simulated by the flexg model for each season from 1989 to 2018 the liquid water inside the snowpack has a key role in snow melting and runoff from snowmelt and glacier melt and the values of swe can help us have a better view of runoff volume as shown in fig 11 the maximum range of swe occurs in october when swe varies between 0 1 and greater than 0 9 in general in the spring and autumn seasons snowpacks can release a higher amount of water into the streamflow during the study period of 30 years 1989 2018 the range of swe was the highest in autumn and the lowest in summer 4 6 simulation of the snow cover area proportion simulated snow covered area sca is estimated based on elevation zones area with a threshold swe of larger than 3 mm the performance of the flexg model in the sca simulation was evaluated using daily available sca data from modis terra cgf mod10a1f version 61 product at the spatial resolution of 500 m from 2000 to 2018 fig 12 shows that the flexg simulated sca agreed well with the satellite sca data particularly in terms of the seasonal variations with the coefficient of determination and rmse equal to 0 61 and 28 04 respectively our results are consistent with a previous study gao et al 2020 regarding the sca simulation in which they found that the simulated sca had a higher r2 0 82 0 86 and similar rmse equal to 18 29 against satellite derived sca in the yigong zangbu river basin in china 4 7 comparison of the flexg model and the hbv model to better test the model the performance of the flexg model simulating of the total runoff and particularly the peak flow defined as the top 5 of total streamflow was compared with the classical lumped hydrological hbv model the hbv model including glacier module table 4 presents the performance of the two models in total runoff simulation for the same calibration and validation periods these results clearly show that the flexg model performed better in our studied basin fig 13 shows the comparison of two models for the peak flow simulation both models were compared with each other by the taylor diagram for peak flow simulation the results showed that the flexg model outperformed hbv for the peak flow simulation in another view the lumped model like the hbv model can allow modelers to simulate the amount of total runoff at the outlet of the basin whereas the flexg model has the flexibility to generate the amount of runoff related to the glacier part and non glacier part of the basin as well as the amount of total runoff at the outlet of basin the statistical results of the semi distributed model flexg denote the reliable capability for simulating the peak flow values compared to the lumped model hbv by comparing with some previous study s outcome gao et al 2021b 2017 2018 the flexg model satisfactorily simulates runoff peaks in glacier regions 4 8 trend analysis of the flexg simulated outputs the seasonal and yearly trend analysis by the mk test was performed to investigate the trend of each simulated output component of the flexg model and corresponding measured streamflow and climatic variables over the study period 30 years the result of the trend analysis was shown by the mk z value and sens s slope estimator sens s slope 10 ss for measured and simulated variables table 5 during the 30 year study period the temperature always had upward trends and it was significant at 10 significance level during the summer and precipitation had upward trends in all seasons except autumn and it was significant at 10 significance level during the spring changing gmbs at different elevations had downward trends and it was significant at 10 significance level during the summer in low middle and high elevations and this issue could be related to the higher glacier melts during the summer the z value of simulated runoff in the glacier part of the basin showed an upward significant trend at 10 significance level during the summer and an upward significant trend at 5 significance level on annual scale and this could be due to the interaction between gmb and glacier runoff which was considered by the flexg there was no significant trend in measured streamflow simulated total runoff by the flexg model simulated runoff from the non glacier part of the basin by flexg and simulated swe the reproduced runoff by hbv model had a downward significant trend at 10 significance level during the summer the annual trend analysis of simulated gmb showed that gmb in low middle and high elevations had downward significant trends at 5 5 and 10 significance levels respectively this is consistent with medwedeff and roe 2017 who also found downward trends in annual gmb on a global scale 5 conclusions snowmelt and glacier melting have a significant role in runoff generation in glacierized areas the current study employed a recently developed semi distributed glacier hydrological model flexg for the first time to understand the contribution of glaciers on runoff generation and its effect on gmb in a glacierized basin in north sweden the study basin was classified into 143 elevation zones with intervals of 10 m in elevation our results found that the flexg model well reproduces the total daily runoff at the basin outlet with kge values of 0 80 and 0 71 during the calibration and validation periods respectively the simulated snow cover area agreed reasonably well with modis satellite snow cover observations indicating the good performance of flexg in the simulation of snow cover dynamics the flexg model was further compared with the classical hbv model and the flexg model yielded better performance in the simulation of total runoff and peak flow also the results showed that the rate of melting snow as well as the rate of gmb changes at lower elevation is higher than at higher elevations therefore changing gmb at low altitudes plays a more significant role in runoff generation in addition runoff at the glacier part was more than in the non glacier part areas in other words the majority of runoff in this basin is affected by glaciers and their changes and the generated runoff has been severely affected by the melting of glaciers trend analysis showed that observed streamflow and simulated total runoff using the flexg model have no significant trends while reproduced runoff by the hbv model shows a significant downward trend in summer also gmbs in low middle and high elevations have significant downward trends in summer credit authorship contribution statement babak mohammadi conceptualization data curation formal analysis investigation methodology resources software validation visualization writing original draft hongkai gao methodology resources validation writing review editing zijing feng methodology software petter pilesjö supervision writing review editing majid cheraghalizadeh methodology visualization zheng duan conceptualization formal analysis funding acquisition project administration supervision validation methodology resources writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the crafoord foundation no 20200595 and no 20210552 
2033,glaciers are one of the main sources of freshwater in cold regions the glacier melting process can significantly impact the glacier mass balance gmb and contribute a large amount of runoff in cold regions this study applied the recently developed semi distributed glacio hydrological conceptual model flexg to understand the glacier melting process and the effect of topography on gmb in the torne river basin northern sweden the study simulated glacier and snow accumulation and ablation as well as runoff from the glacier and non glacier areas of the basin using the flexg model for the time period 1989 2018 the flexg model considers the influence of topography on runoff generation and in this study the basin was classified into 143 zones depending on elevation and aspect in order to gain a comprehensive view of the performance of the flexg model the classical lumped hydrological model hbv was used and compared with the flexg model in simulating total streamflow and peak runoff at the outlet of the basin our results revealed that the flexg model performed well in reproducing the streamflow also better than the hbv model with metric kling gupta efficiency kge of 0 80 and 0 71 for the calibration and validation periods respectively we also found that the flexg model performs better in peak runoff simulation than the hbv model the flexg simulated snow cover area proportion agreed well with the modis satellite snow cover product r2 0 60 and rmse 28 the gmb in different elevation zones was simulated and a downward trend was found for gmb changes during the study period because of climate change keywords arctic region catchment hydrology glacier hydrology hydrological model runoff simulation satellite data data availability data will be made available on request 1 introduction glaciers as a part of important water resources in cold regions play a remarkable role in various research areas such as hydrological modeling glaciological studies agricultural climatological and downstream ecosystem studies gao et al 2021b hock 2005 yang et al 2012 glaciers as natural water reservoirs reduce streamflow variability stabilize water resources and protect large populations from drought stress especially in arid regions pritchard 2019 gao et al 2021a the meltwater of glaciers plays a key role in streamflow quality and quantity singh et al 2021 therefore understanding the spatial and temporal dynamics of glaciers and the contribution of meltwater to runoff generation are essential for many application such as hydrological modeling and prediction water resources management and ecosystem assessment in cold regions cooper et al 2011 verbunt et al 2003 liu et al 2020 in this regard ideally long term measurements of glacier extent glacier meltwater and total streamflow are needed however unfortunately in situ measurements are often limited in many cold regions across the world mainly due to inaccessibility and harsh environments duethmann et al 2015 hydrological models are based on the simplified representation of the complex water cycle and hydrological processes hydrological models can be used to extrapolate limited available measurements in both space and time to improve our understanding of hydrological processes beven 2011 these models provide one of the popular ways to predict streamflow under environmental and climate changes in different regions rintis and setyoasri 2016 abebe et al 2010 duan et al 2019 hydrological models have been widely recognized as an essential tool to support water resources management controlling water structures designing urban water systems irrigation planning hydropower designing and flood risk assessment etc gichamo and tarboton 2019 uhlmann et al 2013 therefore development of hydrological models is vital for hydrological agricultural ecological and climatological studies song et al 2020 munzimi et al 2019 given the important role of glaciers and extensive snow cover in cold regions ayala et al 2020 gao et al 2020 dynamics of glaciers and snow and their hydrological processes should be sufficiently represented in hydrological models in order to achieve reasonable streamflow simulation in recent years many efforts have been made to test and develop various hydrological models for streamflow simulation in different glacierized regions for example the hbv hydrologiska byråns vattenbalansavdelning model was employed for analyzing the effect of climate change on the runoff and glacier changes in yushugou basin china zhou et al 2022 the hbv model was also used to estimate the snowmelt and its contribution to the total streamflow in the panchachuli glacier region in central himalaya rautela et al 2022 wang et al 2020 coupled a modified version of the hbv hbv d by an improved temperature index approach for gmb and runoff simulation in the tianshan mountains china the hbv model has also been successfully used to quantify glacio hydrological variables such as gmb kong et al 2021 rainfall runoff glacier runoff and snow runoff su et al 2021 in different regions zhao et al 2019 used the variable infiltration capacity macroscale hydrological model vic cas for the monthly streamflow simulation in a glacierized basin in the tibetan plateau van beusekom and viger 2016 integrated the glacier module into the precipitation runoff modeling system prms to develop the prms glacier model for streamflow simulation this model was successfully tested for streamflow simulation in two glacierized basins in alaska gao et al 2018 developed a semi distributed hydrological model flexg for glacierized basins this model uses the temperature index method distributed by topographical data to divide the entire basin into glacial and non glacial areas for calculating runoff in the glacier areas the xinanjiang storage capacity curve was employed in the flexg model for calculating runoff formation in the non glacier area gao et al 2017 the flexg model has been successfully tested and found to perform well in streamflow simulation in many mid latitude and high altitude glacierized regions in china gao et al 2017 2018 many other models were successfully applied in a number of glacierized basins in different parts of the world brown et al 2014 burger et al 2019 chen et al 2019 certainly each hydrological model has its own strengths and weaknesses the most important feature of a hydrological model for glacierized basins is its ability in 1 reasonable representation of various hydrological processes particularly from glaciers and snow 2 the separation of runoff contribution from different sources e g the glacier and non glacier parts and 3 the validity of simulated variables by observations after literature reviews and initial comparison of the features of different hydrological models the flexg model stands out because it integrates snow and glacier processes and can generate many outputs including runoff simulation in glacier and non glacier parts snow water equivalent swe snow cover area proportion sca gmb etc gao et al 2017 2021b previous studies reported the good performance of the flexg model but mostly in mid latitude cold regions mainly in china the flexg model has not been tested in other glacier areas with different climate and environmental conditions and especially not on glaciers in high latitude zones such as the arctic region therefore it is interesting and scientifically relevant to evaluate the flexg model in other regions this study aims to do so in the torne river basin in northern sweden for the first time the objectives of this study are to 1 evaluate the performance of the flexg model for streamflow simulation and gmb simulation for a 30 year long period 1989 2018 for each elevation zone 2 assess the role of glaciers in runoff generation in the study basin by separating runoff from the glacier and non glacier areas 3 investigate the effects of altitude on gmb spatial distribution 4 simulate swe and sca at a daily scale by the flexg model 5 examine and compare the ability of the flexg model in simulating peak runoff with the classical hydrological model hbv 6 perform the trend analysis of simulated variables over the entire period 2 study area this study focused on the torne river basin which is a glacierized basin in north sweden the basin is located in the latitude 68 8 68 24 north and the longitude 18 3 18 56 east as shown in fig 1 the basin area is 547 6 km2 it is one of the few high latitude research stations with a long time series of measured streamflow data and meteorological variables the digital elevation model dem data shows that the elevation ranges from 342 to 1781 m above the mean sea level the basin is located in the polar climate zone according to the köppen geiger climate classification kottek et al 2006 the basin is located in the permafrost zone and snow and glaciers provide essential water resources in this basin dahlke et al 2012 ploum et al 2019 3 data and methods 3 1 data and data processing the required daily climatic air temperature t and precipitation p and hydrologic data streamflow for the time period 1985 2018 were obtained from the swedish meteorological and hydrological institute smhi https www smhi se the location of the meteorological station and the streamflow station is show in fig 1 the dem data were obtained from european union s earth observation program copernicus https land copernicus eu the european digital elevation model eu dem version 1 1 was used in this study and it is at the spatial resolution of 25 m with vertical accuracy root mean square error equal to 7 m the ice thickness information was obtained from eth zurich s research collection farinotti et al 2019 snow cover area dataset related to modis terra cloud gap filled cgf mod10a1f version 61 remotely sensed data was obtained by national snow and ice data center https nsidc org the torne river basin is located in the arctic region and the minimum maximum and daily average temperatures during the study period are 32 1 c 23 1 c and 0 29 c respectively the amount of minimum maximum and daily average precipitation are 0 mm 61 9 mm and 0 93 mm respectively the minimum maximum and the daily average streamflow in the outlet of the basin are 0 13 m3 s 1 219 m3 s 1 and 14 42 m3 s 1 respectively the entire time series of streamflow data were split into the 3 75 year warming up 1985 1988 15 year calibration 1989 2003 and 15 year validation 2004 2018 there is only one weather station in the studied area and a distributed input force to drive the flexg model is required to achieve this a linear distribution method was used for generating distributed points in the studied basin the precipitation and temperature distribution for each altitude range was determined by interpolating the observation data from the abisko meteorological station the distribution of aspects and elevations in the glacier and non glacier areas in the study basin is shown in fig 2 there are 22 glaciers in the study area fig 3 shows their geographic locations and their ice thickness distribution the required dataset for ice thickness analysis was obtained from farinotti et al 2019 the randolph glacier inventory 6 0 was used for obtaining glacier location area and height rgi 2017 three glaciers rgi60 08 00261 rgi60 08 00460 and rgi60 08 00468 were clipped based on the studied basin boundary the ice thickness map shows that the rgi60 08 00463 is the thickest glacier in the studied basin with ice larger than 70 m flexg considers the lapse rate for precipitation and air temperature changes during the modeling process gao et al 2017 2018 after initial testing and literature review the rates of 5 100 m and 0 4 c 100 m were used for increasing precipitation and decreasing air temperature with elevation change respectively in addition the limitation of using one climate station the only available station may add some uncertainties to hydrological modeling renard et al 2010 including i uncertainty in the interpolation to distributed data via lapse rate in the flexg model input and ii uncertainty in the flexg model calibration and uncertainty in precipitation and temperature may further have an effect on the model parameter setting during the calibration however previous studies reported that using one climate station can have an acceptable result for running distributed hydrological models in small catchments koch et al 2016 maneta et al 2007 due to the influence of altitude and shadows a regional representation based on meteorological point observations is a common problem in mountainous areas hrachowitz and weiler 2011 most meteorological stations are located in low altitudes to ensure suitable access to services stahl et al 2006 which usually results in high distortion of the observed variables because terrain has a significant impact on the distribution and energy balance of spatial forcing data see e g euser et al 2015 it also affects the temporal and spatial changes of glacier and snow dynamics verbunt et al 2003 solar radiation is the primary control for snow ice melting hock 2005 in order to consider the influence of altitude on temperature and runoff generation the basin was divided into 143 elevation zones at the interval of 10 m each elevation zone was further segregated by three azimuth zones in terms of aspects north 315 45 south 135 225 and east west line azimuth 45 135 and 225 315 to account for the effects of aspects on glacier and snow melting and runoff generation considering the different elevations aspects and glacier and non glacial areas the studied basin with 22 glaciers was classified into the glacier and non glacier classes fig 2 3 2 the flexg model 3 2 1 snowfall and snowmelt simulation in the flexg model precipitation is converted to solid precipitation particularly snow ps mm and liquid precipitation particularly rain pl mm according to air temperature t c and the threshold air temperature tt c with equations 1 and 2 1 p s p t t t 0 t t t 2 p i p t t t 0 t t t where tt is an essential parameter to calibrate in the flexg model berghuijs et al 2016 wen et al 2013 kienzle 2008 previous studies showed that tt could be even as low as 5 c in nordic regions seibert 1997 thus we used a wide prior range 6 c to 4 c to find optimal values of tt for testing its influence on the flexg model ability in simulation of hydrograph as well as glacier and snow dynamics hock 2003 singh et al 2000 the snowpack is a porous medium and liquid water can be held by the snowpack fujita and sakai 2014 gao et al 2021a the water holding capacity of the snowpack relies on its physical characteristics particularly the snow depth and density patil et al 2020 mahmoodzada et al 2022 for shallow snowpacks the snowpack may not necessarily retain water but it may release it to the bottom layer which will ultimately have sufficient lubrication to cause sliding down creep similar to the movement of the whole snowpack over the glacier ice beria et al 2018 singh et al 1997 the flexg model uses equations 3 to 7 for modeling the snowmelt process 3 d s w dt p s r rf m s 4 d s wl dt p l m s r rf p e 5 m s f dd t t t t t t 0 t t t 6 p e s wl c wh s w s wl c wh s w 0 s wl c wh s w 7 r rf f dd f rr t t t t t t 0 t t t where sw refers to the solid snowpack swl refers to the liquid water inside the snowpack ms is the amount of melted snow rrf indicates the refreezing water from liquid storage pe refers to the generated runoff from ice soil surface fdd indicates the factor of degree day mm c day 1 tt is the threshold temperature zhang et al 2006 ca is a factor for the effect of aspect on the melting process and cwh is a certain fraction which snowpack can hold the liquid melting water without runoff generation fr r is a correction coefficient used for simulating liquid water refreezing when the air temperature is below tt and the snowmelt was calculated for each elevation zones individually konz et al 2007 seibert 1997 3 2 2 glacier melt simulation the mechanism for simulating glacier melt is based on the temperature index approach which is the same as snowmelt but with a different degree day factor if there is no snow cover then ice begins to melt and there will be a generation process for glacier runoff mg mm day 1 and also qg and qf g show streamflow for the glacier part of the basin l t and subsurface glacier stormflow l t respectively because the ice surface is mostly covered by impurity substance with less albedo the glaciers degree day factor is considered to be larger than snow cover braithwaite and olesen 1989 for calculating the released runoff from glacier areas equation 8 10 the flexg model also needs an assumed linear reservoir sf g and a parameter for recession kf g day zhang et al 2006 gao et al 2017 8 m g f dd c g t t t t t t a n d s w 0 0 t t t o r s w 0 9 d s f g dt p l m g q f g 10 q g s f g k f g 3 2 3 non glacier area rainfall runoff simulation for runoff simulation in non glacier areas rainfall and snowmelt were considered as unsaturated rootzone reservoirs su the amount of runoff is simulated based on the relative water content of the soil su su max and the amount of water entering the soil pe and the actual evaporation ea is estimated based on the relative water content seibert 1997 so snowmelt and rainfall in non glacier areas are simulated by equation 11 gao et al 2017 11 d s u dt p e e a r u pe mm day refers to the effective rainfall the actual evaporation is formulated by ea mm day 1 the relative soil moisture su su max mm ce a free parameter and potential evaporation e0 mm day 1 are used for calculating ea the storage capacity curve is determined by the xinanjiang approach zhao 1992 ru is the amount of runoff generated from unsaturated reservoirs su zhao and liu 1995 12 r u p e 1 1 s u 1 β s u m a x β 13 e a e 0 s u c e s u m a x 14 d s f dt r f q f 15 d s s dt r s q s 16 q f s f k f 17 q s s s k s where the rootzone storage capacity is shown by su max mm β represents a parameter of shape and sf and ss are linear reservoirs also the amount of recharge related to the reservoir s fast response was introduced by rf mm day 1 and it was calculated by ru day the amount of recharge related to the reservoir s slow response was presented by rs mm day 1 and it was calculated by ru qf mm day 1 is subsurface fast runoff qs mm day 1 is groundwater slow runoff kf day is the fast response parameter and ks day is the slow response parameter 3 2 4 implementation of the flexg model the flexg model has 13 parameters and we used the generalized likelihood uncertainty estimation glue beven and binley 1992 for calibrating the flexg model parameters gao et al 2017 a monte carlo sampling strategy was implemented with 10 000 homogeneous distribution parameters and then the best 1 of parameter sets were used for further analysis the best parameter sets are shown in fig 4 the kling gupta efficiency kge gupta et al 2009 was employed as the objective function during the model calibration the result of the monte carlo strategy is provided in section 4 1 since the current study is the first application of flexg in the high altitude domain literature review gao et al 2017 2018 and our preliminary analysis were performed to obtain a reliable range for flexg parameters during the calibration period table 1 presents the used range for the flexg model parameters during the calibration 3 3 the hbv model the hydrologiska byråns vattenbalansavdelning hbv model is a classic lumped hydrological model and it has been widely used for runoff simulation in different areas the hbv was developed at the smhi in the early 1970s bergström 1976 this model has been used to predict runoff and evaluate water resources in scandinavian countries by considering the effect of rainfall and snowfall separately huang et al 2019 li et al 2015 the hbv model is a conceptual model for field scale hydrological processes and it is applicable as a lumped model by dividing the field into several sub fields bergström and lindström 2015 the hbv model works by considering precipitation temperature evapotranspiration snowfall soil storage and the conceptual stage of basin reaction for runoff generation abebe et al 2010 the model has been widely employed to simulate streamflow in the nordic region e g lindström et al 1997 the hbv model requires evapotranspiration air temperature and precipitation as the essential inputs for runoff simulation this model considers three sub routine levels for runoff modeling i the soil moisture stage ii the snow stage iii the runoff response stage more details and descriptions of the hbv model structure can be found in bergström 1995 lindström et al 1997 and seibert and vis 2012 the current study used the hbv model that includes the glacier module the monte carlo calibration method was used with the same kge as the objective function during the calibration of the hbv model the parameter ranges and their optimal values for the hbv model in our study are listed in table 2 3 4 metrics for evaluating the model performance four widely used statistical metrics namely coefficient of determination r2 kling gupta efficiency kge gupta et al 2009 nash sutcliffe efficiency nse nash and sutcliffe 1970 and root mean square error rmse were used in the current study to evaluate the model performance table 3 presents the formulas of the used metrics 3 5 mann kendall trend test trend analysis of the flexg model simulated output components was performed by employing the mann kendall test of prewhitened time series data mk test based on the storch approach von storch 1999 for a better understanding of changes in different hydrological variables the mann kendall trend test was first introduced by mann in 1945 mann 1945 and then developed by kendall in 1966 like other statistical tests the test is based on comparing hypothesis zero h0 and hypothesis one h1 and ultimately decides whether to accept h0 yue and wang 2004 the h0 test is based on randomness and the absence of trends in the data series and acceptance of h1 rejection of h0 indicates the existence of trends in the data series if the z statistic is positive there is an uptrend and if it is negative it is considered a downtrend hamed 2009 4 results and discussion 4 1 calibration of the flexg model parameters the flexg model has 13 parameters to calibrate the initial range of the parameter set is generally consistent with the previous study gao et al 2017 2020 2021a b the model parameters behavior is demonstrated by dotty plots as shown in fig 4 after running the monte carlo approach and after evaluating the performance of the flexg model the optimal values for each parameter are presented in table 1 the calibration process by previous studies showed that the t t tended to be 0 c in urumqi glacier no 1 catchment northwest china gao et al 2017 and 1 c to 0 5 c in dongkemadi glacier tibetan plateau gao et al 2021a while in the current study area the t t tended to be between 4 c to 4 c the coefficient for the recession of glacier runoff k fg was considered as the range of 1 3 in urumqi glacier no 1 catchment and the range of 5 10 in dongkemadi glacier while the k fg of 0 8 was found in the studied area in northern sweden the degree day factor for snow f dd was found between 2 and 10 as suitable range in the current study while f dd was set in the range of 2 7 in yigong zangbu river basin gao et al 2021a 4 2 flexg model performance in runoff simulation streamflow simulation using the process based model may be significantly disputing due to complex physical hydrological processes that may not be comprehended in some terms of hydrological phenomena zhu et al 2020 li et al 2020 swift et al 2005 regarding the importance of glacier regions as stable water resources for available water resources systems simulating streamflow values at the snow covered regions via suitable approaches is an essential requirement therefore this study investigated the ability of the flexg model in runoff simulation related to the glacier and non glacier parts of the studied basin fig 5 shows a comparison of simulated daily runoff by the flexg model and observed total runoff at the basin for the calibration period 1989 2003 against the daily precipitation p and temperature t the model gives satisfactory results for the calibration period with the metrics of kge 0 80 nse 0 64 and r2 0 65 the flexg model has been successfully used in many different glacierized basins in china e g the dongkemadi glacier river basin including a 15 km2 glacier coverage in the tibetan plateau by gao et al 2021a who reported kge values around 0 88 and 0 70 for the calibration and validation periods respectively the flexg employed in the urumqi glacier northwest china by gao et al 2018 showed a different range of kge in which the best result of kge was reported as 0 75 and 0 73 for the calibration and validation periods respectively the measured and simulated runoff values during the validation period 2004 2018 are shown in fig 6 in the validation period the flexg model is able to produce a better simulation of the extremum streamflow values minima and maxima also the model has been able to satisfactorily simulate runoff behavior during the validation period which has provided acceptable performance as a model in glacial regions this is consistent with previous results by gao et al 2017 however the statistical metrics showed a slightly weaker performance during the validation period with kge 0 71 nse 0 47 and r2 0 58 according to the criteria mentioned in previous studies the performance of the flexg model can be classified as satisfactory which proves the robustness of the flexg model in the glacierized basin and allows additional research on future predictions also examining the behavior of precipitation and temperature time series at the same time as measured streamflow we concluded that this model is consistent with streamflow precipitation and temperature behavior in the catchment in general the flexg model can reproduce historical hydrography which allows us to make predictions on the impacts of climate change on runoff fig 7 shows the time series of the simulated runoff from the glacierized area qglacier and the simulated runoff from the non glacier region of the basin qnon glacier the flexg model has been able to reasonably simulate the peak points of the runoff during the calibration and validation period another important result is that in the study basin generated runoff from glacier areas and non glacier areas have the same contribution to total runoff generation the results proved that a large ratio of runoff comes from glaciers in the studied basin indicating the important role of glaciers and their changes on runoff generation in the torne river basin this conclusion is consistent with the results from gao et al 2020 they also found an upward trend in the simulated runoff and that result is in the same direction as the results of the current study 4 3 simulation of glacier mass balance gmb fig 8 shows the simulated gmb in the glacierized basin from the flexg model during the entire studied time period the highest amount of gmb changes occurred in 2013 with an amount of 204 06 mm w e while in 1993 the lowest amount of gmb changes occurred with a value of 113 05 mm w e the overall simulated gmb over 30 years showed that the rate of gmb changes is decreasing as shown by the red trend line in fig 8 gao et al 2018 reported similar results in other regions in china they reproduced gmb by the flexg model in urumqi glacier china and their result showed a decrease in gmb from 1959 to 2007 and our study showed a negative rate for the glacier mass volume in northern sweden our results are also consistent with hugonnet et al 2021 that reported negative glacier mass changes in scandinavia 4 4 the role of topography on glacier mass balance another feature of the flexg model is that it can separate the gmb in relation to each elevation zone in order to spatially separate the gmb 143 elevation zones were selected as study zones with an interval of 10 m fig 9 shows some of these elevation zones as representative of changes this figure shows the gmb millimeter water equivalent per year mm w e in each elevation zone relative to the daily average in each month all elevation zones have more gmbs loss in july and august which can be reasonable due to the higher temperature in these months the gmb loss has been occurring more intensely at lower altitudes so that at lower altitudes the thinning rate is 2450 32 mm w e to 37 77 mm w e while the rate of gmb changes at higher altitudes reach values of 555 94 mm w e to 50 75 mm w e figs 9 and 10 show that there is a wider more elongated chart of the gmb at low altitudes per month which indicates that there is a wider range for gmb changing at low altitudes per month the daily average of gmb changes at an altitude of 1007 49 m in july starts from less than 2400 mm w e and reaches beyond 412 mm w e the range of the glacier retraction rate at higher altitudes is less than at low altitudes so at 1647 14 m the glacier change rate is around 555 94 mm w e to 50 75 mm w e by fewer fluctuation 4 5 simulation of snowmelt rate fig 11 shows the box plot of the snow water equivalent swe simulated by the flexg model for each season from 1989 to 2018 the liquid water inside the snowpack has a key role in snow melting and runoff from snowmelt and glacier melt and the values of swe can help us have a better view of runoff volume as shown in fig 11 the maximum range of swe occurs in october when swe varies between 0 1 and greater than 0 9 in general in the spring and autumn seasons snowpacks can release a higher amount of water into the streamflow during the study period of 30 years 1989 2018 the range of swe was the highest in autumn and the lowest in summer 4 6 simulation of the snow cover area proportion simulated snow covered area sca is estimated based on elevation zones area with a threshold swe of larger than 3 mm the performance of the flexg model in the sca simulation was evaluated using daily available sca data from modis terra cgf mod10a1f version 61 product at the spatial resolution of 500 m from 2000 to 2018 fig 12 shows that the flexg simulated sca agreed well with the satellite sca data particularly in terms of the seasonal variations with the coefficient of determination and rmse equal to 0 61 and 28 04 respectively our results are consistent with a previous study gao et al 2020 regarding the sca simulation in which they found that the simulated sca had a higher r2 0 82 0 86 and similar rmse equal to 18 29 against satellite derived sca in the yigong zangbu river basin in china 4 7 comparison of the flexg model and the hbv model to better test the model the performance of the flexg model simulating of the total runoff and particularly the peak flow defined as the top 5 of total streamflow was compared with the classical lumped hydrological hbv model the hbv model including glacier module table 4 presents the performance of the two models in total runoff simulation for the same calibration and validation periods these results clearly show that the flexg model performed better in our studied basin fig 13 shows the comparison of two models for the peak flow simulation both models were compared with each other by the taylor diagram for peak flow simulation the results showed that the flexg model outperformed hbv for the peak flow simulation in another view the lumped model like the hbv model can allow modelers to simulate the amount of total runoff at the outlet of the basin whereas the flexg model has the flexibility to generate the amount of runoff related to the glacier part and non glacier part of the basin as well as the amount of total runoff at the outlet of basin the statistical results of the semi distributed model flexg denote the reliable capability for simulating the peak flow values compared to the lumped model hbv by comparing with some previous study s outcome gao et al 2021b 2017 2018 the flexg model satisfactorily simulates runoff peaks in glacier regions 4 8 trend analysis of the flexg simulated outputs the seasonal and yearly trend analysis by the mk test was performed to investigate the trend of each simulated output component of the flexg model and corresponding measured streamflow and climatic variables over the study period 30 years the result of the trend analysis was shown by the mk z value and sens s slope estimator sens s slope 10 ss for measured and simulated variables table 5 during the 30 year study period the temperature always had upward trends and it was significant at 10 significance level during the summer and precipitation had upward trends in all seasons except autumn and it was significant at 10 significance level during the spring changing gmbs at different elevations had downward trends and it was significant at 10 significance level during the summer in low middle and high elevations and this issue could be related to the higher glacier melts during the summer the z value of simulated runoff in the glacier part of the basin showed an upward significant trend at 10 significance level during the summer and an upward significant trend at 5 significance level on annual scale and this could be due to the interaction between gmb and glacier runoff which was considered by the flexg there was no significant trend in measured streamflow simulated total runoff by the flexg model simulated runoff from the non glacier part of the basin by flexg and simulated swe the reproduced runoff by hbv model had a downward significant trend at 10 significance level during the summer the annual trend analysis of simulated gmb showed that gmb in low middle and high elevations had downward significant trends at 5 5 and 10 significance levels respectively this is consistent with medwedeff and roe 2017 who also found downward trends in annual gmb on a global scale 5 conclusions snowmelt and glacier melting have a significant role in runoff generation in glacierized areas the current study employed a recently developed semi distributed glacier hydrological model flexg for the first time to understand the contribution of glaciers on runoff generation and its effect on gmb in a glacierized basin in north sweden the study basin was classified into 143 elevation zones with intervals of 10 m in elevation our results found that the flexg model well reproduces the total daily runoff at the basin outlet with kge values of 0 80 and 0 71 during the calibration and validation periods respectively the simulated snow cover area agreed reasonably well with modis satellite snow cover observations indicating the good performance of flexg in the simulation of snow cover dynamics the flexg model was further compared with the classical hbv model and the flexg model yielded better performance in the simulation of total runoff and peak flow also the results showed that the rate of melting snow as well as the rate of gmb changes at lower elevation is higher than at higher elevations therefore changing gmb at low altitudes plays a more significant role in runoff generation in addition runoff at the glacier part was more than in the non glacier part areas in other words the majority of runoff in this basin is affected by glaciers and their changes and the generated runoff has been severely affected by the melting of glaciers trend analysis showed that observed streamflow and simulated total runoff using the flexg model have no significant trends while reproduced runoff by the hbv model shows a significant downward trend in summer also gmbs in low middle and high elevations have significant downward trends in summer credit authorship contribution statement babak mohammadi conceptualization data curation formal analysis investigation methodology resources software validation visualization writing original draft hongkai gao methodology resources validation writing review editing zijing feng methodology software petter pilesjö supervision writing review editing majid cheraghalizadeh methodology visualization zheng duan conceptualization formal analysis funding acquisition project administration supervision validation methodology resources writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the crafoord foundation no 20200595 and no 20210552 
2034,firstly a new global floodability index with a resolution of 3 arc second is built from topography based information provided by the merit database using a neural network approach the topography and permanent water were defined in a coherent way ensuring the coherency between the resulting floodability index and permanent water which is unprecedented in previous versions the evaluation of the floodability index is done with independent observation datasets on surface water and land cover showing good performances in areas where surface water is naturally driven by topography conditions and limitation in human affected areas and some specific environments like peatland secondly some of the applications that the floodability index can serve are introduced including downscaling low resolution data analyzing and comparing datasets at different resolution and data fusion keywords floodability index surface water wetland topography data fusion data availability data will be made available on request 1 introduction the importance of surface water to life on earth has been documented countless times regarding many different aspects such as the environment and habitats ramsar convention secretariat 2016 gokce 2019 the production activities yoo and boyd 1994 pimentel et al 1997 dieter et al 2018 and the public health sector pruss et al 2002 konradsen et al 2013 satellite data has been applied to study and monitor surface water for a long time three types of data are commonly used 1 the optical pekel et al 2016 buchhorn et al 2020 2 the active microwave hess et al 2015 rosenqvist et al 2020 and 3 the passive microwave sippel et al 1998 prigent et al 2007 parrens et al 2017 observations by the optical sensors can be done frequently with a medium to high spatial resolution e g modis with a 250 m resolution and a near daily revisit time or sentinel 2 constellation with a 10 m resolution and a 5 day revisit time however such observations are affected by weather conditions and vegetation coverage the active microwave or sar observations have high spatial resolution and are much less sensitive to clouds however the backscattering signal from ground is sensitive to vegetation and active microwave data from past satellites have very low temporal resolution e g 35 days for ers1 2 new generation of satellites despite of much shorter revisit time along with high spatial resolution e g 10 m resolution and 6 day revisit time of sentinel 1 constellation does not yet offer data regularly for every region the passive microwave is the least sensitive to vegetation but has a strong limitation of very coarse resolution e g 27 km resolution of smmr sensor onboard the nimbus 7 satellite table 1 describes briefly some surface water datasets for reference there are nowadays plenty of satellite derived products on surface water with each of them having its own pros and cons from their original nature and with their varying spatial and temporal resolution the question is now how to exploit them in an efficient way we present a global topography based floodability index as a tool to change or put multiple datasets into the same grid so that we can easily compare analyze and fuse them to obtain a more comprehensive dataset on surface water topography based floodability index is defined as a proxy for the probability that a pixel will be inundated compared to adjacent pixels consider a floodability index ranging from 0 to 1 the higher the index compared to its neighbors the more likely the pixel will be inundated compared to its neighborhood in other words it is a local assessment tool provided at the global scale that helps understanding the local spatial pattern of inundation with that operation the floodability index can be used for instance to downscale low resolution lr data fluet chouinard et al 2015 aires et al 2017 downscaling is actually not only a way to retrieve high resolution hr information from lr data but also a stepping stone to aggregate multiple lr data at high resolution for different purposes as stated above it is possible to derive a floodability index from satellite observations in this case the floodability index is defined as the probability for a pixel to be inundated this derivation requires data over a long time record therefore it can be done more easily at a local scale for some regions e g over niger delta by aires et al 2013b over vietnam by aires et al 2017 both with modis time series in this case the floodability index is at the resolution of the satellite data and it is possible to have a seasonality or even more specific characteristics at the global scale so far it is impossible to obtain a hr floodability index from satellite observation due to the deficiency of hr global data to work satisfactorily in all conditions for long term with enough time samples this is actually expected to be solved by the upcoming launch of the swot satellite in 2022 https swot jpl nasa gov here we use topography information to take advantage of the global availability of this kind of data and because the flooding state is naturally driven partly by topography conditions such as slope and elevation woods 2005 flooding is also dependent on other factors like land cover or irrigating activities woods 2005 vriend 2005 but for now we only take into account topography and temporarily disregard other factors previously such a topography based floodability index was proposed by fluet chouinard et al 2015 which can be considered as the first version and later by aires et al 2017 with more detail and some improvements better resolution optimized topography variables selection neural network model although the floodability index is based on topography information some surface water datasets are still needed in the process the coherency between all the sources of information required to develop the floodability index is essential for instance to ensure that one river is not double counted or that the coast of a lake is more likely to be inundated than the pixels further from the lake this coherency however was not ensured in previous versions this study aims at two goals 1 to improve the floodability index previously designed in our group and 2 to show some potential applications of the floodability index for 1 first we use a newer topography that has been improved by solving problem of trees and better filtering the shuttle radar topography mission srtm problems yamazaki et al 2017 2019 second a high resolution permanent water information resulting from the data fusion of several hr water datasets yamazaki et al 2014 2015 will be integrated in our floodability index this means that the floodability index performs already a data fusion of surface water datasets third the topography based parameters used to build the floodability index will be defined in complete coherency with the permanent water database to further increase their coherency for 2 such applications could be performed with any floodability index we will obviously illustrate them here using our new floodability index section 2 describes all the datasets used in this study including those for the floodability index construction and those for evaluation and applications how the floodability index is build will be presented in detail in section 3 and the results will be shown in section 4 section 5 will introduce some of the potential applications of the floodability index 2 databases 2 1 merit topography the multi error removed improved terrain digital elevation model merit dem provides globally terrain elevation at a horizontal spatial resolution of 3 a r c s e c o n d 90 m at the equator it was developed from existing digital elevator models dems including shuttle radar topography mission srtm 3 of nasa alos world 3d aw3d of jaxa and viewfinder panoramas vfp dem the major error components from the original dems were separated and eliminated hence a better vertical accuracy for this newer dem yamazaki et al 2017 based on the merit dem hydrography information was retrieved yamazaki et al 2019 from which a variety of related variables were calculated some of these variables are related to the presence of water aires et al 2017 and will be considered in the floodability index construction such as flow distance to and height above the small medium and big nearest drainage and terrain slope some examples are given in fig 1a d small medium and large drainages are defined by an upper flow accumulation area greater than 0 5 km 2 10 km 2 and 100 km 2 respectively the big advantage of using merit dem is its coherency with the observed permanent water that will be mentioned in section 2 2 2 such coherency was not assured in the hydrosheds dem used in previous studies fluet chouinard et al 2015 aires et al 2017 2 2 surface water databases 2 2 1 global lakes and wetlands database glwd the glwd synthesizes and combines various existing maps data and information such as the digital chart of the world world conservation monitoring centre wcmc and others lehner and döll 2004 it presents the lake and wetland distribution at three levels at level 1 and level 2 the database includes polygons representing large lakes and reservoirs glwd 1 and smaller permanent water bodies glwd 2 data at level 3 glwd 3 given in raster format at a resolution of 30 s provides a classification of lakes and wetlands fig 1e twelve types are listed in table 2 glwd 3 with its diverse classes will be used to train the floodability index model 2 2 2 permanent water the permanent water data fig 1f itself does not exist but is extracted from the global database of river width at 3 second resolution the river width data is obtained from multiple data sources including global 1 arc second water body map g1wbm global surface water occurrence gswo and open street map yamazaki et al 2014 2015 this database is advantageous since it describes even small canals for some regions and prevents virtual drainage due to a possibly misleading topography the river width is calculated and assigned for river center line pixels pixels that are permanent water but not in the center line are also noted hence a permanent water mask was derived with its high resolution the permanent water data is used to supplement the glwd with the small permanent water bodies and rivers that are not resolved by glwd 2 3 datasets for evaluation and applications 2 3 1 global surface water gsw the gsw dataset maps the spatial and temporal variability of open surface water at global scale and qualifies its long term changes pekel et al 2016 the dataset was obtained from three million landsat images spanning over 32 years 1984 2015 with a spatial resolution of 30 m an expert system was used to classify each landsat pixel as water dry and non valid observation the classification results were assembled for each month in the 32 years forming the monthly water history product this product was then used to derive some thematic maps on surface water dynamic characterization including the gswo global surface water occurrence which represents the frequency of water presence over the 32 year period and the maximum water extent which indicates every pixel that has ever been detected as water during the whole period due to the image acquisition by optical sensors of landsat satellites one can find a lot of missing data non observation in each single monthly water map of the gsw such spatial discontinuities can make the monthly maps useless in studies over large areas however when assembling multiple monthly maps over a long time period to make thematic products the generated statistical information makes sense for open water despite the fact that the water beneath vegetation cannot be reported gswo documents an actually observed frequency of open water presence as previously commented this is not what the floodability index reports still gswo is a good choice to assess the floodability index section 4 because they are both datasets at global scale and with high resolution and open water should be in agreement the maximum water extent and the monthly water history will be used for some applications of the floodability index section 5 2 3 2 globcover landcover map the globcover landcover map is a product of the globcover project an initiative of esa and partners bicheron et al 2006 the map was generated from meris envisat observations it covers the 180 w to 180 e 90 n to 65 s domain with a spatial resolution of 300 m 22 types of land cover were classified including different kinds of cropland forest shrub water etc the globcover map can provide reference information for the evaluation of the floodability index section 4 2 3 3 gfplain250m dataset of earth s floodplains the global dataset gfplain250m nardi et al 2019 provides a binary delineation of floodplains at 8 33 arcsecond resolution 250 m at equator the spatial extent of floodplains was obtained by a geomorphic algorithm which implemented terrain analysis procedures on shuttle radar topography mission srtm digital terrain model the gfplain250m map excludes areas with insignificant water availability such as deserts and icy high latitude this dataset will be used to evaluate the floodability index over some regions section 4 2 3 4 global inundation extent from multiple satellite giems giems is a global dataset of surface water extent and dynamics over a period of 15 years 1993 2007 the estimate of giems was obtained by a multi wavelength algorithm that exploited a range of satellite observations the passive microwave measurement from special sensor microwave imager ssm i the active microwave backscattering coefficients from european remote sensing ers satellite and the visible and near infrared reflectances from avhrr and the derived normalized difference vegetation index ndvi the inundation is primarily detected based on microwave emissivity calculated from the brightness temperature measured by ssm i the active microwave data with a high contrast between open water and water beneath vegetation helps estimate the contribution of vegetation to ssm i measurement the other data source ndvi helps differentiate bare soil and inundation in semi arid areas the detailed algorithm can be found in prigent et al 2001 2007 giems estimates the inundated fraction on a monthly basis over an equal area grid with each cell occupying about 773 km 2 0 25 0 25 at the equator the combination of different types of observations takes advantage of their complementary strengths and thus the problems related to each individual one is limited the evaluation of giems with existing independent datasets shows its consistency with them in various environments major river system irrigated fields small lakes network and at various latitudes tropical boreal however there is also a tendency of giems to underestimate the low fractional inundation and overestimate the high fractional inundation giems will be used in this study to illustrate some applications of the floodability index it will be downscaled then analyzed and fused with other datasets at high resolution section 5 2 3 5 inundation fraction data from thmb model the thmb terrestrial hydrology model with biogeochemistry model is a numerical model that simulates the water flow including river discharge water height above flood stage and inundation fraction based on topography conditions from dem precipitation from local climate data base flow from ecosystem model surface runoff from ecosystem model and evaporation from water surfaces from local climate data and energy balance model coe et al 2012 the model runs on a 5 min grid 9 km at the equator over the amazon and tocantins basin from 1939 to 1998 the original time step of the model was 1 h the model output was then averaged on a monthly basis similar to giems the monthly inundation fraction data from this model version 1 2 will be used in the application section section 5 2 3 6 sar data from the mosaics of japanese earth resources satellite jers 1 sar imagery a comprehensive map of wetland extent vegetation cover types and dual season inundation states in the lowland amazon basin was produced by hess et al 2015 the acquisition time of the sar images was during october november 1995 and may june 1996 the low and high water seasons in this area are defined as these two times to produce the classification map a wetland mask was first derived by a segmentation and clustering process based on the mean backscattering coefficient the wetland pixels were then processed using a rule set based on dual season backscattering values and labeled in five vegetation cover types non vegetated herbaceous shrub woodland forest and in two inundation states flooded non flooded the dataset is available in a geographic coordinate system with a 3 a r c s e c o n d resolution similar to the floodability index the land cover information it provides is not taken into account in this study but the flooding states are representative for the average low and high water conditions corresponding to the minimum and maximum water extent in a year therefore the flooding information of the two seasons is extracted from the dataset it would be interesting to fuse it with the low resolution data downscaled by the floodability index section 5 3 2 3 7 tree cover density the tree cover density is a data layer under the collection 2 of the copernicus global land cover layers provided by the copernicus global land service buchhorn et al 2020 the collection is an annual global dataset 180 w to 180 e 78 25 n to 60 s 2015 to 2019 produced from sentinel 2 observations with an overall accuracy 80 the spatial resolution is 100 m at the equator tree cover density will be used for the evaluation of the floodability index section 4 3 floodability index model the general flowchart for building our floodability index is shown in fig 2 firstly a sampling procedure is implemented globally on topography and water data among 11 potential topography variables useful ones are selected together with water information forming the learning dataset to train the neural network model topography is input water is output after being trained the model uses global data of selected topography variables as inputs to produce the floodability index 3 1 neural network the floodability index is modeled using a neural network nn with inputs being topography variables the floodability index can be associated with a conditional probability of a pixel to be inundated given its neighboring conditions let x be the input vector of the nn x be the vector of topography attributes of a given pixel and c its inundation state c 0 for dry c 1 for inundation the floodability index can be expressed by p c 1 x x and will take a value from 0 to 1 before the nn can be used to estimate the inundation probability it needs to be trained with a learning dataset which will be described in section 3 2 an appropriate configuration is also necessary to be defined so that the nn does not suffer from under or over fitting after several tests not shown here we decided to use a fully connected feed forward nn with one hidden layer of 10 neurons which is believed to provide a good compromise it is complex enough to represent p c 1 x x and simple enough to perform well on the data it has never seen 3 2 learning dataset a nn operates based on what it has been trained so the selected samples should represent all possibilities as much as possible hence two notes 1 for a global floodability index able to perform well in every environment the samples need to scatter across various environments under different hydrography and topography conditions if this is not done the floodability index model will work well in some areas but not in others for example if the model is trained almost with low elevation and flat condition like in deltas it will be confused when encountering high elevation and steep terrain 2 the learning dataset must not be dominated either by water samples c 1 or dry ones c 0 if sampling is done totally randomly over the globe there will be much fewer water samples than dry ones because surface water occupies only a small area on earth continental surface then the water samples will be treated as noise in the training phase of the nn therefore it is important to control that the learning dataset is made up of 50 of water samples and 50 of dry ones with this sampling strategy we built a learning dataset denoted by β 1 β s e e 1 n with s s a m p l e i o i input vector o target n the number of samples 15 000 000 the target o is a binary variable based on permanent water data and glwd for all the pixels that are permanent water the target is given 1 although we do not estimate the floodability index for permanent water we use them in the learning stage because they represent good samples of surface water for the remaining pixels the target is defined by glwd all lakes and wetlands documented by glwd give targets of 1 otherwise 0 note again that the nn can be trained with binary targets but still gives continuous values in the operational mode so it is able to represent the proxy of a probability for input vector i various topography variables are available but maybe not all of them are useful guyon and elisseeff 2003 introduced some variable selection methodologies among which a wrapper is preferable in this study as it is able to assess the usefulness of variable subsets regardless of the chosen model forward selection a flavor of the wrapper methodology was used by aires et al 2017 to decide which among 11 available variables should be used to produce the floodability index the idea is to find the best variable i 1 then find the second best i 2 that can be combined with i 1 to give best result then the third best i 3 that best combines with i 1 and i 2 and so forth normally the performance of the nn gets better each time one variable is added until a certain point k for which the performance saturates the group of variables preceding k will be selected as input for the model while the remaining variables are considered as redundant and are neglected removing redundant variables can reduce the computational load and avoid overtraining by reducing the model s complexity bishop 1995 we conduct such a forward variable selection with the merit topography dataset and select seven variables that will be used to construct the floodability index including hand sma height above the nearest small drainage m hand med height above the nearest medium drainage m hand lar height above the nearest large drainage m dist sma distance to the nearest small drainage km dist med distance to the nearest medium drainage km slope slope m m uparea upper flow accumulation km 2 eventually the final learning dataset consists of 15 000 000 samples half water half dry each with an input vector of 7 topography attributes and a single binary output 3 3 new floodability index after training the nn with the levenberg marquardt algorithm we obtain a floodability index ranging from 0 to 1 that can be used everywhere globally note that it is a static information the new floodability index is completely coherent with the permanent water because the topography variables have been defined based on this permanent water information the floodability index construction can therefore be considered as a data fusion of several datasets section 2 4 assessment of the floodability index 4 1 global results the global map of floodability index is shown in fig 3 permanent water is represented in black visually the floodability index appears to be coherent the drainage networks are well presented with high values of the index and the index value gets lower when going away from the drainages over some arid areas e g african desert or australia one can find very high floodability index however this does not mean that such arid areas are as wet or prone to be inundated as major basins like on the amazon the floodability index does not imply the frequency of water presence the floodability index is built using topography information topography can identify a pixel as a potential river pixel because of the topography configuration of that pixel but hydrologically this pixel might never have water this explains why the floodability index can be high in arid areas the floodability index does not either give a comparison between pixels far away from each other because they are not under the same hydrological and climate conditions instead it is a local hierarchy of pixels indicating the chances to be inundated among adjacent pixels if we use the floodability index to downscale a coarse resolution inundation dataset if this coarse resolution dataset says that there is no water in the arid region the floodability index will not put water where there is none arid areas can experience flash floods e g in eastern desert of egypt mashaly and ghoneim 2018 oman and north brazil saber et al 2022 with very complex flood responses that depend on many factors such as intense storms steep slopes and the lack of vegetation lin 1999 the floodability index here does not account for all factors but only topography conditions but can help to isolate pixels that are more prone to flash floods in such areas to assess the high spatial resolution of this new floodability index two layers of zoom have been represented to show the type of details that can be obtained these zooms have been done over louisiana usa with a part of the mississippi basin the amazon basin the congo basin the himalaya mountainous area and some russian peatland 4 2 comparison with previous version this new improved floodability index is compared with its previous version which was developed by aires et al 2017 at the global scale when aggregated to the resolution of 0 025 the two versions have a correlation coefficient of 0 7 this is not a very pertinent quality measure since the floodability index is not done to compare regions across the globe but rather to provide a pixel hierarchy at the local scale however such measurements can be a scene of progress in quality when considering pixels in the maximum water extent derived from gswo pixels with water occurrence 0 the old floodability index is correlated with gswo by a coefficient of 0 4 while this coefficient is increased to the 0 5 level with the new floodability index when looking at the global maps it is not an easy task to differentiate the old and the new floodability indices however it is possible to zoom on several locations to see more details e g in fig 4 full resolution the new version appears to be an improvement compared to the older version we can see in fig 4a that artifacts from the older topography have been suppressed the continuity of the new floodability is also improved in some cases e g fig 4b case 3 in fig 4 illustrates that the location of river beds could be shifted in the older version of the floodability index due to imprecise topography and due to the fact that permanent water information was not used to define the nn inputs of the model these 3 cases show some improvements that could not be seen at the global scale but that can drastically change when looking at the more local scale of course the new floodability index is far from the perfection but it is shown here how it is possible to improve it when feeding the model with better information and when the modeling is done in a smarter way beside the coherency with permanent water the integration of permanent water in the new floodability index is also one of its advantages over the old version especially for the downscaling of low resolution data this is illustrated by fig 5 the old a and new b floodability indices can be compared in the first row in this given domain the permanent water occupies 21 of the area if we allocate 21 of water in this domain using the older floodability index the water surface in blue fig 5c is far from the true permanent water this can be understood since there is no constraint in the old floodability index or the downscaling in aires et al 2017 to force the retrieval of any permanent waters this problem is well solved when using the new floodability index that actually integrates the permanent water information pixels in blue fig 5d if the flooding is increased by 10 to 31 fig 5e f the agreement is much better which is a good sign for the coherency of the two floodability indices but errors are still present what is noticeable in fig 5f is that the added flooding is well placed around the permanent water as it was designed for 4 3 regional assessment fig 6 compares the floodability index with gswo globecover map gfplain250m and glwd over the amazon basin the biggest drainage basin in the world almost entirely covered by the amazon forest the hydrological structures in gswo are well represented by the floodability index this similarity high water occurrence versus high floodability index occurs where there is little tree cover which can be seen in the tree density map and by the red dots in the scatterplot fig 6b d this is a good point of the floodability index it can represent well what gswo reports however many smaller structures in the floodability index are not found in the gswo specifically where there is a higher tree density there is no judge on this dissimilarity as no easy direct observation can be used to evaluate them vegetation prevents the optical observations to detect water below it in places with high tree density gswo simply cannot detect water presence the land cover map fig 6e shows some notable wetlands and water bodies for example large wetlands in the south of the basin 14 s 66 w in the west 5 s 75 w in the center 0 s 62 w and in the east 3 s 52 w have very high floodability index that appears to be reasonable the wetlands in glwd fig 6f are reproduced well in most cases with a high floodability index glwd itself does not include small hydrological structures as its resolution is not very high 30 but thanks to the additional high resolution information of permanent water data that is fused small structures are present in the new floodability index in fig 6g the floodplains from gfplain250m are represented even small water bodies they clearly correspond to high floodability index areas this good agreement between the floodability index and this independent dataset is very positive it can be explained by the topography origin of the two datasets though it was exploited in two different ways results of some other areas with different natural conditions are put in appendix overall the floodability index works well in areas where inundation is driven by natural topography e g congo basin fig a 1 and less efficiently where there are artificial irrigation practices e g mekong delta fig a 2 peatland is a specific environment where flooding is naturally controlled but it is driven more locally by precipitation for instance and less by river connection in such areas the floodability index cannot always reproduce well the complex distribution of surface water e g siberian peatland fig a 3 and a specific work would be necessary to improve the floodability index in this type of environment 4 4 limitation of building this floodability index the floodability index presented here is not perfect and building it also has some limitations first it immensely focuses on natural conditions more related to river networks making the floodability index less pertinent where flooding is independent of river flows such as rice fields where inundation is often under human control or peatlands where inundation is mainly caused by precipitation or groundwater adding more information for such particular regions e g number of rice crops per year groundwater level could be a solution to improve the floodability index this is not straightforward to be done on a global scale to obtain a global floodability index this could be done more easily on local scales to produce multiple localized floodability indices however how to smoothly piece them together would then be a challenging problem the second limitation concerns the evaluation of the floodability index it is not an easy task because of the lack of good references a global high resolution dataset providing long term water dynamics adequate even below vegetation would be ideal but such a dataset is currently unavailable neither from space nor from in situ inventories good data for particular regions of the world may exist in some locations and can be used locally but they are difficult to access a good evaluation is important as it can help examine thoroughly where the index works well and where it does not from a thorough evaluation we could review the floodability index model and improve it if necessary this study neglects the fact that topography may change in form of land subsidence or uplift and rivers may change their paths although those changes generally happen very little over time ignoring them is therefore not a big issue they can be more significant in some locations however topography and hydrography data are not available for short term intervals so this cannot be considered in this framework 5 applications of the floodability index like a swiss army knife the floodability index has several potential applications downscaling low resolution data is the most natural application it is also the core of almost all other ones below we will introduce some other applications of this floodability index 5 1 downscaling tool low resolution lr surface water datasets originates for instance from passive microwave observations or model simulations sippel et al 1998 prigent et al 2001 yamazaki et al 2011 coe et al 2012 they are delivered regularly in all weather and day night conditions which makes them an essential tool to monitor surface water in general and wetlands more specifically both giems primarily from passive microwave and gsw monthly water history from landsat are available on a monthly basis but only giems provides spatially continuous estimation in every time step while gsw suffers from many missing data however the low spatial resolution makes data like giems not well suited for local applications downscaling lr data is therefore a practical solution to obtain high resolution information this has been done in many studies such as galantowicz 2002 li et al 2013 and aires et al 2013a among various downscaling techniques that have been proposed using the floodability index emerged as an efficient method with its global operability fluet chouinard et al 2015 used the floodability index to downscale globally giems from resolution of 0 25 to 15 arc second creating the static product giems d15 this method was then further developed by aires et al 2017 resulting in the giems d3 a dynamic dataset with a resolution of 3 arc second the principle of this downscaling technique is that if a lr cell is n inundated water will be allocated to the n high resolution hr pixels with highest floodability index as the pixels with higher floodability index are more prone to be inundated than the other ones we developed in companion paper ii thu hang and aires 2023 a more sophisticated technique with the permanent water integrated coherently into our floodability index our downscaling is also a way to fuse hr water permanent information into the lr data in a totally coherent way due to the technical and systemic complexity the whole downscaling process will be described in detail in the paper ii of this study thu hang and aires 2023 5 2 analysis tool at high spatial resolution it is very difficult to compare datasets available at different resolution the solution is to transfer them to the same grid either by upscaling or downscaling them in this way one can more easily perform both visual and quantitative analyses upscaling allows comparing them at low resolution and seems simpler however many important details can be missed in this way downscaling to analyze them at high resolution requires more efforts but it is the only solution to access some details on the datasets the floodability index via its downscaling application is therefore a useful tool to analyze datasets at different spatial resolutions here we illustrate how to use the floodability index as an analysis tool confronting giems 25 km resolution and the thmb model 9 km resolution data both datasets provide inundation fraction on a monthly basis data of one month may 1996 were extracted and downscaled to water extent maps at 90 m resolution called giems d and thmb d fig 7 note that the downscaling of both giems and the model is already an interesting improvement of these two datasets in fact many interesting hr features have been introduced in them by this process one can find easily where water is allocated by the one of the two or by both datasets orange green and blue in the hr water extent map synthesized with some diagnostics in table 3 128 000 km 2 of water about half of the total of water is reported by both giems d and thmb d distributed mostly in the mainstream and central lines of channels however giems d records more water than thmb d the mismatches between the two datasets occur mainly in the vicinity of big streams the thmb d does not report the big wetland in the north but many strange patterns scattering over the area giems d is likely to be more reliable because it is based on real observation if this is the case giems d could certainly be useful to calibrate or constrain the model and data assimilation could also be considered at the hr 5 3 data fusion tool to combine multiple surface water datasets with several available sources of surface water observation each with its own advantages and inconvenience data fusion is a way to better exploit all the available observations to produce a better dataset however how to aggregate data effectively is indeed challenging depending on the application the data fusion needs to be adapted from simple to more complex approaches it will be shown below three examples of how to use the floodability index as a data fusion tool over the amazon basin 5 3 1 monthly water extent firstly for the monthly water extent giems prigent et al 2007 thmb model coe et al 2012 and gsw monthly water pekel et al 2016 data at a single time step will be fused see fig 8 the floodability index is used to downscale giems and thmb data from 25 km and 9 km resolution to 90 m resulting in giems d and thmb d on the contrary the gsw with a resolution of 30 m is upscaled to the same grid as the other two each water extent map is now binary with water dry states our illustrative fusion rule is that if at least 2 3 more than half datasets are water then the pixel is set as water in the output map this very simple rule is based on a confidence level generally the more datasets that report water at a point the more likely water should be present there we do not choose the threshold of 3 i e water is present if all datasets agree on that because any dataset can have omission error for example the gsw often fails to detect water below dense tree cover meanwhile the threshold of 1 is inadequate to conclude the water presence because any dataset can have commission error if we have 20 datasets instead of three as now we will have more choices of thresholding and what would be obtained finally can be even more reliable fleischmann et al 2021 5 3 2 minimum maximum extent the data fusion of minimum maximum extent from giems gswo pekel et al 2016 and sar hess et al 2015 data is considered in fig 9 providing the minimum and maximum water extent is essential for water or ecology management 180 months of giems are downscaled using the floodability index after that the minimum extent is derived by identifying pixels that are always inundated during the whole period and the maximum extent is derived by including all pixels that have been inundated at least one time the minimum maximum extent from gswo after upscaling this data is obtained in a similar way minimum extent is where occurrence 0 9 and maximum extent is where occurrence 0 the occurrence threshold of 0 9 for minimum extent is used instead of 1 detected as water all times to compensate for the omissive detection due to vegetation or some other erroneous classification suggested by aires et al 2018 the sar data provided at 90 m resolution with the 2 states of inundation is ready for use we take the maximum extent of all the minimum maximum extents to obtain the new minimum maximum extent considering such a maximum is useful if what is of interest is how far away water can spread from the streams in the low and high season for example one wants to build an infrastructure close to the river but never inundated 5 3 3 water occurrence water occurrence must be derived from a time series of water data and so far not many data sources provide data continuously over a long time period we choose giems thmb and gswo because of their different natures i e passive microwave observation model simulation optical observation the data fusion is illustrated in fig 10 the time series of giems and thmb were first downscaled by the floodability index giems d and thmb d then the water occurrence from each dataset was calculated as noted in section 5 2 many scattered bizarre patterns can be found in the thmb data they are most likely model errors therefore we use the maximum extent map from data fusion of giems sar and gsw data section 5 3 2 to correct the thmb d outside the reference maximum extent water occurrence becomes 0 with this correction the erroneous model patterns are eliminated the gswo is upscaled from 30 m to 90 m by taking the average water occurrence differs from the first two cases in that it is a continuous value so the simplest version of data fusion is averaging water occurrence from all datasets however gswo with its optical origin does not work well below vegetation therefore we use the tree density cover data to control the contribution of gswo in the data fusion if tree density is high 70 optical observation almost fails to detect water gswo is neglected new water occurrence giems d thmb d 2 if tree density is not very high 70 but gswo still underestimates water presence gswo 10 of min giems d thmb d new water occurrence giems d thmb d 2 if tree density is not very high and three datasets give comparable values new water occurrence giems d thmb d gswo 3 in the fused water occurrence map any ever inundated area reported by at least one dataset is represented such as the large wetland in the north missed by the thmb model and very fuzzy in gsw data but well detected by giems the hydrology structure is made more apparent than in the individual datasets for example the main river with considerably high values stands out from the floodplain on its banks note that this data fusion rule could be made even more complex if more auxiliary data was exploited a machine learning approach could be considered but this is outside the scope of this paper 5 4 ancillary tool for water retrieval the retrieval of surface water from remote sensing data can be an ambiguous task for instance shadow or bare ground can be misclassified as surface water with sar data because they all have low backscattering coefficient i e appear as dark areas in sar images erroneous water detection can also happen with optical data due to the similarity of the spectral characteristics between water bodies and other objects different approaches have been proposed to eliminate such false detection such as martinis et al 2015 using slope and elevation information to differentiate water and shadows in mountainous areas behnamian et al 2017 implementing a cleanup procedure based on water boundaries detection or aires et al 2013b using several a priori information to de noise and perform missing data filling using the floodability index as ancillary information would also be an interesting way to improve the accuracy of surface water retrieval by remote sensing the idea is that the retrieval can exploit the spatial coherency provided by the floodability index for instance if a water pixel has a significantly lower floodability index than dry pixels within the local area that pixel was probably wrongly classified this is a post processing using the floodability index as in aires et al 2017 however the floodability index can also potentially used as one of the inputs of the retrieval algorithm because it synthesized already many sources of information such as topography and other surface water products we list here this potential application of the floodability index however it will not be illustrated in this paper but will be tested in a forthcoming study 6 conclusions and perspectives a new floodability index was produced from topography based information using a neural network model compared to previous versions this floodability index includes certain improvements firstly we used a better topography dataset as input it is more accurate and totally coherent with the permanent water dataset that is used the issue of surface water artifacts has been avoided and the patterns of the floodability index are also improved in the new version secondly we integrated permanent water information into the floodability index which makes the floodability index more pertinent for its various applications the floodability index is already itself a tool to gather multiple hydrological information with its help the comparison analysis and fusion of multiple datasets at different resolution become more feasible it could even facilitate the water retrieval by using a post processing step or as an input information for the retrieval in a context where multiple datasets appear with different spatial resolutions and with completely different natures from visible to active and passive microwave observations the future relies in a new approach to optimally combine them to obtain the optimal synthesis it was shown here several ways to do so using the floodability index due to the reliance on only topography based information the floodability index is not perfect especially over areas where inundation is not driven by natural topography condition however we have seen that it is possible to improve it by improving the information that it uses any new topography and new permanent waters could be integrated to obtain a better index other information e g surface water observations if combined will help achieve a better quality of the floodability index for instance over peatlands that are very particular areas with many isolated water bodies we could use other a priori datasets of peatland observations e g data of hugelius et al 2020 to develop a dedicated floodability index for areas with human controlled flooding we can use irrigation information in addition e g cropland map of salmon et al 2015 information on wetland type could be useful too in the floodability index and this could be implemented in the nn approach adopted here finally regionalizing the floodability index is a potential strategy to better take into account local conditions instead of a single neural network trained from the globally sampled training dataset small regions can have their own floodability index models built from their own training datasets however compiling regional floodability indices to obtain a consistent global picture without discontinuities across regions boundaries is then a challenge that needs to be addressed not only important to reconcile datasets at different spatial resolution the floodability index can also facilitate the data fusion of datasets with different temporal resolution the floodability index can be used for temporal interpolation e g producing weekly data from monthly data thus synchronizing datasets for easier comparison or fusion credit authorship contribution statement thu hang nguyen methodology software validation investigation writing original draft visualization filipe aires conceptualization methodology writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to thank the embassy of france in vietnam for funding this work via the france excellence scholarship program and thank campus france for their logistic support and facilitation appendix regional assessment of the floodability index in the congo basin fig a 1 the wetland around the major river is striking in all maps except the gswo due to the dense tree cover the drainage system not presented by the globecover map but quite obvious in gswo and glwd is reproduced well by the floodability index the lakes and wetlands in the southeast of the basin are conspicuous across all four maps in the mekong basin largely covered by cropland the floodability index gswo and glwd do not agree much fig a 2 this is not surprising given that in croplands surface water is more controlled by irrigation than by topography note also that the optical observation cannot detect water when the crop is high such a discrepancy was also noted in aires et al 2017 when comparing the floodability index from topography with the one from modis data over rice paddy area in vietnam peatland is a specific type of wetland concentrated mainly in the boreal region fig a 3 shows a peatland domain in siberia where the inundation seems to be totally natural there are similar patterns between floodability index and gswo with a quite clear correlation between them if the tree density is low red points in the scatter plot most of water bodies in globecover have high floodability index and gswo values however the floodability index does not capture well the interlace wetland in the bottom middle of the globecover map and the complex mixture of freshwater marsh floodplain and peatland in the north part of glwd 
2034,firstly a new global floodability index with a resolution of 3 arc second is built from topography based information provided by the merit database using a neural network approach the topography and permanent water were defined in a coherent way ensuring the coherency between the resulting floodability index and permanent water which is unprecedented in previous versions the evaluation of the floodability index is done with independent observation datasets on surface water and land cover showing good performances in areas where surface water is naturally driven by topography conditions and limitation in human affected areas and some specific environments like peatland secondly some of the applications that the floodability index can serve are introduced including downscaling low resolution data analyzing and comparing datasets at different resolution and data fusion keywords floodability index surface water wetland topography data fusion data availability data will be made available on request 1 introduction the importance of surface water to life on earth has been documented countless times regarding many different aspects such as the environment and habitats ramsar convention secretariat 2016 gokce 2019 the production activities yoo and boyd 1994 pimentel et al 1997 dieter et al 2018 and the public health sector pruss et al 2002 konradsen et al 2013 satellite data has been applied to study and monitor surface water for a long time three types of data are commonly used 1 the optical pekel et al 2016 buchhorn et al 2020 2 the active microwave hess et al 2015 rosenqvist et al 2020 and 3 the passive microwave sippel et al 1998 prigent et al 2007 parrens et al 2017 observations by the optical sensors can be done frequently with a medium to high spatial resolution e g modis with a 250 m resolution and a near daily revisit time or sentinel 2 constellation with a 10 m resolution and a 5 day revisit time however such observations are affected by weather conditions and vegetation coverage the active microwave or sar observations have high spatial resolution and are much less sensitive to clouds however the backscattering signal from ground is sensitive to vegetation and active microwave data from past satellites have very low temporal resolution e g 35 days for ers1 2 new generation of satellites despite of much shorter revisit time along with high spatial resolution e g 10 m resolution and 6 day revisit time of sentinel 1 constellation does not yet offer data regularly for every region the passive microwave is the least sensitive to vegetation but has a strong limitation of very coarse resolution e g 27 km resolution of smmr sensor onboard the nimbus 7 satellite table 1 describes briefly some surface water datasets for reference there are nowadays plenty of satellite derived products on surface water with each of them having its own pros and cons from their original nature and with their varying spatial and temporal resolution the question is now how to exploit them in an efficient way we present a global topography based floodability index as a tool to change or put multiple datasets into the same grid so that we can easily compare analyze and fuse them to obtain a more comprehensive dataset on surface water topography based floodability index is defined as a proxy for the probability that a pixel will be inundated compared to adjacent pixels consider a floodability index ranging from 0 to 1 the higher the index compared to its neighbors the more likely the pixel will be inundated compared to its neighborhood in other words it is a local assessment tool provided at the global scale that helps understanding the local spatial pattern of inundation with that operation the floodability index can be used for instance to downscale low resolution lr data fluet chouinard et al 2015 aires et al 2017 downscaling is actually not only a way to retrieve high resolution hr information from lr data but also a stepping stone to aggregate multiple lr data at high resolution for different purposes as stated above it is possible to derive a floodability index from satellite observations in this case the floodability index is defined as the probability for a pixel to be inundated this derivation requires data over a long time record therefore it can be done more easily at a local scale for some regions e g over niger delta by aires et al 2013b over vietnam by aires et al 2017 both with modis time series in this case the floodability index is at the resolution of the satellite data and it is possible to have a seasonality or even more specific characteristics at the global scale so far it is impossible to obtain a hr floodability index from satellite observation due to the deficiency of hr global data to work satisfactorily in all conditions for long term with enough time samples this is actually expected to be solved by the upcoming launch of the swot satellite in 2022 https swot jpl nasa gov here we use topography information to take advantage of the global availability of this kind of data and because the flooding state is naturally driven partly by topography conditions such as slope and elevation woods 2005 flooding is also dependent on other factors like land cover or irrigating activities woods 2005 vriend 2005 but for now we only take into account topography and temporarily disregard other factors previously such a topography based floodability index was proposed by fluet chouinard et al 2015 which can be considered as the first version and later by aires et al 2017 with more detail and some improvements better resolution optimized topography variables selection neural network model although the floodability index is based on topography information some surface water datasets are still needed in the process the coherency between all the sources of information required to develop the floodability index is essential for instance to ensure that one river is not double counted or that the coast of a lake is more likely to be inundated than the pixels further from the lake this coherency however was not ensured in previous versions this study aims at two goals 1 to improve the floodability index previously designed in our group and 2 to show some potential applications of the floodability index for 1 first we use a newer topography that has been improved by solving problem of trees and better filtering the shuttle radar topography mission srtm problems yamazaki et al 2017 2019 second a high resolution permanent water information resulting from the data fusion of several hr water datasets yamazaki et al 2014 2015 will be integrated in our floodability index this means that the floodability index performs already a data fusion of surface water datasets third the topography based parameters used to build the floodability index will be defined in complete coherency with the permanent water database to further increase their coherency for 2 such applications could be performed with any floodability index we will obviously illustrate them here using our new floodability index section 2 describes all the datasets used in this study including those for the floodability index construction and those for evaluation and applications how the floodability index is build will be presented in detail in section 3 and the results will be shown in section 4 section 5 will introduce some of the potential applications of the floodability index 2 databases 2 1 merit topography the multi error removed improved terrain digital elevation model merit dem provides globally terrain elevation at a horizontal spatial resolution of 3 a r c s e c o n d 90 m at the equator it was developed from existing digital elevator models dems including shuttle radar topography mission srtm 3 of nasa alos world 3d aw3d of jaxa and viewfinder panoramas vfp dem the major error components from the original dems were separated and eliminated hence a better vertical accuracy for this newer dem yamazaki et al 2017 based on the merit dem hydrography information was retrieved yamazaki et al 2019 from which a variety of related variables were calculated some of these variables are related to the presence of water aires et al 2017 and will be considered in the floodability index construction such as flow distance to and height above the small medium and big nearest drainage and terrain slope some examples are given in fig 1a d small medium and large drainages are defined by an upper flow accumulation area greater than 0 5 km 2 10 km 2 and 100 km 2 respectively the big advantage of using merit dem is its coherency with the observed permanent water that will be mentioned in section 2 2 2 such coherency was not assured in the hydrosheds dem used in previous studies fluet chouinard et al 2015 aires et al 2017 2 2 surface water databases 2 2 1 global lakes and wetlands database glwd the glwd synthesizes and combines various existing maps data and information such as the digital chart of the world world conservation monitoring centre wcmc and others lehner and döll 2004 it presents the lake and wetland distribution at three levels at level 1 and level 2 the database includes polygons representing large lakes and reservoirs glwd 1 and smaller permanent water bodies glwd 2 data at level 3 glwd 3 given in raster format at a resolution of 30 s provides a classification of lakes and wetlands fig 1e twelve types are listed in table 2 glwd 3 with its diverse classes will be used to train the floodability index model 2 2 2 permanent water the permanent water data fig 1f itself does not exist but is extracted from the global database of river width at 3 second resolution the river width data is obtained from multiple data sources including global 1 arc second water body map g1wbm global surface water occurrence gswo and open street map yamazaki et al 2014 2015 this database is advantageous since it describes even small canals for some regions and prevents virtual drainage due to a possibly misleading topography the river width is calculated and assigned for river center line pixels pixels that are permanent water but not in the center line are also noted hence a permanent water mask was derived with its high resolution the permanent water data is used to supplement the glwd with the small permanent water bodies and rivers that are not resolved by glwd 2 3 datasets for evaluation and applications 2 3 1 global surface water gsw the gsw dataset maps the spatial and temporal variability of open surface water at global scale and qualifies its long term changes pekel et al 2016 the dataset was obtained from three million landsat images spanning over 32 years 1984 2015 with a spatial resolution of 30 m an expert system was used to classify each landsat pixel as water dry and non valid observation the classification results were assembled for each month in the 32 years forming the monthly water history product this product was then used to derive some thematic maps on surface water dynamic characterization including the gswo global surface water occurrence which represents the frequency of water presence over the 32 year period and the maximum water extent which indicates every pixel that has ever been detected as water during the whole period due to the image acquisition by optical sensors of landsat satellites one can find a lot of missing data non observation in each single monthly water map of the gsw such spatial discontinuities can make the monthly maps useless in studies over large areas however when assembling multiple monthly maps over a long time period to make thematic products the generated statistical information makes sense for open water despite the fact that the water beneath vegetation cannot be reported gswo documents an actually observed frequency of open water presence as previously commented this is not what the floodability index reports still gswo is a good choice to assess the floodability index section 4 because they are both datasets at global scale and with high resolution and open water should be in agreement the maximum water extent and the monthly water history will be used for some applications of the floodability index section 5 2 3 2 globcover landcover map the globcover landcover map is a product of the globcover project an initiative of esa and partners bicheron et al 2006 the map was generated from meris envisat observations it covers the 180 w to 180 e 90 n to 65 s domain with a spatial resolution of 300 m 22 types of land cover were classified including different kinds of cropland forest shrub water etc the globcover map can provide reference information for the evaluation of the floodability index section 4 2 3 3 gfplain250m dataset of earth s floodplains the global dataset gfplain250m nardi et al 2019 provides a binary delineation of floodplains at 8 33 arcsecond resolution 250 m at equator the spatial extent of floodplains was obtained by a geomorphic algorithm which implemented terrain analysis procedures on shuttle radar topography mission srtm digital terrain model the gfplain250m map excludes areas with insignificant water availability such as deserts and icy high latitude this dataset will be used to evaluate the floodability index over some regions section 4 2 3 4 global inundation extent from multiple satellite giems giems is a global dataset of surface water extent and dynamics over a period of 15 years 1993 2007 the estimate of giems was obtained by a multi wavelength algorithm that exploited a range of satellite observations the passive microwave measurement from special sensor microwave imager ssm i the active microwave backscattering coefficients from european remote sensing ers satellite and the visible and near infrared reflectances from avhrr and the derived normalized difference vegetation index ndvi the inundation is primarily detected based on microwave emissivity calculated from the brightness temperature measured by ssm i the active microwave data with a high contrast between open water and water beneath vegetation helps estimate the contribution of vegetation to ssm i measurement the other data source ndvi helps differentiate bare soil and inundation in semi arid areas the detailed algorithm can be found in prigent et al 2001 2007 giems estimates the inundated fraction on a monthly basis over an equal area grid with each cell occupying about 773 km 2 0 25 0 25 at the equator the combination of different types of observations takes advantage of their complementary strengths and thus the problems related to each individual one is limited the evaluation of giems with existing independent datasets shows its consistency with them in various environments major river system irrigated fields small lakes network and at various latitudes tropical boreal however there is also a tendency of giems to underestimate the low fractional inundation and overestimate the high fractional inundation giems will be used in this study to illustrate some applications of the floodability index it will be downscaled then analyzed and fused with other datasets at high resolution section 5 2 3 5 inundation fraction data from thmb model the thmb terrestrial hydrology model with biogeochemistry model is a numerical model that simulates the water flow including river discharge water height above flood stage and inundation fraction based on topography conditions from dem precipitation from local climate data base flow from ecosystem model surface runoff from ecosystem model and evaporation from water surfaces from local climate data and energy balance model coe et al 2012 the model runs on a 5 min grid 9 km at the equator over the amazon and tocantins basin from 1939 to 1998 the original time step of the model was 1 h the model output was then averaged on a monthly basis similar to giems the monthly inundation fraction data from this model version 1 2 will be used in the application section section 5 2 3 6 sar data from the mosaics of japanese earth resources satellite jers 1 sar imagery a comprehensive map of wetland extent vegetation cover types and dual season inundation states in the lowland amazon basin was produced by hess et al 2015 the acquisition time of the sar images was during october november 1995 and may june 1996 the low and high water seasons in this area are defined as these two times to produce the classification map a wetland mask was first derived by a segmentation and clustering process based on the mean backscattering coefficient the wetland pixels were then processed using a rule set based on dual season backscattering values and labeled in five vegetation cover types non vegetated herbaceous shrub woodland forest and in two inundation states flooded non flooded the dataset is available in a geographic coordinate system with a 3 a r c s e c o n d resolution similar to the floodability index the land cover information it provides is not taken into account in this study but the flooding states are representative for the average low and high water conditions corresponding to the minimum and maximum water extent in a year therefore the flooding information of the two seasons is extracted from the dataset it would be interesting to fuse it with the low resolution data downscaled by the floodability index section 5 3 2 3 7 tree cover density the tree cover density is a data layer under the collection 2 of the copernicus global land cover layers provided by the copernicus global land service buchhorn et al 2020 the collection is an annual global dataset 180 w to 180 e 78 25 n to 60 s 2015 to 2019 produced from sentinel 2 observations with an overall accuracy 80 the spatial resolution is 100 m at the equator tree cover density will be used for the evaluation of the floodability index section 4 3 floodability index model the general flowchart for building our floodability index is shown in fig 2 firstly a sampling procedure is implemented globally on topography and water data among 11 potential topography variables useful ones are selected together with water information forming the learning dataset to train the neural network model topography is input water is output after being trained the model uses global data of selected topography variables as inputs to produce the floodability index 3 1 neural network the floodability index is modeled using a neural network nn with inputs being topography variables the floodability index can be associated with a conditional probability of a pixel to be inundated given its neighboring conditions let x be the input vector of the nn x be the vector of topography attributes of a given pixel and c its inundation state c 0 for dry c 1 for inundation the floodability index can be expressed by p c 1 x x and will take a value from 0 to 1 before the nn can be used to estimate the inundation probability it needs to be trained with a learning dataset which will be described in section 3 2 an appropriate configuration is also necessary to be defined so that the nn does not suffer from under or over fitting after several tests not shown here we decided to use a fully connected feed forward nn with one hidden layer of 10 neurons which is believed to provide a good compromise it is complex enough to represent p c 1 x x and simple enough to perform well on the data it has never seen 3 2 learning dataset a nn operates based on what it has been trained so the selected samples should represent all possibilities as much as possible hence two notes 1 for a global floodability index able to perform well in every environment the samples need to scatter across various environments under different hydrography and topography conditions if this is not done the floodability index model will work well in some areas but not in others for example if the model is trained almost with low elevation and flat condition like in deltas it will be confused when encountering high elevation and steep terrain 2 the learning dataset must not be dominated either by water samples c 1 or dry ones c 0 if sampling is done totally randomly over the globe there will be much fewer water samples than dry ones because surface water occupies only a small area on earth continental surface then the water samples will be treated as noise in the training phase of the nn therefore it is important to control that the learning dataset is made up of 50 of water samples and 50 of dry ones with this sampling strategy we built a learning dataset denoted by β 1 β s e e 1 n with s s a m p l e i o i input vector o target n the number of samples 15 000 000 the target o is a binary variable based on permanent water data and glwd for all the pixels that are permanent water the target is given 1 although we do not estimate the floodability index for permanent water we use them in the learning stage because they represent good samples of surface water for the remaining pixels the target is defined by glwd all lakes and wetlands documented by glwd give targets of 1 otherwise 0 note again that the nn can be trained with binary targets but still gives continuous values in the operational mode so it is able to represent the proxy of a probability for input vector i various topography variables are available but maybe not all of them are useful guyon and elisseeff 2003 introduced some variable selection methodologies among which a wrapper is preferable in this study as it is able to assess the usefulness of variable subsets regardless of the chosen model forward selection a flavor of the wrapper methodology was used by aires et al 2017 to decide which among 11 available variables should be used to produce the floodability index the idea is to find the best variable i 1 then find the second best i 2 that can be combined with i 1 to give best result then the third best i 3 that best combines with i 1 and i 2 and so forth normally the performance of the nn gets better each time one variable is added until a certain point k for which the performance saturates the group of variables preceding k will be selected as input for the model while the remaining variables are considered as redundant and are neglected removing redundant variables can reduce the computational load and avoid overtraining by reducing the model s complexity bishop 1995 we conduct such a forward variable selection with the merit topography dataset and select seven variables that will be used to construct the floodability index including hand sma height above the nearest small drainage m hand med height above the nearest medium drainage m hand lar height above the nearest large drainage m dist sma distance to the nearest small drainage km dist med distance to the nearest medium drainage km slope slope m m uparea upper flow accumulation km 2 eventually the final learning dataset consists of 15 000 000 samples half water half dry each with an input vector of 7 topography attributes and a single binary output 3 3 new floodability index after training the nn with the levenberg marquardt algorithm we obtain a floodability index ranging from 0 to 1 that can be used everywhere globally note that it is a static information the new floodability index is completely coherent with the permanent water because the topography variables have been defined based on this permanent water information the floodability index construction can therefore be considered as a data fusion of several datasets section 2 4 assessment of the floodability index 4 1 global results the global map of floodability index is shown in fig 3 permanent water is represented in black visually the floodability index appears to be coherent the drainage networks are well presented with high values of the index and the index value gets lower when going away from the drainages over some arid areas e g african desert or australia one can find very high floodability index however this does not mean that such arid areas are as wet or prone to be inundated as major basins like on the amazon the floodability index does not imply the frequency of water presence the floodability index is built using topography information topography can identify a pixel as a potential river pixel because of the topography configuration of that pixel but hydrologically this pixel might never have water this explains why the floodability index can be high in arid areas the floodability index does not either give a comparison between pixels far away from each other because they are not under the same hydrological and climate conditions instead it is a local hierarchy of pixels indicating the chances to be inundated among adjacent pixels if we use the floodability index to downscale a coarse resolution inundation dataset if this coarse resolution dataset says that there is no water in the arid region the floodability index will not put water where there is none arid areas can experience flash floods e g in eastern desert of egypt mashaly and ghoneim 2018 oman and north brazil saber et al 2022 with very complex flood responses that depend on many factors such as intense storms steep slopes and the lack of vegetation lin 1999 the floodability index here does not account for all factors but only topography conditions but can help to isolate pixels that are more prone to flash floods in such areas to assess the high spatial resolution of this new floodability index two layers of zoom have been represented to show the type of details that can be obtained these zooms have been done over louisiana usa with a part of the mississippi basin the amazon basin the congo basin the himalaya mountainous area and some russian peatland 4 2 comparison with previous version this new improved floodability index is compared with its previous version which was developed by aires et al 2017 at the global scale when aggregated to the resolution of 0 025 the two versions have a correlation coefficient of 0 7 this is not a very pertinent quality measure since the floodability index is not done to compare regions across the globe but rather to provide a pixel hierarchy at the local scale however such measurements can be a scene of progress in quality when considering pixels in the maximum water extent derived from gswo pixels with water occurrence 0 the old floodability index is correlated with gswo by a coefficient of 0 4 while this coefficient is increased to the 0 5 level with the new floodability index when looking at the global maps it is not an easy task to differentiate the old and the new floodability indices however it is possible to zoom on several locations to see more details e g in fig 4 full resolution the new version appears to be an improvement compared to the older version we can see in fig 4a that artifacts from the older topography have been suppressed the continuity of the new floodability is also improved in some cases e g fig 4b case 3 in fig 4 illustrates that the location of river beds could be shifted in the older version of the floodability index due to imprecise topography and due to the fact that permanent water information was not used to define the nn inputs of the model these 3 cases show some improvements that could not be seen at the global scale but that can drastically change when looking at the more local scale of course the new floodability index is far from the perfection but it is shown here how it is possible to improve it when feeding the model with better information and when the modeling is done in a smarter way beside the coherency with permanent water the integration of permanent water in the new floodability index is also one of its advantages over the old version especially for the downscaling of low resolution data this is illustrated by fig 5 the old a and new b floodability indices can be compared in the first row in this given domain the permanent water occupies 21 of the area if we allocate 21 of water in this domain using the older floodability index the water surface in blue fig 5c is far from the true permanent water this can be understood since there is no constraint in the old floodability index or the downscaling in aires et al 2017 to force the retrieval of any permanent waters this problem is well solved when using the new floodability index that actually integrates the permanent water information pixels in blue fig 5d if the flooding is increased by 10 to 31 fig 5e f the agreement is much better which is a good sign for the coherency of the two floodability indices but errors are still present what is noticeable in fig 5f is that the added flooding is well placed around the permanent water as it was designed for 4 3 regional assessment fig 6 compares the floodability index with gswo globecover map gfplain250m and glwd over the amazon basin the biggest drainage basin in the world almost entirely covered by the amazon forest the hydrological structures in gswo are well represented by the floodability index this similarity high water occurrence versus high floodability index occurs where there is little tree cover which can be seen in the tree density map and by the red dots in the scatterplot fig 6b d this is a good point of the floodability index it can represent well what gswo reports however many smaller structures in the floodability index are not found in the gswo specifically where there is a higher tree density there is no judge on this dissimilarity as no easy direct observation can be used to evaluate them vegetation prevents the optical observations to detect water below it in places with high tree density gswo simply cannot detect water presence the land cover map fig 6e shows some notable wetlands and water bodies for example large wetlands in the south of the basin 14 s 66 w in the west 5 s 75 w in the center 0 s 62 w and in the east 3 s 52 w have very high floodability index that appears to be reasonable the wetlands in glwd fig 6f are reproduced well in most cases with a high floodability index glwd itself does not include small hydrological structures as its resolution is not very high 30 but thanks to the additional high resolution information of permanent water data that is fused small structures are present in the new floodability index in fig 6g the floodplains from gfplain250m are represented even small water bodies they clearly correspond to high floodability index areas this good agreement between the floodability index and this independent dataset is very positive it can be explained by the topography origin of the two datasets though it was exploited in two different ways results of some other areas with different natural conditions are put in appendix overall the floodability index works well in areas where inundation is driven by natural topography e g congo basin fig a 1 and less efficiently where there are artificial irrigation practices e g mekong delta fig a 2 peatland is a specific environment where flooding is naturally controlled but it is driven more locally by precipitation for instance and less by river connection in such areas the floodability index cannot always reproduce well the complex distribution of surface water e g siberian peatland fig a 3 and a specific work would be necessary to improve the floodability index in this type of environment 4 4 limitation of building this floodability index the floodability index presented here is not perfect and building it also has some limitations first it immensely focuses on natural conditions more related to river networks making the floodability index less pertinent where flooding is independent of river flows such as rice fields where inundation is often under human control or peatlands where inundation is mainly caused by precipitation or groundwater adding more information for such particular regions e g number of rice crops per year groundwater level could be a solution to improve the floodability index this is not straightforward to be done on a global scale to obtain a global floodability index this could be done more easily on local scales to produce multiple localized floodability indices however how to smoothly piece them together would then be a challenging problem the second limitation concerns the evaluation of the floodability index it is not an easy task because of the lack of good references a global high resolution dataset providing long term water dynamics adequate even below vegetation would be ideal but such a dataset is currently unavailable neither from space nor from in situ inventories good data for particular regions of the world may exist in some locations and can be used locally but they are difficult to access a good evaluation is important as it can help examine thoroughly where the index works well and where it does not from a thorough evaluation we could review the floodability index model and improve it if necessary this study neglects the fact that topography may change in form of land subsidence or uplift and rivers may change their paths although those changes generally happen very little over time ignoring them is therefore not a big issue they can be more significant in some locations however topography and hydrography data are not available for short term intervals so this cannot be considered in this framework 5 applications of the floodability index like a swiss army knife the floodability index has several potential applications downscaling low resolution data is the most natural application it is also the core of almost all other ones below we will introduce some other applications of this floodability index 5 1 downscaling tool low resolution lr surface water datasets originates for instance from passive microwave observations or model simulations sippel et al 1998 prigent et al 2001 yamazaki et al 2011 coe et al 2012 they are delivered regularly in all weather and day night conditions which makes them an essential tool to monitor surface water in general and wetlands more specifically both giems primarily from passive microwave and gsw monthly water history from landsat are available on a monthly basis but only giems provides spatially continuous estimation in every time step while gsw suffers from many missing data however the low spatial resolution makes data like giems not well suited for local applications downscaling lr data is therefore a practical solution to obtain high resolution information this has been done in many studies such as galantowicz 2002 li et al 2013 and aires et al 2013a among various downscaling techniques that have been proposed using the floodability index emerged as an efficient method with its global operability fluet chouinard et al 2015 used the floodability index to downscale globally giems from resolution of 0 25 to 15 arc second creating the static product giems d15 this method was then further developed by aires et al 2017 resulting in the giems d3 a dynamic dataset with a resolution of 3 arc second the principle of this downscaling technique is that if a lr cell is n inundated water will be allocated to the n high resolution hr pixels with highest floodability index as the pixels with higher floodability index are more prone to be inundated than the other ones we developed in companion paper ii thu hang and aires 2023 a more sophisticated technique with the permanent water integrated coherently into our floodability index our downscaling is also a way to fuse hr water permanent information into the lr data in a totally coherent way due to the technical and systemic complexity the whole downscaling process will be described in detail in the paper ii of this study thu hang and aires 2023 5 2 analysis tool at high spatial resolution it is very difficult to compare datasets available at different resolution the solution is to transfer them to the same grid either by upscaling or downscaling them in this way one can more easily perform both visual and quantitative analyses upscaling allows comparing them at low resolution and seems simpler however many important details can be missed in this way downscaling to analyze them at high resolution requires more efforts but it is the only solution to access some details on the datasets the floodability index via its downscaling application is therefore a useful tool to analyze datasets at different spatial resolutions here we illustrate how to use the floodability index as an analysis tool confronting giems 25 km resolution and the thmb model 9 km resolution data both datasets provide inundation fraction on a monthly basis data of one month may 1996 were extracted and downscaled to water extent maps at 90 m resolution called giems d and thmb d fig 7 note that the downscaling of both giems and the model is already an interesting improvement of these two datasets in fact many interesting hr features have been introduced in them by this process one can find easily where water is allocated by the one of the two or by both datasets orange green and blue in the hr water extent map synthesized with some diagnostics in table 3 128 000 km 2 of water about half of the total of water is reported by both giems d and thmb d distributed mostly in the mainstream and central lines of channels however giems d records more water than thmb d the mismatches between the two datasets occur mainly in the vicinity of big streams the thmb d does not report the big wetland in the north but many strange patterns scattering over the area giems d is likely to be more reliable because it is based on real observation if this is the case giems d could certainly be useful to calibrate or constrain the model and data assimilation could also be considered at the hr 5 3 data fusion tool to combine multiple surface water datasets with several available sources of surface water observation each with its own advantages and inconvenience data fusion is a way to better exploit all the available observations to produce a better dataset however how to aggregate data effectively is indeed challenging depending on the application the data fusion needs to be adapted from simple to more complex approaches it will be shown below three examples of how to use the floodability index as a data fusion tool over the amazon basin 5 3 1 monthly water extent firstly for the monthly water extent giems prigent et al 2007 thmb model coe et al 2012 and gsw monthly water pekel et al 2016 data at a single time step will be fused see fig 8 the floodability index is used to downscale giems and thmb data from 25 km and 9 km resolution to 90 m resulting in giems d and thmb d on the contrary the gsw with a resolution of 30 m is upscaled to the same grid as the other two each water extent map is now binary with water dry states our illustrative fusion rule is that if at least 2 3 more than half datasets are water then the pixel is set as water in the output map this very simple rule is based on a confidence level generally the more datasets that report water at a point the more likely water should be present there we do not choose the threshold of 3 i e water is present if all datasets agree on that because any dataset can have omission error for example the gsw often fails to detect water below dense tree cover meanwhile the threshold of 1 is inadequate to conclude the water presence because any dataset can have commission error if we have 20 datasets instead of three as now we will have more choices of thresholding and what would be obtained finally can be even more reliable fleischmann et al 2021 5 3 2 minimum maximum extent the data fusion of minimum maximum extent from giems gswo pekel et al 2016 and sar hess et al 2015 data is considered in fig 9 providing the minimum and maximum water extent is essential for water or ecology management 180 months of giems are downscaled using the floodability index after that the minimum extent is derived by identifying pixels that are always inundated during the whole period and the maximum extent is derived by including all pixels that have been inundated at least one time the minimum maximum extent from gswo after upscaling this data is obtained in a similar way minimum extent is where occurrence 0 9 and maximum extent is where occurrence 0 the occurrence threshold of 0 9 for minimum extent is used instead of 1 detected as water all times to compensate for the omissive detection due to vegetation or some other erroneous classification suggested by aires et al 2018 the sar data provided at 90 m resolution with the 2 states of inundation is ready for use we take the maximum extent of all the minimum maximum extents to obtain the new minimum maximum extent considering such a maximum is useful if what is of interest is how far away water can spread from the streams in the low and high season for example one wants to build an infrastructure close to the river but never inundated 5 3 3 water occurrence water occurrence must be derived from a time series of water data and so far not many data sources provide data continuously over a long time period we choose giems thmb and gswo because of their different natures i e passive microwave observation model simulation optical observation the data fusion is illustrated in fig 10 the time series of giems and thmb were first downscaled by the floodability index giems d and thmb d then the water occurrence from each dataset was calculated as noted in section 5 2 many scattered bizarre patterns can be found in the thmb data they are most likely model errors therefore we use the maximum extent map from data fusion of giems sar and gsw data section 5 3 2 to correct the thmb d outside the reference maximum extent water occurrence becomes 0 with this correction the erroneous model patterns are eliminated the gswo is upscaled from 30 m to 90 m by taking the average water occurrence differs from the first two cases in that it is a continuous value so the simplest version of data fusion is averaging water occurrence from all datasets however gswo with its optical origin does not work well below vegetation therefore we use the tree density cover data to control the contribution of gswo in the data fusion if tree density is high 70 optical observation almost fails to detect water gswo is neglected new water occurrence giems d thmb d 2 if tree density is not very high 70 but gswo still underestimates water presence gswo 10 of min giems d thmb d new water occurrence giems d thmb d 2 if tree density is not very high and three datasets give comparable values new water occurrence giems d thmb d gswo 3 in the fused water occurrence map any ever inundated area reported by at least one dataset is represented such as the large wetland in the north missed by the thmb model and very fuzzy in gsw data but well detected by giems the hydrology structure is made more apparent than in the individual datasets for example the main river with considerably high values stands out from the floodplain on its banks note that this data fusion rule could be made even more complex if more auxiliary data was exploited a machine learning approach could be considered but this is outside the scope of this paper 5 4 ancillary tool for water retrieval the retrieval of surface water from remote sensing data can be an ambiguous task for instance shadow or bare ground can be misclassified as surface water with sar data because they all have low backscattering coefficient i e appear as dark areas in sar images erroneous water detection can also happen with optical data due to the similarity of the spectral characteristics between water bodies and other objects different approaches have been proposed to eliminate such false detection such as martinis et al 2015 using slope and elevation information to differentiate water and shadows in mountainous areas behnamian et al 2017 implementing a cleanup procedure based on water boundaries detection or aires et al 2013b using several a priori information to de noise and perform missing data filling using the floodability index as ancillary information would also be an interesting way to improve the accuracy of surface water retrieval by remote sensing the idea is that the retrieval can exploit the spatial coherency provided by the floodability index for instance if a water pixel has a significantly lower floodability index than dry pixels within the local area that pixel was probably wrongly classified this is a post processing using the floodability index as in aires et al 2017 however the floodability index can also potentially used as one of the inputs of the retrieval algorithm because it synthesized already many sources of information such as topography and other surface water products we list here this potential application of the floodability index however it will not be illustrated in this paper but will be tested in a forthcoming study 6 conclusions and perspectives a new floodability index was produced from topography based information using a neural network model compared to previous versions this floodability index includes certain improvements firstly we used a better topography dataset as input it is more accurate and totally coherent with the permanent water dataset that is used the issue of surface water artifacts has been avoided and the patterns of the floodability index are also improved in the new version secondly we integrated permanent water information into the floodability index which makes the floodability index more pertinent for its various applications the floodability index is already itself a tool to gather multiple hydrological information with its help the comparison analysis and fusion of multiple datasets at different resolution become more feasible it could even facilitate the water retrieval by using a post processing step or as an input information for the retrieval in a context where multiple datasets appear with different spatial resolutions and with completely different natures from visible to active and passive microwave observations the future relies in a new approach to optimally combine them to obtain the optimal synthesis it was shown here several ways to do so using the floodability index due to the reliance on only topography based information the floodability index is not perfect especially over areas where inundation is not driven by natural topography condition however we have seen that it is possible to improve it by improving the information that it uses any new topography and new permanent waters could be integrated to obtain a better index other information e g surface water observations if combined will help achieve a better quality of the floodability index for instance over peatlands that are very particular areas with many isolated water bodies we could use other a priori datasets of peatland observations e g data of hugelius et al 2020 to develop a dedicated floodability index for areas with human controlled flooding we can use irrigation information in addition e g cropland map of salmon et al 2015 information on wetland type could be useful too in the floodability index and this could be implemented in the nn approach adopted here finally regionalizing the floodability index is a potential strategy to better take into account local conditions instead of a single neural network trained from the globally sampled training dataset small regions can have their own floodability index models built from their own training datasets however compiling regional floodability indices to obtain a consistent global picture without discontinuities across regions boundaries is then a challenge that needs to be addressed not only important to reconcile datasets at different spatial resolution the floodability index can also facilitate the data fusion of datasets with different temporal resolution the floodability index can be used for temporal interpolation e g producing weekly data from monthly data thus synchronizing datasets for easier comparison or fusion credit authorship contribution statement thu hang nguyen methodology software validation investigation writing original draft visualization filipe aires conceptualization methodology writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to thank the embassy of france in vietnam for funding this work via the france excellence scholarship program and thank campus france for their logistic support and facilitation appendix regional assessment of the floodability index in the congo basin fig a 1 the wetland around the major river is striking in all maps except the gswo due to the dense tree cover the drainage system not presented by the globecover map but quite obvious in gswo and glwd is reproduced well by the floodability index the lakes and wetlands in the southeast of the basin are conspicuous across all four maps in the mekong basin largely covered by cropland the floodability index gswo and glwd do not agree much fig a 2 this is not surprising given that in croplands surface water is more controlled by irrigation than by topography note also that the optical observation cannot detect water when the crop is high such a discrepancy was also noted in aires et al 2017 when comparing the floodability index from topography with the one from modis data over rice paddy area in vietnam peatland is a specific type of wetland concentrated mainly in the boreal region fig a 3 shows a peatland domain in siberia where the inundation seems to be totally natural there are similar patterns between floodability index and gswo with a quite clear correlation between them if the tree density is low red points in the scatter plot most of water bodies in globecover have high floodability index and gswo values however the floodability index does not capture well the interlace wetland in the bottom middle of the globecover map and the complex mixture of freshwater marsh floodplain and peatland in the north part of glwd 
