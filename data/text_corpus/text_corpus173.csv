index,text
865,flow and transport in heterogeneous porous media often exhibit anomalous behavior a physical analog example is the uni directional infiltration of a viscous liquid into a horizontal oriented hele shaw cell containing through thickness flow obstacles a system designed to mimic a gravel sand medium with impervious inclusions when there are no obstacles present or the obstacles form a multi repeating pattern the change of the length of infiltration f with time t tends to follow a fickian like scaling f t 1 2 in the presence of obstacle fields laid out as sierpinski carpet fractals infiltration is anomalous i e f tn n 1 2 here we study infiltration into such hele shaw cells first we investigate infiltration into a square cell containing one fractal carpet and make the observation that it is possible to generate both sub n 1 2 and super n 1 2 diffusive behaviors within identical heterogeneity configurations we show that this can be explained in terms of a scaling analysis developed from results of random walk simulations in fractal obstacles a result indicating that the nature of the domain boundary controls the exponent n of the resulting anomalous transport further we investigate infiltration into a rectangular cell containing several repeats of a given sierpinski carpet at very early times before the liquid encounters any obstacles the infiltration is fickian when the liquid encounters the first smallest scale obstacle the infiltration sharply transitions to sub diffusive subsequently around the time where the liquid has sampled all of the heterogeneity length scales in the system there is a rapid transition back to fickian behavior an explanation for this second transition is obtained by developing a simplified infiltration model based on the definition of a representative averaged hydraulic conductivity keywords anomalous transport infiltration fractal dimension 1 introduction the term anomalous transport typically refers to transport processes where the scale of spreading of a substance does not scale in time in a manner consistent with a fickian description the prototypical example so called anomalous diffusion is where the classical time exponent of n 1 2 associated with the spreading length scale of a diffusing plume ℓ tn is replaced by super n 1 2 or sub n 1 2 diffusion exponent although the name might suggest otherwise anomalous transport occurs in and across so many systems that some have suggested it should be called ubiquitous transport eliazar and klafter 2011 physical systems that display characteristics of anomalous transport are abundant and within the context of water resources include flow and transport in porous media benson et al 2001 de anna et al 2013 edery et al 2014 le borgne et al 2008 streams and rivers aubeneau et al 2014 2015 gooseff et al 2005 haggerty et al 2002 fractured media berkowitz and scher 1997 kang et al 2016 wang and cardenas 2014 turbulent flows cushman roisin 2008 biofilms seymour et al 2004 sediment transport foufoula georgiou et al 2010 ganti et al 2010 gonzález et al 2017 and surficial earth processes falcini et al 2013 finnegan et al 2014 schumer et al 2009 stark et al 2009 voller et al 2012 voller and paola 2010 the origins of anomalous transport are not unique and causes can be difficult to disentangle e g dentz and bolster 2010 but the emergence of anomalous behaviors is typically characterized by the presence of broad scale heterogeneities occurring over multiple scales as is often seen in natural porous media berkowitz et al 2006 bijeljic and blunt 2006 dentz et al 2004 gouze et al 2008 kang et al 2014 le borgne and gouze 2008 neuman and tartakovsky 2009 serrano 1997 voller 2011 where the heterogeneity ranging from grain sizes of millimeters to geological inclusions of km promote the appearance of fast paths as well as regions of hold up while anomalous transport has been observed time and time again in natural systems a general open question still remains as how to link physical characteristics of the heterogeneous medium to measurable quantities associated with anomalous transport such as the time exponent n with a view to addressing this voller and co workers filipovitch et al 2016 voller 2015 used numerical and physical experiments to track the infiltration of liquid into a horizontally oriented hele shaw cell i e a two dimensional potential flow domain established between two closely spaced plates in their setup the cell had a square plan view ℓ ℓ and the combination of the plate spacing and the viscosity of the infiltrating fluid were chosen in such a way that the measured and simulated infiltration could be considered to be equivalent to darcy flow into a sand gravel medium batchelor 1967 the role of the control of heterogeneity on the nature of the infiltration was investigated and three general configurations were considered i empty cells with no flow obstacles ii flow obstacles forming in plan view a repeating pattern and iii flow obstacles forming in plan view a truncated fractal pattern serpinski carpet in each test cell a directional flow into the initially empty gas filled cavity was driven by a fixed head applied along the inlet edge at x 0 the progress of infiltration into the cell was measured as 1 f t plan view area of invading liquid ℓ referred to as the length of infiltration in both experiments and simulations for cases i and ii when no obstacles were present or when obstacles formed a repeating pattern the length of infiltration tended to a fickian diffusion like signal i e 2 f a t n n 1 2 where a is a constant filipovitch et al 2016 voller 2015 when the obstacles formed a truncated sierpinski carpet a power law scaling was still observed but the exponent was sub diffusive i e n 1 2 further the agreement between the experimental measurements and the numerical simulation predictions is within 95 confidence limits a result that suggests the physical validity of the simulation the observations of the nature of infiltration into a fractal obstacle field has also been related to the fundamental physics of diffusion processes the governing equations used in the direct continuum simulations of infiltration into hele shaw cells voller 2015 have the form of a diffusion transport process i e they are controlled by the divergence of a potential gradient in simple settings e g the spreading of an initial pulse it is well known that we can directly relate diffusion transport to the tracking of the random brownian motion of particles on a lattice einstein 1905 starting from this point aarão reis 2016 constructed a simulation for the infiltration of random walkers from a common boundary with continuos refreshment into a sierpinski carpet observations of the change of infiltration length with time determined from the sum of lattice sites occupied by particles showed anomalous sub diffusive exponents n consistent with the experiments and continuum simulations of infiltration into obstacle filled hele shaw cells filipovitch et al 2016 voller 2015 further through accounting for the dimensions of the bulk fractal and of the boundary from which the walkers entered the carpet aarão reis 2016 established a scaling that related the infiltration exponent n to the well known sub diffusive time exponent ν 1 2 characterizing the spreading of free walkers starting from a common point within a fractal obstacle field havlin and ben avraham 2002 kim et al 1993 reis 1995 in the study of infiltration into obstacle fields open questions with important practical implications remain first while anomalous behaviours has been reported in the literature there is ambiguity as to its exact nature for example in fitting a fractional richards model to data of moisture migration into brick gerolymatou et al 2006 arrive at a sub diffusive infiltration time exponent of n 0 43 in line with values reported by voller 2015 and filipovitch et al 2016 on the other hand in another experimental study of moisture infiltration into brick küntz and lavallée 2001 report a super diffusive time exponent of n 0 58 this begs the question how might it be possible for essentially the same heterogeneous system to exhibit both sub and super dispersive infiltration behaviors further we note that the hele shaw experimental and simulation observations were all made over the time scale of the complete infiltration into a unit fractal obstacle field in the context of solute transport through a heterogeneous system however we would expect depending on the time and spacial scales to see transitions between normal and anomalous behaviors e g bolster et al 2011 dentz et al 2004 sund et al 2016 taylor 1954 whitaker 2013 at early times before the solute encounters any heterogeneity obstacles the transport is fickian anomalous transport occurs at intermediate time scales where the solute is navigating through heterogeneities with ever changing length scales once the solute samples all heterogeneity in the system normal transport is resumed thus we ask are such transport transitions observed in our hele shaw infiltration simulations and if so once the largest obstacle heterogeneity length scale has been sampled by the infiltrating liquid how long does it take for the infiltration to return to a normal diffusive scaling the core objectives of this work are to identify the conditions under which both sub and super diffusive behaviors can be induced by the exact same heterogeneous system and to examine the nature of transitions between normal and anomalous behavior towards this end we begin by revisiting the main results in the recent work on infiltration into a hele shaw cell containing flow obstacles filipovitch et al 2016 voller 2015 we make two new departures from this starting point in the first instance we demonstrate that the observed sub diffusive behaviors seen in these direct continuum simulations match the scaling reported for random walk simulations aarão reis 2016 this allows us to induce super diffusive infiltration behaviors in our simulations by simply relocating the boundary over which the liquid is introduced into the hele shaw cell in our second departure we investigate diffusion behavior transitions during the infiltration into an obstacle field made up of a repeating pattern of unit sierpinski carpets we note two transitions at early time from normal to sub and then at later time from sub back to normal emphasis is placed on identifying the location and scale over which the second of these transitions occurs 2 sub diffusive and normal diffusive behavior we start by summarizing key results for infiltration in the fractal carpets reported in voller 2015 and filipovitch et al 2016 we consider the three plan form carpet designs shown in fig 1 these are 2nd order sierpinski 1 1 carpets of pattern numbers a n 3 and b n 5 as well as c a carpet of size 4 1 made up of a four fold repeat of a 2nd order 1 1 sierpinski carpet with pattern number n 4 the carpet spaces are initially empty gas filled and liquid is infiltrated along the left hand side by imposing a fixed unit pressure head along the boundary x 0 the pressure head on the liquid gas front within the carpet is set zero voller 2015 developed a volume of fluid hirt and nichols 1981 method to track the infiltration which we use here drawing on extensively verified and validated codes bruschke and advani 1990 voller and chen 1996 in these simulations in the spaces between obstacles we set the fluidic conductivity as k s p a c e 1 within obstacles we set the conductivity to an arbitrarily small value e g k o b s 10 15 for the n 3 and n 5 carpets the numerical calculations use a fixed time step value of δ t 0 01 and space steps δ x δ y 1 2 n 2 values that provide grid independent results simulations are terminated when the first infiltrating liquid arrives in the last column of cells in the computational domain some further information on the numerics are provided in an appendix details of the full numerical implementation testing and verification is detailed in voller 2015 and experimental validation can be found in filipovitch et al 2016 fig 2 reports as red circles the simulated log log plot of length of infiltration f vs time t for the n 3 and n 5 single carpets as previously observed by voller 2015 the black lines provide sound fits to the simulated measurements note however due to the fact that the local conditions distribution of space and obstacle undergo abrupt changes each time the infiltrating fluid encounters a new line of obstacles we should not expect the infiltration predictions to be smooth i e there will be slope changes in the log log plot these log periodic oscillations are characteristic of kinetic models in deterministic fractals akkermans et al 2008 bab et al 2008a 2008b and were recently observed in simulations of random walker infiltration from a boundary in high order carpet patterns aarão reis 2016 nevertheless the overall infiltration predictions in fig 2 show a strong linear trend well characterized by a single sub diffusive time exponent n 1 2 in the original work by voller 2015 the exponents n were obtained by a least squares fit here by contrast we make the ansatz that the scaling relating the time exponent of random walk infiltration from a boundary to random walkers from a point within the obstacle domain identified by aarão reis 2016 can also be used for our continuum simulations see details in appendix that is we determine the time exponent from 3 n ν d f d b where 4 d f log 4 n 4 log n is the fractal dimension of the carpet obstacle pattern db is the fractal dimension of the boundary over which the liquid enters and ν 1 2 is the sub diffusive time exponent of the spreading of random walkers within the carpet the appropriate values for our carpets are given in the table 1 for the power law fits in fig 2 the boundary over which the liquid enters is free of any obstacles so its fractal dimension is d b 1 and the values ν given in table 1 are those previously calculated from random walk simulations starting from a common point within the appropriate carpets balankin 2017 kim et al 1993 reis 1995 thus we stress that the exponent values obtained from eq 3 and listed in the last column of table 1 are independent of those observed in our simulations the only tie to simulation data is setting the coefficient a in the power law eq 2 see penultimate column in table 1 obtained by forcing the power law fit to pass through the terminal simulation values of f and t the infiltration simulation into the four fold repeated 2nd order n 4 using δ t 0 01 δ x δ y 1 n 2 fig 3 tells a slightly different story due to the fact that in the left hand panel of the figure a power law with exponent n 1 2 provides a close fit to the simulation it appears that the f vs t data confirms the assertion made by voller 2015 that when the obstacle pattern is repeated the infiltration exhibits normal diffusive behavior inspection of the log log plot in the right panel however reveals a more subtle picture in particular at earlier times the advance of infiltration appears to be sub diffusive and well characterized by the time exponent n 0 366 determined by the scaling in eq 3 see table 1 only at later times do we see the transition to normal diffusive behavior n 0 5 3 super diffusive behavior the scaling in eq 3 provides us with an independent means of arriving at the exponent n for single fractal carpets all that is required is a calculation of the fractal dimension of the carpet and its borders along with an estimate of the time exponent associated with the free diffusion of a particle in the carpets we have tested so far the fractal dimension of the border over which the infiltrating liquid enters the carpet is free of obstacles and hence has a fractal dimension of d b 1 it is straightforward though to create systems where the distribution of obstacles along the infiltrating boundary is fractal for example consider our n 3 and n 5 carpets shifted a half phase to the right see fig 4 in this case the flow obstacles along the infiltration boundary form a cantor set with fractal dimension 5 d b log 2 log n by setting this new boundary the scaling in eq 3 suggests power law exponents n 1 2 see table 1 which would mean that for the exact same geometry the infiltration will now exhibit a super as opposed to a sub diffusive behavior we confirm this by direct simulation on the n 3 fig 5 a and n 5 fig 5 b shifted carpets where power laws using the appropriate calculated super diffusion exponents in table 1 provide a close match to observed values this demonstrates that a domain with essentially the same heterogeneity structure can generate both sub and super diffusive behavior as a support for this finding we point to a consistency with our results and observations of moisture infiltration into building materials in particular we note that the un shifted n 3 carpet has a sub diffusive exponent of n 0 424 close to the experimental exponent n exp 0 43 reported by gerolymatou et al 2006 while the super diffusive exponent n 0 599 associated with the shifted n 3 carpet is close to the experimental exponent n exp 0 58 reported by küntz and lavallée 2001 further recognize that in our analysis it is assumed that the advance of the fluid front within the carpet will be sub diffusive ν 1 2 in some situations however the advance of the fluid front may not be sub diffusive for example a recent neutron scattering study of water absorption in clay and silicate bricks shows fluid front evolution with exponents ν as high as 0 7 and 0 9 el abd 2015 in these cases the appearance of a super diffusive infiltration is more likely but if on the infiltrating boundary d b 1 and the fractal dimension df of the domain is sufficiently small sub diffusive infiltration may still be observed comparison of figs 1 a b and 4 a b helps to explain the apparently paradoxical feature of generating sub and super diffusion form the same heterogeneity structure when infiltration begins at a border of dimension 1 fig 1 a b the front encounters a higher degree of disorder as it enters the carpet with continuously decreasing available open length in the vertical y direction thus the front penetration slows down as compared to a free medium which explains the sub diffusive scaling on the other hand when the infiltration begins at a fractal border fig 4 a b as the front moves it encounters a continuously increasing available length the front penetration is consequently faster in comparison with the normal diffusive scaling seen in a medium with homogeneously distributed obstacles this explains the appearance of super diffusive scaling observation that the same form of heterogeneity can induce both sub or super diffusion has strong practical implications many argue that it would be ideal to be able to predict anomalous behaviors and transport parameters knowing only some structural details of a heterogeneous porous medium yet the results here suggest that merely having a geostatistical representation of the heterogeneity of the system may not provide sufficient information to predict anything about the nature of anomalous behaviors one might expect rather knowing details of the distribution of heterogeneity relative to the boundary is also essential 4 transitions from anomalous to normal behavior to close we return to the previous result for infiltration into a carpet with a repeating pattern fig 1c here we explore the nature of the transition from anomalous to normal diffusion our hypothesis based on previous understanding of solute transport in heterogeneous porous media e g bolster et al 2011 dentz et al 2004 sund et al 2016 is at that at very early times the infiltration will be fickian n 0 5 then as the liquid encounters the first smallest obstacle this will switch to anomalous and once the liquid has sampled all of the obstacle length scales a fickian scaling will be reestablished to test this we define two transition points we obtain the first t 1 f 1 by using the known analytical relationship f 2 t that holds in the initial obstacle free region we arrive at the second t 2 f 2 by recording the time and infiltration length values at the point where half of the numerical cells in the last column of the first carpet x 1 are filled with liquid i e i 1 n c e l l f i j n c e l l 2 where ncell is the number of cells in the column and 0 f i j 1 is the value of the liquid fraction in cell i j our idea is that this condition coincides with the time when a representative vertical oriented in the y direction averaged filling front xm t passes from the first to the second carpet on study of the actual filling front fig 6 at this point we note that the lagging part of the infiltration liquid is just clearing the largest obstacle x 0 75 while the leading liquid has entered the second carpet x 1 thus we consider this situation to be a sound indication that the infiltrating liquid has sampled all of the heterogeneity length scales present in the system in the simulation of the infiltration into this carpet sequence in order to capture the very early time behavior we use a variable time step starting with the value δ t 0 5 10 5 and increasing the value by 10 between each time step until we reach the fixed maximum value δ t max 0 5 the space step used is δ x δ y 1 n c e l l n c e l l 48 following the simulation we fit two power laws with slopes n 0 5 through each of our transition points and join these two points with a third power law see black lines in fig 7 observation shows that these power law fits pass through the simulation data see red circles and reveal clear transitions knees in the transport exponents with an early and late time fickian diffusion behavior and an intermediate sub diffusion behavior when the infiltrating liquid is sampling all the possible heterogeneity length scales located within the first fractal carpet moreover the exponent of the fit in the intermediate region n 0 360 closely matches the exponent predicted with the random walk simulation scaling n 0 366 proposed by aarão reis 2016 see eq 3 and table 1 5 explanation of the anomalous to normal transition an interesting feature stands out in fig 7 namely the relatively rapid nature of the transitions in scaling which appear to occur sharply as the fluid encounters the first obstacle and exits the first carpet respectively here we explore these transitions in more detail to start we note that the first transition is a finite size effect which is revealed because of the low m 2 order of the carpet used in our simulation this we feel is a reasonable mimic of what might be observed in a field setting but recognize that the signal of the initial normal behavior would be quickly quenched when the order of the fractal patten is increased to illustrate in a mth order n carpet if we normalize the time to infiltrate the first carpet as t 1 the time for the liquid to reach the first obstacle is n 2 m as the order m increases it is easy to see how this time rapidly becomes negligible our particular focus will be on the second transition in fig 7 i e the transition from anomalous back to normal diffusion this transition will always manifest regardless of the order of the fractal carpet in some ways this transition behavior is reminiscent of taylor dispersion e g bolster et al 2011 sund et al 2016 taylor 1954 whitaker 2013 i e the scaling of the second centered moment of a solute plume will scale in a fickian manner after a time scale τ where the solute has had sufficient time to sample all of the heterogeneity in the system by diffusion τ l 2 d where l is a characteristic length scale and d is the diffusion coefficient however there are two important differences first the extremely sharp transition between the regimes here stands out in contrast to somewhat smoother transitions in transport systems and second the transition is entirely determined by the structure of the heterogeneity and how long it takes for the applied pressure head to drive the front through the structure essentially there is no equivalent stochastic process like diffusion that causes the sampling and potentially different rates of sampling of the heterogeneity rather due to the pseudo steady state as the fluid advances to a new point in the domain the full effect of the pressure head at the infiltrating boundary is instantaneously communicated as such in the case of a solute the combination of heterogeneity structure and the strength of diffusion dictates the type and persistence of anomalous transport while here it is the structure alone that dictates the behavior by considering a simplified treatment of the infiltration into a fractal obstacle field we can provide a somewhat mechanistic explanation for this behavior let us set the dimensions of each of the carpets as 1 1 and denote its porosity by μ also let us assume that we can characterize the infiltration into our sequence of 4 j 1 2 3 4 carpets by considering that at any point in time the averaged position of the liquid infiltration can be represented as a vertical front xm t aligned with the y axis and normal to the prevailing x directed flow then drawing on a result in voller 2015 and consistently with the scaling approach in the appendix we can show that the infiltration length into any given carpet element in the sequence is given as f c a r p e t μ s α α d f 1 where s x m j 1 is the position of the averaged filling front relative to the left hand edge of the carpet in question in this way when the filling front is in the j th carpet the infiltration length for the sequence as a whole is given by 6 f μ j 1 x m j 1 α from this expression we notice that in the first carpet j 1 the infiltration length has a power law relationship with the liquid front position in an average sense however we also note that f is constrained to increase in a linear fashion with xm i e at integer values of x m j 1 2 3 f j μ further since for our carpets with pattern numbers n 3 4 or 5 the values of 0 7 α 1 once we move beyond x m 1 we will only see small and decreasing deviations from a linear relationship between f and xm thus as our averaged filling front moves out of the first carpet there is a strong tendency for changes in the infiltration length to become linear with front position i e when infiltration moves beyond the first carpet the diffusion nature of f is determined by the diffusion nature of xm to move forward let us examine how the diffusion nature of xm changes with advance through the carpet sequence on noting that the pressure head along x 0 is h 1 and assigning the pressure head along the liquid front to be gauge h x m 0 we can assume that the speed of the front at a given time will be determined by the product of an effective conductivity k xm and the average head gradient between x 0 and the front on appropriate scaling in time 7 d x m d t ν k x m x m the material in our appendix tells us that in the first carpet xm tν this implies from eq 7 that the representative conductivity in the first carpet scales as k 1 x m x m 2 ν 1 ν from this we can infer that in any carpet in our sequence j 1 2 3 4 the representative conductivity in j 1 x j 1 s is k j s s 2 ν 1 ν where 0 s x m j 1 1 is length of liquid penetration into the carpet j on the other hand the representative conductivity in the entire liquid domain to that point 0 x xm needs to be evaluated by the harmonic mean 8 k x m j 1 x m k j 1 s x m s 2 ν 1 ν 1 j 1 s j 1 s 1 ν ν note by the choice of our dimensions that the representative conductivity for each fully filled carpet is k j 1 1 on recognizing that typical values of ν 0 45 0 5 we can conclude that in and beyond the second carpet j 1 the value of the representative conductivity eq 8 takes on a nearly constant value close to 1 as a result in this region the solution of eq 7 is 9 x m 2 ν t 1 2 ν 1 2 x m 1 where the correction term 1 2 ν arises due to the initial condition at the end of the first carpet i e t 1 when x m 1 since as already noted we expect that ν 0 45 0 5 this correction will be small and we see that in and beyond the second carpet the average front movement has a close to normal behavior x m t 1 2 thus due to the fact that beyond the first carpet the infiltration f scales with the average front position xm we should expect that the infiltration will revert to a close to normal diffusive time scaling once our averaged filling front enters the second carpet the key driver in this reversion to normal diffusion is embodied in the nature of the averaging conductivity k xm see eq 8 at early infiltration times when the liquid front is in the first carpet the length scale over which the averaging is made is small and infiltrating liquid encounters changes in the heterogeneity obstacle size that have a large influence in determining the value of k here we see a power law dependence in k on xm and through solution of eq 7 a sub diffusive power law dependence of xm in time is maintained this reflects the fact that as the liquid infiltrated into the first carpet it continues to encounter further non explored heterogeneity structures that are sufficient to keep ahead of the quenching effect of an increasing averaging length scale but once the infiltrating fluid has sampled all of the heterogeneity length scales the earlier power law exponent cannot keep up with the averaging length scale from this point on while we still see fluctuations in the value k xm they are around the constant value k 1 and their amplitude rapidly decays with advance of xm so this simple description provides a mechanism to explain how a transition from anomalous to close to normal transport will rapidly occur once all of the heterogeneity length scales in the system have been sampled 6 robustness of the results here it has been useful to study infiltration into fractal obstacle fields for three reasons 1 we have a validated direct numerical simulation 2 predictions from the simulation provide a clear signal of anomalous transport behavior and 3 the time exponent of this signal can be explained by the scaling associated with brownian motion in fractal obstacle fields to have practical benefit however it is worthwhile to discuss how the infiltration into our hele shaw analog is related to more realistic porous media flows first we note that the flow in a hele shaw cell is a potential flow and thus the fluid flux is directly analogous to a darcy flux in a porous medium indeed in the hele shaw infiltration experiments reported in filipovitch et al 2016 it is argued through scaling that the set up matches the infiltration of water into a background media of gravel sand containing impermeable flow obstacles in a physical setting however we would expect much less abrupt changes in the fluidic conductivity thus one way to model what might happen in a more realistic porous medium is to relax the sharp contrast between the channels and obstacles in our hele shaw infiltration simulation we do this by simulating infiltration into an n 3 order 2 cell δ x 1 36 δ t 0 005 assigning to each computational cell in a channel a fluidic conductivity chosen at random from the range 0 5 1 5 and to each cell in an obstacle a random value from the range 0 0 0 5 in this way our obstacles become permeable and the boundaries between channels and obstacles becomes fuzzy see insets in fig 8 the simulations for this case still recover a strong power law behavior and clearly show the switch from sub to super diffusion as the placement of the now fuzzy obstacles is shifted thus this simulation indicates that in studying infiltration into more realistic porous media we should expect to see the anomalous behaviors observed in our hele shaw simulations of course as reflected in many of the papers we have cited here a keen area of anomalous transport research is directed at understanding transport of solute phases in saturated porous media in these situations it is possible that the exact behaviors associated with infiltration may manifest in a different fashion however since the underlying governing equations in both infiltration and solute transport in porous media share a common root i e the darcy equation we do expect to see similar behaviors that is a switch between sub and super diffusion depending on the nature of the injection boundary and transition to normal transport once all heterogeneity sizes have been sampled there is no doubt that the simulations presented here and previous numerical and experimental work filipovitch et al 2016 voller 2015 reveal that infiltration into fractal carpet objects even with low order truncations is power law in time with a sub diffusive exponent what is surprising as noted originally in voller 2015 is the fact that the exponent predicted for a particular carpet design does not have a notable dependence on the order of the fractal truncation although this has been fully investigated elsewhere filipovitch et al 2016 voller 2015 for completeness it is given a brief exploration here to this end fig 9 shows the log log infiltration predictions for an n 3 carpets with orders 2 3 and 4 the best fit slopes through these predictions is extremely consistent n 0 422 n 0 421 and n 0 421 respectively and close to the fractal theoretical value of n 0 424 see table 1 thus we conclude that to obtain anomalous diffusion for infiltration it is not necessary to have a high order fractal distribution of heterogeneity rather 2 3 heterogeneity scales with the largest approaching the domain size is sufficient 7 conclusion in brief summary the basic component in our infiltration simulations is a horizontally x y plain oriented hele shaw cell containing a fractal pattern sierpinski carpet of flow obstacles the direction of flow is principally in the x direction and liquid is infiltrated into an initially gas filled domain by imposing and holding a fixed gauge pressure along the y boundary at x 0 we have shown that when our domain consists of a single carpet aligned so that the boundary over which the infiltrating liquid enters is free of flow obstacles the evolution of the infiltration length exhibits a sub diffusive behavior on the other hand when the infiltrating boundary contains obstacles distributed as a fractal a cantor set simply obtained by shifting the same obstacle carpet as before a 1 2 phase in the longitudinal direction the infiltration is super diffusive both the sub and super behaviors can be explained by an appropriate scaling analysis in addition we have examined infiltration into a domain consisting of a repeating sequence of fractal carpet components here we show that once the infiltrating flow has sampled the largest heterogeneity length scale in the system i e as the infiltrating flow moves from the first to the second component the progress of the infiltration undergoes a rapid transition from an anomalous to a normal diffusive like scaling the hele shaw cell set up used in our simulations which have been extensively validated with experimental measurements is physically equivalent to horizontal fluid infiltration into a gravel sand porous medium containing a distribution of flow obstacles of varying sizes thus our findings have direct consequences for porous media in particular our results explain why it would be possible simply by the nature of the boundary over which flow enters to observe both sub and super transport behaviors in a given medium without fundamentally changing the nature of the heterogeneity distribution within the medium itself our results also clearly illustrate and explain why once the length scale of the domain of interest exceeds the length scale of the largest heterogeneity in the system signals of anomalous behavior are rapidly quenched and normal scaling begins acknowledgments vrv acknowledge support from the national science foundation through grant ear 1318593 generalized transport models in earth surface dynamics fdaar was partially supported by an mts visiting professorship provided by the department of civil environmental and geo engineering university of minnesota and by brazilian agencies cnpq 304766 2014 3 and faperj e 26 202941 2015 we also appreciate significant discussion with colleagues kimberly hill mark meerschaert and chris paola in addition we acknowledge the insight and suggestions provided by the reviewers appendix a appendix scaling approach here we outline the arguments for adopting the scaling approach which relates the length of infiltration to the exponent ν 1 2 associated with the spreading of a free random walker within a fractal obstacle pattern first to provide some background we discuss key features of the direct simulation for infiltration into a hele shaw cell as outlined in voller 2015 the governing equation is the volume of fluid equation a 1 f t k h where 0 f 1 is a fill factor and h is the pressure head the initial condition is f 0 and the boundary conditions are a fixed head h 1 along x 0 a fixed head h 0 along x 1 and no flow conditions along y 0 and y 1 to make a connection between the fill factor and head the additional constitutive relationship h 0 at all points where f 1 is used in the numerical solution the subsequent simulation see details in voller 2015 is based on a mesh ncell ncell of uniform sized node centered control volumes the main dependent variables stored at a node point i j is the pressure head h i j and the fill fraction f i j this latter variable takes values of f i j 1 in control volumes that are filled with liquid f i j 0 in control volumes that are empty gas filled and 0 f i j 1 at the liquid gas front note as detailed in voller 2015 the numerical solution in the simulation is constructed so that the liquid only has a width of one control volume and that in control volumes where f i j 1 the associated pressure head takes the value h i j 0 from the numerically calculated nodal fill fraction field we can make two separate measures for the advance of infiltration with time the first is the advance xm t of an averaged liquid front that is aligned in the y direction in our work here motivated by the fundamental connection between the diffusion and random walk processes we have made the ansatz that the time exponent associated with this representative filling front matches the exponent ν observed in the spreading of random walkers starting from a common point within the fractal carpet see 4th column of table 1 i e xm tν the other measure of infiltration as used in the body of the paper is the infiltration length f t n c e l l 2 f i j i e the plan view area containing the liquid the length of the boundary over which the infiltrating liquid enters the domain in this way we can envisage a representative infiltration process in an n pattern carpet of order m 1 where at each stage p 1 2 3 m to maintain similarity the expanding infiltrated domain fits into a box of size xm xm with x m n p length units i e a domain that covers the p th order of the chosen carpet to move forward we note that the fraction of open area in this domain the porosity is given by μ 4 n 4 n 2 p assuming a cantor set distribution of obstacles for the carpets shifted a half phase to the right fig 4 the fraction of open length on the infiltrating boundary is μ b 2 n p thus according to the given definition the infiltration length can be expressed as a 2 f μ n p n p μ b n p from here with appropriate algebra it can be shown that f x m d f d b on substitution of the power law for the advance of the representative liquid front xm tν the scaling relation in eq 3 immediately follows 
865,flow and transport in heterogeneous porous media often exhibit anomalous behavior a physical analog example is the uni directional infiltration of a viscous liquid into a horizontal oriented hele shaw cell containing through thickness flow obstacles a system designed to mimic a gravel sand medium with impervious inclusions when there are no obstacles present or the obstacles form a multi repeating pattern the change of the length of infiltration f with time t tends to follow a fickian like scaling f t 1 2 in the presence of obstacle fields laid out as sierpinski carpet fractals infiltration is anomalous i e f tn n 1 2 here we study infiltration into such hele shaw cells first we investigate infiltration into a square cell containing one fractal carpet and make the observation that it is possible to generate both sub n 1 2 and super n 1 2 diffusive behaviors within identical heterogeneity configurations we show that this can be explained in terms of a scaling analysis developed from results of random walk simulations in fractal obstacles a result indicating that the nature of the domain boundary controls the exponent n of the resulting anomalous transport further we investigate infiltration into a rectangular cell containing several repeats of a given sierpinski carpet at very early times before the liquid encounters any obstacles the infiltration is fickian when the liquid encounters the first smallest scale obstacle the infiltration sharply transitions to sub diffusive subsequently around the time where the liquid has sampled all of the heterogeneity length scales in the system there is a rapid transition back to fickian behavior an explanation for this second transition is obtained by developing a simplified infiltration model based on the definition of a representative averaged hydraulic conductivity keywords anomalous transport infiltration fractal dimension 1 introduction the term anomalous transport typically refers to transport processes where the scale of spreading of a substance does not scale in time in a manner consistent with a fickian description the prototypical example so called anomalous diffusion is where the classical time exponent of n 1 2 associated with the spreading length scale of a diffusing plume ℓ tn is replaced by super n 1 2 or sub n 1 2 diffusion exponent although the name might suggest otherwise anomalous transport occurs in and across so many systems that some have suggested it should be called ubiquitous transport eliazar and klafter 2011 physical systems that display characteristics of anomalous transport are abundant and within the context of water resources include flow and transport in porous media benson et al 2001 de anna et al 2013 edery et al 2014 le borgne et al 2008 streams and rivers aubeneau et al 2014 2015 gooseff et al 2005 haggerty et al 2002 fractured media berkowitz and scher 1997 kang et al 2016 wang and cardenas 2014 turbulent flows cushman roisin 2008 biofilms seymour et al 2004 sediment transport foufoula georgiou et al 2010 ganti et al 2010 gonzález et al 2017 and surficial earth processes falcini et al 2013 finnegan et al 2014 schumer et al 2009 stark et al 2009 voller et al 2012 voller and paola 2010 the origins of anomalous transport are not unique and causes can be difficult to disentangle e g dentz and bolster 2010 but the emergence of anomalous behaviors is typically characterized by the presence of broad scale heterogeneities occurring over multiple scales as is often seen in natural porous media berkowitz et al 2006 bijeljic and blunt 2006 dentz et al 2004 gouze et al 2008 kang et al 2014 le borgne and gouze 2008 neuman and tartakovsky 2009 serrano 1997 voller 2011 where the heterogeneity ranging from grain sizes of millimeters to geological inclusions of km promote the appearance of fast paths as well as regions of hold up while anomalous transport has been observed time and time again in natural systems a general open question still remains as how to link physical characteristics of the heterogeneous medium to measurable quantities associated with anomalous transport such as the time exponent n with a view to addressing this voller and co workers filipovitch et al 2016 voller 2015 used numerical and physical experiments to track the infiltration of liquid into a horizontally oriented hele shaw cell i e a two dimensional potential flow domain established between two closely spaced plates in their setup the cell had a square plan view ℓ ℓ and the combination of the plate spacing and the viscosity of the infiltrating fluid were chosen in such a way that the measured and simulated infiltration could be considered to be equivalent to darcy flow into a sand gravel medium batchelor 1967 the role of the control of heterogeneity on the nature of the infiltration was investigated and three general configurations were considered i empty cells with no flow obstacles ii flow obstacles forming in plan view a repeating pattern and iii flow obstacles forming in plan view a truncated fractal pattern serpinski carpet in each test cell a directional flow into the initially empty gas filled cavity was driven by a fixed head applied along the inlet edge at x 0 the progress of infiltration into the cell was measured as 1 f t plan view area of invading liquid ℓ referred to as the length of infiltration in both experiments and simulations for cases i and ii when no obstacles were present or when obstacles formed a repeating pattern the length of infiltration tended to a fickian diffusion like signal i e 2 f a t n n 1 2 where a is a constant filipovitch et al 2016 voller 2015 when the obstacles formed a truncated sierpinski carpet a power law scaling was still observed but the exponent was sub diffusive i e n 1 2 further the agreement between the experimental measurements and the numerical simulation predictions is within 95 confidence limits a result that suggests the physical validity of the simulation the observations of the nature of infiltration into a fractal obstacle field has also been related to the fundamental physics of diffusion processes the governing equations used in the direct continuum simulations of infiltration into hele shaw cells voller 2015 have the form of a diffusion transport process i e they are controlled by the divergence of a potential gradient in simple settings e g the spreading of an initial pulse it is well known that we can directly relate diffusion transport to the tracking of the random brownian motion of particles on a lattice einstein 1905 starting from this point aarão reis 2016 constructed a simulation for the infiltration of random walkers from a common boundary with continuos refreshment into a sierpinski carpet observations of the change of infiltration length with time determined from the sum of lattice sites occupied by particles showed anomalous sub diffusive exponents n consistent with the experiments and continuum simulations of infiltration into obstacle filled hele shaw cells filipovitch et al 2016 voller 2015 further through accounting for the dimensions of the bulk fractal and of the boundary from which the walkers entered the carpet aarão reis 2016 established a scaling that related the infiltration exponent n to the well known sub diffusive time exponent ν 1 2 characterizing the spreading of free walkers starting from a common point within a fractal obstacle field havlin and ben avraham 2002 kim et al 1993 reis 1995 in the study of infiltration into obstacle fields open questions with important practical implications remain first while anomalous behaviours has been reported in the literature there is ambiguity as to its exact nature for example in fitting a fractional richards model to data of moisture migration into brick gerolymatou et al 2006 arrive at a sub diffusive infiltration time exponent of n 0 43 in line with values reported by voller 2015 and filipovitch et al 2016 on the other hand in another experimental study of moisture infiltration into brick küntz and lavallée 2001 report a super diffusive time exponent of n 0 58 this begs the question how might it be possible for essentially the same heterogeneous system to exhibit both sub and super dispersive infiltration behaviors further we note that the hele shaw experimental and simulation observations were all made over the time scale of the complete infiltration into a unit fractal obstacle field in the context of solute transport through a heterogeneous system however we would expect depending on the time and spacial scales to see transitions between normal and anomalous behaviors e g bolster et al 2011 dentz et al 2004 sund et al 2016 taylor 1954 whitaker 2013 at early times before the solute encounters any heterogeneity obstacles the transport is fickian anomalous transport occurs at intermediate time scales where the solute is navigating through heterogeneities with ever changing length scales once the solute samples all heterogeneity in the system normal transport is resumed thus we ask are such transport transitions observed in our hele shaw infiltration simulations and if so once the largest obstacle heterogeneity length scale has been sampled by the infiltrating liquid how long does it take for the infiltration to return to a normal diffusive scaling the core objectives of this work are to identify the conditions under which both sub and super diffusive behaviors can be induced by the exact same heterogeneous system and to examine the nature of transitions between normal and anomalous behavior towards this end we begin by revisiting the main results in the recent work on infiltration into a hele shaw cell containing flow obstacles filipovitch et al 2016 voller 2015 we make two new departures from this starting point in the first instance we demonstrate that the observed sub diffusive behaviors seen in these direct continuum simulations match the scaling reported for random walk simulations aarão reis 2016 this allows us to induce super diffusive infiltration behaviors in our simulations by simply relocating the boundary over which the liquid is introduced into the hele shaw cell in our second departure we investigate diffusion behavior transitions during the infiltration into an obstacle field made up of a repeating pattern of unit sierpinski carpets we note two transitions at early time from normal to sub and then at later time from sub back to normal emphasis is placed on identifying the location and scale over which the second of these transitions occurs 2 sub diffusive and normal diffusive behavior we start by summarizing key results for infiltration in the fractal carpets reported in voller 2015 and filipovitch et al 2016 we consider the three plan form carpet designs shown in fig 1 these are 2nd order sierpinski 1 1 carpets of pattern numbers a n 3 and b n 5 as well as c a carpet of size 4 1 made up of a four fold repeat of a 2nd order 1 1 sierpinski carpet with pattern number n 4 the carpet spaces are initially empty gas filled and liquid is infiltrated along the left hand side by imposing a fixed unit pressure head along the boundary x 0 the pressure head on the liquid gas front within the carpet is set zero voller 2015 developed a volume of fluid hirt and nichols 1981 method to track the infiltration which we use here drawing on extensively verified and validated codes bruschke and advani 1990 voller and chen 1996 in these simulations in the spaces between obstacles we set the fluidic conductivity as k s p a c e 1 within obstacles we set the conductivity to an arbitrarily small value e g k o b s 10 15 for the n 3 and n 5 carpets the numerical calculations use a fixed time step value of δ t 0 01 and space steps δ x δ y 1 2 n 2 values that provide grid independent results simulations are terminated when the first infiltrating liquid arrives in the last column of cells in the computational domain some further information on the numerics are provided in an appendix details of the full numerical implementation testing and verification is detailed in voller 2015 and experimental validation can be found in filipovitch et al 2016 fig 2 reports as red circles the simulated log log plot of length of infiltration f vs time t for the n 3 and n 5 single carpets as previously observed by voller 2015 the black lines provide sound fits to the simulated measurements note however due to the fact that the local conditions distribution of space and obstacle undergo abrupt changes each time the infiltrating fluid encounters a new line of obstacles we should not expect the infiltration predictions to be smooth i e there will be slope changes in the log log plot these log periodic oscillations are characteristic of kinetic models in deterministic fractals akkermans et al 2008 bab et al 2008a 2008b and were recently observed in simulations of random walker infiltration from a boundary in high order carpet patterns aarão reis 2016 nevertheless the overall infiltration predictions in fig 2 show a strong linear trend well characterized by a single sub diffusive time exponent n 1 2 in the original work by voller 2015 the exponents n were obtained by a least squares fit here by contrast we make the ansatz that the scaling relating the time exponent of random walk infiltration from a boundary to random walkers from a point within the obstacle domain identified by aarão reis 2016 can also be used for our continuum simulations see details in appendix that is we determine the time exponent from 3 n ν d f d b where 4 d f log 4 n 4 log n is the fractal dimension of the carpet obstacle pattern db is the fractal dimension of the boundary over which the liquid enters and ν 1 2 is the sub diffusive time exponent of the spreading of random walkers within the carpet the appropriate values for our carpets are given in the table 1 for the power law fits in fig 2 the boundary over which the liquid enters is free of any obstacles so its fractal dimension is d b 1 and the values ν given in table 1 are those previously calculated from random walk simulations starting from a common point within the appropriate carpets balankin 2017 kim et al 1993 reis 1995 thus we stress that the exponent values obtained from eq 3 and listed in the last column of table 1 are independent of those observed in our simulations the only tie to simulation data is setting the coefficient a in the power law eq 2 see penultimate column in table 1 obtained by forcing the power law fit to pass through the terminal simulation values of f and t the infiltration simulation into the four fold repeated 2nd order n 4 using δ t 0 01 δ x δ y 1 n 2 fig 3 tells a slightly different story due to the fact that in the left hand panel of the figure a power law with exponent n 1 2 provides a close fit to the simulation it appears that the f vs t data confirms the assertion made by voller 2015 that when the obstacle pattern is repeated the infiltration exhibits normal diffusive behavior inspection of the log log plot in the right panel however reveals a more subtle picture in particular at earlier times the advance of infiltration appears to be sub diffusive and well characterized by the time exponent n 0 366 determined by the scaling in eq 3 see table 1 only at later times do we see the transition to normal diffusive behavior n 0 5 3 super diffusive behavior the scaling in eq 3 provides us with an independent means of arriving at the exponent n for single fractal carpets all that is required is a calculation of the fractal dimension of the carpet and its borders along with an estimate of the time exponent associated with the free diffusion of a particle in the carpets we have tested so far the fractal dimension of the border over which the infiltrating liquid enters the carpet is free of obstacles and hence has a fractal dimension of d b 1 it is straightforward though to create systems where the distribution of obstacles along the infiltrating boundary is fractal for example consider our n 3 and n 5 carpets shifted a half phase to the right see fig 4 in this case the flow obstacles along the infiltration boundary form a cantor set with fractal dimension 5 d b log 2 log n by setting this new boundary the scaling in eq 3 suggests power law exponents n 1 2 see table 1 which would mean that for the exact same geometry the infiltration will now exhibit a super as opposed to a sub diffusive behavior we confirm this by direct simulation on the n 3 fig 5 a and n 5 fig 5 b shifted carpets where power laws using the appropriate calculated super diffusion exponents in table 1 provide a close match to observed values this demonstrates that a domain with essentially the same heterogeneity structure can generate both sub and super diffusive behavior as a support for this finding we point to a consistency with our results and observations of moisture infiltration into building materials in particular we note that the un shifted n 3 carpet has a sub diffusive exponent of n 0 424 close to the experimental exponent n exp 0 43 reported by gerolymatou et al 2006 while the super diffusive exponent n 0 599 associated with the shifted n 3 carpet is close to the experimental exponent n exp 0 58 reported by küntz and lavallée 2001 further recognize that in our analysis it is assumed that the advance of the fluid front within the carpet will be sub diffusive ν 1 2 in some situations however the advance of the fluid front may not be sub diffusive for example a recent neutron scattering study of water absorption in clay and silicate bricks shows fluid front evolution with exponents ν as high as 0 7 and 0 9 el abd 2015 in these cases the appearance of a super diffusive infiltration is more likely but if on the infiltrating boundary d b 1 and the fractal dimension df of the domain is sufficiently small sub diffusive infiltration may still be observed comparison of figs 1 a b and 4 a b helps to explain the apparently paradoxical feature of generating sub and super diffusion form the same heterogeneity structure when infiltration begins at a border of dimension 1 fig 1 a b the front encounters a higher degree of disorder as it enters the carpet with continuously decreasing available open length in the vertical y direction thus the front penetration slows down as compared to a free medium which explains the sub diffusive scaling on the other hand when the infiltration begins at a fractal border fig 4 a b as the front moves it encounters a continuously increasing available length the front penetration is consequently faster in comparison with the normal diffusive scaling seen in a medium with homogeneously distributed obstacles this explains the appearance of super diffusive scaling observation that the same form of heterogeneity can induce both sub or super diffusion has strong practical implications many argue that it would be ideal to be able to predict anomalous behaviors and transport parameters knowing only some structural details of a heterogeneous porous medium yet the results here suggest that merely having a geostatistical representation of the heterogeneity of the system may not provide sufficient information to predict anything about the nature of anomalous behaviors one might expect rather knowing details of the distribution of heterogeneity relative to the boundary is also essential 4 transitions from anomalous to normal behavior to close we return to the previous result for infiltration into a carpet with a repeating pattern fig 1c here we explore the nature of the transition from anomalous to normal diffusion our hypothesis based on previous understanding of solute transport in heterogeneous porous media e g bolster et al 2011 dentz et al 2004 sund et al 2016 is at that at very early times the infiltration will be fickian n 0 5 then as the liquid encounters the first smallest obstacle this will switch to anomalous and once the liquid has sampled all of the obstacle length scales a fickian scaling will be reestablished to test this we define two transition points we obtain the first t 1 f 1 by using the known analytical relationship f 2 t that holds in the initial obstacle free region we arrive at the second t 2 f 2 by recording the time and infiltration length values at the point where half of the numerical cells in the last column of the first carpet x 1 are filled with liquid i e i 1 n c e l l f i j n c e l l 2 where ncell is the number of cells in the column and 0 f i j 1 is the value of the liquid fraction in cell i j our idea is that this condition coincides with the time when a representative vertical oriented in the y direction averaged filling front xm t passes from the first to the second carpet on study of the actual filling front fig 6 at this point we note that the lagging part of the infiltration liquid is just clearing the largest obstacle x 0 75 while the leading liquid has entered the second carpet x 1 thus we consider this situation to be a sound indication that the infiltrating liquid has sampled all of the heterogeneity length scales present in the system in the simulation of the infiltration into this carpet sequence in order to capture the very early time behavior we use a variable time step starting with the value δ t 0 5 10 5 and increasing the value by 10 between each time step until we reach the fixed maximum value δ t max 0 5 the space step used is δ x δ y 1 n c e l l n c e l l 48 following the simulation we fit two power laws with slopes n 0 5 through each of our transition points and join these two points with a third power law see black lines in fig 7 observation shows that these power law fits pass through the simulation data see red circles and reveal clear transitions knees in the transport exponents with an early and late time fickian diffusion behavior and an intermediate sub diffusion behavior when the infiltrating liquid is sampling all the possible heterogeneity length scales located within the first fractal carpet moreover the exponent of the fit in the intermediate region n 0 360 closely matches the exponent predicted with the random walk simulation scaling n 0 366 proposed by aarão reis 2016 see eq 3 and table 1 5 explanation of the anomalous to normal transition an interesting feature stands out in fig 7 namely the relatively rapid nature of the transitions in scaling which appear to occur sharply as the fluid encounters the first obstacle and exits the first carpet respectively here we explore these transitions in more detail to start we note that the first transition is a finite size effect which is revealed because of the low m 2 order of the carpet used in our simulation this we feel is a reasonable mimic of what might be observed in a field setting but recognize that the signal of the initial normal behavior would be quickly quenched when the order of the fractal patten is increased to illustrate in a mth order n carpet if we normalize the time to infiltrate the first carpet as t 1 the time for the liquid to reach the first obstacle is n 2 m as the order m increases it is easy to see how this time rapidly becomes negligible our particular focus will be on the second transition in fig 7 i e the transition from anomalous back to normal diffusion this transition will always manifest regardless of the order of the fractal carpet in some ways this transition behavior is reminiscent of taylor dispersion e g bolster et al 2011 sund et al 2016 taylor 1954 whitaker 2013 i e the scaling of the second centered moment of a solute plume will scale in a fickian manner after a time scale τ where the solute has had sufficient time to sample all of the heterogeneity in the system by diffusion τ l 2 d where l is a characteristic length scale and d is the diffusion coefficient however there are two important differences first the extremely sharp transition between the regimes here stands out in contrast to somewhat smoother transitions in transport systems and second the transition is entirely determined by the structure of the heterogeneity and how long it takes for the applied pressure head to drive the front through the structure essentially there is no equivalent stochastic process like diffusion that causes the sampling and potentially different rates of sampling of the heterogeneity rather due to the pseudo steady state as the fluid advances to a new point in the domain the full effect of the pressure head at the infiltrating boundary is instantaneously communicated as such in the case of a solute the combination of heterogeneity structure and the strength of diffusion dictates the type and persistence of anomalous transport while here it is the structure alone that dictates the behavior by considering a simplified treatment of the infiltration into a fractal obstacle field we can provide a somewhat mechanistic explanation for this behavior let us set the dimensions of each of the carpets as 1 1 and denote its porosity by μ also let us assume that we can characterize the infiltration into our sequence of 4 j 1 2 3 4 carpets by considering that at any point in time the averaged position of the liquid infiltration can be represented as a vertical front xm t aligned with the y axis and normal to the prevailing x directed flow then drawing on a result in voller 2015 and consistently with the scaling approach in the appendix we can show that the infiltration length into any given carpet element in the sequence is given as f c a r p e t μ s α α d f 1 where s x m j 1 is the position of the averaged filling front relative to the left hand edge of the carpet in question in this way when the filling front is in the j th carpet the infiltration length for the sequence as a whole is given by 6 f μ j 1 x m j 1 α from this expression we notice that in the first carpet j 1 the infiltration length has a power law relationship with the liquid front position in an average sense however we also note that f is constrained to increase in a linear fashion with xm i e at integer values of x m j 1 2 3 f j μ further since for our carpets with pattern numbers n 3 4 or 5 the values of 0 7 α 1 once we move beyond x m 1 we will only see small and decreasing deviations from a linear relationship between f and xm thus as our averaged filling front moves out of the first carpet there is a strong tendency for changes in the infiltration length to become linear with front position i e when infiltration moves beyond the first carpet the diffusion nature of f is determined by the diffusion nature of xm to move forward let us examine how the diffusion nature of xm changes with advance through the carpet sequence on noting that the pressure head along x 0 is h 1 and assigning the pressure head along the liquid front to be gauge h x m 0 we can assume that the speed of the front at a given time will be determined by the product of an effective conductivity k xm and the average head gradient between x 0 and the front on appropriate scaling in time 7 d x m d t ν k x m x m the material in our appendix tells us that in the first carpet xm tν this implies from eq 7 that the representative conductivity in the first carpet scales as k 1 x m x m 2 ν 1 ν from this we can infer that in any carpet in our sequence j 1 2 3 4 the representative conductivity in j 1 x j 1 s is k j s s 2 ν 1 ν where 0 s x m j 1 1 is length of liquid penetration into the carpet j on the other hand the representative conductivity in the entire liquid domain to that point 0 x xm needs to be evaluated by the harmonic mean 8 k x m j 1 x m k j 1 s x m s 2 ν 1 ν 1 j 1 s j 1 s 1 ν ν note by the choice of our dimensions that the representative conductivity for each fully filled carpet is k j 1 1 on recognizing that typical values of ν 0 45 0 5 we can conclude that in and beyond the second carpet j 1 the value of the representative conductivity eq 8 takes on a nearly constant value close to 1 as a result in this region the solution of eq 7 is 9 x m 2 ν t 1 2 ν 1 2 x m 1 where the correction term 1 2 ν arises due to the initial condition at the end of the first carpet i e t 1 when x m 1 since as already noted we expect that ν 0 45 0 5 this correction will be small and we see that in and beyond the second carpet the average front movement has a close to normal behavior x m t 1 2 thus due to the fact that beyond the first carpet the infiltration f scales with the average front position xm we should expect that the infiltration will revert to a close to normal diffusive time scaling once our averaged filling front enters the second carpet the key driver in this reversion to normal diffusion is embodied in the nature of the averaging conductivity k xm see eq 8 at early infiltration times when the liquid front is in the first carpet the length scale over which the averaging is made is small and infiltrating liquid encounters changes in the heterogeneity obstacle size that have a large influence in determining the value of k here we see a power law dependence in k on xm and through solution of eq 7 a sub diffusive power law dependence of xm in time is maintained this reflects the fact that as the liquid infiltrated into the first carpet it continues to encounter further non explored heterogeneity structures that are sufficient to keep ahead of the quenching effect of an increasing averaging length scale but once the infiltrating fluid has sampled all of the heterogeneity length scales the earlier power law exponent cannot keep up with the averaging length scale from this point on while we still see fluctuations in the value k xm they are around the constant value k 1 and their amplitude rapidly decays with advance of xm so this simple description provides a mechanism to explain how a transition from anomalous to close to normal transport will rapidly occur once all of the heterogeneity length scales in the system have been sampled 6 robustness of the results here it has been useful to study infiltration into fractal obstacle fields for three reasons 1 we have a validated direct numerical simulation 2 predictions from the simulation provide a clear signal of anomalous transport behavior and 3 the time exponent of this signal can be explained by the scaling associated with brownian motion in fractal obstacle fields to have practical benefit however it is worthwhile to discuss how the infiltration into our hele shaw analog is related to more realistic porous media flows first we note that the flow in a hele shaw cell is a potential flow and thus the fluid flux is directly analogous to a darcy flux in a porous medium indeed in the hele shaw infiltration experiments reported in filipovitch et al 2016 it is argued through scaling that the set up matches the infiltration of water into a background media of gravel sand containing impermeable flow obstacles in a physical setting however we would expect much less abrupt changes in the fluidic conductivity thus one way to model what might happen in a more realistic porous medium is to relax the sharp contrast between the channels and obstacles in our hele shaw infiltration simulation we do this by simulating infiltration into an n 3 order 2 cell δ x 1 36 δ t 0 005 assigning to each computational cell in a channel a fluidic conductivity chosen at random from the range 0 5 1 5 and to each cell in an obstacle a random value from the range 0 0 0 5 in this way our obstacles become permeable and the boundaries between channels and obstacles becomes fuzzy see insets in fig 8 the simulations for this case still recover a strong power law behavior and clearly show the switch from sub to super diffusion as the placement of the now fuzzy obstacles is shifted thus this simulation indicates that in studying infiltration into more realistic porous media we should expect to see the anomalous behaviors observed in our hele shaw simulations of course as reflected in many of the papers we have cited here a keen area of anomalous transport research is directed at understanding transport of solute phases in saturated porous media in these situations it is possible that the exact behaviors associated with infiltration may manifest in a different fashion however since the underlying governing equations in both infiltration and solute transport in porous media share a common root i e the darcy equation we do expect to see similar behaviors that is a switch between sub and super diffusion depending on the nature of the injection boundary and transition to normal transport once all heterogeneity sizes have been sampled there is no doubt that the simulations presented here and previous numerical and experimental work filipovitch et al 2016 voller 2015 reveal that infiltration into fractal carpet objects even with low order truncations is power law in time with a sub diffusive exponent what is surprising as noted originally in voller 2015 is the fact that the exponent predicted for a particular carpet design does not have a notable dependence on the order of the fractal truncation although this has been fully investigated elsewhere filipovitch et al 2016 voller 2015 for completeness it is given a brief exploration here to this end fig 9 shows the log log infiltration predictions for an n 3 carpets with orders 2 3 and 4 the best fit slopes through these predictions is extremely consistent n 0 422 n 0 421 and n 0 421 respectively and close to the fractal theoretical value of n 0 424 see table 1 thus we conclude that to obtain anomalous diffusion for infiltration it is not necessary to have a high order fractal distribution of heterogeneity rather 2 3 heterogeneity scales with the largest approaching the domain size is sufficient 7 conclusion in brief summary the basic component in our infiltration simulations is a horizontally x y plain oriented hele shaw cell containing a fractal pattern sierpinski carpet of flow obstacles the direction of flow is principally in the x direction and liquid is infiltrated into an initially gas filled domain by imposing and holding a fixed gauge pressure along the y boundary at x 0 we have shown that when our domain consists of a single carpet aligned so that the boundary over which the infiltrating liquid enters is free of flow obstacles the evolution of the infiltration length exhibits a sub diffusive behavior on the other hand when the infiltrating boundary contains obstacles distributed as a fractal a cantor set simply obtained by shifting the same obstacle carpet as before a 1 2 phase in the longitudinal direction the infiltration is super diffusive both the sub and super behaviors can be explained by an appropriate scaling analysis in addition we have examined infiltration into a domain consisting of a repeating sequence of fractal carpet components here we show that once the infiltrating flow has sampled the largest heterogeneity length scale in the system i e as the infiltrating flow moves from the first to the second component the progress of the infiltration undergoes a rapid transition from an anomalous to a normal diffusive like scaling the hele shaw cell set up used in our simulations which have been extensively validated with experimental measurements is physically equivalent to horizontal fluid infiltration into a gravel sand porous medium containing a distribution of flow obstacles of varying sizes thus our findings have direct consequences for porous media in particular our results explain why it would be possible simply by the nature of the boundary over which flow enters to observe both sub and super transport behaviors in a given medium without fundamentally changing the nature of the heterogeneity distribution within the medium itself our results also clearly illustrate and explain why once the length scale of the domain of interest exceeds the length scale of the largest heterogeneity in the system signals of anomalous behavior are rapidly quenched and normal scaling begins acknowledgments vrv acknowledge support from the national science foundation through grant ear 1318593 generalized transport models in earth surface dynamics fdaar was partially supported by an mts visiting professorship provided by the department of civil environmental and geo engineering university of minnesota and by brazilian agencies cnpq 304766 2014 3 and faperj e 26 202941 2015 we also appreciate significant discussion with colleagues kimberly hill mark meerschaert and chris paola in addition we acknowledge the insight and suggestions provided by the reviewers appendix a appendix scaling approach here we outline the arguments for adopting the scaling approach which relates the length of infiltration to the exponent ν 1 2 associated with the spreading of a free random walker within a fractal obstacle pattern first to provide some background we discuss key features of the direct simulation for infiltration into a hele shaw cell as outlined in voller 2015 the governing equation is the volume of fluid equation a 1 f t k h where 0 f 1 is a fill factor and h is the pressure head the initial condition is f 0 and the boundary conditions are a fixed head h 1 along x 0 a fixed head h 0 along x 1 and no flow conditions along y 0 and y 1 to make a connection between the fill factor and head the additional constitutive relationship h 0 at all points where f 1 is used in the numerical solution the subsequent simulation see details in voller 2015 is based on a mesh ncell ncell of uniform sized node centered control volumes the main dependent variables stored at a node point i j is the pressure head h i j and the fill fraction f i j this latter variable takes values of f i j 1 in control volumes that are filled with liquid f i j 0 in control volumes that are empty gas filled and 0 f i j 1 at the liquid gas front note as detailed in voller 2015 the numerical solution in the simulation is constructed so that the liquid only has a width of one control volume and that in control volumes where f i j 1 the associated pressure head takes the value h i j 0 from the numerically calculated nodal fill fraction field we can make two separate measures for the advance of infiltration with time the first is the advance xm t of an averaged liquid front that is aligned in the y direction in our work here motivated by the fundamental connection between the diffusion and random walk processes we have made the ansatz that the time exponent associated with this representative filling front matches the exponent ν observed in the spreading of random walkers starting from a common point within the fractal carpet see 4th column of table 1 i e xm tν the other measure of infiltration as used in the body of the paper is the infiltration length f t n c e l l 2 f i j i e the plan view area containing the liquid the length of the boundary over which the infiltrating liquid enters the domain in this way we can envisage a representative infiltration process in an n pattern carpet of order m 1 where at each stage p 1 2 3 m to maintain similarity the expanding infiltrated domain fits into a box of size xm xm with x m n p length units i e a domain that covers the p th order of the chosen carpet to move forward we note that the fraction of open area in this domain the porosity is given by μ 4 n 4 n 2 p assuming a cantor set distribution of obstacles for the carpets shifted a half phase to the right fig 4 the fraction of open length on the infiltrating boundary is μ b 2 n p thus according to the given definition the infiltration length can be expressed as a 2 f μ n p n p μ b n p from here with appropriate algebra it can be shown that f x m d f d b on substitution of the power law for the advance of the representative liquid front xm tν the scaling relation in eq 3 immediately follows 
866,the global prevalence of rapid and extensive land use change necessitates hydrologic modelling methodologies capable of handling non stationarity this is particularly true in the context of hydrologic forecasting using data assimilation data assimilation has been shown to dramatically improve forecast skill in hydrologic and meteorological applications although such improvements are conditional on using bias free observations and model simulations a hydrologic model calibrated to a particular set of land cover conditions has the potential to produce biased simulations when the catchment is disturbed this paper sheds new light on the impacts of bias or systematic errors in hydrologic data assimilation in the context of forecasting in catchments with changing land surface conditions and a model calibrated to pre change conditions we posit that in such cases the impact of systematic model errors on assimilation or forecast quality is dependent on the inherent prediction uncertainty that persists even in pre change conditions through experiments on a range of catchments we develop a conceptual relationship between total prediction uncertainty and the impacts of land cover changes on the hydrologic regime to demonstrate how forecast quality is affected when using state estimation data assimilation with no modifications to account for land cover changes this work shows that systematic model errors as a result of changing or changed catchment conditions do not always necessitate adjustments to the modelling or assimilation methodology for instance through re calibration of the hydrologic model time varying model parameters or revised offline online bias estimation 1 introduction as our population and ecological footprint expands so too does our impact on terrestrial hydrology rapid agricultural and economic development has already seen the loss of large swaths of forest across the globe the fao estimates global forest cover has been reduced by an area roughly the size of south africa since 1990 fao 2015 projected increases to population in close proximity to tropical forests unesco 2009 mean that land use change and its impacts on hydrology and water resources will continue to be an important issue such extensive current and potential future land use change necessitates the development of appropriate hydrologic modelling paradigms for water and land use management the traditional calibrate validate modelling method is in theory poorly suited to catchments with changing land surface conditions this has implications for short and seasonal forecasting methods that typically rely on a hydrologic model calibrated to a particular set of catchment conditions that may no longer be representative of the forecast interval in such cases there is the potential for generating biased forecasts even when state estimation data assimilation methods are utilised land use change and its implications for catchment hydrology have been studied extensively in recent years researchers have examined how hydrologic processes are altered due to land surface disturbance e g yang et al 2012 painter et al 2007 and whether changes can be detected in hydrologic observations see for example peña arancibia et al 2012 bloschl et al 2007 for instance deforestation has often been linked to increases in peak runoff and annual runoff yields in small experimental catchments e g ruprecht schofield 1991 dias et al 2015 although such an association is less clear in larger catchments where climatic influences and complex process interactions may dominate wilk et al 2001 zhou et al 2010 modeling studies have also been carried out to determine how catchment hydrology can be affected by likely future land use change scenarios see for example huisman et al 2009 choi deal 2008 isik et al 2013 however few have investigated how predictive modelling methodologies must be adapted for catchment systems that are no longer stationary some recent approaches include the work of westra et al 2014 who proposed allowing model parameters to vary as a function of selected covariates efstratiadis et al 2015 who proposed a modelling approach that explicitly incorporates known land use changes and pathiraja et al 2016a who presented a data assimilation based approach to estimate time variations in model parameters in response to signals of change in hydrologic observations however this issue has received little attention in the context of hydrologic forecasting and data assimilation data assimilation da has gained increasing attention in recent decades as a means of improving hydrologic forecasts through improved initial conditions dechant moradkhani 2011 lee et al 2011 and model parameters dechant moradkhani 2012 vrugt et al 2006 xie et al 2014 plaza et al 2012 whilst simultaneously providing uncertainty estimates many flood forecasting da studies have demonstrated improvements to forecasting skill using various da algorithms and by assimilating a range of observations including in situ and satellite soil moisture e g aubert et al 2003 alvarez garreton et al 2014 crow ryu 2009 yan moradkhani 2016 discharge e g noh et al 2013 li et al 2015 chen et al 2013 pauwels de lannoy 2009 clark et al 2008 and river levels e g ricci et al 2011 neal et al 2007 matgen et al 2010 there have been far fewer studies examining how data assimilation based streamflow forecasting performs in catchments experiencing land use change in such cases model predictions may contain systematic errors if the hydrologic model has been calibrated to land cover conditions prior to the disturbance previous studies have noted the importance of removing or accounting for systematic biases in observations and model simulations when undertaking data assimilation see for example alvarez garreton et al 2014 delannoy et al 2007 balmaseda et al 2007 dee dasilva 1998 ryu et al 2009 pauwels et al 2013 found that the benefits from standard state estimation da may be marginal when the calibrated hydrologic model gives predictions with large biases common methods to explicitly deal with forecast bias involve estimating static e g montzka et al 2011 smith et al 2013 and time varying pathiraja et al 2016a 2016b parameters online as well as through online bias estimation algorithms or bias aware filters see for instance dee 2005 pauwels et al 2013 rasmussen et al 2016 systematic errors are potentially a more pressing issue when undertaking standard state estimation da in catchments with time varying system properties however we posit that in such cases the impact of systematic errors on assimilation or forecast quality is dependent on the inherent prediction uncertainty that persists even in pre change conditions we demonstrate this by examining how forecasting performance is affected when models and da algorithms are established for a particular catchment condition and a land cover change occurs with no attempts made to account for potential biases introduced by the changes we characterize the total uncertainty of model simulations going beyond parameter and input uncertainty to also account for structural uncertainties which are particularly important for high flow events shoaib et al 2016 all statistical moments of model uncertainty are considered instead of only the mean and or covariance as is typically the case in hydrologic da studies the adopted model uncertainty quantification method assumes stationarity and no online or offline bias correction strategies are used to adapt to changed conditions observed meteorological inputs are utilized rather than meteorological forecasts in order to focus solely on the effects of changing land surface conditions on streamflow forecasting performance streamflow forecasting at various lead times is undertaken for 4 catchments paired catchment systems of varying size and levels of deforestation gauged streamflow observations are assimilated using a particle filter and the state dependent model uncertainty method of pathiraja et al 2017a to characterize total model uncertainty the remainder of this paper is structured as follows background on the methods utilised in this study is presented in section 2 in section 3 we provide details of the study catchments including land use change and the impacts of land use change on the observed hydrologic regime the experimental setup including the hydrologic models utilised and application of data assimilation and model uncertainty estimation methods are discussed in section 4 the experimental outcomes are presented section 5 followed by an analysis of results and discussion of their implications for da science in section 6 finally we conclude with a summary of the main outcomes and further work 2 background data assimilation methods aim to optimally combine observations and prior information usually from a numerical model based on their respective uncertainties effective da requires the use of appropriate assimilation methods for the task at hand as well as an accurate characterization of all uncertainties present liu et al 2012 we briefly discuss each of these components below 2 1 data assimilation using the particle filter suppose the true system of interest can be expressed by the following stochastic discrete time state space equations 1 x t f x t 1 u t ω t 2 y t h x t ɛ t where x t is the true latent state i e unobserved or hidden vector at time t u t is the model input vector at time t e g rainfall pet f is the assumed forward model h is the assumed observation operator that maps latent states to observation space output space and y t is the true system output at time t ω t and ε t are random noise terms that describe the one step transition error where ω t captures deficiencies in f and possibly u t and ε t captures deficiencies in h this means that initial condition uncertainties i e in x t 1 are not considered in ω t and ε t suppose also that available observations of the system outputs y t are corrupted by random noise 3 y t o y t η t where y t o is the observation vector at time t and η t is a random noise term that captures instrument and other errors sequential da methods involve finding solutions to bayes formula in a recursive manner the prior distribution is usually characterized by the model simulations and the likelihood function takes into consideration uncertainty in the observations the posterior density for any given time t is given by the following equation 4 p x 0 t y 0 t o p x t x t 1 p y t o x t p y t o y 0 t 1 o p x 0 t 1 y 0 t 1 o where y 0 t o y 0 o y 1 o y t o p x 0 t y 0 t o is the joint filtering posterior distribution for the trajectory from time 0 to time t p x t x t 1 characterises the uncertainty in transitioning from the state at time t 1 to the state at time t also known as the prior density p y t o x t is the likelihood of observing y t o given our prior knowledge of the states at time t and p y t o y 0 t 1 o is a normalizing constant despite the conceptual simplicity of eq 4 computing the posterior can be difficult for complex real world problems where analytical solutions are intractable several algorithms have been developed since the 1960s for such purposes including kalman and particle filters smoothers the standard kalman filter gives the optimal filtering posterior in the minimum variance sense for linear stochastic dynamic systems with gaussian errors kalman 1960 a number of kalman filter variants most notably the ensemble kalman filter evensen 1994 have been developed to improve their applicability to real world non linear and non gaussian systems unlike kalman filter based methods sequential monte carlo smc a subset of which are particle filters aim to propagate the full distribution through time without relying on assumptions of linearity or gaussianity doucet et al 2000 however a number of implementation issues such as the need for large particle sizes have limited their use in high dimensional real world applications we utilise a particle filter algorithm in this study to capture the potentially non gaussian features of the estimated prior density in deriving the posterior in particle filtering prior and posterior probability densities at any given time t are approximated by the weighted sum of a set of n particles with associated weights φ t i i 1 n arulampalam et al 2002 jeremiah et al 2012 here we use the particle filter markov chain monte carlo pf mcmc arulampalam et al 2004 which has been developed to avoid known issues associated with the standard particle filter such as degeneracy and sample impoverishment see e g chen 2003 a single assimilation cycle of the pf mcmc proceeds as follows for further details refer to arulampalam et al 2004 step 1 estimate the prior probability density function p x t x t 1 this is typically estimated by propagating each of the posterior particles x t 1 i through the forward model f and observation operator h note the superscript indicates posterior and superscript indicates prior 5 x t i f x t 1 i u t f o r i 1 n 6 y t i h x t i f o r i 1 n where x t i and y t i are model simulations however this approach does not take into account uncertainties in the forward model and or observation operator section 2 2 of this article discusses how such uncertainties can be incorporated to generate a more accurate representation of the prior step 2 estimate likelihood and update weights particle weights from time t 1 φ t 1 i i 1 n are updated based on the likelihood of the observations given the model simulations p y t o x t i using importance sampling and assuming the importance function is the prior density 7 φ t i φ t 1 i p y t o x t i f o r i 1 n 8 x t i x t i f o r i 1 n step 3 particle resampling this step is undertaken to combat the issue of degeneracy whereby all but a few particles are given negligible weight particles with high probability i e high weight are replicated whilst particles with very low probability are discarded resampling is undertaken only when the effective sample size a measure of degeneracy liu chen 1998 drops below a predefined threshold the sample importance resampling algorithm of smith gelfand 1992 is adopted in this study step 4 mcmc step finally an mcmc step is carried out to avoid the potential loss of sample diversity associated with resampling due to high weighted particles being over represented also known as sample impoverishment a perturbation is first applied to the resampled states from step 4 typically from a gaussian density and the perturbed state x t i p is accepted if a randomly generated number is less than the metropolis hastings acceptance probability α that is 9 x t i x t i p i f u α x t i r o t h e r w i s e 10 u u 0 1 11 α m i n 1 p y t o x t i p p x t i p x t 1 i r p y t o x t i r p x t i r x t 1 i r the final posterior density for time t is represented by the set of points x t i φ t i r i 1 n note the weights are unchanged in this step 2 2 state dependent model uncertainty estimation sdmu uncertainty characterization in da can be a challenging task particularly for model simulations which generally have many interacting sources of uncertainty that are individually difficult to quantify many standard methods such as perturbing model forcings parameters and or states from an assumed probability distribution are limited in that they only partially consider model structural uncertainty or rely on potentially incorrect assumptions about the error statistics which are commonly assumed gaussian in this study we adopt a method developed in previous work pathiraja et al 2017a that aims to objectively characterize model uncertainty without any such assumptions it considers the total uncertainty of model simulations instead of just parametric and or forcing uncertainty and higher order statistical moments of the error distribution are estimated by way of non parametric density estimation instead of focusing solely on the mean and or variance the approach works by using a training period to develop estimates of the additive errors in the model equations f and observation operator h from which an error density is estimated using nonparametric density estimation it is suited to cases where the observed variables are of primary interest and where the magnitude of measurement errors is negligible in comparison to model errors the approach has been shown to provide more realistic uncertainty estimates in a streamflow forecasting application resulting in improved forecasts compared to standard approaches the method is summarized below and in fig 1 for further details see pathiraja et al 2017a offline training phase step t1 a training period of length t is selected ensuring that meteorological conditions and system states likely to be experienced in the assimilation period are captured this is then used to gather data on the additive errors in the latent states and outputs estimation of these errors requires knowledge of the true latent state at the previous time x t 1 refer eq 1 since they do not include initial condition errors since this is unavailable an optimize and model strategy is utilized to obtain a best estimate of the initial condition given by x t 1 the optimization aims to generate an initial condition that provides the best fit between observations and model simulations at time t 1 using available estimates of x t 2 denoted x t 2 given by equation 13 with the appropriate change in time index as the initial guess for the optimization algorithm in this study we adopt the levenberg marquardt algorithm this is then forced through the model equations to generate x t 1 specifically the following equations are solved at all points in time in the training period 12 x t 2 o p t argmin x t 2 h f x t 2 u t 1 y t 1 o 13 x t 1 f x t 2 o p t u t 1 the advantage of this optimize and model strategy instead of optimizing x t 1 directly using h x t 1 is that physical constraints imposed by the state space model equations are satisfied because both the observation operator and the forward model are considered step t2 the aim is to then estimate the additive errors in the latent states ω t and outputs γ t where γ t denotes the total simulation error combining errors in f and h i e sum of ε t and function of ω t 14 γ t y t h f x t 1 u t h f x t 1 u t ω t ɛ t h f x t 1 u t with an estimate for the initial conditions the additive errors can be estimated by re arranging eqs 1 3 and using available observations with the assumption that measurement error is negligble compared to the model errors 15 x t f x t 1 u t 16 γ t y t o h x t 17 ω t x t x t where x t is estimated as per step 1 but with the time increment increased by 1 step t3 kernel conditional density estimation is then applied to the sample of points γ t ω t x t 1 t 3 t to derive an estimate for conditional density of the model errors given the latent states p γ ω x t 1 depending on the application model inputs such as rainfall may also be included as covariates as is the case in this study see section 4 2 we adopt the method of hayfield and racine 2008 as implemented in the np package in r to estimate this density assimilation phase step a1 for any given time t during the assimilation phase the posterior particles from the previous time t 1 x t 1 i i 1 n are forced through the model equations to generate model simulations of the latent states and outputs given by eqs 5 and 6 step a2 the posterior particles x t 1 i i 1 n and potentially any observed inputs u t are used to sample from the kernel density estimate and added on to the simulated latent states and outputs from the previous step 18 y t i y t i g t i 19 x t i x t i w t i 20 g t i w t i p γ ω x t 1 the particles x t i y t i and their associated weights form the prior density p x t x t 1 or probabilistic forecast if used in a forecasting setting note that the most important assumption in the method described above is that the statistical properties of the additive errors conditioned on the model latent states are stationary through time or at least do not change between the training and assimilation period this study will examine the extent to which streamflow forecast performance is degraded when this assumption is violated due to changing land surface conditions 3 study catchments we examine four catchments paired catchment systems of varying size properties levels of deforestation and changes to the hydrologic regimes details on the study catchments are provided below see also table 1 for a summary of catchment characteristics the location of the catchments and gauging stations are provided in fig 2 3 1 wights salmon lemon ernies south western australia located in south west western australia these two paired catchment systems are part of an experimental forest established to understand the effects of deforestation on catchment hydrology bari smettem 2004 at the beginning of the monitoring period i e 1974 all four catchments were densely forested with vegetation dominated by jarrah eucalyptus marginata and marri eucalyptus calophylla ruprecht schofield 1989 bettenay et al 1980 ruprecht schofield 1989 and bari smettem 2004 describe the treatment works undertaken which are summarized herein in the summer of 1976 1977 the wights catchment was fully cleared and 50 of the downstream portion of the lemon catchment was cleared with merchantable timber logged and the remaining timber burnt to the ground both catchments were sown with clover and grass over the may june period of 1977 from 1978 onwards both catchments were converted to agricultural use which included pinwheel raking sheep and horse grazing ploughing and bulldozer leveling salmon adjacent to wights and ernies adjacent to lemon were kept as control catchments that is no treatment was undertaken over the entire monitoring period the control catchments have been used to represent pre change conditions due to the short length of data for the treated catchments prior to land use change only 2 5 years all four catchments are relatively small in size ranging from 82 to 350 ha the local climate is characterized by dry hot summers mean monthly rainfall of approx 10 15 mm and mean daily maximum temperature approx 30 c and wet cold winters mean monthly rainfall of approx 170 mm and mean daily maximum temperature approx 15 c mccullough lund 2010 average annual rainfall at wights salmon is slightly higher than at lemon ernies which is approximately 40 km away 930 mm compared with 670 mm mean annual class a pan evaporation is around 1630 mm year based on measurements at collie about 20 km northeast of the wights catchment luke burke 1987 further details on catchment characteristics including vegetation topography and soil profile can be found in bettenay et al 1980 daily point rainfall and streamflow volume data for all catchments were obtained from the west australian department of water potential evapotranspiration pet for both catchments was estimated from evaporation maps of average monthly pet provided in luke burke 1987 which was then disaggregated uniformly to a daily time step the same daily pet time series was adopted for each catchment within the pair due to the spatial resolution of the evaporation maps changes to the hydrologic regime are evident for both wights and lemon although they are more pronounced for wights compare fig 3 a and b an immediate increase in the annual runoff coefficient a n n u a l r u n o f f v o l u m e a n n u a l r a i n f a l l v o l u m e is evident following clearing at wights the runoff coefficient continues to increase for roughly 6 years until equilibrium is established with an average annual runoff coefficient of 0 5 compared to 0 3 during the pre clear period see fig 3b similarly an immediate increase in peak flows is apparent annual maximum daily flows are near identical for wights and salmon maximum difference of 2 ml day during the pre clear period in the first year after clearing 1977 there is a sharp increase in the annual maximum daily flow such that it is 12 ml greater than the annual maximum daily flow in salmon lemon experienced a similar increase in the annual runoff coefficient as wights but over a relatively longer period compare fig 3a and 3b although in the post clear period streamflow is initiated roughly 2 to 3 months earlier than in ernies changes at the daily scale are less in magnitude than wights in terms of daily metrics it took 8 years for annual maximum daily flows in lemon to increase above ernies by 12 ml ie the observed amount of increase in wights in the first year after clearing the less prominent changes in lemon compared to wights are likely due to the less extensive disturbance to the land surface conditions 3 2 nammuc vietnam nammuc is a small medium sized catchment 2880 km2 located in the red river basin in northern vietnam the catchment is of interest as vietnam experienced extensive deforestation starting mainly from the 1970s although the government has recently established reforestation policies wwf 2013 land cover changes in the nammuc catchment appear to be patchy with evergreen leaf decreasing from about 60 30 in favor of cropland which increased from about 23 52 in contrast to the other catchments considered in this study it is not easy to identify distinct time periods of changing vs no change conditions based on the available land cover map information and the changes to observed runoff we conclude that the most rapid and extensive case of deforestation occurred in the early 1990s further catchment details including land use change are provided in pathiraja et al 2017b and are summarized here the local climate is monsoon dominated with two distinct seasons the wet season ranges from may to october and is characterized by high temperatures about 28 c whilst the dry season ranges from november to april with cooler temperatures about 18 c average annual rainfall across the catchment varies between 1300 and 2000 mm the annual streamflow pattern is unimodal with high flows during the wet season usually peaking in july vu 1993 available data consists of point daily rainfall and temperature recorded at 4 stations within and around the catchment area and daily catchment outlet streamflow see fig 2 records spanning the period 1975 2004 were made available by the vietnamese institute of water resources planning catchment averaged rainfall and temperature was calculated using thiessen polygons with potential evapotranspiration estimated using the empirical temperature latitude based hamon pet method hamon 1961 fig 3c shows that changes in the hydrologic regime are noticeable from the mid 1990s onwards the annual runoff coefficient varies between 0 4 and 0 6 during the nominal pre change period and between 0 6 and 0 8 afterwards according to pathiraja et al 2017b and wang et al 2012 there are increases to wet and dry season streamflow although increases in annual runoff yields are mostly due to an increase in the baseflow volume 3 3 comet north eastern australia the comet catchment is located in the fitzroy basin in central queensland australia prior to settlement it was forested with vegetation dominated by mixed eucalyptus species 52 as well as brigalow acacia harpophylla 23 and softwood scrubs 8 siriwardena et al 2006 details of the treatment works and catchment characteristics including climatic conditions are provided in siriwardena et al 2006 and are summarized below they report that clearing was undertaken in the mid 1960s and largely completed by 1970 eventually leading to agricultural activity including cropping grazing and construction of gully dams as a result clearing of regrowth has continued till present with peña arancibia et al 2012 reporting that an additional 10 of native vegetation or plantation forest was cleared between 1980 and 2009 forest cover has reduced from 91 of the total catchment area to approximately 40 see table 1 of siriwardena et al 2006 clearing has predominantly occurred close to the main river channel as well as near the downstream tributaries refer fig 2 and also peña arancibia et al 2012 comet is the largest of the study catchments at 16 400 km2 the local climate is dry sub humid with a summer rainfall dominance i e hot wet summers average daily maximum temperatures range from 34 c in january to 22 c in july streamflow is ephemeral generally occurring during the wetter months december april and peaking in february mean annual rainfall is approximately 660 mm and a small rainfall gradient across the catchment from east to west is apparent potential evapotranspiration is estimated to be slightly less than at the western australian catchments with average annual values of 1260 mm based on estimates using the hamon method hamon 1961 gridded daily rainfall was used due to the size of the catchment and the sparsity of gauges in close proximity with records spanning pre and post change conditions this was sourced from the australian water availability project awap raupach et al 2008 which consists of interpolated gridded daily rainfall at approximately 5 km grid size covering the entire australian continent rainfall in each grid cell overlapping the catchment was used to generate lumped catchment averaged daily rainfall for this study daily streamflow data for the catchment comes from the comet weir maintained by the queensland department of natural resources and mines obtained from the water information monitoring portal https water monitoring information qld gov au the gauge was converted to a continuous logger and moved 6 4 km upstream in 1971 although this has not produced a discontinuity in the record siriwardena et al 2006 and given the length of the tributary it is not likely to make a material difference on the overall flow records streamflow data was reported to be of good quality with only 3 of the record being a derived discharge the data was also checked for consistency with estimated rainfall daily pet was estimated using the empirical temperature latitude based hamon pet method hamon 1961 and average daily temperatures from weather stations maintained by the australian bureau of meteorology http www bom gov au climate data index shtml in terms of annual metrics changes to the hydrologic regime following clearing are not as pronounced in comet as the other catchments refer fig 3d this is partly due to the ephemeral nature of the catchment annual runoff coefficient 0 15 and the complexity and interaction of rainfall runoff processes over such a large area siriwardena et al 2006 nonetheless previous studies have provided some insights into the effects of land use change on the comet catchment at various time scales peña arancibia et al 2012 identified two distinct periods of post change response as indicated in fig 3d and table 2 and concluded from an analysis of daily streamflow metrics that clearing of native vegetation enhanced stormflow of all magnitudes immediately after clearing although slower flows were decreased however they attributed changes to annual streamflow yields mainly to climatic variability based on results from a tomer schilling analysis tomer schilling 2009 an analysis of the daily streamflow and rainfall records show that response time was also affected by clearing but that this returned to normal in the 2nd post clear period prior to clearing the lag time between peak rainfall and peak runoff was generally 3 5 days this decreased to 2 days in some cases immediately after clearing and then returned to 3 5 days in the 2nd post clear period 4 experimental setup 4 1 hydrological models conceptual rainfall runoff models were adopted for each of the study catchments given that only streamflow observations at the outlet are available previous work with the western australian catchments showed that the probability distributed model pdm moore 2007 modified to include a deep groundwater leak produced good streamflow simulations pathiraja et al 2016b the 9 parameter hydrologiska byrans vattenbalansavdelning hbv model bergström et al 1995 without snow component was utilized for the nammuc catchment based on previous work by pathiraja et al 2017b finally gr4j perrin et al 2003 was used for the comet catchment based on the work of pagano et al 2010 who showed its efficacy in modeling a range of australian catchments all three models have a low dimensional state space 3 4 and 2 states for modified pdm hbv and gr4j respectively the models were calibrated to daily streamflow data using the shuffled complex evolution algorithm sce ua duan et al 1993 for pdm and gr4j and the borg evolutionary algorithm hadka and reed 2013 for hbv using a portion of the pre change period as defined in table 2 and the nash sutcliffe efficiency as the objective function 4 2 streamflow forecasting using data assimilation streamflow forecasts at 1 3 and 5 day lead times were generated by assimilating daily streamflow observations with model predictions model predictions were produced by combining additive uncertainty estimates from the sdmu method described in section 2 2 with hydrologic model simulations generated using observed meteorological inputs i e perfect meteorological forecast uncertainty estimates were developed for pre change conditions and model parameters were not modified during the assimilation phase meaning that the model structure itself was not altered in any way to account for changing land surface conditions the pf mcmc arulampalam et al 2004 described in section 2 was adopted as the da algorithm due to the nonlinearity of the hydrological model equations the potential to encounter highly non gaussian probability densities and the relatively low dimensionality of the state space meaning the use of particle filters is feasible a particle ensemble size of 100 was adopted for each of the catchments as previous work on state estimation with models of similar dimensionality found this to be sufficient pathiraja et al 2017a recall from eqs 18 to 20 that the prior or forecast density at any given time is represented by first propagating the posterior particles from the previous time through the hydrologic model and then perturbing these with errors sampled from p γ ω x t 1 u t the conditional density p γ ω x t 1 u t was estimated using the state dependent model uncertainty estimation method pathiraja et al 2017a as summarized in section 2 2 the training phase used to develop this density was selected for each catchment so that it was outside of the calibration period but within the pre change phase refer table 2 it is important to ensure that the training period captures the full range of possible meteorological conditions likely to be encountered in the assimilation phase these periods were selected by striking a balance between this need and having enough data available for assimilation this is discussed in more detail at the end of this section error estimates for the hidden states such as the soil moisture state ω were developed as per steps t1 and t2 in section 2 2 see also fig 1 the levenberg marquardt algorithm was used as the optimization algorithm and a range of initial guesses were considered at each time due to the stable dynamics of the model equations the covariates for the conditional error density i e x t 1 ut were defined as the model latent states at time t 1 or a reduced form of these and various forms of observed antecedent rainfall antecedent rainfall covariates were selected based on their correlation strength with the output errors γ for the smaller western australian catchments rainfall on the current day was sufficient to capture the effect of meteorological conditions on state and output errors for the larger nammuc and comet catchments rainfall over a longer time window previous 2 and 5 days respectively was also considered in the covariate vector due to the longer response time of the catchments kernel conditional density estimation was then undertaken on the training data sample using the r package np hayfield racine 2008 the maximum likelihood cross validation approach of hall et al 2004 in the np package was used to estimate kernel bandwidths adaptive bandwidth factors abramson 1982 were used during the sampling process for regions of sparse training data to obtain smooth probability densities in terms of uncertainty characterization for the measurements input errors in rainfall and pet are included in the total model uncertainty estimate given by the sdmu method furthermore it is assumed that the statistics of meteorological observation errors are the same in pre and post change conditions this is a reasonable assumption so long as the same observing techniques are used throughout as is the case for the catchments considered in this study the assimilated runoff depths were assumed to contain zero mean heteroscedastic normally distributed instrument errors so that larger runoff depths have greater uncertainty see for instance salamon feyen 2009 the likelihood evaluated in the pf mcmc is therefore given by p y t o s t i n y t i μ y t o σ 2 a y t o i 1 n where a 0 1 a multiplier of 0 1 was chosen based on estimates adopted for similar gauges in hydrologic da studies e g clark et al 2008 weerts serafy 2006 xie et al 2014 lead times of 1 3 and 5 days were considered in generating streamflow forecasts for the pre change and post change conditions in order to isolate the effect of land use change on forecasting performance observed rainfall was used as the model input when generating all forecasts so as to exclude the effect of meteorological forecast errors a similar setup is used in other forecasting studies that aim to examine specific elements of da performance e g coustau et al 2013 likewise periods in the assimilation phase for both the pre change and post change conditions with storm magnitudes outside the range of those in the training period were excluded from the analysis this is to ensure that changes to forecast performance are not a result of storms not experienced in the training period this was required for only a minor portion of the assimilation period for the south western australian catchments see table 2 note that the unusually wet years from 1952 1958 were also excluded by peña arancibia et al 2012 in their analyses of changes in the comet catchment 4 3 forecast evaluation metrics probabilistic streamflow forecasts for the pre change and post change conditions were assessed using a range of forecast evaluation metrics the following forecast characteristics were quantified based on the work of hamill 2001 jolliffe stephenson 2012 murphy 1993 wilks 2005 wilks 2011 reliability a reliable forecast system gives an unbiased estimate of the observed frequencies associated with different forecast probability values wilks 2005 according to wilks 2011 reliability measures typically involve sorting the forecast observation pair into groups according to the value of the forecast variable the crps and logarithmic score discussed below are used here as measures of reliability resolution this refers to how distinct the forecast density is for different events for instance the climatological mean has zero resolution because it produces the same forecast at all points in time even when two different outcomes have different frequencies of occurrence wilks 2005 the continuous rank probability score crps provides a measure of both reliability and resolution hersbach 2000 and is frequently utilised in hydrologic forecasting e g li et al 2015 brown seo 2013 the crps measures how closely the cumulative distribution functions of the ensemble forecast and the corresponding observations match candille talagrand 2005 22 c r p s t f t f y f t o y 2 d y where f t f y empirical cumulative distribution function of the forecast of variable y at time t and f t o y cumulative distribution function of the observation of y at time t for cases where only a single observation of y is available at each time the associated cumulative distribution function is given by the heaviside step function 23 f t o y 0 y y t o 1 y y t o despite its use in the geosciences the crps can be a poor metric for complex forecast densities e g multiple modes high skewness as encountered in this study refer section 6 for example smith et al 2015 showed that the crps can give misleadingly good scores to outcomes that fall in between two modes of a bimodal forecast density we therefore use the logarithmic score ls good 1952 in addition to the crps to quantify reliability and resolution as it is better suited to non gaussian forecast densities the ls is a proper scoring rule that rewards forecasts where the outcome or observation falls in regions of high probability 24 l s t log f t y t o where ft indicates the predictive density function given for instance by a kernel density estimate of the weighted particles and y t o is the observation at time t accuracy this refers to the average correspondence between individual forecasts in an ensemble i e each particle and the observation wilks 2005 here the absolute deviation and the root mean square error rmse were used to measure accuracy the absolute deviation a k a absolute error is simply the absolute distance between the forecast and the observation for a probabilistic forecast a summary statistic is usually invoked to convert to a point forecast typically the ensemble mean e g li et al 2015 in this study we use the forecast mode as a measure of central tendency due to the presence of skewed and multi modal densities it is hereafter referred to as the absolute mode error 25 a m e t argmax y f t y y t o where ft y is the forecast density at time t and the argmax operator gives the value of the variable y that maximises the forecast density i e the mode the rmse provides second moment information about forecast accuracy when the probabilistic forecast is represented by a set of weighted particles the rmse is given by the square root of the average of the squared deviations between each particle and the observation 26 r m s e y t i 1 n φ t i y t i y t o 2 where y t i is the ith particle of the forecast at time t y t o is the observation at time t φ t i is the weight associated with the ith particle at time t and n is the number of particles lower values of all aforementioned forecast metrics indicate better performance the crps rmse and ame are all expressed in units of the forecast variable in this case mm for runoff depth whilst the log score is dimensionless note that for the purposes of calculating summary statistics and for ease of comparison each of the scores crps ls rmse and ame were normalized by their maximum observed values in order to obtain values in the 0 1 interval 5 results streamflow forecasting was undertaken for all study catchments using the pf mcmc and sdmu model uncertainty quantification method trained on pre change data with forecast performance evaluated via the full range of forecast evaluation metrics overall there was no detectable change in forecast performance between pre and post change conditions for all catchments except comet immediately after clearing 1971 1980 and wights these conclusions apply at all three lead times of 1 3 and 5 days this is based on an analysis of the difference in forecast score vs observed runoff depth relationship for pre and post change conditions figs 4 to 6 a nonparametric lowess smooth was applied to the data for undisturbed pre change and post change conditions see red and blue lines in figs 4 to 6 the particular lowess method adopted involves performing local linear regression using a weighted least squares approach where data points close to the point of interest determined by the span parameter receive higher weight cross validation was used to determine the optimal span parameter in the lowess fit that minimizes the sum of squared errors such a nonparametric method is suited to the case studies considered herein due to the difficulty in assigning a parametric regression model additionally the width of 95 confidence intervals on the lowess fit were approximated by 1 96 standard deviations where the standard deviation was estimated by empirical bootstrapping see red and blue shaded regions in figs 4 to 6 finally hypothesis testing was performed to determine whether there is a statistically significant difference in the regression curves for undisturbed pre and post change conditions we used the anova type test statistic introduced by young bowman 1995 for testing for the equality of 2 nonparametric regression curves the distribution of the test statistic was determined by the residual resampling bootstrap method a practically and statistically significant difference in the score vs runoff depth curves is apparent for both wights and comet immediately after clearing 1971 1980 as evidenced by the non overlapping confidence intervals and hypothesis test results figs 4 to 6 and tables 3 5 forecast quality is degraded considerably across all measures and lead times after land use change occurs particularly for high flow events forecast performances differences are much less pronounced for the other catchments for lemon hypothesis testing indicates the presence of statistically significant differences in most forecast scores across all lead times however the differences are of minimal practical significance see figs 4 6 and consist of a slight forecast improvement at mid range flows similar arguments can be made when considering the 2nd post change period 1981 1990 for comet and also the nammuc catchment note that for several forecast metrics for nammuc the statistically significant differences reflect a slight improvement in forecast performance for high flows in post change conditions we also examine the variation in forecast performance over time for post change conditions given that forecast performance varies with flow magnitude we focus our attention here on high flows which are generally more difficult to forecast well specifically we examine one day ahead forecast metrics for high flows in each year where high flows are defined as those exceeding the annual 98th percentile flow depth fig 7 shows the average difference between high flow forecast metrics for a given post change year and the lowess fit in pre change conditions for the corresponding flow value where positive deviations indicate a reduction in forecast skill in post change conditions i e increase in score value such an analysis ensures that forecast metrics of similar flow magnitudes are compared between undisturbed pre change and post change conditions the width of the 95 confidence intervals on the lowess fit for pre change conditions at the respective high flow values is also provided given by the red region in fig 7 fig 7 shows that for wights all aspects of forecast skill accuracy resolution and reliability are degraded for the years with the largest annual streamflow maxima with average ames corresponding to approximately 20 of the observed flows additionally there is greater variability in forecast score deviation across time compared to the other study catchments forecast degradation is most severe in the first 7 years after clearing i e up to 1983 see fig 7 although good forecast performance can be obtained in some high flow years e g in 1979 where the observed range of high flows is similar to 1977 which had severely degraded forecasts it appears that high flow forecast performance improves such that it is of similar quality to the undisturbed condition at approximately the same time as the establishment of equilibrium hydrologic conditions in around 1985 bari smettem 2004 overall however the tendency for degraded forecasts of high flows in post change conditions is apparent and more likely than in the other study catchments lemon is similar in size to wights but had only 50 of its land cleared with resulting changes to the hydrologic regime less pronounced and taking longer to develop than wights compare annual runoff coefficient over time in fig 3 despite observed changes to streamflow forecast quality is largely maintained in post change conditions across lead times see fig 4 6 one day ahead forecasts of high flows are of similar quality to forecasts of equivalent magnitude flows in pre change conditions this is demonstrated in fig 7 which shows that score deviations across time are almost always within the 95 pre change confidence intervals for all metrics with a slight improvement in some years of 0 5 mm for the crps rmse and ame unlike wights and lemon nammuc experienced patchy relatively more gradual changes from predominantly forested to cropland cover the most prominent changes to observed runoff have been to low flows with a 7 increase in the annual bfi from 1970 to 2000 pathiraja et al 2017b we therefore focus our attention on low flows defined as flows less than the annual 5th percentile to examine variations in forecast performance over time as shown in fig 8 contrary to expectations there is a minor but persistent improvement in low flow forecast performance across all metrics in the post change period see negative deviations in fig 8 this occurs because baseflows which are easier to predict at short time scales compared to low flow events are larger in magnitude in the post change period meaning that they are more often compared to low flow events in pre change conditions as these are similar in magnitude nonetheless improvements are minor 0 1 mm for the crps rmse and ame and ultimately support the conclusion that forecast performance has not degraded despite changes to land surface conditions similar to lemons roughly 50 of the comet catchment area experienced land cover change although such changes occurred over a much longer period of time and over a much larger area see table 1 and 2 changes to high flow forecast performance in the period 1971 1980 immediately after the period of rapid land cover change bear a resemblance to those of wights figs 4 6 looking at forecast performance over time fig 7 it is evident that 2 out of the 4 years with flow depths exceeding 4 mm have degraded forecasts across all metrics deviations are well outside the pre change confidence intervals conversely the period from 1981 to 1990 saw minimal changes to forecast performance across time compared to pre change conditions as demonstrated in fig 7 this is despite continual but minor clearing that occurred between 1980 and 2009 as well as enhanced observed storm flows of all magnitudes in this period peña arancibia et al 2012 these results can be partly attributed to the reduction in observed annual streamflow maxima compared to the wetter period 1971 1980 due to la nina conditions in the 1970s see fig 7 but also to the magnitude of prediction uncertainty relative to observed changes as discussed in section 6 6 discussion previous studies have highlighted the importance of accounting for and removing systematic model biases in hydrologic data assimilation because of its potential to significantly degrade assimilation quality e g alvarez garreton et al 2014 delannoy et al 2007 balmaseda et al 2007 dee dasilva 1998 ryu et al 2009 the experiments in this study demonstrate that not all systematic changes to catchment conditions with a detectable impact on observations require a deliberate re adjustment of the assimilation algorithm or related components for instance through a re calibrated model time varying parameters or revised offline bias estimation module in order to maintain forecast assimilation quality recall that in these experiments no modifications to the assimilation algorithm or models have been made to account for changed conditions furthermore recall that a forecast is generated by adding a random model error term to the model simulation and that such model error statistics have been informed by pre change data only these conclusions apply so long as the same da algorithm is used in pre and post change conditions and only if an effective da technique is used we now demonstrate that systematic model errors due to changes in catchment characteristics do not degrade forecasting performance when such errors are within the range of inherent prediction uncertainty specifically the time scale and magnitude of the impacts on the hydrologic regime relative to the inherent model prediction uncertainty determines whether assimilation and forecast performance is negatively impacted this is conceptualized in fig 9 which shows the relationship between the impact of land cover change on the hydrologic regime and forecast degradation for state estimation da it is somewhat difficult however to draw conclusions about types of land cover change and the effect on forecast performance as demonstrated by the 4 test cases this is likely due to the complex interaction between the spatial pattern of land use change the period over which change occurred catchment size resilience and climatic conditions instead we discuss the ability of state estimation da to maintain forecast quality for different types of impacts on system observations type a b and c impacts as defined below and in fig 9 representative examples of changes to forecast performance in each basin are shown in fig 10 6 1 type a impacts these experiments demonstrate that bias correction or model structural adjustments are required to avoid forecast degradation when using da for catchments with type a impacts type a impacts are defined as those where land cover changes have impacts on the hydrologic regime at time scales less than or equal to the assimilation model time step and where those impacts are larger in magnitude than the prediction uncertainty here prediction uncertainty refers collectively to initial condition model input parameter and structural uncertainties encountered at a given time which is characterized by the prior distribution factors that may cause type a impacts other than the nature of land cover change include model quality and accuracy of uncertainty characterization type a impacts are more likely to occur if a model is assigned low predictive uncertainty either because it produces simulations that are highly consistent with observations in pre change conditions or because certain aspects of prediction uncertainty are ignored commonly structural uncertainties examples of type a impacts are changes to the timing and magnitude of the peak flow response at the assimilation model time scale as seen in wights and comet 1st post change period poor forecast performance of peak daily flows in the post change period for wights can be attributed to 1 a reduction in catchment response time by roughly one day for extreme rainfall events i e peak flow occurs one day earlier than in pre change conditions recalling that assimilation is carried out at a daily time step and 2 a significant increase in daily peak flow depth well outside the range of streamflow prediction uncertainty this can be seen in fig 10c where the forecast for wights on 21 july 1981 severely underestimates the peak as it arrives a day earlier than expected c f the more accurate forecast for salmon the next day fig 10b when the peak is expected which is also half the magnitude of the peak at wights similarly changes to the timing of peak daily flows in comet after land cover change lead to poor forecast quality in the period 1971 1980 1st post change period a representative example is shown in fig 11 where forecasts of the rising limb and peak for the february 1978 event are severely underestimated this is due to the reduction in lag between peak runoff and rainfall to 3 days see fig 10i where response time was generally 4 5 days in pre change conditions see fig 10h which shows a lag of 5 days for a similar storm event in 1928 standard data assimilation alone is thus insufficient to correct for such systematic errors requiring additional bias correction or model structural adjustment techniques see for example pathiraja et al 2016b where a time varying parameter da framework improved peak flow predictions in the wights catchment another approach is to reduce the model and assimilation time step but this is not always feasible for computationally expensive models and when observations are available at a coarser temporal resolution 6 2 type b impacts not all impacts at a fine temporal scale lead to forecast degradation when using standard state estimation da catchments experiencing type b impacts same as type a except changes to the hydrologic regime are smaller in magnitude than the prediction uncertainty have no detectable change in forecast quality this is because the assimilation algorithm is able to gradually adjust the model latent states to reflect changed conditions loosely speaking some forms of systematic model errors can be corrected by the da algorithm in stable systems so long as they are within the estimated range of prediction uncertainty for both the latent states and outputs type b impacts are evident in lemons where daily streamflow depths in the may to july period are increased after clearing due to the generation of surface runoff earlier in the year compare the may to august period between ernies and lemon in fig 12 increases to daily flows are fairly gradual and within the range of prediction uncertainty thereby allowing the particle filter to select internal states that reflect the gradual increase in catchment wetness this allows for high quality model simulations even prior to the addition of estimated model errors see for instance the forecast on 2 august 1991 for lemon in fig 10e similar conclusions can be drawn for early season flows for wights see for instance fig 13 in such cases standard state estimation da will maintain forecast quality of pre change conditions without needing to explicitly account for systematic catchment changes although this is conditional on the accurate characterization of total uncertainty in model latent states in pre change conditions which is achieved using the sdmu approach 6 3 type c impacts similarly forecast quality can be maintained by using standard state estimation da in catchments with type c impacts same as type b except changes to the hydrologic regime predominantly occur at time scales longer than the model time step assimilation window the reason for this is much the same as for type b impacts in that the assimilation algorithm is able to gradually adjust latent states and outputs in response to systematic changes in the observations examples of type c impacts include changes to low flow volumes at the annual scale when undertaking daily streamflow assimilation as seen in nammuc in this case state estimation da promotes a gradual increase in shallow layer storage levels stw1 of the hbv model in direct correlation with increasing observed baseflow see fig 10a where both annual storage state and baseflow are represented as averages over the period january to march this allows for low flow forecast quality to be maintained despite systematic changes to baseflow dynamics see fig 14 note that interflow from the shallow layer store in hbv represents low flows in this catchment since the calibrated perc parameter was zero 7 conclusions data assimilation has been shown to dramatically improve forecast skill in hydrologic and meteorological applications although such improvements are conditional on using bias free observations and model simulations this work sheds new light on the impacts of systematic model errors in hydrologic data assimilation particularly in regards to streamflow forecasting in catchments with changing land surface conditions as the hydrologic model is more likely to produce biased simulations outside its calibration period we find that assimilation and forecast quality is not always negatively impacted when no efforts are made to explicitly account for model biases resulting from land cover changes rather the magnitude of such systematic errors relative to the inherent uncertainty in the model simulations even in pre change conditions is the key determinant of forecast degradation this is demonstrated through an assimilation study of a range of catchments with changing land cover conditions a particle filter was used to assimilated daily catchment outlet streamflow in four catchments experiencing varying levels of deforestation 1 lemon 3 44 km2 2 wights 0 94 km2 3 nammuc 2880 km2 and 4 comet 16 440 km2 each with a measurable impact on the hydrologic regime a total model uncertainty estimation method pathiraja et al 2017a was utilized to quantify the combined effects of parameter structural and input uncertainties in generating forecasts in pre change conditions these model uncertainty estimates were used to generate the prior density for streamflow assimilation in both pre and post change conditions contrary to expectations streamflow forecast performance at 1 3 and 5 day lead times in post change conditions degraded only in 2 cases wights and comet 1st post change period we find that forecast degradation occurs only when land cover changes lead to type a impacts on the hydrologic regime which is defined herein as having the following 2 properties 1 changes to the observed streamflow regime occur at time scales equal to or less than the assimilation window model time step e g peak daily runoff occurring 1 to 2 days earlier than in pre change conditions with assimilation undertaken at a daily time step and 2 when the magnitude of these changes exceeds the typical range of prediction uncertainty in pre change conditions e g significant increase in peak daily flow depth for similar meteorological conditions when the magnitude of such changes is within the range of prediction uncertainty in pre change conditions i e type b and type c impacts lemon nammuc and comet 2nd post change period the da algorithm is able to gradually adjust the internal model states without requiring any explicit modification to the algorithm or model uncertainty estimates this work shows that changes to catchment conditions do not always require adjustments to the modeling or assimilation methodology for instance through re calibration of the hydrologic model time varying model parameters or revised offline online bias estimation such strategies would be necessary for cases where the time scales of systematic errors are less than the assimilation model time step and when their magnitude is greater than the inherent prediction uncertainty during pre change conditions although we have only investigated a handful of catchments we have considered a range of catchment sizes climatic conditions land use change patterns and models future work will attempt to consider a larger pool of catchments in order to examine the interplay between various factors including pattern type and rate of land use change as well as hydrologic model quality and climatic conditions it is hoped that this will equip modellers and data assimilation scientists with a set of predictors to determine which conditions trigger the need for systematic error correction when undertaking da in changing catchments acknowledgments this study was funded by the australian research council as part of the discovery project dp140102394 this research has been partially funded by deutsche forschungsgemeinschaft dfg through the grant sfb 1294 data assimilation dr marshall is additionally supported through a future fellowship ft120100269 we gratefully acknowledge the vietnamese institute of water resources planning andrea castelletti west australian department of water and queensland department of natural resources and mines for provision of data used in this study data for the nammuc catchment was collected under the project imrr integrated and sustainable water management of red thai binh rivers system in changing climate funded by the italian ministry of foreign affairs delibera n 142 del 8 novembre 2010 we also gratefully acknowledge dr andrea castelletti for provision of data used in this study data utilized in this study can be made available from the authors upon request 
866,the global prevalence of rapid and extensive land use change necessitates hydrologic modelling methodologies capable of handling non stationarity this is particularly true in the context of hydrologic forecasting using data assimilation data assimilation has been shown to dramatically improve forecast skill in hydrologic and meteorological applications although such improvements are conditional on using bias free observations and model simulations a hydrologic model calibrated to a particular set of land cover conditions has the potential to produce biased simulations when the catchment is disturbed this paper sheds new light on the impacts of bias or systematic errors in hydrologic data assimilation in the context of forecasting in catchments with changing land surface conditions and a model calibrated to pre change conditions we posit that in such cases the impact of systematic model errors on assimilation or forecast quality is dependent on the inherent prediction uncertainty that persists even in pre change conditions through experiments on a range of catchments we develop a conceptual relationship between total prediction uncertainty and the impacts of land cover changes on the hydrologic regime to demonstrate how forecast quality is affected when using state estimation data assimilation with no modifications to account for land cover changes this work shows that systematic model errors as a result of changing or changed catchment conditions do not always necessitate adjustments to the modelling or assimilation methodology for instance through re calibration of the hydrologic model time varying model parameters or revised offline online bias estimation 1 introduction as our population and ecological footprint expands so too does our impact on terrestrial hydrology rapid agricultural and economic development has already seen the loss of large swaths of forest across the globe the fao estimates global forest cover has been reduced by an area roughly the size of south africa since 1990 fao 2015 projected increases to population in close proximity to tropical forests unesco 2009 mean that land use change and its impacts on hydrology and water resources will continue to be an important issue such extensive current and potential future land use change necessitates the development of appropriate hydrologic modelling paradigms for water and land use management the traditional calibrate validate modelling method is in theory poorly suited to catchments with changing land surface conditions this has implications for short and seasonal forecasting methods that typically rely on a hydrologic model calibrated to a particular set of catchment conditions that may no longer be representative of the forecast interval in such cases there is the potential for generating biased forecasts even when state estimation data assimilation methods are utilised land use change and its implications for catchment hydrology have been studied extensively in recent years researchers have examined how hydrologic processes are altered due to land surface disturbance e g yang et al 2012 painter et al 2007 and whether changes can be detected in hydrologic observations see for example peña arancibia et al 2012 bloschl et al 2007 for instance deforestation has often been linked to increases in peak runoff and annual runoff yields in small experimental catchments e g ruprecht schofield 1991 dias et al 2015 although such an association is less clear in larger catchments where climatic influences and complex process interactions may dominate wilk et al 2001 zhou et al 2010 modeling studies have also been carried out to determine how catchment hydrology can be affected by likely future land use change scenarios see for example huisman et al 2009 choi deal 2008 isik et al 2013 however few have investigated how predictive modelling methodologies must be adapted for catchment systems that are no longer stationary some recent approaches include the work of westra et al 2014 who proposed allowing model parameters to vary as a function of selected covariates efstratiadis et al 2015 who proposed a modelling approach that explicitly incorporates known land use changes and pathiraja et al 2016a who presented a data assimilation based approach to estimate time variations in model parameters in response to signals of change in hydrologic observations however this issue has received little attention in the context of hydrologic forecasting and data assimilation data assimilation da has gained increasing attention in recent decades as a means of improving hydrologic forecasts through improved initial conditions dechant moradkhani 2011 lee et al 2011 and model parameters dechant moradkhani 2012 vrugt et al 2006 xie et al 2014 plaza et al 2012 whilst simultaneously providing uncertainty estimates many flood forecasting da studies have demonstrated improvements to forecasting skill using various da algorithms and by assimilating a range of observations including in situ and satellite soil moisture e g aubert et al 2003 alvarez garreton et al 2014 crow ryu 2009 yan moradkhani 2016 discharge e g noh et al 2013 li et al 2015 chen et al 2013 pauwels de lannoy 2009 clark et al 2008 and river levels e g ricci et al 2011 neal et al 2007 matgen et al 2010 there have been far fewer studies examining how data assimilation based streamflow forecasting performs in catchments experiencing land use change in such cases model predictions may contain systematic errors if the hydrologic model has been calibrated to land cover conditions prior to the disturbance previous studies have noted the importance of removing or accounting for systematic biases in observations and model simulations when undertaking data assimilation see for example alvarez garreton et al 2014 delannoy et al 2007 balmaseda et al 2007 dee dasilva 1998 ryu et al 2009 pauwels et al 2013 found that the benefits from standard state estimation da may be marginal when the calibrated hydrologic model gives predictions with large biases common methods to explicitly deal with forecast bias involve estimating static e g montzka et al 2011 smith et al 2013 and time varying pathiraja et al 2016a 2016b parameters online as well as through online bias estimation algorithms or bias aware filters see for instance dee 2005 pauwels et al 2013 rasmussen et al 2016 systematic errors are potentially a more pressing issue when undertaking standard state estimation da in catchments with time varying system properties however we posit that in such cases the impact of systematic errors on assimilation or forecast quality is dependent on the inherent prediction uncertainty that persists even in pre change conditions we demonstrate this by examining how forecasting performance is affected when models and da algorithms are established for a particular catchment condition and a land cover change occurs with no attempts made to account for potential biases introduced by the changes we characterize the total uncertainty of model simulations going beyond parameter and input uncertainty to also account for structural uncertainties which are particularly important for high flow events shoaib et al 2016 all statistical moments of model uncertainty are considered instead of only the mean and or covariance as is typically the case in hydrologic da studies the adopted model uncertainty quantification method assumes stationarity and no online or offline bias correction strategies are used to adapt to changed conditions observed meteorological inputs are utilized rather than meteorological forecasts in order to focus solely on the effects of changing land surface conditions on streamflow forecasting performance streamflow forecasting at various lead times is undertaken for 4 catchments paired catchment systems of varying size and levels of deforestation gauged streamflow observations are assimilated using a particle filter and the state dependent model uncertainty method of pathiraja et al 2017a to characterize total model uncertainty the remainder of this paper is structured as follows background on the methods utilised in this study is presented in section 2 in section 3 we provide details of the study catchments including land use change and the impacts of land use change on the observed hydrologic regime the experimental setup including the hydrologic models utilised and application of data assimilation and model uncertainty estimation methods are discussed in section 4 the experimental outcomes are presented section 5 followed by an analysis of results and discussion of their implications for da science in section 6 finally we conclude with a summary of the main outcomes and further work 2 background data assimilation methods aim to optimally combine observations and prior information usually from a numerical model based on their respective uncertainties effective da requires the use of appropriate assimilation methods for the task at hand as well as an accurate characterization of all uncertainties present liu et al 2012 we briefly discuss each of these components below 2 1 data assimilation using the particle filter suppose the true system of interest can be expressed by the following stochastic discrete time state space equations 1 x t f x t 1 u t ω t 2 y t h x t ɛ t where x t is the true latent state i e unobserved or hidden vector at time t u t is the model input vector at time t e g rainfall pet f is the assumed forward model h is the assumed observation operator that maps latent states to observation space output space and y t is the true system output at time t ω t and ε t are random noise terms that describe the one step transition error where ω t captures deficiencies in f and possibly u t and ε t captures deficiencies in h this means that initial condition uncertainties i e in x t 1 are not considered in ω t and ε t suppose also that available observations of the system outputs y t are corrupted by random noise 3 y t o y t η t where y t o is the observation vector at time t and η t is a random noise term that captures instrument and other errors sequential da methods involve finding solutions to bayes formula in a recursive manner the prior distribution is usually characterized by the model simulations and the likelihood function takes into consideration uncertainty in the observations the posterior density for any given time t is given by the following equation 4 p x 0 t y 0 t o p x t x t 1 p y t o x t p y t o y 0 t 1 o p x 0 t 1 y 0 t 1 o where y 0 t o y 0 o y 1 o y t o p x 0 t y 0 t o is the joint filtering posterior distribution for the trajectory from time 0 to time t p x t x t 1 characterises the uncertainty in transitioning from the state at time t 1 to the state at time t also known as the prior density p y t o x t is the likelihood of observing y t o given our prior knowledge of the states at time t and p y t o y 0 t 1 o is a normalizing constant despite the conceptual simplicity of eq 4 computing the posterior can be difficult for complex real world problems where analytical solutions are intractable several algorithms have been developed since the 1960s for such purposes including kalman and particle filters smoothers the standard kalman filter gives the optimal filtering posterior in the minimum variance sense for linear stochastic dynamic systems with gaussian errors kalman 1960 a number of kalman filter variants most notably the ensemble kalman filter evensen 1994 have been developed to improve their applicability to real world non linear and non gaussian systems unlike kalman filter based methods sequential monte carlo smc a subset of which are particle filters aim to propagate the full distribution through time without relying on assumptions of linearity or gaussianity doucet et al 2000 however a number of implementation issues such as the need for large particle sizes have limited their use in high dimensional real world applications we utilise a particle filter algorithm in this study to capture the potentially non gaussian features of the estimated prior density in deriving the posterior in particle filtering prior and posterior probability densities at any given time t are approximated by the weighted sum of a set of n particles with associated weights φ t i i 1 n arulampalam et al 2002 jeremiah et al 2012 here we use the particle filter markov chain monte carlo pf mcmc arulampalam et al 2004 which has been developed to avoid known issues associated with the standard particle filter such as degeneracy and sample impoverishment see e g chen 2003 a single assimilation cycle of the pf mcmc proceeds as follows for further details refer to arulampalam et al 2004 step 1 estimate the prior probability density function p x t x t 1 this is typically estimated by propagating each of the posterior particles x t 1 i through the forward model f and observation operator h note the superscript indicates posterior and superscript indicates prior 5 x t i f x t 1 i u t f o r i 1 n 6 y t i h x t i f o r i 1 n where x t i and y t i are model simulations however this approach does not take into account uncertainties in the forward model and or observation operator section 2 2 of this article discusses how such uncertainties can be incorporated to generate a more accurate representation of the prior step 2 estimate likelihood and update weights particle weights from time t 1 φ t 1 i i 1 n are updated based on the likelihood of the observations given the model simulations p y t o x t i using importance sampling and assuming the importance function is the prior density 7 φ t i φ t 1 i p y t o x t i f o r i 1 n 8 x t i x t i f o r i 1 n step 3 particle resampling this step is undertaken to combat the issue of degeneracy whereby all but a few particles are given negligible weight particles with high probability i e high weight are replicated whilst particles with very low probability are discarded resampling is undertaken only when the effective sample size a measure of degeneracy liu chen 1998 drops below a predefined threshold the sample importance resampling algorithm of smith gelfand 1992 is adopted in this study step 4 mcmc step finally an mcmc step is carried out to avoid the potential loss of sample diversity associated with resampling due to high weighted particles being over represented also known as sample impoverishment a perturbation is first applied to the resampled states from step 4 typically from a gaussian density and the perturbed state x t i p is accepted if a randomly generated number is less than the metropolis hastings acceptance probability α that is 9 x t i x t i p i f u α x t i r o t h e r w i s e 10 u u 0 1 11 α m i n 1 p y t o x t i p p x t i p x t 1 i r p y t o x t i r p x t i r x t 1 i r the final posterior density for time t is represented by the set of points x t i φ t i r i 1 n note the weights are unchanged in this step 2 2 state dependent model uncertainty estimation sdmu uncertainty characterization in da can be a challenging task particularly for model simulations which generally have many interacting sources of uncertainty that are individually difficult to quantify many standard methods such as perturbing model forcings parameters and or states from an assumed probability distribution are limited in that they only partially consider model structural uncertainty or rely on potentially incorrect assumptions about the error statistics which are commonly assumed gaussian in this study we adopt a method developed in previous work pathiraja et al 2017a that aims to objectively characterize model uncertainty without any such assumptions it considers the total uncertainty of model simulations instead of just parametric and or forcing uncertainty and higher order statistical moments of the error distribution are estimated by way of non parametric density estimation instead of focusing solely on the mean and or variance the approach works by using a training period to develop estimates of the additive errors in the model equations f and observation operator h from which an error density is estimated using nonparametric density estimation it is suited to cases where the observed variables are of primary interest and where the magnitude of measurement errors is negligible in comparison to model errors the approach has been shown to provide more realistic uncertainty estimates in a streamflow forecasting application resulting in improved forecasts compared to standard approaches the method is summarized below and in fig 1 for further details see pathiraja et al 2017a offline training phase step t1 a training period of length t is selected ensuring that meteorological conditions and system states likely to be experienced in the assimilation period are captured this is then used to gather data on the additive errors in the latent states and outputs estimation of these errors requires knowledge of the true latent state at the previous time x t 1 refer eq 1 since they do not include initial condition errors since this is unavailable an optimize and model strategy is utilized to obtain a best estimate of the initial condition given by x t 1 the optimization aims to generate an initial condition that provides the best fit between observations and model simulations at time t 1 using available estimates of x t 2 denoted x t 2 given by equation 13 with the appropriate change in time index as the initial guess for the optimization algorithm in this study we adopt the levenberg marquardt algorithm this is then forced through the model equations to generate x t 1 specifically the following equations are solved at all points in time in the training period 12 x t 2 o p t argmin x t 2 h f x t 2 u t 1 y t 1 o 13 x t 1 f x t 2 o p t u t 1 the advantage of this optimize and model strategy instead of optimizing x t 1 directly using h x t 1 is that physical constraints imposed by the state space model equations are satisfied because both the observation operator and the forward model are considered step t2 the aim is to then estimate the additive errors in the latent states ω t and outputs γ t where γ t denotes the total simulation error combining errors in f and h i e sum of ε t and function of ω t 14 γ t y t h f x t 1 u t h f x t 1 u t ω t ɛ t h f x t 1 u t with an estimate for the initial conditions the additive errors can be estimated by re arranging eqs 1 3 and using available observations with the assumption that measurement error is negligble compared to the model errors 15 x t f x t 1 u t 16 γ t y t o h x t 17 ω t x t x t where x t is estimated as per step 1 but with the time increment increased by 1 step t3 kernel conditional density estimation is then applied to the sample of points γ t ω t x t 1 t 3 t to derive an estimate for conditional density of the model errors given the latent states p γ ω x t 1 depending on the application model inputs such as rainfall may also be included as covariates as is the case in this study see section 4 2 we adopt the method of hayfield and racine 2008 as implemented in the np package in r to estimate this density assimilation phase step a1 for any given time t during the assimilation phase the posterior particles from the previous time t 1 x t 1 i i 1 n are forced through the model equations to generate model simulations of the latent states and outputs given by eqs 5 and 6 step a2 the posterior particles x t 1 i i 1 n and potentially any observed inputs u t are used to sample from the kernel density estimate and added on to the simulated latent states and outputs from the previous step 18 y t i y t i g t i 19 x t i x t i w t i 20 g t i w t i p γ ω x t 1 the particles x t i y t i and their associated weights form the prior density p x t x t 1 or probabilistic forecast if used in a forecasting setting note that the most important assumption in the method described above is that the statistical properties of the additive errors conditioned on the model latent states are stationary through time or at least do not change between the training and assimilation period this study will examine the extent to which streamflow forecast performance is degraded when this assumption is violated due to changing land surface conditions 3 study catchments we examine four catchments paired catchment systems of varying size properties levels of deforestation and changes to the hydrologic regimes details on the study catchments are provided below see also table 1 for a summary of catchment characteristics the location of the catchments and gauging stations are provided in fig 2 3 1 wights salmon lemon ernies south western australia located in south west western australia these two paired catchment systems are part of an experimental forest established to understand the effects of deforestation on catchment hydrology bari smettem 2004 at the beginning of the monitoring period i e 1974 all four catchments were densely forested with vegetation dominated by jarrah eucalyptus marginata and marri eucalyptus calophylla ruprecht schofield 1989 bettenay et al 1980 ruprecht schofield 1989 and bari smettem 2004 describe the treatment works undertaken which are summarized herein in the summer of 1976 1977 the wights catchment was fully cleared and 50 of the downstream portion of the lemon catchment was cleared with merchantable timber logged and the remaining timber burnt to the ground both catchments were sown with clover and grass over the may june period of 1977 from 1978 onwards both catchments were converted to agricultural use which included pinwheel raking sheep and horse grazing ploughing and bulldozer leveling salmon adjacent to wights and ernies adjacent to lemon were kept as control catchments that is no treatment was undertaken over the entire monitoring period the control catchments have been used to represent pre change conditions due to the short length of data for the treated catchments prior to land use change only 2 5 years all four catchments are relatively small in size ranging from 82 to 350 ha the local climate is characterized by dry hot summers mean monthly rainfall of approx 10 15 mm and mean daily maximum temperature approx 30 c and wet cold winters mean monthly rainfall of approx 170 mm and mean daily maximum temperature approx 15 c mccullough lund 2010 average annual rainfall at wights salmon is slightly higher than at lemon ernies which is approximately 40 km away 930 mm compared with 670 mm mean annual class a pan evaporation is around 1630 mm year based on measurements at collie about 20 km northeast of the wights catchment luke burke 1987 further details on catchment characteristics including vegetation topography and soil profile can be found in bettenay et al 1980 daily point rainfall and streamflow volume data for all catchments were obtained from the west australian department of water potential evapotranspiration pet for both catchments was estimated from evaporation maps of average monthly pet provided in luke burke 1987 which was then disaggregated uniformly to a daily time step the same daily pet time series was adopted for each catchment within the pair due to the spatial resolution of the evaporation maps changes to the hydrologic regime are evident for both wights and lemon although they are more pronounced for wights compare fig 3 a and b an immediate increase in the annual runoff coefficient a n n u a l r u n o f f v o l u m e a n n u a l r a i n f a l l v o l u m e is evident following clearing at wights the runoff coefficient continues to increase for roughly 6 years until equilibrium is established with an average annual runoff coefficient of 0 5 compared to 0 3 during the pre clear period see fig 3b similarly an immediate increase in peak flows is apparent annual maximum daily flows are near identical for wights and salmon maximum difference of 2 ml day during the pre clear period in the first year after clearing 1977 there is a sharp increase in the annual maximum daily flow such that it is 12 ml greater than the annual maximum daily flow in salmon lemon experienced a similar increase in the annual runoff coefficient as wights but over a relatively longer period compare fig 3a and 3b although in the post clear period streamflow is initiated roughly 2 to 3 months earlier than in ernies changes at the daily scale are less in magnitude than wights in terms of daily metrics it took 8 years for annual maximum daily flows in lemon to increase above ernies by 12 ml ie the observed amount of increase in wights in the first year after clearing the less prominent changes in lemon compared to wights are likely due to the less extensive disturbance to the land surface conditions 3 2 nammuc vietnam nammuc is a small medium sized catchment 2880 km2 located in the red river basin in northern vietnam the catchment is of interest as vietnam experienced extensive deforestation starting mainly from the 1970s although the government has recently established reforestation policies wwf 2013 land cover changes in the nammuc catchment appear to be patchy with evergreen leaf decreasing from about 60 30 in favor of cropland which increased from about 23 52 in contrast to the other catchments considered in this study it is not easy to identify distinct time periods of changing vs no change conditions based on the available land cover map information and the changes to observed runoff we conclude that the most rapid and extensive case of deforestation occurred in the early 1990s further catchment details including land use change are provided in pathiraja et al 2017b and are summarized here the local climate is monsoon dominated with two distinct seasons the wet season ranges from may to october and is characterized by high temperatures about 28 c whilst the dry season ranges from november to april with cooler temperatures about 18 c average annual rainfall across the catchment varies between 1300 and 2000 mm the annual streamflow pattern is unimodal with high flows during the wet season usually peaking in july vu 1993 available data consists of point daily rainfall and temperature recorded at 4 stations within and around the catchment area and daily catchment outlet streamflow see fig 2 records spanning the period 1975 2004 were made available by the vietnamese institute of water resources planning catchment averaged rainfall and temperature was calculated using thiessen polygons with potential evapotranspiration estimated using the empirical temperature latitude based hamon pet method hamon 1961 fig 3c shows that changes in the hydrologic regime are noticeable from the mid 1990s onwards the annual runoff coefficient varies between 0 4 and 0 6 during the nominal pre change period and between 0 6 and 0 8 afterwards according to pathiraja et al 2017b and wang et al 2012 there are increases to wet and dry season streamflow although increases in annual runoff yields are mostly due to an increase in the baseflow volume 3 3 comet north eastern australia the comet catchment is located in the fitzroy basin in central queensland australia prior to settlement it was forested with vegetation dominated by mixed eucalyptus species 52 as well as brigalow acacia harpophylla 23 and softwood scrubs 8 siriwardena et al 2006 details of the treatment works and catchment characteristics including climatic conditions are provided in siriwardena et al 2006 and are summarized below they report that clearing was undertaken in the mid 1960s and largely completed by 1970 eventually leading to agricultural activity including cropping grazing and construction of gully dams as a result clearing of regrowth has continued till present with peña arancibia et al 2012 reporting that an additional 10 of native vegetation or plantation forest was cleared between 1980 and 2009 forest cover has reduced from 91 of the total catchment area to approximately 40 see table 1 of siriwardena et al 2006 clearing has predominantly occurred close to the main river channel as well as near the downstream tributaries refer fig 2 and also peña arancibia et al 2012 comet is the largest of the study catchments at 16 400 km2 the local climate is dry sub humid with a summer rainfall dominance i e hot wet summers average daily maximum temperatures range from 34 c in january to 22 c in july streamflow is ephemeral generally occurring during the wetter months december april and peaking in february mean annual rainfall is approximately 660 mm and a small rainfall gradient across the catchment from east to west is apparent potential evapotranspiration is estimated to be slightly less than at the western australian catchments with average annual values of 1260 mm based on estimates using the hamon method hamon 1961 gridded daily rainfall was used due to the size of the catchment and the sparsity of gauges in close proximity with records spanning pre and post change conditions this was sourced from the australian water availability project awap raupach et al 2008 which consists of interpolated gridded daily rainfall at approximately 5 km grid size covering the entire australian continent rainfall in each grid cell overlapping the catchment was used to generate lumped catchment averaged daily rainfall for this study daily streamflow data for the catchment comes from the comet weir maintained by the queensland department of natural resources and mines obtained from the water information monitoring portal https water monitoring information qld gov au the gauge was converted to a continuous logger and moved 6 4 km upstream in 1971 although this has not produced a discontinuity in the record siriwardena et al 2006 and given the length of the tributary it is not likely to make a material difference on the overall flow records streamflow data was reported to be of good quality with only 3 of the record being a derived discharge the data was also checked for consistency with estimated rainfall daily pet was estimated using the empirical temperature latitude based hamon pet method hamon 1961 and average daily temperatures from weather stations maintained by the australian bureau of meteorology http www bom gov au climate data index shtml in terms of annual metrics changes to the hydrologic regime following clearing are not as pronounced in comet as the other catchments refer fig 3d this is partly due to the ephemeral nature of the catchment annual runoff coefficient 0 15 and the complexity and interaction of rainfall runoff processes over such a large area siriwardena et al 2006 nonetheless previous studies have provided some insights into the effects of land use change on the comet catchment at various time scales peña arancibia et al 2012 identified two distinct periods of post change response as indicated in fig 3d and table 2 and concluded from an analysis of daily streamflow metrics that clearing of native vegetation enhanced stormflow of all magnitudes immediately after clearing although slower flows were decreased however they attributed changes to annual streamflow yields mainly to climatic variability based on results from a tomer schilling analysis tomer schilling 2009 an analysis of the daily streamflow and rainfall records show that response time was also affected by clearing but that this returned to normal in the 2nd post clear period prior to clearing the lag time between peak rainfall and peak runoff was generally 3 5 days this decreased to 2 days in some cases immediately after clearing and then returned to 3 5 days in the 2nd post clear period 4 experimental setup 4 1 hydrological models conceptual rainfall runoff models were adopted for each of the study catchments given that only streamflow observations at the outlet are available previous work with the western australian catchments showed that the probability distributed model pdm moore 2007 modified to include a deep groundwater leak produced good streamflow simulations pathiraja et al 2016b the 9 parameter hydrologiska byrans vattenbalansavdelning hbv model bergström et al 1995 without snow component was utilized for the nammuc catchment based on previous work by pathiraja et al 2017b finally gr4j perrin et al 2003 was used for the comet catchment based on the work of pagano et al 2010 who showed its efficacy in modeling a range of australian catchments all three models have a low dimensional state space 3 4 and 2 states for modified pdm hbv and gr4j respectively the models were calibrated to daily streamflow data using the shuffled complex evolution algorithm sce ua duan et al 1993 for pdm and gr4j and the borg evolutionary algorithm hadka and reed 2013 for hbv using a portion of the pre change period as defined in table 2 and the nash sutcliffe efficiency as the objective function 4 2 streamflow forecasting using data assimilation streamflow forecasts at 1 3 and 5 day lead times were generated by assimilating daily streamflow observations with model predictions model predictions were produced by combining additive uncertainty estimates from the sdmu method described in section 2 2 with hydrologic model simulations generated using observed meteorological inputs i e perfect meteorological forecast uncertainty estimates were developed for pre change conditions and model parameters were not modified during the assimilation phase meaning that the model structure itself was not altered in any way to account for changing land surface conditions the pf mcmc arulampalam et al 2004 described in section 2 was adopted as the da algorithm due to the nonlinearity of the hydrological model equations the potential to encounter highly non gaussian probability densities and the relatively low dimensionality of the state space meaning the use of particle filters is feasible a particle ensemble size of 100 was adopted for each of the catchments as previous work on state estimation with models of similar dimensionality found this to be sufficient pathiraja et al 2017a recall from eqs 18 to 20 that the prior or forecast density at any given time is represented by first propagating the posterior particles from the previous time through the hydrologic model and then perturbing these with errors sampled from p γ ω x t 1 u t the conditional density p γ ω x t 1 u t was estimated using the state dependent model uncertainty estimation method pathiraja et al 2017a as summarized in section 2 2 the training phase used to develop this density was selected for each catchment so that it was outside of the calibration period but within the pre change phase refer table 2 it is important to ensure that the training period captures the full range of possible meteorological conditions likely to be encountered in the assimilation phase these periods were selected by striking a balance between this need and having enough data available for assimilation this is discussed in more detail at the end of this section error estimates for the hidden states such as the soil moisture state ω were developed as per steps t1 and t2 in section 2 2 see also fig 1 the levenberg marquardt algorithm was used as the optimization algorithm and a range of initial guesses were considered at each time due to the stable dynamics of the model equations the covariates for the conditional error density i e x t 1 ut were defined as the model latent states at time t 1 or a reduced form of these and various forms of observed antecedent rainfall antecedent rainfall covariates were selected based on their correlation strength with the output errors γ for the smaller western australian catchments rainfall on the current day was sufficient to capture the effect of meteorological conditions on state and output errors for the larger nammuc and comet catchments rainfall over a longer time window previous 2 and 5 days respectively was also considered in the covariate vector due to the longer response time of the catchments kernel conditional density estimation was then undertaken on the training data sample using the r package np hayfield racine 2008 the maximum likelihood cross validation approach of hall et al 2004 in the np package was used to estimate kernel bandwidths adaptive bandwidth factors abramson 1982 were used during the sampling process for regions of sparse training data to obtain smooth probability densities in terms of uncertainty characterization for the measurements input errors in rainfall and pet are included in the total model uncertainty estimate given by the sdmu method furthermore it is assumed that the statistics of meteorological observation errors are the same in pre and post change conditions this is a reasonable assumption so long as the same observing techniques are used throughout as is the case for the catchments considered in this study the assimilated runoff depths were assumed to contain zero mean heteroscedastic normally distributed instrument errors so that larger runoff depths have greater uncertainty see for instance salamon feyen 2009 the likelihood evaluated in the pf mcmc is therefore given by p y t o s t i n y t i μ y t o σ 2 a y t o i 1 n where a 0 1 a multiplier of 0 1 was chosen based on estimates adopted for similar gauges in hydrologic da studies e g clark et al 2008 weerts serafy 2006 xie et al 2014 lead times of 1 3 and 5 days were considered in generating streamflow forecasts for the pre change and post change conditions in order to isolate the effect of land use change on forecasting performance observed rainfall was used as the model input when generating all forecasts so as to exclude the effect of meteorological forecast errors a similar setup is used in other forecasting studies that aim to examine specific elements of da performance e g coustau et al 2013 likewise periods in the assimilation phase for both the pre change and post change conditions with storm magnitudes outside the range of those in the training period were excluded from the analysis this is to ensure that changes to forecast performance are not a result of storms not experienced in the training period this was required for only a minor portion of the assimilation period for the south western australian catchments see table 2 note that the unusually wet years from 1952 1958 were also excluded by peña arancibia et al 2012 in their analyses of changes in the comet catchment 4 3 forecast evaluation metrics probabilistic streamflow forecasts for the pre change and post change conditions were assessed using a range of forecast evaluation metrics the following forecast characteristics were quantified based on the work of hamill 2001 jolliffe stephenson 2012 murphy 1993 wilks 2005 wilks 2011 reliability a reliable forecast system gives an unbiased estimate of the observed frequencies associated with different forecast probability values wilks 2005 according to wilks 2011 reliability measures typically involve sorting the forecast observation pair into groups according to the value of the forecast variable the crps and logarithmic score discussed below are used here as measures of reliability resolution this refers to how distinct the forecast density is for different events for instance the climatological mean has zero resolution because it produces the same forecast at all points in time even when two different outcomes have different frequencies of occurrence wilks 2005 the continuous rank probability score crps provides a measure of both reliability and resolution hersbach 2000 and is frequently utilised in hydrologic forecasting e g li et al 2015 brown seo 2013 the crps measures how closely the cumulative distribution functions of the ensemble forecast and the corresponding observations match candille talagrand 2005 22 c r p s t f t f y f t o y 2 d y where f t f y empirical cumulative distribution function of the forecast of variable y at time t and f t o y cumulative distribution function of the observation of y at time t for cases where only a single observation of y is available at each time the associated cumulative distribution function is given by the heaviside step function 23 f t o y 0 y y t o 1 y y t o despite its use in the geosciences the crps can be a poor metric for complex forecast densities e g multiple modes high skewness as encountered in this study refer section 6 for example smith et al 2015 showed that the crps can give misleadingly good scores to outcomes that fall in between two modes of a bimodal forecast density we therefore use the logarithmic score ls good 1952 in addition to the crps to quantify reliability and resolution as it is better suited to non gaussian forecast densities the ls is a proper scoring rule that rewards forecasts where the outcome or observation falls in regions of high probability 24 l s t log f t y t o where ft indicates the predictive density function given for instance by a kernel density estimate of the weighted particles and y t o is the observation at time t accuracy this refers to the average correspondence between individual forecasts in an ensemble i e each particle and the observation wilks 2005 here the absolute deviation and the root mean square error rmse were used to measure accuracy the absolute deviation a k a absolute error is simply the absolute distance between the forecast and the observation for a probabilistic forecast a summary statistic is usually invoked to convert to a point forecast typically the ensemble mean e g li et al 2015 in this study we use the forecast mode as a measure of central tendency due to the presence of skewed and multi modal densities it is hereafter referred to as the absolute mode error 25 a m e t argmax y f t y y t o where ft y is the forecast density at time t and the argmax operator gives the value of the variable y that maximises the forecast density i e the mode the rmse provides second moment information about forecast accuracy when the probabilistic forecast is represented by a set of weighted particles the rmse is given by the square root of the average of the squared deviations between each particle and the observation 26 r m s e y t i 1 n φ t i y t i y t o 2 where y t i is the ith particle of the forecast at time t y t o is the observation at time t φ t i is the weight associated with the ith particle at time t and n is the number of particles lower values of all aforementioned forecast metrics indicate better performance the crps rmse and ame are all expressed in units of the forecast variable in this case mm for runoff depth whilst the log score is dimensionless note that for the purposes of calculating summary statistics and for ease of comparison each of the scores crps ls rmse and ame were normalized by their maximum observed values in order to obtain values in the 0 1 interval 5 results streamflow forecasting was undertaken for all study catchments using the pf mcmc and sdmu model uncertainty quantification method trained on pre change data with forecast performance evaluated via the full range of forecast evaluation metrics overall there was no detectable change in forecast performance between pre and post change conditions for all catchments except comet immediately after clearing 1971 1980 and wights these conclusions apply at all three lead times of 1 3 and 5 days this is based on an analysis of the difference in forecast score vs observed runoff depth relationship for pre and post change conditions figs 4 to 6 a nonparametric lowess smooth was applied to the data for undisturbed pre change and post change conditions see red and blue lines in figs 4 to 6 the particular lowess method adopted involves performing local linear regression using a weighted least squares approach where data points close to the point of interest determined by the span parameter receive higher weight cross validation was used to determine the optimal span parameter in the lowess fit that minimizes the sum of squared errors such a nonparametric method is suited to the case studies considered herein due to the difficulty in assigning a parametric regression model additionally the width of 95 confidence intervals on the lowess fit were approximated by 1 96 standard deviations where the standard deviation was estimated by empirical bootstrapping see red and blue shaded regions in figs 4 to 6 finally hypothesis testing was performed to determine whether there is a statistically significant difference in the regression curves for undisturbed pre and post change conditions we used the anova type test statistic introduced by young bowman 1995 for testing for the equality of 2 nonparametric regression curves the distribution of the test statistic was determined by the residual resampling bootstrap method a practically and statistically significant difference in the score vs runoff depth curves is apparent for both wights and comet immediately after clearing 1971 1980 as evidenced by the non overlapping confidence intervals and hypothesis test results figs 4 to 6 and tables 3 5 forecast quality is degraded considerably across all measures and lead times after land use change occurs particularly for high flow events forecast performances differences are much less pronounced for the other catchments for lemon hypothesis testing indicates the presence of statistically significant differences in most forecast scores across all lead times however the differences are of minimal practical significance see figs 4 6 and consist of a slight forecast improvement at mid range flows similar arguments can be made when considering the 2nd post change period 1981 1990 for comet and also the nammuc catchment note that for several forecast metrics for nammuc the statistically significant differences reflect a slight improvement in forecast performance for high flows in post change conditions we also examine the variation in forecast performance over time for post change conditions given that forecast performance varies with flow magnitude we focus our attention here on high flows which are generally more difficult to forecast well specifically we examine one day ahead forecast metrics for high flows in each year where high flows are defined as those exceeding the annual 98th percentile flow depth fig 7 shows the average difference between high flow forecast metrics for a given post change year and the lowess fit in pre change conditions for the corresponding flow value where positive deviations indicate a reduction in forecast skill in post change conditions i e increase in score value such an analysis ensures that forecast metrics of similar flow magnitudes are compared between undisturbed pre change and post change conditions the width of the 95 confidence intervals on the lowess fit for pre change conditions at the respective high flow values is also provided given by the red region in fig 7 fig 7 shows that for wights all aspects of forecast skill accuracy resolution and reliability are degraded for the years with the largest annual streamflow maxima with average ames corresponding to approximately 20 of the observed flows additionally there is greater variability in forecast score deviation across time compared to the other study catchments forecast degradation is most severe in the first 7 years after clearing i e up to 1983 see fig 7 although good forecast performance can be obtained in some high flow years e g in 1979 where the observed range of high flows is similar to 1977 which had severely degraded forecasts it appears that high flow forecast performance improves such that it is of similar quality to the undisturbed condition at approximately the same time as the establishment of equilibrium hydrologic conditions in around 1985 bari smettem 2004 overall however the tendency for degraded forecasts of high flows in post change conditions is apparent and more likely than in the other study catchments lemon is similar in size to wights but had only 50 of its land cleared with resulting changes to the hydrologic regime less pronounced and taking longer to develop than wights compare annual runoff coefficient over time in fig 3 despite observed changes to streamflow forecast quality is largely maintained in post change conditions across lead times see fig 4 6 one day ahead forecasts of high flows are of similar quality to forecasts of equivalent magnitude flows in pre change conditions this is demonstrated in fig 7 which shows that score deviations across time are almost always within the 95 pre change confidence intervals for all metrics with a slight improvement in some years of 0 5 mm for the crps rmse and ame unlike wights and lemon nammuc experienced patchy relatively more gradual changes from predominantly forested to cropland cover the most prominent changes to observed runoff have been to low flows with a 7 increase in the annual bfi from 1970 to 2000 pathiraja et al 2017b we therefore focus our attention on low flows defined as flows less than the annual 5th percentile to examine variations in forecast performance over time as shown in fig 8 contrary to expectations there is a minor but persistent improvement in low flow forecast performance across all metrics in the post change period see negative deviations in fig 8 this occurs because baseflows which are easier to predict at short time scales compared to low flow events are larger in magnitude in the post change period meaning that they are more often compared to low flow events in pre change conditions as these are similar in magnitude nonetheless improvements are minor 0 1 mm for the crps rmse and ame and ultimately support the conclusion that forecast performance has not degraded despite changes to land surface conditions similar to lemons roughly 50 of the comet catchment area experienced land cover change although such changes occurred over a much longer period of time and over a much larger area see table 1 and 2 changes to high flow forecast performance in the period 1971 1980 immediately after the period of rapid land cover change bear a resemblance to those of wights figs 4 6 looking at forecast performance over time fig 7 it is evident that 2 out of the 4 years with flow depths exceeding 4 mm have degraded forecasts across all metrics deviations are well outside the pre change confidence intervals conversely the period from 1981 to 1990 saw minimal changes to forecast performance across time compared to pre change conditions as demonstrated in fig 7 this is despite continual but minor clearing that occurred between 1980 and 2009 as well as enhanced observed storm flows of all magnitudes in this period peña arancibia et al 2012 these results can be partly attributed to the reduction in observed annual streamflow maxima compared to the wetter period 1971 1980 due to la nina conditions in the 1970s see fig 7 but also to the magnitude of prediction uncertainty relative to observed changes as discussed in section 6 6 discussion previous studies have highlighted the importance of accounting for and removing systematic model biases in hydrologic data assimilation because of its potential to significantly degrade assimilation quality e g alvarez garreton et al 2014 delannoy et al 2007 balmaseda et al 2007 dee dasilva 1998 ryu et al 2009 the experiments in this study demonstrate that not all systematic changes to catchment conditions with a detectable impact on observations require a deliberate re adjustment of the assimilation algorithm or related components for instance through a re calibrated model time varying parameters or revised offline bias estimation module in order to maintain forecast assimilation quality recall that in these experiments no modifications to the assimilation algorithm or models have been made to account for changed conditions furthermore recall that a forecast is generated by adding a random model error term to the model simulation and that such model error statistics have been informed by pre change data only these conclusions apply so long as the same da algorithm is used in pre and post change conditions and only if an effective da technique is used we now demonstrate that systematic model errors due to changes in catchment characteristics do not degrade forecasting performance when such errors are within the range of inherent prediction uncertainty specifically the time scale and magnitude of the impacts on the hydrologic regime relative to the inherent model prediction uncertainty determines whether assimilation and forecast performance is negatively impacted this is conceptualized in fig 9 which shows the relationship between the impact of land cover change on the hydrologic regime and forecast degradation for state estimation da it is somewhat difficult however to draw conclusions about types of land cover change and the effect on forecast performance as demonstrated by the 4 test cases this is likely due to the complex interaction between the spatial pattern of land use change the period over which change occurred catchment size resilience and climatic conditions instead we discuss the ability of state estimation da to maintain forecast quality for different types of impacts on system observations type a b and c impacts as defined below and in fig 9 representative examples of changes to forecast performance in each basin are shown in fig 10 6 1 type a impacts these experiments demonstrate that bias correction or model structural adjustments are required to avoid forecast degradation when using da for catchments with type a impacts type a impacts are defined as those where land cover changes have impacts on the hydrologic regime at time scales less than or equal to the assimilation model time step and where those impacts are larger in magnitude than the prediction uncertainty here prediction uncertainty refers collectively to initial condition model input parameter and structural uncertainties encountered at a given time which is characterized by the prior distribution factors that may cause type a impacts other than the nature of land cover change include model quality and accuracy of uncertainty characterization type a impacts are more likely to occur if a model is assigned low predictive uncertainty either because it produces simulations that are highly consistent with observations in pre change conditions or because certain aspects of prediction uncertainty are ignored commonly structural uncertainties examples of type a impacts are changes to the timing and magnitude of the peak flow response at the assimilation model time scale as seen in wights and comet 1st post change period poor forecast performance of peak daily flows in the post change period for wights can be attributed to 1 a reduction in catchment response time by roughly one day for extreme rainfall events i e peak flow occurs one day earlier than in pre change conditions recalling that assimilation is carried out at a daily time step and 2 a significant increase in daily peak flow depth well outside the range of streamflow prediction uncertainty this can be seen in fig 10c where the forecast for wights on 21 july 1981 severely underestimates the peak as it arrives a day earlier than expected c f the more accurate forecast for salmon the next day fig 10b when the peak is expected which is also half the magnitude of the peak at wights similarly changes to the timing of peak daily flows in comet after land cover change lead to poor forecast quality in the period 1971 1980 1st post change period a representative example is shown in fig 11 where forecasts of the rising limb and peak for the february 1978 event are severely underestimated this is due to the reduction in lag between peak runoff and rainfall to 3 days see fig 10i where response time was generally 4 5 days in pre change conditions see fig 10h which shows a lag of 5 days for a similar storm event in 1928 standard data assimilation alone is thus insufficient to correct for such systematic errors requiring additional bias correction or model structural adjustment techniques see for example pathiraja et al 2016b where a time varying parameter da framework improved peak flow predictions in the wights catchment another approach is to reduce the model and assimilation time step but this is not always feasible for computationally expensive models and when observations are available at a coarser temporal resolution 6 2 type b impacts not all impacts at a fine temporal scale lead to forecast degradation when using standard state estimation da catchments experiencing type b impacts same as type a except changes to the hydrologic regime are smaller in magnitude than the prediction uncertainty have no detectable change in forecast quality this is because the assimilation algorithm is able to gradually adjust the model latent states to reflect changed conditions loosely speaking some forms of systematic model errors can be corrected by the da algorithm in stable systems so long as they are within the estimated range of prediction uncertainty for both the latent states and outputs type b impacts are evident in lemons where daily streamflow depths in the may to july period are increased after clearing due to the generation of surface runoff earlier in the year compare the may to august period between ernies and lemon in fig 12 increases to daily flows are fairly gradual and within the range of prediction uncertainty thereby allowing the particle filter to select internal states that reflect the gradual increase in catchment wetness this allows for high quality model simulations even prior to the addition of estimated model errors see for instance the forecast on 2 august 1991 for lemon in fig 10e similar conclusions can be drawn for early season flows for wights see for instance fig 13 in such cases standard state estimation da will maintain forecast quality of pre change conditions without needing to explicitly account for systematic catchment changes although this is conditional on the accurate characterization of total uncertainty in model latent states in pre change conditions which is achieved using the sdmu approach 6 3 type c impacts similarly forecast quality can be maintained by using standard state estimation da in catchments with type c impacts same as type b except changes to the hydrologic regime predominantly occur at time scales longer than the model time step assimilation window the reason for this is much the same as for type b impacts in that the assimilation algorithm is able to gradually adjust latent states and outputs in response to systematic changes in the observations examples of type c impacts include changes to low flow volumes at the annual scale when undertaking daily streamflow assimilation as seen in nammuc in this case state estimation da promotes a gradual increase in shallow layer storage levels stw1 of the hbv model in direct correlation with increasing observed baseflow see fig 10a where both annual storage state and baseflow are represented as averages over the period january to march this allows for low flow forecast quality to be maintained despite systematic changes to baseflow dynamics see fig 14 note that interflow from the shallow layer store in hbv represents low flows in this catchment since the calibrated perc parameter was zero 7 conclusions data assimilation has been shown to dramatically improve forecast skill in hydrologic and meteorological applications although such improvements are conditional on using bias free observations and model simulations this work sheds new light on the impacts of systematic model errors in hydrologic data assimilation particularly in regards to streamflow forecasting in catchments with changing land surface conditions as the hydrologic model is more likely to produce biased simulations outside its calibration period we find that assimilation and forecast quality is not always negatively impacted when no efforts are made to explicitly account for model biases resulting from land cover changes rather the magnitude of such systematic errors relative to the inherent uncertainty in the model simulations even in pre change conditions is the key determinant of forecast degradation this is demonstrated through an assimilation study of a range of catchments with changing land cover conditions a particle filter was used to assimilated daily catchment outlet streamflow in four catchments experiencing varying levels of deforestation 1 lemon 3 44 km2 2 wights 0 94 km2 3 nammuc 2880 km2 and 4 comet 16 440 km2 each with a measurable impact on the hydrologic regime a total model uncertainty estimation method pathiraja et al 2017a was utilized to quantify the combined effects of parameter structural and input uncertainties in generating forecasts in pre change conditions these model uncertainty estimates were used to generate the prior density for streamflow assimilation in both pre and post change conditions contrary to expectations streamflow forecast performance at 1 3 and 5 day lead times in post change conditions degraded only in 2 cases wights and comet 1st post change period we find that forecast degradation occurs only when land cover changes lead to type a impacts on the hydrologic regime which is defined herein as having the following 2 properties 1 changes to the observed streamflow regime occur at time scales equal to or less than the assimilation window model time step e g peak daily runoff occurring 1 to 2 days earlier than in pre change conditions with assimilation undertaken at a daily time step and 2 when the magnitude of these changes exceeds the typical range of prediction uncertainty in pre change conditions e g significant increase in peak daily flow depth for similar meteorological conditions when the magnitude of such changes is within the range of prediction uncertainty in pre change conditions i e type b and type c impacts lemon nammuc and comet 2nd post change period the da algorithm is able to gradually adjust the internal model states without requiring any explicit modification to the algorithm or model uncertainty estimates this work shows that changes to catchment conditions do not always require adjustments to the modeling or assimilation methodology for instance through re calibration of the hydrologic model time varying model parameters or revised offline online bias estimation such strategies would be necessary for cases where the time scales of systematic errors are less than the assimilation model time step and when their magnitude is greater than the inherent prediction uncertainty during pre change conditions although we have only investigated a handful of catchments we have considered a range of catchment sizes climatic conditions land use change patterns and models future work will attempt to consider a larger pool of catchments in order to examine the interplay between various factors including pattern type and rate of land use change as well as hydrologic model quality and climatic conditions it is hoped that this will equip modellers and data assimilation scientists with a set of predictors to determine which conditions trigger the need for systematic error correction when undertaking da in changing catchments acknowledgments this study was funded by the australian research council as part of the discovery project dp140102394 this research has been partially funded by deutsche forschungsgemeinschaft dfg through the grant sfb 1294 data assimilation dr marshall is additionally supported through a future fellowship ft120100269 we gratefully acknowledge the vietnamese institute of water resources planning andrea castelletti west australian department of water and queensland department of natural resources and mines for provision of data used in this study data for the nammuc catchment was collected under the project imrr integrated and sustainable water management of red thai binh rivers system in changing climate funded by the italian ministry of foreign affairs delibera n 142 del 8 novembre 2010 we also gratefully acknowledge dr andrea castelletti for provision of data used in this study data utilized in this study can be made available from the authors upon request 
867,improving the understanding of subsurface systems and thus reducing prediction uncertainty requires collection of data as the collection of subsurface data is costly it is important that the data collection scheme is cost effective design of a cost effective data collection scheme i e data worth analysis requires quantifying model parameter prediction and both current and potential data uncertainties assessment of these uncertainties in large scale stochastic subsurface hydrological model simulations using standard monte carlo mc sampling or surrogate modeling is extremely computationally intensive sometimes even infeasible in this work we propose an efficient bayesian data worth analysis using a multilevel monte carlo mlmc method compared to the standard mc that requires a significantly large number of high fidelity model executions to achieve a prescribed accuracy in estimating expectations the mlmc can substantially reduce computational costs using multifidelity approximations since the bayesian data worth analysis involves a great deal of expectation estimation the cost saving of the mlmc in the assessment can be outstanding while the proposed mlmc based data worth analysis is broadly applicable we use it for a highly heterogeneous two phase subsurface flow simulation to select an optimal candidate data set that gives the largest uncertainty reduction in predicting mass flow rates at four production wells the choices made by the mlmc estimation are validated by the actual measurements of the potential data and consistent with the standard mc estimation but compared to the standard mc the mlmc greatly reduces the computational costs keywords multilevel monte carlo method bayesian data worth analysis uncertainty quantification computational efficiency 1 introduction rational management of water resources and environmental systems usually relies on subsurface flow and transport models to predict their responses to existing pollution and possible remediation schemes however the model predictions contain large uncertainties these uncertainties mainly originate from our poor knowledge of the heterogeneous porous media properties and hydrogeological and biogeochemical processes governing the flow transport and contaminant reactions improving our understanding of the systems and thus reducing the uncertainties requires collection of suitable data as the collection of subsurface data is expensive the design of a data collection scheme should be cost effective i e the expected benefit of new data exceeds its cost a major benefit of new data is its potential to improve our understanding of the systems through reducing the model prediction uncertainty and enhancing the prediction confidence various cost effective data collection schemes i e data worth analysis have been applied to water related problems since the 1970s davis et al 1972 maddock 1973 in the 1980s the application started extending to groundwater problems e g ben zvi et al 1988 and reichard and evans 1989 among current data worth analysis approaches in groundwater modeling one methodology is developed based on sensitivity analysis dausman et al 2010 fienen et al 2010 hill and tiedeman 2007 tiedeman et al 2004 these methods are computationally fast but they require model calibration and assume the models are linear finsterle 2015 the other methodology is developed within a fully bayesian framework these methods have no assumptions of the model and of the distributions of measurements and model parameters for example leube et al 2012 developed a predia preposterior data impact assessor method to assess the expected impact of data on prediction confidence and illustrated the method using a synthetic groundwater contaminant transport problem with random hydraulic conductivity fields nowak et al 2012 extended the application of predia to help the optimal design of a monitoring program so as to achieve confident decisions based on bayesian hypothesis testing principles in the meantime neuman et al 2012 proposed a bayesian data worth assessment approach dw based on the law of total probability and quantified the reduction in prediction uncertainty using the difference of prediction variance before and after the new data were considered lu et al 2012 applied the dw method to pneumatic permeability data from vertical and inclined boreholes drilled into unsaturated fractured tuff and dai et al 1972 2016 integrated the ensemble kalman filter enkf method within the dw framework to enable a sequential data worth assessment recently man et al 2017 developed an anova based transformed probabilistic collocation method for bayesian data worth analysis the core of the data worth analysis lies in evaluating the prediction uncertainty which comprises the quantification of parameter uncertainty model uncertainty and current and potential data uncertainties bayesian data worth analysis approaches quantify the uncertainties in detail using probability theory and their implementation usually involves monte carlo mc sampling for example leube et al 2012 used the bootstrap filter nowak et al 2012 neuman et al 2012 and lu et al 2012 used brute force mc simulation and man et al 2017 used both the mc and markov chain mc the mc methods are flexible enough to be applied to a wide range of problems including nonlinear models as the mc analysis entirely preserves statistical features of arbitrarily high orders toward the prediction also they can handle high dimensional problems as the mc convergence barely depends on the stochastic dimensionality however the mc approaches involve repeated sampling and a large ensemble size is usually required to ensure approximation accuracy even worse when employed in the data worth analysis the mc process should be conducted for each set of candidate data that to be potentially collected which results in a massive computational requirement for a long simulation model and or a large set of potential data the computing cost is often unaffordable limiting the application of the data worth analysis several strategies have been proposed to reduce the computational complexity of the bayesian data worth analysis one is estimating the prediction uncertainty through linearization using first order second moment methods cirpka et al 2004 neuman et al 2012 the linearization strategy is straightforward and computationally efficient but it is limited to linear and weakly nonlinear problems moreover as this method propagates mean and variance from parameters and data to predictions through sensitivity analysis the number of model parameters and data volume matter a large size could pose difficulties in computational load and storage capacities this usually happens in large grid models where parameters are treated as random fields as in the geostatistical study fritz et al 2009 another strategy is basing models on stochastic moment equations mes xue et al 2014 solving mes are computationally cheap but the solutions may be a poor approximation for problems with a large number of parameters furthermore the me methods require solving equations satisfied by the conditional statistical moments e g ensemble means and covariances of quantities of interest qoi such as hydraulic heads and fluxes in random heterogeneous media thus the me methods being an intrusive approach have to solve different mes for different qoi and all equations need to be constructed individually alternatively enkf methods have been integrated in bayesian data worth analysis as a nonintrusive and straightforward methodology that can be applied to nonlinear problems zhang et al 2005 enkf is usually optimal for situations where the considered variables satisfy a multivariate gaussian distribution enkf is essentially a mc method so a large ensemble size is often required for an accurate approximation to improve the computational efficiency li and xiu 2009 and zeng and zhang 2010 proposed a probabilistic collocation based ensemble kalman filter pckf approach that constructs a fast to evaluate surrogate model to replace the actual simulation model run in the enkf sampling dai et al 1972 2016 embedded the pckf method into the bayesian data worth analysis and had successful applications in a contaminant transport model and a geologic co2 sequestration system with a medium parameter size for a problem with a large number of uncertain parameters e g a highly heterogeneous random field in geostatistical studies the surrogate based methods may not be suitable because its computational cost explodes exponentially with increasing stochastic dimensions zhang et al 2013 2016 in this work we propose an efficient bayesian data worth analysis using a multilevel monte carlo mlmc method the primary concept behind multilevel methods is to solve the underlying mathematical model on a sequence of geometric meshes with increasing resolutions giles 2008 from coarse meshes low resolution levels to fine meshes high resolution levels as opposed to the standard mc method that performs sampling only at the highest resolution the mlmc method calculates the predictive statistics by conducting sampling on multiple levels of resolution in a way that achieves the same accuracy but reduces the overall computing cost the computational saving comes from the fact that the mlmc can optimally allocate more samples on the coarse levels with inexpensive model executions and fewer samples on the fine levels with expensive model runs the implementation of the mlmc is no more complicated than the standard mc and the mlmc shares many beneficial features of the mc sampling for example its minor dependence of the stochastic dimensions therefore the mlmc based data worth analysis can be efficiently applied to high dimensional problems with a large number of parameters the bayesian data worth analysis considered in this study is derived based on the law of total probability where the data worth is measured by a reduction in model prediction uncertainty brought by the data the prediction uncertainty is evaluated by the prediction variance and the prediction uncertainty reduction is calculated as the difference of the prediction variance before and after the new data is collected this data worth analysis involves a large number of expectation estimations in this work we use the mlmc method to estimate the expectations so as to improve the computational efficiency the mlmc methods are being popularly applied to solve high dimensional uncertainty quantification problems for example cliffe et al 2011 applied the mlmc method to a heterogeneous groundwater flow model to estimate the expectation of the flux at the center of the domain and the cumulative outflow from the entire region lu et al 2016 employed the mlmc method to a highly heterogeneous oil reservoir simulation to approximate the probability distribution of the mass flow rate at a production well here for the first time we integrate the mlmc method into the bayesian data worth analysis to evaluate the reduction of prediction uncertainty our approach takes advantage of the mlmc s efficiency in estimating statistical expectations and makes the computationally intensive bayesian data worth analysis applicable to high dimensional stochastic problems the proposed method is applied to a two phase subsurface flow in a two dimensional random permeability field this application is novel and important because the field is highly heterogeneous and with limited permeability measurements most of the field is uncertain this causes a large uncertainty in predicting the mass flow rates at production wells in order to reduce the prediction uncertainty we have several choices of potential data for selection from which we use our bayesian data worth analysis to collect the one giving the largest uncertainty reduction the rest of the paper is organized as follows in section 2 we define a two phase subsurface flow and transport problem and describe the data worth analysis in this study in section 3 the bayesian data worth analysis approach is formulated following that we introduce the standard mc and the mlmc methods in evaluating the data worth in section 4 we apply the mlmc based data worth analysis to a two phase subsurface flow model where its effectiveness and efficiency is demonstrated in comparison with the assessment from the standard mc in section 5 we end this paper by giving major conclusions 2 problem description an incompressible and immiscible two phase flow problem is considered in this study the two phases water and oil flow through a heterogeneous porous media where gravity and capillary effects are neglected and porosity is assumed a constant assume the water and oil saturations have s w s o 1 the simulation of water injection into an oil reservoir in a bounded domain d for a time period t can be described by a flow equation for fluid pressure p and a transport equation for the phase saturation as helmig 1997 1 v g where v k λ s w p for t x t d ϕ s w t f s w v g w ρ w subject to suitable boundary and initial conditions in the formulation v is the darcy velocity of the engaging fluids g is the source term ϕ is the porosity ρ represents the density the term f sw v describes viscous force and k is the permeability variable that we consider to be isotropic and is simulated as a random field total mobility is defined by λ s w λ w s w λ o s w where λi models the reduced mobility of phase i due to presence of the other phase i represents w or o the mobility λw and λo depends on the viscosity μw and μo the irreducible oil saturation sor and the connate water saturation swc in the following way 2 λ w s w s n w 2 μ w λ o s w 1 s n w 2 μ o s n w s w s w c 1 s o r s w c we are interested in estimating the mass flow rates q at some given time space points for a specific point t x it has q ρ v t x a with a representing the cross sectional area the uncertainty associated with the random permeability k field causes the uncertainty in prediction of q more than often the permeability field is very heterogeneous and the available measurements of k are very sparse resulting in a large uncertainty in k and thus a great uncertainty in predicting q collection of new k data can reduce the uncertainty but different choice of k measurements may yield different degrees of uncertainty reduction xue et al 2014 hence given the limited budget it is important to make an optimal selection from a set of choices of k that can bring the most valuable information to produce the largest uncertainty reduction in predicting q in this effort an efficient and effective data worth analysis using the mlmc method is proposed and applied to a two phase subsurface flow model to solve the uncertainty reduction problems in a highly heterogeneous permeability field 3 methodology in this section we first briefly introduce the bayesian method of data worth analysis proposed by neuman et al 2012 and define the metric of uncertainty reduction then we describe the mc and mlmc approaches to estimate the uncertainty reduction metric in the description we focus on analyzing the computational costs required by the two methods 3 1 bayesian data worth analysis consider a random vector q as the quantities of interest which is to be predicted using a numerical model characterized by a random parameter field the random parameter field has some measurement data denoted as d conditional on d the expectation and variance of predicting q can be represented as e q d and v q d suppose that the available data d can be augmented by another hypothetical data set c which has not yet been collected and is therefore uncertain random but the statistics of c can be estimated based on current data d according to the law of total expectation and the law of total variance it has 3 e q d e c d e q d c 4 v q d e c d v q d c v c d e q d c the term v c d e q d c v q d e c d v q d c represents a difference between the variance conditional on d and the expected variance conditional jointly on the augmented data set d c as the difference is a variance i e a non negative value conditioning on d and c jointly results in a lower variance than conditioning on d solely thus collection of additional data c ensures the uncertainty reduction in predicting q we define the difference v c d e q d c as the metric of uncertainty reduction brought by the potential data c in evaluation of v c d e q d c we first need to calculate e q d c for each sample of c the expectation e q d c can be estimated by mc sampling that requires repeated simulations of q in practice the equations in eq 1 are usually solved by numerical methods e g finite element or finite volume techniques which demand a discrete mesh of the spatial domain d we denote by t m the mesh for computation with m cells in d and use q m to represent the corresponding numerical solution of q we assume that as m the expected value e q m d c e q d c in the following we estimate e q m d c using both mc and mlmc methods among a set of choices of c and then calculate and compare the uncertainty reduction metric v c d e q m d c between the c options and select the one giving the largest reduction for notation simplicity in sections 3 2 and 3 3 we use e q to represent e q d c and e q m to represent e q m d c for a sample of c 3 2 the mc method for estimating expectation the standard mc estimator for e q m is 5 q m mc 1 n m c i 1 n m c q m i where q m i is the i t h sample of q m and nmc is the total number of independent samples note that q m mc is an unbiased estimator for e q m i e e q m mc e q m using eq 5 to estimate e q includes two sources of errors 1 the approximation of e q by e q m where the error is related to the spatial discretization in our problem and 2 the estimation of the expected value e q m by a finite sample average q m mc where the difference is caused by the mc sampling error here we use a mean squared error mse to measure the accuracy of the standard mc estimator and analyze the contribution of both errors the mse is defined as 6 e q m mc 2 e q m mc e q 2 e q m mc e q m mc 2 e q m mc e q 2 v q m mc e q m mc e q 2 since v q m mc n m c 1 v q m and e q m mc e q m we get 7 e q m mc 2 n m c 1 v q m e q m q 2 the first term in the mse of eq 7 is the variance of the mc estimator which is small as v q m is small and decays inversely with the number of samples nmc the second term is the square of the difference in expectation between q m and q which can be reduced by using a fine mesh with a large m thus for the estimator q m mc to achieve an accurate approximation of e q with a small mse a large number of samples need to be simulated on a sufficient fine mesh which results in a huge computational cost more importantly in the data worth analysis the evaluation of v c d e q d c requires the estimation of e q for each sample of potential data c and this calculation may apply to a set of choices of c assume the cost to compute one sample of q m is c m we generate nc realizations of random c to evaluate its uncertainty and have ns sets of candidates to select the optimal c the total cost of the standard mc estimator with nmc samples for the data worth analysis is 8 c mc n s n c n m c c m note that in eq 8 we assume each candidate of c requires the same number of realizations nc to consider its randomness and the same mc sample size nmc for convergence in reality nc and nmc can be different for different c candidates ns in eq 8 is problem dependent nc is usually large to capture the potential c data uncertainty one way to reduce the total cost in eq 8 is to reduce the cost in mc estimation of e q m i e the cost c q m mc n m c c m where c m is the computational time for simulating one q m sample on the mesh t m and nmc is the sample size that is determined by the estimation accuracy for the mc estimator to achieve a root mean squared error rmse accuracy ε a sufficient condition is that both terms in eq 7 are less than ε2 2 this requires 9 n m c 2 v q m ɛ 2 for a small ε and a large variance v q m nmc can be very large reducing nmc the number of mc samples and m the simulation domain resolution lowers the computational cost but in the meantime causes a high estimation error to reduce the computational cost without sacrificing the accuracy we introduce the mlmc method in the data worth analysis 3 3 the mlmc method for estimating expectation in estimation of the expectation e q m the main idea of the mlmc method is to not just sample from one approximation q m of q but from a sequence of approximations q m ℓ ℓ 0 l associated with the meshes t m ℓ ℓ 0 l with m 0 m 1 m l m usually m ℓ 1 2 d m ℓ for ℓ 1 l where d is dimension of the spatial domain d ℓ is the level of mesh resolution and m ℓ is the number of grid cells of the mesh t m ℓ in this way the expectation e q m can be expressed as 10 e q m e q m 0 ℓ 1 l e q m ℓ q m ℓ 1 ℓ 0 l e y ℓ where y 0 q m 0 and y ℓ q m ℓ q m ℓ 1 for 1 ℓ l as such e y ℓ for ℓ 0 l in eq 10 can be estimated by the following mc estimator 11 y 0 1 n 0 i 1 n 0 q m 0 i and y ℓ 1 n ℓ i 1 n ℓ q m ℓ i q m ℓ 1 i where n ℓ is the number of samples on level ℓ note that the quantities q m ℓ i and q m ℓ 1 i are computed using the same set of n ℓ samples but simulated on two meshes t m ℓ and t m ℓ 1 of consecutive levels consequently the mlmc estimator of e q m is 12 q m ml ℓ 0 l y ℓ the expression in eq 10 indicates that the expectation on the finest level is equal to the expectation on the coarsest level plus a sum of differences in expectation between simulations on consecutive levels in this way the less accurate estimate on the coarsest level can be gradually corrected by the estimates on the finer levels thereby the mlmc estimator can achieve the same accuracy as the standard mc estimator defined solely on the finest level in the same manner of eq 7 the mse of the mlmc estimator can be written as 13 e q m ml 2 ℓ 0 l n ℓ 1 v y ℓ e q m q 2 where the second term of the discritization error in eq 13 is exactly the same as in eq 7 similar to the standard mc one sufficient condition to achieve the rmse accuracy ε is that both terms in eq 13 are smaller than ε2 2 to make the first term of the sampling error smaller than ε2 2 n ℓ can be determined in multiple ways here we determine n ℓ using the same strategy as in giles 2008 such that the variance of estimator q m ml is minimized that is we set n ℓ c 0 v y ℓ c ℓ where c ℓ c m ℓ c m ℓ 1 is the computational cost of simulating one realization of y ℓ and c 0 0 is a constant by setting ℓ 0 l n ℓ 1 v y ℓ ɛ 2 2 we can solve c 0 2 ɛ 2 ℓ 0 l v y ℓ c ℓ thus 14 n ℓ 2 ɛ 2 v y ℓ c ℓ k 0 l v y k c k for ℓ 0 l the total cost of the mlmc estimator q m ml can be expressed by 15 c q m ml ℓ 0 l c ℓ n ℓ with n ℓ defined in eq 14 it can be shown that to achieve the desired ε the cost of the mlmc estimation of e q is asymptotically lower than that of the standard mc as long as the variance v y ℓ decays reciprocally with ℓ i e v y ℓ 0 as ℓ cliffe et al 2011 proved that if the variance v y ℓ decays faster with ℓ than c ℓ increases the dominant term will be on the coarsest level 0 the cost saving of mlmc compared to standard mc reflects the ratio of the costs of samples on the coarsest level 0 to those on the finest level l if the variance v y ℓ decays slower than the cost c ℓ increases the dominant term will be on the finest level l and the cost saving of the mlmc compared to the standard mc will be approximately v y l v y 0 so in both cases the mlmc has a substantial gain on the other hand it can be verified that v q m is approximately a constant independent of m which results in v y ℓ v q m and consequently n ℓ n mc for a large ℓ according to eqs 9 and 14 and as demonstrated in the numerical example in section 4 thus mlmc simulates a large number of samples at the coarse levels with inexpensive model runs but much fewer samples at the fine levels with expensive model runs in contrast the standard mc simulates a very large number of samples on the finest level solely with the most expensive model executions in this way the mlmc improves the computational efficiency in the estimation of e q in the data worth analysis the cost saving of mlmc can be more significant when evaluating the uncertainty reduction v c d e q d c for the same ns sets of candidates of c with each c having the same nc realizations as in eq 8 the total cost of the mlmc estimator for the data worth analysis is 16 c ml n s n c ℓ 0 l c ℓ n ℓ when mlmc brings a one fold reduction in computing time in the estimation of e q it can bring ns nc times reduction in the data worth analysis compared to the standard mc for a large selection of c with a large number of realizations the cost saving brought by mlmc can be remarkable 4 numerical example and results in this section we apply the mlmc based bayesian data worth analysis to a two phase subsurface flow simulation to demonstrate the effectiveness of the method the numerical example is designed as a synthetic case where the true data values are known to show the efficiency the mlmc results are compared with the standard mc results 4 1 model description the numerical experiment is designed based on the fine grid model from the tenth spe comparative solution project christie and blunt 2001 the original model is three dimensional 3d with a domain size of 365 76 670 56 51 816 m3 and 60 220 85 grid cells the permeability field is highly heterogeneous the model simulates mass flow rates of four production wells at the four corners of the domain the simulation of this fine scale model is very time costly and computationally unaffordable for the data worth analysis for example with the reactive flow and transport simulator pflotran mills et al 2007 running on ten processors it takes about eight hours to complete one simulation of 100 days to evaluate our data worth analysis in a reasonable time we simplify the original model as follows i the newly designed model has a two dimensional 2d domain with a relatively coarse domain resolution i e a 360 720 m2 domain with a total of 60 120 grid cells and ii the synthetic permeability k field is tailored from the first layer of the original geological model as shown in fig 1 and the porosity is set to a constant of 0 1 we are interested in the mass flow rates at the four production wells fig 1 after 1 2 3 4 and 5 days from the initial time with injection in the center of the domain although it is a simplification of the 3d case the 2d model considered here is still high dimensional and highly heterogeneous the number of stochastic degrees of freedom of the model is 60 120 this large number of parameters is extremely challenging for the stochastic galerkin and stochastic collocation methods because their computational cost grows exponentially with the parameter size thus the less dimensional dependent mc sampling methods are typically the choice in addition the model has large variability in permeability the variance of the permeability is about 1 13 m4 and the coefficient of variation is about 297 this large variability suggests that a great number of mc samples are needed for obtaining accurate approximation due to the slow rate of convergence of the standard mc method the high dimensionality and heterogeneity makes the data worth analysis exceptionally difficult for this 2d example we will use the mlmc method to improve the computational efficiency in the analysis note that although the flow model in this example is relatively simple the mlmc method can be applied to more complicated subsurface flow models including those with internal boundary conditions the application of the mlmc method is straightforward after a sequence of numerical models with increasing spatial resolutions are designed assume currently only 36 permeability data are available denoted as data d uniformly distributed in the domain fig 1 as the data d are not enough to accurately describe the permeability field it causes large uncertainty in predicting the mass flow rates q at the four production wells p1 p4 now we have four choices of locations to collect additional data denoted as c 1 c 2 c 3 and c 4 and each choice has 9 data shown in fig 1 our objective is to select one set of potential data c to provide more valuable information of the permeability field and thus bring a larger uncertainty reduction in predicting q we essentially consider four optional schemes that correspond to the data worth analysis at the four wells we are asking the question given that one has funds to measure permeability k in only one area to improve the prediction in one well among which areas c 1 c 2 c 3 and c 4 should one conduct such measurements as comparison the data worth analysis is conducted using both the standard mc and the mlmc methods to estimate the prediction uncertainty reduction as validation the selection of the optimal potential data giving the largest uncertainty reduction is compared against the choice made according to the actual measurements of c the actual measurements of the four potential data sets are denoted as c 1 c 2 c 3 and c 4 respectively and we evaluate the quantities of interest q in its logarithm form 4 2 data worth analysis for actual data to evaluate the prediction uncertainty of q associated with current data set d and the augmented data sets d c 1 d c 2 d c 3 and d c 4 we employ the following procedures first we assume that the logarithm of the random permeability field log k is a gaussian process whose mean and covariance can be inferred from its measurements conditioned on the five sets of data and based on the sampled variograms shown in fig 2 random realizations of the log k fields are generated using the sequential gaussian simulation program in gslib package deutsch and journel 1998 then for each realization of log k we simulate the mass flow rates at the four production wells using pflotran next we calculate the variance of q based on the five data sets respectively and consequently evaluate the uncertainty reduction brought by the actual augmented data and select the data set among c 1 c 2 c 3 and c 4 that gives the largest reduction in predicting the mass flow rate in each production well we generate 5000 realizations of random log k field while insuring that its mean and variance has stabilized this produces 5000 values of the mass flow rates q m fig 3 shows the density of the 5000 log q m samples at the four production wells based on current data d and the augmented data sets d c i where i represents 1 2 3 and 4 here and hereinafter the narrower range of predicted log q m based on d c i than on d only indicates that adding more permeability data reduces the uncertainty in predicting q this is true for prediction in all four production wells the specific uncertainty reduction brought by the augmented data is quantified in fig 4 where it shows the values of v q m d v q m d c for five simulation times at the four wells and q m is evaluated in its logarithm form fig 4 shows that augmenting the permeability measurements from any choice of potential data c reduces the variance in predicting q however a different choice gives a different amount of reduction for a specific production well for example collecting new data c 1 results in greatest uncertainty reduction in well p1 the collection of data c 2 reduces the prediction uncertainty the most in well p2 c 3 is the right choice for well p3 and c 4 is the best selection for well p4 these top choices are obviously superior to the second choices giving remarkably larger variance reduction the results are not surprising considering the relative locations of the four new data sets to the production wells as shown in fig 1 the data set c i is at the corner of the domain where the well p i is located here i represents 1 2 3 and 4 respectively 4 3 data worth analysis for randomly generated data using mc methods since in reality the actual measurements of c are unknown we generate 200 samples of c conditional on available data d by assuming c follows a multivariate normal distribution where the mean values and covariance matrix are estimated by kriging method using the revised kt3d package in gslib fig 5 shows the actual measurements c the 200 samples of the estimated c and their mean values and 95 confidence intervals for the four candidates of c the mean values of estimated c are seen to represent a smoothed version of c suggesting a bias in the generated c values almost all the c lie inside the confidence intervals indicating that the generated c values generally reflect the information contained in c the standard mc method is used to estimate the uncertainty reduction v c d e q d c brought by the sampled c first for each sample c k k 1 2 200 we recalibrate the variogram model against the augmented data set d c k and then according to the fitted variogram model and conditional on the expanded data base we generate a certain number of realizations of the random log k field next for each realization of log k we simulate the mass flow rates at the production wells and obtain the q m the number of model simulations i e the realization size of log k for each c k is determined by the variance v q m d c k and the given rmse accuracy ε through n m c 2 ɛ 2 v q m d c k as the variance of q m is different for different c k the resulting nmc is different to ensure the estimated e q m d c satisfies the desired accuracy we take the maximum variance max v q m d c over all c samples to calculate the mc sample size and use the corresponding maxnmc consistently to estimate each e q m d c k for k 1 2 200 according to eq 5 finally we calculate the variance v c d e q m d c over the c samples to estimate the uncertainty reduction given the desired accuracy ɛ 0 05 fig 6 shows the maximum variance max v q m d c and the corresponding maxnmc values based on the augmented data sets d c 1 d c 2 d c 3 and d c 4 when using the standard mc to predict q in the four production wells it can be seen that a small variance of q m leads to a small nmc among the four choices of potential data c conditioning on c i gives the smallest variance of q m at the well pi for i 1 2 3 and 4 for example to estimate e q m d c at well p1 the required number of q m samples based on c 1 c 2 c 3 and c 4 is 681 1243 817 and 1266 respectively with the smallest nmc for c 1 fig 7 shows the uncertainty reduction v c d e q m d c at the four production wells brought by the augmented data sets with estimated c where the mass flow rate is evaluated in its logarithm form we verified that 200 realizations of c are enough for the calculation of v c d e q m d c to stabilize a comparison of fig 7 to fig 4 reveals that their temporal patterns are similar for example in prediction of the uncertainty reduction at well p1 the reduction amount brought by data c 1 decreases as simulation time increases this is observed for both actual c 1 and estimated c 1 likewise the similar temporal patterns can be seen for well p2 when bringing the new data c 2 and for well p3 when bringing the data c 3 more importantly our generated c values lead to a correct choice of the potential data as validated by their actual measurements that is collecting c i data obtains the largest uncertainty reduction in predicting q at well p i besides the top choices the orders of other options of c with respect to uncertainty reduction based on the estimated c are consistent with those based on the actual measured c for example in prediction of q at well p2 the uncertainty reduction brought by the four c candidates according to their estimated and measured values is in the same order i e c 2 c 1 c 3 and c 4 from the largest to the smallest and they both select c 2 as the optimal choice meanwhile we notice that the generated c values underestimate the potential of such data to help reduce the prediction uncertainty as it is shown the reduced variances plotted in fig 7 are consistently smaller than those depicted in fig 4 the reason could be that our generated c values do not contain all information about the actual measurements c as observed in fig 5 the generated c are able to enclose c within its 95 confidence intervals but fail to capture all the fluctuations of c the data worth analysis is not expected to give the exact benefit that the potential data would bring before the data can be actually measured the method is successful when it makes a relatively correct choice among several options in fact the vital importance of data worth analysis is that it helps decision makers make an optimal decision before an execution of a plan in this sense the defined bayesian data worth analysis has a successful application in this two phase subsurface flow simulation 4 4 data worth analysis for randomly generated data using mlmc methods although the proposed data worth analysis based on estimated c works well using the mc estimation the computational cost is very expensive which limits its potential application to a large domain problem in this section the mlmc method is used to estimate e q d c with the goal to improve the computational efficiency in calculating the uncertainty reduction v c d e q d c to implement the mlmc method we first generate a set of candidate meshes for the definition of t m ℓ ℓ 0 l the finest mesh is set to the measurement scale of the random log k field with 60 120 grid cells the coarser meshes are constructed by halving the resolution of their succeeding finer meshes in each direction in this way we obtain seven candidate meshes with resolutions 1 2 2 4 4 8 8 15 15 30 30 60 and 60 120 respectively we choose the coarsest mesh as 4 8 so that the length of one grid cell is smaller than the correlation length of the random field in such a way to reduce the variance of v y ℓ cliffe et al 2011 lu et al 2016 in this setting the last five meshes are used in the calculation denoted as t m ℓ for ℓ 0 1 2 3 4 respectively with the highest level l 4 fig 8 depicts the five meshes where the log k values of the coarse meshes are upscaled from the finest mesh using arithmetic mean some advanced upscaling methods can be applied li et al 2011a 2011b wen and gómez hernández 1996 they are expected to be able to further improve the computational efficiency of the mlmc method by reducing the difference of neighboring levels and corresponding accelerating the variance decay when using the mlmc method to estimate e q d c with a series of estimation of e y ℓ based on eq 10 and eq 11 the number of y ℓ samples on each level ℓ is determined by v y ℓ according to eq 14 n ℓ is different over c samples as the variance of y ℓ is different among the c k like the standard mc estimation we calculate the maximum variance max v y ℓ over c k and use the corresponding maxn ℓ to estimate e y ℓ according to eq 11 given the same ɛ 0 05 fig 9 plots max v q m ℓ and max v y ℓ for levels ℓ 0 1 2 3 4 based on the augmented data sets d c 1 d c 2 d c 3 and d c 4 in predicting the mass flow rates at the four production wells the figure indicates that in each case the variance max v q m ℓ is almost a constant for all the levels max v y ℓ is smaller than max v q m ℓ and decreases with levels so at the finest level l 4 the max v y l is much smaller than max v q m resulting in a quite fewer number of the most expensive model simulations in mlmc than in the standard mc the corresponding maxn ℓ is plotted in fig 10 it can see that maxn ℓ decays fast along levels for example in the prediction of q at well p1 based on data d c 1 the maxn 4 is 53 and the maxn 0 is 3203 the decrease is 3150 in the 16 prediction scenarios presented in fig 10 i e predication of q at well pi based on data d c j with i j 1 2 3 4 the largest difference between maxn 0 and maxn 4 is 5431 when using data d c 4 to predict well p1 the rapid decrease of n ℓ from the coarsest level ℓ 0 to the finest level ℓ 4 indicates that a fewer and fewer number of models are simulated on the high resolution domain with expensive computational costs this results in potential cost savings of mlmc compared to the standard mc that conducts all the model simulations on the highest resolution domain for example in predicting q at well p1 based on data d c 1 the standard mc needs max n m c 681 model simulations on the finest domain to achieve the accuracy ɛ 0 05 while the mlmc just needs max n 4 53 model runs on the same domain resolution although mlmc requires additional simulations on lower levels the total computational cost of mlmc can be still less than that of standard mc because of the relatively small simulation time spent on the coarse levels fig 11 depicts the computational time c m ℓ of a single model run on the meshes t m ℓ at the five levels the values are the average of ten simulations the figure indicates that c m ℓ increases slowly at the first three levels where the simulation times are relatively minor and even negligible then c m ℓ has a dramatic increase from ℓ 3 to ℓ 4 and climbs up to a large cost at the finest level where the standard mc conducts all its simulations this exponential growth of c m ℓ and the strategy of mlmc result in the total simulation time of mlmc significantly smaller than that of the standard mc fig 12 summarizes the total computational time of the standard mc i e c q m mc and the mlmc i e c q m ml in eq 15 in each of the 16 prediction scenarios i e the cost in estimating e q d c j k at well pi based on one realization of c j k for i j 1 2 3 4 the figure shows that the computational time of the mlmc is significantly lower than that of the standard mc in all the 16 prediction scenarios the cost saving of the mlmc can be up to 22 8 hours in the prediction of well p4 based on d c 1 in data worth analysis we evaluate the uncertainty reduction v c d e q d c for the four potential c options so the costs presented in fig 12 should multiply the number of c realizations nc for each candidate c and then summarize over all four c candidates as we use the same nc for the four c candidates the total cost of the standard mc used in data worth analysis in predicting q at well p1 would be the summation of the four blue points values in fig 12 a multiplying nc i e 200 the corresponding total cost of the mlmc would be the summation of the four red points values in the same figure multiplying 200 in this manner we calculate the total costs of the standard mc and the mlmc for the data worth analysis at the four production wells and summarize the results in table 1 the table indicates that to achieve the same accuracy ɛ 0 05 in estimating the prediction uncertainty reduction of q at one production well mlmc only needs about 1 6 computational time of the standard mc considering that the cost of the standard mc can be up to 733 days using one processor the cost saving of the mlmc is very significant more than 600 days in this work we set the accuracy ɛ 0 05 for a higher accuracy requirement a larger computational gain from the mlmc can be expected as proved in cliffe et al 2011 and discussed in lu et al 2016 with the substantially less computational time the mlmc can give a consistent result with the standard mc in selecting the optimal potential data for the maximum uncertainty reduction as shown in fig 13 between the four c options adding c i brings the largest uncertainty reduction in predicting q at the well pi where i 1 2 3 and 4 this choice is consistent with the results based on the standard mc estimation which is validated by the actual measurements of the data c besides in comparison of fig 7 and fig 13 we find that the estimated variance reductions based on the standard mc and the mlmc are very close to each other with the largest relative difference about 6 and this slight difference could come from the randomness the results suggest that the bayesian data worth analysis using the mlmc method can not only effectively help select an appropriate data set but also give a similar quantification as the standard mc more efficiently 5 conclusions in this work we propose an efficient bayesian data worth analysis using a multilevel monte carlo mlmc method the proposed approach tackles a challenging problem of designing a cost effective data collection scheme in large scale stochastic subsurface hydrological modeling the cost effective data collection design i e data worth analysis requires quantifying model parameter prediction current data and potential data uncertainties however uncertainty quantification in large scale stochastic problems is extremely computationally intensive sometimes even infeasible using standard monte carlo mc sampling or advanced surrogate modeling this study uses the mlmc method in the bayesian data worth analysis with improved computational efficiency the application to a highly heterogeneous subsurface flow model indicates that the proposed mlmc based data worth analysis can select an optimal candidate data set that brings the most valuable information in predicting mass flow rates at four production wells the mlmc results are validated by the actual measurements of the potential data and similar with the estimation obtained from the standard mc but compared to the standard mc the mlmc greatly reduces the computational costs in the assessment with up to 600 days and 85 cost savings when one processor is used our study is limited to a predetermined choice of potential data so as to allow validating our results against known data and justifying the proposed method in practice one can apply an optimization algorithm to select among alternative candidates for a case with a large number of candidates available a significant computational cost saving from the mlmc can be expected as for each candidate data the mlmc can be applied to improve the computational efficiency though demonstration of our mlmc based data worth analysis has been limited to a subsurface flow model the approach is general enough to apply to a wide range of problems acknowledgments primary support for this work was provided by the scientific discovery through advanced computing scidac program funded by the u s department of energy office of advanced scientific computing research ascr and office of biological and environmental research ber additional support was provided by ber s terrestrial ecosystem science scientific focus area tes sfa project the authors are supported by oak ridge national laboratory which is supported by the office of science of the u s department of energy under contract de ac05 00or22725 
867,improving the understanding of subsurface systems and thus reducing prediction uncertainty requires collection of data as the collection of subsurface data is costly it is important that the data collection scheme is cost effective design of a cost effective data collection scheme i e data worth analysis requires quantifying model parameter prediction and both current and potential data uncertainties assessment of these uncertainties in large scale stochastic subsurface hydrological model simulations using standard monte carlo mc sampling or surrogate modeling is extremely computationally intensive sometimes even infeasible in this work we propose an efficient bayesian data worth analysis using a multilevel monte carlo mlmc method compared to the standard mc that requires a significantly large number of high fidelity model executions to achieve a prescribed accuracy in estimating expectations the mlmc can substantially reduce computational costs using multifidelity approximations since the bayesian data worth analysis involves a great deal of expectation estimation the cost saving of the mlmc in the assessment can be outstanding while the proposed mlmc based data worth analysis is broadly applicable we use it for a highly heterogeneous two phase subsurface flow simulation to select an optimal candidate data set that gives the largest uncertainty reduction in predicting mass flow rates at four production wells the choices made by the mlmc estimation are validated by the actual measurements of the potential data and consistent with the standard mc estimation but compared to the standard mc the mlmc greatly reduces the computational costs keywords multilevel monte carlo method bayesian data worth analysis uncertainty quantification computational efficiency 1 introduction rational management of water resources and environmental systems usually relies on subsurface flow and transport models to predict their responses to existing pollution and possible remediation schemes however the model predictions contain large uncertainties these uncertainties mainly originate from our poor knowledge of the heterogeneous porous media properties and hydrogeological and biogeochemical processes governing the flow transport and contaminant reactions improving our understanding of the systems and thus reducing the uncertainties requires collection of suitable data as the collection of subsurface data is expensive the design of a data collection scheme should be cost effective i e the expected benefit of new data exceeds its cost a major benefit of new data is its potential to improve our understanding of the systems through reducing the model prediction uncertainty and enhancing the prediction confidence various cost effective data collection schemes i e data worth analysis have been applied to water related problems since the 1970s davis et al 1972 maddock 1973 in the 1980s the application started extending to groundwater problems e g ben zvi et al 1988 and reichard and evans 1989 among current data worth analysis approaches in groundwater modeling one methodology is developed based on sensitivity analysis dausman et al 2010 fienen et al 2010 hill and tiedeman 2007 tiedeman et al 2004 these methods are computationally fast but they require model calibration and assume the models are linear finsterle 2015 the other methodology is developed within a fully bayesian framework these methods have no assumptions of the model and of the distributions of measurements and model parameters for example leube et al 2012 developed a predia preposterior data impact assessor method to assess the expected impact of data on prediction confidence and illustrated the method using a synthetic groundwater contaminant transport problem with random hydraulic conductivity fields nowak et al 2012 extended the application of predia to help the optimal design of a monitoring program so as to achieve confident decisions based on bayesian hypothesis testing principles in the meantime neuman et al 2012 proposed a bayesian data worth assessment approach dw based on the law of total probability and quantified the reduction in prediction uncertainty using the difference of prediction variance before and after the new data were considered lu et al 2012 applied the dw method to pneumatic permeability data from vertical and inclined boreholes drilled into unsaturated fractured tuff and dai et al 1972 2016 integrated the ensemble kalman filter enkf method within the dw framework to enable a sequential data worth assessment recently man et al 2017 developed an anova based transformed probabilistic collocation method for bayesian data worth analysis the core of the data worth analysis lies in evaluating the prediction uncertainty which comprises the quantification of parameter uncertainty model uncertainty and current and potential data uncertainties bayesian data worth analysis approaches quantify the uncertainties in detail using probability theory and their implementation usually involves monte carlo mc sampling for example leube et al 2012 used the bootstrap filter nowak et al 2012 neuman et al 2012 and lu et al 2012 used brute force mc simulation and man et al 2017 used both the mc and markov chain mc the mc methods are flexible enough to be applied to a wide range of problems including nonlinear models as the mc analysis entirely preserves statistical features of arbitrarily high orders toward the prediction also they can handle high dimensional problems as the mc convergence barely depends on the stochastic dimensionality however the mc approaches involve repeated sampling and a large ensemble size is usually required to ensure approximation accuracy even worse when employed in the data worth analysis the mc process should be conducted for each set of candidate data that to be potentially collected which results in a massive computational requirement for a long simulation model and or a large set of potential data the computing cost is often unaffordable limiting the application of the data worth analysis several strategies have been proposed to reduce the computational complexity of the bayesian data worth analysis one is estimating the prediction uncertainty through linearization using first order second moment methods cirpka et al 2004 neuman et al 2012 the linearization strategy is straightforward and computationally efficient but it is limited to linear and weakly nonlinear problems moreover as this method propagates mean and variance from parameters and data to predictions through sensitivity analysis the number of model parameters and data volume matter a large size could pose difficulties in computational load and storage capacities this usually happens in large grid models where parameters are treated as random fields as in the geostatistical study fritz et al 2009 another strategy is basing models on stochastic moment equations mes xue et al 2014 solving mes are computationally cheap but the solutions may be a poor approximation for problems with a large number of parameters furthermore the me methods require solving equations satisfied by the conditional statistical moments e g ensemble means and covariances of quantities of interest qoi such as hydraulic heads and fluxes in random heterogeneous media thus the me methods being an intrusive approach have to solve different mes for different qoi and all equations need to be constructed individually alternatively enkf methods have been integrated in bayesian data worth analysis as a nonintrusive and straightforward methodology that can be applied to nonlinear problems zhang et al 2005 enkf is usually optimal for situations where the considered variables satisfy a multivariate gaussian distribution enkf is essentially a mc method so a large ensemble size is often required for an accurate approximation to improve the computational efficiency li and xiu 2009 and zeng and zhang 2010 proposed a probabilistic collocation based ensemble kalman filter pckf approach that constructs a fast to evaluate surrogate model to replace the actual simulation model run in the enkf sampling dai et al 1972 2016 embedded the pckf method into the bayesian data worth analysis and had successful applications in a contaminant transport model and a geologic co2 sequestration system with a medium parameter size for a problem with a large number of uncertain parameters e g a highly heterogeneous random field in geostatistical studies the surrogate based methods may not be suitable because its computational cost explodes exponentially with increasing stochastic dimensions zhang et al 2013 2016 in this work we propose an efficient bayesian data worth analysis using a multilevel monte carlo mlmc method the primary concept behind multilevel methods is to solve the underlying mathematical model on a sequence of geometric meshes with increasing resolutions giles 2008 from coarse meshes low resolution levels to fine meshes high resolution levels as opposed to the standard mc method that performs sampling only at the highest resolution the mlmc method calculates the predictive statistics by conducting sampling on multiple levels of resolution in a way that achieves the same accuracy but reduces the overall computing cost the computational saving comes from the fact that the mlmc can optimally allocate more samples on the coarse levels with inexpensive model executions and fewer samples on the fine levels with expensive model runs the implementation of the mlmc is no more complicated than the standard mc and the mlmc shares many beneficial features of the mc sampling for example its minor dependence of the stochastic dimensions therefore the mlmc based data worth analysis can be efficiently applied to high dimensional problems with a large number of parameters the bayesian data worth analysis considered in this study is derived based on the law of total probability where the data worth is measured by a reduction in model prediction uncertainty brought by the data the prediction uncertainty is evaluated by the prediction variance and the prediction uncertainty reduction is calculated as the difference of the prediction variance before and after the new data is collected this data worth analysis involves a large number of expectation estimations in this work we use the mlmc method to estimate the expectations so as to improve the computational efficiency the mlmc methods are being popularly applied to solve high dimensional uncertainty quantification problems for example cliffe et al 2011 applied the mlmc method to a heterogeneous groundwater flow model to estimate the expectation of the flux at the center of the domain and the cumulative outflow from the entire region lu et al 2016 employed the mlmc method to a highly heterogeneous oil reservoir simulation to approximate the probability distribution of the mass flow rate at a production well here for the first time we integrate the mlmc method into the bayesian data worth analysis to evaluate the reduction of prediction uncertainty our approach takes advantage of the mlmc s efficiency in estimating statistical expectations and makes the computationally intensive bayesian data worth analysis applicable to high dimensional stochastic problems the proposed method is applied to a two phase subsurface flow in a two dimensional random permeability field this application is novel and important because the field is highly heterogeneous and with limited permeability measurements most of the field is uncertain this causes a large uncertainty in predicting the mass flow rates at production wells in order to reduce the prediction uncertainty we have several choices of potential data for selection from which we use our bayesian data worth analysis to collect the one giving the largest uncertainty reduction the rest of the paper is organized as follows in section 2 we define a two phase subsurface flow and transport problem and describe the data worth analysis in this study in section 3 the bayesian data worth analysis approach is formulated following that we introduce the standard mc and the mlmc methods in evaluating the data worth in section 4 we apply the mlmc based data worth analysis to a two phase subsurface flow model where its effectiveness and efficiency is demonstrated in comparison with the assessment from the standard mc in section 5 we end this paper by giving major conclusions 2 problem description an incompressible and immiscible two phase flow problem is considered in this study the two phases water and oil flow through a heterogeneous porous media where gravity and capillary effects are neglected and porosity is assumed a constant assume the water and oil saturations have s w s o 1 the simulation of water injection into an oil reservoir in a bounded domain d for a time period t can be described by a flow equation for fluid pressure p and a transport equation for the phase saturation as helmig 1997 1 v g where v k λ s w p for t x t d ϕ s w t f s w v g w ρ w subject to suitable boundary and initial conditions in the formulation v is the darcy velocity of the engaging fluids g is the source term ϕ is the porosity ρ represents the density the term f sw v describes viscous force and k is the permeability variable that we consider to be isotropic and is simulated as a random field total mobility is defined by λ s w λ w s w λ o s w where λi models the reduced mobility of phase i due to presence of the other phase i represents w or o the mobility λw and λo depends on the viscosity μw and μo the irreducible oil saturation sor and the connate water saturation swc in the following way 2 λ w s w s n w 2 μ w λ o s w 1 s n w 2 μ o s n w s w s w c 1 s o r s w c we are interested in estimating the mass flow rates q at some given time space points for a specific point t x it has q ρ v t x a with a representing the cross sectional area the uncertainty associated with the random permeability k field causes the uncertainty in prediction of q more than often the permeability field is very heterogeneous and the available measurements of k are very sparse resulting in a large uncertainty in k and thus a great uncertainty in predicting q collection of new k data can reduce the uncertainty but different choice of k measurements may yield different degrees of uncertainty reduction xue et al 2014 hence given the limited budget it is important to make an optimal selection from a set of choices of k that can bring the most valuable information to produce the largest uncertainty reduction in predicting q in this effort an efficient and effective data worth analysis using the mlmc method is proposed and applied to a two phase subsurface flow model to solve the uncertainty reduction problems in a highly heterogeneous permeability field 3 methodology in this section we first briefly introduce the bayesian method of data worth analysis proposed by neuman et al 2012 and define the metric of uncertainty reduction then we describe the mc and mlmc approaches to estimate the uncertainty reduction metric in the description we focus on analyzing the computational costs required by the two methods 3 1 bayesian data worth analysis consider a random vector q as the quantities of interest which is to be predicted using a numerical model characterized by a random parameter field the random parameter field has some measurement data denoted as d conditional on d the expectation and variance of predicting q can be represented as e q d and v q d suppose that the available data d can be augmented by another hypothetical data set c which has not yet been collected and is therefore uncertain random but the statistics of c can be estimated based on current data d according to the law of total expectation and the law of total variance it has 3 e q d e c d e q d c 4 v q d e c d v q d c v c d e q d c the term v c d e q d c v q d e c d v q d c represents a difference between the variance conditional on d and the expected variance conditional jointly on the augmented data set d c as the difference is a variance i e a non negative value conditioning on d and c jointly results in a lower variance than conditioning on d solely thus collection of additional data c ensures the uncertainty reduction in predicting q we define the difference v c d e q d c as the metric of uncertainty reduction brought by the potential data c in evaluation of v c d e q d c we first need to calculate e q d c for each sample of c the expectation e q d c can be estimated by mc sampling that requires repeated simulations of q in practice the equations in eq 1 are usually solved by numerical methods e g finite element or finite volume techniques which demand a discrete mesh of the spatial domain d we denote by t m the mesh for computation with m cells in d and use q m to represent the corresponding numerical solution of q we assume that as m the expected value e q m d c e q d c in the following we estimate e q m d c using both mc and mlmc methods among a set of choices of c and then calculate and compare the uncertainty reduction metric v c d e q m d c between the c options and select the one giving the largest reduction for notation simplicity in sections 3 2 and 3 3 we use e q to represent e q d c and e q m to represent e q m d c for a sample of c 3 2 the mc method for estimating expectation the standard mc estimator for e q m is 5 q m mc 1 n m c i 1 n m c q m i where q m i is the i t h sample of q m and nmc is the total number of independent samples note that q m mc is an unbiased estimator for e q m i e e q m mc e q m using eq 5 to estimate e q includes two sources of errors 1 the approximation of e q by e q m where the error is related to the spatial discretization in our problem and 2 the estimation of the expected value e q m by a finite sample average q m mc where the difference is caused by the mc sampling error here we use a mean squared error mse to measure the accuracy of the standard mc estimator and analyze the contribution of both errors the mse is defined as 6 e q m mc 2 e q m mc e q 2 e q m mc e q m mc 2 e q m mc e q 2 v q m mc e q m mc e q 2 since v q m mc n m c 1 v q m and e q m mc e q m we get 7 e q m mc 2 n m c 1 v q m e q m q 2 the first term in the mse of eq 7 is the variance of the mc estimator which is small as v q m is small and decays inversely with the number of samples nmc the second term is the square of the difference in expectation between q m and q which can be reduced by using a fine mesh with a large m thus for the estimator q m mc to achieve an accurate approximation of e q with a small mse a large number of samples need to be simulated on a sufficient fine mesh which results in a huge computational cost more importantly in the data worth analysis the evaluation of v c d e q d c requires the estimation of e q for each sample of potential data c and this calculation may apply to a set of choices of c assume the cost to compute one sample of q m is c m we generate nc realizations of random c to evaluate its uncertainty and have ns sets of candidates to select the optimal c the total cost of the standard mc estimator with nmc samples for the data worth analysis is 8 c mc n s n c n m c c m note that in eq 8 we assume each candidate of c requires the same number of realizations nc to consider its randomness and the same mc sample size nmc for convergence in reality nc and nmc can be different for different c candidates ns in eq 8 is problem dependent nc is usually large to capture the potential c data uncertainty one way to reduce the total cost in eq 8 is to reduce the cost in mc estimation of e q m i e the cost c q m mc n m c c m where c m is the computational time for simulating one q m sample on the mesh t m and nmc is the sample size that is determined by the estimation accuracy for the mc estimator to achieve a root mean squared error rmse accuracy ε a sufficient condition is that both terms in eq 7 are less than ε2 2 this requires 9 n m c 2 v q m ɛ 2 for a small ε and a large variance v q m nmc can be very large reducing nmc the number of mc samples and m the simulation domain resolution lowers the computational cost but in the meantime causes a high estimation error to reduce the computational cost without sacrificing the accuracy we introduce the mlmc method in the data worth analysis 3 3 the mlmc method for estimating expectation in estimation of the expectation e q m the main idea of the mlmc method is to not just sample from one approximation q m of q but from a sequence of approximations q m ℓ ℓ 0 l associated with the meshes t m ℓ ℓ 0 l with m 0 m 1 m l m usually m ℓ 1 2 d m ℓ for ℓ 1 l where d is dimension of the spatial domain d ℓ is the level of mesh resolution and m ℓ is the number of grid cells of the mesh t m ℓ in this way the expectation e q m can be expressed as 10 e q m e q m 0 ℓ 1 l e q m ℓ q m ℓ 1 ℓ 0 l e y ℓ where y 0 q m 0 and y ℓ q m ℓ q m ℓ 1 for 1 ℓ l as such e y ℓ for ℓ 0 l in eq 10 can be estimated by the following mc estimator 11 y 0 1 n 0 i 1 n 0 q m 0 i and y ℓ 1 n ℓ i 1 n ℓ q m ℓ i q m ℓ 1 i where n ℓ is the number of samples on level ℓ note that the quantities q m ℓ i and q m ℓ 1 i are computed using the same set of n ℓ samples but simulated on two meshes t m ℓ and t m ℓ 1 of consecutive levels consequently the mlmc estimator of e q m is 12 q m ml ℓ 0 l y ℓ the expression in eq 10 indicates that the expectation on the finest level is equal to the expectation on the coarsest level plus a sum of differences in expectation between simulations on consecutive levels in this way the less accurate estimate on the coarsest level can be gradually corrected by the estimates on the finer levels thereby the mlmc estimator can achieve the same accuracy as the standard mc estimator defined solely on the finest level in the same manner of eq 7 the mse of the mlmc estimator can be written as 13 e q m ml 2 ℓ 0 l n ℓ 1 v y ℓ e q m q 2 where the second term of the discritization error in eq 13 is exactly the same as in eq 7 similar to the standard mc one sufficient condition to achieve the rmse accuracy ε is that both terms in eq 13 are smaller than ε2 2 to make the first term of the sampling error smaller than ε2 2 n ℓ can be determined in multiple ways here we determine n ℓ using the same strategy as in giles 2008 such that the variance of estimator q m ml is minimized that is we set n ℓ c 0 v y ℓ c ℓ where c ℓ c m ℓ c m ℓ 1 is the computational cost of simulating one realization of y ℓ and c 0 0 is a constant by setting ℓ 0 l n ℓ 1 v y ℓ ɛ 2 2 we can solve c 0 2 ɛ 2 ℓ 0 l v y ℓ c ℓ thus 14 n ℓ 2 ɛ 2 v y ℓ c ℓ k 0 l v y k c k for ℓ 0 l the total cost of the mlmc estimator q m ml can be expressed by 15 c q m ml ℓ 0 l c ℓ n ℓ with n ℓ defined in eq 14 it can be shown that to achieve the desired ε the cost of the mlmc estimation of e q is asymptotically lower than that of the standard mc as long as the variance v y ℓ decays reciprocally with ℓ i e v y ℓ 0 as ℓ cliffe et al 2011 proved that if the variance v y ℓ decays faster with ℓ than c ℓ increases the dominant term will be on the coarsest level 0 the cost saving of mlmc compared to standard mc reflects the ratio of the costs of samples on the coarsest level 0 to those on the finest level l if the variance v y ℓ decays slower than the cost c ℓ increases the dominant term will be on the finest level l and the cost saving of the mlmc compared to the standard mc will be approximately v y l v y 0 so in both cases the mlmc has a substantial gain on the other hand it can be verified that v q m is approximately a constant independent of m which results in v y ℓ v q m and consequently n ℓ n mc for a large ℓ according to eqs 9 and 14 and as demonstrated in the numerical example in section 4 thus mlmc simulates a large number of samples at the coarse levels with inexpensive model runs but much fewer samples at the fine levels with expensive model runs in contrast the standard mc simulates a very large number of samples on the finest level solely with the most expensive model executions in this way the mlmc improves the computational efficiency in the estimation of e q in the data worth analysis the cost saving of mlmc can be more significant when evaluating the uncertainty reduction v c d e q d c for the same ns sets of candidates of c with each c having the same nc realizations as in eq 8 the total cost of the mlmc estimator for the data worth analysis is 16 c ml n s n c ℓ 0 l c ℓ n ℓ when mlmc brings a one fold reduction in computing time in the estimation of e q it can bring ns nc times reduction in the data worth analysis compared to the standard mc for a large selection of c with a large number of realizations the cost saving brought by mlmc can be remarkable 4 numerical example and results in this section we apply the mlmc based bayesian data worth analysis to a two phase subsurface flow simulation to demonstrate the effectiveness of the method the numerical example is designed as a synthetic case where the true data values are known to show the efficiency the mlmc results are compared with the standard mc results 4 1 model description the numerical experiment is designed based on the fine grid model from the tenth spe comparative solution project christie and blunt 2001 the original model is three dimensional 3d with a domain size of 365 76 670 56 51 816 m3 and 60 220 85 grid cells the permeability field is highly heterogeneous the model simulates mass flow rates of four production wells at the four corners of the domain the simulation of this fine scale model is very time costly and computationally unaffordable for the data worth analysis for example with the reactive flow and transport simulator pflotran mills et al 2007 running on ten processors it takes about eight hours to complete one simulation of 100 days to evaluate our data worth analysis in a reasonable time we simplify the original model as follows i the newly designed model has a two dimensional 2d domain with a relatively coarse domain resolution i e a 360 720 m2 domain with a total of 60 120 grid cells and ii the synthetic permeability k field is tailored from the first layer of the original geological model as shown in fig 1 and the porosity is set to a constant of 0 1 we are interested in the mass flow rates at the four production wells fig 1 after 1 2 3 4 and 5 days from the initial time with injection in the center of the domain although it is a simplification of the 3d case the 2d model considered here is still high dimensional and highly heterogeneous the number of stochastic degrees of freedom of the model is 60 120 this large number of parameters is extremely challenging for the stochastic galerkin and stochastic collocation methods because their computational cost grows exponentially with the parameter size thus the less dimensional dependent mc sampling methods are typically the choice in addition the model has large variability in permeability the variance of the permeability is about 1 13 m4 and the coefficient of variation is about 297 this large variability suggests that a great number of mc samples are needed for obtaining accurate approximation due to the slow rate of convergence of the standard mc method the high dimensionality and heterogeneity makes the data worth analysis exceptionally difficult for this 2d example we will use the mlmc method to improve the computational efficiency in the analysis note that although the flow model in this example is relatively simple the mlmc method can be applied to more complicated subsurface flow models including those with internal boundary conditions the application of the mlmc method is straightforward after a sequence of numerical models with increasing spatial resolutions are designed assume currently only 36 permeability data are available denoted as data d uniformly distributed in the domain fig 1 as the data d are not enough to accurately describe the permeability field it causes large uncertainty in predicting the mass flow rates q at the four production wells p1 p4 now we have four choices of locations to collect additional data denoted as c 1 c 2 c 3 and c 4 and each choice has 9 data shown in fig 1 our objective is to select one set of potential data c to provide more valuable information of the permeability field and thus bring a larger uncertainty reduction in predicting q we essentially consider four optional schemes that correspond to the data worth analysis at the four wells we are asking the question given that one has funds to measure permeability k in only one area to improve the prediction in one well among which areas c 1 c 2 c 3 and c 4 should one conduct such measurements as comparison the data worth analysis is conducted using both the standard mc and the mlmc methods to estimate the prediction uncertainty reduction as validation the selection of the optimal potential data giving the largest uncertainty reduction is compared against the choice made according to the actual measurements of c the actual measurements of the four potential data sets are denoted as c 1 c 2 c 3 and c 4 respectively and we evaluate the quantities of interest q in its logarithm form 4 2 data worth analysis for actual data to evaluate the prediction uncertainty of q associated with current data set d and the augmented data sets d c 1 d c 2 d c 3 and d c 4 we employ the following procedures first we assume that the logarithm of the random permeability field log k is a gaussian process whose mean and covariance can be inferred from its measurements conditioned on the five sets of data and based on the sampled variograms shown in fig 2 random realizations of the log k fields are generated using the sequential gaussian simulation program in gslib package deutsch and journel 1998 then for each realization of log k we simulate the mass flow rates at the four production wells using pflotran next we calculate the variance of q based on the five data sets respectively and consequently evaluate the uncertainty reduction brought by the actual augmented data and select the data set among c 1 c 2 c 3 and c 4 that gives the largest reduction in predicting the mass flow rate in each production well we generate 5000 realizations of random log k field while insuring that its mean and variance has stabilized this produces 5000 values of the mass flow rates q m fig 3 shows the density of the 5000 log q m samples at the four production wells based on current data d and the augmented data sets d c i where i represents 1 2 3 and 4 here and hereinafter the narrower range of predicted log q m based on d c i than on d only indicates that adding more permeability data reduces the uncertainty in predicting q this is true for prediction in all four production wells the specific uncertainty reduction brought by the augmented data is quantified in fig 4 where it shows the values of v q m d v q m d c for five simulation times at the four wells and q m is evaluated in its logarithm form fig 4 shows that augmenting the permeability measurements from any choice of potential data c reduces the variance in predicting q however a different choice gives a different amount of reduction for a specific production well for example collecting new data c 1 results in greatest uncertainty reduction in well p1 the collection of data c 2 reduces the prediction uncertainty the most in well p2 c 3 is the right choice for well p3 and c 4 is the best selection for well p4 these top choices are obviously superior to the second choices giving remarkably larger variance reduction the results are not surprising considering the relative locations of the four new data sets to the production wells as shown in fig 1 the data set c i is at the corner of the domain where the well p i is located here i represents 1 2 3 and 4 respectively 4 3 data worth analysis for randomly generated data using mc methods since in reality the actual measurements of c are unknown we generate 200 samples of c conditional on available data d by assuming c follows a multivariate normal distribution where the mean values and covariance matrix are estimated by kriging method using the revised kt3d package in gslib fig 5 shows the actual measurements c the 200 samples of the estimated c and their mean values and 95 confidence intervals for the four candidates of c the mean values of estimated c are seen to represent a smoothed version of c suggesting a bias in the generated c values almost all the c lie inside the confidence intervals indicating that the generated c values generally reflect the information contained in c the standard mc method is used to estimate the uncertainty reduction v c d e q d c brought by the sampled c first for each sample c k k 1 2 200 we recalibrate the variogram model against the augmented data set d c k and then according to the fitted variogram model and conditional on the expanded data base we generate a certain number of realizations of the random log k field next for each realization of log k we simulate the mass flow rates at the production wells and obtain the q m the number of model simulations i e the realization size of log k for each c k is determined by the variance v q m d c k and the given rmse accuracy ε through n m c 2 ɛ 2 v q m d c k as the variance of q m is different for different c k the resulting nmc is different to ensure the estimated e q m d c satisfies the desired accuracy we take the maximum variance max v q m d c over all c samples to calculate the mc sample size and use the corresponding maxnmc consistently to estimate each e q m d c k for k 1 2 200 according to eq 5 finally we calculate the variance v c d e q m d c over the c samples to estimate the uncertainty reduction given the desired accuracy ɛ 0 05 fig 6 shows the maximum variance max v q m d c and the corresponding maxnmc values based on the augmented data sets d c 1 d c 2 d c 3 and d c 4 when using the standard mc to predict q in the four production wells it can be seen that a small variance of q m leads to a small nmc among the four choices of potential data c conditioning on c i gives the smallest variance of q m at the well pi for i 1 2 3 and 4 for example to estimate e q m d c at well p1 the required number of q m samples based on c 1 c 2 c 3 and c 4 is 681 1243 817 and 1266 respectively with the smallest nmc for c 1 fig 7 shows the uncertainty reduction v c d e q m d c at the four production wells brought by the augmented data sets with estimated c where the mass flow rate is evaluated in its logarithm form we verified that 200 realizations of c are enough for the calculation of v c d e q m d c to stabilize a comparison of fig 7 to fig 4 reveals that their temporal patterns are similar for example in prediction of the uncertainty reduction at well p1 the reduction amount brought by data c 1 decreases as simulation time increases this is observed for both actual c 1 and estimated c 1 likewise the similar temporal patterns can be seen for well p2 when bringing the new data c 2 and for well p3 when bringing the data c 3 more importantly our generated c values lead to a correct choice of the potential data as validated by their actual measurements that is collecting c i data obtains the largest uncertainty reduction in predicting q at well p i besides the top choices the orders of other options of c with respect to uncertainty reduction based on the estimated c are consistent with those based on the actual measured c for example in prediction of q at well p2 the uncertainty reduction brought by the four c candidates according to their estimated and measured values is in the same order i e c 2 c 1 c 3 and c 4 from the largest to the smallest and they both select c 2 as the optimal choice meanwhile we notice that the generated c values underestimate the potential of such data to help reduce the prediction uncertainty as it is shown the reduced variances plotted in fig 7 are consistently smaller than those depicted in fig 4 the reason could be that our generated c values do not contain all information about the actual measurements c as observed in fig 5 the generated c are able to enclose c within its 95 confidence intervals but fail to capture all the fluctuations of c the data worth analysis is not expected to give the exact benefit that the potential data would bring before the data can be actually measured the method is successful when it makes a relatively correct choice among several options in fact the vital importance of data worth analysis is that it helps decision makers make an optimal decision before an execution of a plan in this sense the defined bayesian data worth analysis has a successful application in this two phase subsurface flow simulation 4 4 data worth analysis for randomly generated data using mlmc methods although the proposed data worth analysis based on estimated c works well using the mc estimation the computational cost is very expensive which limits its potential application to a large domain problem in this section the mlmc method is used to estimate e q d c with the goal to improve the computational efficiency in calculating the uncertainty reduction v c d e q d c to implement the mlmc method we first generate a set of candidate meshes for the definition of t m ℓ ℓ 0 l the finest mesh is set to the measurement scale of the random log k field with 60 120 grid cells the coarser meshes are constructed by halving the resolution of their succeeding finer meshes in each direction in this way we obtain seven candidate meshes with resolutions 1 2 2 4 4 8 8 15 15 30 30 60 and 60 120 respectively we choose the coarsest mesh as 4 8 so that the length of one grid cell is smaller than the correlation length of the random field in such a way to reduce the variance of v y ℓ cliffe et al 2011 lu et al 2016 in this setting the last five meshes are used in the calculation denoted as t m ℓ for ℓ 0 1 2 3 4 respectively with the highest level l 4 fig 8 depicts the five meshes where the log k values of the coarse meshes are upscaled from the finest mesh using arithmetic mean some advanced upscaling methods can be applied li et al 2011a 2011b wen and gómez hernández 1996 they are expected to be able to further improve the computational efficiency of the mlmc method by reducing the difference of neighboring levels and corresponding accelerating the variance decay when using the mlmc method to estimate e q d c with a series of estimation of e y ℓ based on eq 10 and eq 11 the number of y ℓ samples on each level ℓ is determined by v y ℓ according to eq 14 n ℓ is different over c samples as the variance of y ℓ is different among the c k like the standard mc estimation we calculate the maximum variance max v y ℓ over c k and use the corresponding maxn ℓ to estimate e y ℓ according to eq 11 given the same ɛ 0 05 fig 9 plots max v q m ℓ and max v y ℓ for levels ℓ 0 1 2 3 4 based on the augmented data sets d c 1 d c 2 d c 3 and d c 4 in predicting the mass flow rates at the four production wells the figure indicates that in each case the variance max v q m ℓ is almost a constant for all the levels max v y ℓ is smaller than max v q m ℓ and decreases with levels so at the finest level l 4 the max v y l is much smaller than max v q m resulting in a quite fewer number of the most expensive model simulations in mlmc than in the standard mc the corresponding maxn ℓ is plotted in fig 10 it can see that maxn ℓ decays fast along levels for example in the prediction of q at well p1 based on data d c 1 the maxn 4 is 53 and the maxn 0 is 3203 the decrease is 3150 in the 16 prediction scenarios presented in fig 10 i e predication of q at well pi based on data d c j with i j 1 2 3 4 the largest difference between maxn 0 and maxn 4 is 5431 when using data d c 4 to predict well p1 the rapid decrease of n ℓ from the coarsest level ℓ 0 to the finest level ℓ 4 indicates that a fewer and fewer number of models are simulated on the high resolution domain with expensive computational costs this results in potential cost savings of mlmc compared to the standard mc that conducts all the model simulations on the highest resolution domain for example in predicting q at well p1 based on data d c 1 the standard mc needs max n m c 681 model simulations on the finest domain to achieve the accuracy ɛ 0 05 while the mlmc just needs max n 4 53 model runs on the same domain resolution although mlmc requires additional simulations on lower levels the total computational cost of mlmc can be still less than that of standard mc because of the relatively small simulation time spent on the coarse levels fig 11 depicts the computational time c m ℓ of a single model run on the meshes t m ℓ at the five levels the values are the average of ten simulations the figure indicates that c m ℓ increases slowly at the first three levels where the simulation times are relatively minor and even negligible then c m ℓ has a dramatic increase from ℓ 3 to ℓ 4 and climbs up to a large cost at the finest level where the standard mc conducts all its simulations this exponential growth of c m ℓ and the strategy of mlmc result in the total simulation time of mlmc significantly smaller than that of the standard mc fig 12 summarizes the total computational time of the standard mc i e c q m mc and the mlmc i e c q m ml in eq 15 in each of the 16 prediction scenarios i e the cost in estimating e q d c j k at well pi based on one realization of c j k for i j 1 2 3 4 the figure shows that the computational time of the mlmc is significantly lower than that of the standard mc in all the 16 prediction scenarios the cost saving of the mlmc can be up to 22 8 hours in the prediction of well p4 based on d c 1 in data worth analysis we evaluate the uncertainty reduction v c d e q d c for the four potential c options so the costs presented in fig 12 should multiply the number of c realizations nc for each candidate c and then summarize over all four c candidates as we use the same nc for the four c candidates the total cost of the standard mc used in data worth analysis in predicting q at well p1 would be the summation of the four blue points values in fig 12 a multiplying nc i e 200 the corresponding total cost of the mlmc would be the summation of the four red points values in the same figure multiplying 200 in this manner we calculate the total costs of the standard mc and the mlmc for the data worth analysis at the four production wells and summarize the results in table 1 the table indicates that to achieve the same accuracy ɛ 0 05 in estimating the prediction uncertainty reduction of q at one production well mlmc only needs about 1 6 computational time of the standard mc considering that the cost of the standard mc can be up to 733 days using one processor the cost saving of the mlmc is very significant more than 600 days in this work we set the accuracy ɛ 0 05 for a higher accuracy requirement a larger computational gain from the mlmc can be expected as proved in cliffe et al 2011 and discussed in lu et al 2016 with the substantially less computational time the mlmc can give a consistent result with the standard mc in selecting the optimal potential data for the maximum uncertainty reduction as shown in fig 13 between the four c options adding c i brings the largest uncertainty reduction in predicting q at the well pi where i 1 2 3 and 4 this choice is consistent with the results based on the standard mc estimation which is validated by the actual measurements of the data c besides in comparison of fig 7 and fig 13 we find that the estimated variance reductions based on the standard mc and the mlmc are very close to each other with the largest relative difference about 6 and this slight difference could come from the randomness the results suggest that the bayesian data worth analysis using the mlmc method can not only effectively help select an appropriate data set but also give a similar quantification as the standard mc more efficiently 5 conclusions in this work we propose an efficient bayesian data worth analysis using a multilevel monte carlo mlmc method the proposed approach tackles a challenging problem of designing a cost effective data collection scheme in large scale stochastic subsurface hydrological modeling the cost effective data collection design i e data worth analysis requires quantifying model parameter prediction current data and potential data uncertainties however uncertainty quantification in large scale stochastic problems is extremely computationally intensive sometimes even infeasible using standard monte carlo mc sampling or advanced surrogate modeling this study uses the mlmc method in the bayesian data worth analysis with improved computational efficiency the application to a highly heterogeneous subsurface flow model indicates that the proposed mlmc based data worth analysis can select an optimal candidate data set that brings the most valuable information in predicting mass flow rates at four production wells the mlmc results are validated by the actual measurements of the potential data and similar with the estimation obtained from the standard mc but compared to the standard mc the mlmc greatly reduces the computational costs in the assessment with up to 600 days and 85 cost savings when one processor is used our study is limited to a predetermined choice of potential data so as to allow validating our results against known data and justifying the proposed method in practice one can apply an optimization algorithm to select among alternative candidates for a case with a large number of candidates available a significant computational cost saving from the mlmc can be expected as for each candidate data the mlmc can be applied to improve the computational efficiency though demonstration of our mlmc based data worth analysis has been limited to a subsurface flow model the approach is general enough to apply to a wide range of problems acknowledgments primary support for this work was provided by the scientific discovery through advanced computing scidac program funded by the u s department of energy office of advanced scientific computing research ascr and office of biological and environmental research ber additional support was provided by ber s terrestrial ecosystem science scientific focus area tes sfa project the authors are supported by oak ridge national laboratory which is supported by the office of science of the u s department of energy under contract de ac05 00or22725 
868,water exploitation for energy production from small hydropower plant shp is increasing despite human pressure on freshwater already being very intense in several countries preserving natural rivers thus requires deeper understanding of the global i e ecological and economic efficiency of flow diversion practice in this work we show that the global efficiency of shp river intakes can be improved by non proportional flow redistribution policies this innovative dynamic water allocation defines the fraction of water released to the river as a nonlinear function of river runoff three swiss shp case studies are considered to systematically test the global performance of such policies under both present and future hydroclimatic regimes the environmental efficiency is plotted versus the economic efficiency showing that efficient solutions align along a pareto frontier which is entirely formed by non proportional policies on the contrary other commonly used distribution policies generally lie below the pareto frontier this confirms the existence of better policies based on non proportional redistribution which should be considered in relation to implementation and operational costs our results recommend abandoning static e g constant minimal flow policies in favour of non proportional dynamic ones towards a more sustainable use of the water resource also considering changing hydroclimatic scenarios keywords run of the river hydropower plants environmental benefits water allocation policy dynamic flow releases hydrological alteration 1 introduction small hydropower plants shp are a class of low capacity typically lower than 10 mw energy production power plants often based on either flow diversion from water intakes or run of the river water use concepts whenever there is water diversion from the river and depending on the operational policy a residual flow is generally released downstream the intake in part driven by the fear of a fukushima scenario and in view of limiting carbon emissions from fossil fuel power generation energy production is turning to renewable sources among others shp installations are growing although the installed global i e all power plant types hydropower potential in some countries already exceeds 70 of the feasible potential e g usa and switzerland see fig 1 some other country e g the united kingdoms currently uses less than 60 of its potential indeed due to both economic reasons and limitations of technology sites with lower hydraulic heads or power outputs were not considered as suitable for energy production in the past this offers some interesting development opportunities for the future provided that environmentally friendly solutions are adopted for further exploitation of freshwater resources in this work we show how the global i e economic and environmental performance of flow diversion practice for feeding shps can be improved by engineering a new class of dynamic residual flow policies and will show this on three real shp case studies we focus on shps without significant storage capacity which withdraw water from an intake installed at a specific river transect and return it downstream below the power house fig 2 among shps the latter is the scheme with the highest environmental impact in terms of affected riverine corridor length in the majority of the cases shps also apply residual flow policies set to constant minimal amounts minimum flow release henceforth referred to as mfr politically simple to define mfr policies have no specific ecological basis and their extensive use systematically affected first the morphology and then the ecosystem of river corridors moyle and mount 2007 poff et al 2007 as today s society acknowledges the value of ecosystem services under resource exploitation arthington et al 2006 the classic mfr policy is not sustainable anymore poff et al 2010 hence dynamic environmental flow releases mimicking the natural flow regime variability have recently been suggested as preferable e g basso and botter 2012 perona et al 2013 in order to cope with the ecosystem resilience to perturbations and reduce the risk of critical transitions to different statistical equilibrium states scheffer 2009 scheffer et al 2012 such dynamic redistribution practices called proportional from now on consist of the release of a certain percentage of the total flow to the environment e g 20 30 while exploiting the remaining fraction up to the plant nominal capacity although innovative and beneficial for the environment compared to minimal flow proportional policies suffer from the fact that the percentage of redistribution is by definition independent of the incoming flow carried by the river in order to find more efficient redistribution rules non proportional policies have been proposed gorla and perona 2013 perona et al 2013 and their global efficiency preliminary investigated by gorla 2014 and razurel et al 2016 in contrast to proportional policies the fraction of water released to the environment is defined by a non linear function which depends on the value of the incoming flow the conceptual basis of non proportional redistribution is the paradigm of sustainable development which recognizes the right of applying limited human pressure to the environment arthington et al 2006 hence the more flexible the redistribution rule is the more efficient the use of water by the riverine ecosystem will be in this paper we extend the work of razurel et al 2016 by first improving the description of the ecohydrological indicators second we numerically simulate hundreds of thousands of non proportional policies and show that pareto efficient redistribution rules i e the pareto frontier are indeed made by non proportional policies third we perform a sensitivity analysis on the weight used to compute the ecohydrological indicator we show the results for three swiss case studies also under the effect of changing hydroclimatic scenarios potentially these policies may be successfully applied to any river intake structures which are primarily used to intercept and divert water from the main stream to serve as either a storage reservoir or directly for a human use 2 methodology and data description 2 1 non proportional water allocation policies the problem of defining the optimal water allocation for dammed systems castelletti et al 2007 niayifar and perona 2017 soncini sessa et al 1999 clearly simplifies for water intakes with negligible storage capacity with reference to fig 2 let us assume that the fraction q 1 t of the total incoming flow i t at the intake is delivered to the power house by virtue of the conservation law the difference 1 q 2 t i t q 1 t will be allocated to the riparian ecosystem the environmental utility for using that water has been shown to be indirectly evaluated by the human use benefit function perona et al 2013 the optimal water allocation can be identified by evaluating which redistribution rule maximizes the global i e economic and environmental benefits obtained by assigning q 1 t to the power house and q 2 t to the environment over a reference time frame gorla and perona 2013 with the purpose of systematically exploring a large number of water allocation policies representing both proportional and non proportional redistribution rules razurel et al 2016 introduced a class of nonlinear functions gorla 2014 by modifying the fermi dirac distribution well known in quantum physics lifshitz and landau 1984 other ways could have been used to define the non proportional allocation function but this one has been chosen because it comprises many reasonable redistributions in a simple mathematical function which is also parsimonious in the number of involved parameters thus the fraction of water that is released to the environment is defined by the following equation 2 f x 1 m y exp a x b c j i i with m a a 1 y 1 m exp a b c and a exp a b c exp a 1 b c this function allows the generation of water allocation policies by varying only few parameters i j a b as hereafter described the parameters i and j are used to set the bound of the fermi function the parameter i ranges within 0 1 and represents the fraction of water left in the river at the beginning of the competition i i m i n the parameter j ranges also within 0 1 and correspond to the fraction of the incoming flow rate left in the river at the end of the competition i i m a x non proportional allocation starts for an incoming flow rate i m i n q m f r q m e c where qmfr represents the minimal flow release and qmec is the minimum flow required to activate the turbines below imin all the water goes to the environment initially a fraction i of the dimensionless flow x i i m i n i m a x i m i n above 0 for i i m i n is allocated to the environment as 3 q 2 f x i i m i n q m f r the minimal flow requirement being thus always guaranteed the competition ends at an incoming flow rate i m a x q n q m e c 1 j q m f r q m e c when the nominal power of the turbine is reached at q 1 q n therefore for imin q imax the water is dynamically allocated between the environment and the hydropower plant depending on the value of the incoming flow i at the end of the competition j 1 is the fraction of x left to the environment see also razurel et al 2016 for details beyond imax river discharge exceeding qn is allocated to the environment spilling when i j the model generates proportional repartition rules in this particular case the quantity of water q 2 allocated to the river is a fixed percentage e g 10 20 of the water inflow i in addition to the minimal flow requirement the parameter a allows a variation of the smoothness of the transition between the environmental water allocation i relative to low flows and j relative to high flows see fig 3 in the limit of a very large a one obtains a steep like transition conversely a small a yields a linear interpolation between i and j by varying the parameter b one introduces a change of concavity and controls the position of the inflection point if the change of concavity is outside the interval imin imax one obtains either a convex or a concave function finally the parameter c gives the overall shape of the curve gray curves in fig 3 show a representative sample of feasible non proportional water repartition rules given by eq 2 these were obtained from 36 combinations of a and b while fixing i and j pink curves correspond to the same 36 combinations of a and b but are obtained by inverting i and j 2 2 ecohydrological indicators river rehabilitation often relies on restoring a more natural flow regime bartholow 2010 petts 2009 which suggests that optimal flow releases should be dynamic and show a variability similar to that of the natural flow regime poff et al 1997 we propose to evaluate the environmental performance of the dynamic releases by building a dimensionless synthetic ecohydrological indicator in particular this joins the assessment provided by the indicators of hydrologic alteration proposed by richter et al 1996 with an evaluation of the habitat availability for fish fig 4 other indicators like the hydro morphological index of diversity hmid developed by gostner et al 2013a exist and have already been applied to real case studies gostner et al 2013b their choice is a valid alternative which depend however on river morphological complexity and general data availability the 32 indicators of hydrologic alteration iha proposed by richter et al 1996 are an effective attempt to quantify the variability of the natural flow dynamics and deviations from it for altered flow regimes coherently with this idea we use the ihas to minimize the hydrologic distance in terms of rate of non attainment rna and coefficient of variation cv between natural conditions and the flow regime resulting from every regulation policy as detailed in gorla and perona 2013 we recall here that the rna is defined as the fraction of simulated years in which each iha falls outside a range defined from the natural flow regime for each iha from rna k and cv k we compute the indicators hyd1 sim and hyd2 sim by first intra and subsequently inter groups of arithmetic means of the iha see gorla and perona 2013 razurel et al 2016 for details 4 h y d 1 s i m 1 e r n a s i m k r n a n a t k 2 5 h y d 2 s i m 1 e c v s i m k c v n a t k 2 where k refers to each of the 32 iha in addition to hydrologic alteration habitat availability also plays an important role in species protection this can be assessed by modelling habitat preference curves generally obtained from river surveys and hydraulic measurements bloesch et al 2005 maddock 1999 milhous et al 1984a in the three projects considered in this work surveys were made on the river reaches impacted by reduced flow with phabsim physical habitat simulation milhous et al 1984b fishing being the main ecosystem of interest in our case weighted usable areas wua curves were computed for one dominant fish species the brown trout discriminating between juveniles and adults ecocontrol 2011 2012 2013 this method was chosen according to the available data mainly the hydrograph fig 4 b shows a qualitative example of the preference curve resulting from phabsim method a common practice to define static threshold like qmfr is to define a breaking point intended as significant changes of the wua curve slope and to consider it as the limit above which a further increase in environmental flow is marginally low as this method represents a static concept we improve and extend its use for evaluating dynamic flow releases we assume that fish stress due to inadequate combination of substrate water depth and speed is more relevant when prolonged in time payne 2003 we use the original wua curves reproducing empirical data and the breaking points recommended in the official project reports in order to identify the threshold blue line in fig 4 b eventually we quantify the number of consecutive days the environmental release is below the threshold and use this as a proxy for fish habitat conditions hab1 sim and hab2 sim thus represent the maximal number of consecutive days computed over the whole simulation time characterized by flows under the critical thresholds identified by breakpoints for juveniles and adults respectively such thresholds were fixed equal to 1 2 m3 s for young fish and 0 73 m3 s for adults in buseno 0 50 m3 s for both categories in cauco and 0 55 m3 s for young fish in ponte brolla where impacts on adults were considered as negligible ecocontrol 2011 2012 2013 we then aggregate hyd1 sim and hyd2 sim into two hydrological sub indicators e 1 and e 2 bounded between 0 and 1 as 6 e 1 1 h y d 1 s i m h y d 1 m i n h y d 1 m a x h y d 1 m i n e 2 1 h y d 2 s i m h y d 2 m i n h y d 2 m a x h y d 2 m i n the indicators with subscript min and max correspond to the scenarios having the minimal and maximal impact on the river respectively in this work they correspond to the natural flow regime no impact and to the minimal flow requirement policy similarly we aggregate hab1 sim and hab2 sim into two fish habitat availability sub indicators e 3 and e 4 7 e 3 1 h a b 1 s i m h a b 1 m i n h a b 1 m a x h a b 1 m i n e 4 1 h a b 2 s i m h a b 2 m i n h a b 2 m a x h a b 2 m i n the hydrological indicator hyd is calculated by doing the weighted geometric average of the sub indicators e 1 and e 2 8 h y d e w 1 ln e 1 w 2 ln e 2 where w 1 and w 2 1 w 1 are the weighting factors of e 1 and e 2 the exponential form is used here as a convenient way of representing the weighted geometrical mean the fish habitat indicator hab is calculated by doing the weighted geometric average of the sub indicators e 3 and e 4 9 h a b e w 3 ln e 3 w 4 ln e 4 where w 3 and w 4 1 w 3 are the weighting factors of e 3 and e 4 the indicators hyd and hab are finally aggregated to calculate the dimensionless synthetic ecohydrological indicator eco 10 e c o e w 5 ln h y d w 6 l n h a b where w 5 and w 6 1 w 5 are the weighting factors of hyd and hab weights should be defined case by case on the basis of expert s opinion and considering the status of the specific riparian ecosystem in this work we chose not to express preferences and weighted all the indicators as equally important in all numerical simulations richter et al 1997 1996 however in order to explore how weighting impact the results we performed a sensitivity analysis for the weighting factor w 5 2 3 case studies we chose three small hydropower case studies henceforth denominated buseno cauco and ponte brolla located in southern switzerland whose details are reported in table 1 for the three case studies we compared the effects of the following sub classes of water allocation policies i scenarios mfr 1 and mfr 2 representing traditional minimal flow requirement policies with one or two thresholds the second one is introduced to increase the minimal flow value from april 1st to september 30th respectively q m f r 1 and q m f r 2 defined in table 1 ii dynamic flow releases proportional to i t fixed percentages going from 10 to 50 with a step of 5 iii dynamic flow releases non proportional to i t flow dependent variable percentages as previously described in particular the non proportional water allocation policies were obtained by varying i and j from 0 02 to 0 70 with 0 01 increment a from 2 to 8 with step equal to 2 b from 0 to 1 with step 1 8 and considering c constant and equal to 1 for a total of 168912 considered alternatives the minimal flow requirement q m f r 1 was enforced by law and was therefore always guaranteed for each simulated scenarios we used 29 years of streamflow data measured by the swiss federal office for the environment as natural inflows i t to evaluate scenarios in the period 1983 2011 for cauco and ponte brolla power plant locations along the river are not the same as the locations from which the historic flow series have been obtained we therefore transposed streamflows measured at buseno https www hydrodaten admin ch fr 2474 html and bignasco https www hydrodaten admin ch fr 2475 html gauging stations using a surface ratio by rescaling them to the respective catchment areas brutsaert 2005 dingman and dingman 1994 the dependence of hydropower production b 1 on river discharge q 1 was approximated by a 2nd degree polynomial equation b 1 m q 1 2 p q 1 q with m p and q depending on each plant turbine and associated to a fitting law showing a fitting correlation coefficient r 2 larger than 0 9 see gorla 2014 for details 2 4 climate change impact on streamflow the effect of climatic changes on water availability for the the periods 2020 49 and 2070 99 has been obtained by considering the emission rcp 6 0 scenario flato et al 2013 which has been extensively applied to project future climate in several alpine regions of switzerland in brief this scenario foresees by the end of the century a mean global increase of earth surface temperature of about 2 8 c during summer with a possible range of 1 7 to 4 5 c in alpine swiss cantons the expected winter temperature variations are approximately 2 c smaller the projected precipitation regime is even more uncertain given the present inherent stochasticity of the phenomenon brönnimann et al 2014 overall streamflows are expected to increase in magnitude in the period 2020 2049 due to the melting and shrinking of alpine glaciers this scenario will progressively move to a nivo pluvial flow regime in the period 2070 2099 characterized by higher flows during late winter early spring time those changes are shown in fig 5 a recent report job et al 2011 describes the evolution of the gornera basin located in southern switzerland near the considered catchments in response to such changes and to stored ice and snow in the basin we considered this scenario as representative for the three basins chosen and based on that we generated time series of daily streamflow expected for the periods 2020 2049 and 2070 2099 for each each basin e g see gorla 2014 2 5 development of a graphical user interface and numerical simulations a graphical user interface gui fig 2 has been developed using the software matlab to facilitate the data treatment and the selection of the optimal water allocation functions among the different scenarios non proportional proportional and mfrs repartition rules for each scenario the energy production and the ecohydrological indicators were computed based on the generated flows as a result the efficiency graph showing the mean annual energy produced during the analyzed period versus the ecohydrological indicator was plotted the pareto front representing the ensemble of optimal water allocation scenarios was identified and enhanced with a red line in the efficiency plot more details are provided in appendix a 3 results 3 1 efficiency plot and selection of optimal scenarios fig 6 shows the performances of buseno hydropower plant in terms of efficiency plot for all the 168912 water repartition rules obtained from eq 2 each gray and pink point of the efficiency plot corresponds to a non proportional repartition policy and can thus be compared to more classic scenarios e g based on minimal flow requirement and proportional water allocation policies as expected scenario mfr 1 has the highest hydropower production and the lowest environmental performance the scenario mfr 2 in buseno in which the minimal release is increased from april 1st to september 30th to a second fixed threshold shows a reduction of hydropower production by 3 4 and an increase of ecohydrological indicators by 2 5 with respect to the performances of mfr 1 this scenario may be improved by applying proportional repartition rules among these the one that leaves 10 of the incoming flow to the environment preserves the energy production of scenario mfr 2 while increasing the ecohydrological benefits by 4 7 however the benefits obtained with the 10 proportional rule can still be improved by moving vertically or horizontally toward the pareto frontier enhancing the ecohydrological indicators and the energy produced respectively a notable result is that the pareto frontier is entirely composed by non proportional repartition rules henceforth referred to as efficient it is worth recalling here that at the pareto frontier it is not possible to improve a scenario by making an indicator better without making another one worse for this power plant changing a proportional repartition rule with an efficient one i e that lies on the pareto frontier causes a 5 hydropower production average improvement and a 3 improvement for the ecohydrological indicators these percentages were obtained with reference to fig 6 by moving vertically and horizontally from proportional alternatives towards points located on the pareto frontier similar results are obtained for cauco power plant but not for the one in ponte brolla as shown in the left hand side panels of fig 7 for the latter case proportional repartition rules perform already well and the ecohydrological indicator resulting from the simulated alternatives is already high thus making the improvement almost negligible the potential improvement of using efficient non proportional distribution to replace proportional distribution is between 0 0 and 0 1 this is mainly due to the fact that in ponte brolla habitat thresholds the blue line shown in fig 4 b turned out to be lower than qmfr because of the particular canyoning morphology of the regulated reach where a minimal flow release also guarantees fish survival consequently among the indicators mainly the hydrologic one i e hyd concurred to the definition of the global ecohydrological indicator eco this result is consistent with that shown by the sensitivity analysis performed while changing the weights used to build the ecohydrological indicator shown ahead that is results similar to ponte brolla power plant can be obtained for both cauco and buseno in the limit of non considering the fish habitat availability a backwards control on sub indicators and fermi s functions see e g subplots in fig 6 should also be done case by case on the basis of experts opinions in order to check the soundness of interesting alternatives 3 2 climate change scenarios our study shows that a general increase in hydropower production is foreseen for the periods 2020 2049 and 2070 2099 for all the three basins fig 7 this right shift toward higher energy production of the efficiency plot can be explained by an increase of streamflow from 2020 to 2049 and a seasonal temporal shift of water availability in the period 2070 2099 as predicted by climate models fig 5 while the aftermath of glacier melting in 2020 2049 is obvious as far as energy production is concerned the effects of higher winter and spring precipitation expected in the second three decades requires an explanation the latter regime sees a flattening of the current river hydrograph with a strong reduction of the summer maximum as a consequence of such redistribution of water availability during the year the number of days when turbines can be activated will increase as the flow necessary for the turbine to operate qmec will be reached more often the impact of climate change on the number of possible operation hours at qn per year is more uncertain especially if no storage is available the ecological effects of regulation under climate change are complex and must be analyzed case by case while an exception can be made for ponte brolla where river morphology always guarantees good habitat availability even under low flow mfr scenario both buseno and cauco will see a worsening of both the proportional and constant flow release policies with respect to non proportional ones table 2 presents the average improvements obtained by moving from proportional to efficient non proportional repartitions located on the pareto frontier for the three case studies and the three time periods the results show that gains can be obtained through the use of optimal allocation rules for the three case studies for buseno the potential gain in ecohydrological indicator goes from 1 8 for the period 1983 2012 to 4 6 for the period 2020 2049 the foreseen amelioration of the energy production is around 2 for the three considered periods the most important results concerning the ecohydrological indicator are those obtained for cauco indeed the foreseen amelioration of the ecohydrological indicator goes from 8 6 for the period 1983 2012 to 22 8 for the period 2070 2099 however the potential gain in energy production is around 1 which is lower than the two other case studies on average ponte brolla shows the lowest gain in ecohydrological indicator less than 1 but the improvement of the energy production for the periods 1983 2012 and 2020 2049 are close to buseno these scenarios are valid assuming that even though the morphology of single river banks is dynamic average fish habitat conditions in a river reach will not change over the considered time horizon 4 discussion 4 1 role of ecohydrological indicator and sensitivity analysis fig 8 shows the results of the sensitivity analysis performed for the three case studies a buseno b cauco and c ponte brolla for each of the three plots the two weighting factors w 1 and w 3 were set to 0 5 while the third factor w 5 was progressively increased from 0 to 1 with a step of 0 001 thus the only parameter that was changed is the weighting of the hydrological indicator hyd and the fish habitat indicator hab to compute the final ecohydrological indicator eco for each combination of factors a new efficiency plot is computed the corresponding average amelioration in both ecohydrological indicator and energy production when replacing proportional rules by non proportional ones were thus calculated and shown on the y axis of the plot notably the sensitivity analysis shows some different results depending on the case study as far as buseno fig 8 a is concerned the average improvement of the ecohydrological indicator red curve with respect to proportional policies is decreasing when the weighting of the hydrological indicator is bigger than the habitat one i e more weight is given to the hydrological indicator the gain of energy production blue curve starts decreasing when w 5 is above 0 6 this shows that giving a superior weight to the hydrological indicator leads to a reduction in the power production gain for cauco fig 8 b the same tendency is observed for the environmental gain however the variation of the power production as a function of the weighting factor w 5 shows some fluctuations in contrast to buseno no clear tendency is observed the results for ponte brolla fig 8 c are different and the improvements of the power production and the ecohydrological indicator are constant independently of the value of w 5 this is explained by the fact that for this specific case the minimal flow release mfr is always greater than the value of the threshold defined to calculate the fish habitat indicator thus the indicator hab is always set to the constant maximum value the order of magnitude of the power production gain is comparable to the other stations but the environmental gain is lower the absolute value of the ecohydrological indicator has to be interpreted carefully since there is no other previous study applying the same methodology to combine the hydrological and fish habitat suitability indicators the indicator has been built to evaluate how far from the natural series each scenario is a value of 1 corresponding to the natural condition thus we are more interested in the comparison of the different allocation scenarios and the results we are showing are more focused on the relative gain that may be obtained by using non proportional policies we show a method to choose the optimal distribution functions by comparing all the possible distribution methods the sub indicators have been chosen according to the available data being mainly the natural hydrograph and the characteristics of the power plant but may be improved if more data are available the allocation rules we are presenting in the paper non proportional have not been implemented yet so there are no empirical data available that allows a comparison between the pre impact and post impact systems 4 2 general considerations and recommendations managing water resources to their maximal extent in alpine countries will necessarily force people to be aware that each unit of energy is generated at some expense of the ecology of the riverine ecosystem as a consequence all the feasible measures to improve in efficiency should be taken into consideration together with implementation costs some costs are very much country dependent and this aspect is not addressed in this work being beyond the scope of the work however the implementation costs for generating dynamic flow releases are worth a few comments this work showed that gains in hydropower production and ecohydrological indicator could be made on average by replacing proportional water allocation policies today s best practice though not yet widespread with non proportional ones located on the pareto frontier table 2 improving both criteria such increments must be considered as actual win win solutions these results are based on testing non proportional redistribution rules on only three homogeneous shp case studies limited to the swiss environment and its socio economic context we showed that the potential improvement lies in the wider range of non proportional repartition rules with respect to traditional policies moreover fig 6 demonstrates how classic minimal flow requirement approaches mfr 1 and mfr 2 can be improved mainly in term of ecohydrological benefit by applying non proportional policies even more than by applying proportional ones both dynamic considering the environment as an independent water user perona et al 2013 with specific needs and features is thus the key to obtaining efficient environmental flow releases such rules will generally result in being non proportional and flow dependent in fact while the efficiency curve of a turbine does not change throughout the year the environmental use of water follows seasonal trends this could easily be added in the model and weighted case by case when specific ecological information is available increasing the number of case studies would statistically strengthen the results and suggest more general rules to understand which power plants can actually be improved in global performances this can be challenging to show particularly because data are often not easily available in this work we decided to express the economical indicator as the energy production in gwh this study focuses on small hydropower plants without storage hence this suggests that the optimal strategy would be to always turbine the water diverted according to the chosen allocation rule however a further improvement would consist in considering the variability of the electricity market price this could be made by changing the dimensionless variable x of the fermi function eq 3 so it does not depend only on the flow rate but also on the market price thereby the value of the produced hydropower production would be optimised pereira cardenal et al 2016 energy provision from renewable sources is a sign of human being responsibility which however requires a strong harmonisation among social economic and political parts the question of how to implement non proportional flow release rules has not been addressed in this work however our present research started to address this problem particularly looking at suitable hydraulic infrastructures that may generate fermi function redistribution rules at zero energy costs bernhard and perona 2017 this is highly desirable in order to pursue innovation not only from an intelligent technological infrastructure point of view but also from a sustainable one 5 conclusions this work shows a simple and innovative numerical approach for defining sustainable and efficient environmental flow releases in river reaches of shp without storage the method has been tested on real data and constraints and could be adopted as a prompt answer to the actual need to conciliate environmental protection and growth of hydropower production a convenient class of functions developed by gorla 2014 and razurel et al 2016 was here comprehensively tested as a practical tool for exploring a representative sample of dynamic flow releases such functions provide a direct link between the practice of comparing different environmental flow policies in particular those using fixed percentages of the incoming flows proportional and those with variable splits between diverted and released flows non proportional the pareto frontier is obtained from the simulated alternatives for each case study and it shows that non proportional rules are generally more efficient than traditional ones both proportional and static it was shown that when applying efficient non proportional repartition rules for regulating the run of the river hydropower plants ameliorations in hydropower and ecohydrological performances can be attained with respect to proportional policies although the three case studies are located in switzerland the results vary from one case to another leading to the conclusion that they depend on the river morphology indeed the canyoning morphology in the case of ponte brolla implies that the mfr value is always higher than the threshold given by the wua curve which results in a maximum value for the fish habitat suitability indicator for cauco the foreseen amelioration for the ecohydrological indicator is the most important it goes from 8 6 for the period 1983 2012 to 22 8 for the period 2070 2099 but the gain in energy production is the lowest around 1 in comparison to the two other case studies buseno and ponte brolla show some similar potential gains in energy production around 2 but for the latter the ecohydrological improvement is almost irrelevant between 0 0 and 0 1 author contribution lorenzo gorla and pierre razurel contributed equally to this work acknowledgements we thank the swiss national science foundation for funding the project remedy grant no pp00p2153028 1 as well as renato gaggini of ecocontrol sa for openly discussing practical details this work was written whilst pp visited as academic guest of the group of climatology at the institute of geography of the university of bern the anonymous reviewers whose comments helped to improve the quality of manuscript are greatly acknowledged appendix a graphical user interface gui fig 2 has been developed using the software matlab to facilitate the data treatment and the selection of the optimal water allocation functions among the different scenarios non proportional proportional and mfrs repartition rules this tool takes the natural river hydrograph and the hydropower plant features efficiency function design flow etc as inputs the desired water allocation policies as well as the ecological threshold can also be set the user friendly architecture of the gui freely available to any user that wants to reservedly test the performances of his own cases 1 1 free download from http www sccer soe ch research hydropower task2 4 or by simply contacting the authors pr pp makes the model particularly suitable for stakeholder planning for water managers operations or for academic purposes numerical simulations were performed in order to model the different allocation functions the natural daily flow i t was redistributed between the hydropower plant and the river by simulating eqs 1 3 according to the selected fermi function and for the entire time series of i t for each scenario the energy production and the ecohydrological indicators were computed based on the generated flows q 1 and q 2 respectively the same procedure was repeated for the whole set of selected fermi function parameters as well as for the proportional and mfrs repartition rules as a result the efficiency graph showing the mean annual energy produced during the analyzed period versus the ecohydrological indicator was plotted the pareto front representing the ensemble of optimal water allocation scenarios was identified and enhanced with a red line in the efficiency plot the simulations to asses the impact of the climate change have been performed in the same way for the three case studies i e buseno cauco and ponte brolla the time series of daily streamflow for the three different time periods i e 2000 2050 and 2100 have been generated from the current natural data series by applying the trend of the rcp 6 0 scenario described in the previous section 2 4 
868,water exploitation for energy production from small hydropower plant shp is increasing despite human pressure on freshwater already being very intense in several countries preserving natural rivers thus requires deeper understanding of the global i e ecological and economic efficiency of flow diversion practice in this work we show that the global efficiency of shp river intakes can be improved by non proportional flow redistribution policies this innovative dynamic water allocation defines the fraction of water released to the river as a nonlinear function of river runoff three swiss shp case studies are considered to systematically test the global performance of such policies under both present and future hydroclimatic regimes the environmental efficiency is plotted versus the economic efficiency showing that efficient solutions align along a pareto frontier which is entirely formed by non proportional policies on the contrary other commonly used distribution policies generally lie below the pareto frontier this confirms the existence of better policies based on non proportional redistribution which should be considered in relation to implementation and operational costs our results recommend abandoning static e g constant minimal flow policies in favour of non proportional dynamic ones towards a more sustainable use of the water resource also considering changing hydroclimatic scenarios keywords run of the river hydropower plants environmental benefits water allocation policy dynamic flow releases hydrological alteration 1 introduction small hydropower plants shp are a class of low capacity typically lower than 10 mw energy production power plants often based on either flow diversion from water intakes or run of the river water use concepts whenever there is water diversion from the river and depending on the operational policy a residual flow is generally released downstream the intake in part driven by the fear of a fukushima scenario and in view of limiting carbon emissions from fossil fuel power generation energy production is turning to renewable sources among others shp installations are growing although the installed global i e all power plant types hydropower potential in some countries already exceeds 70 of the feasible potential e g usa and switzerland see fig 1 some other country e g the united kingdoms currently uses less than 60 of its potential indeed due to both economic reasons and limitations of technology sites with lower hydraulic heads or power outputs were not considered as suitable for energy production in the past this offers some interesting development opportunities for the future provided that environmentally friendly solutions are adopted for further exploitation of freshwater resources in this work we show how the global i e economic and environmental performance of flow diversion practice for feeding shps can be improved by engineering a new class of dynamic residual flow policies and will show this on three real shp case studies we focus on shps without significant storage capacity which withdraw water from an intake installed at a specific river transect and return it downstream below the power house fig 2 among shps the latter is the scheme with the highest environmental impact in terms of affected riverine corridor length in the majority of the cases shps also apply residual flow policies set to constant minimal amounts minimum flow release henceforth referred to as mfr politically simple to define mfr policies have no specific ecological basis and their extensive use systematically affected first the morphology and then the ecosystem of river corridors moyle and mount 2007 poff et al 2007 as today s society acknowledges the value of ecosystem services under resource exploitation arthington et al 2006 the classic mfr policy is not sustainable anymore poff et al 2010 hence dynamic environmental flow releases mimicking the natural flow regime variability have recently been suggested as preferable e g basso and botter 2012 perona et al 2013 in order to cope with the ecosystem resilience to perturbations and reduce the risk of critical transitions to different statistical equilibrium states scheffer 2009 scheffer et al 2012 such dynamic redistribution practices called proportional from now on consist of the release of a certain percentage of the total flow to the environment e g 20 30 while exploiting the remaining fraction up to the plant nominal capacity although innovative and beneficial for the environment compared to minimal flow proportional policies suffer from the fact that the percentage of redistribution is by definition independent of the incoming flow carried by the river in order to find more efficient redistribution rules non proportional policies have been proposed gorla and perona 2013 perona et al 2013 and their global efficiency preliminary investigated by gorla 2014 and razurel et al 2016 in contrast to proportional policies the fraction of water released to the environment is defined by a non linear function which depends on the value of the incoming flow the conceptual basis of non proportional redistribution is the paradigm of sustainable development which recognizes the right of applying limited human pressure to the environment arthington et al 2006 hence the more flexible the redistribution rule is the more efficient the use of water by the riverine ecosystem will be in this paper we extend the work of razurel et al 2016 by first improving the description of the ecohydrological indicators second we numerically simulate hundreds of thousands of non proportional policies and show that pareto efficient redistribution rules i e the pareto frontier are indeed made by non proportional policies third we perform a sensitivity analysis on the weight used to compute the ecohydrological indicator we show the results for three swiss case studies also under the effect of changing hydroclimatic scenarios potentially these policies may be successfully applied to any river intake structures which are primarily used to intercept and divert water from the main stream to serve as either a storage reservoir or directly for a human use 2 methodology and data description 2 1 non proportional water allocation policies the problem of defining the optimal water allocation for dammed systems castelletti et al 2007 niayifar and perona 2017 soncini sessa et al 1999 clearly simplifies for water intakes with negligible storage capacity with reference to fig 2 let us assume that the fraction q 1 t of the total incoming flow i t at the intake is delivered to the power house by virtue of the conservation law the difference 1 q 2 t i t q 1 t will be allocated to the riparian ecosystem the environmental utility for using that water has been shown to be indirectly evaluated by the human use benefit function perona et al 2013 the optimal water allocation can be identified by evaluating which redistribution rule maximizes the global i e economic and environmental benefits obtained by assigning q 1 t to the power house and q 2 t to the environment over a reference time frame gorla and perona 2013 with the purpose of systematically exploring a large number of water allocation policies representing both proportional and non proportional redistribution rules razurel et al 2016 introduced a class of nonlinear functions gorla 2014 by modifying the fermi dirac distribution well known in quantum physics lifshitz and landau 1984 other ways could have been used to define the non proportional allocation function but this one has been chosen because it comprises many reasonable redistributions in a simple mathematical function which is also parsimonious in the number of involved parameters thus the fraction of water that is released to the environment is defined by the following equation 2 f x 1 m y exp a x b c j i i with m a a 1 y 1 m exp a b c and a exp a b c exp a 1 b c this function allows the generation of water allocation policies by varying only few parameters i j a b as hereafter described the parameters i and j are used to set the bound of the fermi function the parameter i ranges within 0 1 and represents the fraction of water left in the river at the beginning of the competition i i m i n the parameter j ranges also within 0 1 and correspond to the fraction of the incoming flow rate left in the river at the end of the competition i i m a x non proportional allocation starts for an incoming flow rate i m i n q m f r q m e c where qmfr represents the minimal flow release and qmec is the minimum flow required to activate the turbines below imin all the water goes to the environment initially a fraction i of the dimensionless flow x i i m i n i m a x i m i n above 0 for i i m i n is allocated to the environment as 3 q 2 f x i i m i n q m f r the minimal flow requirement being thus always guaranteed the competition ends at an incoming flow rate i m a x q n q m e c 1 j q m f r q m e c when the nominal power of the turbine is reached at q 1 q n therefore for imin q imax the water is dynamically allocated between the environment and the hydropower plant depending on the value of the incoming flow i at the end of the competition j 1 is the fraction of x left to the environment see also razurel et al 2016 for details beyond imax river discharge exceeding qn is allocated to the environment spilling when i j the model generates proportional repartition rules in this particular case the quantity of water q 2 allocated to the river is a fixed percentage e g 10 20 of the water inflow i in addition to the minimal flow requirement the parameter a allows a variation of the smoothness of the transition between the environmental water allocation i relative to low flows and j relative to high flows see fig 3 in the limit of a very large a one obtains a steep like transition conversely a small a yields a linear interpolation between i and j by varying the parameter b one introduces a change of concavity and controls the position of the inflection point if the change of concavity is outside the interval imin imax one obtains either a convex or a concave function finally the parameter c gives the overall shape of the curve gray curves in fig 3 show a representative sample of feasible non proportional water repartition rules given by eq 2 these were obtained from 36 combinations of a and b while fixing i and j pink curves correspond to the same 36 combinations of a and b but are obtained by inverting i and j 2 2 ecohydrological indicators river rehabilitation often relies on restoring a more natural flow regime bartholow 2010 petts 2009 which suggests that optimal flow releases should be dynamic and show a variability similar to that of the natural flow regime poff et al 1997 we propose to evaluate the environmental performance of the dynamic releases by building a dimensionless synthetic ecohydrological indicator in particular this joins the assessment provided by the indicators of hydrologic alteration proposed by richter et al 1996 with an evaluation of the habitat availability for fish fig 4 other indicators like the hydro morphological index of diversity hmid developed by gostner et al 2013a exist and have already been applied to real case studies gostner et al 2013b their choice is a valid alternative which depend however on river morphological complexity and general data availability the 32 indicators of hydrologic alteration iha proposed by richter et al 1996 are an effective attempt to quantify the variability of the natural flow dynamics and deviations from it for altered flow regimes coherently with this idea we use the ihas to minimize the hydrologic distance in terms of rate of non attainment rna and coefficient of variation cv between natural conditions and the flow regime resulting from every regulation policy as detailed in gorla and perona 2013 we recall here that the rna is defined as the fraction of simulated years in which each iha falls outside a range defined from the natural flow regime for each iha from rna k and cv k we compute the indicators hyd1 sim and hyd2 sim by first intra and subsequently inter groups of arithmetic means of the iha see gorla and perona 2013 razurel et al 2016 for details 4 h y d 1 s i m 1 e r n a s i m k r n a n a t k 2 5 h y d 2 s i m 1 e c v s i m k c v n a t k 2 where k refers to each of the 32 iha in addition to hydrologic alteration habitat availability also plays an important role in species protection this can be assessed by modelling habitat preference curves generally obtained from river surveys and hydraulic measurements bloesch et al 2005 maddock 1999 milhous et al 1984a in the three projects considered in this work surveys were made on the river reaches impacted by reduced flow with phabsim physical habitat simulation milhous et al 1984b fishing being the main ecosystem of interest in our case weighted usable areas wua curves were computed for one dominant fish species the brown trout discriminating between juveniles and adults ecocontrol 2011 2012 2013 this method was chosen according to the available data mainly the hydrograph fig 4 b shows a qualitative example of the preference curve resulting from phabsim method a common practice to define static threshold like qmfr is to define a breaking point intended as significant changes of the wua curve slope and to consider it as the limit above which a further increase in environmental flow is marginally low as this method represents a static concept we improve and extend its use for evaluating dynamic flow releases we assume that fish stress due to inadequate combination of substrate water depth and speed is more relevant when prolonged in time payne 2003 we use the original wua curves reproducing empirical data and the breaking points recommended in the official project reports in order to identify the threshold blue line in fig 4 b eventually we quantify the number of consecutive days the environmental release is below the threshold and use this as a proxy for fish habitat conditions hab1 sim and hab2 sim thus represent the maximal number of consecutive days computed over the whole simulation time characterized by flows under the critical thresholds identified by breakpoints for juveniles and adults respectively such thresholds were fixed equal to 1 2 m3 s for young fish and 0 73 m3 s for adults in buseno 0 50 m3 s for both categories in cauco and 0 55 m3 s for young fish in ponte brolla where impacts on adults were considered as negligible ecocontrol 2011 2012 2013 we then aggregate hyd1 sim and hyd2 sim into two hydrological sub indicators e 1 and e 2 bounded between 0 and 1 as 6 e 1 1 h y d 1 s i m h y d 1 m i n h y d 1 m a x h y d 1 m i n e 2 1 h y d 2 s i m h y d 2 m i n h y d 2 m a x h y d 2 m i n the indicators with subscript min and max correspond to the scenarios having the minimal and maximal impact on the river respectively in this work they correspond to the natural flow regime no impact and to the minimal flow requirement policy similarly we aggregate hab1 sim and hab2 sim into two fish habitat availability sub indicators e 3 and e 4 7 e 3 1 h a b 1 s i m h a b 1 m i n h a b 1 m a x h a b 1 m i n e 4 1 h a b 2 s i m h a b 2 m i n h a b 2 m a x h a b 2 m i n the hydrological indicator hyd is calculated by doing the weighted geometric average of the sub indicators e 1 and e 2 8 h y d e w 1 ln e 1 w 2 ln e 2 where w 1 and w 2 1 w 1 are the weighting factors of e 1 and e 2 the exponential form is used here as a convenient way of representing the weighted geometrical mean the fish habitat indicator hab is calculated by doing the weighted geometric average of the sub indicators e 3 and e 4 9 h a b e w 3 ln e 3 w 4 ln e 4 where w 3 and w 4 1 w 3 are the weighting factors of e 3 and e 4 the indicators hyd and hab are finally aggregated to calculate the dimensionless synthetic ecohydrological indicator eco 10 e c o e w 5 ln h y d w 6 l n h a b where w 5 and w 6 1 w 5 are the weighting factors of hyd and hab weights should be defined case by case on the basis of expert s opinion and considering the status of the specific riparian ecosystem in this work we chose not to express preferences and weighted all the indicators as equally important in all numerical simulations richter et al 1997 1996 however in order to explore how weighting impact the results we performed a sensitivity analysis for the weighting factor w 5 2 3 case studies we chose three small hydropower case studies henceforth denominated buseno cauco and ponte brolla located in southern switzerland whose details are reported in table 1 for the three case studies we compared the effects of the following sub classes of water allocation policies i scenarios mfr 1 and mfr 2 representing traditional minimal flow requirement policies with one or two thresholds the second one is introduced to increase the minimal flow value from april 1st to september 30th respectively q m f r 1 and q m f r 2 defined in table 1 ii dynamic flow releases proportional to i t fixed percentages going from 10 to 50 with a step of 5 iii dynamic flow releases non proportional to i t flow dependent variable percentages as previously described in particular the non proportional water allocation policies were obtained by varying i and j from 0 02 to 0 70 with 0 01 increment a from 2 to 8 with step equal to 2 b from 0 to 1 with step 1 8 and considering c constant and equal to 1 for a total of 168912 considered alternatives the minimal flow requirement q m f r 1 was enforced by law and was therefore always guaranteed for each simulated scenarios we used 29 years of streamflow data measured by the swiss federal office for the environment as natural inflows i t to evaluate scenarios in the period 1983 2011 for cauco and ponte brolla power plant locations along the river are not the same as the locations from which the historic flow series have been obtained we therefore transposed streamflows measured at buseno https www hydrodaten admin ch fr 2474 html and bignasco https www hydrodaten admin ch fr 2475 html gauging stations using a surface ratio by rescaling them to the respective catchment areas brutsaert 2005 dingman and dingman 1994 the dependence of hydropower production b 1 on river discharge q 1 was approximated by a 2nd degree polynomial equation b 1 m q 1 2 p q 1 q with m p and q depending on each plant turbine and associated to a fitting law showing a fitting correlation coefficient r 2 larger than 0 9 see gorla 2014 for details 2 4 climate change impact on streamflow the effect of climatic changes on water availability for the the periods 2020 49 and 2070 99 has been obtained by considering the emission rcp 6 0 scenario flato et al 2013 which has been extensively applied to project future climate in several alpine regions of switzerland in brief this scenario foresees by the end of the century a mean global increase of earth surface temperature of about 2 8 c during summer with a possible range of 1 7 to 4 5 c in alpine swiss cantons the expected winter temperature variations are approximately 2 c smaller the projected precipitation regime is even more uncertain given the present inherent stochasticity of the phenomenon brönnimann et al 2014 overall streamflows are expected to increase in magnitude in the period 2020 2049 due to the melting and shrinking of alpine glaciers this scenario will progressively move to a nivo pluvial flow regime in the period 2070 2099 characterized by higher flows during late winter early spring time those changes are shown in fig 5 a recent report job et al 2011 describes the evolution of the gornera basin located in southern switzerland near the considered catchments in response to such changes and to stored ice and snow in the basin we considered this scenario as representative for the three basins chosen and based on that we generated time series of daily streamflow expected for the periods 2020 2049 and 2070 2099 for each each basin e g see gorla 2014 2 5 development of a graphical user interface and numerical simulations a graphical user interface gui fig 2 has been developed using the software matlab to facilitate the data treatment and the selection of the optimal water allocation functions among the different scenarios non proportional proportional and mfrs repartition rules for each scenario the energy production and the ecohydrological indicators were computed based on the generated flows as a result the efficiency graph showing the mean annual energy produced during the analyzed period versus the ecohydrological indicator was plotted the pareto front representing the ensemble of optimal water allocation scenarios was identified and enhanced with a red line in the efficiency plot more details are provided in appendix a 3 results 3 1 efficiency plot and selection of optimal scenarios fig 6 shows the performances of buseno hydropower plant in terms of efficiency plot for all the 168912 water repartition rules obtained from eq 2 each gray and pink point of the efficiency plot corresponds to a non proportional repartition policy and can thus be compared to more classic scenarios e g based on minimal flow requirement and proportional water allocation policies as expected scenario mfr 1 has the highest hydropower production and the lowest environmental performance the scenario mfr 2 in buseno in which the minimal release is increased from april 1st to september 30th to a second fixed threshold shows a reduction of hydropower production by 3 4 and an increase of ecohydrological indicators by 2 5 with respect to the performances of mfr 1 this scenario may be improved by applying proportional repartition rules among these the one that leaves 10 of the incoming flow to the environment preserves the energy production of scenario mfr 2 while increasing the ecohydrological benefits by 4 7 however the benefits obtained with the 10 proportional rule can still be improved by moving vertically or horizontally toward the pareto frontier enhancing the ecohydrological indicators and the energy produced respectively a notable result is that the pareto frontier is entirely composed by non proportional repartition rules henceforth referred to as efficient it is worth recalling here that at the pareto frontier it is not possible to improve a scenario by making an indicator better without making another one worse for this power plant changing a proportional repartition rule with an efficient one i e that lies on the pareto frontier causes a 5 hydropower production average improvement and a 3 improvement for the ecohydrological indicators these percentages were obtained with reference to fig 6 by moving vertically and horizontally from proportional alternatives towards points located on the pareto frontier similar results are obtained for cauco power plant but not for the one in ponte brolla as shown in the left hand side panels of fig 7 for the latter case proportional repartition rules perform already well and the ecohydrological indicator resulting from the simulated alternatives is already high thus making the improvement almost negligible the potential improvement of using efficient non proportional distribution to replace proportional distribution is between 0 0 and 0 1 this is mainly due to the fact that in ponte brolla habitat thresholds the blue line shown in fig 4 b turned out to be lower than qmfr because of the particular canyoning morphology of the regulated reach where a minimal flow release also guarantees fish survival consequently among the indicators mainly the hydrologic one i e hyd concurred to the definition of the global ecohydrological indicator eco this result is consistent with that shown by the sensitivity analysis performed while changing the weights used to build the ecohydrological indicator shown ahead that is results similar to ponte brolla power plant can be obtained for both cauco and buseno in the limit of non considering the fish habitat availability a backwards control on sub indicators and fermi s functions see e g subplots in fig 6 should also be done case by case on the basis of experts opinions in order to check the soundness of interesting alternatives 3 2 climate change scenarios our study shows that a general increase in hydropower production is foreseen for the periods 2020 2049 and 2070 2099 for all the three basins fig 7 this right shift toward higher energy production of the efficiency plot can be explained by an increase of streamflow from 2020 to 2049 and a seasonal temporal shift of water availability in the period 2070 2099 as predicted by climate models fig 5 while the aftermath of glacier melting in 2020 2049 is obvious as far as energy production is concerned the effects of higher winter and spring precipitation expected in the second three decades requires an explanation the latter regime sees a flattening of the current river hydrograph with a strong reduction of the summer maximum as a consequence of such redistribution of water availability during the year the number of days when turbines can be activated will increase as the flow necessary for the turbine to operate qmec will be reached more often the impact of climate change on the number of possible operation hours at qn per year is more uncertain especially if no storage is available the ecological effects of regulation under climate change are complex and must be analyzed case by case while an exception can be made for ponte brolla where river morphology always guarantees good habitat availability even under low flow mfr scenario both buseno and cauco will see a worsening of both the proportional and constant flow release policies with respect to non proportional ones table 2 presents the average improvements obtained by moving from proportional to efficient non proportional repartitions located on the pareto frontier for the three case studies and the three time periods the results show that gains can be obtained through the use of optimal allocation rules for the three case studies for buseno the potential gain in ecohydrological indicator goes from 1 8 for the period 1983 2012 to 4 6 for the period 2020 2049 the foreseen amelioration of the energy production is around 2 for the three considered periods the most important results concerning the ecohydrological indicator are those obtained for cauco indeed the foreseen amelioration of the ecohydrological indicator goes from 8 6 for the period 1983 2012 to 22 8 for the period 2070 2099 however the potential gain in energy production is around 1 which is lower than the two other case studies on average ponte brolla shows the lowest gain in ecohydrological indicator less than 1 but the improvement of the energy production for the periods 1983 2012 and 2020 2049 are close to buseno these scenarios are valid assuming that even though the morphology of single river banks is dynamic average fish habitat conditions in a river reach will not change over the considered time horizon 4 discussion 4 1 role of ecohydrological indicator and sensitivity analysis fig 8 shows the results of the sensitivity analysis performed for the three case studies a buseno b cauco and c ponte brolla for each of the three plots the two weighting factors w 1 and w 3 were set to 0 5 while the third factor w 5 was progressively increased from 0 to 1 with a step of 0 001 thus the only parameter that was changed is the weighting of the hydrological indicator hyd and the fish habitat indicator hab to compute the final ecohydrological indicator eco for each combination of factors a new efficiency plot is computed the corresponding average amelioration in both ecohydrological indicator and energy production when replacing proportional rules by non proportional ones were thus calculated and shown on the y axis of the plot notably the sensitivity analysis shows some different results depending on the case study as far as buseno fig 8 a is concerned the average improvement of the ecohydrological indicator red curve with respect to proportional policies is decreasing when the weighting of the hydrological indicator is bigger than the habitat one i e more weight is given to the hydrological indicator the gain of energy production blue curve starts decreasing when w 5 is above 0 6 this shows that giving a superior weight to the hydrological indicator leads to a reduction in the power production gain for cauco fig 8 b the same tendency is observed for the environmental gain however the variation of the power production as a function of the weighting factor w 5 shows some fluctuations in contrast to buseno no clear tendency is observed the results for ponte brolla fig 8 c are different and the improvements of the power production and the ecohydrological indicator are constant independently of the value of w 5 this is explained by the fact that for this specific case the minimal flow release mfr is always greater than the value of the threshold defined to calculate the fish habitat indicator thus the indicator hab is always set to the constant maximum value the order of magnitude of the power production gain is comparable to the other stations but the environmental gain is lower the absolute value of the ecohydrological indicator has to be interpreted carefully since there is no other previous study applying the same methodology to combine the hydrological and fish habitat suitability indicators the indicator has been built to evaluate how far from the natural series each scenario is a value of 1 corresponding to the natural condition thus we are more interested in the comparison of the different allocation scenarios and the results we are showing are more focused on the relative gain that may be obtained by using non proportional policies we show a method to choose the optimal distribution functions by comparing all the possible distribution methods the sub indicators have been chosen according to the available data being mainly the natural hydrograph and the characteristics of the power plant but may be improved if more data are available the allocation rules we are presenting in the paper non proportional have not been implemented yet so there are no empirical data available that allows a comparison between the pre impact and post impact systems 4 2 general considerations and recommendations managing water resources to their maximal extent in alpine countries will necessarily force people to be aware that each unit of energy is generated at some expense of the ecology of the riverine ecosystem as a consequence all the feasible measures to improve in efficiency should be taken into consideration together with implementation costs some costs are very much country dependent and this aspect is not addressed in this work being beyond the scope of the work however the implementation costs for generating dynamic flow releases are worth a few comments this work showed that gains in hydropower production and ecohydrological indicator could be made on average by replacing proportional water allocation policies today s best practice though not yet widespread with non proportional ones located on the pareto frontier table 2 improving both criteria such increments must be considered as actual win win solutions these results are based on testing non proportional redistribution rules on only three homogeneous shp case studies limited to the swiss environment and its socio economic context we showed that the potential improvement lies in the wider range of non proportional repartition rules with respect to traditional policies moreover fig 6 demonstrates how classic minimal flow requirement approaches mfr 1 and mfr 2 can be improved mainly in term of ecohydrological benefit by applying non proportional policies even more than by applying proportional ones both dynamic considering the environment as an independent water user perona et al 2013 with specific needs and features is thus the key to obtaining efficient environmental flow releases such rules will generally result in being non proportional and flow dependent in fact while the efficiency curve of a turbine does not change throughout the year the environmental use of water follows seasonal trends this could easily be added in the model and weighted case by case when specific ecological information is available increasing the number of case studies would statistically strengthen the results and suggest more general rules to understand which power plants can actually be improved in global performances this can be challenging to show particularly because data are often not easily available in this work we decided to express the economical indicator as the energy production in gwh this study focuses on small hydropower plants without storage hence this suggests that the optimal strategy would be to always turbine the water diverted according to the chosen allocation rule however a further improvement would consist in considering the variability of the electricity market price this could be made by changing the dimensionless variable x of the fermi function eq 3 so it does not depend only on the flow rate but also on the market price thereby the value of the produced hydropower production would be optimised pereira cardenal et al 2016 energy provision from renewable sources is a sign of human being responsibility which however requires a strong harmonisation among social economic and political parts the question of how to implement non proportional flow release rules has not been addressed in this work however our present research started to address this problem particularly looking at suitable hydraulic infrastructures that may generate fermi function redistribution rules at zero energy costs bernhard and perona 2017 this is highly desirable in order to pursue innovation not only from an intelligent technological infrastructure point of view but also from a sustainable one 5 conclusions this work shows a simple and innovative numerical approach for defining sustainable and efficient environmental flow releases in river reaches of shp without storage the method has been tested on real data and constraints and could be adopted as a prompt answer to the actual need to conciliate environmental protection and growth of hydropower production a convenient class of functions developed by gorla 2014 and razurel et al 2016 was here comprehensively tested as a practical tool for exploring a representative sample of dynamic flow releases such functions provide a direct link between the practice of comparing different environmental flow policies in particular those using fixed percentages of the incoming flows proportional and those with variable splits between diverted and released flows non proportional the pareto frontier is obtained from the simulated alternatives for each case study and it shows that non proportional rules are generally more efficient than traditional ones both proportional and static it was shown that when applying efficient non proportional repartition rules for regulating the run of the river hydropower plants ameliorations in hydropower and ecohydrological performances can be attained with respect to proportional policies although the three case studies are located in switzerland the results vary from one case to another leading to the conclusion that they depend on the river morphology indeed the canyoning morphology in the case of ponte brolla implies that the mfr value is always higher than the threshold given by the wua curve which results in a maximum value for the fish habitat suitability indicator for cauco the foreseen amelioration for the ecohydrological indicator is the most important it goes from 8 6 for the period 1983 2012 to 22 8 for the period 2070 2099 but the gain in energy production is the lowest around 1 in comparison to the two other case studies buseno and ponte brolla show some similar potential gains in energy production around 2 but for the latter the ecohydrological improvement is almost irrelevant between 0 0 and 0 1 author contribution lorenzo gorla and pierre razurel contributed equally to this work acknowledgements we thank the swiss national science foundation for funding the project remedy grant no pp00p2153028 1 as well as renato gaggini of ecocontrol sa for openly discussing practical details this work was written whilst pp visited as academic guest of the group of climatology at the institute of geography of the university of bern the anonymous reviewers whose comments helped to improve the quality of manuscript are greatly acknowledged appendix a graphical user interface gui fig 2 has been developed using the software matlab to facilitate the data treatment and the selection of the optimal water allocation functions among the different scenarios non proportional proportional and mfrs repartition rules this tool takes the natural river hydrograph and the hydropower plant features efficiency function design flow etc as inputs the desired water allocation policies as well as the ecological threshold can also be set the user friendly architecture of the gui freely available to any user that wants to reservedly test the performances of his own cases 1 1 free download from http www sccer soe ch research hydropower task2 4 or by simply contacting the authors pr pp makes the model particularly suitable for stakeholder planning for water managers operations or for academic purposes numerical simulations were performed in order to model the different allocation functions the natural daily flow i t was redistributed between the hydropower plant and the river by simulating eqs 1 3 according to the selected fermi function and for the entire time series of i t for each scenario the energy production and the ecohydrological indicators were computed based on the generated flows q 1 and q 2 respectively the same procedure was repeated for the whole set of selected fermi function parameters as well as for the proportional and mfrs repartition rules as a result the efficiency graph showing the mean annual energy produced during the analyzed period versus the ecohydrological indicator was plotted the pareto front representing the ensemble of optimal water allocation scenarios was identified and enhanced with a red line in the efficiency plot the simulations to asses the impact of the climate change have been performed in the same way for the three case studies i e buseno cauco and ponte brolla the time series of daily streamflow for the three different time periods i e 2000 2050 and 2100 have been generated from the current natural data series by applying the trend of the rcp 6 0 scenario described in the previous section 2 4 
869,in this work coupled cahn hilliard phase field and navier stokes equations were solved using finite element method to address the effects of micro fracture and its characterizations on water oil displacements in a heterogeneous porous medium sensitivity studies at a wide range of viscosity ratios m and capillary numbers ca and the resultant log ca log m stability phase diagram revealed that in both media with without fracture the three regimes of viscous fingering capillary fingering and stable displacement similarly occur however presence of the fracture caused water channeling phenomenon which resulted in reduction of the number of active fingers and hence the final oil recovery factor at high ca especially in the stable regime with log ca 2 5 and log m 0 recovery factor for the fractured medium was relatively identical with the non fractured one at log m 0 the fracture was fully swept but flow instabilities were observed inside the fracture at lower m values especially for log ca 4 6 in the case of the fractured medium at log ca 4 6 and log m 0 capillary dominant flow it is observed that the primary breakthrough took place by a finger progressed through the matrix not those channeled through the fracture geometrical properties of the fracture including length aperture and orientation highly affected both displacement profile and efficiency the fracture length inversely influenced the oil recovery factor it was observed that there is a critical fracture width almost half of the medium average pore diameter at which the recovery factor of the medium during displacement is minimum compared to the media having thinner and thicker fractures minor channeling effect in the media with thinner fracture and larger fracture swept volume as well as high fracture matrix cross flow in the media with thicker fracture were detected as the main cause of this non monotonic behavior in the models with thick fractures with the thickness higher than the average pore diameter considerable trapped oil volumes were observed inside the fracture at low m values the fracture orientation had the most impressive effect on oil recovery compared to the other studied parameters where the oil recovery factor incremented more than 20 as the fracture rotated 90 from flow direction due to the dominant effect of the channeling phenomenon the change in the medium wettability from slightly oil wet to slightly water wet did not considerably affect the displacement profile in the fractured medium however oil recovery factor increased as the medium became more water wet the fracture area was fully swept by the injected water in the oil wet and neutral wet media however flow instabilities were observed inside the fracture of the water wet medium due to counter current imbibition between fracture matrix micro scale mechanisms of pore doublet effect interface coalesce snap off and reverse movements were captured during the studied unstable displacements keywords fractured porous medium phase field method two phase flow capillary fingering viscous fingering wettability 1 introduction displacement of a fluid in porous media by another miscible immiscible one is a process which has applications in variety of fields such as enhanced oil recovery co2 sequestration and contamination transports juanes et al 2006 matthai et al 2007 harrar et al 2007 iglauer et al 2012 arabloo et al 2015 akhlaghi amiri et al 2014 rokhforouz and akhlaghi amiri 2017a liu and wu 2016 during last decades many researchers have studied the governing mechanisms in two phase displacements through porous media both experimentally and numerically at macro and micro scales akhlaghi amiri and hamouda 2014 erfani gahrooei and ghazanfari 2017 joekar niasar and hassanizadeh 2012 karadimitriou and hassanizadeh 2012 liu et al 2016 mason and morrow 2013 meakin and tartakovsky 2009 morrow and mason 2001 raeini et al 2014 rokhforouz and akhlaghi amiri 2017a b li et al 2017a b however for deep understanding the flow behavior in porous media characterization of pore level physics of the phenomena is crucial raeini et al 2014 rokhforouz and akhlaghi amiri 2017a b lenormand et al 1988 and recently zhang et al 2011 conducted a series of micromodel experiments to assess the influences of viscous and capillary forces on the flow regimes they studied simultaneous effects of the capillary number ca and the viscosity ratio between displacing and displaced phases m on flow behavior and mapped the observed displacement regimes on a log m log ca stability phase diagram by tuning ca and m three types of flow regimes including viscous fingering capillary fingering and stable displacement were recognized lenormand et al 1983 1988 lenormand 1989 zhang et al 2011 later similar observations were made in pore scale numerical works done by bandara et al 2013 using a pair wise force smoothed particle hydrodynamics model and akhlaghi amiri and hamouda 2014 using phase field method ferer et al 2004 described the flow regimes in air water displacement experiments through glass micro model by two extreme limits of invasion percolation with trapping and diffusion limited aggregation a modified local capillary number was introduced by cottin et al 2010 in an experimental work using micro fluidic chips to better explain displacement phenomena the influence of viscous fingering on dynamic pressure saturation curves was studied by løvoll et al 2011 using a porous medium made of glass beads liu et al 2015 applied lattice boltzmann technique to simulate the drainage process in homogeneous and heterogeneous pore network it was found that the extent and behavior of the preferential flow such as fingering are strongly dependent upon the ca m and media heterogeneity they also showed that the boundaries of regimes may differ for different pore networks in the case of displacements in porous media containing micro fractures which frequently occurs in naturally fractured hydrocarbon reservoirs the characterization of the flow regimes is more challenging due to a more complicated medium geometry and difference in visco capillary properties of the rock matrix and fractures abushaikha et al 2010 however one can rarely find pore scale experimental and numerical studies on this field farzaneh et al 2010 conducted a series of two phase flow experiments in fractured porous media using glass micro models the effect of the geometrical characteristics of fracture including their number orientation discontinuity overlap distribution and spacing was investigated they observed that in a pattern with fracture oriented against the main flow direction displacement efficiency increased in comparison to other cases abedi et al 2012 and sedaghat et al 2013 experimentally studied the role of the fracture geometrical properties during polymer flooding using five spot glass micromodel saturated with oil it was revealed that the fracture orientation has the most effect on oil recovery compared to the other fracture geometrical parameters in the case of orthogonal to flow fractures longer fracture led to a higher displacement efficiency using 2d fractured glass micromodels shariatpanahi et al 2005 showed that immiscible gas injection performance is a strong function of the fracture orientation while the displacement by water injection is not very sensitive to the fracture orientation kianinejad et al 2015 observed that displacement efficiency is higher in the cases of longer fracture and or fracture perpendicular to the flow direction monteagudo and firoozabadi 2004 used control volume method to model two phase immiscible flow in two dimensional discrete fractured media capillary pressure was shown to have a determining effect on the process this work numerically addresses the effects of micro fracture characterizations on displacement regimes in porous media which are controlled by viscous and capillary forces phase field an interface capturing method is used for numerical simulation of phenomena at pore scale since it is capable of handling complex geometries of fractured media without using model approximations compared to the other approaches akhlaghi amiri and hamouda 2013 2014 maaref et al 2017 rokhforouz and akhlaghi amiri 2017a governing equations are solved using a robust finite element solver comsol multiphysics 2012 viscous and capillary instabilities are compared in the simulated non fractured and fractured heterogeneous porous media by tuning ca and m and under different wettability conditions the effects of the geometrical fracture parameters including length aperture and orientation are verified 2 numerical model this section presents the computational geometry governing equations and the numerical scheme used in this study 2 1 model geometry and boundary conditions the heterogeneous pattern of the simulated porous medium fig 1 was taken from a real dolomite rock section using coreldraw graphics suite x7 software maaref et al 2017 geometrical properties of the simulated medium are summarized in table 1 the medium was initially saturated with the oil phase which was displaced by water phase being injected with a constant velocity from the left hand side of the medium as shown in fig 1 zero pressure was assumed at the outlets on the right hand side of the medium symmetry boundary conditions were employed on the lateral sides the boundary condition of grain surfaces is considered as a wetted wall with specific contact angles it was supposed that the fluids are incompressible with constant physical properties and phase change does not occur gravity was neglected assuming 2d horizontal flow 2 2 governing equations and numerical scheme the injected fluid velocity uinj in the simulated porous media was in the range of 5 10 3 and 1 cm s resulting in reynolds number in the range of 0 05 to 10 hence flow in all the simulations was laminar cahn hilliard phase field method cahn and hilliard 1958 coupled with navier stokes and continuity equations were employed to solve the interfacial problems phase field order parameter ϕ is defined such that the relative concentration of the two components are 1 ϕ 2 and 1 ϕ 2 in this definition ϕ 1 represents the bulks of the two phases and 1 ϕ 1 represents the interface all the fluid physical properties are interpolated between the two phases using the relative concentrations of the phases 1 ϑ ϕ 1 ϕ 2 ϑ 1 1 ϕ 2 ϑ 2 where ϑ denotes a phase property such as viscosity μ or density ρ the modified navier stokes and continuity equations are coupled with the phase field equation to capture moving interface the equation system is then as follows 2 ρ u t ρ u u p μ u u t g ϕ 3 u 0 4 ϕ t u ϕ γ λ ɛ 2 ψ 5 ψ ɛ 2 ϕ ϕ 2 1 ϕ where p denotes pressure u is the fluid velocity field and t is the time in addition ψ is an auxiliary parameter to decompose fourth order cahn hilliard equation to the two second order equations ε is the interface thickness parameter γ is the mobility λ is the mixing energy density and g is the chemical potential g λ 2 ϕ ϕ ϕ 2 1 ɛ 2 in this model mixing energy fmix is obtained by familiar ginzburg landau formulation as follows yue et al 2006 zhou et al 2010 6 f m i x ϕ ϕ 1 2 λ ϕ 2 λ 4 ɛ 2 ϕ 2 1 2 surface tension is considered as an intrinsic property corresponding to the excess free energy density of the interfacial region as ε 0 the ratio λ ɛ produces the interfacial tension in the classical sense σ 2 2 λ 3 ɛ yue et al 2006 other details about phase field theory and formulation can be find elsewhere badalassi et al 2003 fichot et al 2007 jacqmin 1999 yue et al 2006 zhou et al 2010 the governing equations are supplemented by standard boundary conditions e g inlet outlet no slip wetted wall and symmetry on the solid wetted grains the following boundary conditions are implemented 7 u 0 8 n ɛ 2 ϕ ɛ 2 cos θ ϕ 9 n γ λ ɛ 2 δ ψ 0 where n is the unit normal to the wall and θ is the surface contact angle the details about other boundary equations can be found elsewhere yue et al 2006 zhou et al 2010 the numerical equations were solved using a robust finite element solver comsol multiphysics 2012 the domain was resolved by triangular elements finer meshes was implemented around the sharp corners and inside the thin throats and the small pores fig 2 and the coarser ones in the larger pore bodies considering the average pore diameter in porous medium as the characteristic length lc and defining cahn number as cn ε lc it was demonstrated by akhlaghi amiri and hamouda 2013 that using cn 0 03 and mesh size h 0 8ε the model convergence and mesh convergence are satisfied for the phase field method by implementing these values in the simulations of this work the average number of mesh elements used in the simulated models including fractured and non fractured was 282 862 with the average size of 0 012 mm the other critical parameter in the phase field formulation is the mobility γ which according to yue et al 2006 has to be large enough to retain a more or less constant interfacial thickness and small enough to keep the convective motion sensitivity studies done by akhlaghi amiri and hamouda 2013 showed that simulations with 0 1 γ 1 results in less volume shrinkage of fluids in the simulation of two phase flow problems so γ 1 was used in the current study the established simulation model was previously validated with the analytical solution of the stratified two phase poiseuille flow through the channel akhlaghi amiri and hamouda 2013 2014 and showed perfect accuracy it also has been verified with micromodel experimental results for different capillary viscous governing problems akhlaghi amiri and hamouda 2014 rokhforouz and akhlaghi amiri 2017a b the simulation results demonstrated good agreements with the reported experimental observations 3 results and discussion the effects of micro fracture geometrical parameters on the visco capillary behavior of two phase displacements compared with non fractured medium under different wettability conditions are addressed here using several sensitivity studies 3 1 fracture effect on visco capillary flow instabilities capillary number is defined as c a μ w u i n j σ ca and viscosity ratio m μ w μ o quantify the flow characteristics and determine the types of flow instabilities in absence of the gravity forces in all the simulated models at a certain time after breakthrough of the injected water the displacement was stabilized so the profiles of the phases thereafter remained unchanged the stabilized fluid distributions inside the non fractured and the fractured heterogeneous porous media are compared for a range of log ca and log m in figs 3 and 4 respectively ca and m ranged three orders of magnitude the fracture dimensionless length ld is defined as the ratio of the fracture length to the average pore diameter and the dimensionless aperture wd is defined as the ratio of the fracture width to the average pore diameter in the simulated fractured medium here ld and wd were set to 28 and 0 65 respectively the blue and the light red colors in figs 3 and 4 represent water and oil phases respectively and the color gradient represents the interface mixing zone the medium here is considered intermediate wet θ π 2 it is confirmed that the displacement process strongly depends on ca and m for the media with without fracture furthermore the fluid regimes seem corresponding in both cases at different values of m and ca except for the flow at the highest ca and m where the water saturation is high for both non fractured and fractured media the other displacement patterns involve instabilities called fingering however two different types of fingering can be identified named viscous and capillary fingerings based on these observations three different displacement regimes may be classified stable displacement viscous fingering and capillary fingering which is in agreement with reported experimental observations lenormand et al 1988 zhang et al 2011 stable displacement is recognized for both media at log ca 2 6 and log m 0 in this flow regime the advancing fluid completely occupies pore bodies before approaching the neighboring pores in the capillary fingering pattern however instabilities take place in the form of wide forward and lateral moving fronts of the displacing phase with an average width more than 3 pore bodies this type is evident at low ca where m is high enough e g log ca 4 6 and m 0 in figs 3 and 4 in the viscous fingering regime on the other hand several multiple loosely connected or disconnected flow streams are formed which progress toward the outlet with an average width in the order of 1 3 pore bodies this fingering type is evident at lower m values e g log m 2 in figs 3 and 4 transition of the fingering type from viscous to capillary is seen at log ca 4 6 as log m increases from 2 to 0 in the case of the fractured medium fig 4 at log ca 4 6 and log m 0 it is observed that the primary breakthrough took place by a finger progressed through the matrix not those channeled through the fracture this observation is an evidence for the capillary dominant fluid displacement in which the advancing phase preferentially flows through the smaller paths of the matrix with the higher capillary forces the agreement of these results with those obtained in the reported numerical akhlaghi amiri and hamouda 2014 and experimental lenormand et al 1988 zhang et al 2011 works demonstrates that the displacement regimes are independent of the medium properties such as porosity and permeability and presence of micro fracture therefore a similar visco capillary behavior of two phase flow in different porous samples may be expected however the detailed displacement properties such as breakthrough time and recovery factor are functions of medium geometry general comparison of figs 3 and 4 demonstrates lower displacement efficiency of the fractured media compared to the corresponding non fractured ones this is due to the channeling of the injected water through the fracture however in the case of the displacement at the highest ca log ca 2 6 this difference is less pronounced it is also evident that the fracture behaved as a highway in which the formed viscous or capillary fingers were joined together and redirected toward the outlet for example in the case of log ca 4 6 and log m 2 the non fractured medium fig 3 contains several loosely viscous fingers originated close to the inlet while in the fractured medium fig 4 there is one major finger through the fracture at the beginning which is then split into several thin fingers in the fractured medium fig 4 at lower m values the fracture area was not fully swept by water this implies that at low m the domain of instabilities is even extended to the fracture zone however in the cases of stable and capillary dominant displacement regimes the flow inside the fracture was piston like it is worth to note that at high ca values log ca 3 6 when m is low enough log m 1 the water oil interface in both fractured and non fractured media experiences high diffusion detected by gradual color variation capillary number increases by either lowering interfacial tension and or increasing injection velocity for example by lowering interfacial tension e g surfactant injection oil water emulsion may be formed within the front in agreement with the experimental observations on surfactant injection arabloo et al 2015 kianinejad et al 2015 which may result in more efficient microscopic displacement the simulated normalized pressure and the velocity field for the non fractured and the fractured porous media are compared in fig 5 the comparison is made at water pore volume injected pvi of 0 06 the pressure is normalized with capillary pressure pc 2σ lc and the velocity is normalized with the water injection velocity uinj in these models log m 2 and log ca 4 6 and for the fractured medium ld 28 and wd 0 65 there is almost a homogenous pressure gradient in the matrix for both media however it is evident that the medium maximum pressure which is happened around the inlet is higher for the non fractured case p pc 15 103 compared to the fractured one p pc 13 103 so the presence of the fracture causes the flow resistance to decrease in the medium during water oil displacement also in the fractured model fig 5 b at the top the fracture orientation affected the pressure distribution the velocity profile in the non fractured medium fig 5 a at the bottom demonstrates two main water streams at the top and bottom of the medium which later form the main advancing fingers see stabilized fluid distribution of this case in fig 3 however in the fractured medium fig 5 b at the bottom water streamlines are concentrated in the fracture and a maximum velocity u uinj 140 observed at the beginning of the fracture as can be seen the velocity of the other water streams in the matrix is minor when the fracture is present medium water saturation sw after displacement stabilization is plotted as a function of log ca and log m in fig 6 for both the fractured medium dashed lines and the non fractured one continuous lines fig 6 a shows that at different m values when ca increases the water saturation of both media approach together as become almost equal at log ca 2 6 where viscous forces are dominant for both fractured and non fractured cases sw increases by ca with a semi linear trend at different m values this is in agreement with the experimental observations by cottin et al 2010 and zhang et al 2011 in non fractured micromodels however as ca decreases sw reduces with slightly higher gradient in the fractured media compared to the non fractured ones as depicted in fig 6 b for both fractured and non fractured cases sw increases by m the trend of sw log m for the fractured media is linear for all the ca values it can be seen that for the non fractured cases with log ca 2 6 the gradient of the variation in sw is relatively lower for log m 1 compared to log m 1 it may indicate that a kind of transition happens from viscous to capillary dominant flow regimes at about log m 1 as m decreases it is also evident in fig 6 b that at log ca 2 6 the trend of sw versus m for the non fractured medium is almost linear and approximately coincides the fractured one based on the observed flow patterns some of them presented in figs 3 and 4 and the reported study on the fluid saturations fig 6 three identified displacement regimes are mapped on a log m log ca stability phase diagram for both fractured and non fractured media presented in fig 7 flow regimes including stable capillary dominant and viscous dominant in corresponding fractured and non fractured media simulated in this study were relatively identical as also observed in figs 3 and 4 so the boundaries in the stability phase diagram fig 7 are considered to be matched for both media however in the case with more tough matrix structure there may be difference in the boundaries of the flow regimes in fractured medium compared to the non fracture one the approximate boundaries of stable displacement are located at log m 0 and log ca 2 5 log m 1 is considered as the boundary for the viscous fingering region and log ca 4 5 is considered as the boundary for the capillary fingering region the shape of the three regions was discussed in detail by lenormand et al 1988 the region boundaries empirically obtained by zhang et al 2011 are in agreement with those obtained in this work each of the three domains on the stability phase diagram fig 7 corresponds to a different basic mechanism where one kind of force is dominant ferer et al 2004 fernández et al 1991 lenormand et al 1988 in the stable displacement the principle force is due to the larger viscosity of the displacing fluid and the capillary effect is minor lenormand et al 1988 the pattern shows almost a flat front with some irregularities at the scale of a few pores figs 3 and 4 at log m 0 and log ca 2 6 the saturation of the displacing phase is high after flow stabilization for the stable displacement the viscous fingering occurs when the viscosity of the displaced fluid is dominant i e larger the pattern shows tree like fingers spread across the network growing toward the outlet figs 3 and 4 at log m 1 and log ca 4 6 viscous fingering regime is also referred to as open branch because no viscous fluid encirclement occurs fernández et al 1991 at low capillary numbers e g low injection rates or high interfacial tension the principal force is due to the capillarity in this condition figs 3 and 4 for log m 0 and log ca 4 6 capillary fingering occurs which spread across the network but the pattern is different from the previous case for very small capillary numbers flows exhibit capillary fingering even at m 0 fernández et al 1991 3 2 effect of geometrical characteristics of fracture fig 8 shows the oil recovery factor rf as a function of water pore volume injected pvi for the simulated model in which log m 2 and log ca 4 6 the results are compared at different dimensionless fracture lengths of ld of 0 non fractured 18 28 and 38 fig 9 shows the fluid profiles after breakthrough of the injected water as the displacement is stabilized in general increasing ld resulted in lower oil recovery factor and earlier water breakthrough which is in agreement with the experimental results kianinejad et al 2015 presence of even a short fracture with ld 18 resulted in a fall in recovery factor almost 10 as observed in fig 8 it can be seen in fig 8 that as fracture dimensionless length increases from 18 to 38 almost doubled oil recovery factor decreases from 0 28 to 0 14 halved it is also worth to note that the time gap between the primary breakthrough time the instant at which rf deviated from linearity and the stabilization time is a function of the fracture length see fig 8 in this time gap active fingers breakthrough see fig 9 in the case of the non fractured medium there are several active fingers which breakthrough one by one before stabilization time so there is a gradual change in rf at this time period however in the case of the medium with the longest fracture ld 38 there is just a major water finger channeled through the fracture so as this finger breakthroughs the stabilization takes place as shown in fig 9 as the length of the fracture increases the progress of the water finger channeled through the fracture becomes more dominant and the progress of other formed viscous fingers in the matrix is more declined in the non fractured medium fig 9 a there are two main fingering areas at the top and the bottom of the medium first the one at the bottom breakthroughs for the fractured case with ld 18 fig 9 b due to the water channeling through the fracture the fingering area at the bottom initiated but its rate of progress is less than the non fractured one so at the breakthrough time it does not reach the outlet as the fracture length increases more fig 9 c and d the propagation of this bottom finger is more depressed since the water channeling effect is more pronounced for the case of the longest fracture the water phase just invaded some adjacent pore bodies around the inlet and the fracture and an early breakthrough happened at pvi 0 12 it is also evident that for the medium with the shortest fracture fig 9 b water swept area inside the fracture is the least almost 70 in other words the fracture water invaded fraction increases by its length in the case of the short fracture the pressure difference between the two fracture ends is low which is almost in the same order of pressure gradient in the matrix therefore the water crossflow from the fracture to the matrix is more possible however in the case with a long fracture the considerable lower flow resistance inside the fracture compared to the neighboring matrix results in preferential water flow through the fracture three different models with the fracture inclinations of α 0 in the flow direction α 45 and α 90 were simulated to study the effect of fracture orientation on displacement behavior in the simulated models log m 2 log ca 4 6 ld 28 and wd 0 65 fig 10 compares oil recovery factor of these models as a function of water pore volume injected pvi it is observed that increasing α results in higher rf and a later breakthrough time which was predicted approximately 30 of the matrix oil is recovered in the case of the model with α 90 while the final recovery of the model with α 45 and α 0 are 20 and 10 respectively i e 10 reduction of rf for each 45 fracture rotation toward flow direction fig 11 shows the stabilized distribution of water and oil in the models with different α in the case of the fracture perpendicular to the flow direction fig 11 a general displacement pattern is similar to the non fractured case fig 9 a however it is observed that presence of a fracture with other orientation fig 11 b and c adversely affects displacement efficiency in agreement with the micro model experimental observations made by abedi et al 2012 farzaneh et al 2010 and sedaghat et al 2013 for example for the case with the fracture in the flow direction fig 11 c water just went through the fracture highway and resulted in an extremely early water breakthrough at pvi 0 1 fig 12 shows rf as a function of water pvi for the simulated models with different fracture dimensionless apertures in the range of 0 3 to 2 a non monotonic behavior is observed in fig 12 it is interesting to see that the case with wd 0 3 has the highest rf while the one with wd 0 65 has the lowest rf by increasing wd above 0 65 rf gradually increased but did not reach that of wd 0 3 this observation indicates that there is a critical fracture width wd 0 65 in the current model in which the medium rf is minimum in general variation of the fracture width in the studied range resulted in less than 10 difference in rf fig 13 demonstrates the stabilized distribution of water and oil in the models with different fracture apertures several loosely connected or disconnected flow paths can be seen in the model with wd 0 3 as the fracture aperture is in the range of the medium pore and throat size see fig 13 a although water channeling through the fracture is minor in the case with wd 0 3 it is affecting the displacement in those with wd 0 65 so the water channeling effect is the major cause of rf reduction as wd increases from 0 3 to 0 65 fig 12 as can be seen by comparison of fig 13 b d for wd 0 65 the size of the single active finger originated from the inlet at the bottom of the medium is relatively similar however the size of the fingers originated from the fracture enlarged by wd the cross flow out of the fracture and into the neighboring matrix pores enhanced by wd also the swept area of the fracture increased by wd therefore rf increased as the fracture was thickened from 0 65 to 2 fig 12 it is also observed that in the case of the wider fractures see fig 13 c and d some amount of oil is trapped inside the fracture both at the middle and at the bottom as the fracture width increases the trapped oil volumes become larger this effect is due to the water front instability fingering inside the fracture which is exaggerated as the fracture becomes wider for detail study of the fluid instabilities inside the fracture fluid displacement within the fracture for the medium with wd 2 is plotted in fig 14 at different instants during water invasion at the first instant pvi 0 1 it is observed that three small drops of oil have been formed water front is advancing inside the fracture however the displacement is not stable piston like it is bypassing some amount of oil due to low m log m 2 and the large fracture width the bypassed oil forms more droplets as can be seen in fig 14 at pvi 0 14 during displacement some of the formed oil droplets join together and form bigger drops pvi 0 17 and 0 2 it is very interesting to see the advancing and the receding contact angles inside the fracture for the bigger drop at pvi 0 17 and the single big drop at pvi 0 2 which are happened under the influence of water momentum within the fracture at a later time after pvi 0 2 the oil drop slightly grows and its shape is changed see fig 13 which may be due to the counter current displacement between the fracture and the neighboring matrix of course the effect of the fracture geometrical parameters including length width and orientation on displacement behavior is a function of ca and m combining the results obtained in sections 3 1 and 3 2 may suggest that increasing ca and m approaching stable displacement will reduce the effect of the fracture parameters however lowering m and ca may exacerbate these effects 3 3 effect of medium surface wettability to quantify the degree of wettability contact angle θ is usually studied which is a boundary condition in determining the interfacial shape brown and neustadter 1980 rabbani et al 2017 a grain surface is generally considered water wet if θ π 2 and oil wet if θ π 2 the influence of wettability is addressed in this section for water oil displacement in the fractured medium as log ca 4 6 log m 2 ld 28 and wd 0 65 it is important to note that when the medium is water wet θ π 2 water injection represents an imbibition process while when the medium is oil wet θ π 2 it represents drainage process fig 15 shows the fluid distributions after displacement stabilization for different θ values of 5π 12 π 2 and 2π 3 corresponding to slightly water wet neutral wet and slightly oil wet respectively as shown in fig 15 the general distribution of the water fingers is quite similar for all the studied wettabilities however as depicted in fig 15 a in the case of the water wet medium water preferentially tends to invade thinner pores and throats compared to the oil wet medium another important difference between these cases is the displacement stability inside the fracture area as can be seen in fig 15 c for the oil wet medium the fracture is fully swept by the invading phase capillary force in the porous matrix is negative for water in the case of the oil wet medium so water prefers to fully invade the fracture with lower capillary forces before drainage into the matrix however in the case of the water wet medium fig 15 a water flow within the fracture is unstable and hence some oil droplets were trapped in the fracture in this case water imbibition takes place between the fracture and the matrix in other words positive capillary forces in the porous matrix in the water wet medium resulted in imbibition of water from the fracture into the porous matrix it is also observed that the growth of the active finger at the bottom of the matrix increases as the medium becomes more water wet in the case of strongly water wet media it was observed that due to the considerable water imbibition rate between fracture and matrix the displacement profile and efficiency approached that of the non fractured medium this is in agreement with the simulation results of monteagudo and firoozabadi 2004 and experimental observations made by pooladi darvish and firoozabadi 2000 the detailed water displacement profiles inside the fracture versus time are depicted in fig 16 for the water wet medium as the interface advances through the fracture pvi 0 3 water imbibes to the matrix hence oil drops enter the fracture zone due to the counter current imbibition process in agreement with the results by rokhforouz and akhlaghi amiri 2017a b small oil drops expelled from the porous matrix into the fracture join together at pvi 0 44 and the formed larger droplets moves toward the end of the fracture pvi 0 58 this counter current imbibition process goes on during displacement process pvi 0 72 until stabilization however based on the previous studies rokhforouz and akhlaghi amiri 2017a b a different porosity and permeability as well as wettability of the matrix would affect the imbibition process and the resultant oil recovery factor rf increases as the medium becomes more water wet fig 17 it is also evident in fig 17 that in the case of the neutral wet medium the time gap between the primary breakthrough and the stabilization time is wider in both water wet and oil wet media flow is stabilized just after primary breakthrough theses means that the number of active fingers in the case of neutral wet medium is larger compared to the other verified wettability conditions fig 18 demonstrates two instants during water invasion process for three enlarged sections of the oil wet medium θ 2 π 3 the simulated model was able to successfully capture different pore scale mechanisms during the displacement process some pore scale displacement mechanisms including pore doublet interface coalesce snap off and reverse displacement were captured due to the capillary pressure effect the non wetting phase water preferentially invades the larger pores and the oil phase is bypassed in the smaller pores shown in fig 18 a with black circle water phase bridges between the adjacent pores and the interface coalesce happens specified with green circle in fig 18 a as specified in fig 18 b with black circle the water phase forms collars in the pore throat which snaps off the oil droplet the presence of concave convex and flat interfaces in the oil wet porous medium is also detected specified with black purple and red arrows respectively complex interplay between the contact angle and pore angularity causes the variation of the interface curvature in accordance with the theoretical results of mason and morrow 1994 fig 18 c shows a pore level reverse displacement as a result of pressure gradient decline across the throat in other words when the interface s curvature changes from convex to concave the interface is forced back into the pore throat against the direction of the main stream flow 4 conclusion this work evaluates the effects of micro fracture geometrical parameters on visco capillary behavior of two phase displacements compared with non fractured medium under different wettability conditions a heterogeneous pattern taken from a real dolomite rock section was considered as the computational domain to perform the simulations the coupled cahn hilliard phase field and navier stokes equations were solved by a finite element method using comsol multiphysics solver the values of mesh sizes and interface thickness parameter were set in order to achieve mesh convergence and model convergence first numerical experiments were performed on the simulated medium with without fracture to study the effects of viscosity ratios m and capillary numbers ca for both media the boundaries of stable displacement were identified at log m 0 and log ca 2 5 and the boundaries for viscous fingering and capillary fingering regions were located at log m 1 and log ca 4 5 respectively presence of the fracture in the fractured medium resulted in water channeling effect which caused reduction of the number of active fingers and hence reduction of the final oil recovery factor at log m 0 the fracture was fully swept by the injected water but flow instabilities were observed inside the fracture at lower m values especially when log ca 4 6 at high ca especially in the stable regime with log ca 2 5 and log m 0 recovery factor for the fractured medium was relatively identical with the non fractured one the effects of geometrical characteristics of fracture including length aperture and orientation were evaluated in the simulated models the presence of even a short fracture affected oil recovery factor it was observed that increasing the fracture length resulted in lower oil recovery factor due to earlier water breakthrough time caused by stronger channeling effect it was observed that there is a critical fracture width almost half of the medium average pore diameter at which the recovery factor of the medium during displacement is minimum compared to the media having thinner and thicker fractures minor channeling effect in the media with thinner fracture and larger fracture swept volume as well as high fracture matrix cross flow in the media with thicker fracture were detected as the main cause of this non monotonic behavior in the models with thick fractures with the thickness higher than the average pore diameter considerable trapped oil volumes were observed inside the fracture at lower m the fracture orientation showed the most impressive effect on oil recovery compared to the other studied fracture geometrical properties the oil recovery factor increased 20 when the fracture orientation changed from flow direction to the direction perpendicular to the flow the medium with perpendicular to the flow fracture behaved almost the same as the non fractured medium the effect of medium wettability was also studied the change of the medium wettability from oil wet to neutral or slightly water wet did not highly affect the displacement profile in the fractured medium however oil recovery factor was increased as the medium became more water wet although the fracture was fully invaded by water in the oil wet and neutral wet media flow instabilities caused by counter current imbibition were observed inside the fracture for water wet medium different pore scale events were captured for different simulated models for example pore doublet interface coalesce snap off and reverse movements were observed during the studied displacement in oil wet medium the presence of concave convex and flat interfaces were also detected the results of this numerical study confirmed the ability of the phase field model to realistically predict different pore level mechanisms during two phase displacements processes through highly sophisticated media containing fractures acknowledgment this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors 
869,in this work coupled cahn hilliard phase field and navier stokes equations were solved using finite element method to address the effects of micro fracture and its characterizations on water oil displacements in a heterogeneous porous medium sensitivity studies at a wide range of viscosity ratios m and capillary numbers ca and the resultant log ca log m stability phase diagram revealed that in both media with without fracture the three regimes of viscous fingering capillary fingering and stable displacement similarly occur however presence of the fracture caused water channeling phenomenon which resulted in reduction of the number of active fingers and hence the final oil recovery factor at high ca especially in the stable regime with log ca 2 5 and log m 0 recovery factor for the fractured medium was relatively identical with the non fractured one at log m 0 the fracture was fully swept but flow instabilities were observed inside the fracture at lower m values especially for log ca 4 6 in the case of the fractured medium at log ca 4 6 and log m 0 capillary dominant flow it is observed that the primary breakthrough took place by a finger progressed through the matrix not those channeled through the fracture geometrical properties of the fracture including length aperture and orientation highly affected both displacement profile and efficiency the fracture length inversely influenced the oil recovery factor it was observed that there is a critical fracture width almost half of the medium average pore diameter at which the recovery factor of the medium during displacement is minimum compared to the media having thinner and thicker fractures minor channeling effect in the media with thinner fracture and larger fracture swept volume as well as high fracture matrix cross flow in the media with thicker fracture were detected as the main cause of this non monotonic behavior in the models with thick fractures with the thickness higher than the average pore diameter considerable trapped oil volumes were observed inside the fracture at low m values the fracture orientation had the most impressive effect on oil recovery compared to the other studied parameters where the oil recovery factor incremented more than 20 as the fracture rotated 90 from flow direction due to the dominant effect of the channeling phenomenon the change in the medium wettability from slightly oil wet to slightly water wet did not considerably affect the displacement profile in the fractured medium however oil recovery factor increased as the medium became more water wet the fracture area was fully swept by the injected water in the oil wet and neutral wet media however flow instabilities were observed inside the fracture of the water wet medium due to counter current imbibition between fracture matrix micro scale mechanisms of pore doublet effect interface coalesce snap off and reverse movements were captured during the studied unstable displacements keywords fractured porous medium phase field method two phase flow capillary fingering viscous fingering wettability 1 introduction displacement of a fluid in porous media by another miscible immiscible one is a process which has applications in variety of fields such as enhanced oil recovery co2 sequestration and contamination transports juanes et al 2006 matthai et al 2007 harrar et al 2007 iglauer et al 2012 arabloo et al 2015 akhlaghi amiri et al 2014 rokhforouz and akhlaghi amiri 2017a liu and wu 2016 during last decades many researchers have studied the governing mechanisms in two phase displacements through porous media both experimentally and numerically at macro and micro scales akhlaghi amiri and hamouda 2014 erfani gahrooei and ghazanfari 2017 joekar niasar and hassanizadeh 2012 karadimitriou and hassanizadeh 2012 liu et al 2016 mason and morrow 2013 meakin and tartakovsky 2009 morrow and mason 2001 raeini et al 2014 rokhforouz and akhlaghi amiri 2017a b li et al 2017a b however for deep understanding the flow behavior in porous media characterization of pore level physics of the phenomena is crucial raeini et al 2014 rokhforouz and akhlaghi amiri 2017a b lenormand et al 1988 and recently zhang et al 2011 conducted a series of micromodel experiments to assess the influences of viscous and capillary forces on the flow regimes they studied simultaneous effects of the capillary number ca and the viscosity ratio between displacing and displaced phases m on flow behavior and mapped the observed displacement regimes on a log m log ca stability phase diagram by tuning ca and m three types of flow regimes including viscous fingering capillary fingering and stable displacement were recognized lenormand et al 1983 1988 lenormand 1989 zhang et al 2011 later similar observations were made in pore scale numerical works done by bandara et al 2013 using a pair wise force smoothed particle hydrodynamics model and akhlaghi amiri and hamouda 2014 using phase field method ferer et al 2004 described the flow regimes in air water displacement experiments through glass micro model by two extreme limits of invasion percolation with trapping and diffusion limited aggregation a modified local capillary number was introduced by cottin et al 2010 in an experimental work using micro fluidic chips to better explain displacement phenomena the influence of viscous fingering on dynamic pressure saturation curves was studied by løvoll et al 2011 using a porous medium made of glass beads liu et al 2015 applied lattice boltzmann technique to simulate the drainage process in homogeneous and heterogeneous pore network it was found that the extent and behavior of the preferential flow such as fingering are strongly dependent upon the ca m and media heterogeneity they also showed that the boundaries of regimes may differ for different pore networks in the case of displacements in porous media containing micro fractures which frequently occurs in naturally fractured hydrocarbon reservoirs the characterization of the flow regimes is more challenging due to a more complicated medium geometry and difference in visco capillary properties of the rock matrix and fractures abushaikha et al 2010 however one can rarely find pore scale experimental and numerical studies on this field farzaneh et al 2010 conducted a series of two phase flow experiments in fractured porous media using glass micro models the effect of the geometrical characteristics of fracture including their number orientation discontinuity overlap distribution and spacing was investigated they observed that in a pattern with fracture oriented against the main flow direction displacement efficiency increased in comparison to other cases abedi et al 2012 and sedaghat et al 2013 experimentally studied the role of the fracture geometrical properties during polymer flooding using five spot glass micromodel saturated with oil it was revealed that the fracture orientation has the most effect on oil recovery compared to the other fracture geometrical parameters in the case of orthogonal to flow fractures longer fracture led to a higher displacement efficiency using 2d fractured glass micromodels shariatpanahi et al 2005 showed that immiscible gas injection performance is a strong function of the fracture orientation while the displacement by water injection is not very sensitive to the fracture orientation kianinejad et al 2015 observed that displacement efficiency is higher in the cases of longer fracture and or fracture perpendicular to the flow direction monteagudo and firoozabadi 2004 used control volume method to model two phase immiscible flow in two dimensional discrete fractured media capillary pressure was shown to have a determining effect on the process this work numerically addresses the effects of micro fracture characterizations on displacement regimes in porous media which are controlled by viscous and capillary forces phase field an interface capturing method is used for numerical simulation of phenomena at pore scale since it is capable of handling complex geometries of fractured media without using model approximations compared to the other approaches akhlaghi amiri and hamouda 2013 2014 maaref et al 2017 rokhforouz and akhlaghi amiri 2017a governing equations are solved using a robust finite element solver comsol multiphysics 2012 viscous and capillary instabilities are compared in the simulated non fractured and fractured heterogeneous porous media by tuning ca and m and under different wettability conditions the effects of the geometrical fracture parameters including length aperture and orientation are verified 2 numerical model this section presents the computational geometry governing equations and the numerical scheme used in this study 2 1 model geometry and boundary conditions the heterogeneous pattern of the simulated porous medium fig 1 was taken from a real dolomite rock section using coreldraw graphics suite x7 software maaref et al 2017 geometrical properties of the simulated medium are summarized in table 1 the medium was initially saturated with the oil phase which was displaced by water phase being injected with a constant velocity from the left hand side of the medium as shown in fig 1 zero pressure was assumed at the outlets on the right hand side of the medium symmetry boundary conditions were employed on the lateral sides the boundary condition of grain surfaces is considered as a wetted wall with specific contact angles it was supposed that the fluids are incompressible with constant physical properties and phase change does not occur gravity was neglected assuming 2d horizontal flow 2 2 governing equations and numerical scheme the injected fluid velocity uinj in the simulated porous media was in the range of 5 10 3 and 1 cm s resulting in reynolds number in the range of 0 05 to 10 hence flow in all the simulations was laminar cahn hilliard phase field method cahn and hilliard 1958 coupled with navier stokes and continuity equations were employed to solve the interfacial problems phase field order parameter ϕ is defined such that the relative concentration of the two components are 1 ϕ 2 and 1 ϕ 2 in this definition ϕ 1 represents the bulks of the two phases and 1 ϕ 1 represents the interface all the fluid physical properties are interpolated between the two phases using the relative concentrations of the phases 1 ϑ ϕ 1 ϕ 2 ϑ 1 1 ϕ 2 ϑ 2 where ϑ denotes a phase property such as viscosity μ or density ρ the modified navier stokes and continuity equations are coupled with the phase field equation to capture moving interface the equation system is then as follows 2 ρ u t ρ u u p μ u u t g ϕ 3 u 0 4 ϕ t u ϕ γ λ ɛ 2 ψ 5 ψ ɛ 2 ϕ ϕ 2 1 ϕ where p denotes pressure u is the fluid velocity field and t is the time in addition ψ is an auxiliary parameter to decompose fourth order cahn hilliard equation to the two second order equations ε is the interface thickness parameter γ is the mobility λ is the mixing energy density and g is the chemical potential g λ 2 ϕ ϕ ϕ 2 1 ɛ 2 in this model mixing energy fmix is obtained by familiar ginzburg landau formulation as follows yue et al 2006 zhou et al 2010 6 f m i x ϕ ϕ 1 2 λ ϕ 2 λ 4 ɛ 2 ϕ 2 1 2 surface tension is considered as an intrinsic property corresponding to the excess free energy density of the interfacial region as ε 0 the ratio λ ɛ produces the interfacial tension in the classical sense σ 2 2 λ 3 ɛ yue et al 2006 other details about phase field theory and formulation can be find elsewhere badalassi et al 2003 fichot et al 2007 jacqmin 1999 yue et al 2006 zhou et al 2010 the governing equations are supplemented by standard boundary conditions e g inlet outlet no slip wetted wall and symmetry on the solid wetted grains the following boundary conditions are implemented 7 u 0 8 n ɛ 2 ϕ ɛ 2 cos θ ϕ 9 n γ λ ɛ 2 δ ψ 0 where n is the unit normal to the wall and θ is the surface contact angle the details about other boundary equations can be found elsewhere yue et al 2006 zhou et al 2010 the numerical equations were solved using a robust finite element solver comsol multiphysics 2012 the domain was resolved by triangular elements finer meshes was implemented around the sharp corners and inside the thin throats and the small pores fig 2 and the coarser ones in the larger pore bodies considering the average pore diameter in porous medium as the characteristic length lc and defining cahn number as cn ε lc it was demonstrated by akhlaghi amiri and hamouda 2013 that using cn 0 03 and mesh size h 0 8ε the model convergence and mesh convergence are satisfied for the phase field method by implementing these values in the simulations of this work the average number of mesh elements used in the simulated models including fractured and non fractured was 282 862 with the average size of 0 012 mm the other critical parameter in the phase field formulation is the mobility γ which according to yue et al 2006 has to be large enough to retain a more or less constant interfacial thickness and small enough to keep the convective motion sensitivity studies done by akhlaghi amiri and hamouda 2013 showed that simulations with 0 1 γ 1 results in less volume shrinkage of fluids in the simulation of two phase flow problems so γ 1 was used in the current study the established simulation model was previously validated with the analytical solution of the stratified two phase poiseuille flow through the channel akhlaghi amiri and hamouda 2013 2014 and showed perfect accuracy it also has been verified with micromodel experimental results for different capillary viscous governing problems akhlaghi amiri and hamouda 2014 rokhforouz and akhlaghi amiri 2017a b the simulation results demonstrated good agreements with the reported experimental observations 3 results and discussion the effects of micro fracture geometrical parameters on the visco capillary behavior of two phase displacements compared with non fractured medium under different wettability conditions are addressed here using several sensitivity studies 3 1 fracture effect on visco capillary flow instabilities capillary number is defined as c a μ w u i n j σ ca and viscosity ratio m μ w μ o quantify the flow characteristics and determine the types of flow instabilities in absence of the gravity forces in all the simulated models at a certain time after breakthrough of the injected water the displacement was stabilized so the profiles of the phases thereafter remained unchanged the stabilized fluid distributions inside the non fractured and the fractured heterogeneous porous media are compared for a range of log ca and log m in figs 3 and 4 respectively ca and m ranged three orders of magnitude the fracture dimensionless length ld is defined as the ratio of the fracture length to the average pore diameter and the dimensionless aperture wd is defined as the ratio of the fracture width to the average pore diameter in the simulated fractured medium here ld and wd were set to 28 and 0 65 respectively the blue and the light red colors in figs 3 and 4 represent water and oil phases respectively and the color gradient represents the interface mixing zone the medium here is considered intermediate wet θ π 2 it is confirmed that the displacement process strongly depends on ca and m for the media with without fracture furthermore the fluid regimes seem corresponding in both cases at different values of m and ca except for the flow at the highest ca and m where the water saturation is high for both non fractured and fractured media the other displacement patterns involve instabilities called fingering however two different types of fingering can be identified named viscous and capillary fingerings based on these observations three different displacement regimes may be classified stable displacement viscous fingering and capillary fingering which is in agreement with reported experimental observations lenormand et al 1988 zhang et al 2011 stable displacement is recognized for both media at log ca 2 6 and log m 0 in this flow regime the advancing fluid completely occupies pore bodies before approaching the neighboring pores in the capillary fingering pattern however instabilities take place in the form of wide forward and lateral moving fronts of the displacing phase with an average width more than 3 pore bodies this type is evident at low ca where m is high enough e g log ca 4 6 and m 0 in figs 3 and 4 in the viscous fingering regime on the other hand several multiple loosely connected or disconnected flow streams are formed which progress toward the outlet with an average width in the order of 1 3 pore bodies this fingering type is evident at lower m values e g log m 2 in figs 3 and 4 transition of the fingering type from viscous to capillary is seen at log ca 4 6 as log m increases from 2 to 0 in the case of the fractured medium fig 4 at log ca 4 6 and log m 0 it is observed that the primary breakthrough took place by a finger progressed through the matrix not those channeled through the fracture this observation is an evidence for the capillary dominant fluid displacement in which the advancing phase preferentially flows through the smaller paths of the matrix with the higher capillary forces the agreement of these results with those obtained in the reported numerical akhlaghi amiri and hamouda 2014 and experimental lenormand et al 1988 zhang et al 2011 works demonstrates that the displacement regimes are independent of the medium properties such as porosity and permeability and presence of micro fracture therefore a similar visco capillary behavior of two phase flow in different porous samples may be expected however the detailed displacement properties such as breakthrough time and recovery factor are functions of medium geometry general comparison of figs 3 and 4 demonstrates lower displacement efficiency of the fractured media compared to the corresponding non fractured ones this is due to the channeling of the injected water through the fracture however in the case of the displacement at the highest ca log ca 2 6 this difference is less pronounced it is also evident that the fracture behaved as a highway in which the formed viscous or capillary fingers were joined together and redirected toward the outlet for example in the case of log ca 4 6 and log m 2 the non fractured medium fig 3 contains several loosely viscous fingers originated close to the inlet while in the fractured medium fig 4 there is one major finger through the fracture at the beginning which is then split into several thin fingers in the fractured medium fig 4 at lower m values the fracture area was not fully swept by water this implies that at low m the domain of instabilities is even extended to the fracture zone however in the cases of stable and capillary dominant displacement regimes the flow inside the fracture was piston like it is worth to note that at high ca values log ca 3 6 when m is low enough log m 1 the water oil interface in both fractured and non fractured media experiences high diffusion detected by gradual color variation capillary number increases by either lowering interfacial tension and or increasing injection velocity for example by lowering interfacial tension e g surfactant injection oil water emulsion may be formed within the front in agreement with the experimental observations on surfactant injection arabloo et al 2015 kianinejad et al 2015 which may result in more efficient microscopic displacement the simulated normalized pressure and the velocity field for the non fractured and the fractured porous media are compared in fig 5 the comparison is made at water pore volume injected pvi of 0 06 the pressure is normalized with capillary pressure pc 2σ lc and the velocity is normalized with the water injection velocity uinj in these models log m 2 and log ca 4 6 and for the fractured medium ld 28 and wd 0 65 there is almost a homogenous pressure gradient in the matrix for both media however it is evident that the medium maximum pressure which is happened around the inlet is higher for the non fractured case p pc 15 103 compared to the fractured one p pc 13 103 so the presence of the fracture causes the flow resistance to decrease in the medium during water oil displacement also in the fractured model fig 5 b at the top the fracture orientation affected the pressure distribution the velocity profile in the non fractured medium fig 5 a at the bottom demonstrates two main water streams at the top and bottom of the medium which later form the main advancing fingers see stabilized fluid distribution of this case in fig 3 however in the fractured medium fig 5 b at the bottom water streamlines are concentrated in the fracture and a maximum velocity u uinj 140 observed at the beginning of the fracture as can be seen the velocity of the other water streams in the matrix is minor when the fracture is present medium water saturation sw after displacement stabilization is plotted as a function of log ca and log m in fig 6 for both the fractured medium dashed lines and the non fractured one continuous lines fig 6 a shows that at different m values when ca increases the water saturation of both media approach together as become almost equal at log ca 2 6 where viscous forces are dominant for both fractured and non fractured cases sw increases by ca with a semi linear trend at different m values this is in agreement with the experimental observations by cottin et al 2010 and zhang et al 2011 in non fractured micromodels however as ca decreases sw reduces with slightly higher gradient in the fractured media compared to the non fractured ones as depicted in fig 6 b for both fractured and non fractured cases sw increases by m the trend of sw log m for the fractured media is linear for all the ca values it can be seen that for the non fractured cases with log ca 2 6 the gradient of the variation in sw is relatively lower for log m 1 compared to log m 1 it may indicate that a kind of transition happens from viscous to capillary dominant flow regimes at about log m 1 as m decreases it is also evident in fig 6 b that at log ca 2 6 the trend of sw versus m for the non fractured medium is almost linear and approximately coincides the fractured one based on the observed flow patterns some of them presented in figs 3 and 4 and the reported study on the fluid saturations fig 6 three identified displacement regimes are mapped on a log m log ca stability phase diagram for both fractured and non fractured media presented in fig 7 flow regimes including stable capillary dominant and viscous dominant in corresponding fractured and non fractured media simulated in this study were relatively identical as also observed in figs 3 and 4 so the boundaries in the stability phase diagram fig 7 are considered to be matched for both media however in the case with more tough matrix structure there may be difference in the boundaries of the flow regimes in fractured medium compared to the non fracture one the approximate boundaries of stable displacement are located at log m 0 and log ca 2 5 log m 1 is considered as the boundary for the viscous fingering region and log ca 4 5 is considered as the boundary for the capillary fingering region the shape of the three regions was discussed in detail by lenormand et al 1988 the region boundaries empirically obtained by zhang et al 2011 are in agreement with those obtained in this work each of the three domains on the stability phase diagram fig 7 corresponds to a different basic mechanism where one kind of force is dominant ferer et al 2004 fernández et al 1991 lenormand et al 1988 in the stable displacement the principle force is due to the larger viscosity of the displacing fluid and the capillary effect is minor lenormand et al 1988 the pattern shows almost a flat front with some irregularities at the scale of a few pores figs 3 and 4 at log m 0 and log ca 2 6 the saturation of the displacing phase is high after flow stabilization for the stable displacement the viscous fingering occurs when the viscosity of the displaced fluid is dominant i e larger the pattern shows tree like fingers spread across the network growing toward the outlet figs 3 and 4 at log m 1 and log ca 4 6 viscous fingering regime is also referred to as open branch because no viscous fluid encirclement occurs fernández et al 1991 at low capillary numbers e g low injection rates or high interfacial tension the principal force is due to the capillarity in this condition figs 3 and 4 for log m 0 and log ca 4 6 capillary fingering occurs which spread across the network but the pattern is different from the previous case for very small capillary numbers flows exhibit capillary fingering even at m 0 fernández et al 1991 3 2 effect of geometrical characteristics of fracture fig 8 shows the oil recovery factor rf as a function of water pore volume injected pvi for the simulated model in which log m 2 and log ca 4 6 the results are compared at different dimensionless fracture lengths of ld of 0 non fractured 18 28 and 38 fig 9 shows the fluid profiles after breakthrough of the injected water as the displacement is stabilized in general increasing ld resulted in lower oil recovery factor and earlier water breakthrough which is in agreement with the experimental results kianinejad et al 2015 presence of even a short fracture with ld 18 resulted in a fall in recovery factor almost 10 as observed in fig 8 it can be seen in fig 8 that as fracture dimensionless length increases from 18 to 38 almost doubled oil recovery factor decreases from 0 28 to 0 14 halved it is also worth to note that the time gap between the primary breakthrough time the instant at which rf deviated from linearity and the stabilization time is a function of the fracture length see fig 8 in this time gap active fingers breakthrough see fig 9 in the case of the non fractured medium there are several active fingers which breakthrough one by one before stabilization time so there is a gradual change in rf at this time period however in the case of the medium with the longest fracture ld 38 there is just a major water finger channeled through the fracture so as this finger breakthroughs the stabilization takes place as shown in fig 9 as the length of the fracture increases the progress of the water finger channeled through the fracture becomes more dominant and the progress of other formed viscous fingers in the matrix is more declined in the non fractured medium fig 9 a there are two main fingering areas at the top and the bottom of the medium first the one at the bottom breakthroughs for the fractured case with ld 18 fig 9 b due to the water channeling through the fracture the fingering area at the bottom initiated but its rate of progress is less than the non fractured one so at the breakthrough time it does not reach the outlet as the fracture length increases more fig 9 c and d the propagation of this bottom finger is more depressed since the water channeling effect is more pronounced for the case of the longest fracture the water phase just invaded some adjacent pore bodies around the inlet and the fracture and an early breakthrough happened at pvi 0 12 it is also evident that for the medium with the shortest fracture fig 9 b water swept area inside the fracture is the least almost 70 in other words the fracture water invaded fraction increases by its length in the case of the short fracture the pressure difference between the two fracture ends is low which is almost in the same order of pressure gradient in the matrix therefore the water crossflow from the fracture to the matrix is more possible however in the case with a long fracture the considerable lower flow resistance inside the fracture compared to the neighboring matrix results in preferential water flow through the fracture three different models with the fracture inclinations of α 0 in the flow direction α 45 and α 90 were simulated to study the effect of fracture orientation on displacement behavior in the simulated models log m 2 log ca 4 6 ld 28 and wd 0 65 fig 10 compares oil recovery factor of these models as a function of water pore volume injected pvi it is observed that increasing α results in higher rf and a later breakthrough time which was predicted approximately 30 of the matrix oil is recovered in the case of the model with α 90 while the final recovery of the model with α 45 and α 0 are 20 and 10 respectively i e 10 reduction of rf for each 45 fracture rotation toward flow direction fig 11 shows the stabilized distribution of water and oil in the models with different α in the case of the fracture perpendicular to the flow direction fig 11 a general displacement pattern is similar to the non fractured case fig 9 a however it is observed that presence of a fracture with other orientation fig 11 b and c adversely affects displacement efficiency in agreement with the micro model experimental observations made by abedi et al 2012 farzaneh et al 2010 and sedaghat et al 2013 for example for the case with the fracture in the flow direction fig 11 c water just went through the fracture highway and resulted in an extremely early water breakthrough at pvi 0 1 fig 12 shows rf as a function of water pvi for the simulated models with different fracture dimensionless apertures in the range of 0 3 to 2 a non monotonic behavior is observed in fig 12 it is interesting to see that the case with wd 0 3 has the highest rf while the one with wd 0 65 has the lowest rf by increasing wd above 0 65 rf gradually increased but did not reach that of wd 0 3 this observation indicates that there is a critical fracture width wd 0 65 in the current model in which the medium rf is minimum in general variation of the fracture width in the studied range resulted in less than 10 difference in rf fig 13 demonstrates the stabilized distribution of water and oil in the models with different fracture apertures several loosely connected or disconnected flow paths can be seen in the model with wd 0 3 as the fracture aperture is in the range of the medium pore and throat size see fig 13 a although water channeling through the fracture is minor in the case with wd 0 3 it is affecting the displacement in those with wd 0 65 so the water channeling effect is the major cause of rf reduction as wd increases from 0 3 to 0 65 fig 12 as can be seen by comparison of fig 13 b d for wd 0 65 the size of the single active finger originated from the inlet at the bottom of the medium is relatively similar however the size of the fingers originated from the fracture enlarged by wd the cross flow out of the fracture and into the neighboring matrix pores enhanced by wd also the swept area of the fracture increased by wd therefore rf increased as the fracture was thickened from 0 65 to 2 fig 12 it is also observed that in the case of the wider fractures see fig 13 c and d some amount of oil is trapped inside the fracture both at the middle and at the bottom as the fracture width increases the trapped oil volumes become larger this effect is due to the water front instability fingering inside the fracture which is exaggerated as the fracture becomes wider for detail study of the fluid instabilities inside the fracture fluid displacement within the fracture for the medium with wd 2 is plotted in fig 14 at different instants during water invasion at the first instant pvi 0 1 it is observed that three small drops of oil have been formed water front is advancing inside the fracture however the displacement is not stable piston like it is bypassing some amount of oil due to low m log m 2 and the large fracture width the bypassed oil forms more droplets as can be seen in fig 14 at pvi 0 14 during displacement some of the formed oil droplets join together and form bigger drops pvi 0 17 and 0 2 it is very interesting to see the advancing and the receding contact angles inside the fracture for the bigger drop at pvi 0 17 and the single big drop at pvi 0 2 which are happened under the influence of water momentum within the fracture at a later time after pvi 0 2 the oil drop slightly grows and its shape is changed see fig 13 which may be due to the counter current displacement between the fracture and the neighboring matrix of course the effect of the fracture geometrical parameters including length width and orientation on displacement behavior is a function of ca and m combining the results obtained in sections 3 1 and 3 2 may suggest that increasing ca and m approaching stable displacement will reduce the effect of the fracture parameters however lowering m and ca may exacerbate these effects 3 3 effect of medium surface wettability to quantify the degree of wettability contact angle θ is usually studied which is a boundary condition in determining the interfacial shape brown and neustadter 1980 rabbani et al 2017 a grain surface is generally considered water wet if θ π 2 and oil wet if θ π 2 the influence of wettability is addressed in this section for water oil displacement in the fractured medium as log ca 4 6 log m 2 ld 28 and wd 0 65 it is important to note that when the medium is water wet θ π 2 water injection represents an imbibition process while when the medium is oil wet θ π 2 it represents drainage process fig 15 shows the fluid distributions after displacement stabilization for different θ values of 5π 12 π 2 and 2π 3 corresponding to slightly water wet neutral wet and slightly oil wet respectively as shown in fig 15 the general distribution of the water fingers is quite similar for all the studied wettabilities however as depicted in fig 15 a in the case of the water wet medium water preferentially tends to invade thinner pores and throats compared to the oil wet medium another important difference between these cases is the displacement stability inside the fracture area as can be seen in fig 15 c for the oil wet medium the fracture is fully swept by the invading phase capillary force in the porous matrix is negative for water in the case of the oil wet medium so water prefers to fully invade the fracture with lower capillary forces before drainage into the matrix however in the case of the water wet medium fig 15 a water flow within the fracture is unstable and hence some oil droplets were trapped in the fracture in this case water imbibition takes place between the fracture and the matrix in other words positive capillary forces in the porous matrix in the water wet medium resulted in imbibition of water from the fracture into the porous matrix it is also observed that the growth of the active finger at the bottom of the matrix increases as the medium becomes more water wet in the case of strongly water wet media it was observed that due to the considerable water imbibition rate between fracture and matrix the displacement profile and efficiency approached that of the non fractured medium this is in agreement with the simulation results of monteagudo and firoozabadi 2004 and experimental observations made by pooladi darvish and firoozabadi 2000 the detailed water displacement profiles inside the fracture versus time are depicted in fig 16 for the water wet medium as the interface advances through the fracture pvi 0 3 water imbibes to the matrix hence oil drops enter the fracture zone due to the counter current imbibition process in agreement with the results by rokhforouz and akhlaghi amiri 2017a b small oil drops expelled from the porous matrix into the fracture join together at pvi 0 44 and the formed larger droplets moves toward the end of the fracture pvi 0 58 this counter current imbibition process goes on during displacement process pvi 0 72 until stabilization however based on the previous studies rokhforouz and akhlaghi amiri 2017a b a different porosity and permeability as well as wettability of the matrix would affect the imbibition process and the resultant oil recovery factor rf increases as the medium becomes more water wet fig 17 it is also evident in fig 17 that in the case of the neutral wet medium the time gap between the primary breakthrough and the stabilization time is wider in both water wet and oil wet media flow is stabilized just after primary breakthrough theses means that the number of active fingers in the case of neutral wet medium is larger compared to the other verified wettability conditions fig 18 demonstrates two instants during water invasion process for three enlarged sections of the oil wet medium θ 2 π 3 the simulated model was able to successfully capture different pore scale mechanisms during the displacement process some pore scale displacement mechanisms including pore doublet interface coalesce snap off and reverse displacement were captured due to the capillary pressure effect the non wetting phase water preferentially invades the larger pores and the oil phase is bypassed in the smaller pores shown in fig 18 a with black circle water phase bridges between the adjacent pores and the interface coalesce happens specified with green circle in fig 18 a as specified in fig 18 b with black circle the water phase forms collars in the pore throat which snaps off the oil droplet the presence of concave convex and flat interfaces in the oil wet porous medium is also detected specified with black purple and red arrows respectively complex interplay between the contact angle and pore angularity causes the variation of the interface curvature in accordance with the theoretical results of mason and morrow 1994 fig 18 c shows a pore level reverse displacement as a result of pressure gradient decline across the throat in other words when the interface s curvature changes from convex to concave the interface is forced back into the pore throat against the direction of the main stream flow 4 conclusion this work evaluates the effects of micro fracture geometrical parameters on visco capillary behavior of two phase displacements compared with non fractured medium under different wettability conditions a heterogeneous pattern taken from a real dolomite rock section was considered as the computational domain to perform the simulations the coupled cahn hilliard phase field and navier stokes equations were solved by a finite element method using comsol multiphysics solver the values of mesh sizes and interface thickness parameter were set in order to achieve mesh convergence and model convergence first numerical experiments were performed on the simulated medium with without fracture to study the effects of viscosity ratios m and capillary numbers ca for both media the boundaries of stable displacement were identified at log m 0 and log ca 2 5 and the boundaries for viscous fingering and capillary fingering regions were located at log m 1 and log ca 4 5 respectively presence of the fracture in the fractured medium resulted in water channeling effect which caused reduction of the number of active fingers and hence reduction of the final oil recovery factor at log m 0 the fracture was fully swept by the injected water but flow instabilities were observed inside the fracture at lower m values especially when log ca 4 6 at high ca especially in the stable regime with log ca 2 5 and log m 0 recovery factor for the fractured medium was relatively identical with the non fractured one the effects of geometrical characteristics of fracture including length aperture and orientation were evaluated in the simulated models the presence of even a short fracture affected oil recovery factor it was observed that increasing the fracture length resulted in lower oil recovery factor due to earlier water breakthrough time caused by stronger channeling effect it was observed that there is a critical fracture width almost half of the medium average pore diameter at which the recovery factor of the medium during displacement is minimum compared to the media having thinner and thicker fractures minor channeling effect in the media with thinner fracture and larger fracture swept volume as well as high fracture matrix cross flow in the media with thicker fracture were detected as the main cause of this non monotonic behavior in the models with thick fractures with the thickness higher than the average pore diameter considerable trapped oil volumes were observed inside the fracture at lower m the fracture orientation showed the most impressive effect on oil recovery compared to the other studied fracture geometrical properties the oil recovery factor increased 20 when the fracture orientation changed from flow direction to the direction perpendicular to the flow the medium with perpendicular to the flow fracture behaved almost the same as the non fractured medium the effect of medium wettability was also studied the change of the medium wettability from oil wet to neutral or slightly water wet did not highly affect the displacement profile in the fractured medium however oil recovery factor was increased as the medium became more water wet although the fracture was fully invaded by water in the oil wet and neutral wet media flow instabilities caused by counter current imbibition were observed inside the fracture for water wet medium different pore scale events were captured for different simulated models for example pore doublet interface coalesce snap off and reverse movements were observed during the studied displacement in oil wet medium the presence of concave convex and flat interfaces were also detected the results of this numerical study confirmed the ability of the phase field model to realistically predict different pore level mechanisms during two phase displacements processes through highly sophisticated media containing fractures acknowledgment this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors 
