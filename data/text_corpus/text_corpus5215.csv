index,text
26075,coastal water supply infrastructure systems are exposed to saltwater intrusion exacerbated by sea level rise stressors to enable assessing the long term resilience of these systems to the impact of sea level rise this study developed a novel hazards humans infrastructure nexus framework that enables the integrated modeling of stochastic processes of hazard scenarios decision theoretic elements of adaptation planning processes of utility agencies and dynamic processes of water supply infrastructure performance using the proposed framework and data collected from south miami dade service area a multi agent simulation model was created to conduct exploratory assessments of the long term resilience of water supply infrastructure under various sea level rise scenarios and adaptation approaches the results showed the capability of the proposed model for scenario landscape generation to discover robust adaptation pathways for enhanced infrastructure resilience under uncertainty the analysis results could provide actionable scientific information to water infrastructure managers to improve their adaptation planning and investment decision making processes keywords infrastructure resilience coastal water infrastructure adaptive decision making adaptation planning sea level rise multi agent simulation abbreviations slr sea level rise swi saltwater intrusion cwsis coastal water supply infrastructure systems lri long term resilience index 1 introduction coastal states in the united states of america are faced with evolving stressors and hazards exacerbated by climate change impacts evolving coastal hazards create significant challenges to u s public safety u s geological survey 2014 because of the 25 most densely populated and rapidly growing u s counties 23 are along a coastline wilson and fischetti 2010 one of the most pressing coastal stressors is sea level rise there is clear scientific evidence that sea level is rising as a result of global warming ipcc 2007 the fifth assessment report of the intergovernmental panel on climate change reported that there has been an unequivocal upward trend in average global sea levels ipcc 2013 sea level rise slr impacts in coastal areas have been studied intensively over the last decades de almeida and mostafavi 2016 slr increases the frequency duration and magnitude of extreme weather events e g storm surge heavy precipitation flooding leading to greater exposure of different coastal infrastructure systems e g water wastewater energy transportation to chronic and acute stressors ghanbari et al 2019 in particular coastal water supply infrastructure systems cwsis are susceptible to slr impacts slr affects water infrastructure in various ways including accelerated degradation of underground utility inundation of low lying facilities contamination of groundwater through saltwater intrusion most water infrastructure systems are not designed to account for slr impacts and hence could not operate under these conditions one of the most serious impacts of slr on the coastal water supply infrastructure is saltwater intrusion werner and simmons 2009 the majority of coastal cities such as ones located in southeast florida use underground aquifers as their main source of freshwater supply heimlich et al 2009 with a constant discharge rate from groundwater wells and after a certain rise in sea levels saltwater would start to flow into the wells from the shores as saltwater intrusion swi moves inland due to rising sea levels the risk of freshwater contamination increases swi would happen in two ways i chronic swi into groundwater aquifer due to slr and ii storm induced swi due to storm surge events exacerbated by slr chronic swi occurs when the ocean hydraulic static pressure is higher than the underground aquifer s pressure which causes the saltwater freshwater interface zone salinity line located beneath the surface move inland over a long time werner and simmons 2009 the rise in sea levels is expected to increase the inland movement of the salinity line because as sea level continues to rise at faster rates the hydraulic pressure exerted by the seawater will also increase resulting in a higher lateral push that drives the salinity line inland werner and simmons 2009 storm induced swi often starts with overtopping of a coastal barrier followed by hinterland inundation and subsequent vertical seawater intrusion elsayed and oumeraci 2018 hurricanes and extreme storm events can cause storm surges that result in swi into coastal aquifers through vertical infiltration of saltwater elsayed and oumeraci 2017 yang et al 2013 in either way swi would affect the freshwater capacity of water supply infrastructure and thus the system s service reliability in meeting the potable water demand of service areas hence swi increases the groundwater salinity that either results in abandonment of contaminated wellfields or requires desalination of the saline water many cities such as miami and hallandale beach in florida have closed a number of their water wellfields due to the contamination of saltwater mdwasd 2014 several studies have investigated the impact measure of swi on cwsis dausman and langevin 2005 elsayed and oumeraci 2017 park et al 2011 prinos et al 2014 and many other studies have evaluated the adaptation and resilience of cwsis under these impacts de almeida and mostafavi 2016 bloetscher et al 2010 elsayed and oumeraci 2018 heimlich et al 2009 schoen et al 2015 a review of the existing literature shows that the steady state and ex post mostafavi 2018 analysis approaches do not provide robust insights for resilient adaptation planning in cwsis due to the uncertain nature of slr and swi impacts requiring more adaptive approaches to planning and decision making batouli and mostafavi 2018 chappin and van der lei 2014 in addition the existing approaches for evaluation of swi impacts on cwsis do not consider adaptation decision making behaviors of human actors e g agencies and users while different factors contributing to the physical impacts of swi on cwsis are well studied little is known about effective adaptation approaches in response to swi impacts such assessment would require consideration of uncertainty and complex interactions in hazards humans infrastructure nexus 1 1 resilience in hazards humans infrastructure nexus an important step towards formulating robust adaptation plans and investment strategies is to specify and characterize the long term scenarios related to cwsis performance under impacts of coastal hazards understanding of long term performance and resilience is essential because due to the significant physical and institutional inertia in water supply infrastructure systems undesirable performance regimes resulting from maladaptation are very difficult to reverse rasoulkhani et al 2017b in the context of complex infrastructure systems such as cwsis long term resilience is defined as the ability to transform and adapt to changes in internal dynamics to cope with chronic or acute hazards and thus sustain performance suitable for the social and economic development of communities park et al 2013 long term resilience is an emergent property that arises due to hazards humans infrastructure interactions and evolves over time rasoulkhani and mostafavi 2018 as shown in fig 1 a hazards humans infrastructure nexus framework encompasses the underlying mechanisms and their attributes and relationships that all shape and influence the long term resilience landscape of cwsis under swi impacts this framework which is based on a complex system approach captures three mechanisms underlying the long term resilience fig 1 i climatic hazards uncertain unfolding of evolving coastal hazards e g swi ii human actor decision making processes adaptation planning processes of utility agencies and iii physical infrastructure performance water supply infrastructure condition and performance the three mechanisms underlying the long term resilience of water supply infrastructure interact with each other in the context of this study slr increases the rate of chronic swi into the water supply infrastructure and also exacerbates the frequency and magnitude of storm surges which both lead to damages and losses in freshwater aquifers in addition the risk of swi would change decision making behaviors and priorities of utility agencies the uncertainty associated with swi affects the priorities of utility agencies to determine an optimal balance between adaptation investments and perception of potential future swi risks given resource limitations and future uncertainty adaptation investments of utility agencies would influence the exposure of physical infrastructure to swi threats accordingly these coupled impacts would determine the performance of water supply system and its ability to serve the intended demand at the desired level of service the specification and characterization of these mechanisms and their interactions would enable modeling various future scenarios of slr and swi to examine the effectiveness of different adaptation strategies 1 2 robust adaptation planning to moderate the potential impacts of coastal hazards e g swi on cwsis planning and implementation of effective adaptation strategies is essential aerts et al 2014 adaptation involves anticipating the adverse effects of hazards and implementing appropriate strategies and measures to reduce the adverse impacts of hazards on coastal communities ipcc 2013 understanding the characteristics of robust adaptation planning holds the key to avoid maladaptation in cwsis maladaptation refers to failure to change behaviors and undertake appropriate strategies such that the infrastructure systems on which a society is depended become unable to provide the required service aerts et al 2014 maladaptation may occur due to failure to take timely actions or taking actions that would be difficult to reverse devising robust adaptation plans and investments is contingent upon evaluation of the long term resilience of infrastructure systems under different adaptation strategies and hazard scenarios batouli and mostafavi 2018 although the existing literature related to robust decision making involves frameworks e g deyle and butler 2013 haasnoot et al 2013 kwakkel et al 2016 lempert et al 2006 shortridge et al 2017 shortridge and zaitchik 2018 for adaptation planning and evaluation of climate change impacts the existing frameworks are not developed specifically to specify and evaluate the characteristics of robust adaptation planning and investment decision making in cwsis under swi impacts to provide insights for robust adaptation strategies in the context of cwsis an exploratory analysis approach is required to examine different adaptation decision making attributes such as hazard perceptions risk attitudes decision intervals investment levels and signposts e g swi proximity exploratory analysis has been proposed as an effective approach for dealing with uncertainty and complexity in climate change adaptation bankes 1993 kwakkel and pruyt 2013 exploratory analysis involves analytical model based methods for decision making under uncertainty model based exploratory analysis uses computational models and simulation experiments to conduct scenario analysis and evaluate the behavior patterns in complex systems such as cwsis agusdinata 2008 bankes 2002 mostafavi 2018 mostafavi et al 2013 unlike traditional simulation approaches exploratory analysis does not aim to predict the behavior of a system for optimizing the system instead exploratory analysis focuses primarily on considering different scenarios based on changes in system behavior and future uncertainty to evaluate the robustness of different pathways of actions under deep uncertainty the present study utilized a model based exploratory analysis to describe how fundamental decision making attributes risk response behaviors and physical infrastructure realities affect the adaptation pathways in response to swi impacts to this end a dynamic time dependent multi agent simulation model was created to capture the attributes and interactions among costal hazards slr and swi adaptation decision making processes risk attitudes investment levels decision intervals and water supply infrastructures exposure condition performance accordingly the study examined the long term scenarios emerging from interactions in hazards humans infrastructure nexus to devise robust adaptation strategies that could mitigate subsequent impacts of swi and improve resilience in cwsis the analysis results facilitated better understanding of the coupled effects of the uncertain coastal hazards and adaptation decision making processes on the long term resilience of cwsis 2 methodology 2 1 multi agent simulation model multi agent simulation is an effective simulation approach for analyzing complex processes and interactions in coupled human infrastructure systems batouli and mostafavi 2018 davidsson 2001 monticino et al 2007 ralha et al 2013 rasoulkhani et al 2019b multi agent simulation enables modeling complex and real world systems through the adoption of influential concepts such as adaptation emergence and self organization al zinati et al 2013 rasoulkhani et al 2018 in multi agent simulation an agent has several essential characteristics active initiating actions reactive responding to external stimulus and autonomy freedom from intervention by any other agents grignard et al 2014 many entities within an infrastructure system can be viewed and modeled as an agent esmalian et al 2019 hence multi agent simulation is suitable approach to model entities and their interactions in hazards humans infrastructure nexus the created multi agent simulation model integrates i stochastic processes related to slr stressors i e swi and storm surge based on the data obtained from previous studies pertaining to future projections of slr ii decision theoretic elements of utility agency s adaptation decision making processes based on theories of bounded rationality and regret aversion and iii dynamic processes of water supply infrastructure exposure and performance under swi impacts the computational representation of the simulation model was developed in an object oriented java based simulation platform i e anylogic 8 the relationships among agent classes and their attributes are based on the empirical information and grounded theories e g bounded rationality regret aversion the agent classes and their attributes and functions are shown in the unified modeling language uml diagram see fig a1 in appendix the model contains three main classes of agents hazard agent human actor agent and infrastructure agent the mathematical representation of each agent their attributes and relationships are discussed below 2 1 1 hazard agent the hazard agent represents the stochastic processes related to two coastal hazards induced by slr including swi and storm surges both of which evolve and are exacerbated due to slr the higher the sea level rises the faster the chronic swi moves due to a great deal of uncertainty in projection of future slr the intergovernmental panel on climate change and the southeast florida regional climate change compact have recommended consideration of a number of alternative scenarios for different trajectories of future slr compact 2015 ipcc 2013 based on their recommendations three scenarios i e low medium and high sea levels were considered in this model based on the ipcc 2013 and the compact 2015 the agencies in southeast florida are utilizing these three scenarios for adaptation planning and decision making in order to evaluate the long term chronic swi it is important to understand how different scenarios of slr lead to swi however the extent of swi not only is affected by slr severity but also depends on a large number of physical and hydraulic parameters such as recharge water table fluctuations hydraulic conductivity soil porosity and mixing properties la licata et al 2011 hence three different possible rates of chronic swi were considered under each slr scenario i fast swi ii moderate swi and iii slow swi simulation of contaminant transport such as swi in coastal aquifers is intrinsically complex and computationally expensive because of the complex flow patterns that develop when freshwater mixes with saline groundwater elsayed and oumeraci 2018 la licata et al 2011 however several numerical models have been specifically developed to evaluate the effect of sea level fluctuations on swi in this study the results obtained from the existing numerical groundwater models in southeast florida were used to determine the rate of swi into the wellfields fig 2 dausman and langevin 2005 fitterman 2014 prinos et al 2014 the information related to different swi scenarios are summarized in fig 2 with this information the salinity line movement chronic swi in the coastal aquifer was determined and then used to compute the year in which each wellfield gets exposed under different slr and swi scenarios over an extended 100 years of the simulation period the chronic swi could be exacerbated due to storm surges in fact storm surge events can accelerate swi and as a result temporarily further push the salinity line towards aquifers park et al 2011 therefore in the hazard agent the occurrence of storm surge events and the magnitude of their immediate impact on swi which is called secondary or immediate impact were incorporated during the analysis period e g 100 years the likelihood and magnitude of storm surge occurrence vary based on the severity of slr the occurrence of storm surge events was modeled using a homogenous stochastic process poisson model as shown in equation 1 according to katz 2010 the poisson process is an effective statistical technique used for analysis of the frequency of occurrence of an extreme event such as hurricanes this technique assumes a linear trend in the logarithm of the rate parameter of the poisson distribution katz 2010 1 p s t o r m s u r g e p s t o r m s u r g e s l r s c e n a r i o λ k e λ k λ e λ where λ is the mean rate of storm surges per year likelihood of having one storm surge event at each year the λ values were determined based on the available data existing in published studies park et al 2011 tebaldi et al 2012 zhang et al 2013 pertaining to the increased frequency and magnitude of surge events in southeast florida accordingly the values of 0 15 0 20 and 0 25 were assigned to λ under low medium and high slr scenarios respectively the mechanics of storm induced swi through surface infiltration or open pits is highly complex and non linear due to the unsteady nature of the stratification denser seawater over less dense freshwater and the significant spatial and geologic inhomogeneities park et al 2011 due to the complexity and high diversity of the involved processes and interactions gingerich et al 2017 the extents of swi caused by storm surges have not been precisely quantified miami dade county 2016 hence in this study the results and insights of existing models that couple the surge statistics with swi to predict the extent of saline infiltration elsayed and oumeraci 2018 la licata et al 2011 yang et al 2013 2018 yu et al 2016 were utilized to approximate the possible extents of secondary swi induced by ss accordingly the following stochastic uniform distributions were defined for the extents of storm induced swi in meter under different slr scenarios equation 2 2 s e c o n d a r y s w i u n i f o r m 30 60 s l r l o w u n i f o r m 60 90 s l r m e d u n i f o r m 90 120 s l r h i g h finally the processes represented in the hazard agent enable integrating the storm induced swi with the chronic swi obtained from fig 2 and calculating the total swi accordingly every year i e the model time step the proximity of the salinity line to each wellfield is determined based on the wellfield s initial distance from the salinity line and the total swi happened until that year equation 3 3 p r o x i m i t y t i n i t i a l d i s t a n c e t o t a l s w i t subsequently the actual and perceived exposure of wellfields to the risk of swi are determined in the human actor agent as explained in the following subsection 2 1 2 human actor agent the human actor agent captures the underlying behavioral factors and decision rules that influence adaptation decision making processes of the utility agency which manages and operates the water wellfields and treatment plants two planning approaches were considered i adaptive planning in which the decisions related to management of the infrastructure are made based on consideration of the future impacts of swi and the agency implements adaptation actions before swi affects the infrastructure and ii reactive planning or no adaptation in which the decisions would be non adaptive and are made without considering the uncertain swi impacts in this approach the agency waits until an infrastructure is affected by swi and then would implement recovery actions such as well relocation or desalination facility installation the adaptation and recovery decision making behaviors of the utility agency were modeled using the action chart shown in fig 3 the details about the processes related to these two planning approaches are discussed in the remainder of this section 2 1 2 1 adaptive planning this study adopted the theory of bounded rationality to model the rational behavior of individuals under certain limitations based on the theory of bounded rationality decision makers who are bounded by imperfect information confined time and limited capacity seek satisfactory solutions rather than optimal solutions simon 1972 the adaptation decision making processes of utility agencies represent all the traits of bounded rational decision making first there is a significant level of uncertainty in the available information about future slr and its impacts second utility agencies are bounded to taking adaptation actions at certain points of time known as decision points when a multiyear capital improvement program is devised wooldridge et al 2002 third all adaptation actions of a utility agency are contingent upon availability of capital improvement funding batouli and mostafavi 2018 the modeling elements of the adaptive planning process are explained as follows 2 1 2 1 1 decision intervals the adaptation decision making process is adaptive in nature fig 4 depicts the processes leading to adaptation decisions of the utility agency the entire period of a capital adaptation plan which is 100 years in this study is usually divided into a number of time intervals and the utility agency makes adaptation decisions at certain decision points the time period between two successive decision points t i 1 t i is referred to as decision interval after the utility agency implemented the adaptation decision at a decision point t i they would observe the actual changes in the state of nature i e slr and swi before taking actions in the next decision point t i 1 at the next decision point the actual slr and swi as well as the risk attitude of the agency is updated based on the new information that has become available the entire process is then repeated for next decision horizons to make new adaptation decisions 2 1 2 1 2 hazard perception the imperfect information about future slr and swi causes the utility agency to make adaptation decisions based on its perception of the future hazards the perception of the agency is influenced by its risk attitude towards likely impacts of hazards since the agency is unsure what scenario of slr and swi would happen in the future they rely on their own perception to perceive the swi among the alternative swi projections hence the perceived swi of the utility agency may be different from the actual swi to model this three different hazard perceptions h including optimistic o most likely m and pessimistic p were defined as shown in equation 4 4 h o s l o w s w i l o w s l r s t o r m s u r g e l o w s l r m m o d s w i m e d s l r s t o r m s u r g e m e d s l r p f a s t s w i h i g h s l r s t o r m s u r g e h i g h s l r 2 1 2 1 3 risk attitude the agency s hazard perception for a decision horizon from t i to t i 1 is determined based on its risk attitude at the beginning of the decision horizon i e at decision point t i the agency s risk attitude determines the decision maker s preference among uncertain outcomes of adaptation decisions the agency could be either risk seeker risk neutral or risk averse at the beginning of the adaptation planning period however its risk attitude may change at each decision point and get updated for the following decision interval this is because risk attitude is formed based on past experience reference point and observations rather than being based on fully rational analytical models leiserowitz 2006 equation 5 summarizes the representation of the agency s risk attitude in the model at each decision point in fact based on the agency s observation during the past decision interval i 1 more updated information becomes available at decision point t i and then the agency would become more less risk seeking if the perceived swi for the past decision horizon is greater lower than the maximum minimum actual swi happened during that decision horizon equation 5 5 if p s w i i 1 a s w i i 1 m i n t h e n b e c o m e m o r e r i s k a v e r s e a t t i m e s s a g e 1 p s w i i 1 a s w i i 1 m a x t h e n b e c o m e m o r e r i s k s e e k e r a t t i m e s s a g e 2 p s w i i 1 a s w i i 1 m i n a s w i i 1 m a x t h e n k e e p t h e c u r r e n t a t t i t u d e a t t i where p s w i i 1 and a s w i i 1 denote the perceived swi and the actual swi during the decision horizon of i 1 respectively accordingly prior to each decision point the utility agency transitions between the risk attitude states based on the message received from equation 5 for example if the agency was risk seeker at the previous decision point t i 1 and observes that the perceived swi is lower than the minimum actual swi happened during the following decision interval i 1 then the agency s risk attitude would change to risk neutral at decision point t i 2 1 2 1 4 signposts signposts specify information that should be tracked in order to determine whether the adaptation action implementation is needed haasnoot et al 2013 the critical values of signpost variables are called triggers beyond which adaptation actions should be implemented haasnoot et al 2013 in this model the distance of swi from wellfields is the signpost based on which the adaptation implementation is determined as shown in the agency s action chart fig 3 if a well has not been contaminated yet its exposure to swi is being evaluated once the risk attitude of the agency and the hazard perception were specified at a decision point the exposure of each wellfield to the perceived swi scenario during the following decision horizon is determined using equation 6 to determine the exposure of wellfields the utility agency performs conditional logic testing based on the perceived swi proximity calculated in equation 3 the agency would consider a proximity threshold as a trigger value which is the minimum distance of swi from the well that the agency would tolerate if a well is located at a distance greater than the proximity threshold from the perceived salinity line it won t be considered at risk for the following decision horizon however if the distance is equal or less than the threshold for the following decision horizon then the well is considered at risk the utility agency can adopt different values of proximity threshold for example if the agency s proximity threshold is 30 m then the agency would consider a well at exposure when the proximity of perceived swi to the well is less than 30 m 6 e x p o s u r e t p r o x i m i t y t t h r e s h o l d 2 1 2 1 5 adaptation actions based on consideration of the exposure of wells to perceived swi if no wells are identified to get exposed to perceived swi the agency does not implement any adaptation actions and proceeds to the next decision point if one or more wells are identified to potentially get exposed to swi the next step of adaptation decision making is to select appropriate adaptation actions in this model four different adaptation actions were considered the adaptation action space includes the following adaptation actions i aquifer recharge implementing deep well injection to control groundwater levels ii salinity barrier installing seepage barrier to protect wellfields iii well relocation closing an exposed contaminated wellfield and exploiting new wellfield farther from the salinity interface location and iv desalination facility adding desalination capacity to the treatment plant each adaptation action has different cost and effect on the water supply system as summarized in table 1 since there is uncertainty related to cost of swi adaptation actions pert probability distributions are defined based on the min most likely and max values reported in previous studies and reports see the last column of table 1 2 1 2 1 6 choice modeling the agency s choice among different adaptation alternatives was modeled based on the theory of regret aversion bell 1982 recent research in behavioral psychology and judgement under uncertainty reveals that regret aversion provides a better model of human choice under uncertainty compared to the standard utility theory since regret can be anticipated prior to choice it can lead to regret minimizing decisions humphrey 2004 according to this theory the risk attitude of the decision maker determines what adaptation action to be selected batouli and mostafavi 2018 as shown in fig 5 a risk seeking agency feels optimistic about the climatic hazard scenario and would select the alternative that results in highest performance under the best case scenario of hazard on the other hand a risk averse agency would want to maximize the system performance under the pessimistic hazard perception worst case scenario finally a risk neutral agency would anticipate the possibility of feeling regret after the uncertainty of future hazards is resolved hence a risk neutral agency would select the alternative that yields the least regret across all hazard scenarios i e optimistic most likely and pessimistic perception to capture the adaptation alternative selection process at each decision point the model determines the planned anticipated performance of the infrastructure system based on the measure of planned level of service the planned level of service p l s t i that the agency anticipates at decision point t i for the following decision horizon based on the implementation of adaptation actions is calculated using equation 7 7 p l s t i t i t i d s u p p l y c a p a c i t y t p r o j e c t e d d e m a n d t where d denotes the duration years of decision intervals supply capacity and projected demand are parameters driven from the infrastructure agent explained in next subsection accordingly the model generates a matrix related to the planned level of service resultant of each adaptation action j under optimistic most likely and pessimistic hazard perceptions h as shown in equation 8 8 p l s j h p l s 1 o p l s 1 m p l s 1 p p l s 2 o p l s 2 m p l s 2 p p l s 3 o p l s 4 o p l s 3 m p l s 4 m p l s 3 p p l s 4 p where rows represent adaptation alternatives and columns represent hazard perceptions for instance p l s 1 o denotes the planned level of service that the agency anticipates for system if adaptation action 1 is implemented given the optimistic hazard occurrence to select an adaptation action to be implemented at each decision point if necessary based upon the exposure the following process is followed if the agency is risk seeker risk averse then the adaptation action a j that results in p l s m a x o p l s m a x p will be selected as shown in equation 9 9 s e l e c t a j i f p l s j o max p l s 1 o p l s 2 o p l s 3 o p l s 4 o r i s k a t t i t u d e r i s k s e e k e r p l s j p m a x p l s 1 p p l s 2 p p l s 3 p p l s 4 p r i s k a t t i t u d e r i s k a v e r s e in case the agency is risk neutral the model also constructs the regret matrix by subtracting the planned level of service achieved under each adaptation action from the maximum possible planned level of service that could be achieved under the adaptation actions equation 10 in equation 10 r e g j h denotes the regret of selecting adaptation action j if hazard h happens 10 r e g j h p l s m a x o p l s 1 o p l s m a x m p l s 1 m p l s m a x p p l s 1 p p l s m a x o p l s 2 o p l s m a x m p l s 2 m p l s m a x p p l s 2 p p l s m a x o p l s 3 o p l s m a x o p l s 4 o p l s m a x m p l s 3 m p l s m a x m p l s 4 m p l s m a x p p l s 3 p p l s m a x p p l s 4 p accordingly the maximum regret related to selecting adaptation action j m a x r e g j is determined using equation 11 11 m a x r e g j max r e g j o r e g j m r e g j p finally the risk neutral agency will select the adaptation action that has the lowest maximum regret minimax regret as shown in equation 12 12 s e l e c t a j i f m a x r e g j min m a x r e g 1 m a x r e g 2 m a x r e g 3 m a x r e g 4 in the adaptation alternative selection process for each risk attitude if there are more than one adaptation action with the same outcome i e highest performance or least regret the agency would select the less expensive one implementation of the selected adaptation actions is contingent upon sufficiency of the available budget in this model the utility agency was assumed to be given a capital improvement fund for the entire 100 years in order to implement adaptation actions mdwasd 2014 due to the uncertainty pertaining to climatic hazards the capital fund should be spent across various decision intervals in this study the capital fund is equally distributed among the decision intervals and thus the available budget b t i for the agency at each decision point is determined based on equation 13 13 b t i b t i 1 c a p i t a l f u n d 100 d where b t i 1 denotes the reminder of budget from previous decision point and d denotes the duration of adopted decision intervals based on the decision making and choice processes discussed above the adaptation decision making behaviors of the utility agency were captured during a 100 year analysis horizon it should be noted that in adaptive planning if a well gets contaminated for any reason the agency will implement recovery actions as discussed in the remainder of this subsection 2 1 2 2 reactive planning in order to better understand and examine the effectiveness of adaptation planning a reactive planning approach was also considered in the model reactive approach is based on responding to events after they have happened in reactive planning the agency monitors the actual condition of infrastructure systems and implements recovery actions when a failure or performance drop happens due to hazards the reactive planning behavior of the agency was modeled as shown in the action chart fig 4 in reactive planning two different recovery actions were considered well relocation and desalination facility the difference between these two actions in reactive planning with the ones in adaptive planning is that there would be a lag delay in recovering from the contaminated wellfield and thus to retrieve the system s level of service under reactive planning strategy when a well is contaminated the utility agency first examines whether the well relocation can be done based on the relocation limit that each well has it was assumed that each well could be relocated only for a certain number of times accordingly if the well could be relocated the agency would close the contaminated wellfield and exploit new wellfield with a farther distance from the saltwater interface location otherwise the agency would install desalination facility to treat the saline water extracted from contaminated wells the outcomes of the human actor agent include the timing and type of adaptation recovery actions these outcomes are used in the infrastructure agent to model changes in the performance of water supply infrastructure system based on adaptation recovery actions 2 1 3 infrastructure agent this agent class captures behaviors of the water supply infrastructure under the coupled impacts of swi and the utility agency adaptation recovery actions this study considered two key components of water supply infrastructure systems water wellfields and water treatment plants however the focus was on modeling the wellfields two sets of processes were captured i physical condition the structural capacity and ability of infrastructure to withstand swi and ii functional performance the infrastructure ability to provide intended demand at the desired level to capture these processes various attributes of wellfields and treatment plants were considered attributes of water wellfields included location distance from the salinity line source aquifer number of wells and installed design capacity the treatment and desalination capacities of the treatment plants were also considered each year the simulation model checks all wells based on the proximity of swi to each well and accordingly the well contamination status is specified using equation 14 14 w c s k t 1 i f p r o x i m i t y k t 0 0 i f p r o x i m i t y k t 0 where w c s k t is the contamination status of well k at year t and p r o x i m i t y k t denotes the proximity of swi interface to well k at year t which is calculated by equation 3 in hazard agent once a well is in contaminated status based on the outcome of utility agency s decision making process either the well may be relocated or extra desalination capacity may be added to the system at the beginning of the simulation the system does not include any desalination capacity accordingly the performance of water supply system is evaluated based on the supply capacity which is the amount of water that the system can supply to the service area the annual water supply capacity of the system is calculated based on the extraction capacity of not contaminated wells e c capacity of treatment plants t c and the amount of desalinated water d w as shown in equation 15 15 s u p p l y c a p a c i t y t min w e l l s e c p l a n t s t c d w in equation 15 desalinated water d w is the amount of water demand that could not met due to contaminated well s so the utility agency would need to desalinate this amount of water if there is no well contaminated then d w is equal to zero also d w can t be greater than the desalination capacity of the plant the water demand imposed to the system is projected based on the population of service area as the population of coastal service areas grow wilson and fischetti 2010 the water supply infrastructure system has to possess the capacity to meet the increasing water demand hence this model captures the dynamic changes in the level of demand imposed to the system over the 100 year analysis period equation 16 represents how the projected demand of every year is calculated based on the population of the service area 16 p r o j e c t e d d e m a n d t p o p c 365 1 p o p g t where p o p c and p o p g denote the base population the daily per capita water consumption and the population growth rate respectively the resilience of the water supply infrastructure system was determined based on a measure called level of service which captures the reliability of water supply to meet the demand the level of service l o s t is calculated using equation 17 17 l o s t s u p p l y c a p a c i t y t p r o j e c t e d d e m a n d t 100 where l o s t values between 0 and 100 as if the supply capacity is greater than the demand the model equalizes supply with demand accordingly the long term resilience index l r i of the system is computed based on equation 18 18 l r i 1 100 1 100 l o s t the greater l r i indicates less loss in the levels of service in the system over the long period of time i e 100 years 3 case study the application of the proposed framework was illustrated to study the adaptation of coastal water supply infrastructure system to impacts of slr and swi in miami dade county fl for over a 100 year analysis horizon the hydrologic and groundwater conditions in southeast florida have been altered significantly due to climate change impacts that the management and operation of water supply infrastructure has become a complex job for the local utility agencies sfwmd 2018 south miami dade area formerly known as the rex utility district is specifically vulnerable to the impacts of slr because it is located in the lower section of miami dade county as seen on the map in fig 6 this area has been experiencing swi in the last couple of decades dausman and langevin 2005 fitterman 2014 prinos et al 2014 the multi agent simulation model developed in this study was built using data and information related to the south miami dade water service area these data and information were collected mainly from the findings of scientific researches e g dausman and langevin 2005 fitterman 2014 prinos et al 2014 and reports e g mdwasd 2014 of local institutions and governmental agencies which are in charge of the water utilities of the region the water supply infrastructure system located in south miami dade area is managed by miami dade water and sewer department mdwasd and is serving mainly the southern part of miami dade county based on the 20 year water supply facilities work plan reported by mdwasd in 2014 this service area has a population of 98690 with a growth rate of 0 88 and an average daily per capita water consumption of 0 5 m3 mdwasd 2014 based on the 20 year water supply facilities work plan mdwasd has planned to consolidate their existing treatment plants and the associated wellfields by construction and operation of south miami heights water treatment plant of the five existing water treatment plants and their individual supplying wellfields in the area only two plants remain in service on a stand by basis after the proposed plant begins operations mdwasd 2014 the information of the three anticipated wellfields in the proposed south miami heights water treatment plant is shown in fig 6 4 model verification and validation to ensure the quality and credibility of the developed multi agent simulation model verification and validation processes were conducted simulation models especially agent based models related to human systems are often criticized for relying on informal and subjective validation or no validation at all in this study a gradual systemic and iterative process was employed to conduct a thorough verification and validation of the simulation model 4 1 internal and external verification various internal and external verification techniques were employed to verify the data logic and computational algorithms related to the simulation model bankes and gillogly 1994 the internal verification of the model was ensured using grounded theories e g bounded rationality and regret aversion for modeling decision making behavior processes furthermore valid empirical data e g infrastructure attributes system demand etc were employed for modeling the infrastructure condition and performance indicators in addition for each component of the model component verification assessment was conducted to verify the completeness coherence consistency and correctness 4cs of the component pace 2000 for instance the model performance was observed under i taking off the function of one component of the model and making sure it influences the outputs to the degree that is specified in the model and ii running the model with extreme values of each component and verifying the functionality of the model under the extreme condition also several random replications of the model were compared to check for the consistency of the outputs accordingly most of the discovered errors had less to do with problems within the theories or empirical rules and more regarding issues with coding correctly the external verification of the model was ensured by building the model rich in causal factors that were examined to see what leads to particular outcomes bharathy and silverman 2010 fig a2 in the appendix shows a screenshot of the simulation output interface where the behavior regime of various outputs pertaining to different entities of the model can be observed the behaviors of model entities e g risk attitude adopted adaptation actions salinity line movement etc were followed so as to identify unusual patterns whenever an unusual pattern was observed the model logic was checked to ensure that the behavior was not due to unreasonable assumptions or imperfect logics 4 2 face validation to further ensure the model credibility and defensibility an iterative participatory face validation was conducted by institutional agencies involved in adaptation processes in the case study region face validation is conducted by having users and people knowledgeable with the system examine the model logics and outputs for rationality rasoulkhani et al 2019a according to carson 2002 a simulation model that has face validity appears to be a reasonable imitation of a real world system to people who are knowledgeable of the real world system therefore the simulation model its components and the preliminary results were presented to four verified subject matter experts who are specialist and seasoned in the area of coastal water infrastructure adaptation and resilience the subject matter experts involved in the validation process were from different affiliations that are partners of the resilient utility coalition the resilient utility coalition provides leadership in assessing utility operations to address the potential effects of climate change and seeks to enhance the usefulness of climate science by developing adaptation strategies and improving water management decision making in the face of climate uncertainty table a1 in the appendix provides information of the subject matter experts who participated in the face validation process after each presentation the subject matter expert was asked to evaluate the simulation model elements including conceptual model computational model data and outputs a questionnaire survey was given to and completed by the subject matter experts to evaluate different features of the model see table a2 in appendix on a scale of 1 5 in which 1 and 5 respectively represented the lowest and highest levels of validity and quality as shown in table a2 on average the subject matter experts evaluated the simulation model with scores above 4 based on these evaluations all the subject matter experts asserted that the behavior of the model is consistent with the real world and the relationships between different components of the model are realistic they also indicated that the simulation and visualization component of the model provides a useful tool for scenario analysis and decision making in the assessment of coastal water supply infrastructure adaptation and resilience under the impacts of slr in addition some insightful inputs constructive comments and minor modifications were suggested by the subject matter experts and were accordingly addressed on the simulation model 5 simulation experiments after verification and validation the simulation model was used for experiment design and analysis various possible scenarios were determined based on changing the values of input parameters into the model fig 7 each of the two planning strategies adaptive and reactive was analyzed across different scenarios details related to which parameters were used in devising scenarios are presented in table 2 through the combination of various values of the input parameters 10 692 and 4 158 scenarios were generated for adaptive and reactive planning strategies respectively for instance the combinations of the adaptive planning scenarios reflect changes in the actual slr and swi as well as in the agency s initial risk attitude available capital funding decision interval and swi proximity threshold due to the stochastic nature of the simulation model a monte carlo experiment was conducted for each specific scenario to determine the mean value of the model output variable the main output variable targeted in this study was the system long term resilience index lri each experiment was replicated as many times as the mean value of lri reached 95 confidence interval 6 results and discussion the simulated output data i e lri obtained from the experiment scenarios were used through statistical methods for exploratory analysis of long term resilience in coastal water supply infrastructure systems cwsis under the coupled impacts of saltwater intrusion and utility agency decision making processes the exploratory analysis results are threefold i assessing the significance of underlying determinants of long term resilience in hazards humans infrastructure nexus ii comparing the effectiveness of adaptive and reactive planning approaches and iii determining the characteristics of robust adaptation decision making 6 1 resilience determinants the first set of analyses focused on analyzing the scenario landscape to identify the most significant determinants of long term performance of water supply infrastructure in the study region to this end chi square automatic interaction detection chaid technique was used for explaining the impact of different system attributes as well as for generating various scenario pathways i e the combination of various attributes leading to a certain outcome chaid is a non parametric data mining tool used to discover the relationship between variables its algorithm incorporates a sequential merge and split procedure based on a chi square test statistic kass g v 1980 chaid analysis builds a meta model to help determine how variables best merge to explain the outcome in the given dependent variable magidson and vermunt 2005 therefore chaid technique was used to identify among a number of variables the most important variables in determining the outcome variable i e lri fig 8 shows the importance level of different exploratory variables in the simulation model and their effect on the lri of the cwsis under adaptive and reactive planning strategies the results presented in fig 8 show that in reactive planning the top two determinants of the system lri were actual swi and actual slr however under adaptive planning strategy the top two determinants were actual slr and capital funding level thus overall the hazard i e slr and swi was the most significant determinant of the system lri in both adaptive and reactive planning approaches this means that regardless of the decision making factors and behavioral attributes of the utility agency the climatic hazards would significantly influence the long term resilience in cwsis nevertheless the adaptive planning strategy was shown to be able to considerably reduce the influence of actual swi on lri this is because under adaptive planning the utility agency is able to update the information and make adaptive decisions according to observations of swi rates and hence select appropriate adaptation actions to mitigate the swi impacts 6 2 adaptive vs reactive approach the second set of analyses examined the effectiveness of adaptation planning approach in enhancing the long term resilience of cwsis the simulated lri obtained from thousands of scenarios were used to compare the system performance under adaptive and reactive approaches fig 9 demonstrates the best probability distributions that fitted to the simulated lri values resulted from various scenarios using the chi square goodness of fit test the best probability distribution fitted for lri values for reactive planning scenarios was a triangular distribution this is mainly because under this strategy the key drivers of lri were swi and slr which both had three possible values e g slow moderate and fast swi this result further confirms that in reactive planning approach the variables related to climatic hazards are dominant factors influencing lri however the best probability distribution fitted for lri values under adaptive planning scenarios was extreme value distribution which belongs to the exponential family e g normal weibull this type of distribution is frequently encountered in the context of lifetime reliability modeling this type of probability distribution demonstrates that failures in system lri could be controlled when adaptation planning was implemented as shown in fig 9 the probability of achieving greater lri in the system varies in adaptive and reactive planning the likelihood of achieving lri greater than 90 is about 94 under adaptive planning approach and 20 under reactive planning also under the reactive planning strategy there is a likelihood of 48 that the system lri would drop below 80 however this likelihood is about zero in adaptive planning scenarios thus the results show that if the utility agency implements adaptation actions i e adaptive planning strategy the chance that the system will achieve a greater lri is significantly greater than when the agency follows a reactive approach as the chaid analysis results suggested the level of capital funding was one of the influential determinants of lri especially in adaptive planning hence further analysis was conducted to specify the sensitivity and effectiveness of adaptive and reactive planning strategies under different levels of capital funding fig 10 and fig 11 depict the impact of capital funding on the system lri under various slr scenarios in the reactive planning and adaptive planning respectively the results presented in these figures first indicate that the capital funding does not impact the system lri if low slr happens however the level of capital funding is considerably influential under high slr hence by increasing the capital funding level the system lri could improve under high slr in addition figs 10 and 11 demonstrate that there is a critical level of capital funding required to mitigate the impact of high slr in both adaptive planning and reactive planning critical capital funding levels are the tipping points after which the system lri is no longer improved by increasing the capital funding as shown in fig 10 if a reactive planning strategy is implemented the capital funding of 1200 million which is for the entire 100 years and equals to 120 per capita per year would mitigate the impact imposed by high slr and would lead to lri of higher than 85 in average however under the adaptive planning strategy fig 11 the critical level of capital funding that mitigates the impact of high slr is 2000 million this amount of capital fund which is equal to 200 per capita per year would yield an average lri above 90 in the system although the critical level of capital funding in reactive planning is 40 less than the one in adaptive planning the results indicate that the adaptive planning is more effective due to the following reasons first under adaptive planning the critical capital funding can fully cover the lri gap induced by slr uncertainty the lri gap is the difference between mean lri under high slr and mean lri under medium or low slr however the reactive planning is not able to fully mitigate this gap as shown on fig 10 under the critical level of capital funding or any amount above that in reactive planning there is a gap between the lri under high slr and the lri under low and medium slr however this gap does not exist in adaptive planning where the system can maintain its lri under high slr at the same level as low and medium slr second under lower levels of capital funding for example 600 million the adaptive planning strategy enables achieving greater lri in comparison with the reactive planning strategy therefore if the utility agency encounters funding constraints the adaptive planning approach is more effective as it would lead to greater lri 6 3 characteristics of robust adaptation planning the third set of analyses evaluated different adaptation decision making attributes of the utility agency to better understand the characteristics of adaptation pathways that yield in improved long term resilience of cwsis one of these characteristics is the risk attitude of utility agencies although in adaptive planning approach the agency was able to update its risk attitude based on observation of the actual swi happened in the past the influence of the initial risk attitude on the system lri was examined fig 12 shows the influence of initial risk attitude under different slr scenarios given the critical level of capital funding is available the results presented in fig 12 indicate that the initial risk attitude of the agency was not influential on changing the system lri especially under low and medium slr even under high slr the risk averse attitude improved the lri very slightly less than 1 unit this is because the agency was able to update its risk attitude through the adaptive decision making process in which periodic updates made at certain decision points throughout the entire period of capital plan therefore the flexibility to adjust upon the updated information mitigates the influence of the agency s initial risk attitude which might not be a right attitude for planning on the effectiveness of adaptation pathways another characteristic examined in this study is the duration of decision intervals in adaptation planning in capital adaptation planning the entire period of capital plan is divided into a number of decision intervals at which investment decisions are made and implemented fig 13 depicts the influence of the duration of decision intervals on the system lri under various ranges of available capital fund the results show that the duration of decision intervals can be influential when the available capital funding is low less than 1000 million when funding is not sufficient shorter durations i e less than 10 years would result in decreased lri in the system however longer decision intervals especially 35 years can result in greater lri since the capital fund is divided equally between decision intervals based on equation 13 when a short term decision interval is followed the available budget might not be sufficient to implement the selected adaptation measure in addition through shorter decision intervals the agency might not have complete information in selection of right adaptation actions at the earlier decision intervals it takes time for the agency to observe and obtain updated information on the actual trend of hazards in the past on the other hand due to the great deal of uncertainty associated with slr projections and swi rates longer decision intervals i e longer than 45 years increase the value at risk var of the capital investment batouli and mostafavi 2018 jorion 2000 thus the results presented in fig 13 alongside the findings of previous studies e g batouli and mostafavi 2018 jorion 2000 suggest that there is an optimum duration of decision intervals in capital adaptation planning that leads to robust adaptation pathways under insufficient funding levels the last characteristic evaluated in this study refers to signposts and triggers in adaptation decision making in the context of this study the distance of swi from wellfields is the signpost based on which the adaptation implementation is determined the swi proximity threshold that the agency sets for taking an adaptation action defined and used in equation 6 is referred to as a trigger value fig 14 shows the influence of trigger values i e proximity thresholds on the system lri under different decision intervals when the actual slr is high and sufficient funding is available the results presented in fig 14 imply that lri is influenced by the utility agency s proximity threshold if shorter decision intervals i e less than 10 years are followed for any reason smaller proximity thresholds i e less than 300 m would result in improved lri in the system therefore the smaller proximity thresholds are effective triggers for the implementation of adaptation decisions under short decision intervals and can contribute to the adaptation pathways to yield greater resilience in the system fig a3 in the appendix represents a 2 d fashion of the graph shown in fig 14 7 concluding remarks the results showed the feasibility and value of the proposed framework and simulation model in integrating various climatic hazard scenarios human decisions and infrastructure elements in order to examine the characteristics of robust adaptation planning and long term resilience in cwsis this study discovered fundamental decision making attributes and risk response behaviors affecting the adaptation decision making processes of utility agencies in response to swi induced by slr and a storm surge induced inundation this information is essential in understanding the dynamics of hazards humans infrastructure nexus that shape the resilience landscape of cwsis and performs as the foundation for modeling and simulating the dynamics of coupled human infrastructure system of water supply in adaptation to coastal hazards in addition identification or estimation of the critical values and tipping points in adaptive pathways has important implications for policy formulation pertaining to the effectiveness of adaptation investments in cwsis this study s resultant insights provide actionable scientific information to coastal water infrastructure mangers planners and decision makers to enhance their adaptation planning and investment decision making processes this study contributes to the body of knowledge by developing theoretical computational and practical foundations needed for assessing the economic environmental and social value of swi adaptation strategies from a theoretical perspective this study characterized underlying mechanisms of hazards humans infrastructure interactions for a more advanced formulating of robust adaptation decisions and a better understanding of long term resilience in cwsis in terms of computational contribution based on the abstracted behaviors and interactions among hazards humans infrastructure nexus a computational simulation model was developed that captures the coupled impacts of slr induced hazards and adaptation planning processes on the long term resilience of water supply infrastructures practically speaking this study contributes to more informed decision making for adaptive design operation and management of cwsis by exploring strategies and indicators that improve the long term performance of water supply infrastructure under the impacts of swi while the outcomes of this study can help the city planners and decision makers in developing adaptation plans to improve their infrastructure resilience it lacks the consideration of the overexploitation impact of freshwater aquifers on the movement of salinity interface previous studies e g walsh and price 2010 have shown that lowering the water head due to intensive groundwater extractions has a significant impact on swi rates in coastal areas however the impact of excessive interaction under different demand scenarios was not captured in the current model also demographic studies e g neumann et al 2015 as well as the united nations environment program unep predict that the percentage of the nearshore population will increase from 60 to 75 in the future which might lead to an increased extraction from freshwater aquifers kalaoun et al 2016 therefore future research should integrate the changes of demand due to rainwater harvesting water recycling groundwater recharge with the findings of this study further research is also required to explore demand side adaptation solutions such as household water conservation technology installation rasoulkhani et al 2017a to reduce water demands in order to enhance the resilience of water supply infrastructures in coastal communities declaration of competing interest no conflict of interest acknowledgement this material is based in part upon work funded by national science foundation nsf sustainability research network srn cooperative agreement 1444758 the authors would like to thank technical liaisons and contributors from the miami dade water and sewer department mdwasd for their guidance and expertise any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf or the mdwasd appendix fig a1 uml class diagram of the multi agent simulation model fig a1 fig a2 simulation output interface fig a2 fig a3 relationship between decision intervals and long term resilience at different swi proximity thresholds fig a3 table a1 information of subject matter experts of face validation process table a1 sme role position background sector years of experience 1 hydrogeologist and professor hydrogeology public agency 26 2 hydrogeologist geosciences public agency 17 3 water resource manager water resources and environmental engineering public agency 15 4 water engineer civil engineering consulting firm 6 5 water resources resilience lead civil engineering consulting firm 12 table a2 face validation results table a2 component model features average score conceptual model validity the components of the model represent the most important features of the system 4 5 the behavior of the components of the model is reasonable 4 5 computational model validity the model explains the dynamics of the system 4 5 the theories and assumptions underlying the model are correct 4 5 the model s representation of the system and the model s structure logic and mathematical and causal relationships are reasonable 4 5 data validity the assumptions regarding model s parameters variables interactions and decision rules are reasonable 4 the level of detail and the relationships used for the model are appropriate for the intended purpose 4 output validity the output of the simulation model has the accuracy required for the model s intended purpose 4 the model could be helpful in the domain of its applicability 4 5 
26075,coastal water supply infrastructure systems are exposed to saltwater intrusion exacerbated by sea level rise stressors to enable assessing the long term resilience of these systems to the impact of sea level rise this study developed a novel hazards humans infrastructure nexus framework that enables the integrated modeling of stochastic processes of hazard scenarios decision theoretic elements of adaptation planning processes of utility agencies and dynamic processes of water supply infrastructure performance using the proposed framework and data collected from south miami dade service area a multi agent simulation model was created to conduct exploratory assessments of the long term resilience of water supply infrastructure under various sea level rise scenarios and adaptation approaches the results showed the capability of the proposed model for scenario landscape generation to discover robust adaptation pathways for enhanced infrastructure resilience under uncertainty the analysis results could provide actionable scientific information to water infrastructure managers to improve their adaptation planning and investment decision making processes keywords infrastructure resilience coastal water infrastructure adaptive decision making adaptation planning sea level rise multi agent simulation abbreviations slr sea level rise swi saltwater intrusion cwsis coastal water supply infrastructure systems lri long term resilience index 1 introduction coastal states in the united states of america are faced with evolving stressors and hazards exacerbated by climate change impacts evolving coastal hazards create significant challenges to u s public safety u s geological survey 2014 because of the 25 most densely populated and rapidly growing u s counties 23 are along a coastline wilson and fischetti 2010 one of the most pressing coastal stressors is sea level rise there is clear scientific evidence that sea level is rising as a result of global warming ipcc 2007 the fifth assessment report of the intergovernmental panel on climate change reported that there has been an unequivocal upward trend in average global sea levels ipcc 2013 sea level rise slr impacts in coastal areas have been studied intensively over the last decades de almeida and mostafavi 2016 slr increases the frequency duration and magnitude of extreme weather events e g storm surge heavy precipitation flooding leading to greater exposure of different coastal infrastructure systems e g water wastewater energy transportation to chronic and acute stressors ghanbari et al 2019 in particular coastal water supply infrastructure systems cwsis are susceptible to slr impacts slr affects water infrastructure in various ways including accelerated degradation of underground utility inundation of low lying facilities contamination of groundwater through saltwater intrusion most water infrastructure systems are not designed to account for slr impacts and hence could not operate under these conditions one of the most serious impacts of slr on the coastal water supply infrastructure is saltwater intrusion werner and simmons 2009 the majority of coastal cities such as ones located in southeast florida use underground aquifers as their main source of freshwater supply heimlich et al 2009 with a constant discharge rate from groundwater wells and after a certain rise in sea levels saltwater would start to flow into the wells from the shores as saltwater intrusion swi moves inland due to rising sea levels the risk of freshwater contamination increases swi would happen in two ways i chronic swi into groundwater aquifer due to slr and ii storm induced swi due to storm surge events exacerbated by slr chronic swi occurs when the ocean hydraulic static pressure is higher than the underground aquifer s pressure which causes the saltwater freshwater interface zone salinity line located beneath the surface move inland over a long time werner and simmons 2009 the rise in sea levels is expected to increase the inland movement of the salinity line because as sea level continues to rise at faster rates the hydraulic pressure exerted by the seawater will also increase resulting in a higher lateral push that drives the salinity line inland werner and simmons 2009 storm induced swi often starts with overtopping of a coastal barrier followed by hinterland inundation and subsequent vertical seawater intrusion elsayed and oumeraci 2018 hurricanes and extreme storm events can cause storm surges that result in swi into coastal aquifers through vertical infiltration of saltwater elsayed and oumeraci 2017 yang et al 2013 in either way swi would affect the freshwater capacity of water supply infrastructure and thus the system s service reliability in meeting the potable water demand of service areas hence swi increases the groundwater salinity that either results in abandonment of contaminated wellfields or requires desalination of the saline water many cities such as miami and hallandale beach in florida have closed a number of their water wellfields due to the contamination of saltwater mdwasd 2014 several studies have investigated the impact measure of swi on cwsis dausman and langevin 2005 elsayed and oumeraci 2017 park et al 2011 prinos et al 2014 and many other studies have evaluated the adaptation and resilience of cwsis under these impacts de almeida and mostafavi 2016 bloetscher et al 2010 elsayed and oumeraci 2018 heimlich et al 2009 schoen et al 2015 a review of the existing literature shows that the steady state and ex post mostafavi 2018 analysis approaches do not provide robust insights for resilient adaptation planning in cwsis due to the uncertain nature of slr and swi impacts requiring more adaptive approaches to planning and decision making batouli and mostafavi 2018 chappin and van der lei 2014 in addition the existing approaches for evaluation of swi impacts on cwsis do not consider adaptation decision making behaviors of human actors e g agencies and users while different factors contributing to the physical impacts of swi on cwsis are well studied little is known about effective adaptation approaches in response to swi impacts such assessment would require consideration of uncertainty and complex interactions in hazards humans infrastructure nexus 1 1 resilience in hazards humans infrastructure nexus an important step towards formulating robust adaptation plans and investment strategies is to specify and characterize the long term scenarios related to cwsis performance under impacts of coastal hazards understanding of long term performance and resilience is essential because due to the significant physical and institutional inertia in water supply infrastructure systems undesirable performance regimes resulting from maladaptation are very difficult to reverse rasoulkhani et al 2017b in the context of complex infrastructure systems such as cwsis long term resilience is defined as the ability to transform and adapt to changes in internal dynamics to cope with chronic or acute hazards and thus sustain performance suitable for the social and economic development of communities park et al 2013 long term resilience is an emergent property that arises due to hazards humans infrastructure interactions and evolves over time rasoulkhani and mostafavi 2018 as shown in fig 1 a hazards humans infrastructure nexus framework encompasses the underlying mechanisms and their attributes and relationships that all shape and influence the long term resilience landscape of cwsis under swi impacts this framework which is based on a complex system approach captures three mechanisms underlying the long term resilience fig 1 i climatic hazards uncertain unfolding of evolving coastal hazards e g swi ii human actor decision making processes adaptation planning processes of utility agencies and iii physical infrastructure performance water supply infrastructure condition and performance the three mechanisms underlying the long term resilience of water supply infrastructure interact with each other in the context of this study slr increases the rate of chronic swi into the water supply infrastructure and also exacerbates the frequency and magnitude of storm surges which both lead to damages and losses in freshwater aquifers in addition the risk of swi would change decision making behaviors and priorities of utility agencies the uncertainty associated with swi affects the priorities of utility agencies to determine an optimal balance between adaptation investments and perception of potential future swi risks given resource limitations and future uncertainty adaptation investments of utility agencies would influence the exposure of physical infrastructure to swi threats accordingly these coupled impacts would determine the performance of water supply system and its ability to serve the intended demand at the desired level of service the specification and characterization of these mechanisms and their interactions would enable modeling various future scenarios of slr and swi to examine the effectiveness of different adaptation strategies 1 2 robust adaptation planning to moderate the potential impacts of coastal hazards e g swi on cwsis planning and implementation of effective adaptation strategies is essential aerts et al 2014 adaptation involves anticipating the adverse effects of hazards and implementing appropriate strategies and measures to reduce the adverse impacts of hazards on coastal communities ipcc 2013 understanding the characteristics of robust adaptation planning holds the key to avoid maladaptation in cwsis maladaptation refers to failure to change behaviors and undertake appropriate strategies such that the infrastructure systems on which a society is depended become unable to provide the required service aerts et al 2014 maladaptation may occur due to failure to take timely actions or taking actions that would be difficult to reverse devising robust adaptation plans and investments is contingent upon evaluation of the long term resilience of infrastructure systems under different adaptation strategies and hazard scenarios batouli and mostafavi 2018 although the existing literature related to robust decision making involves frameworks e g deyle and butler 2013 haasnoot et al 2013 kwakkel et al 2016 lempert et al 2006 shortridge et al 2017 shortridge and zaitchik 2018 for adaptation planning and evaluation of climate change impacts the existing frameworks are not developed specifically to specify and evaluate the characteristics of robust adaptation planning and investment decision making in cwsis under swi impacts to provide insights for robust adaptation strategies in the context of cwsis an exploratory analysis approach is required to examine different adaptation decision making attributes such as hazard perceptions risk attitudes decision intervals investment levels and signposts e g swi proximity exploratory analysis has been proposed as an effective approach for dealing with uncertainty and complexity in climate change adaptation bankes 1993 kwakkel and pruyt 2013 exploratory analysis involves analytical model based methods for decision making under uncertainty model based exploratory analysis uses computational models and simulation experiments to conduct scenario analysis and evaluate the behavior patterns in complex systems such as cwsis agusdinata 2008 bankes 2002 mostafavi 2018 mostafavi et al 2013 unlike traditional simulation approaches exploratory analysis does not aim to predict the behavior of a system for optimizing the system instead exploratory analysis focuses primarily on considering different scenarios based on changes in system behavior and future uncertainty to evaluate the robustness of different pathways of actions under deep uncertainty the present study utilized a model based exploratory analysis to describe how fundamental decision making attributes risk response behaviors and physical infrastructure realities affect the adaptation pathways in response to swi impacts to this end a dynamic time dependent multi agent simulation model was created to capture the attributes and interactions among costal hazards slr and swi adaptation decision making processes risk attitudes investment levels decision intervals and water supply infrastructures exposure condition performance accordingly the study examined the long term scenarios emerging from interactions in hazards humans infrastructure nexus to devise robust adaptation strategies that could mitigate subsequent impacts of swi and improve resilience in cwsis the analysis results facilitated better understanding of the coupled effects of the uncertain coastal hazards and adaptation decision making processes on the long term resilience of cwsis 2 methodology 2 1 multi agent simulation model multi agent simulation is an effective simulation approach for analyzing complex processes and interactions in coupled human infrastructure systems batouli and mostafavi 2018 davidsson 2001 monticino et al 2007 ralha et al 2013 rasoulkhani et al 2019b multi agent simulation enables modeling complex and real world systems through the adoption of influential concepts such as adaptation emergence and self organization al zinati et al 2013 rasoulkhani et al 2018 in multi agent simulation an agent has several essential characteristics active initiating actions reactive responding to external stimulus and autonomy freedom from intervention by any other agents grignard et al 2014 many entities within an infrastructure system can be viewed and modeled as an agent esmalian et al 2019 hence multi agent simulation is suitable approach to model entities and their interactions in hazards humans infrastructure nexus the created multi agent simulation model integrates i stochastic processes related to slr stressors i e swi and storm surge based on the data obtained from previous studies pertaining to future projections of slr ii decision theoretic elements of utility agency s adaptation decision making processes based on theories of bounded rationality and regret aversion and iii dynamic processes of water supply infrastructure exposure and performance under swi impacts the computational representation of the simulation model was developed in an object oriented java based simulation platform i e anylogic 8 the relationships among agent classes and their attributes are based on the empirical information and grounded theories e g bounded rationality regret aversion the agent classes and their attributes and functions are shown in the unified modeling language uml diagram see fig a1 in appendix the model contains three main classes of agents hazard agent human actor agent and infrastructure agent the mathematical representation of each agent their attributes and relationships are discussed below 2 1 1 hazard agent the hazard agent represents the stochastic processes related to two coastal hazards induced by slr including swi and storm surges both of which evolve and are exacerbated due to slr the higher the sea level rises the faster the chronic swi moves due to a great deal of uncertainty in projection of future slr the intergovernmental panel on climate change and the southeast florida regional climate change compact have recommended consideration of a number of alternative scenarios for different trajectories of future slr compact 2015 ipcc 2013 based on their recommendations three scenarios i e low medium and high sea levels were considered in this model based on the ipcc 2013 and the compact 2015 the agencies in southeast florida are utilizing these three scenarios for adaptation planning and decision making in order to evaluate the long term chronic swi it is important to understand how different scenarios of slr lead to swi however the extent of swi not only is affected by slr severity but also depends on a large number of physical and hydraulic parameters such as recharge water table fluctuations hydraulic conductivity soil porosity and mixing properties la licata et al 2011 hence three different possible rates of chronic swi were considered under each slr scenario i fast swi ii moderate swi and iii slow swi simulation of contaminant transport such as swi in coastal aquifers is intrinsically complex and computationally expensive because of the complex flow patterns that develop when freshwater mixes with saline groundwater elsayed and oumeraci 2018 la licata et al 2011 however several numerical models have been specifically developed to evaluate the effect of sea level fluctuations on swi in this study the results obtained from the existing numerical groundwater models in southeast florida were used to determine the rate of swi into the wellfields fig 2 dausman and langevin 2005 fitterman 2014 prinos et al 2014 the information related to different swi scenarios are summarized in fig 2 with this information the salinity line movement chronic swi in the coastal aquifer was determined and then used to compute the year in which each wellfield gets exposed under different slr and swi scenarios over an extended 100 years of the simulation period the chronic swi could be exacerbated due to storm surges in fact storm surge events can accelerate swi and as a result temporarily further push the salinity line towards aquifers park et al 2011 therefore in the hazard agent the occurrence of storm surge events and the magnitude of their immediate impact on swi which is called secondary or immediate impact were incorporated during the analysis period e g 100 years the likelihood and magnitude of storm surge occurrence vary based on the severity of slr the occurrence of storm surge events was modeled using a homogenous stochastic process poisson model as shown in equation 1 according to katz 2010 the poisson process is an effective statistical technique used for analysis of the frequency of occurrence of an extreme event such as hurricanes this technique assumes a linear trend in the logarithm of the rate parameter of the poisson distribution katz 2010 1 p s t o r m s u r g e p s t o r m s u r g e s l r s c e n a r i o λ k e λ k λ e λ where λ is the mean rate of storm surges per year likelihood of having one storm surge event at each year the λ values were determined based on the available data existing in published studies park et al 2011 tebaldi et al 2012 zhang et al 2013 pertaining to the increased frequency and magnitude of surge events in southeast florida accordingly the values of 0 15 0 20 and 0 25 were assigned to λ under low medium and high slr scenarios respectively the mechanics of storm induced swi through surface infiltration or open pits is highly complex and non linear due to the unsteady nature of the stratification denser seawater over less dense freshwater and the significant spatial and geologic inhomogeneities park et al 2011 due to the complexity and high diversity of the involved processes and interactions gingerich et al 2017 the extents of swi caused by storm surges have not been precisely quantified miami dade county 2016 hence in this study the results and insights of existing models that couple the surge statistics with swi to predict the extent of saline infiltration elsayed and oumeraci 2018 la licata et al 2011 yang et al 2013 2018 yu et al 2016 were utilized to approximate the possible extents of secondary swi induced by ss accordingly the following stochastic uniform distributions were defined for the extents of storm induced swi in meter under different slr scenarios equation 2 2 s e c o n d a r y s w i u n i f o r m 30 60 s l r l o w u n i f o r m 60 90 s l r m e d u n i f o r m 90 120 s l r h i g h finally the processes represented in the hazard agent enable integrating the storm induced swi with the chronic swi obtained from fig 2 and calculating the total swi accordingly every year i e the model time step the proximity of the salinity line to each wellfield is determined based on the wellfield s initial distance from the salinity line and the total swi happened until that year equation 3 3 p r o x i m i t y t i n i t i a l d i s t a n c e t o t a l s w i t subsequently the actual and perceived exposure of wellfields to the risk of swi are determined in the human actor agent as explained in the following subsection 2 1 2 human actor agent the human actor agent captures the underlying behavioral factors and decision rules that influence adaptation decision making processes of the utility agency which manages and operates the water wellfields and treatment plants two planning approaches were considered i adaptive planning in which the decisions related to management of the infrastructure are made based on consideration of the future impacts of swi and the agency implements adaptation actions before swi affects the infrastructure and ii reactive planning or no adaptation in which the decisions would be non adaptive and are made without considering the uncertain swi impacts in this approach the agency waits until an infrastructure is affected by swi and then would implement recovery actions such as well relocation or desalination facility installation the adaptation and recovery decision making behaviors of the utility agency were modeled using the action chart shown in fig 3 the details about the processes related to these two planning approaches are discussed in the remainder of this section 2 1 2 1 adaptive planning this study adopted the theory of bounded rationality to model the rational behavior of individuals under certain limitations based on the theory of bounded rationality decision makers who are bounded by imperfect information confined time and limited capacity seek satisfactory solutions rather than optimal solutions simon 1972 the adaptation decision making processes of utility agencies represent all the traits of bounded rational decision making first there is a significant level of uncertainty in the available information about future slr and its impacts second utility agencies are bounded to taking adaptation actions at certain points of time known as decision points when a multiyear capital improvement program is devised wooldridge et al 2002 third all adaptation actions of a utility agency are contingent upon availability of capital improvement funding batouli and mostafavi 2018 the modeling elements of the adaptive planning process are explained as follows 2 1 2 1 1 decision intervals the adaptation decision making process is adaptive in nature fig 4 depicts the processes leading to adaptation decisions of the utility agency the entire period of a capital adaptation plan which is 100 years in this study is usually divided into a number of time intervals and the utility agency makes adaptation decisions at certain decision points the time period between two successive decision points t i 1 t i is referred to as decision interval after the utility agency implemented the adaptation decision at a decision point t i they would observe the actual changes in the state of nature i e slr and swi before taking actions in the next decision point t i 1 at the next decision point the actual slr and swi as well as the risk attitude of the agency is updated based on the new information that has become available the entire process is then repeated for next decision horizons to make new adaptation decisions 2 1 2 1 2 hazard perception the imperfect information about future slr and swi causes the utility agency to make adaptation decisions based on its perception of the future hazards the perception of the agency is influenced by its risk attitude towards likely impacts of hazards since the agency is unsure what scenario of slr and swi would happen in the future they rely on their own perception to perceive the swi among the alternative swi projections hence the perceived swi of the utility agency may be different from the actual swi to model this three different hazard perceptions h including optimistic o most likely m and pessimistic p were defined as shown in equation 4 4 h o s l o w s w i l o w s l r s t o r m s u r g e l o w s l r m m o d s w i m e d s l r s t o r m s u r g e m e d s l r p f a s t s w i h i g h s l r s t o r m s u r g e h i g h s l r 2 1 2 1 3 risk attitude the agency s hazard perception for a decision horizon from t i to t i 1 is determined based on its risk attitude at the beginning of the decision horizon i e at decision point t i the agency s risk attitude determines the decision maker s preference among uncertain outcomes of adaptation decisions the agency could be either risk seeker risk neutral or risk averse at the beginning of the adaptation planning period however its risk attitude may change at each decision point and get updated for the following decision interval this is because risk attitude is formed based on past experience reference point and observations rather than being based on fully rational analytical models leiserowitz 2006 equation 5 summarizes the representation of the agency s risk attitude in the model at each decision point in fact based on the agency s observation during the past decision interval i 1 more updated information becomes available at decision point t i and then the agency would become more less risk seeking if the perceived swi for the past decision horizon is greater lower than the maximum minimum actual swi happened during that decision horizon equation 5 5 if p s w i i 1 a s w i i 1 m i n t h e n b e c o m e m o r e r i s k a v e r s e a t t i m e s s a g e 1 p s w i i 1 a s w i i 1 m a x t h e n b e c o m e m o r e r i s k s e e k e r a t t i m e s s a g e 2 p s w i i 1 a s w i i 1 m i n a s w i i 1 m a x t h e n k e e p t h e c u r r e n t a t t i t u d e a t t i where p s w i i 1 and a s w i i 1 denote the perceived swi and the actual swi during the decision horizon of i 1 respectively accordingly prior to each decision point the utility agency transitions between the risk attitude states based on the message received from equation 5 for example if the agency was risk seeker at the previous decision point t i 1 and observes that the perceived swi is lower than the minimum actual swi happened during the following decision interval i 1 then the agency s risk attitude would change to risk neutral at decision point t i 2 1 2 1 4 signposts signposts specify information that should be tracked in order to determine whether the adaptation action implementation is needed haasnoot et al 2013 the critical values of signpost variables are called triggers beyond which adaptation actions should be implemented haasnoot et al 2013 in this model the distance of swi from wellfields is the signpost based on which the adaptation implementation is determined as shown in the agency s action chart fig 3 if a well has not been contaminated yet its exposure to swi is being evaluated once the risk attitude of the agency and the hazard perception were specified at a decision point the exposure of each wellfield to the perceived swi scenario during the following decision horizon is determined using equation 6 to determine the exposure of wellfields the utility agency performs conditional logic testing based on the perceived swi proximity calculated in equation 3 the agency would consider a proximity threshold as a trigger value which is the minimum distance of swi from the well that the agency would tolerate if a well is located at a distance greater than the proximity threshold from the perceived salinity line it won t be considered at risk for the following decision horizon however if the distance is equal or less than the threshold for the following decision horizon then the well is considered at risk the utility agency can adopt different values of proximity threshold for example if the agency s proximity threshold is 30 m then the agency would consider a well at exposure when the proximity of perceived swi to the well is less than 30 m 6 e x p o s u r e t p r o x i m i t y t t h r e s h o l d 2 1 2 1 5 adaptation actions based on consideration of the exposure of wells to perceived swi if no wells are identified to get exposed to perceived swi the agency does not implement any adaptation actions and proceeds to the next decision point if one or more wells are identified to potentially get exposed to swi the next step of adaptation decision making is to select appropriate adaptation actions in this model four different adaptation actions were considered the adaptation action space includes the following adaptation actions i aquifer recharge implementing deep well injection to control groundwater levels ii salinity barrier installing seepage barrier to protect wellfields iii well relocation closing an exposed contaminated wellfield and exploiting new wellfield farther from the salinity interface location and iv desalination facility adding desalination capacity to the treatment plant each adaptation action has different cost and effect on the water supply system as summarized in table 1 since there is uncertainty related to cost of swi adaptation actions pert probability distributions are defined based on the min most likely and max values reported in previous studies and reports see the last column of table 1 2 1 2 1 6 choice modeling the agency s choice among different adaptation alternatives was modeled based on the theory of regret aversion bell 1982 recent research in behavioral psychology and judgement under uncertainty reveals that regret aversion provides a better model of human choice under uncertainty compared to the standard utility theory since regret can be anticipated prior to choice it can lead to regret minimizing decisions humphrey 2004 according to this theory the risk attitude of the decision maker determines what adaptation action to be selected batouli and mostafavi 2018 as shown in fig 5 a risk seeking agency feels optimistic about the climatic hazard scenario and would select the alternative that results in highest performance under the best case scenario of hazard on the other hand a risk averse agency would want to maximize the system performance under the pessimistic hazard perception worst case scenario finally a risk neutral agency would anticipate the possibility of feeling regret after the uncertainty of future hazards is resolved hence a risk neutral agency would select the alternative that yields the least regret across all hazard scenarios i e optimistic most likely and pessimistic perception to capture the adaptation alternative selection process at each decision point the model determines the planned anticipated performance of the infrastructure system based on the measure of planned level of service the planned level of service p l s t i that the agency anticipates at decision point t i for the following decision horizon based on the implementation of adaptation actions is calculated using equation 7 7 p l s t i t i t i d s u p p l y c a p a c i t y t p r o j e c t e d d e m a n d t where d denotes the duration years of decision intervals supply capacity and projected demand are parameters driven from the infrastructure agent explained in next subsection accordingly the model generates a matrix related to the planned level of service resultant of each adaptation action j under optimistic most likely and pessimistic hazard perceptions h as shown in equation 8 8 p l s j h p l s 1 o p l s 1 m p l s 1 p p l s 2 o p l s 2 m p l s 2 p p l s 3 o p l s 4 o p l s 3 m p l s 4 m p l s 3 p p l s 4 p where rows represent adaptation alternatives and columns represent hazard perceptions for instance p l s 1 o denotes the planned level of service that the agency anticipates for system if adaptation action 1 is implemented given the optimistic hazard occurrence to select an adaptation action to be implemented at each decision point if necessary based upon the exposure the following process is followed if the agency is risk seeker risk averse then the adaptation action a j that results in p l s m a x o p l s m a x p will be selected as shown in equation 9 9 s e l e c t a j i f p l s j o max p l s 1 o p l s 2 o p l s 3 o p l s 4 o r i s k a t t i t u d e r i s k s e e k e r p l s j p m a x p l s 1 p p l s 2 p p l s 3 p p l s 4 p r i s k a t t i t u d e r i s k a v e r s e in case the agency is risk neutral the model also constructs the regret matrix by subtracting the planned level of service achieved under each adaptation action from the maximum possible planned level of service that could be achieved under the adaptation actions equation 10 in equation 10 r e g j h denotes the regret of selecting adaptation action j if hazard h happens 10 r e g j h p l s m a x o p l s 1 o p l s m a x m p l s 1 m p l s m a x p p l s 1 p p l s m a x o p l s 2 o p l s m a x m p l s 2 m p l s m a x p p l s 2 p p l s m a x o p l s 3 o p l s m a x o p l s 4 o p l s m a x m p l s 3 m p l s m a x m p l s 4 m p l s m a x p p l s 3 p p l s m a x p p l s 4 p accordingly the maximum regret related to selecting adaptation action j m a x r e g j is determined using equation 11 11 m a x r e g j max r e g j o r e g j m r e g j p finally the risk neutral agency will select the adaptation action that has the lowest maximum regret minimax regret as shown in equation 12 12 s e l e c t a j i f m a x r e g j min m a x r e g 1 m a x r e g 2 m a x r e g 3 m a x r e g 4 in the adaptation alternative selection process for each risk attitude if there are more than one adaptation action with the same outcome i e highest performance or least regret the agency would select the less expensive one implementation of the selected adaptation actions is contingent upon sufficiency of the available budget in this model the utility agency was assumed to be given a capital improvement fund for the entire 100 years in order to implement adaptation actions mdwasd 2014 due to the uncertainty pertaining to climatic hazards the capital fund should be spent across various decision intervals in this study the capital fund is equally distributed among the decision intervals and thus the available budget b t i for the agency at each decision point is determined based on equation 13 13 b t i b t i 1 c a p i t a l f u n d 100 d where b t i 1 denotes the reminder of budget from previous decision point and d denotes the duration of adopted decision intervals based on the decision making and choice processes discussed above the adaptation decision making behaviors of the utility agency were captured during a 100 year analysis horizon it should be noted that in adaptive planning if a well gets contaminated for any reason the agency will implement recovery actions as discussed in the remainder of this subsection 2 1 2 2 reactive planning in order to better understand and examine the effectiveness of adaptation planning a reactive planning approach was also considered in the model reactive approach is based on responding to events after they have happened in reactive planning the agency monitors the actual condition of infrastructure systems and implements recovery actions when a failure or performance drop happens due to hazards the reactive planning behavior of the agency was modeled as shown in the action chart fig 4 in reactive planning two different recovery actions were considered well relocation and desalination facility the difference between these two actions in reactive planning with the ones in adaptive planning is that there would be a lag delay in recovering from the contaminated wellfield and thus to retrieve the system s level of service under reactive planning strategy when a well is contaminated the utility agency first examines whether the well relocation can be done based on the relocation limit that each well has it was assumed that each well could be relocated only for a certain number of times accordingly if the well could be relocated the agency would close the contaminated wellfield and exploit new wellfield with a farther distance from the saltwater interface location otherwise the agency would install desalination facility to treat the saline water extracted from contaminated wells the outcomes of the human actor agent include the timing and type of adaptation recovery actions these outcomes are used in the infrastructure agent to model changes in the performance of water supply infrastructure system based on adaptation recovery actions 2 1 3 infrastructure agent this agent class captures behaviors of the water supply infrastructure under the coupled impacts of swi and the utility agency adaptation recovery actions this study considered two key components of water supply infrastructure systems water wellfields and water treatment plants however the focus was on modeling the wellfields two sets of processes were captured i physical condition the structural capacity and ability of infrastructure to withstand swi and ii functional performance the infrastructure ability to provide intended demand at the desired level to capture these processes various attributes of wellfields and treatment plants were considered attributes of water wellfields included location distance from the salinity line source aquifer number of wells and installed design capacity the treatment and desalination capacities of the treatment plants were also considered each year the simulation model checks all wells based on the proximity of swi to each well and accordingly the well contamination status is specified using equation 14 14 w c s k t 1 i f p r o x i m i t y k t 0 0 i f p r o x i m i t y k t 0 where w c s k t is the contamination status of well k at year t and p r o x i m i t y k t denotes the proximity of swi interface to well k at year t which is calculated by equation 3 in hazard agent once a well is in contaminated status based on the outcome of utility agency s decision making process either the well may be relocated or extra desalination capacity may be added to the system at the beginning of the simulation the system does not include any desalination capacity accordingly the performance of water supply system is evaluated based on the supply capacity which is the amount of water that the system can supply to the service area the annual water supply capacity of the system is calculated based on the extraction capacity of not contaminated wells e c capacity of treatment plants t c and the amount of desalinated water d w as shown in equation 15 15 s u p p l y c a p a c i t y t min w e l l s e c p l a n t s t c d w in equation 15 desalinated water d w is the amount of water demand that could not met due to contaminated well s so the utility agency would need to desalinate this amount of water if there is no well contaminated then d w is equal to zero also d w can t be greater than the desalination capacity of the plant the water demand imposed to the system is projected based on the population of service area as the population of coastal service areas grow wilson and fischetti 2010 the water supply infrastructure system has to possess the capacity to meet the increasing water demand hence this model captures the dynamic changes in the level of demand imposed to the system over the 100 year analysis period equation 16 represents how the projected demand of every year is calculated based on the population of the service area 16 p r o j e c t e d d e m a n d t p o p c 365 1 p o p g t where p o p c and p o p g denote the base population the daily per capita water consumption and the population growth rate respectively the resilience of the water supply infrastructure system was determined based on a measure called level of service which captures the reliability of water supply to meet the demand the level of service l o s t is calculated using equation 17 17 l o s t s u p p l y c a p a c i t y t p r o j e c t e d d e m a n d t 100 where l o s t values between 0 and 100 as if the supply capacity is greater than the demand the model equalizes supply with demand accordingly the long term resilience index l r i of the system is computed based on equation 18 18 l r i 1 100 1 100 l o s t the greater l r i indicates less loss in the levels of service in the system over the long period of time i e 100 years 3 case study the application of the proposed framework was illustrated to study the adaptation of coastal water supply infrastructure system to impacts of slr and swi in miami dade county fl for over a 100 year analysis horizon the hydrologic and groundwater conditions in southeast florida have been altered significantly due to climate change impacts that the management and operation of water supply infrastructure has become a complex job for the local utility agencies sfwmd 2018 south miami dade area formerly known as the rex utility district is specifically vulnerable to the impacts of slr because it is located in the lower section of miami dade county as seen on the map in fig 6 this area has been experiencing swi in the last couple of decades dausman and langevin 2005 fitterman 2014 prinos et al 2014 the multi agent simulation model developed in this study was built using data and information related to the south miami dade water service area these data and information were collected mainly from the findings of scientific researches e g dausman and langevin 2005 fitterman 2014 prinos et al 2014 and reports e g mdwasd 2014 of local institutions and governmental agencies which are in charge of the water utilities of the region the water supply infrastructure system located in south miami dade area is managed by miami dade water and sewer department mdwasd and is serving mainly the southern part of miami dade county based on the 20 year water supply facilities work plan reported by mdwasd in 2014 this service area has a population of 98690 with a growth rate of 0 88 and an average daily per capita water consumption of 0 5 m3 mdwasd 2014 based on the 20 year water supply facilities work plan mdwasd has planned to consolidate their existing treatment plants and the associated wellfields by construction and operation of south miami heights water treatment plant of the five existing water treatment plants and their individual supplying wellfields in the area only two plants remain in service on a stand by basis after the proposed plant begins operations mdwasd 2014 the information of the three anticipated wellfields in the proposed south miami heights water treatment plant is shown in fig 6 4 model verification and validation to ensure the quality and credibility of the developed multi agent simulation model verification and validation processes were conducted simulation models especially agent based models related to human systems are often criticized for relying on informal and subjective validation or no validation at all in this study a gradual systemic and iterative process was employed to conduct a thorough verification and validation of the simulation model 4 1 internal and external verification various internal and external verification techniques were employed to verify the data logic and computational algorithms related to the simulation model bankes and gillogly 1994 the internal verification of the model was ensured using grounded theories e g bounded rationality and regret aversion for modeling decision making behavior processes furthermore valid empirical data e g infrastructure attributes system demand etc were employed for modeling the infrastructure condition and performance indicators in addition for each component of the model component verification assessment was conducted to verify the completeness coherence consistency and correctness 4cs of the component pace 2000 for instance the model performance was observed under i taking off the function of one component of the model and making sure it influences the outputs to the degree that is specified in the model and ii running the model with extreme values of each component and verifying the functionality of the model under the extreme condition also several random replications of the model were compared to check for the consistency of the outputs accordingly most of the discovered errors had less to do with problems within the theories or empirical rules and more regarding issues with coding correctly the external verification of the model was ensured by building the model rich in causal factors that were examined to see what leads to particular outcomes bharathy and silverman 2010 fig a2 in the appendix shows a screenshot of the simulation output interface where the behavior regime of various outputs pertaining to different entities of the model can be observed the behaviors of model entities e g risk attitude adopted adaptation actions salinity line movement etc were followed so as to identify unusual patterns whenever an unusual pattern was observed the model logic was checked to ensure that the behavior was not due to unreasonable assumptions or imperfect logics 4 2 face validation to further ensure the model credibility and defensibility an iterative participatory face validation was conducted by institutional agencies involved in adaptation processes in the case study region face validation is conducted by having users and people knowledgeable with the system examine the model logics and outputs for rationality rasoulkhani et al 2019a according to carson 2002 a simulation model that has face validity appears to be a reasonable imitation of a real world system to people who are knowledgeable of the real world system therefore the simulation model its components and the preliminary results were presented to four verified subject matter experts who are specialist and seasoned in the area of coastal water infrastructure adaptation and resilience the subject matter experts involved in the validation process were from different affiliations that are partners of the resilient utility coalition the resilient utility coalition provides leadership in assessing utility operations to address the potential effects of climate change and seeks to enhance the usefulness of climate science by developing adaptation strategies and improving water management decision making in the face of climate uncertainty table a1 in the appendix provides information of the subject matter experts who participated in the face validation process after each presentation the subject matter expert was asked to evaluate the simulation model elements including conceptual model computational model data and outputs a questionnaire survey was given to and completed by the subject matter experts to evaluate different features of the model see table a2 in appendix on a scale of 1 5 in which 1 and 5 respectively represented the lowest and highest levels of validity and quality as shown in table a2 on average the subject matter experts evaluated the simulation model with scores above 4 based on these evaluations all the subject matter experts asserted that the behavior of the model is consistent with the real world and the relationships between different components of the model are realistic they also indicated that the simulation and visualization component of the model provides a useful tool for scenario analysis and decision making in the assessment of coastal water supply infrastructure adaptation and resilience under the impacts of slr in addition some insightful inputs constructive comments and minor modifications were suggested by the subject matter experts and were accordingly addressed on the simulation model 5 simulation experiments after verification and validation the simulation model was used for experiment design and analysis various possible scenarios were determined based on changing the values of input parameters into the model fig 7 each of the two planning strategies adaptive and reactive was analyzed across different scenarios details related to which parameters were used in devising scenarios are presented in table 2 through the combination of various values of the input parameters 10 692 and 4 158 scenarios were generated for adaptive and reactive planning strategies respectively for instance the combinations of the adaptive planning scenarios reflect changes in the actual slr and swi as well as in the agency s initial risk attitude available capital funding decision interval and swi proximity threshold due to the stochastic nature of the simulation model a monte carlo experiment was conducted for each specific scenario to determine the mean value of the model output variable the main output variable targeted in this study was the system long term resilience index lri each experiment was replicated as many times as the mean value of lri reached 95 confidence interval 6 results and discussion the simulated output data i e lri obtained from the experiment scenarios were used through statistical methods for exploratory analysis of long term resilience in coastal water supply infrastructure systems cwsis under the coupled impacts of saltwater intrusion and utility agency decision making processes the exploratory analysis results are threefold i assessing the significance of underlying determinants of long term resilience in hazards humans infrastructure nexus ii comparing the effectiveness of adaptive and reactive planning approaches and iii determining the characteristics of robust adaptation decision making 6 1 resilience determinants the first set of analyses focused on analyzing the scenario landscape to identify the most significant determinants of long term performance of water supply infrastructure in the study region to this end chi square automatic interaction detection chaid technique was used for explaining the impact of different system attributes as well as for generating various scenario pathways i e the combination of various attributes leading to a certain outcome chaid is a non parametric data mining tool used to discover the relationship between variables its algorithm incorporates a sequential merge and split procedure based on a chi square test statistic kass g v 1980 chaid analysis builds a meta model to help determine how variables best merge to explain the outcome in the given dependent variable magidson and vermunt 2005 therefore chaid technique was used to identify among a number of variables the most important variables in determining the outcome variable i e lri fig 8 shows the importance level of different exploratory variables in the simulation model and their effect on the lri of the cwsis under adaptive and reactive planning strategies the results presented in fig 8 show that in reactive planning the top two determinants of the system lri were actual swi and actual slr however under adaptive planning strategy the top two determinants were actual slr and capital funding level thus overall the hazard i e slr and swi was the most significant determinant of the system lri in both adaptive and reactive planning approaches this means that regardless of the decision making factors and behavioral attributes of the utility agency the climatic hazards would significantly influence the long term resilience in cwsis nevertheless the adaptive planning strategy was shown to be able to considerably reduce the influence of actual swi on lri this is because under adaptive planning the utility agency is able to update the information and make adaptive decisions according to observations of swi rates and hence select appropriate adaptation actions to mitigate the swi impacts 6 2 adaptive vs reactive approach the second set of analyses examined the effectiveness of adaptation planning approach in enhancing the long term resilience of cwsis the simulated lri obtained from thousands of scenarios were used to compare the system performance under adaptive and reactive approaches fig 9 demonstrates the best probability distributions that fitted to the simulated lri values resulted from various scenarios using the chi square goodness of fit test the best probability distribution fitted for lri values for reactive planning scenarios was a triangular distribution this is mainly because under this strategy the key drivers of lri were swi and slr which both had three possible values e g slow moderate and fast swi this result further confirms that in reactive planning approach the variables related to climatic hazards are dominant factors influencing lri however the best probability distribution fitted for lri values under adaptive planning scenarios was extreme value distribution which belongs to the exponential family e g normal weibull this type of distribution is frequently encountered in the context of lifetime reliability modeling this type of probability distribution demonstrates that failures in system lri could be controlled when adaptation planning was implemented as shown in fig 9 the probability of achieving greater lri in the system varies in adaptive and reactive planning the likelihood of achieving lri greater than 90 is about 94 under adaptive planning approach and 20 under reactive planning also under the reactive planning strategy there is a likelihood of 48 that the system lri would drop below 80 however this likelihood is about zero in adaptive planning scenarios thus the results show that if the utility agency implements adaptation actions i e adaptive planning strategy the chance that the system will achieve a greater lri is significantly greater than when the agency follows a reactive approach as the chaid analysis results suggested the level of capital funding was one of the influential determinants of lri especially in adaptive planning hence further analysis was conducted to specify the sensitivity and effectiveness of adaptive and reactive planning strategies under different levels of capital funding fig 10 and fig 11 depict the impact of capital funding on the system lri under various slr scenarios in the reactive planning and adaptive planning respectively the results presented in these figures first indicate that the capital funding does not impact the system lri if low slr happens however the level of capital funding is considerably influential under high slr hence by increasing the capital funding level the system lri could improve under high slr in addition figs 10 and 11 demonstrate that there is a critical level of capital funding required to mitigate the impact of high slr in both adaptive planning and reactive planning critical capital funding levels are the tipping points after which the system lri is no longer improved by increasing the capital funding as shown in fig 10 if a reactive planning strategy is implemented the capital funding of 1200 million which is for the entire 100 years and equals to 120 per capita per year would mitigate the impact imposed by high slr and would lead to lri of higher than 85 in average however under the adaptive planning strategy fig 11 the critical level of capital funding that mitigates the impact of high slr is 2000 million this amount of capital fund which is equal to 200 per capita per year would yield an average lri above 90 in the system although the critical level of capital funding in reactive planning is 40 less than the one in adaptive planning the results indicate that the adaptive planning is more effective due to the following reasons first under adaptive planning the critical capital funding can fully cover the lri gap induced by slr uncertainty the lri gap is the difference between mean lri under high slr and mean lri under medium or low slr however the reactive planning is not able to fully mitigate this gap as shown on fig 10 under the critical level of capital funding or any amount above that in reactive planning there is a gap between the lri under high slr and the lri under low and medium slr however this gap does not exist in adaptive planning where the system can maintain its lri under high slr at the same level as low and medium slr second under lower levels of capital funding for example 600 million the adaptive planning strategy enables achieving greater lri in comparison with the reactive planning strategy therefore if the utility agency encounters funding constraints the adaptive planning approach is more effective as it would lead to greater lri 6 3 characteristics of robust adaptation planning the third set of analyses evaluated different adaptation decision making attributes of the utility agency to better understand the characteristics of adaptation pathways that yield in improved long term resilience of cwsis one of these characteristics is the risk attitude of utility agencies although in adaptive planning approach the agency was able to update its risk attitude based on observation of the actual swi happened in the past the influence of the initial risk attitude on the system lri was examined fig 12 shows the influence of initial risk attitude under different slr scenarios given the critical level of capital funding is available the results presented in fig 12 indicate that the initial risk attitude of the agency was not influential on changing the system lri especially under low and medium slr even under high slr the risk averse attitude improved the lri very slightly less than 1 unit this is because the agency was able to update its risk attitude through the adaptive decision making process in which periodic updates made at certain decision points throughout the entire period of capital plan therefore the flexibility to adjust upon the updated information mitigates the influence of the agency s initial risk attitude which might not be a right attitude for planning on the effectiveness of adaptation pathways another characteristic examined in this study is the duration of decision intervals in adaptation planning in capital adaptation planning the entire period of capital plan is divided into a number of decision intervals at which investment decisions are made and implemented fig 13 depicts the influence of the duration of decision intervals on the system lri under various ranges of available capital fund the results show that the duration of decision intervals can be influential when the available capital funding is low less than 1000 million when funding is not sufficient shorter durations i e less than 10 years would result in decreased lri in the system however longer decision intervals especially 35 years can result in greater lri since the capital fund is divided equally between decision intervals based on equation 13 when a short term decision interval is followed the available budget might not be sufficient to implement the selected adaptation measure in addition through shorter decision intervals the agency might not have complete information in selection of right adaptation actions at the earlier decision intervals it takes time for the agency to observe and obtain updated information on the actual trend of hazards in the past on the other hand due to the great deal of uncertainty associated with slr projections and swi rates longer decision intervals i e longer than 45 years increase the value at risk var of the capital investment batouli and mostafavi 2018 jorion 2000 thus the results presented in fig 13 alongside the findings of previous studies e g batouli and mostafavi 2018 jorion 2000 suggest that there is an optimum duration of decision intervals in capital adaptation planning that leads to robust adaptation pathways under insufficient funding levels the last characteristic evaluated in this study refers to signposts and triggers in adaptation decision making in the context of this study the distance of swi from wellfields is the signpost based on which the adaptation implementation is determined the swi proximity threshold that the agency sets for taking an adaptation action defined and used in equation 6 is referred to as a trigger value fig 14 shows the influence of trigger values i e proximity thresholds on the system lri under different decision intervals when the actual slr is high and sufficient funding is available the results presented in fig 14 imply that lri is influenced by the utility agency s proximity threshold if shorter decision intervals i e less than 10 years are followed for any reason smaller proximity thresholds i e less than 300 m would result in improved lri in the system therefore the smaller proximity thresholds are effective triggers for the implementation of adaptation decisions under short decision intervals and can contribute to the adaptation pathways to yield greater resilience in the system fig a3 in the appendix represents a 2 d fashion of the graph shown in fig 14 7 concluding remarks the results showed the feasibility and value of the proposed framework and simulation model in integrating various climatic hazard scenarios human decisions and infrastructure elements in order to examine the characteristics of robust adaptation planning and long term resilience in cwsis this study discovered fundamental decision making attributes and risk response behaviors affecting the adaptation decision making processes of utility agencies in response to swi induced by slr and a storm surge induced inundation this information is essential in understanding the dynamics of hazards humans infrastructure nexus that shape the resilience landscape of cwsis and performs as the foundation for modeling and simulating the dynamics of coupled human infrastructure system of water supply in adaptation to coastal hazards in addition identification or estimation of the critical values and tipping points in adaptive pathways has important implications for policy formulation pertaining to the effectiveness of adaptation investments in cwsis this study s resultant insights provide actionable scientific information to coastal water infrastructure mangers planners and decision makers to enhance their adaptation planning and investment decision making processes this study contributes to the body of knowledge by developing theoretical computational and practical foundations needed for assessing the economic environmental and social value of swi adaptation strategies from a theoretical perspective this study characterized underlying mechanisms of hazards humans infrastructure interactions for a more advanced formulating of robust adaptation decisions and a better understanding of long term resilience in cwsis in terms of computational contribution based on the abstracted behaviors and interactions among hazards humans infrastructure nexus a computational simulation model was developed that captures the coupled impacts of slr induced hazards and adaptation planning processes on the long term resilience of water supply infrastructures practically speaking this study contributes to more informed decision making for adaptive design operation and management of cwsis by exploring strategies and indicators that improve the long term performance of water supply infrastructure under the impacts of swi while the outcomes of this study can help the city planners and decision makers in developing adaptation plans to improve their infrastructure resilience it lacks the consideration of the overexploitation impact of freshwater aquifers on the movement of salinity interface previous studies e g walsh and price 2010 have shown that lowering the water head due to intensive groundwater extractions has a significant impact on swi rates in coastal areas however the impact of excessive interaction under different demand scenarios was not captured in the current model also demographic studies e g neumann et al 2015 as well as the united nations environment program unep predict that the percentage of the nearshore population will increase from 60 to 75 in the future which might lead to an increased extraction from freshwater aquifers kalaoun et al 2016 therefore future research should integrate the changes of demand due to rainwater harvesting water recycling groundwater recharge with the findings of this study further research is also required to explore demand side adaptation solutions such as household water conservation technology installation rasoulkhani et al 2017a to reduce water demands in order to enhance the resilience of water supply infrastructures in coastal communities declaration of competing interest no conflict of interest acknowledgement this material is based in part upon work funded by national science foundation nsf sustainability research network srn cooperative agreement 1444758 the authors would like to thank technical liaisons and contributors from the miami dade water and sewer department mdwasd for their guidance and expertise any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf or the mdwasd appendix fig a1 uml class diagram of the multi agent simulation model fig a1 fig a2 simulation output interface fig a2 fig a3 relationship between decision intervals and long term resilience at different swi proximity thresholds fig a3 table a1 information of subject matter experts of face validation process table a1 sme role position background sector years of experience 1 hydrogeologist and professor hydrogeology public agency 26 2 hydrogeologist geosciences public agency 17 3 water resource manager water resources and environmental engineering public agency 15 4 water engineer civil engineering consulting firm 6 5 water resources resilience lead civil engineering consulting firm 12 table a2 face validation results table a2 component model features average score conceptual model validity the components of the model represent the most important features of the system 4 5 the behavior of the components of the model is reasonable 4 5 computational model validity the model explains the dynamics of the system 4 5 the theories and assumptions underlying the model are correct 4 5 the model s representation of the system and the model s structure logic and mathematical and causal relationships are reasonable 4 5 data validity the assumptions regarding model s parameters variables interactions and decision rules are reasonable 4 the level of detail and the relationships used for the model are appropriate for the intended purpose 4 output validity the output of the simulation model has the accuracy required for the model s intended purpose 4 the model could be helpful in the domain of its applicability 4 5 
26076,physically based distributed hydrologic models require geospatial and time series data that take considerable time and effort in processing them into model inputs tools that automate and speed up input processing facilitate the application of these models in this study we developed a set of web based data services called hydrods to provide hydrologic data processing software as a service hydrods provides functions for processing watershed terrain canopy climate and soil data the services are accessed through a python client library that facilitates developing simple but effective data processing workflows with python evaluations of hydrods by setting up the utah energy balance and topnet models for multiple headwater watersheds in the colorado river basin show that hydrods reduces the input preparation time compared to manual processing it also removes the requirements for software installation and maintenance by the user and the python workflows enhance reproducibility of hydrologic data processing and tracking of provenance keywords hydrods web based data services distributed hydrologic modeling geographic information systems hydrologic data cyberinfrastructure software availability program name hydrods description a set of web based hydrologic data services for preparation of input data for selected physically based distributed grid or subwatershed model elements hydrologic models hydrods comprises python modules for watershed analysis terrain and land cover data processing climate data access and processing and generating soil properties data individual service functions accessed through a python client library may be chained together to form a python workflow to perform a set of related tasks platform centos linux for hosting the web services accessed from any platform license 3 clause bsd license open source source code https github com ci water hydro ds documentation https github com ci water hydro ds wiki hydrods web api description developers tseganeh z gichamo nazmus s sazib david g tarboton pabitra dash 1 introduction physically based distributed hydrologic models are used for simulation of the hydrologic cycle to help answer questions related to water resource availability and quality to assess the effect of changes in climate or land cover and support water resources management along with many other applications an important challenge associated with the application of physically based distributed hydrological models is that they require more input data than their conceptual often lumped counterparts while the rationale for high resolution physically based models is that better results can be achieved through detailed process representation obtaining the extensive set of input data required by these models is a critical challenge leonard and duffy 2013 call this set of input data essential terrestrial variables etv obtaining etvs in a format organized for use in distributed models is a significant bottleneck in distributed hydrologic modelling the ability to configure and populate distributed models with data could enhance or hinder their use prior work with regard to hydrologic data availability has focused on the task of enhancing access to data from different providers through web services horsburgh et al 2009 tarboton et al 2009b ames et al 2012 using standardized data formats taylor 2012 almoradie et al 2013a however the data obtained remain the raw data provided by the sources and generally require further processing to generate suitable inputs to hydrological models in terms of the data structure file format and variable type required by a model e g relative humidity rather than specific humidity as a result in our experience researchers and practitioners implementing hydrologic models spend a significant amount of time transforming the data available into the formats needed by models jones et al 2013 the data pre processing tools currently available are generally desktop based and often limited by their customization to specific hydrologic models e g kumar et al 2009 the increasing availability of cyberinfrastructure resources provides an opportunity to extend such data pre processing ability beyond the desktop environment wang et al 2013 and adopt the paradigm of software as a service developing data processing tools as web based services will help to enhance access to these tools for users without necessarily requiring them to be a cyber expert wright et al 2013 web services that can be accessed by multiple users facilitate better collaborative problem solving nyerges et al 2013 wang 2010 in addition they encourage the use of standardized data formats e g waterml and netcdf by multiple models in this paper we introduce a set of web based hydrological data processing services called hydrods hydrods provides a number of data processing functionalities including watershed delineation terrain processing estimation of canopy variables retrieval and processing of weather forcing data procuring soil data and generating soil properties data are stored and shared in three widely used data formats geotiff raster shapefile and multi dimensional netcdf the data services are comprised of functions that can be used independently or form workflows that integrate a number of related tasks the services are accessed through a python client library that facilitates developing simple but effective data processing workflows with python providing access to data processing tools from an accessible and relatively easy to use programming environment data processed by hydrods can be transferred to hydroshare a platform for sharing of hydrologic data and models tarboton et al 2014a the objective here was to provide the means to setup python workflows for preparation of input data for distributed hydrologic models the services we developed support the utah energy balance ueb snowmelt model tarboton et al 1995 and topnet hydrologic model bandaragoda et al 2004 in the next section we provide background information on prior work dealing with data access and processing for hydrologic modeling and the need for web based data services that motivated this work in section 3 we report the required functionality design and implementation of hydrods in section 4 we evaluate the data services using a case study of setting up instances of ueb and topnet models for multiple headwater watersheds in the colorado river basin results and discussion are given in section 5 followed by summary and conclusions in section 6 2 background 2 1 input data processing for hydrologic models providing access to hydrological data from different repositories through web services has been the focus of the consortium of universities for the advancement of hydrologic science inc hydrologic information system cuahsi his horsburgh et al 2009 tarboton et al 2009b cuahsi his provides software tools for publishing and retrieving time series data through standardized web services in an xml format called waterml tarboton et al 2011 valentine et al 2012 2007 beran et al 2009 waterml2 was later developed as an open geospatial consortium ogc standard for hydrologic time series data representation and exchange across multiple information systems taylor 2012 standardized web services and protocols facilitate interoperability between different data service providers and consumers clients for easy access to and retrieval of data client applications can search for and download data made available through the cuahsi his data services hydrodesktop the cuahsi his data access client ames et al 2012 provided an early one stop shopping platform to hydrologists by enabling map based selection of a watershed or the extent of the domain of interest and data download extraction and analysis this desktop functionality has now been replaced by the cuahsi data client web tool http data cuahsi org for cuahsi his data selection and extraction agencies such the u s geological survey usgs http waterservices usgs gov and national oceanic and atmospheric administration national centers for environmental information noaa ncei http www ncdc noaa gov cdo web webservices and other data and model service providers have also made data from their repositories accessible using web services and data standards such as waterml and other ogc web service standards almoradie et al 2013b these systems help reduce the time spent by researchers searching for and downloading data while availability of hydrological data through web services from sources such as cuahsi his usgs noaa or other organizations is growing pre processing is often needed to generate suitable inputs to hydrological models in addition cuahsi his compliant data services are currently limited to time series data at fixed geographic locations e g points using the observations data model odm horsburgh et al 2008 no support is provided in cuahsi his services for multi dimensional space time data such as those stored in network common data form netcdf data format rew et al 2014 hence part of the data pre processing tasks for distributed hydrological models involves organizing data in the input format suitable for the specific model often arrays of space time data input data pre processing often starts with geospatial analyses including watershed delineation stream network generation and specification of modeling units such as hydrologic response units hru or structured or unstructured grids of required spatial resolution then input variables based on the watershed terrain land cover characteristics and climate forcing are mapped to the modeling units carlson et al 2014 this mapping of continuous or discrete values to model units may require aggregation or interpolation in both space and time extraction of hydrological variables from digital elevation models e g terrain slope aspect topographic wetness index is also part of the pre processing required to develop model inputs in addition some model parameters need to be generated or estimated from observations for example land cover variables such as canopy indices have to be derived based on land cover type maps or from remote sensing images and friction coefficients have to be estimated from the vegetation and geomorphological information of river reaches these data pre processing tasks can take a significant portion of the hydrological modeler s time and effort and data pre processor tools have been shown to considerably reduce the time required for model scenario setup and execution berry et al 2014 an additional benefit is reproducibility and the opportunity to support best of practice pre processing methods rather than expedient methods that may be selected by a user preparing model inputs manually using general purpose tools available to them 2 2 web based data and modelling services at present many geospatial data analyses are carried out using desktop based gis tools some of these gis tools are stand alone software products such as the arcgis software suite from esri http www esri com or the open source qgis http www qgis org en site and grass http grass osgeo org software others are integrated with the hydrologic models they prepare inputs for there is commercial and open source modeling software that supports input data pre processing as an integral part of hydrologic modeling one example of commercial software is the mike she model s gis based graphical user interface and gis database from dhi http www dhigroup com an example of open source model data processing tools is pihmgis bhatt et al 2008 2014 kumar et al 2009 in which a gis framework for model input pre processing and input and output visualization is tightly coupled to the penn state integrated hydrologic model pihm http www pihm psu edu with the increasing availability of cyberinfrastructure there is an opportunity to extend model input data pre processing tools to web based services such web based services could build on or provide additional services to the general purpose geospatial and hydrologic data services such as cybergis arcgis online and hydroterre cybergis https cybergis illinois edu is a web based approach to the delivery of gis functionality as data and software services wang et al 2013 wang 2010 wright et al 2013 cybergis supports large scale data intensive modelling problems with spatial analysis tools that require more than just a few processing cores arcgis online http arcgis com is an extension of arcgis to a web based service that enables the rapid growth of available content provides enhanced capability through being able to access cloud based resources sharing and collaboration around geospatial developments etc wright et al 2013 hydroterre is a web based hydrologic model data and visualization service http www hydroterre psu edu hydroterre help ethos aspx that has made available about 200 tb of essential terrestrial variables etvs including elevation soils geology land cover precipitation and atmospheric conditions sub watersheds and national hydrography dataset nhd stream reaches data are indexed by usgs nhd hydrological unit code level 12 huc 12 sub watersheds and can be downloaded to support detailed hydrologic modelling using pihm or other models leonard and duffy 2013 2014 2016 hydroterre data model and visualization workflows capture provenance and enable reproducibility leonard and duffy 2016 there are also developments such as ecohydrolib and rhessysworkflows miles and band 2015 and waterhub http water hub org that deal with specific models ecohydrolib was developed as a set of general data access and processing libraries that form building blocks for rhessysworkflows the input data preparation workflows to generate instances of the regional hydroecological simulation system rhessys model miles and band 2015 rhessysworkflows preserve metadata that enable reproducibility miles 2014 a web based modeling service is provided by waterhub which allows parameterized swat soil water assessment tool models and their input data to be uploaded run on hpc resources and shared among users merwade et al 2012 this service also provides a web based data preparation and modeling environment access to existing models their input output datasets and a mechanism to perform simultaneous simulations rajib et al 2016 the web based technologies underlying these services enable taking advantage of high performance computation resources provided they are available distributed data storage facilities analysis tools from multiple service tool providers to deal with spatial big data evans et al 2013 and collaboration between researchers possibly remotely located from each other from a user client point of view spatial analysis capabilities are readily accessible through the world wide web without requiring any local software installation this eliminates data size limitation of pcs the need to install software and operating system platform dependence a web based development environment facilitates better collaborative problem solving nyerges et al 2013 wang 2010 eases access to analytic tools e g geospatial analyses for non experts wright et al 2013 and enables implementation of science gateway functionalities that provide access to hpc centers wilkins diehr et al 2008 an example web based development environment integrating multiple of the above functionalities is hydroshare http www hydroshare org hydroshare is a collaborative environment for sharing hydrologic data and models taking advantage of modern information communication technology and cyberinfrastructure hydroshare supports the capability for users to store their work in the hydrologically oriented resource formats including time series geographic features and rasters and model programs and instances hydroshare resources created by one user may be shared with others and hydroshare s web service application programming interface api enables programmatic access to create and or work directly with resources stored in the system horsburgh et al 2016 morsy et al 2017 tarboton et al 2014a 2014b 3 development of a set of web based hydrologic data processing services hydrods 3 1 required functionality the first step in the development of hydrods was identifying the functionality of data services required to support the input pre processing for physically based gridded models commonly used in surface water hydrology i e etvs this was influenced by input data pre processing tasks shown in figs 1 and 2 for the ueb and topnet models respectively these figures represent workflows of tasks that are required to be executed to get the inputs for the ueb and topnet models for a given watershed and specific modeling period both these models require input data characterizing terrain slope and aspect land cover canopy type information and weather forcing inputs of precipitation temperature relative humidity wind speed and solar radiation in addition the topnet model requires soil data topographic wetness distribution data and distance to stream distribution for runoff routing in both cases the input data preparation starts with the definition of the modeling domain that often requires watershed delineation based on digital elevation model dem processing prior to the development of hydrods the acquisition and preprocessing of input datasets for the ueb and topnet models had to be done manually the steps involved include watershed delineation generation of modeling elements grids in ueb subwatersheds in topnet extraction of terrain variables from dem estimation of canopy variables based on datasets such as the national land cover database nlcd homer et al 2015 or satellite remote sensing products e g modis in addition operations are required for spatial interpolation aggregation and downscaling of weather forcing into model grid cells from gridded data sources or weighted interpolation of point precipitation gages conversion of gridded spatial datasets into modeling parameters computation of hydrologically relevant model specific variables from topography datasets e g wetness index and distance to stream distributions generation of soil properties from soil survey geographic database ssurgo soil survey staff 2019 and file format conversions to the formats used by ueb and or topnet models are also part of the input preprocessing tasks undertaking these tasks requires significant understanding of the data sources and gis skills and it takes considerable time and effort especially for new users that must learn the data processing steps and complicated software configuration as well as the requirement for documenting their work in a reproducible way the hydrods data services are needed to support execution of workflows similar to figs 1 and 2 as web services so that a user does not need to undertake these tasks manually on a desktop pc the ueb and topnet models were selected as the starting models this was because of the need to be able to efficiently set up multiple ueb models for use in water supply forecasting research gichamo 2019 and the fact that topnet is already in use in streamflow forecasting applications e g clark et al 2008 and was being used for hydrologic modeling examining the impact of climate change on streamflow regime sazib 2016 these requirements provided impetus for developing general purpose model setup capability while developed for these specific models the data required are also commonly used in other distributed hydrologic models e g precipitation temperature relative humidity wind speed radiation and the services developed here have potential to be more broadly applicable to other models in addition the three data formats used by hydrods shapefile geotiff and netcdf are among the most widely used formats for representing these classes of data the required functionality identified included select a model domain geographic location of watershed of interest and if necessary delineate the watershed draining to an outlet point compute hydrological variables from a digital elevation model dem including slope aspect topographic wetness index etc estimate canopy variables and vegetation indices such as the leaf area index based on the national land cover database nlcd generate soil properties based on soil survey geographic database ssurgo data calculate wetness index and distance to stream distribution create node and reach link information for topnet model calculate weights used to interpolate precipitation from gage locations to model grid cells perform coordinate system conversions resampling and sub setting to the desired model scale including grid spacing support and extent retrieve weather forcing data from national data sources e g daymet nasa nldas and process and map to model elements convert between data formats e g geotiff raster to netcdf and vice versa carry out arithmetic operations on array data stored in netcdf or geotiff formats create hydroshare resources from data generated by hydrods the data may be individual files such as a watershed delineated from a dem or a set of model inputs and or outputs also support moving existing resources in hydroshare to hydrods for processing create a model instance input package e g all of the required input files to execute a model for a selected geospatial domain miscellaneous file manipulation services such as upload download delete zip show metadata of a resource etc authentication and user access control for security saving work within a storage space allocated for a user and managing the contents of this storage 3 2 design and architecture fig 3 shows the high level organization of hydrods including hydrods services and hydrods python client library the hydrods services are restful apis https en wikipedia org wiki representational state transfer these services consist of data processing and user space and account management tools that were designed to meet the requirements listed above these services can be categorized into two major types as 1 services providing general etvs for the modeling domain and 2 model specific ueb topnet services the etvs are variables that can be applied to other models that take as input gridded datasets in netcdf or geotiff file formats examples of etv services include watershed delineation generation of stream networks processing of weather and soil data model specific services include creation of node and reach link files topnet creation of wetness index topnet and creation of model parameters ueb and topnet table 1 and table 2 show sample services for each category in addition we added functions for common hydrological data processing tasks such as interpolation resampling and projection of geospatial data some of the data accessible through hydrods are staged on the hydrods servers for fast access however time variable data such as meteorological forcing need to be periodically updated by harvesting the data for recent years after it has become available the hydrods services are comprised of tools implemented as a set of python functions for accessing and processing of data in raster geotiff vector shapefile and multi dimensional space time netcdf formats each tool contains one or more atomic data processing functions each function with a single task thus for the tools that comprise hydrods the design and implementation approach we followed was that each function is a stand alone service that gets executed separately the account management functions provide user authentication services as well as ability for the users to manage the files in their user space with functionality to upload or download data to or from their user space in hydrods with the linkage between hydrods and hydroshare a user is able to transfer data processed in hydrods to hydroshare this provides a mechanism by which data and model packages created by one user may be shared with others tarboton et al 2014a 2014b the hydrods python client library is a set of python functions that can be invoked from user computer to make calls to hydrods for each data service function on the server side a corresponding interface is implemented in the hydrods client library the hydrods client library makes it easier to access these data services and thus facilitates scripting and execution of workflows that use the services from a programming environment on a desktop computer the hydrods client library can also be used by desktop applications to access the data services example client software that interacts with hydrods through the client library is shown in fig 4 this google map based graphical user interface gui program was developed using python to enable calling hydrods watershed delineation function by graphically specifying the bounding box around the watershed of interest and watershed outlet location upon a request from a user desktop through the python client library the data services are executed on the server side where needed service libraries and dependencies have been installed and configured freeing the user from these dependency configuration challenges 3 3 implementations the watershed and terrain services are based on functions from the taudem tarboton 2015 tesfa et al 2011 tarboton et al 2009a and gdal geospatial libraries gdal development team 2014 the watershed and terrain functions deal with rasters and shapefiles and hence functions for creating and editing these file formats also make up part of the services the watershed tools delineate the watershed upstream of the outlet location after extracting a subset of the dem and resampling it to the required grid cell size in addition to watershed delineation a stream network is defined and delineated based on the taudem peuker douglas valley identification and stream drop approach peucker and douglas 1975 tarboton and ames 2001 this approach chooses the appropriate threshold to delineate a stream network consistent with geomorphological properties the outputs from this tool are stream network subwatersheds draining to each stream network reach wetness index and distance to stream the terrain functions involve processing of a raw dem and extraction of hydrological variables such as slope and aspect currently a dem data file containing the one arc second 30 m spatial resolution national elevation dataset ned dem covering the western u s 128 0017 to 101 9983 longitude and 28 9983 to 50 0017 latitude is available on the hydrods server as the starting point for the watershed and terrain functions the western u s was the focus of the research project supporting this work to model a watershed outside of the western u s but in the contiguous u s conus hydrods has a wrapper function that is used to download at run time the one arc second dem from usgs web services ftp rockyftp cr usgs gov vdelivery datasets staged ned 1 img based on user specified boundary information in geographic coordinates if a user wants to use different dem data than those currently served by hydrods they can upload their own dem or move a raster resource from hydroshare to their user space in hydrods the land cover services use the 2011 national land cover database homer et al 2015 together with a look up table of canopy variables for each land cover category to map the canopy variables into the watershed grid these services are limited by the empirical canopy variable values available for each land cover class and currently apply only to the variables canopy height fraction of grid cell area covered by vegetation and leaf area index that are required by the ueb model these can be updated when more and or better information become available for example vegetation variables from remotely sensed moderate resolution imaging spectroradiometer modis https modis gsfc nasa gov products can be uploaded by the modeler into their working directory in hydrods and used the soil data services provide rasters of soil properties such as soil hydraulic conductivity transmissivity and porosity for the delineated watershed based on data from the ssurgo database http websoilsurvey nrcs usda gov ssurgo segments the landscape into soil map units with each unit comprised of a number of components each of which represents the soil as a number of layers horizons there is a nrcs soil data access service that provides horizon level soil properties based the components in each map unit we hosted the soil map unit key raster which is static information but to obtain soil properties invoke the nrcs service on the fly to retrieve horizon soil properties for map units contained within the input watershed a two step weighting process for deriving soil unit average soil properties was implemented using r first the horizon level soil values are weighted by their thicknesses and then the component values are weighted by their percentage composition the aggregate soil property values are converted into an r raster object with cell values containing soil properties this function used functionality from existing r packages such as soildb ssoap and raster the results of this function are soil properties rasters for the watershed the climate services provide access to and processing capabilities for data in netcdf format daily data for precipitation maximum and minimum temperature vapor pressure shortwave radiation snow water equivalent and day length from daymet thornton et al 2014 with 1 km spatial resolution covering the conus for the period 2005 2015 are currently available in the hydrods server to facilitate efficient access there is also a wrapper function using the daymetr codes and raster packages to download daymet precipitation temperature and vapor pressure data for a specific time period and a specific watershed this function is based on the batch downloading utility of daymetr to download weather variables at multiple points and convert them to daily interpolated surface weather variables hourly data of precipitation temperature surface pressure shortwave and longwave radiation zonal and meridional wind speed and specific humidity from the national land data assimilation system nldas mitchell et al 2004 with horizontal resolution of 0 125 degree geographic coordinates covering the conus are available for the period 2005 2015 the nldas data are organized in yearly netcdf files for efficiency the climate services include functions for downscaling and elevation adjustment of temperature precipitation and vapor pressure based on a downscaling methodology described by sen gupta and tarboton 2016 the model specific services rely on and build upon these general services for topnet model the create reach and node link function shown in table 2 generates files and tables that define the association between model nodes sub catchments river riches and their properties this function uses outputs obtained from the watershed delineation service similarly the topographic wetness index distribution inputs to topnet are computed from outputs generated by the watershed delineation the ueb and topnet model parameters are time invariant and describe the unchanging properties of the watersheds and subwatersheds for topnet these are expressed at the spatial scale of a subwatershed these parameters are derived by averaging over the grid cells within the subwatershed for topnet the create model parameters function uses extracted soil land use and land cover data as inputs and aggregated parameter values for each subwatershed are written into the model parameters file for ueb parameters are assumed constant over the whole watershed and generally taken to be transferrable across watersheds without requiring calibration topnet is configured to derive aggregated subwatershed precipitation inputs as a weighted sum of point precipitation measurements the weights associated with each gauge for each subwatershed are calculated as part of the pre processing by the create rainweight function using linear interpolation based on delaunay triangles formed with a vertex at each rain gauge adjusted using an annual rainfall surface to account for topographic effects the method for determining precipitation weights is described in bandaragoda et al 2004 this procedure provides a way to estimate precipitation as a smooth surface based on nearby surrounding gauges while at the same time adjusting point gauge values for topographic effects the adjustment for topographic effects is required because often precipitation is recorded at low elevation and hence may not accurately represent the precipitation in parts of the watershed with higher elevation the web services were written in python and implemented in django python web framework https www djangoproject com the service code uses existing functions as much as possible and provide python or r wrappers to functions from taudem gdal netcdf libraries rew and davis 1990 rew et al 2006 netcdf operators nco zender 2008 and national web services such as those from usgs daymet epa ssurgo figs 5 and 6 illustrate the operations carried out for the taudem peuker douglas watershed delineation and the ssurgo soil data services respectively as stated earlier these services were implemented on a web server and they are accessed from a user desktop through a single python library hydrods python client library when using the python client the only software required by a user is a python interpreting environment with the python requests module http docs python requests org en latest installed transmission of function calls and data transfer between client and server uses hydrods restful apis over http protocol 4 evaluation of hydrods with input data preparation for the utah energy balance snowmelt model ueb and the topnet hydrologic model 4 1 motivation and case studies the colorado basin river forecast center cbrfc provides streamflow forecasts for watersheds in the colorado river and great salt lake basins cbrfc basin where a significant portion of the annual surface water input comes from snowmelt that primarily falls in the mountainous headwater watersheds currently the cbrfc uses the national weather service river forecasting system nwsrfs that consists of a temperature index snowmelt model anderson 1973 2006 peck 1976 burnash and singh 1995 the motivation for this case study arose from the desire to evaluate the ueb snowmelt model tarboton et al 1995 for inclusion in the nwsrfs ueb is a physically based point energy and mass balance model with a single ground snowpack layer and a vegetation component that accounts for major snow processes in forested watersheds mahat et al 2013 mahat and tarboton 2013 2014 2012 luce and tarboton 2010 you et al 2014 as a single layer model ueb is parsimonious avoiding some of the complexities for more detailed multi layered snowmelt models in addition the gridded version of the model has parallel processing capability using message passing interface mpi and graphics processing unit gpu methods to speed up simulation gichamo and tarboton 2020 these factors make ueb a promising candidate for spatially distributed modeling in support of operational streamflow forecasting where computational time can be critical gichamo and tarboton 2019 one of the issues that needed to be addressed in order to be able to use ueb in the streamflow forecasting system was whether the input data available for the energy balance model were of sufficient quality and could be efficiently prepared for forecast watersheds in this study we evaluated the hydrods for preparation of the inputs to the ueb model for multiple forecast watersheds in the cbrfc basin we quantified how much improvement was achieved by hydrods when compared to desktop based gis tools in terms of the time taken to prepare input data using each approach we also demonstrated the value of the data services to facilitate repeatability and reproducibility and the tracking of provenance through an automated workflow script in addition the use of web services reduces the need for individual users to have a local data copy and data organizing software in addition we evaluated the services for preparation of input for the topnet model for one of the cbrfc forecast watersheds topnet bandaragoda et al 2004 ibbitt and woods 2004 is a distributed hydrologic model in which topographically delineated subwatersheds used as modeling units discharge into the stream network the stream network is then used to route streamflow to the watershed outlet topnet was developed by combining topmodel beven and kirkby 1979 beven et al 1995 with channel routing bandaragoda et al 2004 ibbitt and woods 2004 a key contribution of topmodel is the parameterization of the soil moisture deficit depth to water table using a topographic index to model the dynamics of variable source areas contributing to saturation excess runoff bandaragoda et al 2004 p 179 additional enhancements in topnet beyond the original topmodel include 1 calculation of reference evapotranspiration using the asce standardized penman monteith method asce ewri 2005 walter et al 2000 and 2 calculation of snowmelt using the utah energy balance snowmelt model tarboton et al 1995 4 2 study watersheds and input data preparation currently the cbrfc models are structured into watersheds that flow to nws streamflow forecast points as such the modeling units are forecast watersheds for which input data are structured independently this makes the procedure manageable to apply the ueb model for streamflow forecasting in the cbrfc basin we needed to set up a model instance for each forecast watershed making a model setup for each watershed using desktop tools currently in use can be time consuming error prone and hard to reproduce recognizing that the same set of data setup operations need to be carried out for each watershed a workflow script to pre process input data for one watershed can be reused for multiple watersheds a number of headwater watersheds in the colorado river basin and the great salt lake basin were selected to set up ueb inputs fig 7 these watersheds were selected because the cbrfc had an interest in evaluating potential improvements to their forecasts from using ueb the hydrods tasks required to get complete ueb model inputs for a given watershed are shown in the flowchart in fig 8 this workflow is encapsulated in a single script file provided as a hydroshare resource gichamo et al 2020 the inputs to this workflow script for a given watershed are the geographic coordinates of the bounding box of the domain holding the watershed outlet location start and end time model target cell size and the spatial reference projection in the form of epsg code http spatialreference org ref epsg to be used for the output the commands in the workflow script can also be called interactively from any python interpreter or as mentioned earlier the service functions can be called from a user application such as shown in fig 4 topnet input preparation was tested for the logan river watershed also a cbrfc forecast watershed to setup a topnet model input package a user needs to provide geographic coordinates for the bounding box around the watershed of interest the approximate outlet location a range of stream threshold values from which an optimum threshold value is estimated for defining the stream network and the modeling period note here that if the user provides incorrect inputs the services report an error and quit once complete the user is provided with a link from which the processed data can be downloaded an example script for the preparation and saving of topnet model input in hydroshare is provided in sazib and tarboton 2020 this script implements the steps shown in fig 9 the topnet input package was also generated manually for the logan river following fig 2 described above as part of the evaluation of hydrods the manually derived topnet input files were then compared with those from hydro ds services and found to match well validating the data services however minor differences 1 5 were found in subwatershed soil properties values due to use of a gridded map unit key raster in hydrods instead of the map unit key shapefile for extracting and processing soil properties from ssurgo the gridded map unit key raster data have a 30 m cell size that approximates the vector polygon of the map unit key in an albers equal area projection this approximation results in the small differences noted above 5 results and discussion 5 1 time spent on input data preparation table 3 shows the time it takes for preparation of ueb input data for the logan river watershed for the water year 2009 for three methods manually on desktop pc using automating scripts on desktop pc and using hydrods through a workflow script running the hydrods script for a different watershed only requires modification of the watershed boundary location of the outlet and projection information as mentioned earlier it took 10 min to run the pre processing and package it and put it into hydroshare it took comparable total time between 9 and 15 min to prepare inputs for the other study watersheds using hydrods as shown in table 4 preparing the inputs manually by the first author with multiple years of experience using desktop based gis software took more than 5 h which was cut to 2 h and 45 min by simply automating the desktop tasks using scripts and that was further reduced to only 10 min when using hydrods the scripts that were used in the desktop environment are similar to those implemented in hydrods thus the difference between the time it takes hydrods to prepare the inputs versus the time taken by scripts on a desktop pc can partly be attributed to the efficient organization of the data in hydrods on a desktop pc even when using scripts that automate the processes user intervention is necessary for instance to locate the delineated watershed and point it to the scripts that run weather forcing pre processing because all the other inputs terrain canopy weather forcing have to be mapped onto the watershed grid file that defines the modeling domain preparing the script to run the hydrods took about 30 min for someone who was already well familiar with the system which is a one time task after which the same script can be re used for different watersheds by only changing the user inputs shown in fig 8 we also report in table 3 the time it took to download data to a desktop pc separately because theoretically at least this is a one time operation note most of the time here was taken by nldas weather data we did not account for the time required to harvest the daymet nldas ned dem and nlcd land cover data into hydrods data servers this task of updating the hydrods data stores with new data when they become available is a one time task which then makes data available to multiple users however we note that while the hydrods data disks at the time of this writing can store up to 10 tb of data the desktop pc on which the test was carried out has a hard disk with a capacity of 500 gb thus once the pre processing of the inputs was finished the intermediate files had to be deleted to free up storage space therefore if we need to carry out similar operations say in few months downloading the data again might be necessary another observation in tables 3 and 4 is that the time for weather forcing data processing is dominated by the wind data from nldas this is because the nldas data has hourly temporal resolution for the entire conus compared to the daily temporal resolution of the daymet data in addition the hourly data for each nldas weather forcing variable comes in an individual netcdf file to increase efficiency of hydrods the nldas data in hydrods were pre organized so that one netcdf file contains data for a year for each variable which considerably reduces the amount of processing effort therefore ignoring the time for downloading data into the desktop pc much of the difference in the nldas data processing time between hydrods and the desktop pc arises from the prior organization of nldas data in hydrods this is an optimal option because the nldas data after harvesting from nasa servers were processed and organized only once before being stored on the hydrods server then multiple users can benefit from this organization thus avoiding redundant and potentially error prone data processing by different users or by the same user multiple times for topnet model the work to prepare a model input package for the logan river watershed using hydrods took about 7 min table 5 when using hydrods the user does not need to remember the specific details of the sequence of steps to follow as they were recorded in the workflow script also helping reproducibility manually setting up the model for the logan river watershed took about 2 h by the second author representing a knowledgeable user familiar with the procedure the difference between the time it takes using hydrods and that on desktop pc can again be attributed to user intervention for downloading the geospatial and time series data manually from the data provider websites and the number of basic geospatial processing tasks that need to be carried out sequentially the topnet input data package created by hydrods was then shared through hydroshare using the create hydroshare resource function this sharing of the topnet input package enables collaboration among team members working on this model it also facilitates publication of the data in support of research findings being published from the results thereby enhancing research reproducibility and trust in the model results here the transfer to hydroshare occurred between servers independent of the user s desktop system a mode of working more amenable to large datasets because data do not have to be copied into the user s desktop pc the shared topnet input package was then downloaded from hydroshare to a local pc where parameters were calibrated and sensitivity analysis was performed this demonstrated the suitability and usability of the hydrods generated package in a typical hydrologic modeling exercise by a graduate student 5 2 workflow scripts reproducibility and provenance the services demonstrably reduced the time and effort required to prepare ueb and topnet inputs which enables water scientists to spend less time extracting and formatting data however in the long run a more useful benefit arises from the fact that the workflow script maintains the provenance of the data processing steps making it easier for modeling workflows to be shared and scientific results to be reproduced leonard and duffy 2014 leonard et al 2019 miles and band 2015 miles 2014 for instance few months after first using the script to prepare the logan river watershed we came back and used the script again with no additional work required and obtained the exact same result thus hydrods facilitates reproducibility and repeatability of hydrologic data processing in addition by changing the user inputs shown in fig 8 the same script can be used for a different watershed this way hydrods facilitate speedy setup of models for the multiple forecast watersheds such as in the cbrfc basin in addition it provides the and ability to take advantage of a pre configured system where the user need not be concerned about the organization of the server side functions data software and hardware where the dependencies are already sorted out by providing the capability to automate the data processing steps preserving provenance and enhancing the reproducibility and repeatability of the hydrologic data processing hydrods thus provides a number of benefits of standard workflow systems goble and de roure 2009 while simplifying the responsibility of the user to handling a single python workflow script more generally the outcome of this work is a development of server side data processing services where lessons learned from the experience could be applied for other models one lesson learned based on our observations using the services was that the provision of access to atomic functions through the hydrods python client library to call individual tasks appears to be not that useful as workflow scripts combining multiple related tasks are often the ones that are applied therefore provision of coarser grained convenience functions e g providing watershed delineation but hiding the constituent functions such as move outlets to streams may be more productive the biggest limitation of hydrods as it stands currently is the fact that the services are limited to gridded data such as those used in the ueb model and subwatershed based inputs customized for the topnet model a number of hydrologic models use unstructured grids or other modeling units such as hydrologic response unit hru the data processing services need to accommodate for such modeling configurations if they are to be used by the wide range of models currently used by the hydrologic community a related but less critical limitation of hydrods is that it only supports geotiff shapefile and netcdf file formats the hierarchical data format hdf https www hdfgroup org is as widely used as netcdf and would add additional flexibility to hydrods if it were supported an alternative is to add hydrods functions for conversion of data from netcdf to other standardized data formats such as hdf and vice versa another limitation of this study is that all the watersheds evaluated were headwater watersheds whose final pre processed and ready to be used in the model input data have relatively small file sizes less than 2 gb the work in this paper deals with large basins such as the colorado river basin by breaking them down into cbrfc forecast watersheds and handling data processing for smaller individual watersheds dealing with individual forecast watersheds with relatively small sizes was a design choice that keeps the size of the data and the computational resources for pre processing of a single watershed easily manageable while taking advantage of automation to address multiple watersheds characterizing how the services perform when increasing the sizes of the watersheds for example by integrating multiple adjacent watersheds may be an important next step in such a scenario the size of the weather forcing data increases more rapidly than the other data types and weather data processing services which currently use serial codes may have to deal with large datasets in netcdf format which could necessitate implementation of parallel processing additional work is also required to deal with the potential increase in processing time due to increase in size of processed data for example a mechanism for queuing and batch processing of large operations with asynchronous notifications to a user that the batch of tasks from a workflow script is completed would be useful this is because it would not be feasible for the user to wait for the web services to return when the execution time extends beyond the 10 min reported in this paper as stated earlier the hydrods services were tested for selected watersheds in the cbrfc basin and while the nlcd land cover data and the nldas and daymet weather forcing data for the years 2005 2015 stored at the hydro ds server cover the whole conus the ned dem and soil map unit rasters hosted at hydrods server are limited to the western us 128 0017 to 101 9983 longitude and 28 9983 to 50 0017 latitude this presents additional challenge to the applicability of the services for watersheds outside of western u s still a user can upload their own data and use the data processing tools although this was not the primary mode of application envisioned for the services at the outset currently extending the available service functionalities requires obtaining appropriate credentials and familiarity with the development environment and the underlying technologies including django gdal taudem netcdf library and netcdf operators nco in addition to python programming skills future developments should consider a simplified way to extend the services to cover more geospatial processing tools and data one way to enable a relatively easy extension of the services by addition of new functionalities is adding a software development kit sdk as a component of the hydrods services the sdk could be as simple as providing sample source codes to modify for new functions or support more advanced features such as tools and libraries to serve as building blocks for new tools functions finally while these results demonstrate that hydrods helps reduce the time and effort required for accessing and pre processing model input data the task of deciding on what hydrological questions to ask depends on the researcher s prior experience in this study deciding the case study involved a number of iterations 6 summary and conclusions hydrods a set of web based data services providing access to distributed gridded and subwatershed based hydrologic data and geospatial and temporal data analysis capabilities for hydrological models was introduced in this paper the services comprise functions for important hydrologic data processing tasks such as watershed delineation terrain processing estimation of canopy variables based on the nlcd generating soil properties based on data from ssurgo and accessing and processing of climate data from daymet and nldas the services are composed of single task functions that can be used independently or can be chained together to form a python workflow for complete generation of model inputs a python library the hydrods client library provides access to the web services through the hydrods client library the services can be used in a python script or desktop applications accessing the services requires only python which means that users can access them from any computing platform with python support hydrods was demonstrated by setting up instances of the utah energy balance ueb and topnet models for watersheds in the colorado river and great salt lake basins the cases demonstrate how hydrods helps reduce the time and effort spent for accessing and pre processing hydrologic model input data a considerable part of the time saved by using hydrods instead of desktop based data processing comes from better organization of data in hydrods the python scripting based data processing workflows enhance reproducibility and repeatability because the same script can be re used the script needs to be modified only to specify few user inputs when used for a different watershed as the workflow script also captures all the steps towards the final model input its provenance is preserved in the script the software as a service paradigm of the web services provides capability for multiple users and relieves users from concerns related to storage and organization of data which is done in the server and software and hardware dependencies which are sorted out when the software is configured on the server based on our observations using the services the provision of access through the hydrods client library to the atomic functions to do individual tasks appears to be not that useful rather the workflow scripts combining multiple coarser granular functions were more productive the work in this paper deals with large basins such as the colorado river basin by breaking them down into cbrfc forecast watersheds and handling data processing for smaller individual watersheds this was a design choice that worked well for this study future studies should address the alternative approach of processing river basins such as the colorado basin as a whole future work should also extend the services to provide inputs for unstructured grid models and models using hrus or other equivalent tessellations of the landscape for hydrods to support a wider range of hydrologic models future development should consider provision of software development kit sdk in hydrods to enable a relatively easy extension of the services with new functionalities declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the u s national science foundation nsf under collaborative grants eps 1135482 and 1135483 and by the utah water research laboratory uwrl any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf or uwrl compute storage support and other resources from the division of research computing in the office of research and graduate studies at utah state university and advanced research computing center at the university of wyoming are gratefully acknowledged 
26076,physically based distributed hydrologic models require geospatial and time series data that take considerable time and effort in processing them into model inputs tools that automate and speed up input processing facilitate the application of these models in this study we developed a set of web based data services called hydrods to provide hydrologic data processing software as a service hydrods provides functions for processing watershed terrain canopy climate and soil data the services are accessed through a python client library that facilitates developing simple but effective data processing workflows with python evaluations of hydrods by setting up the utah energy balance and topnet models for multiple headwater watersheds in the colorado river basin show that hydrods reduces the input preparation time compared to manual processing it also removes the requirements for software installation and maintenance by the user and the python workflows enhance reproducibility of hydrologic data processing and tracking of provenance keywords hydrods web based data services distributed hydrologic modeling geographic information systems hydrologic data cyberinfrastructure software availability program name hydrods description a set of web based hydrologic data services for preparation of input data for selected physically based distributed grid or subwatershed model elements hydrologic models hydrods comprises python modules for watershed analysis terrain and land cover data processing climate data access and processing and generating soil properties data individual service functions accessed through a python client library may be chained together to form a python workflow to perform a set of related tasks platform centos linux for hosting the web services accessed from any platform license 3 clause bsd license open source source code https github com ci water hydro ds documentation https github com ci water hydro ds wiki hydrods web api description developers tseganeh z gichamo nazmus s sazib david g tarboton pabitra dash 1 introduction physically based distributed hydrologic models are used for simulation of the hydrologic cycle to help answer questions related to water resource availability and quality to assess the effect of changes in climate or land cover and support water resources management along with many other applications an important challenge associated with the application of physically based distributed hydrological models is that they require more input data than their conceptual often lumped counterparts while the rationale for high resolution physically based models is that better results can be achieved through detailed process representation obtaining the extensive set of input data required by these models is a critical challenge leonard and duffy 2013 call this set of input data essential terrestrial variables etv obtaining etvs in a format organized for use in distributed models is a significant bottleneck in distributed hydrologic modelling the ability to configure and populate distributed models with data could enhance or hinder their use prior work with regard to hydrologic data availability has focused on the task of enhancing access to data from different providers through web services horsburgh et al 2009 tarboton et al 2009b ames et al 2012 using standardized data formats taylor 2012 almoradie et al 2013a however the data obtained remain the raw data provided by the sources and generally require further processing to generate suitable inputs to hydrological models in terms of the data structure file format and variable type required by a model e g relative humidity rather than specific humidity as a result in our experience researchers and practitioners implementing hydrologic models spend a significant amount of time transforming the data available into the formats needed by models jones et al 2013 the data pre processing tools currently available are generally desktop based and often limited by their customization to specific hydrologic models e g kumar et al 2009 the increasing availability of cyberinfrastructure resources provides an opportunity to extend such data pre processing ability beyond the desktop environment wang et al 2013 and adopt the paradigm of software as a service developing data processing tools as web based services will help to enhance access to these tools for users without necessarily requiring them to be a cyber expert wright et al 2013 web services that can be accessed by multiple users facilitate better collaborative problem solving nyerges et al 2013 wang 2010 in addition they encourage the use of standardized data formats e g waterml and netcdf by multiple models in this paper we introduce a set of web based hydrological data processing services called hydrods hydrods provides a number of data processing functionalities including watershed delineation terrain processing estimation of canopy variables retrieval and processing of weather forcing data procuring soil data and generating soil properties data are stored and shared in three widely used data formats geotiff raster shapefile and multi dimensional netcdf the data services are comprised of functions that can be used independently or form workflows that integrate a number of related tasks the services are accessed through a python client library that facilitates developing simple but effective data processing workflows with python providing access to data processing tools from an accessible and relatively easy to use programming environment data processed by hydrods can be transferred to hydroshare a platform for sharing of hydrologic data and models tarboton et al 2014a the objective here was to provide the means to setup python workflows for preparation of input data for distributed hydrologic models the services we developed support the utah energy balance ueb snowmelt model tarboton et al 1995 and topnet hydrologic model bandaragoda et al 2004 in the next section we provide background information on prior work dealing with data access and processing for hydrologic modeling and the need for web based data services that motivated this work in section 3 we report the required functionality design and implementation of hydrods in section 4 we evaluate the data services using a case study of setting up instances of ueb and topnet models for multiple headwater watersheds in the colorado river basin results and discussion are given in section 5 followed by summary and conclusions in section 6 2 background 2 1 input data processing for hydrologic models providing access to hydrological data from different repositories through web services has been the focus of the consortium of universities for the advancement of hydrologic science inc hydrologic information system cuahsi his horsburgh et al 2009 tarboton et al 2009b cuahsi his provides software tools for publishing and retrieving time series data through standardized web services in an xml format called waterml tarboton et al 2011 valentine et al 2012 2007 beran et al 2009 waterml2 was later developed as an open geospatial consortium ogc standard for hydrologic time series data representation and exchange across multiple information systems taylor 2012 standardized web services and protocols facilitate interoperability between different data service providers and consumers clients for easy access to and retrieval of data client applications can search for and download data made available through the cuahsi his data services hydrodesktop the cuahsi his data access client ames et al 2012 provided an early one stop shopping platform to hydrologists by enabling map based selection of a watershed or the extent of the domain of interest and data download extraction and analysis this desktop functionality has now been replaced by the cuahsi data client web tool http data cuahsi org for cuahsi his data selection and extraction agencies such the u s geological survey usgs http waterservices usgs gov and national oceanic and atmospheric administration national centers for environmental information noaa ncei http www ncdc noaa gov cdo web webservices and other data and model service providers have also made data from their repositories accessible using web services and data standards such as waterml and other ogc web service standards almoradie et al 2013b these systems help reduce the time spent by researchers searching for and downloading data while availability of hydrological data through web services from sources such as cuahsi his usgs noaa or other organizations is growing pre processing is often needed to generate suitable inputs to hydrological models in addition cuahsi his compliant data services are currently limited to time series data at fixed geographic locations e g points using the observations data model odm horsburgh et al 2008 no support is provided in cuahsi his services for multi dimensional space time data such as those stored in network common data form netcdf data format rew et al 2014 hence part of the data pre processing tasks for distributed hydrological models involves organizing data in the input format suitable for the specific model often arrays of space time data input data pre processing often starts with geospatial analyses including watershed delineation stream network generation and specification of modeling units such as hydrologic response units hru or structured or unstructured grids of required spatial resolution then input variables based on the watershed terrain land cover characteristics and climate forcing are mapped to the modeling units carlson et al 2014 this mapping of continuous or discrete values to model units may require aggregation or interpolation in both space and time extraction of hydrological variables from digital elevation models e g terrain slope aspect topographic wetness index is also part of the pre processing required to develop model inputs in addition some model parameters need to be generated or estimated from observations for example land cover variables such as canopy indices have to be derived based on land cover type maps or from remote sensing images and friction coefficients have to be estimated from the vegetation and geomorphological information of river reaches these data pre processing tasks can take a significant portion of the hydrological modeler s time and effort and data pre processor tools have been shown to considerably reduce the time required for model scenario setup and execution berry et al 2014 an additional benefit is reproducibility and the opportunity to support best of practice pre processing methods rather than expedient methods that may be selected by a user preparing model inputs manually using general purpose tools available to them 2 2 web based data and modelling services at present many geospatial data analyses are carried out using desktop based gis tools some of these gis tools are stand alone software products such as the arcgis software suite from esri http www esri com or the open source qgis http www qgis org en site and grass http grass osgeo org software others are integrated with the hydrologic models they prepare inputs for there is commercial and open source modeling software that supports input data pre processing as an integral part of hydrologic modeling one example of commercial software is the mike she model s gis based graphical user interface and gis database from dhi http www dhigroup com an example of open source model data processing tools is pihmgis bhatt et al 2008 2014 kumar et al 2009 in which a gis framework for model input pre processing and input and output visualization is tightly coupled to the penn state integrated hydrologic model pihm http www pihm psu edu with the increasing availability of cyberinfrastructure there is an opportunity to extend model input data pre processing tools to web based services such web based services could build on or provide additional services to the general purpose geospatial and hydrologic data services such as cybergis arcgis online and hydroterre cybergis https cybergis illinois edu is a web based approach to the delivery of gis functionality as data and software services wang et al 2013 wang 2010 wright et al 2013 cybergis supports large scale data intensive modelling problems with spatial analysis tools that require more than just a few processing cores arcgis online http arcgis com is an extension of arcgis to a web based service that enables the rapid growth of available content provides enhanced capability through being able to access cloud based resources sharing and collaboration around geospatial developments etc wright et al 2013 hydroterre is a web based hydrologic model data and visualization service http www hydroterre psu edu hydroterre help ethos aspx that has made available about 200 tb of essential terrestrial variables etvs including elevation soils geology land cover precipitation and atmospheric conditions sub watersheds and national hydrography dataset nhd stream reaches data are indexed by usgs nhd hydrological unit code level 12 huc 12 sub watersheds and can be downloaded to support detailed hydrologic modelling using pihm or other models leonard and duffy 2013 2014 2016 hydroterre data model and visualization workflows capture provenance and enable reproducibility leonard and duffy 2016 there are also developments such as ecohydrolib and rhessysworkflows miles and band 2015 and waterhub http water hub org that deal with specific models ecohydrolib was developed as a set of general data access and processing libraries that form building blocks for rhessysworkflows the input data preparation workflows to generate instances of the regional hydroecological simulation system rhessys model miles and band 2015 rhessysworkflows preserve metadata that enable reproducibility miles 2014 a web based modeling service is provided by waterhub which allows parameterized swat soil water assessment tool models and their input data to be uploaded run on hpc resources and shared among users merwade et al 2012 this service also provides a web based data preparation and modeling environment access to existing models their input output datasets and a mechanism to perform simultaneous simulations rajib et al 2016 the web based technologies underlying these services enable taking advantage of high performance computation resources provided they are available distributed data storage facilities analysis tools from multiple service tool providers to deal with spatial big data evans et al 2013 and collaboration between researchers possibly remotely located from each other from a user client point of view spatial analysis capabilities are readily accessible through the world wide web without requiring any local software installation this eliminates data size limitation of pcs the need to install software and operating system platform dependence a web based development environment facilitates better collaborative problem solving nyerges et al 2013 wang 2010 eases access to analytic tools e g geospatial analyses for non experts wright et al 2013 and enables implementation of science gateway functionalities that provide access to hpc centers wilkins diehr et al 2008 an example web based development environment integrating multiple of the above functionalities is hydroshare http www hydroshare org hydroshare is a collaborative environment for sharing hydrologic data and models taking advantage of modern information communication technology and cyberinfrastructure hydroshare supports the capability for users to store their work in the hydrologically oriented resource formats including time series geographic features and rasters and model programs and instances hydroshare resources created by one user may be shared with others and hydroshare s web service application programming interface api enables programmatic access to create and or work directly with resources stored in the system horsburgh et al 2016 morsy et al 2017 tarboton et al 2014a 2014b 3 development of a set of web based hydrologic data processing services hydrods 3 1 required functionality the first step in the development of hydrods was identifying the functionality of data services required to support the input pre processing for physically based gridded models commonly used in surface water hydrology i e etvs this was influenced by input data pre processing tasks shown in figs 1 and 2 for the ueb and topnet models respectively these figures represent workflows of tasks that are required to be executed to get the inputs for the ueb and topnet models for a given watershed and specific modeling period both these models require input data characterizing terrain slope and aspect land cover canopy type information and weather forcing inputs of precipitation temperature relative humidity wind speed and solar radiation in addition the topnet model requires soil data topographic wetness distribution data and distance to stream distribution for runoff routing in both cases the input data preparation starts with the definition of the modeling domain that often requires watershed delineation based on digital elevation model dem processing prior to the development of hydrods the acquisition and preprocessing of input datasets for the ueb and topnet models had to be done manually the steps involved include watershed delineation generation of modeling elements grids in ueb subwatersheds in topnet extraction of terrain variables from dem estimation of canopy variables based on datasets such as the national land cover database nlcd homer et al 2015 or satellite remote sensing products e g modis in addition operations are required for spatial interpolation aggregation and downscaling of weather forcing into model grid cells from gridded data sources or weighted interpolation of point precipitation gages conversion of gridded spatial datasets into modeling parameters computation of hydrologically relevant model specific variables from topography datasets e g wetness index and distance to stream distributions generation of soil properties from soil survey geographic database ssurgo soil survey staff 2019 and file format conversions to the formats used by ueb and or topnet models are also part of the input preprocessing tasks undertaking these tasks requires significant understanding of the data sources and gis skills and it takes considerable time and effort especially for new users that must learn the data processing steps and complicated software configuration as well as the requirement for documenting their work in a reproducible way the hydrods data services are needed to support execution of workflows similar to figs 1 and 2 as web services so that a user does not need to undertake these tasks manually on a desktop pc the ueb and topnet models were selected as the starting models this was because of the need to be able to efficiently set up multiple ueb models for use in water supply forecasting research gichamo 2019 and the fact that topnet is already in use in streamflow forecasting applications e g clark et al 2008 and was being used for hydrologic modeling examining the impact of climate change on streamflow regime sazib 2016 these requirements provided impetus for developing general purpose model setup capability while developed for these specific models the data required are also commonly used in other distributed hydrologic models e g precipitation temperature relative humidity wind speed radiation and the services developed here have potential to be more broadly applicable to other models in addition the three data formats used by hydrods shapefile geotiff and netcdf are among the most widely used formats for representing these classes of data the required functionality identified included select a model domain geographic location of watershed of interest and if necessary delineate the watershed draining to an outlet point compute hydrological variables from a digital elevation model dem including slope aspect topographic wetness index etc estimate canopy variables and vegetation indices such as the leaf area index based on the national land cover database nlcd generate soil properties based on soil survey geographic database ssurgo data calculate wetness index and distance to stream distribution create node and reach link information for topnet model calculate weights used to interpolate precipitation from gage locations to model grid cells perform coordinate system conversions resampling and sub setting to the desired model scale including grid spacing support and extent retrieve weather forcing data from national data sources e g daymet nasa nldas and process and map to model elements convert between data formats e g geotiff raster to netcdf and vice versa carry out arithmetic operations on array data stored in netcdf or geotiff formats create hydroshare resources from data generated by hydrods the data may be individual files such as a watershed delineated from a dem or a set of model inputs and or outputs also support moving existing resources in hydroshare to hydrods for processing create a model instance input package e g all of the required input files to execute a model for a selected geospatial domain miscellaneous file manipulation services such as upload download delete zip show metadata of a resource etc authentication and user access control for security saving work within a storage space allocated for a user and managing the contents of this storage 3 2 design and architecture fig 3 shows the high level organization of hydrods including hydrods services and hydrods python client library the hydrods services are restful apis https en wikipedia org wiki representational state transfer these services consist of data processing and user space and account management tools that were designed to meet the requirements listed above these services can be categorized into two major types as 1 services providing general etvs for the modeling domain and 2 model specific ueb topnet services the etvs are variables that can be applied to other models that take as input gridded datasets in netcdf or geotiff file formats examples of etv services include watershed delineation generation of stream networks processing of weather and soil data model specific services include creation of node and reach link files topnet creation of wetness index topnet and creation of model parameters ueb and topnet table 1 and table 2 show sample services for each category in addition we added functions for common hydrological data processing tasks such as interpolation resampling and projection of geospatial data some of the data accessible through hydrods are staged on the hydrods servers for fast access however time variable data such as meteorological forcing need to be periodically updated by harvesting the data for recent years after it has become available the hydrods services are comprised of tools implemented as a set of python functions for accessing and processing of data in raster geotiff vector shapefile and multi dimensional space time netcdf formats each tool contains one or more atomic data processing functions each function with a single task thus for the tools that comprise hydrods the design and implementation approach we followed was that each function is a stand alone service that gets executed separately the account management functions provide user authentication services as well as ability for the users to manage the files in their user space with functionality to upload or download data to or from their user space in hydrods with the linkage between hydrods and hydroshare a user is able to transfer data processed in hydrods to hydroshare this provides a mechanism by which data and model packages created by one user may be shared with others tarboton et al 2014a 2014b the hydrods python client library is a set of python functions that can be invoked from user computer to make calls to hydrods for each data service function on the server side a corresponding interface is implemented in the hydrods client library the hydrods client library makes it easier to access these data services and thus facilitates scripting and execution of workflows that use the services from a programming environment on a desktop computer the hydrods client library can also be used by desktop applications to access the data services example client software that interacts with hydrods through the client library is shown in fig 4 this google map based graphical user interface gui program was developed using python to enable calling hydrods watershed delineation function by graphically specifying the bounding box around the watershed of interest and watershed outlet location upon a request from a user desktop through the python client library the data services are executed on the server side where needed service libraries and dependencies have been installed and configured freeing the user from these dependency configuration challenges 3 3 implementations the watershed and terrain services are based on functions from the taudem tarboton 2015 tesfa et al 2011 tarboton et al 2009a and gdal geospatial libraries gdal development team 2014 the watershed and terrain functions deal with rasters and shapefiles and hence functions for creating and editing these file formats also make up part of the services the watershed tools delineate the watershed upstream of the outlet location after extracting a subset of the dem and resampling it to the required grid cell size in addition to watershed delineation a stream network is defined and delineated based on the taudem peuker douglas valley identification and stream drop approach peucker and douglas 1975 tarboton and ames 2001 this approach chooses the appropriate threshold to delineate a stream network consistent with geomorphological properties the outputs from this tool are stream network subwatersheds draining to each stream network reach wetness index and distance to stream the terrain functions involve processing of a raw dem and extraction of hydrological variables such as slope and aspect currently a dem data file containing the one arc second 30 m spatial resolution national elevation dataset ned dem covering the western u s 128 0017 to 101 9983 longitude and 28 9983 to 50 0017 latitude is available on the hydrods server as the starting point for the watershed and terrain functions the western u s was the focus of the research project supporting this work to model a watershed outside of the western u s but in the contiguous u s conus hydrods has a wrapper function that is used to download at run time the one arc second dem from usgs web services ftp rockyftp cr usgs gov vdelivery datasets staged ned 1 img based on user specified boundary information in geographic coordinates if a user wants to use different dem data than those currently served by hydrods they can upload their own dem or move a raster resource from hydroshare to their user space in hydrods the land cover services use the 2011 national land cover database homer et al 2015 together with a look up table of canopy variables for each land cover category to map the canopy variables into the watershed grid these services are limited by the empirical canopy variable values available for each land cover class and currently apply only to the variables canopy height fraction of grid cell area covered by vegetation and leaf area index that are required by the ueb model these can be updated when more and or better information become available for example vegetation variables from remotely sensed moderate resolution imaging spectroradiometer modis https modis gsfc nasa gov products can be uploaded by the modeler into their working directory in hydrods and used the soil data services provide rasters of soil properties such as soil hydraulic conductivity transmissivity and porosity for the delineated watershed based on data from the ssurgo database http websoilsurvey nrcs usda gov ssurgo segments the landscape into soil map units with each unit comprised of a number of components each of which represents the soil as a number of layers horizons there is a nrcs soil data access service that provides horizon level soil properties based the components in each map unit we hosted the soil map unit key raster which is static information but to obtain soil properties invoke the nrcs service on the fly to retrieve horizon soil properties for map units contained within the input watershed a two step weighting process for deriving soil unit average soil properties was implemented using r first the horizon level soil values are weighted by their thicknesses and then the component values are weighted by their percentage composition the aggregate soil property values are converted into an r raster object with cell values containing soil properties this function used functionality from existing r packages such as soildb ssoap and raster the results of this function are soil properties rasters for the watershed the climate services provide access to and processing capabilities for data in netcdf format daily data for precipitation maximum and minimum temperature vapor pressure shortwave radiation snow water equivalent and day length from daymet thornton et al 2014 with 1 km spatial resolution covering the conus for the period 2005 2015 are currently available in the hydrods server to facilitate efficient access there is also a wrapper function using the daymetr codes and raster packages to download daymet precipitation temperature and vapor pressure data for a specific time period and a specific watershed this function is based on the batch downloading utility of daymetr to download weather variables at multiple points and convert them to daily interpolated surface weather variables hourly data of precipitation temperature surface pressure shortwave and longwave radiation zonal and meridional wind speed and specific humidity from the national land data assimilation system nldas mitchell et al 2004 with horizontal resolution of 0 125 degree geographic coordinates covering the conus are available for the period 2005 2015 the nldas data are organized in yearly netcdf files for efficiency the climate services include functions for downscaling and elevation adjustment of temperature precipitation and vapor pressure based on a downscaling methodology described by sen gupta and tarboton 2016 the model specific services rely on and build upon these general services for topnet model the create reach and node link function shown in table 2 generates files and tables that define the association between model nodes sub catchments river riches and their properties this function uses outputs obtained from the watershed delineation service similarly the topographic wetness index distribution inputs to topnet are computed from outputs generated by the watershed delineation the ueb and topnet model parameters are time invariant and describe the unchanging properties of the watersheds and subwatersheds for topnet these are expressed at the spatial scale of a subwatershed these parameters are derived by averaging over the grid cells within the subwatershed for topnet the create model parameters function uses extracted soil land use and land cover data as inputs and aggregated parameter values for each subwatershed are written into the model parameters file for ueb parameters are assumed constant over the whole watershed and generally taken to be transferrable across watersheds without requiring calibration topnet is configured to derive aggregated subwatershed precipitation inputs as a weighted sum of point precipitation measurements the weights associated with each gauge for each subwatershed are calculated as part of the pre processing by the create rainweight function using linear interpolation based on delaunay triangles formed with a vertex at each rain gauge adjusted using an annual rainfall surface to account for topographic effects the method for determining precipitation weights is described in bandaragoda et al 2004 this procedure provides a way to estimate precipitation as a smooth surface based on nearby surrounding gauges while at the same time adjusting point gauge values for topographic effects the adjustment for topographic effects is required because often precipitation is recorded at low elevation and hence may not accurately represent the precipitation in parts of the watershed with higher elevation the web services were written in python and implemented in django python web framework https www djangoproject com the service code uses existing functions as much as possible and provide python or r wrappers to functions from taudem gdal netcdf libraries rew and davis 1990 rew et al 2006 netcdf operators nco zender 2008 and national web services such as those from usgs daymet epa ssurgo figs 5 and 6 illustrate the operations carried out for the taudem peuker douglas watershed delineation and the ssurgo soil data services respectively as stated earlier these services were implemented on a web server and they are accessed from a user desktop through a single python library hydrods python client library when using the python client the only software required by a user is a python interpreting environment with the python requests module http docs python requests org en latest installed transmission of function calls and data transfer between client and server uses hydrods restful apis over http protocol 4 evaluation of hydrods with input data preparation for the utah energy balance snowmelt model ueb and the topnet hydrologic model 4 1 motivation and case studies the colorado basin river forecast center cbrfc provides streamflow forecasts for watersheds in the colorado river and great salt lake basins cbrfc basin where a significant portion of the annual surface water input comes from snowmelt that primarily falls in the mountainous headwater watersheds currently the cbrfc uses the national weather service river forecasting system nwsrfs that consists of a temperature index snowmelt model anderson 1973 2006 peck 1976 burnash and singh 1995 the motivation for this case study arose from the desire to evaluate the ueb snowmelt model tarboton et al 1995 for inclusion in the nwsrfs ueb is a physically based point energy and mass balance model with a single ground snowpack layer and a vegetation component that accounts for major snow processes in forested watersheds mahat et al 2013 mahat and tarboton 2013 2014 2012 luce and tarboton 2010 you et al 2014 as a single layer model ueb is parsimonious avoiding some of the complexities for more detailed multi layered snowmelt models in addition the gridded version of the model has parallel processing capability using message passing interface mpi and graphics processing unit gpu methods to speed up simulation gichamo and tarboton 2020 these factors make ueb a promising candidate for spatially distributed modeling in support of operational streamflow forecasting where computational time can be critical gichamo and tarboton 2019 one of the issues that needed to be addressed in order to be able to use ueb in the streamflow forecasting system was whether the input data available for the energy balance model were of sufficient quality and could be efficiently prepared for forecast watersheds in this study we evaluated the hydrods for preparation of the inputs to the ueb model for multiple forecast watersheds in the cbrfc basin we quantified how much improvement was achieved by hydrods when compared to desktop based gis tools in terms of the time taken to prepare input data using each approach we also demonstrated the value of the data services to facilitate repeatability and reproducibility and the tracking of provenance through an automated workflow script in addition the use of web services reduces the need for individual users to have a local data copy and data organizing software in addition we evaluated the services for preparation of input for the topnet model for one of the cbrfc forecast watersheds topnet bandaragoda et al 2004 ibbitt and woods 2004 is a distributed hydrologic model in which topographically delineated subwatersheds used as modeling units discharge into the stream network the stream network is then used to route streamflow to the watershed outlet topnet was developed by combining topmodel beven and kirkby 1979 beven et al 1995 with channel routing bandaragoda et al 2004 ibbitt and woods 2004 a key contribution of topmodel is the parameterization of the soil moisture deficit depth to water table using a topographic index to model the dynamics of variable source areas contributing to saturation excess runoff bandaragoda et al 2004 p 179 additional enhancements in topnet beyond the original topmodel include 1 calculation of reference evapotranspiration using the asce standardized penman monteith method asce ewri 2005 walter et al 2000 and 2 calculation of snowmelt using the utah energy balance snowmelt model tarboton et al 1995 4 2 study watersheds and input data preparation currently the cbrfc models are structured into watersheds that flow to nws streamflow forecast points as such the modeling units are forecast watersheds for which input data are structured independently this makes the procedure manageable to apply the ueb model for streamflow forecasting in the cbrfc basin we needed to set up a model instance for each forecast watershed making a model setup for each watershed using desktop tools currently in use can be time consuming error prone and hard to reproduce recognizing that the same set of data setup operations need to be carried out for each watershed a workflow script to pre process input data for one watershed can be reused for multiple watersheds a number of headwater watersheds in the colorado river basin and the great salt lake basin were selected to set up ueb inputs fig 7 these watersheds were selected because the cbrfc had an interest in evaluating potential improvements to their forecasts from using ueb the hydrods tasks required to get complete ueb model inputs for a given watershed are shown in the flowchart in fig 8 this workflow is encapsulated in a single script file provided as a hydroshare resource gichamo et al 2020 the inputs to this workflow script for a given watershed are the geographic coordinates of the bounding box of the domain holding the watershed outlet location start and end time model target cell size and the spatial reference projection in the form of epsg code http spatialreference org ref epsg to be used for the output the commands in the workflow script can also be called interactively from any python interpreter or as mentioned earlier the service functions can be called from a user application such as shown in fig 4 topnet input preparation was tested for the logan river watershed also a cbrfc forecast watershed to setup a topnet model input package a user needs to provide geographic coordinates for the bounding box around the watershed of interest the approximate outlet location a range of stream threshold values from which an optimum threshold value is estimated for defining the stream network and the modeling period note here that if the user provides incorrect inputs the services report an error and quit once complete the user is provided with a link from which the processed data can be downloaded an example script for the preparation and saving of topnet model input in hydroshare is provided in sazib and tarboton 2020 this script implements the steps shown in fig 9 the topnet input package was also generated manually for the logan river following fig 2 described above as part of the evaluation of hydrods the manually derived topnet input files were then compared with those from hydro ds services and found to match well validating the data services however minor differences 1 5 were found in subwatershed soil properties values due to use of a gridded map unit key raster in hydrods instead of the map unit key shapefile for extracting and processing soil properties from ssurgo the gridded map unit key raster data have a 30 m cell size that approximates the vector polygon of the map unit key in an albers equal area projection this approximation results in the small differences noted above 5 results and discussion 5 1 time spent on input data preparation table 3 shows the time it takes for preparation of ueb input data for the logan river watershed for the water year 2009 for three methods manually on desktop pc using automating scripts on desktop pc and using hydrods through a workflow script running the hydrods script for a different watershed only requires modification of the watershed boundary location of the outlet and projection information as mentioned earlier it took 10 min to run the pre processing and package it and put it into hydroshare it took comparable total time between 9 and 15 min to prepare inputs for the other study watersheds using hydrods as shown in table 4 preparing the inputs manually by the first author with multiple years of experience using desktop based gis software took more than 5 h which was cut to 2 h and 45 min by simply automating the desktop tasks using scripts and that was further reduced to only 10 min when using hydrods the scripts that were used in the desktop environment are similar to those implemented in hydrods thus the difference between the time it takes hydrods to prepare the inputs versus the time taken by scripts on a desktop pc can partly be attributed to the efficient organization of the data in hydrods on a desktop pc even when using scripts that automate the processes user intervention is necessary for instance to locate the delineated watershed and point it to the scripts that run weather forcing pre processing because all the other inputs terrain canopy weather forcing have to be mapped onto the watershed grid file that defines the modeling domain preparing the script to run the hydrods took about 30 min for someone who was already well familiar with the system which is a one time task after which the same script can be re used for different watersheds by only changing the user inputs shown in fig 8 we also report in table 3 the time it took to download data to a desktop pc separately because theoretically at least this is a one time operation note most of the time here was taken by nldas weather data we did not account for the time required to harvest the daymet nldas ned dem and nlcd land cover data into hydrods data servers this task of updating the hydrods data stores with new data when they become available is a one time task which then makes data available to multiple users however we note that while the hydrods data disks at the time of this writing can store up to 10 tb of data the desktop pc on which the test was carried out has a hard disk with a capacity of 500 gb thus once the pre processing of the inputs was finished the intermediate files had to be deleted to free up storage space therefore if we need to carry out similar operations say in few months downloading the data again might be necessary another observation in tables 3 and 4 is that the time for weather forcing data processing is dominated by the wind data from nldas this is because the nldas data has hourly temporal resolution for the entire conus compared to the daily temporal resolution of the daymet data in addition the hourly data for each nldas weather forcing variable comes in an individual netcdf file to increase efficiency of hydrods the nldas data in hydrods were pre organized so that one netcdf file contains data for a year for each variable which considerably reduces the amount of processing effort therefore ignoring the time for downloading data into the desktop pc much of the difference in the nldas data processing time between hydrods and the desktop pc arises from the prior organization of nldas data in hydrods this is an optimal option because the nldas data after harvesting from nasa servers were processed and organized only once before being stored on the hydrods server then multiple users can benefit from this organization thus avoiding redundant and potentially error prone data processing by different users or by the same user multiple times for topnet model the work to prepare a model input package for the logan river watershed using hydrods took about 7 min table 5 when using hydrods the user does not need to remember the specific details of the sequence of steps to follow as they were recorded in the workflow script also helping reproducibility manually setting up the model for the logan river watershed took about 2 h by the second author representing a knowledgeable user familiar with the procedure the difference between the time it takes using hydrods and that on desktop pc can again be attributed to user intervention for downloading the geospatial and time series data manually from the data provider websites and the number of basic geospatial processing tasks that need to be carried out sequentially the topnet input data package created by hydrods was then shared through hydroshare using the create hydroshare resource function this sharing of the topnet input package enables collaboration among team members working on this model it also facilitates publication of the data in support of research findings being published from the results thereby enhancing research reproducibility and trust in the model results here the transfer to hydroshare occurred between servers independent of the user s desktop system a mode of working more amenable to large datasets because data do not have to be copied into the user s desktop pc the shared topnet input package was then downloaded from hydroshare to a local pc where parameters were calibrated and sensitivity analysis was performed this demonstrated the suitability and usability of the hydrods generated package in a typical hydrologic modeling exercise by a graduate student 5 2 workflow scripts reproducibility and provenance the services demonstrably reduced the time and effort required to prepare ueb and topnet inputs which enables water scientists to spend less time extracting and formatting data however in the long run a more useful benefit arises from the fact that the workflow script maintains the provenance of the data processing steps making it easier for modeling workflows to be shared and scientific results to be reproduced leonard and duffy 2014 leonard et al 2019 miles and band 2015 miles 2014 for instance few months after first using the script to prepare the logan river watershed we came back and used the script again with no additional work required and obtained the exact same result thus hydrods facilitates reproducibility and repeatability of hydrologic data processing in addition by changing the user inputs shown in fig 8 the same script can be used for a different watershed this way hydrods facilitate speedy setup of models for the multiple forecast watersheds such as in the cbrfc basin in addition it provides the and ability to take advantage of a pre configured system where the user need not be concerned about the organization of the server side functions data software and hardware where the dependencies are already sorted out by providing the capability to automate the data processing steps preserving provenance and enhancing the reproducibility and repeatability of the hydrologic data processing hydrods thus provides a number of benefits of standard workflow systems goble and de roure 2009 while simplifying the responsibility of the user to handling a single python workflow script more generally the outcome of this work is a development of server side data processing services where lessons learned from the experience could be applied for other models one lesson learned based on our observations using the services was that the provision of access to atomic functions through the hydrods python client library to call individual tasks appears to be not that useful as workflow scripts combining multiple related tasks are often the ones that are applied therefore provision of coarser grained convenience functions e g providing watershed delineation but hiding the constituent functions such as move outlets to streams may be more productive the biggest limitation of hydrods as it stands currently is the fact that the services are limited to gridded data such as those used in the ueb model and subwatershed based inputs customized for the topnet model a number of hydrologic models use unstructured grids or other modeling units such as hydrologic response unit hru the data processing services need to accommodate for such modeling configurations if they are to be used by the wide range of models currently used by the hydrologic community a related but less critical limitation of hydrods is that it only supports geotiff shapefile and netcdf file formats the hierarchical data format hdf https www hdfgroup org is as widely used as netcdf and would add additional flexibility to hydrods if it were supported an alternative is to add hydrods functions for conversion of data from netcdf to other standardized data formats such as hdf and vice versa another limitation of this study is that all the watersheds evaluated were headwater watersheds whose final pre processed and ready to be used in the model input data have relatively small file sizes less than 2 gb the work in this paper deals with large basins such as the colorado river basin by breaking them down into cbrfc forecast watersheds and handling data processing for smaller individual watersheds dealing with individual forecast watersheds with relatively small sizes was a design choice that keeps the size of the data and the computational resources for pre processing of a single watershed easily manageable while taking advantage of automation to address multiple watersheds characterizing how the services perform when increasing the sizes of the watersheds for example by integrating multiple adjacent watersheds may be an important next step in such a scenario the size of the weather forcing data increases more rapidly than the other data types and weather data processing services which currently use serial codes may have to deal with large datasets in netcdf format which could necessitate implementation of parallel processing additional work is also required to deal with the potential increase in processing time due to increase in size of processed data for example a mechanism for queuing and batch processing of large operations with asynchronous notifications to a user that the batch of tasks from a workflow script is completed would be useful this is because it would not be feasible for the user to wait for the web services to return when the execution time extends beyond the 10 min reported in this paper as stated earlier the hydrods services were tested for selected watersheds in the cbrfc basin and while the nlcd land cover data and the nldas and daymet weather forcing data for the years 2005 2015 stored at the hydro ds server cover the whole conus the ned dem and soil map unit rasters hosted at hydrods server are limited to the western us 128 0017 to 101 9983 longitude and 28 9983 to 50 0017 latitude this presents additional challenge to the applicability of the services for watersheds outside of western u s still a user can upload their own data and use the data processing tools although this was not the primary mode of application envisioned for the services at the outset currently extending the available service functionalities requires obtaining appropriate credentials and familiarity with the development environment and the underlying technologies including django gdal taudem netcdf library and netcdf operators nco in addition to python programming skills future developments should consider a simplified way to extend the services to cover more geospatial processing tools and data one way to enable a relatively easy extension of the services by addition of new functionalities is adding a software development kit sdk as a component of the hydrods services the sdk could be as simple as providing sample source codes to modify for new functions or support more advanced features such as tools and libraries to serve as building blocks for new tools functions finally while these results demonstrate that hydrods helps reduce the time and effort required for accessing and pre processing model input data the task of deciding on what hydrological questions to ask depends on the researcher s prior experience in this study deciding the case study involved a number of iterations 6 summary and conclusions hydrods a set of web based data services providing access to distributed gridded and subwatershed based hydrologic data and geospatial and temporal data analysis capabilities for hydrological models was introduced in this paper the services comprise functions for important hydrologic data processing tasks such as watershed delineation terrain processing estimation of canopy variables based on the nlcd generating soil properties based on data from ssurgo and accessing and processing of climate data from daymet and nldas the services are composed of single task functions that can be used independently or can be chained together to form a python workflow for complete generation of model inputs a python library the hydrods client library provides access to the web services through the hydrods client library the services can be used in a python script or desktop applications accessing the services requires only python which means that users can access them from any computing platform with python support hydrods was demonstrated by setting up instances of the utah energy balance ueb and topnet models for watersheds in the colorado river and great salt lake basins the cases demonstrate how hydrods helps reduce the time and effort spent for accessing and pre processing hydrologic model input data a considerable part of the time saved by using hydrods instead of desktop based data processing comes from better organization of data in hydrods the python scripting based data processing workflows enhance reproducibility and repeatability because the same script can be re used the script needs to be modified only to specify few user inputs when used for a different watershed as the workflow script also captures all the steps towards the final model input its provenance is preserved in the script the software as a service paradigm of the web services provides capability for multiple users and relieves users from concerns related to storage and organization of data which is done in the server and software and hardware dependencies which are sorted out when the software is configured on the server based on our observations using the services the provision of access through the hydrods client library to the atomic functions to do individual tasks appears to be not that useful rather the workflow scripts combining multiple coarser granular functions were more productive the work in this paper deals with large basins such as the colorado river basin by breaking them down into cbrfc forecast watersheds and handling data processing for smaller individual watersheds this was a design choice that worked well for this study future studies should address the alternative approach of processing river basins such as the colorado basin as a whole future work should also extend the services to provide inputs for unstructured grid models and models using hrus or other equivalent tessellations of the landscape for hydrods to support a wider range of hydrologic models future development should consider provision of software development kit sdk in hydrods to enable a relatively easy extension of the services with new functionalities declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the u s national science foundation nsf under collaborative grants eps 1135482 and 1135483 and by the utah water research laboratory uwrl any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf or uwrl compute storage support and other resources from the division of research computing in the office of research and graduate studies at utah state university and advanced research computing center at the university of wyoming are gratefully acknowledged 
26077,the aim of this paper is to create a user friendly computational tool for analysis of wildland fire behavior and its effect on urban and other structures a physics based multiphase computational fluid dynamics cfd model of wildfire initiation and spread has been developed and incorporated into the multi purpose cfd software phoenics it accounts for all the important physicochemical processes drying pyrolysis char combustion turbulent combustion of gaseous products of pyrolysis exchange of mass momentum and energy between gas and solid phase turbulent flow and convective conductive and radiative heat transfer turbulence is modeled by using a rng k ε model and the radiative heat transfer is represented by the immersol model the arrhenius type kinetics are used for heterogeneous reactions and the eddy breakup approach is applied for gaseous combustion the model has been validated using the experimental data keywords wildland fire combustion rate of spread software 1 introduction wildland fires are extremely complex and destructive phenomena and their behavior depends on the state of vegetation meteorological conditions and ground terrain experimental studies of wildfire behavior are expensive and challenging tasks this makes the development of robust and accurate models of wildfire behavior an extremely important activity there are various types of wildland fire models statistical empirical semi empirical and physics based this paper is devoted to the development and validation of a physics based multiphase computational fluid dynamics cfd model of wildland fire initiation and spread and smoke dispersion over the past 30 years significant progress in the development of physics based wildfire models has been achieved in particular fully physical multiphase wildfire models have been developed by grishin et al 1986 grishin 1997 porterie et al 1998 2000 2005 morvan and dupuy 2001 and mell et al 2007 according to a review by morvan 2011 one of the most advanced fully physical multiphase wildfire models is the three dimensional 3d model wfds wildland urban interface fire dynamics simulator developed at the building and fire research laboratory bfrl of nist the validation of wfds is ongoing its recent validation was conducted by menage et al 2012 by using the experimental data of mendes lopes et al 2003 on surface fire propagation in a bed of pinus pinaster needles the same set of data was also used by porterie et al 2000 in validating their multiphase model in recent years a number of experimental and theoretical works have been performed by el houssami et al 2016 2018 padhi et al 2016 and frangieha et al 2018 to study the combustion of different porous wildland fuels numerical simulations were compared to laboratory experiments carried out with porous pine needles beds el houssami et al 2016 2018 shrub fuels padhi et al 2016 and grass frangieha et al 2018 the relevance of various sub models used to close the multiphase cfd models was assessed the process of forest fire propagation was analyzed by grishin 1997 and perminov 2013 with use of simplified two dimensional 2d multiphase formulation the equations of three dimensional 3d model were integrated by these researchers over the height of the forest canopy and the resulting 2d system of equations was solved to study the dynamics of wildfire spread and the preventive measures such as fire breaks and barriers the dynamic turbulent viscosity was determined using simplified local equilibrium model of turbulence grishin 1997 and the arrhenius type kinetics were applied for both heterogeneous reactions and gaseous combustion in the present study a fully physical multiphase 3d model of wildland fire behavior was developed and incorporated into the commercial general purpose cfd software phoenics employed as a framework and a solver http www cham co uk phoenics php the model contains the main features proposed by previous researchers i e grishin 1997 and porterie et al 1998 2000 and it accounts for all the important physical and physicochemical processes drying pyrolysis char combustion turbulent combustion of gaseous products of pyrolysis exchange of mass momentum and energy between gas and solid phase turbulent gas flow and convective conductive and radiative heat transfer the use of phoenics software as a framework for modeling allows model applications by potential users students researchers fire management teams etc without any special cfd background due to availability of user friendly software interface documentation and technical support moreover an open and general structure of software enables users to modify the model test various built in models of turbulence and radiation try various numerical schemes and import geometries from cad packages in order to model complex shapes of objects in wildland urban interface wui the novelty of the current paper relative to the previous studies is that a physics based multiphase 3d wildfire model which is based on available data on chemical kinetics of heterogeneous reactions eddy break up approach for gaseous combustion rng k ε turbulence model and immersol radiation model has been incorporated for the first time into the general purpose cfd software and validated using the experimental data of mendes lopes et al 2003 on surface fire propagation in a bed of pine needles in the following sections the physical and mathematical formulation is presented section 2 the numerical method is described section 3 and the simulation results are discussed and compared with experimental data section 4 2 physical and mathematical formulation 2 1 modeling assumptions following a multiphase modeling approach proposed by grishin 1997 and porterie et al 2000 the forest is considered in this paper as a chemically reactive multiphase medium containing gas phase with a volume fraction of φ g and condensed phase with a volume fraction of φ s liquid water dry organic matter solid pyrolysis products and mineral part of fuel the interaction between phases is modeled by two sets of phase governing equations linked with proper source terms expressing the gas flow resistance multiphase heat transfer and chemical reactions the model accounts for drying pyrolysis char combustion turbulent combustion of gaseous products of pyrolysis turbulent gas flow and heat transfer in this study the radiative heat transfer is modeled by means of the immersol model spalding 1995 2013 which is essentially an extension of the p 1 approximation to handle optically thin as well as optically thick media and soot formation is ignored the arrhenius type kinetics are used for heterogeneous reactions drying pyrolysis and char combustion and the eddy dissipation concept edc of magnussen and hjertager 1976 is applied for modeling the gaseous combustion turbulence is modeled by using the renormalization group rng k ε model yakhot and smith 1992 as proposed by grishin et al 1986 grishin 1997 and porterie et al 1998 2000 2005 the degradation of the solid fuel via drying pyrolysis and char combustion and the combustion of volatiles arising from the pyrolysis process is summarized in the present study by the following simplified four step reaction mechanism 1 endothermic drying reaction wsf νh2o h2o 1 νh2o dsf 2 endothermic pyrolysis reaction dsf νchar char 1 νchar gpp 3 exothermic charcoal oxidation carbon combustion c o 2 co 2 s 1 8 3 4 exothermal oxidation of combustible gaseous pyrolysis products co combustion co 1 2o 2 co 2 s 2 4 7 where wsf dsf and gpp symbolize the wet solid fuel dry solid fuel and gaseous pyrolysis products respectively written in mass ν h2o andν char are the stoichiometric coefficients for drying and pyrolysis s 1 and s 2 are the stoichiometric ratios for heterogeneous and homogeneous reactions as a consequence of pyrolysis char and gaseous pyrolysis products gpp are formed soot is neglected as its mass fraction is less than 1 of the total mass of soot and gaseous mixture porterie et al 2005 char consists of pure carbon 80 97 and ash gpp include combustible and noncombustible parts it is assumed in the above mechanism that char is pure carbon c and the combustible part of gpp is an effective gas of the co type the co combustion reaction is assumed to be infinite fast and the local co burning rate is taken to be the lowest of the turbulence dispersion rates of either fuel co or oxygen magnussen and hjertager 1976 and morvan and dupuy 2001 the gas phase is simplified as a mixture of five major components o 2 co co 2 h 2 o and n 2 fig 1 shows the 3d domain containing the gas flow region a fuel bed representing the forest and an ignition line the specific sizes of domain and fuel bed vary in various case studies 2 2 gas phase equations the gas phase governing equations are written in a generic form as follows 1 t ρ φ x i ρ u i φ γ φ φ x i s φ here t is the time x i is the spacial coordinate i 1 2 3 ρ is the gas mixture density u i is the velocity component in x i direction and the specific expressions for dependent variable φ diffusive exchange coefficient γ φ and source term s φ are given in table 1 below the gas phase volume fraction φ g is taken equal to unity in equation 1 as φ g 1 φ s where the volume fraction of condensed phase φ s is very small in the present study φ s 0 016 the gas density is calculated from the ideal gas law equation of state for mixture of gases p ρ r t α 1 3 с α m α where p is the gas pressure t is the absolute gas temperature r is the universal gas constant c α is the mass fraction of α species of gas mixture index α 1 2 3 where 1 corresponds to oxygen 2 to carbon monoxide 3 to all other components of the gas mixture α 1 3 с α 1 m α is the molecular weight of α component of gas phase here h is the gas enthalpy k is the turbulent kinetic energy ε is the dissipation rate of turbulent kinetic energy μ and μ t are the dynamic molecular and turbulent viscosities calculated from equations μ 1 479 10 6 t 1 5 t 116 275 μ t c μ ρ k 2 ε pr sc pr t and sc t are the molecular and turbulent prandtl and schmidt numbers pr 1005 μ 0 0258 sc pr t sc t 1 ϭ k ϭ ε c μ с ε 1 с ε 2 с ε 3 are the empirical constants of turbulent model ϭ k 0 7194 ϭ ε 0 7194 c μ 0 0845 1 42 с ε 2 1 68 cε 3 1 0 g i is the gravity acceleration component g 0 0 g in the term of momentum equation that describes the buoyancy forces ρ e is the reference density and p is the pressure perturbation relative to the hydrostatic reference condition u is the gas velocity vector having three velocity components u 1 u 2 u 3 a s is the specific wetted area of fuel bed a s φ s σ s σ s is the surface area to volume ratio of solid particle c d is a particle drag coefficient c d 24 1 0 15 re es 0 687 re es re es 800 depending on the effective particle reynolds number re es ρ u d es μ which is calculated using the equivalent spherical particle diameter d es 6 σ s h s is the particle heat transfer coefficient h s λ n u s d s depending on the heat conductivity of gas λ particle nusselt number nu s and equivalent diameter of cylindrical particle d s 4 σ s nu s is a function of particle reynolds number re s ρ u d s μ nu s 0 683res 0 466 q 5 is the heat effect of gas phase combustion of carbon monoxide q 5 107 j kg σ is the stefan boltzman constant t s is the absolute temperature of solid phase t 3 is the radiosity temperature defined as r i 4 σ 1 4 where r i is the incident radiation wm 2 ε 1 is the absorption coefficient of gas phase r rng is an additional term proposed in the rng k ε model by yakhot and smith 1992 p k and w k are the turbulence production destruction terms defined by launder and spalding 1974 p k is the volumetric production rate of k by shear forces and w k is the volumetric production rate of k by gravitational forces interacting with density gradients it should be mentioned that the energy equation was formulated in terms of temperature rather than enthalpy using a link between enthalpy and temperature dh cp g dt where cp g is the specific heat of gas mixture the specific heat was involved in convection and transient terms and thermal conductivity k g entered as a multiplier of temperature gradient in heat conduction terms a constant value of k g equal to 0 0258 wm 1k 1 and a constant value of cp g equal to 1005 jkg 1k 1 were used in the present work for simplicity the future studies will account for dependencies of k g and cp g on gas composition and temperature the mass production consumption rates m m 5 m 51 and m 52 are defined as the following grishin 1997 and porterie et al 2000 2 m 1 α с r 1 r 2 m с m 1 r 3 3 m 5 4 ρ ε k min c 2 с 1 s 2 4 m 51 1 2 m 1 m 2 m 5 r 3 5 m 52 ν g 1 α c r 1 m 5 6 r 1 k 1 ρ 1 ϕ 1 exp e 1 r t s 7 r 2 k 2 ρ 2 ϕ 2 t s 0 5 exp e 2 r t s 8 r 3 k 3 ρ c 1 σ s φ 3 exp e 3 r t s here m 1 m 2 and m c are the molecular weights of oxygen carbon monoxide and carbon α с and ν g are the coke number and the fraction of combustible gaseous products of pyrolysis defined by grishin et al 1986 and grishin 1997 α с ν char 0 06 ν g 0 7 r 1 r 2 and r 3 are the mass rates of chemical reactions pyrolysis drying and charcoal combustion approximated by arrhenius laws whose parameters i e pre exponential constants k i and activation energies e i are available from grishin et al 1986 and porterie et al 2000 k 1 3 63e 4 s 1 k 2 6 105 k1 2 s 1 k 3 430 ms 1 e 1 r 7250 k e 2 r 5800 k e 3 r 9000 k 2 3 solid phase equations the rates of degradation of condensed phase are computed from the equations grishin 1997 9 ρ 1 ϕ 1 t r 1 10 ρ 2 ϕ 2 t r 2 11 ρ 3 ϕ 3 t α c r 1 m c m 1 r 3 12 ρ 4 ϕ 4 t 0 13 i 1 5 φ i 1 14 φ s i 1 4 φ i as suggested by grishin 1997 and porterie et al 2000 the solid particles are considered thermally thin and their temperature is computed from the following conservation equation 15 i 1 4 ρ i c p i ϕ i t s t q 1 r 1 q 2 r 2 q 3 r 3 4 ε 2 σ t 3 4 t s 4 a s h s t t s here and above ρ i φ i and c pi are the density volume fraction and specific heat of a phase component 1 dry organic substance 2 liquid water 3 condensed products of pyrolysis 4 mineral component of fuel 5 gas phase q i are the heat release rates of chemical reactions in this study for i 1 2 3 and 4 ρ i 680 1000 200 and 200 kgm 3 c pi 2 0 4 18 0 9 and 1 0 kjkg 1k 1 q 1 418 jkg 1 and q 3 1 2 107 jkg 1 as proposed by porterie et al 2000 and q 2 3 106 jkg 1 as suggested by grishin et al 1986 the initial volume fractions of condensed phase are calculated from equations grishin et al 1986 16 ϕ 1 e ρ 0 1 ν a s h ρ 1 ϕ 2 e w ρ 0 1 ν a s h 100 ρ 2 ϕ 3 e 0 ϕ 4 e ρ 0 ν a s h ρ 4 here ρ 0 is the bulk density of fuel ν a s h is the ashes content ν a s h 0 04 w is the fuel moisture content in the validation study section 4 ρ 0 10 kg m3 w 10 and equation 16 result in the following initial values of φ i ϕ 1 e 0 014 ϕ 2 e 9 6 10 4 ϕ 4 e 2 10 3 2 4 radiation model the radiative transfer equation rte is written with use of a phoenics variable t 3 defined in http www cham co uk phoenics d polis d enc enc rad3 htm 17 x i λ 3 t 3 x i 4 ε 1 σ t 3 4 t 4 4 ε 2 σ t 3 4 t s 4 λ 3 4 σ t 3 3 0 75 ε 1 ε 2 1 w g a p here ε 1 and ε 2 are the absorption coefficients of gas and solid phases ε1 which depends on gas temperature and mass fractions of products of gaseous combustion was taken equal to a constant value of 0 1 m 1 for simplicity in this study ε 2 φ s σ s 4 φ s d s according to porterie et al 1998 equation 17 is a formulation of the immersol radiation model as proposed by spalding 2013 it is similar to rte in p1 approximation used by porterie et al 1998 with the only difference that an additional term 1 w g a p is included w g a p is the gap between the solid walls the presence of the 1 w g a p term extends the immersol model to handle the transparent gases such as air i e the optically thin limit ε 1 ε 2 0 3 numerical method 3 1 solution domain boundary and initial conditions the model described in the previous section was validated for a case which was studied experimentally by mendes lopes et al 2003 and numerically by porterie et al 1998 2000 and menage et al 2012 in this case the fuel bed has the following input parameters porterie et al 1998 2000 a height of 5 cm a fuel load value of 0 5 kg m2 a needles density of 680 kg m3 a bulk fuel density of 10 kg m3 an initial moisture content of 10 and a surface to volume ratio of needles σ s of about 5511 m 1 a 2 2 m 1 m x 0 05 m fuel bed was considered within a 4 2 m 1 m x 0 9 m domain see fig 1 the governing equations 1 17 were solved numerically using phoenics cfd solver in its transient mode at the initial stage the constant values of all the field variables pressure velocity components phase temperatures and mass fractions were specified the wind profile was considered uniform at the flow inlet i e three constant wind speeds of 1 2 and 3 m s and a turbulent intensity of 5 were specified at the flow inlet in different runs the inlet value of turbulence length scale el1 variable in phoenics is derived automatically from correlations of rng k ε turbulence model http www cham co uk phoenics d polis d enc el1 htm the standard wall function approach was used to simulate the gas flow near the domain bottom outflow boundary conditions fixed pressure were applied at the top and right boundaries of the domain the default built in phoenics thermal boundary conditions were applied for immersol model http www cham co uk phoenics d polis d enc enc rad3 htm c the ignition source ignition line on fig 1 was located at the beginning of fuel bed at 1 m distance from the origin and the ignition was simulated by introducing a volumetric heat source of 0 1 m length over the whole fuel bed width and height the temperature of this region was linearly increased from 700 k to 1000 k during the first 8 s of simulation to generate the heat source required for fuel bed ignition 3 2 computational mesh time discretization and solution convergence for the sake of simplicity a 2d formulation was applied by ignoring the gas flow and transport of mass and energy in x 2 direction a computational grid of 190 40 cells was used based on the grid sensitivity study the grid was non uniform with a minimum grid size of 5 mm in the fuel bed region the time step changed from 0 005 s to 0 01 s for different stages of the process no more than 25 iterations were required to obtain the convergence at each time step the transient runs were conducted to simulate 90 s of real time different grid sizes were tested during the grid sensitivity study and a recommendation by morvan 2011 was applied the smallest grid size was less than the extinction length scale δ r 4 φ s σ s d s φ s which was equal to 43 mm in our study the time steps were in the range from 10 3 to 10 2 s morvan and dupuy 2001 4 results and discussion the focus of this study was on the model s capability to predict the fire rate of spread ros measured by mendes lopes et al 2003 and to reproduce the main flow patterns predicted numerically by porterie et al 1998 2000 the ros was calculated in accordance with porterie et al 1998 2000 as a speed of propagation of the isotherm t s 600 k or 500 k at the ground level fig 2 shows the transient propagation of pyrolysis front defined with use of isotherm t s 600 k for three wind speeds of 1 2 and 3 m s the quasi steady values of ros defined as rates of change of front positions with time are 1 2 2 5 and 4 3 cm s respectively these values compare well with the experimental ros values of mendes lopes et al 2003 measured at zero slope of bed 1 04 2 08 and 4 92 cm s respectively figs 3 and 4 show the distributions of solid phase temperature t s and mass fractions of oxygen c 1 and carbon monoxide c 2 a gaseous product of pyrolysis predicted at x 3 0 m and t 20 s for wind speeds of 1 and 2 m s respectively the fuel bed heating from propagating fire causes water evaporation pyrolysis between 400 k and 500 k and char combustion at about 700 k the carbon monoxide which is released during pyrolysis participates in gaseous combustion and its mass fraction drops to zero the oxygen mass fraction reduces in the pyrolysis zone due to creation of co in that zone and then it drops to zero within the combustion zone due to oxygen consumption as the wind velocity increases from 1 to 2 m s the width of combustion zone is extended as a result of intensification of heat and mass transfer figs 5 and 6 show the distributions of gas temperature and velocity predicted at different instants of time a t 20 s b t 40 s and c t 60 s for wind speeds of 1 and 2 m s respectively at a wind speed of 1 m s a large clockwise eddy is formed ahead of strong buoyant plume and the plume is oscillating with time as wind speed increases from 1 to 2 m s a transition from buoyancy dominated regime to wind driven regime is observed and the plume becomes more stable these flow patterns were also reported by porterie et al 2000 5 conclusions a multiphase cfd model of wildfire initiation and spread has been developed and incorporated into the multi purpose cfd software phoenics the model accounts for all the important physical and physicochemical processes drying pyrolysis char combustion turbulent combustion of gaseous products of pyrolysis exchange of mass momentum and energy between gas and solid phase turbulent gas flow and convective conductive and radiative heat transfer turbulence is modeled by using the rng k ε model and the radiative heat transfer is approached with the immersol model spalding 1995 2013 that is similar to p1 approximation the arrhenius type kinetics is used for heterogeneous reactions drying pyrolysis and char combustion and the eddy dissipation concept is applied for modeling the gaseous combustion the model was validated using the experimental data of mendes lopes et al 2003 on surface fire propagation in a bed of pinus pinaster needles studied in a wind tunnel the predicted rate of spread ros agreed well with experimental values obtained at various wind speeds from 1 to 3 m s the model is being further developed by modifying the radiative heat transfer model and it will be validated using the data on large forest fires including crown fires declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
26077,the aim of this paper is to create a user friendly computational tool for analysis of wildland fire behavior and its effect on urban and other structures a physics based multiphase computational fluid dynamics cfd model of wildfire initiation and spread has been developed and incorporated into the multi purpose cfd software phoenics it accounts for all the important physicochemical processes drying pyrolysis char combustion turbulent combustion of gaseous products of pyrolysis exchange of mass momentum and energy between gas and solid phase turbulent flow and convective conductive and radiative heat transfer turbulence is modeled by using a rng k ε model and the radiative heat transfer is represented by the immersol model the arrhenius type kinetics are used for heterogeneous reactions and the eddy breakup approach is applied for gaseous combustion the model has been validated using the experimental data keywords wildland fire combustion rate of spread software 1 introduction wildland fires are extremely complex and destructive phenomena and their behavior depends on the state of vegetation meteorological conditions and ground terrain experimental studies of wildfire behavior are expensive and challenging tasks this makes the development of robust and accurate models of wildfire behavior an extremely important activity there are various types of wildland fire models statistical empirical semi empirical and physics based this paper is devoted to the development and validation of a physics based multiphase computational fluid dynamics cfd model of wildland fire initiation and spread and smoke dispersion over the past 30 years significant progress in the development of physics based wildfire models has been achieved in particular fully physical multiphase wildfire models have been developed by grishin et al 1986 grishin 1997 porterie et al 1998 2000 2005 morvan and dupuy 2001 and mell et al 2007 according to a review by morvan 2011 one of the most advanced fully physical multiphase wildfire models is the three dimensional 3d model wfds wildland urban interface fire dynamics simulator developed at the building and fire research laboratory bfrl of nist the validation of wfds is ongoing its recent validation was conducted by menage et al 2012 by using the experimental data of mendes lopes et al 2003 on surface fire propagation in a bed of pinus pinaster needles the same set of data was also used by porterie et al 2000 in validating their multiphase model in recent years a number of experimental and theoretical works have been performed by el houssami et al 2016 2018 padhi et al 2016 and frangieha et al 2018 to study the combustion of different porous wildland fuels numerical simulations were compared to laboratory experiments carried out with porous pine needles beds el houssami et al 2016 2018 shrub fuels padhi et al 2016 and grass frangieha et al 2018 the relevance of various sub models used to close the multiphase cfd models was assessed the process of forest fire propagation was analyzed by grishin 1997 and perminov 2013 with use of simplified two dimensional 2d multiphase formulation the equations of three dimensional 3d model were integrated by these researchers over the height of the forest canopy and the resulting 2d system of equations was solved to study the dynamics of wildfire spread and the preventive measures such as fire breaks and barriers the dynamic turbulent viscosity was determined using simplified local equilibrium model of turbulence grishin 1997 and the arrhenius type kinetics were applied for both heterogeneous reactions and gaseous combustion in the present study a fully physical multiphase 3d model of wildland fire behavior was developed and incorporated into the commercial general purpose cfd software phoenics employed as a framework and a solver http www cham co uk phoenics php the model contains the main features proposed by previous researchers i e grishin 1997 and porterie et al 1998 2000 and it accounts for all the important physical and physicochemical processes drying pyrolysis char combustion turbulent combustion of gaseous products of pyrolysis exchange of mass momentum and energy between gas and solid phase turbulent gas flow and convective conductive and radiative heat transfer the use of phoenics software as a framework for modeling allows model applications by potential users students researchers fire management teams etc without any special cfd background due to availability of user friendly software interface documentation and technical support moreover an open and general structure of software enables users to modify the model test various built in models of turbulence and radiation try various numerical schemes and import geometries from cad packages in order to model complex shapes of objects in wildland urban interface wui the novelty of the current paper relative to the previous studies is that a physics based multiphase 3d wildfire model which is based on available data on chemical kinetics of heterogeneous reactions eddy break up approach for gaseous combustion rng k ε turbulence model and immersol radiation model has been incorporated for the first time into the general purpose cfd software and validated using the experimental data of mendes lopes et al 2003 on surface fire propagation in a bed of pine needles in the following sections the physical and mathematical formulation is presented section 2 the numerical method is described section 3 and the simulation results are discussed and compared with experimental data section 4 2 physical and mathematical formulation 2 1 modeling assumptions following a multiphase modeling approach proposed by grishin 1997 and porterie et al 2000 the forest is considered in this paper as a chemically reactive multiphase medium containing gas phase with a volume fraction of φ g and condensed phase with a volume fraction of φ s liquid water dry organic matter solid pyrolysis products and mineral part of fuel the interaction between phases is modeled by two sets of phase governing equations linked with proper source terms expressing the gas flow resistance multiphase heat transfer and chemical reactions the model accounts for drying pyrolysis char combustion turbulent combustion of gaseous products of pyrolysis turbulent gas flow and heat transfer in this study the radiative heat transfer is modeled by means of the immersol model spalding 1995 2013 which is essentially an extension of the p 1 approximation to handle optically thin as well as optically thick media and soot formation is ignored the arrhenius type kinetics are used for heterogeneous reactions drying pyrolysis and char combustion and the eddy dissipation concept edc of magnussen and hjertager 1976 is applied for modeling the gaseous combustion turbulence is modeled by using the renormalization group rng k ε model yakhot and smith 1992 as proposed by grishin et al 1986 grishin 1997 and porterie et al 1998 2000 2005 the degradation of the solid fuel via drying pyrolysis and char combustion and the combustion of volatiles arising from the pyrolysis process is summarized in the present study by the following simplified four step reaction mechanism 1 endothermic drying reaction wsf νh2o h2o 1 νh2o dsf 2 endothermic pyrolysis reaction dsf νchar char 1 νchar gpp 3 exothermic charcoal oxidation carbon combustion c o 2 co 2 s 1 8 3 4 exothermal oxidation of combustible gaseous pyrolysis products co combustion co 1 2o 2 co 2 s 2 4 7 where wsf dsf and gpp symbolize the wet solid fuel dry solid fuel and gaseous pyrolysis products respectively written in mass ν h2o andν char are the stoichiometric coefficients for drying and pyrolysis s 1 and s 2 are the stoichiometric ratios for heterogeneous and homogeneous reactions as a consequence of pyrolysis char and gaseous pyrolysis products gpp are formed soot is neglected as its mass fraction is less than 1 of the total mass of soot and gaseous mixture porterie et al 2005 char consists of pure carbon 80 97 and ash gpp include combustible and noncombustible parts it is assumed in the above mechanism that char is pure carbon c and the combustible part of gpp is an effective gas of the co type the co combustion reaction is assumed to be infinite fast and the local co burning rate is taken to be the lowest of the turbulence dispersion rates of either fuel co or oxygen magnussen and hjertager 1976 and morvan and dupuy 2001 the gas phase is simplified as a mixture of five major components o 2 co co 2 h 2 o and n 2 fig 1 shows the 3d domain containing the gas flow region a fuel bed representing the forest and an ignition line the specific sizes of domain and fuel bed vary in various case studies 2 2 gas phase equations the gas phase governing equations are written in a generic form as follows 1 t ρ φ x i ρ u i φ γ φ φ x i s φ here t is the time x i is the spacial coordinate i 1 2 3 ρ is the gas mixture density u i is the velocity component in x i direction and the specific expressions for dependent variable φ diffusive exchange coefficient γ φ and source term s φ are given in table 1 below the gas phase volume fraction φ g is taken equal to unity in equation 1 as φ g 1 φ s where the volume fraction of condensed phase φ s is very small in the present study φ s 0 016 the gas density is calculated from the ideal gas law equation of state for mixture of gases p ρ r t α 1 3 с α m α where p is the gas pressure t is the absolute gas temperature r is the universal gas constant c α is the mass fraction of α species of gas mixture index α 1 2 3 where 1 corresponds to oxygen 2 to carbon monoxide 3 to all other components of the gas mixture α 1 3 с α 1 m α is the molecular weight of α component of gas phase here h is the gas enthalpy k is the turbulent kinetic energy ε is the dissipation rate of turbulent kinetic energy μ and μ t are the dynamic molecular and turbulent viscosities calculated from equations μ 1 479 10 6 t 1 5 t 116 275 μ t c μ ρ k 2 ε pr sc pr t and sc t are the molecular and turbulent prandtl and schmidt numbers pr 1005 μ 0 0258 sc pr t sc t 1 ϭ k ϭ ε c μ с ε 1 с ε 2 с ε 3 are the empirical constants of turbulent model ϭ k 0 7194 ϭ ε 0 7194 c μ 0 0845 1 42 с ε 2 1 68 cε 3 1 0 g i is the gravity acceleration component g 0 0 g in the term of momentum equation that describes the buoyancy forces ρ e is the reference density and p is the pressure perturbation relative to the hydrostatic reference condition u is the gas velocity vector having three velocity components u 1 u 2 u 3 a s is the specific wetted area of fuel bed a s φ s σ s σ s is the surface area to volume ratio of solid particle c d is a particle drag coefficient c d 24 1 0 15 re es 0 687 re es re es 800 depending on the effective particle reynolds number re es ρ u d es μ which is calculated using the equivalent spherical particle diameter d es 6 σ s h s is the particle heat transfer coefficient h s λ n u s d s depending on the heat conductivity of gas λ particle nusselt number nu s and equivalent diameter of cylindrical particle d s 4 σ s nu s is a function of particle reynolds number re s ρ u d s μ nu s 0 683res 0 466 q 5 is the heat effect of gas phase combustion of carbon monoxide q 5 107 j kg σ is the stefan boltzman constant t s is the absolute temperature of solid phase t 3 is the radiosity temperature defined as r i 4 σ 1 4 where r i is the incident radiation wm 2 ε 1 is the absorption coefficient of gas phase r rng is an additional term proposed in the rng k ε model by yakhot and smith 1992 p k and w k are the turbulence production destruction terms defined by launder and spalding 1974 p k is the volumetric production rate of k by shear forces and w k is the volumetric production rate of k by gravitational forces interacting with density gradients it should be mentioned that the energy equation was formulated in terms of temperature rather than enthalpy using a link between enthalpy and temperature dh cp g dt where cp g is the specific heat of gas mixture the specific heat was involved in convection and transient terms and thermal conductivity k g entered as a multiplier of temperature gradient in heat conduction terms a constant value of k g equal to 0 0258 wm 1k 1 and a constant value of cp g equal to 1005 jkg 1k 1 were used in the present work for simplicity the future studies will account for dependencies of k g and cp g on gas composition and temperature the mass production consumption rates m m 5 m 51 and m 52 are defined as the following grishin 1997 and porterie et al 2000 2 m 1 α с r 1 r 2 m с m 1 r 3 3 m 5 4 ρ ε k min c 2 с 1 s 2 4 m 51 1 2 m 1 m 2 m 5 r 3 5 m 52 ν g 1 α c r 1 m 5 6 r 1 k 1 ρ 1 ϕ 1 exp e 1 r t s 7 r 2 k 2 ρ 2 ϕ 2 t s 0 5 exp e 2 r t s 8 r 3 k 3 ρ c 1 σ s φ 3 exp e 3 r t s here m 1 m 2 and m c are the molecular weights of oxygen carbon monoxide and carbon α с and ν g are the coke number and the fraction of combustible gaseous products of pyrolysis defined by grishin et al 1986 and grishin 1997 α с ν char 0 06 ν g 0 7 r 1 r 2 and r 3 are the mass rates of chemical reactions pyrolysis drying and charcoal combustion approximated by arrhenius laws whose parameters i e pre exponential constants k i and activation energies e i are available from grishin et al 1986 and porterie et al 2000 k 1 3 63e 4 s 1 k 2 6 105 k1 2 s 1 k 3 430 ms 1 e 1 r 7250 k e 2 r 5800 k e 3 r 9000 k 2 3 solid phase equations the rates of degradation of condensed phase are computed from the equations grishin 1997 9 ρ 1 ϕ 1 t r 1 10 ρ 2 ϕ 2 t r 2 11 ρ 3 ϕ 3 t α c r 1 m c m 1 r 3 12 ρ 4 ϕ 4 t 0 13 i 1 5 φ i 1 14 φ s i 1 4 φ i as suggested by grishin 1997 and porterie et al 2000 the solid particles are considered thermally thin and their temperature is computed from the following conservation equation 15 i 1 4 ρ i c p i ϕ i t s t q 1 r 1 q 2 r 2 q 3 r 3 4 ε 2 σ t 3 4 t s 4 a s h s t t s here and above ρ i φ i and c pi are the density volume fraction and specific heat of a phase component 1 dry organic substance 2 liquid water 3 condensed products of pyrolysis 4 mineral component of fuel 5 gas phase q i are the heat release rates of chemical reactions in this study for i 1 2 3 and 4 ρ i 680 1000 200 and 200 kgm 3 c pi 2 0 4 18 0 9 and 1 0 kjkg 1k 1 q 1 418 jkg 1 and q 3 1 2 107 jkg 1 as proposed by porterie et al 2000 and q 2 3 106 jkg 1 as suggested by grishin et al 1986 the initial volume fractions of condensed phase are calculated from equations grishin et al 1986 16 ϕ 1 e ρ 0 1 ν a s h ρ 1 ϕ 2 e w ρ 0 1 ν a s h 100 ρ 2 ϕ 3 e 0 ϕ 4 e ρ 0 ν a s h ρ 4 here ρ 0 is the bulk density of fuel ν a s h is the ashes content ν a s h 0 04 w is the fuel moisture content in the validation study section 4 ρ 0 10 kg m3 w 10 and equation 16 result in the following initial values of φ i ϕ 1 e 0 014 ϕ 2 e 9 6 10 4 ϕ 4 e 2 10 3 2 4 radiation model the radiative transfer equation rte is written with use of a phoenics variable t 3 defined in http www cham co uk phoenics d polis d enc enc rad3 htm 17 x i λ 3 t 3 x i 4 ε 1 σ t 3 4 t 4 4 ε 2 σ t 3 4 t s 4 λ 3 4 σ t 3 3 0 75 ε 1 ε 2 1 w g a p here ε 1 and ε 2 are the absorption coefficients of gas and solid phases ε1 which depends on gas temperature and mass fractions of products of gaseous combustion was taken equal to a constant value of 0 1 m 1 for simplicity in this study ε 2 φ s σ s 4 φ s d s according to porterie et al 1998 equation 17 is a formulation of the immersol radiation model as proposed by spalding 2013 it is similar to rte in p1 approximation used by porterie et al 1998 with the only difference that an additional term 1 w g a p is included w g a p is the gap between the solid walls the presence of the 1 w g a p term extends the immersol model to handle the transparent gases such as air i e the optically thin limit ε 1 ε 2 0 3 numerical method 3 1 solution domain boundary and initial conditions the model described in the previous section was validated for a case which was studied experimentally by mendes lopes et al 2003 and numerically by porterie et al 1998 2000 and menage et al 2012 in this case the fuel bed has the following input parameters porterie et al 1998 2000 a height of 5 cm a fuel load value of 0 5 kg m2 a needles density of 680 kg m3 a bulk fuel density of 10 kg m3 an initial moisture content of 10 and a surface to volume ratio of needles σ s of about 5511 m 1 a 2 2 m 1 m x 0 05 m fuel bed was considered within a 4 2 m 1 m x 0 9 m domain see fig 1 the governing equations 1 17 were solved numerically using phoenics cfd solver in its transient mode at the initial stage the constant values of all the field variables pressure velocity components phase temperatures and mass fractions were specified the wind profile was considered uniform at the flow inlet i e three constant wind speeds of 1 2 and 3 m s and a turbulent intensity of 5 were specified at the flow inlet in different runs the inlet value of turbulence length scale el1 variable in phoenics is derived automatically from correlations of rng k ε turbulence model http www cham co uk phoenics d polis d enc el1 htm the standard wall function approach was used to simulate the gas flow near the domain bottom outflow boundary conditions fixed pressure were applied at the top and right boundaries of the domain the default built in phoenics thermal boundary conditions were applied for immersol model http www cham co uk phoenics d polis d enc enc rad3 htm c the ignition source ignition line on fig 1 was located at the beginning of fuel bed at 1 m distance from the origin and the ignition was simulated by introducing a volumetric heat source of 0 1 m length over the whole fuel bed width and height the temperature of this region was linearly increased from 700 k to 1000 k during the first 8 s of simulation to generate the heat source required for fuel bed ignition 3 2 computational mesh time discretization and solution convergence for the sake of simplicity a 2d formulation was applied by ignoring the gas flow and transport of mass and energy in x 2 direction a computational grid of 190 40 cells was used based on the grid sensitivity study the grid was non uniform with a minimum grid size of 5 mm in the fuel bed region the time step changed from 0 005 s to 0 01 s for different stages of the process no more than 25 iterations were required to obtain the convergence at each time step the transient runs were conducted to simulate 90 s of real time different grid sizes were tested during the grid sensitivity study and a recommendation by morvan 2011 was applied the smallest grid size was less than the extinction length scale δ r 4 φ s σ s d s φ s which was equal to 43 mm in our study the time steps were in the range from 10 3 to 10 2 s morvan and dupuy 2001 4 results and discussion the focus of this study was on the model s capability to predict the fire rate of spread ros measured by mendes lopes et al 2003 and to reproduce the main flow patterns predicted numerically by porterie et al 1998 2000 the ros was calculated in accordance with porterie et al 1998 2000 as a speed of propagation of the isotherm t s 600 k or 500 k at the ground level fig 2 shows the transient propagation of pyrolysis front defined with use of isotherm t s 600 k for three wind speeds of 1 2 and 3 m s the quasi steady values of ros defined as rates of change of front positions with time are 1 2 2 5 and 4 3 cm s respectively these values compare well with the experimental ros values of mendes lopes et al 2003 measured at zero slope of bed 1 04 2 08 and 4 92 cm s respectively figs 3 and 4 show the distributions of solid phase temperature t s and mass fractions of oxygen c 1 and carbon monoxide c 2 a gaseous product of pyrolysis predicted at x 3 0 m and t 20 s for wind speeds of 1 and 2 m s respectively the fuel bed heating from propagating fire causes water evaporation pyrolysis between 400 k and 500 k and char combustion at about 700 k the carbon monoxide which is released during pyrolysis participates in gaseous combustion and its mass fraction drops to zero the oxygen mass fraction reduces in the pyrolysis zone due to creation of co in that zone and then it drops to zero within the combustion zone due to oxygen consumption as the wind velocity increases from 1 to 2 m s the width of combustion zone is extended as a result of intensification of heat and mass transfer figs 5 and 6 show the distributions of gas temperature and velocity predicted at different instants of time a t 20 s b t 40 s and c t 60 s for wind speeds of 1 and 2 m s respectively at a wind speed of 1 m s a large clockwise eddy is formed ahead of strong buoyant plume and the plume is oscillating with time as wind speed increases from 1 to 2 m s a transition from buoyancy dominated regime to wind driven regime is observed and the plume becomes more stable these flow patterns were also reported by porterie et al 2000 5 conclusions a multiphase cfd model of wildfire initiation and spread has been developed and incorporated into the multi purpose cfd software phoenics the model accounts for all the important physical and physicochemical processes drying pyrolysis char combustion turbulent combustion of gaseous products of pyrolysis exchange of mass momentum and energy between gas and solid phase turbulent gas flow and convective conductive and radiative heat transfer turbulence is modeled by using the rng k ε model and the radiative heat transfer is approached with the immersol model spalding 1995 2013 that is similar to p1 approximation the arrhenius type kinetics is used for heterogeneous reactions drying pyrolysis and char combustion and the eddy dissipation concept is applied for modeling the gaseous combustion the model was validated using the experimental data of mendes lopes et al 2003 on surface fire propagation in a bed of pinus pinaster needles studied in a wind tunnel the predicted rate of spread ros agreed well with experimental values obtained at various wind speeds from 1 to 3 m s the model is being further developed by modifying the radiative heat transfer model and it will be validated using the data on large forest fires including crown fires declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
26078,the hydrology of cold regions has been studied for decades with substantial progress in process understanding and prediction simultaneously work on nutrient yields from agricultural land in cold regions has shown much slower progress advancement of nutrient modelling is constrained by well documented issues of spatial heterogeneity climate dependency data limitations and over parameterization of models as well as challenges specific to cold regions due to the complex and often unknown behaviour of hydro biogeochemical processes at temperatures close to and below freezing where a phase change occurs this review is a critical discussion of these issues by taking a close look at the conceptual models and methods behind used catchment nutrient models the impact of differences in model structure and the methods used for the prediction of hydrological processes erosion and biogeochemical cycles are examined the appropriateness of scale scope and complexity of models are discussed to propose future research directions keywords seasonal snow cover nutrient transport cold region processes hydrological controls catchment models 1 introduction agricultural activities and associated runoff of excess nutrients have impaired the ecological function of streams and lakes around the world carpenter et al 1998 withers and lord 2002 schindler et al 2012 this has led to significant efforts to reduce nutrient export and contain the growing global problem of eutrophication and cyanobacterial blooms however the quantification and prediction of nutrient exports to streams lakes and estuaries remain a difficult challenge despite decades of research on nutrient cycling and transport wade et al 2008 cold climate regions are characterized by an average air temperature above 10 c in the warmest months and below 3 c in the coldest months peel et al 2007 cold regions hydrology is conceived as occurring in catchments where snowcover and frozen soils play a notable role in the hydrological cycle here the problem of nutrient transport and pollution is affected by snow related processes because they have a strong impact on flow generation erosion and nutrient export e g brooks and williams 1999 eimers et al 2009 casson et al 2012 the spring freshet is often the major annual runoff event and its magnitude and timing depend on both fall winter processes and antecedent conditions such as soil moisture snowfall and snow redistribution as well as the characteristics of the snowmelt event such as duration intensity and presence of frozen soils in the canadian prairies for instance snowmelt runoff can account for more than 80 of the total annual runoff volume gray and landine 1988 and contribute the most nitrogen n and phosphorus p exported yearly corriveau et al 2013 in these areas snowmelt volume melt rate and seasonally frozen soils are critical factors determining runoff soil contact and erodibility e g ollesch et al 2006 panuska and karthikeyan 2010 tiessen et al 2010 sub zero temperatures snowpacks freeze thaw cycling and frozen soils may affect the biogeochemistry of these areas with impacts on nitrogen p and n there is considerable debate about the appropriateness of scale scope complexity and accuracy of water quality models moore et al 2006 conventional process based catchment nutrient models are increasingly complex and heavily parameterized but substantially simplify reality beck 1987 wade et al 2008 costa et al 2019b uncertainties associated with hydrological and biogeochemical responses at various spatial scales and sparse sporadic water quality measurements with only rare measurements of key processes and pools further complicate the adequate use of catchment nutrient models raising critical questions for the design application and benefit of such modelling tools this paper reviews the structure and conceptual foundation within widely used catchment nutrient models that have been applied in cold regions it focusses primarily on processes specific to cold regions hydrology which are the processes involving snow ice and frozen soils as they affect the hydrological cycle general hydrological and biogeochemical processes are also examined and discussed special attention is given to the processes directly affecting nutrient transport magnitude timing and location with an emphasis on the spring freshet as a period of great nutrient export to rivers and lakes the review and model comparisons are used to 1 provide suggestions for model selection and recommendations for future research directions and 2 discuss the appropriateness of scale scope and complexity of nutrient models 2 nutrient export in cold agricultural regions key processes the movement of water and subsequent transport of nutrients in cold agricultural environments is strongly affected by the interplay of various snow climate soil and anthropogenic processes fig 1 the relative importance of these processes varies depending on the location and land use 2 1 snowmelt and soil dynamics hydrology transport and erosion chemicals accumulate in snowpacks winter and soil and snow processes including snowmelt and infiltration into frozen soils determine the hydrological pathways and residence times pomeroy et al 2007 of these chemicals nutrient transport is often high during snowmelt e g mcconkey et al 1997 corriveau et al 2013 due to frozen or thawing soils reducing infiltration and increasing runoff gaynor and bissonnette 1992 gray et al 2001 in the canadian prairies for instance most n and p is exported during this period corriveau et al 2013 this is due to the duration and extent of snowmelt far exceeding that of the convective rainfall events that drive summer rainfall runoff generation tiessen et al 2010 meltwater p and n may be infiltrated into the soil substrate runoff overland or follow a combination of surface and subsurface pathways depending on the infiltrability of the soil granger et al 1984 which may be affected by preferential infiltration of ions lilbaek 2007 the presence of basal ice lilbæk and pomeroy 2008 concrete frost jones and pomeroy 2001 and macropores zhao and gray 1999 in addition to the soil physical characteristics and soil water content gray et al 2001 basal ice consists of an impermeable or patchy concrete frost that restricts and acts like a switch on infiltration gray et al 1985 jones and pomeroy 2001 lilbaek 2007 its effect on flow generation and solute transport is complex and varies dunne and black 1971 laudon et al 2004 but basal ice generally increases runoff efficiency by decreasing soil infiltrability and affects runoff chemistry by limiting the access to the soil matrix lilbaek 2007 a significant portion of nutrients can be stored and transformed in wetlands and topographic depressions neely and baker 1989 johnston 1991 crumpton and isenhart 1993 birgand et al 2007 and this storage may not necessarily connect to reach major streams and lakes in some regions spence et al 2010 shook et al 1993 with a relatively impermeable layer of clay rich glacial till deposits shaw and hendry 1998 wetlands in regions such as the prairie pothole region fill and dry in response to pluvial cycles and drought and only spill in extremely wet conditions fang and pomeroy 2008 snowmelt intensity and the presence of frozen soils determine the dominant infiltration mechanism i e infiltration excess or saturation excess runoff which has impacts on nutrient transport infiltration excess runoff occurs with what gray et al 1985 termed the limited case where soils are unsaturated and water entry into the soil ice water matrix voids controls infiltration rates infiltration excess is often associated with large soil p losses mcdowell 2012 because it often occurs when runoff begins and the soil is still rich in nutrients additionally thaw freeze cycles reduce soil cohesive strength and increase soil erodibility which promote sediment transport in early snowmelt e g edwards et al 1995 this combined with frozen soils which reduce soil permeability and increase runoff often leads to large nutrient losses during this period the relationship between the proportion of dissolved vs particulate nutrients and extent of soil frost is complex varies during the melt period and depends on the soil freeze thaw dynamics during early snowmelt dissolved nutrients both organic forms and inorganic forms largely no 3 nh 4 and po 4 are transported in higher portions than particulates due to reduced erosion from frozen soils tiessen et al 2010 cade menun et al 2013 enhanced soil erodibility from freeze thaw cycles often only occurs after the first spring thaw wall et al 1988 2 2 snowmelt and soil dynamics biogeochemistry cold climate conditions as well as soil and nutrient management have a strong impact on nutrient and sediment exports deelstra et al 2009 han et al 2010 the nongrowing season is a critical period for nutrient losses in cold agricultural areas liu et al 2019a including those in europe ulén et al 2019 the laurentian great lakes region good et al 2019 plach et al 2019 sadhukhan et al 2019 and northern great plains of north america liu et al 2019b n in soils is biologically influenced by processes such as mineralization and nitrification denitrification which are affected by oxygen e g bodelier et al 1996 temperature and soil type and moisture e g saad and conrad 1993 as well as the availability and quality of organic matter e g breitenbeck and bremner 1987 mineral no 3 is a major n species which is readily transported with runoff and vulnerable to leaching snider et al 2017 due to its high solubility in water and negative charge so that it is not well retained by soil particles the dynamics of p in soils i e release storage and speciation depend on ph temperature and organic carbon and is affected by erosion and sorption to soil minerals and organic matter the mass of p sorbed can increase with increasing concentrations of cations such as calcium ca 2 magnesium mg 2 and iron fe 2 or fe 3 in the soil haynes 1984 sanyal and de datta 1991 khosravi et al 2018 but frozen soils in cold regions can enhance the mobility of p by reducing runoff soil contact and limit sorption rates in early spring snowmelt cade menun et al 2013 enhanced p release from prolonged runoff exposure to flooded acidic e g ann et al 1999 ajmone marsan et al 2006 scalenghe et al 2012 and alkaline e g ponnamperuma 1972 amarawansha et al 2015 soils is in sharp contrast to the otherwise often slow rate of soil nutrient release from soils dharmakeerthi et al 2019 schneider et al 2019 macrae et al 2010 amarawansha et al 2015 examined 12 alkaline soils from manitoba during summer conditions laboratory incubation at 22 c and found for instance that p ca mg and mn were related this suggested that p release was controlled by the dissolution of mg and ca phosphates and reductive dissolution of mn phosphates however p release from flooded soils at low temperatures remains poorly understood despite its importance to nutrient transport during spring snowmelt amarawansha 2013 low temperatures are generally associated with decreased rates of biological processing including plant uptake mineralization and nitrification denitrification however high microbiological activity has been observed during snowmelt clark et al 2009 brooks et al 1996 tranter and jones 2001 in frozen and snow covered soils peters and driscoll 1987 brooks et al 1996 jones 1999 sebestyen et al 2008 clark et al 2009 pellerin et al 2012 snider et al 2017 and in snowpacks jones and deblois 1987 brooks et al 1996 mladenov et al 2012 despite the low temperatures evidence of active biological processes has been found even in cold barren and carbon limited alpine soils williams et al 1997 2007 brooks et al 1999 king et al 2008 arctic soils quinton and pomeroy 2006 jones 1999 as well as under lake ice cavaliere and baulch 2018 these winter and snowmelt processes can have important impacts on spring nutrient export for example mineralization of soil organic matter during winter has been linked to increased total dissolved p tdp availability during snowmelt freppaz et al 2007 and reduced root uptake following freeze thaw events has been associated with increased nutrient losses matzner and borken 2008 the impact of freeze thaw ft on nutrient release from soils liu et al 2019a and plant materials costa et al 2019a liu et al 2019a vanrobaeys et al 2019 including crop residue and vegetated strips is complex but has been receiving increasing attention liu et al 2019a dormant frozen plants tend to cause higher nutrient losses during snowmelt runoff timmons et al 1970 particularly p in actively growing young plants elliott 2013 the maximum amount of biomass p released which tends to occur in the first few hours of snowmelt costa et al 2019a depends on the extent of the plant cellular tissue damage that is affected by the number of ft cycles temperature of frost and temperature tolerance of the crop species e g bechmann et al 2005 øgaard 2015 cober et al 2018 costa et al 2019a five ft cycles during a hard frost temperature 18 c have been observed to frequently cause the maximum nutrient release costa et al 2019a bechmann et al 2005 snow is also a reservoir of nutrients in snow covered areas tranter and jones 2001 and snow n accumulated from atmospheric deposition can constitute a relevant source of readily available dissolved inorganic n din largely no 3 and nh 4 during snowmelt jones 1991 although its contribution to the total n pool of soils is often small soils contain primarily humic material not readily bio available i e organic n thus inorganic n from snow can stimulate microbial and plant growth in early snowmelt jones 1999 ionic pulses in early snowmelt discharge are also common and may lead to temporary acidification of streams marsh and pomeroy 1999 this is caused by ice crystal metamorphism in snow forcing the reallocation of ions within the snowpack colbeck 1976 brimblecombe et al 1985 pomeroy et al 2005 lilbaek 2007 it has been shown that this can cause the release of 50 80 of all snow ions within the initial 1 3 of the melt maulé and stein 1990 with the process being potentially further exaggerated when meltwater runs over a basal ice layer hodson 2006 several laboratory and field studies have shown however that snow din can be quickly depleted in early snowmelt particularly nh 4 via biological assimilation by snow algae jones and sochanska 1985 jones and deblois 1987 delmas et al 1996 wetlands and agricultural ponds and reservoirs often retain runoff water under normal conditions fang and pomeroy 2008 shook and pomeroy 2011 and they have the ability to attenuate nutrient exports price and waddington 2000 fisher and acreman 2004 tiessen et al 2010 for a long time e g fernandes et al 1996 however they have a limited storage capacity helfield and diamond 1997 and certain factors may cause the release of soluble n and p species fisher and acreman 2004 in the northern great plains the processes controlling retention and release of nutrients are complex and poorly understood baulch et al 2019a requiring further research amongst the factors commonly related to the retention or release of nutrients in wetlands are 1 nutrient loading and duration 2 hydraulic loading retention time 3 sediment oxygen redox water logging 4 vegetation processes 5 flow pathways 6 fluctuating water table height and 7 carbon content of wetland fisher and acreman 2004 2 3 agriculture practices agricultural practices affect nutrient transport if they alter hydrology nutrient sources or the interaction between water and nutrients during snowmelt runoff baulch et al 2019b for a detailed analysis nutrient transport is often higher during snowmelt but nutrients applied to soils during the growing season may also be transported in rainfall runoff events in the spring and summer nicholaichuk 1967 hansen et al 2002 glozier et al 2006 liu et al 2013b nutrient management primarily the addition of fertilizers or manure to support crop growth is an important determinant of nutrient sources the amount and form of nutrients applied their placement depth relative to runoff water penetration and location with respect to runoff pathways and timing relative to runoff all contribute to the availability of nutrients for transport in runoff little et al 2007 tillage practices and perennial vegetation can affect runoff pathways and timing through their control on the height of stubble residue on the land during winter that impacts snow redistribution by blowing snow transport pomeroy and gray 1995 pomeroy et al 1993 blowing snow redistribution to drainage channels and wetlands promotes snowmelt runoff amounts that exceed winter precipitation to these landscape units and makes them exceedingly important for runoff generation and in controlling streamflow generation processes these practices can also influence infiltration and p stratification elliott and efetha 1999 renton et al 2015 as well as the amount of nutrients in soil and vegetation that can interact with runoff tiessen et al 2010 liu et al 2014 riparian buffer strips of perennial vegetation between cropland and streams will have a similar influence on blowing snow trapping and runoff generation however this influence is in a small but critical portion of the landscape that is very often the contributing area for runoff generation shook et al 2015 sheppard et al 2006 kieta et al 2018 when perennial vegetation is grazed nutrients are altered through consumption of forage and deposition of urine and feces haynes 1984 and soil structure can deteriorate from compaction limiting infiltration naeth et al 1990 the pathway of runoff can also be affected by surface wetland or tile drainage and this can affect the timing of runoff and exposure to nutrient sources brunet and westbrook 2012 king et al 2015 for example while surface flow quickly interacts with surficial soil layers and is able to transport nutrients located mainly in these regions infiltration and tile flow interact with nutrients that may have leached through the soil profile through a more prolonged process 3 review of modelling methods a relatively large number of water quality models are available mekonnen 2016 updated a previous compilation of water quality models by shoemaker 1997 and identified 74 different models the physical and biochemical principles underlining the methods used in each of these models as well as the level of sophistication used in the methods deployed frequently differ between models often due to historical reasons i e the initial motivation for developing the model also catchment models require the representation of numerous processes the importance of which varies between catchments this often results in model developers representing processes differently depending on the model purpose and intended application five models were selected for comparison in this study the criteria used for model selection aimed at providing a wide ranging overview of the different modelling philosophies and strategies selected models were 1 catchment scale and 2 process based in addition they support 3 long term simulations and 4 simulate cold regions processes for practical reasons the analysis is limited to models that have been 5 widely used 6 somehow tested in cold climates and 7 for which adequate theoretical documentation is publicly available based on these criteria and taking into consideration a recent global overview and evaluation of watershed nutrient modelling presented by wellen et al 2015 a study which focused primarily on model performance comparisons and best practices in model implementation the models selected for review in this study are as follows hype hydrological predictions for the environment lindström et al 2010 arheimer et al 2012 hspf hydrological simulation program fortran bicknell et al 2005 duda et al 2012 inca and inca p integrated model of nitrogen and phosphorus in catchments whitehead et al 1998 wade et al 2002 2007 jackson blake et al 2016 swat soil and water assessment tool arnold et al 1998 and annagnps the annualized agricultural non point source bosch et al 1998 the analysis of the models is based on the latest official technical documentation available for each model hype march 2016 hspf september 1996 inca 1998 inca p july 2016 swat september 2011 annagnps march 2005 refereed publications were also used to complement the information provided in the manuals there is a version of swat developed for canada canswat that includes empirical algorithms for snow redistribution and frozen soils as well as bmp modules for representation of small reservoir holding pond wetland conservation tillage forage conversion riparian grazing management yang 2019 however because information about the technical background and performance of canswat is very limited it could not be included in this review the models selected for this analysis are compared regarding their 1 conceptual basis 2 model structure and 3 process representation the objective of this study is to provide a general overview of the current modelling practices to highlight options for model application further model development and limitations to model implementation in some environments 3 1 conceptual basis catchment nutrient models have been primarily developed to support nutrient management however the purpose for and the context in which these models have been initially developed influences aspects related to model structure theoretical foundations and temporal and spatial scales table 1 despite highlighting different modelling aspects in the official documentation model applications show that they share the common purpose of supporting the evaluation of the effect of management decisions and climate change on water quality hype however has been particularly designed with a focus on large ungauged basins all models are semi distributed e g utilize hydrological response units hrus and enable flexible temporal resolutions but the daily timestep is a default setting for all models except hspf that runs at hourly time intervals by default also annagpns allows for fully distributed i e structured mesh domain discretization inca and annagnps provide the simplest vertical discretization of the soil profile amongst the models examined which consists of a 2 layer system where the upper layer corresponds to the reactive soil zone and the lower layer corresponds to the deeper groundwater zone the upper layer in annagnps is considered a tillage layer with a fixed thickness at the opposite end of the spectrum is hspf with the most complex soil structure the soil is divided into four compartments with customizable properties hype enables up to 3 layers and flexible parameterization 3 2 process representation the computation of catchment processes in cold agricultural regions poses tremendous challenges due to hydrological biogeochemical and management variability across seasons and landscapes winter and snowmelt processes impose specific challenges to measuring and characterizing processes such as snow sublimation blowing snow redistribution snow water content snowmelt runoff frozen soil freeze thaw cycles and overwinter biogeochemical cycling in this section the methods implemented in the different models for calculation of processes of 1 general hydrology 2 cold regions hydrology and 3 biogeochemistry are reviewed and compared focus is given to aspects of hydrology and biogeochemistry during the winter and snowmelt period with the broad topic of general hydrology i e rainfall runoff being only briefly covered the results are summarized in easy to read circular plots with the full review material used to produce these graphics provided as supplementary material to facilitate model inter comparisons 3 2 1 hydrological processes adequate background hydrological hydraulic modelling is a critical step for successful nutrient transport simulations fig 2 summarizes the modelling capabilities of the reviewed models regarding both general panel a and cold regions panel b hydrology due to the diversity of the methods deployed in these models the methods are classified as 1 physically based or with more sophisticated process representation 2 semi empirical or with intermediate sophistication in process representation or 3 empirical the reader is referred to supplementary material for access to all the detailed information compiled about each model that was used to produce the figure annagnps has overall the most comprehensive and physically based representation of both general and cold regions processes and inca has the simplest evapotranspiration is the only general hydrology process panel a that is simulated with similar type of methods across all models i e penman monteith and other comparable methods groundwater and erosion are either neglected or simulated based on empirical methods in all models although differences exist in the level of detail and complexity of the approaches used while it is common that groundwater is simplified in mostly hydrological models the sensitivity of groundwater representation can vary markedly across regions panagoulia and dimou 1996 erickson and stefan 2009 carey et al 2013 although erosional transport of particulate p is important in most landscapes tiessen et al 2010 su et al 2010 modelling erosion remains a major scientific challenge fu et al 2019 and most models rely on parametric estimations of kinetic energy from raindrops and surface runoff to predict erosion with swat and annagnps applying the popular rusle method renard et al 1991 for erosion tile drainage drainage is simulated in most of the models based on the popular empirical hooghoudt equation or other simplified parameteric expressions annagnps is the most sophisticated and physically based model concerning cold regions processes the model can simulate most of the processes examined primarily because it solves the full energy balance across the snowpack and soil layers which is used to estimate snowmelt runoff over frozen soils rain on snow and erosion of soil layers these processes combined have been recognized as an important control of nutrient export at field scales costa et al 2017 costa and pomeroy 2019 and the importance of their physically based representation has long been documented pomeroy et al 2007 one process that is neglected in annagnps is the wind redistribution of snow a process that can significantly affect the spatial patterns of snowmelt runoff generation in open windswept agricultural regions such as the northern us great plains and the canadian prairies in north america e g pomeroy and gray 1995 while none of the models simulate blowing snow explicitly hype and swat have simple approaches to estimate snowcover heterogeneity i e sub grid variability of late season snow water equivalent swe based on elevation and land use in the case of hype and areal depletion curves based on elevation bands in the case of swat inca in turn is on the other side of the spectrum as to the simulation of cold regions processes it provides the most straightforward model framework largely based on parametric or empirical methods snowfall snow accumulation snowmelt and soil temperature are calculated in all models but the methods used vary snowmelt is calculated using the empirical temperature index method in most models except for hspf and annagnps which compute the snowpack energy balance the use of the temperature index model can be problematic because it requires recalibration for every new regional climatic input walter et al 2005 requires different parameters for rain on snowmelt and does not work well where snowmelt is dominated by solar radiation inputs male and gray 1981 the soil temperature is computed based on empirical relationships with air temperature in most models annagnps is the exception as it obtains this information from the energy balance computations the reader is referred to supplementary material for a more detailed description of the methods used in each model 3 2 2 biogeochemistry nutrient pools sources sinks and biogeochemistry 3 2 2 1 soil nutrient pools table 2 summarizes the nutrient pools used to model the different n and p species simulated in each model the conceptual basis used to represent n and p speciation varies significantly across models with hspf and swat emerging as the models with the most complex partitioning of mineral and organic n and p in soils 8 and 6 pools for n and p respectively and inca with the simplest 2 and 3 pools for n and p respectively in the case of mineral n hype and annagnps lump all species as dissolved inorganic nitrogen din despite this obscuring the different specific controls on the fate of nh 4 and no 3 the remaining models differentiate between no 3 and nh 4 with hspf further subdividing nh 4 into soluble and particulate bound fractions in the case of organic n don is only simulated by hype and hspf with the latter subdividing it further into labile and refractory fractions although don is an overlooked pathway of nitrogen loss it can be the dominant nitrogen species exported in agricultural systems with the environmental effects of urea for instance raising growing concerns e g donald et al 2013 2011 hype provides the simplest conceptual model for simulation of organic n that is based on don residue and labile pon named as fastn and refractory pon named as slown pools annagnps swat and hspf divide particulate organic n pon into three pools i e fresh residue and labile and refractory pon inca does not simulate organic n explicitly including its effect through sink and source terms this is a limitation since organic n can be reactive even in cold climates e g chantigny et al 2019 and also accumulate in agricultural areas through the continuous use of excess fertilizer which can cause large accumulation of organic n in soils and groundwater that can persist for decades van meter et al 2018 substantial differences also exist in the way p is simulated in the reviewed models inca p has the simplest conceptual model comprising of 3 pools a tdp pool a mineral labile p pool and a combined lumped inactive organic and mineral p pool hype and hspf are more detailed and divide mineral p into srp and particulate bound p while annagnps and swat go further and include an additional pool for mineral non active soluble p the organic p pools in hype are dop fastp which appears to comprise both residue p and labile pop and slowp representing refractory pop dop is only modelled in hype but all the remaining particulate fractions of organic p are simulated by the remaining models however there are important differences in the way the particulate organic pools pop are sub divided annagnps and hspf have similar approaches which consist of differentiating between fresh residue p and humic p swat uses a similar method but further divides humic p into active and stable sub pools finally hype uses a slightly different conceptual model where plant residue and the active portions of humic p are lumped into one single pool organic fastp with the remaining organic p being classified as slowp which stands for slow reacting organic p the approaches used in the models to group the different nutrient species into shared pools has implications on the type and number of transformation pathways that are explicitly represented to a large extent this determines the scope of the model since it defines the biogeochemical processes that can be included in the simulations however increasing the number of pools and overall complexity of biogeochemical pathways imposes significant challenges more nutrient pools and biogeochemical pathways require additional model parameterization a procedure that is often problematic because of little supporting field data this suggests that simpler biogeochemical modelling approaches like those in inca and hype that require a smaller number of parameters to calibrate may be more suitable for practical applications involving estimation of seasonal nutrient export at larger spatial scales however complex models like hspf that involve more demanding input data assimilation may be more appropriate for challenging applications in data rich environments we argue that such models may also be better armed to support process research since there is in principle a better match between observed and simulated n and p pools and biogeochemical pathways however model uncertainty and equifinality may intensify if models rely heavily on parameter calibration instead of on observable transferable and regionalized parameters 3 2 2 2 soil n biogeochemistry table 3 shows the biogeochemical transformations of soil n represented in each model which are limited by the nutrient pools i e model state variables they simulate for the sake of model inter comparison the transformations pathways are divided into 7 groups based on reaction types 1 denitrification 2 nitrification 3 sorption mobilization 4 desorption dissolution 5 mineralization 6 organic decomposition within organic species and 7 organic synthesis from both mineral and organic species organic decomposition is used to group all the transformations that involve going from more stable more complex species to labile simpler organic n forms with the opposite being designated as synthesis return dissolution is used to describe the processing where pon dissolves in water transforming into don hspf has the most complex biogeochemical model structure enabling a total of 11 reaction pathways from 7 transformation groups between its 8 n pools in turn inca and hype have the simplest model structure providing five possible reaction pathways from 4 transformations groups hype and annagnps simulate din without differentiating between no 3 and nh 4 rows a and b in the table hence nitrification transformation of nh 4 into no 3 is not simulated and denitrification is indirectly computed from a pre determined i e hard coded fraction of din that is intended to represent the proportion of no 3 in din this obscures the different controls on the fate of nh 4 e g volatilization versus no 3 e g denitrification the remaining models simulate no 3 and nh 4 explicitly mineralization is computed by all models although the number of transformation pathways for this type of reaction varies row e hype and hspf enable only one mineralization pathway each from fastn i e labile n to din in the case of hype and from pon labile to nh 4 in the case of hspf inca accounts for mineralization through source terms while annagnps and swat allowing mineralization to occur from both plant residue and organic labile n to both labile and refractory pon sorption and immobilization of soil n are only simulated by hspf row c which allows accounting for transformations from soluble labile and refractory no 3 nh 4 and don into labile and refractory pon and particulate bound nh 4 in turn processing of pon to don is only contemplated by hype and hspf row d with inca indirectly considering this effect through sink terms for both no 3 and nh 4 decomposition from refractory or residual pon to labile pon i e within organic n species is possible in all models row f except in inca which only simulates mineral n i e no 3 and nh 4 on the one hand hype and hspf provide the simplest conceptual model for this type of reactions which only includes the transformation of labile pon into refractory pon on the other hand swat and annagnps enable the simulation of residue on and refractory pon into labile pon with annagnps further accounting for the transformation of residue on into refractory pon finally synthesis return within organic n species row g the inverse of decomposition in row f is only simulated within annagnps and hspf through labile pon into refractory pon transformations fig 3 shows the controls of the different n reactions included in the models reviewed all transformations are represented as first order reactions that depend on the n species consumed e g nitrate in the case of denitrification and a decay parameter estimated or calibrated based on different environmental controls the impact of temperature on reaction rates is included in all models and for all transformations the impact of soil moisture is also taken into account in most models and most reactions except hspf which only enables limiting nitrification for soil moisture however the effect of substrate limitation on denitrification is only considered by swat and hype which use organic carbon concentrations and half saturation constants or the michaelis constant for application of the michaelis menten kinetics respectively half saturation constants are also used to limit sorption rates in hype and hspf 3 2 2 3 soil p biogeochemistry table 4 compares the biogeochemical transformations for soil p represented in each model similarly to the n models the transformations of p species are grouped into reaction types a sorption desorption b mineralization c decomposition and weathering d immobilization mineral to organic e synthesis return between mineral species f dissolution g decomposition and f synthesis return between organic species also here organic and mineral decomposition is used to group all the transformations that involve going from more stable more complex organic n species to labile simpler forms with the opposite being designated as synthesis return the term dissolution is used to describe the process where pop dissolves in water transforming into dop the term mineralization includes all transformations that convert an organic p species into a mineral p species the term decomposition within mineral or organic species is used to group all the transformations that alter a stable refractory p species into a reactive labile one and the term weathering is only used to explicitly include this designation as used in inca to characterize processing of inactivep into labilep inca p and hspf have the simplest biogeochemical model for p swat and annagnps have the most elaborate sorption desorption dynamics are simulated by all models row a but there are significant differences in the methods deployed while hype and hspf use the freundlich isotherms swat uses a p availability index inca uses an updated version of the equilibrium p concentration which employs a dynamic variable calculated as a function of adsorbed p and annagnps determines this equilibrium dynamics as a function of the soil sediment clay fraction and an empirical partitioning coefficient mineralization is computed by all models row b with hype and hspf providing the simplest pathway from pop to srp and swat allowing for additional mineralization routes from the different forms of pop popres and popact into the various species of mineral p srp minerpsol partp and or po 4 act depending on the model and the pools it simulates in turn mineral to organic immobilization row d is only simulated by hspf via active soluble po 4 popact conversion into pop decomposition row c and synthesis return row e within mineral p species are only simulated by swat and annagnps through a series of transformation pathways from to decomposition synthesis partp and minerpsol to from synthesis decomposition srp hspf only allows the processing of po 4 act into pop immobilization row d and inca takes a more generic approach based on exchanges between labile p and inactive p pools the cycling of p within organic forms rows f j is only simulated by hype annagnps and swat with inca treating all organic p as a single pool named inactive p rows d and e at the inca column while annagnps has the simplest conceptual model for p one which is limited to the decomposition of residue pop into pop row g swat has the most complex conceptual model that enables the decomposition of popres into popact and popstab and of popstab into popact row g as well as synthesis return of popact into popstab row j hype only allows for the decomposition of fastp corresponding to popres and popact together into slowp corresponding to popstab the processing of pop corresponding to fastn and slown in hype into dop is only considered in the hype model row f fig 4 shows the controls of the reaction rates included in the models reviewed similar to the n cycle the temperature dependency is the environmental factor most commonly used across the different models to control the rate of the various transformation pathways within the p cycle except for sorption desorption slice a here sediment properties are used in all models for application of the freundlich isotherm or other methods see table 4 soil moisture is also used in hype hspf and annagnps to control mineralization rates slice b as well as decomposition rates slices c and g in hype and swat sorption desorption slice a in hspf and processing of pop into dop slice f in hype annagnps uses a rather complex approach to simulate decomposition within mineral species partp into srp and minerpsol into srp slice c through a piece wise function that depends on soil ph levels here factors such as temperature soil moisture the concentration of caco3 and organic carbon are selectively combined to compute decomposition rates depending on ph levels the reader is referred to supplementary material for more information about the data compiled to generate tables 2 4 there details are also provided regarding the types of nutrient sources enabled in each model 4 discussion 4 1 model limitations and strengths suggestions for model selection and recommendations for future directions the models reviewed in this study exhibit conceptual differences that can be important depending on the region and application these differences are related to the characterization of the case study domain horizontal and vertical computational elements and the methods deployed to simulate the different hydrological and biogeochemical processes in this section various modelling aspects are discussed as to their strengths and limitations for different model applications this is used to provide suggestions for model selection and advance recommendations for future research 4 1 1 model structure the problem with heavily stratified soils the models reviewed are all semi distributed e g hydrological response units hrus and generally divide the soil into between two to four layers except for swat that allows for up to 10 layers and annagnps that can also be used as a fully distributed model i e structured mesh while the use of few vertical layers is common practice in hydrology e g hype topkapi ciarapica and todini 2002 additional surficial soil layers may be needed in heavily stratified agricultural soils i e subject to tillage to adequately represent the accumulation of fertilizer or manure application and soil mixing practices which determine the opportunity of soil nutrients to interact with runoff it has been shown that the amount and form of nutrients applied their placement depth relative to runoff water penetration and timing relative to runoff all contribute to the availability of nutrients for transport in runoff little et al 2007 tillage practices can influence soil stratification runoff pathways and infiltration elliott and efetha 1999 renton et al 2015 and the amount of nutrients in soil and vegetation that can interact with runoff tiessen et al 2010 liu et al 2014 coarse vertical soil resolutions in models also require the averaging of detailed soil data from soil surveys for use as model input forcing or for validation which may lead to the loss of important information to understand the temporal and spatial scale of processes 4 1 2 general hydrology call for meaningful model structures based on observable and transferable parameters there are several differences in the representation of hydrological processes between the models differences which may have important implications for nutrient transport predictions the modelling of rainfall runoff infiltration is generally based on simplified empirical methods such as the scs curve number approach techniques based on saturation and infiltration excess values or residence times and baseflow index concepts with only swat using the green ampt method for infiltration and the rational method for peak runoff rates similarly the routing schemes for streamflow used are primarily based on basic hydraulic principles that range between empirical and power law based functions depth area volume flow relationship tables and variable storage and the muskingum method with the manning s equation for open channel flow while some level of empiricisms and parameterization is often needed for more efficient computations at catchment scales meaningful model structures that combine methods based on observable and transferable parameters are needed we argue that methods like the scs curve number approach for rainfall runoff estimation and empirical or power law formulations for routing for instance are undesirable for they rely on non observable less transferable and non regionalized model parameters 4 1 3 cold regions hydrology most models decades behind the science the hydrology of cold regions is strongly influenced by the seasonality of air temperature and soil and snow energy balances snow redistribution with snowmelt and frozen soils for instance being key to determining the opportunity for runoff and soil interactions pomeroy et al 2007 tiessen et al 2010 costa et al 2017 2019b however this comparative study identified several key cold regions processes with lack or poor representation in models 1 snow redistribution and sublimation by wind including chemical transformations during transport pomeroy et al 1991 1993 pomeroy and jones 1996 2 energetics of snowmelt and areal snowcover depletion e g debeer and pomeroy 2017 3 snowpack physics and chemistry of flow including preferential elution costa et al 2018 costa and pomeroy 2019 4 basal ice layer formation and impacts on runoff soil interaction lilbæk and pomeroy 2008 5 infiltration into unsaturated frozen soils including preferential infiltration of ions lilbaek 2007 6 thaw of saturated and unsaturated frozen soils and implications for erodibility and solute mixing e g edwards et al 1995 7 shallow subsurface flow mechanisms such as cracks and tile drainage zhang et al 2016 7 rain on snow and its impact on early nutrient transport chemistry and velocity of detention flow costa and pomeroy 2019 and 8 ponding processes depressional storage and wetlands brunet and westbrook 2012 snow redistribution for instance can significantly affect the spatial patterns of snowmelt runoff generation in open windswept agricultural regions such as the northern us great plains and the canadian prairies in north america e g pomeroy and li 2000 pomeroy and gray 1995 however this process is neglected in all models with only hype and swat accounting for snowcover heterogeneity and areal depletion via simplified parametric modelling schemes simultaneously most of the models combine the temperature index method for calculation of snowmelt with other empirical methods for computation of soil temperature with only annagnps solving the full energy balance for both snow and soil however despite the simplicity of the temperature index method being attractive for catchment simulations research has shown that it can be problematic because of non physical temporally unstable and difficult to regionalize temperature index parameters walter et al 2005 in addition to neglecting sublimation losses during snow ablation this can lead to misrepresentation of snowmelt rates with consequences for the estimation of chemical transport timing and magnitude during this short but critical time of nutrient export corriveau et al 2013 also the problem of infiltration into frozen soils which may be critical for runoff infiltration and nutrient transport forecasting in cold regions e g walter et al 2005 can be addressed in a more physically based manner if the dynamics of snow and soil energy balances are simulated 4 1 4 erosion remains a major scientific challenge the modelling of erosion is scientifically challenging and including the impact of cold regions processes such as frozen soils snowmelt intensity and freeze thaw cycles remains largely unrepresented in models the empirical rusle method is generally regarded as the state of the art with regards to the simulation of erosion at catchment scales but it is only used in swat and annagnps with the remaining models relying on other empirical formulations based on the estimation of falling raindrops and surface runoff mobilization energies however despite the popularity of rusle this method has limitations for cold climates 1 it is an empirical method derived from a limited set of observations 2 it has been designed for average long term erosion risk assessments 3 it does not consider the effect of antecedent soil moisture conditions and soil stratification on soil cohesion and 4 it is unsuitable for prediction of sediment transport throughout individual rainfall runoff or snowmelt events because splash erosion soil transport and soil deposition are not treated as dynamic processes foster et al 2000a b in cold regions higher erosion rates are expected over partially frozen soils than in unfrozen soils because frozen conditions typically precede unfrozen ones and particulates are often depleted mainly from the topsoil during early snowmelt where partially frozen soils are common e g ollesch et al 2006 panuska and karthikeyan 2010 frozen soils which reduce soil permeability and increase runoff and often lead to significant nutrient losses during snowmelt e g ollesch et al 2006 panuska and karthikeyan 2010 are also ignored in most of the models reviewed except for annagnps research shows that the relationship between the dissolved particulate fraction and extent of soil frost is complex varies during the melt period and depends on soil frost dynamics as melt starts restricted infiltration caused by soil frost reduces soil erodibility which results in lower particulate p than dissolved p losses e g tiessen et al 2010 freeze thaw cycles reduce the cohesive strength of soils increasing erodibility which intensifies sediment transport in early snowmelt e g edwards et al 1995 however this central aspect of erosion in cold regions is neglected in the models 4 1 5 nutrient pools biogeochemical cycles model selection should depend on regional dominant transformation processes this section focuses on the modelling of biogeochemical processes however it is well known that the criteria often used for model selection is strongly based on hydrological proceses while in many cases these are needed due to lack of representation of important processes in some models e g rain on snow is only simulated by hspf and annagnps the ability of models to represent biogeochemical cycling in ways that capture key regional biogeochemical processes in meaningful ways should receive careful consideration in order to maximize the outcome of the modelling effort as well as its acceptance by different stakeholders and scientific communities e g hydrologists and biogeochemists most nutrient pools and biogeochemical processes are directly or indirectly simulated by all models despite differences in the way the different mineral or organic nutrient species are grouped into shared pools see tables 3 and 4 for n and p respectively some differences however may be relevant for some model applications for example don and dop are ignored in all models except hype although this does not affect the capacity of models to capture the dominant n species exported in agricultural systems it limits their ability to address nutrient species of growing concern such as urea e g donald et al 2013 2011 which are raising growing concerns likewise the simulation of din by some models without differentiating between no 3 and nh 4 i e hype and annagnps hides the different controls on the fate of the different n species represented by din namely nitrification denitrification and nh 4 volatilization which are directly computed from din or a hard coded fraction of it finally the processing of pon into don and pop into dop all organic nutrient pools are only computed in hype and hspf with inca indirectly considering this process through sink terms for both no 3 and nh 4 in reality however the equilibrium concentration of po 4 in soil solution depends on both desorption and dissolution of inorganic p as well as mineralization of organic p condron et al 2005 environmental factors mediate the rate of biogeochemical transformations but there are differences in the controls characterized in the models while the impact of temperature on n and p transformations rates is considered in all models the effect of soil moisture is only more broadly included in the simulation of the n cycle the impact of substrate limitation is only accounted for in hype for all n and p reactions via half saturation constants it is also considered in swat for denitrification and annagnps for p mineralization and desorption via fixed organic carbon concentrations or fractions however the temporal spatial dynamics of such substrates may result in feedback processes via coupled cycles for example organic carbon may change over time in soil and riverbed sediments e g seasonally causing denitrification rates to change e g bijay singh et al 1988 pfenning and mcmahon 1997 an effect that is not captured in any of the models all models compute sorption desorption of dop into partp based on the sediment properties through the freundlich isotherm or other methods annagnps simulates transformations within mineral partp or minerpsol into srp or organic popres into popact or popstab and popstab into popact species in greater detail than the remaining models see table 4 which may be more suitable for agricultural regions where the historical use of fertilizers may have lead to nutrient accumulation in soils i e nutrient legacy here soil temperature soil moisture soil ph levels and caco 3 and organic carbon concentrations are selectively combined piece wise functions to compute the decomposition rates we argue that this is an essential step towards more robust conceptual models since as opposed to the traditional basic first order kinetic model typically used such approaches may account for the sensitivity of soil organic matter decomposition to temperature and the quality of the substrate which has been extensively observed e g cotrufo et al 1994 hartley and ineson 2008 the enhacement of snowmelt nutrient release from plants subject to freeze thaw e g liu et al 2019a cober et al 2018 liu et al 2013a macrae et al 2010 see section 2 is not included in any of the models which only allow for pre determined constant plant n and p loads see suplementary material however recent progress in the modelling of these processes has been reported and should be considered for integration into catchment models costa et al 2019a for example performed a series of lab experiments where alfafa was subject to different freeze thaw treatments and used the results to develop the first process based model for prediction of the temporal dynamics of n and p release from plants during snowmelt the coupling of biogeochemical cycles and reaction networks can lead to high complexity a problem that is exacerated by the mix and match of hydrological and nutrient cycles between models while this problem does not seem to have a simple solution we argue that the key regional processes should be identified first and used to select the simplest model possible that is able to capture the dominant processes identified 4 1 6 limitations for long term simulations accumulation of organic immobile pools need more attention this review shows that while mineralization processes i e from organic to mineral are simulated in all models synthesis return of organic n or organic p i e labile pon pop into refractory don dop tables 3 and 4 are only simulated in hspf and annagnps with hspf being the only model to account for the sorption immobilization of mineral n into pon or particulate bound nh 4 this finding suggests that the primary emphasis of the reviewed models as a representative sample of the current modelling capacity is on the short term prediction of readily available mineral n and srp as a response to fertilizer manure applications this is accomplished either through direct mineral fertilizer inputs or through decomposition row f within organic n species table 3 followed or directly by mineralization of organic n and p species row e table 3 originating for instance from manure applications conversely the simulation of n and p accumulation in soils through synthesis return of labile pon pop is mostly ignored except for the most sophisticated models annagnps and hspf this suggests that simpler models are particularly suited for short term intra annual simulations to capture the impact of fertilizer use and plant residue on runoff n exports but they may be less appropriate for long term inter annual simulations where soil n and p accumulation return and legacies may be relevant e g van meter et al 2016 mclauchlan 2006 simulataneously the selection of the model should take into account the availability of data e g type of data data gaps and analytical issues for model set up and verification 4 2 appropriateness of scale scope and complexity further considerations for model selection focus on biogeochemistry the various nutrient species are grouped into shared nutrient pools differently depending on the model with substantial implications on the type and number of transformation pathways that can be explicitly simulated in part this informs the scope of the model as it dictates how complex biogeochemical processes and interactions are translated into mathematical systems however increasing the complexity of biogeochemical models and the number of nutrient pools has numerous challenges more nutrient pools involve new biogeochemical pathways that need to be characterized and parameterized a process that frequently is performed with little supporting field data this suggests that simpler models like inca and hype that require fewer parameters to calibrate may be recommended for applied research in data poor environments however the nature of simpler models makes them unavoidably more deeply dependent on non physical and unobservable parameters this increases the reliance of these models on calibration in turn more detailed representation of processes in complex models like hspf can make them arguably better suited for research studies where detailed process es dynamics are needed they can also possibly be more effective as interdisciplinary collaboration tools since there is a better match between observed and modelled nutrient cycles which may increase the perceived valued of and trust in the model amongst collaborators and help to reduce communication barriers across disciplines e g modellers hydrologists agrologists biogeochemists soil scientists however meaningful model structures with observable and transferable parameters that match the type of primary biophysical and chemical processes occurring over cold regions catchments and scales temporal and spatial are needed such that catchment discretization is meaningful and used to improve model performance and integrity without excessive reliance on parameter calibration also except for hspf the models reviewed have been primarily designed for daily time steps which means that they cannot possibly generate hydrographs for small catchments from snowmelt events that have high temporal variability and that occur over a few days costa et al 2017 2019b while it is true that most models can be applied at different timesteps than the daily default and some can receive inputs at subdaily time intervals the representation of crucial periods where subdaily variation is important remains inadequate for example the snowmelt period can have major changes in hydrology on timesteps of hours which also affect nutrient sources and concentrations the use of daily temperature index snowmelt models precludes any information on sub daily melt rates hence although many models use approaches such as direct linear scaling of processes and rates key processes which change over short timescales are often not adequately represented examining the scalability of the simulation methods deployed in the models and shifting to approaches that minimize scale dependencies e g snowpack energy balance instead of the temperature index method for snowmelt calculations is therefore a recommendation of this study 5 conclusions the adequacy of process representations and model structural uncertainty of five popular catchment scale hydrological nutrient models suitable for cold regions hype inca swat hspf and annagnps has been examined to inform criteria for model selection discuss the appropriateness of scale scope and complexity and provide recommendations for future research the study involved examining the methods used for prediction of processes of general hydrology cold regions hydrology and n and p biogeochemical cycling it was found that the hydrology of these models is often largely based on simplified methods and relies heavily on parameter calibration this points at the need for meaningful model structures with observable and transferable parameters that match the type of primary biophysical and chemical processes occurring over cold regions catchments and scales temporal and spatial this is important to ensure that the discretization of the catchment domain is meaningful and used to improve model performance and integrity without excessive reliance on parameter calibration the following cold regions processes were poorly represented or completely missing in most models and should be addressed in future research and model developments 1 snow redistribution and sublimation by wind including chemical transformations during transport 2 energetics of snowmelt and areal snowcover depletion 3 snowpack physics and chemistry of flow including preferential elution 4 basal ice layer formation and impacts on runoff soil interaction 5 soil and water freeze thaw effects on nutrient release 6 infiltration into unsaturated frozen soils including preferential infiltration of ions 7 thaw of saturated and unsaturated frozen soils and implications for erodibility and solute mixing 8 shallow subsurface flow mechanisms such as cracks and tile drainage 9 rain on snow and its impact on early nutrient transport chemistry and velocity of detention flow and 10 ponding this study suggests that highly stratified soils arising from agricultural practices may be difficult to represent adequately in such models because the vertical discretization of the soils is sometimes limited to two vertical layers most nutrient pools and biogeochemical processes are directly or indirectly simulated by all models despite differences in the way the different mineral or organic nutrient species are grouped into share nutrient pools some differences however may be relevant for some model applications don and dop are neglected in most models and soil temperature is the main factor used across models to limit the rate of all transformations between different species in both the n and p cycle other environmental factors such as soil moisture are mainly used for the simulation of n transformations and the effect of substrate limitation and ph are generally neglected also the accumulation of organic immobile nutrient pools in soils due to historical use of fertilizers needs more attention it was found that most catchment scale models are particularly suited for short term intra annual studies of the impact of fertilizer manure applications on runoff nutrient export since the n and p biogeochemical models focus primarily on decomposition and mineralization with accumulation return processes being largely neglected except for more sophisticated and customizable biogeochemical models such as found in hspf the selection of the appropriate conceptual model should therefore depend on the regional dominant transformation processes to avoid excessive model uncertainty arising from model parameter non identifiability it is highlighted that simpler biogeochemical models like those in inca and hype which require fewer parameters to calibrate may be more suitable for seasonal concentration estimates in data poor environments if there is sufficient data to calibrate their parameters more sophisticated models like hspf and annagnps involving more challenging input data assimilation may however potentially be used in more challenging applications to support basic research and have more identifiable parameters declaration of competing interest the authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest such as honoraria educational grants participation in speakers bureaus membership employment consultancies stock ownership or other equity interest and expert testimony or patent licensing arrangements or non fi financial interest such as personal or professional relationships affiliations knowledge or beliefs in the subject matter or materials discussed in this manuscript acknowledgments the authors would like to thank the global water futures programme the canada excellence research chair in water security the canada research chair in water resources and climate change the canadian water network and the natural sciences and engineering research council nserc through its create in water security and discovery grants 463960 2015 for financial support appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104586 
26078,the hydrology of cold regions has been studied for decades with substantial progress in process understanding and prediction simultaneously work on nutrient yields from agricultural land in cold regions has shown much slower progress advancement of nutrient modelling is constrained by well documented issues of spatial heterogeneity climate dependency data limitations and over parameterization of models as well as challenges specific to cold regions due to the complex and often unknown behaviour of hydro biogeochemical processes at temperatures close to and below freezing where a phase change occurs this review is a critical discussion of these issues by taking a close look at the conceptual models and methods behind used catchment nutrient models the impact of differences in model structure and the methods used for the prediction of hydrological processes erosion and biogeochemical cycles are examined the appropriateness of scale scope and complexity of models are discussed to propose future research directions keywords seasonal snow cover nutrient transport cold region processes hydrological controls catchment models 1 introduction agricultural activities and associated runoff of excess nutrients have impaired the ecological function of streams and lakes around the world carpenter et al 1998 withers and lord 2002 schindler et al 2012 this has led to significant efforts to reduce nutrient export and contain the growing global problem of eutrophication and cyanobacterial blooms however the quantification and prediction of nutrient exports to streams lakes and estuaries remain a difficult challenge despite decades of research on nutrient cycling and transport wade et al 2008 cold climate regions are characterized by an average air temperature above 10 c in the warmest months and below 3 c in the coldest months peel et al 2007 cold regions hydrology is conceived as occurring in catchments where snowcover and frozen soils play a notable role in the hydrological cycle here the problem of nutrient transport and pollution is affected by snow related processes because they have a strong impact on flow generation erosion and nutrient export e g brooks and williams 1999 eimers et al 2009 casson et al 2012 the spring freshet is often the major annual runoff event and its magnitude and timing depend on both fall winter processes and antecedent conditions such as soil moisture snowfall and snow redistribution as well as the characteristics of the snowmelt event such as duration intensity and presence of frozen soils in the canadian prairies for instance snowmelt runoff can account for more than 80 of the total annual runoff volume gray and landine 1988 and contribute the most nitrogen n and phosphorus p exported yearly corriveau et al 2013 in these areas snowmelt volume melt rate and seasonally frozen soils are critical factors determining runoff soil contact and erodibility e g ollesch et al 2006 panuska and karthikeyan 2010 tiessen et al 2010 sub zero temperatures snowpacks freeze thaw cycling and frozen soils may affect the biogeochemistry of these areas with impacts on nitrogen p and n there is considerable debate about the appropriateness of scale scope complexity and accuracy of water quality models moore et al 2006 conventional process based catchment nutrient models are increasingly complex and heavily parameterized but substantially simplify reality beck 1987 wade et al 2008 costa et al 2019b uncertainties associated with hydrological and biogeochemical responses at various spatial scales and sparse sporadic water quality measurements with only rare measurements of key processes and pools further complicate the adequate use of catchment nutrient models raising critical questions for the design application and benefit of such modelling tools this paper reviews the structure and conceptual foundation within widely used catchment nutrient models that have been applied in cold regions it focusses primarily on processes specific to cold regions hydrology which are the processes involving snow ice and frozen soils as they affect the hydrological cycle general hydrological and biogeochemical processes are also examined and discussed special attention is given to the processes directly affecting nutrient transport magnitude timing and location with an emphasis on the spring freshet as a period of great nutrient export to rivers and lakes the review and model comparisons are used to 1 provide suggestions for model selection and recommendations for future research directions and 2 discuss the appropriateness of scale scope and complexity of nutrient models 2 nutrient export in cold agricultural regions key processes the movement of water and subsequent transport of nutrients in cold agricultural environments is strongly affected by the interplay of various snow climate soil and anthropogenic processes fig 1 the relative importance of these processes varies depending on the location and land use 2 1 snowmelt and soil dynamics hydrology transport and erosion chemicals accumulate in snowpacks winter and soil and snow processes including snowmelt and infiltration into frozen soils determine the hydrological pathways and residence times pomeroy et al 2007 of these chemicals nutrient transport is often high during snowmelt e g mcconkey et al 1997 corriveau et al 2013 due to frozen or thawing soils reducing infiltration and increasing runoff gaynor and bissonnette 1992 gray et al 2001 in the canadian prairies for instance most n and p is exported during this period corriveau et al 2013 this is due to the duration and extent of snowmelt far exceeding that of the convective rainfall events that drive summer rainfall runoff generation tiessen et al 2010 meltwater p and n may be infiltrated into the soil substrate runoff overland or follow a combination of surface and subsurface pathways depending on the infiltrability of the soil granger et al 1984 which may be affected by preferential infiltration of ions lilbaek 2007 the presence of basal ice lilbæk and pomeroy 2008 concrete frost jones and pomeroy 2001 and macropores zhao and gray 1999 in addition to the soil physical characteristics and soil water content gray et al 2001 basal ice consists of an impermeable or patchy concrete frost that restricts and acts like a switch on infiltration gray et al 1985 jones and pomeroy 2001 lilbaek 2007 its effect on flow generation and solute transport is complex and varies dunne and black 1971 laudon et al 2004 but basal ice generally increases runoff efficiency by decreasing soil infiltrability and affects runoff chemistry by limiting the access to the soil matrix lilbaek 2007 a significant portion of nutrients can be stored and transformed in wetlands and topographic depressions neely and baker 1989 johnston 1991 crumpton and isenhart 1993 birgand et al 2007 and this storage may not necessarily connect to reach major streams and lakes in some regions spence et al 2010 shook et al 1993 with a relatively impermeable layer of clay rich glacial till deposits shaw and hendry 1998 wetlands in regions such as the prairie pothole region fill and dry in response to pluvial cycles and drought and only spill in extremely wet conditions fang and pomeroy 2008 snowmelt intensity and the presence of frozen soils determine the dominant infiltration mechanism i e infiltration excess or saturation excess runoff which has impacts on nutrient transport infiltration excess runoff occurs with what gray et al 1985 termed the limited case where soils are unsaturated and water entry into the soil ice water matrix voids controls infiltration rates infiltration excess is often associated with large soil p losses mcdowell 2012 because it often occurs when runoff begins and the soil is still rich in nutrients additionally thaw freeze cycles reduce soil cohesive strength and increase soil erodibility which promote sediment transport in early snowmelt e g edwards et al 1995 this combined with frozen soils which reduce soil permeability and increase runoff often leads to large nutrient losses during this period the relationship between the proportion of dissolved vs particulate nutrients and extent of soil frost is complex varies during the melt period and depends on the soil freeze thaw dynamics during early snowmelt dissolved nutrients both organic forms and inorganic forms largely no 3 nh 4 and po 4 are transported in higher portions than particulates due to reduced erosion from frozen soils tiessen et al 2010 cade menun et al 2013 enhanced soil erodibility from freeze thaw cycles often only occurs after the first spring thaw wall et al 1988 2 2 snowmelt and soil dynamics biogeochemistry cold climate conditions as well as soil and nutrient management have a strong impact on nutrient and sediment exports deelstra et al 2009 han et al 2010 the nongrowing season is a critical period for nutrient losses in cold agricultural areas liu et al 2019a including those in europe ulén et al 2019 the laurentian great lakes region good et al 2019 plach et al 2019 sadhukhan et al 2019 and northern great plains of north america liu et al 2019b n in soils is biologically influenced by processes such as mineralization and nitrification denitrification which are affected by oxygen e g bodelier et al 1996 temperature and soil type and moisture e g saad and conrad 1993 as well as the availability and quality of organic matter e g breitenbeck and bremner 1987 mineral no 3 is a major n species which is readily transported with runoff and vulnerable to leaching snider et al 2017 due to its high solubility in water and negative charge so that it is not well retained by soil particles the dynamics of p in soils i e release storage and speciation depend on ph temperature and organic carbon and is affected by erosion and sorption to soil minerals and organic matter the mass of p sorbed can increase with increasing concentrations of cations such as calcium ca 2 magnesium mg 2 and iron fe 2 or fe 3 in the soil haynes 1984 sanyal and de datta 1991 khosravi et al 2018 but frozen soils in cold regions can enhance the mobility of p by reducing runoff soil contact and limit sorption rates in early spring snowmelt cade menun et al 2013 enhanced p release from prolonged runoff exposure to flooded acidic e g ann et al 1999 ajmone marsan et al 2006 scalenghe et al 2012 and alkaline e g ponnamperuma 1972 amarawansha et al 2015 soils is in sharp contrast to the otherwise often slow rate of soil nutrient release from soils dharmakeerthi et al 2019 schneider et al 2019 macrae et al 2010 amarawansha et al 2015 examined 12 alkaline soils from manitoba during summer conditions laboratory incubation at 22 c and found for instance that p ca mg and mn were related this suggested that p release was controlled by the dissolution of mg and ca phosphates and reductive dissolution of mn phosphates however p release from flooded soils at low temperatures remains poorly understood despite its importance to nutrient transport during spring snowmelt amarawansha 2013 low temperatures are generally associated with decreased rates of biological processing including plant uptake mineralization and nitrification denitrification however high microbiological activity has been observed during snowmelt clark et al 2009 brooks et al 1996 tranter and jones 2001 in frozen and snow covered soils peters and driscoll 1987 brooks et al 1996 jones 1999 sebestyen et al 2008 clark et al 2009 pellerin et al 2012 snider et al 2017 and in snowpacks jones and deblois 1987 brooks et al 1996 mladenov et al 2012 despite the low temperatures evidence of active biological processes has been found even in cold barren and carbon limited alpine soils williams et al 1997 2007 brooks et al 1999 king et al 2008 arctic soils quinton and pomeroy 2006 jones 1999 as well as under lake ice cavaliere and baulch 2018 these winter and snowmelt processes can have important impacts on spring nutrient export for example mineralization of soil organic matter during winter has been linked to increased total dissolved p tdp availability during snowmelt freppaz et al 2007 and reduced root uptake following freeze thaw events has been associated with increased nutrient losses matzner and borken 2008 the impact of freeze thaw ft on nutrient release from soils liu et al 2019a and plant materials costa et al 2019a liu et al 2019a vanrobaeys et al 2019 including crop residue and vegetated strips is complex but has been receiving increasing attention liu et al 2019a dormant frozen plants tend to cause higher nutrient losses during snowmelt runoff timmons et al 1970 particularly p in actively growing young plants elliott 2013 the maximum amount of biomass p released which tends to occur in the first few hours of snowmelt costa et al 2019a depends on the extent of the plant cellular tissue damage that is affected by the number of ft cycles temperature of frost and temperature tolerance of the crop species e g bechmann et al 2005 øgaard 2015 cober et al 2018 costa et al 2019a five ft cycles during a hard frost temperature 18 c have been observed to frequently cause the maximum nutrient release costa et al 2019a bechmann et al 2005 snow is also a reservoir of nutrients in snow covered areas tranter and jones 2001 and snow n accumulated from atmospheric deposition can constitute a relevant source of readily available dissolved inorganic n din largely no 3 and nh 4 during snowmelt jones 1991 although its contribution to the total n pool of soils is often small soils contain primarily humic material not readily bio available i e organic n thus inorganic n from snow can stimulate microbial and plant growth in early snowmelt jones 1999 ionic pulses in early snowmelt discharge are also common and may lead to temporary acidification of streams marsh and pomeroy 1999 this is caused by ice crystal metamorphism in snow forcing the reallocation of ions within the snowpack colbeck 1976 brimblecombe et al 1985 pomeroy et al 2005 lilbaek 2007 it has been shown that this can cause the release of 50 80 of all snow ions within the initial 1 3 of the melt maulé and stein 1990 with the process being potentially further exaggerated when meltwater runs over a basal ice layer hodson 2006 several laboratory and field studies have shown however that snow din can be quickly depleted in early snowmelt particularly nh 4 via biological assimilation by snow algae jones and sochanska 1985 jones and deblois 1987 delmas et al 1996 wetlands and agricultural ponds and reservoirs often retain runoff water under normal conditions fang and pomeroy 2008 shook and pomeroy 2011 and they have the ability to attenuate nutrient exports price and waddington 2000 fisher and acreman 2004 tiessen et al 2010 for a long time e g fernandes et al 1996 however they have a limited storage capacity helfield and diamond 1997 and certain factors may cause the release of soluble n and p species fisher and acreman 2004 in the northern great plains the processes controlling retention and release of nutrients are complex and poorly understood baulch et al 2019a requiring further research amongst the factors commonly related to the retention or release of nutrients in wetlands are 1 nutrient loading and duration 2 hydraulic loading retention time 3 sediment oxygen redox water logging 4 vegetation processes 5 flow pathways 6 fluctuating water table height and 7 carbon content of wetland fisher and acreman 2004 2 3 agriculture practices agricultural practices affect nutrient transport if they alter hydrology nutrient sources or the interaction between water and nutrients during snowmelt runoff baulch et al 2019b for a detailed analysis nutrient transport is often higher during snowmelt but nutrients applied to soils during the growing season may also be transported in rainfall runoff events in the spring and summer nicholaichuk 1967 hansen et al 2002 glozier et al 2006 liu et al 2013b nutrient management primarily the addition of fertilizers or manure to support crop growth is an important determinant of nutrient sources the amount and form of nutrients applied their placement depth relative to runoff water penetration and location with respect to runoff pathways and timing relative to runoff all contribute to the availability of nutrients for transport in runoff little et al 2007 tillage practices and perennial vegetation can affect runoff pathways and timing through their control on the height of stubble residue on the land during winter that impacts snow redistribution by blowing snow transport pomeroy and gray 1995 pomeroy et al 1993 blowing snow redistribution to drainage channels and wetlands promotes snowmelt runoff amounts that exceed winter precipitation to these landscape units and makes them exceedingly important for runoff generation and in controlling streamflow generation processes these practices can also influence infiltration and p stratification elliott and efetha 1999 renton et al 2015 as well as the amount of nutrients in soil and vegetation that can interact with runoff tiessen et al 2010 liu et al 2014 riparian buffer strips of perennial vegetation between cropland and streams will have a similar influence on blowing snow trapping and runoff generation however this influence is in a small but critical portion of the landscape that is very often the contributing area for runoff generation shook et al 2015 sheppard et al 2006 kieta et al 2018 when perennial vegetation is grazed nutrients are altered through consumption of forage and deposition of urine and feces haynes 1984 and soil structure can deteriorate from compaction limiting infiltration naeth et al 1990 the pathway of runoff can also be affected by surface wetland or tile drainage and this can affect the timing of runoff and exposure to nutrient sources brunet and westbrook 2012 king et al 2015 for example while surface flow quickly interacts with surficial soil layers and is able to transport nutrients located mainly in these regions infiltration and tile flow interact with nutrients that may have leached through the soil profile through a more prolonged process 3 review of modelling methods a relatively large number of water quality models are available mekonnen 2016 updated a previous compilation of water quality models by shoemaker 1997 and identified 74 different models the physical and biochemical principles underlining the methods used in each of these models as well as the level of sophistication used in the methods deployed frequently differ between models often due to historical reasons i e the initial motivation for developing the model also catchment models require the representation of numerous processes the importance of which varies between catchments this often results in model developers representing processes differently depending on the model purpose and intended application five models were selected for comparison in this study the criteria used for model selection aimed at providing a wide ranging overview of the different modelling philosophies and strategies selected models were 1 catchment scale and 2 process based in addition they support 3 long term simulations and 4 simulate cold regions processes for practical reasons the analysis is limited to models that have been 5 widely used 6 somehow tested in cold climates and 7 for which adequate theoretical documentation is publicly available based on these criteria and taking into consideration a recent global overview and evaluation of watershed nutrient modelling presented by wellen et al 2015 a study which focused primarily on model performance comparisons and best practices in model implementation the models selected for review in this study are as follows hype hydrological predictions for the environment lindström et al 2010 arheimer et al 2012 hspf hydrological simulation program fortran bicknell et al 2005 duda et al 2012 inca and inca p integrated model of nitrogen and phosphorus in catchments whitehead et al 1998 wade et al 2002 2007 jackson blake et al 2016 swat soil and water assessment tool arnold et al 1998 and annagnps the annualized agricultural non point source bosch et al 1998 the analysis of the models is based on the latest official technical documentation available for each model hype march 2016 hspf september 1996 inca 1998 inca p july 2016 swat september 2011 annagnps march 2005 refereed publications were also used to complement the information provided in the manuals there is a version of swat developed for canada canswat that includes empirical algorithms for snow redistribution and frozen soils as well as bmp modules for representation of small reservoir holding pond wetland conservation tillage forage conversion riparian grazing management yang 2019 however because information about the technical background and performance of canswat is very limited it could not be included in this review the models selected for this analysis are compared regarding their 1 conceptual basis 2 model structure and 3 process representation the objective of this study is to provide a general overview of the current modelling practices to highlight options for model application further model development and limitations to model implementation in some environments 3 1 conceptual basis catchment nutrient models have been primarily developed to support nutrient management however the purpose for and the context in which these models have been initially developed influences aspects related to model structure theoretical foundations and temporal and spatial scales table 1 despite highlighting different modelling aspects in the official documentation model applications show that they share the common purpose of supporting the evaluation of the effect of management decisions and climate change on water quality hype however has been particularly designed with a focus on large ungauged basins all models are semi distributed e g utilize hydrological response units hrus and enable flexible temporal resolutions but the daily timestep is a default setting for all models except hspf that runs at hourly time intervals by default also annagpns allows for fully distributed i e structured mesh domain discretization inca and annagnps provide the simplest vertical discretization of the soil profile amongst the models examined which consists of a 2 layer system where the upper layer corresponds to the reactive soil zone and the lower layer corresponds to the deeper groundwater zone the upper layer in annagnps is considered a tillage layer with a fixed thickness at the opposite end of the spectrum is hspf with the most complex soil structure the soil is divided into four compartments with customizable properties hype enables up to 3 layers and flexible parameterization 3 2 process representation the computation of catchment processes in cold agricultural regions poses tremendous challenges due to hydrological biogeochemical and management variability across seasons and landscapes winter and snowmelt processes impose specific challenges to measuring and characterizing processes such as snow sublimation blowing snow redistribution snow water content snowmelt runoff frozen soil freeze thaw cycles and overwinter biogeochemical cycling in this section the methods implemented in the different models for calculation of processes of 1 general hydrology 2 cold regions hydrology and 3 biogeochemistry are reviewed and compared focus is given to aspects of hydrology and biogeochemistry during the winter and snowmelt period with the broad topic of general hydrology i e rainfall runoff being only briefly covered the results are summarized in easy to read circular plots with the full review material used to produce these graphics provided as supplementary material to facilitate model inter comparisons 3 2 1 hydrological processes adequate background hydrological hydraulic modelling is a critical step for successful nutrient transport simulations fig 2 summarizes the modelling capabilities of the reviewed models regarding both general panel a and cold regions panel b hydrology due to the diversity of the methods deployed in these models the methods are classified as 1 physically based or with more sophisticated process representation 2 semi empirical or with intermediate sophistication in process representation or 3 empirical the reader is referred to supplementary material for access to all the detailed information compiled about each model that was used to produce the figure annagnps has overall the most comprehensive and physically based representation of both general and cold regions processes and inca has the simplest evapotranspiration is the only general hydrology process panel a that is simulated with similar type of methods across all models i e penman monteith and other comparable methods groundwater and erosion are either neglected or simulated based on empirical methods in all models although differences exist in the level of detail and complexity of the approaches used while it is common that groundwater is simplified in mostly hydrological models the sensitivity of groundwater representation can vary markedly across regions panagoulia and dimou 1996 erickson and stefan 2009 carey et al 2013 although erosional transport of particulate p is important in most landscapes tiessen et al 2010 su et al 2010 modelling erosion remains a major scientific challenge fu et al 2019 and most models rely on parametric estimations of kinetic energy from raindrops and surface runoff to predict erosion with swat and annagnps applying the popular rusle method renard et al 1991 for erosion tile drainage drainage is simulated in most of the models based on the popular empirical hooghoudt equation or other simplified parameteric expressions annagnps is the most sophisticated and physically based model concerning cold regions processes the model can simulate most of the processes examined primarily because it solves the full energy balance across the snowpack and soil layers which is used to estimate snowmelt runoff over frozen soils rain on snow and erosion of soil layers these processes combined have been recognized as an important control of nutrient export at field scales costa et al 2017 costa and pomeroy 2019 and the importance of their physically based representation has long been documented pomeroy et al 2007 one process that is neglected in annagnps is the wind redistribution of snow a process that can significantly affect the spatial patterns of snowmelt runoff generation in open windswept agricultural regions such as the northern us great plains and the canadian prairies in north america e g pomeroy and gray 1995 while none of the models simulate blowing snow explicitly hype and swat have simple approaches to estimate snowcover heterogeneity i e sub grid variability of late season snow water equivalent swe based on elevation and land use in the case of hype and areal depletion curves based on elevation bands in the case of swat inca in turn is on the other side of the spectrum as to the simulation of cold regions processes it provides the most straightforward model framework largely based on parametric or empirical methods snowfall snow accumulation snowmelt and soil temperature are calculated in all models but the methods used vary snowmelt is calculated using the empirical temperature index method in most models except for hspf and annagnps which compute the snowpack energy balance the use of the temperature index model can be problematic because it requires recalibration for every new regional climatic input walter et al 2005 requires different parameters for rain on snowmelt and does not work well where snowmelt is dominated by solar radiation inputs male and gray 1981 the soil temperature is computed based on empirical relationships with air temperature in most models annagnps is the exception as it obtains this information from the energy balance computations the reader is referred to supplementary material for a more detailed description of the methods used in each model 3 2 2 biogeochemistry nutrient pools sources sinks and biogeochemistry 3 2 2 1 soil nutrient pools table 2 summarizes the nutrient pools used to model the different n and p species simulated in each model the conceptual basis used to represent n and p speciation varies significantly across models with hspf and swat emerging as the models with the most complex partitioning of mineral and organic n and p in soils 8 and 6 pools for n and p respectively and inca with the simplest 2 and 3 pools for n and p respectively in the case of mineral n hype and annagnps lump all species as dissolved inorganic nitrogen din despite this obscuring the different specific controls on the fate of nh 4 and no 3 the remaining models differentiate between no 3 and nh 4 with hspf further subdividing nh 4 into soluble and particulate bound fractions in the case of organic n don is only simulated by hype and hspf with the latter subdividing it further into labile and refractory fractions although don is an overlooked pathway of nitrogen loss it can be the dominant nitrogen species exported in agricultural systems with the environmental effects of urea for instance raising growing concerns e g donald et al 2013 2011 hype provides the simplest conceptual model for simulation of organic n that is based on don residue and labile pon named as fastn and refractory pon named as slown pools annagnps swat and hspf divide particulate organic n pon into three pools i e fresh residue and labile and refractory pon inca does not simulate organic n explicitly including its effect through sink and source terms this is a limitation since organic n can be reactive even in cold climates e g chantigny et al 2019 and also accumulate in agricultural areas through the continuous use of excess fertilizer which can cause large accumulation of organic n in soils and groundwater that can persist for decades van meter et al 2018 substantial differences also exist in the way p is simulated in the reviewed models inca p has the simplest conceptual model comprising of 3 pools a tdp pool a mineral labile p pool and a combined lumped inactive organic and mineral p pool hype and hspf are more detailed and divide mineral p into srp and particulate bound p while annagnps and swat go further and include an additional pool for mineral non active soluble p the organic p pools in hype are dop fastp which appears to comprise both residue p and labile pop and slowp representing refractory pop dop is only modelled in hype but all the remaining particulate fractions of organic p are simulated by the remaining models however there are important differences in the way the particulate organic pools pop are sub divided annagnps and hspf have similar approaches which consist of differentiating between fresh residue p and humic p swat uses a similar method but further divides humic p into active and stable sub pools finally hype uses a slightly different conceptual model where plant residue and the active portions of humic p are lumped into one single pool organic fastp with the remaining organic p being classified as slowp which stands for slow reacting organic p the approaches used in the models to group the different nutrient species into shared pools has implications on the type and number of transformation pathways that are explicitly represented to a large extent this determines the scope of the model since it defines the biogeochemical processes that can be included in the simulations however increasing the number of pools and overall complexity of biogeochemical pathways imposes significant challenges more nutrient pools and biogeochemical pathways require additional model parameterization a procedure that is often problematic because of little supporting field data this suggests that simpler biogeochemical modelling approaches like those in inca and hype that require a smaller number of parameters to calibrate may be more suitable for practical applications involving estimation of seasonal nutrient export at larger spatial scales however complex models like hspf that involve more demanding input data assimilation may be more appropriate for challenging applications in data rich environments we argue that such models may also be better armed to support process research since there is in principle a better match between observed and simulated n and p pools and biogeochemical pathways however model uncertainty and equifinality may intensify if models rely heavily on parameter calibration instead of on observable transferable and regionalized parameters 3 2 2 2 soil n biogeochemistry table 3 shows the biogeochemical transformations of soil n represented in each model which are limited by the nutrient pools i e model state variables they simulate for the sake of model inter comparison the transformations pathways are divided into 7 groups based on reaction types 1 denitrification 2 nitrification 3 sorption mobilization 4 desorption dissolution 5 mineralization 6 organic decomposition within organic species and 7 organic synthesis from both mineral and organic species organic decomposition is used to group all the transformations that involve going from more stable more complex species to labile simpler organic n forms with the opposite being designated as synthesis return dissolution is used to describe the processing where pon dissolves in water transforming into don hspf has the most complex biogeochemical model structure enabling a total of 11 reaction pathways from 7 transformation groups between its 8 n pools in turn inca and hype have the simplest model structure providing five possible reaction pathways from 4 transformations groups hype and annagnps simulate din without differentiating between no 3 and nh 4 rows a and b in the table hence nitrification transformation of nh 4 into no 3 is not simulated and denitrification is indirectly computed from a pre determined i e hard coded fraction of din that is intended to represent the proportion of no 3 in din this obscures the different controls on the fate of nh 4 e g volatilization versus no 3 e g denitrification the remaining models simulate no 3 and nh 4 explicitly mineralization is computed by all models although the number of transformation pathways for this type of reaction varies row e hype and hspf enable only one mineralization pathway each from fastn i e labile n to din in the case of hype and from pon labile to nh 4 in the case of hspf inca accounts for mineralization through source terms while annagnps and swat allowing mineralization to occur from both plant residue and organic labile n to both labile and refractory pon sorption and immobilization of soil n are only simulated by hspf row c which allows accounting for transformations from soluble labile and refractory no 3 nh 4 and don into labile and refractory pon and particulate bound nh 4 in turn processing of pon to don is only contemplated by hype and hspf row d with inca indirectly considering this effect through sink terms for both no 3 and nh 4 decomposition from refractory or residual pon to labile pon i e within organic n species is possible in all models row f except in inca which only simulates mineral n i e no 3 and nh 4 on the one hand hype and hspf provide the simplest conceptual model for this type of reactions which only includes the transformation of labile pon into refractory pon on the other hand swat and annagnps enable the simulation of residue on and refractory pon into labile pon with annagnps further accounting for the transformation of residue on into refractory pon finally synthesis return within organic n species row g the inverse of decomposition in row f is only simulated within annagnps and hspf through labile pon into refractory pon transformations fig 3 shows the controls of the different n reactions included in the models reviewed all transformations are represented as first order reactions that depend on the n species consumed e g nitrate in the case of denitrification and a decay parameter estimated or calibrated based on different environmental controls the impact of temperature on reaction rates is included in all models and for all transformations the impact of soil moisture is also taken into account in most models and most reactions except hspf which only enables limiting nitrification for soil moisture however the effect of substrate limitation on denitrification is only considered by swat and hype which use organic carbon concentrations and half saturation constants or the michaelis constant for application of the michaelis menten kinetics respectively half saturation constants are also used to limit sorption rates in hype and hspf 3 2 2 3 soil p biogeochemistry table 4 compares the biogeochemical transformations for soil p represented in each model similarly to the n models the transformations of p species are grouped into reaction types a sorption desorption b mineralization c decomposition and weathering d immobilization mineral to organic e synthesis return between mineral species f dissolution g decomposition and f synthesis return between organic species also here organic and mineral decomposition is used to group all the transformations that involve going from more stable more complex organic n species to labile simpler forms with the opposite being designated as synthesis return the term dissolution is used to describe the process where pop dissolves in water transforming into dop the term mineralization includes all transformations that convert an organic p species into a mineral p species the term decomposition within mineral or organic species is used to group all the transformations that alter a stable refractory p species into a reactive labile one and the term weathering is only used to explicitly include this designation as used in inca to characterize processing of inactivep into labilep inca p and hspf have the simplest biogeochemical model for p swat and annagnps have the most elaborate sorption desorption dynamics are simulated by all models row a but there are significant differences in the methods deployed while hype and hspf use the freundlich isotherms swat uses a p availability index inca uses an updated version of the equilibrium p concentration which employs a dynamic variable calculated as a function of adsorbed p and annagnps determines this equilibrium dynamics as a function of the soil sediment clay fraction and an empirical partitioning coefficient mineralization is computed by all models row b with hype and hspf providing the simplest pathway from pop to srp and swat allowing for additional mineralization routes from the different forms of pop popres and popact into the various species of mineral p srp minerpsol partp and or po 4 act depending on the model and the pools it simulates in turn mineral to organic immobilization row d is only simulated by hspf via active soluble po 4 popact conversion into pop decomposition row c and synthesis return row e within mineral p species are only simulated by swat and annagnps through a series of transformation pathways from to decomposition synthesis partp and minerpsol to from synthesis decomposition srp hspf only allows the processing of po 4 act into pop immobilization row d and inca takes a more generic approach based on exchanges between labile p and inactive p pools the cycling of p within organic forms rows f j is only simulated by hype annagnps and swat with inca treating all organic p as a single pool named inactive p rows d and e at the inca column while annagnps has the simplest conceptual model for p one which is limited to the decomposition of residue pop into pop row g swat has the most complex conceptual model that enables the decomposition of popres into popact and popstab and of popstab into popact row g as well as synthesis return of popact into popstab row j hype only allows for the decomposition of fastp corresponding to popres and popact together into slowp corresponding to popstab the processing of pop corresponding to fastn and slown in hype into dop is only considered in the hype model row f fig 4 shows the controls of the reaction rates included in the models reviewed similar to the n cycle the temperature dependency is the environmental factor most commonly used across the different models to control the rate of the various transformation pathways within the p cycle except for sorption desorption slice a here sediment properties are used in all models for application of the freundlich isotherm or other methods see table 4 soil moisture is also used in hype hspf and annagnps to control mineralization rates slice b as well as decomposition rates slices c and g in hype and swat sorption desorption slice a in hspf and processing of pop into dop slice f in hype annagnps uses a rather complex approach to simulate decomposition within mineral species partp into srp and minerpsol into srp slice c through a piece wise function that depends on soil ph levels here factors such as temperature soil moisture the concentration of caco3 and organic carbon are selectively combined to compute decomposition rates depending on ph levels the reader is referred to supplementary material for more information about the data compiled to generate tables 2 4 there details are also provided regarding the types of nutrient sources enabled in each model 4 discussion 4 1 model limitations and strengths suggestions for model selection and recommendations for future directions the models reviewed in this study exhibit conceptual differences that can be important depending on the region and application these differences are related to the characterization of the case study domain horizontal and vertical computational elements and the methods deployed to simulate the different hydrological and biogeochemical processes in this section various modelling aspects are discussed as to their strengths and limitations for different model applications this is used to provide suggestions for model selection and advance recommendations for future research 4 1 1 model structure the problem with heavily stratified soils the models reviewed are all semi distributed e g hydrological response units hrus and generally divide the soil into between two to four layers except for swat that allows for up to 10 layers and annagnps that can also be used as a fully distributed model i e structured mesh while the use of few vertical layers is common practice in hydrology e g hype topkapi ciarapica and todini 2002 additional surficial soil layers may be needed in heavily stratified agricultural soils i e subject to tillage to adequately represent the accumulation of fertilizer or manure application and soil mixing practices which determine the opportunity of soil nutrients to interact with runoff it has been shown that the amount and form of nutrients applied their placement depth relative to runoff water penetration and timing relative to runoff all contribute to the availability of nutrients for transport in runoff little et al 2007 tillage practices can influence soil stratification runoff pathways and infiltration elliott and efetha 1999 renton et al 2015 and the amount of nutrients in soil and vegetation that can interact with runoff tiessen et al 2010 liu et al 2014 coarse vertical soil resolutions in models also require the averaging of detailed soil data from soil surveys for use as model input forcing or for validation which may lead to the loss of important information to understand the temporal and spatial scale of processes 4 1 2 general hydrology call for meaningful model structures based on observable and transferable parameters there are several differences in the representation of hydrological processes between the models differences which may have important implications for nutrient transport predictions the modelling of rainfall runoff infiltration is generally based on simplified empirical methods such as the scs curve number approach techniques based on saturation and infiltration excess values or residence times and baseflow index concepts with only swat using the green ampt method for infiltration and the rational method for peak runoff rates similarly the routing schemes for streamflow used are primarily based on basic hydraulic principles that range between empirical and power law based functions depth area volume flow relationship tables and variable storage and the muskingum method with the manning s equation for open channel flow while some level of empiricisms and parameterization is often needed for more efficient computations at catchment scales meaningful model structures that combine methods based on observable and transferable parameters are needed we argue that methods like the scs curve number approach for rainfall runoff estimation and empirical or power law formulations for routing for instance are undesirable for they rely on non observable less transferable and non regionalized model parameters 4 1 3 cold regions hydrology most models decades behind the science the hydrology of cold regions is strongly influenced by the seasonality of air temperature and soil and snow energy balances snow redistribution with snowmelt and frozen soils for instance being key to determining the opportunity for runoff and soil interactions pomeroy et al 2007 tiessen et al 2010 costa et al 2017 2019b however this comparative study identified several key cold regions processes with lack or poor representation in models 1 snow redistribution and sublimation by wind including chemical transformations during transport pomeroy et al 1991 1993 pomeroy and jones 1996 2 energetics of snowmelt and areal snowcover depletion e g debeer and pomeroy 2017 3 snowpack physics and chemistry of flow including preferential elution costa et al 2018 costa and pomeroy 2019 4 basal ice layer formation and impacts on runoff soil interaction lilbæk and pomeroy 2008 5 infiltration into unsaturated frozen soils including preferential infiltration of ions lilbaek 2007 6 thaw of saturated and unsaturated frozen soils and implications for erodibility and solute mixing e g edwards et al 1995 7 shallow subsurface flow mechanisms such as cracks and tile drainage zhang et al 2016 7 rain on snow and its impact on early nutrient transport chemistry and velocity of detention flow costa and pomeroy 2019 and 8 ponding processes depressional storage and wetlands brunet and westbrook 2012 snow redistribution for instance can significantly affect the spatial patterns of snowmelt runoff generation in open windswept agricultural regions such as the northern us great plains and the canadian prairies in north america e g pomeroy and li 2000 pomeroy and gray 1995 however this process is neglected in all models with only hype and swat accounting for snowcover heterogeneity and areal depletion via simplified parametric modelling schemes simultaneously most of the models combine the temperature index method for calculation of snowmelt with other empirical methods for computation of soil temperature with only annagnps solving the full energy balance for both snow and soil however despite the simplicity of the temperature index method being attractive for catchment simulations research has shown that it can be problematic because of non physical temporally unstable and difficult to regionalize temperature index parameters walter et al 2005 in addition to neglecting sublimation losses during snow ablation this can lead to misrepresentation of snowmelt rates with consequences for the estimation of chemical transport timing and magnitude during this short but critical time of nutrient export corriveau et al 2013 also the problem of infiltration into frozen soils which may be critical for runoff infiltration and nutrient transport forecasting in cold regions e g walter et al 2005 can be addressed in a more physically based manner if the dynamics of snow and soil energy balances are simulated 4 1 4 erosion remains a major scientific challenge the modelling of erosion is scientifically challenging and including the impact of cold regions processes such as frozen soils snowmelt intensity and freeze thaw cycles remains largely unrepresented in models the empirical rusle method is generally regarded as the state of the art with regards to the simulation of erosion at catchment scales but it is only used in swat and annagnps with the remaining models relying on other empirical formulations based on the estimation of falling raindrops and surface runoff mobilization energies however despite the popularity of rusle this method has limitations for cold climates 1 it is an empirical method derived from a limited set of observations 2 it has been designed for average long term erosion risk assessments 3 it does not consider the effect of antecedent soil moisture conditions and soil stratification on soil cohesion and 4 it is unsuitable for prediction of sediment transport throughout individual rainfall runoff or snowmelt events because splash erosion soil transport and soil deposition are not treated as dynamic processes foster et al 2000a b in cold regions higher erosion rates are expected over partially frozen soils than in unfrozen soils because frozen conditions typically precede unfrozen ones and particulates are often depleted mainly from the topsoil during early snowmelt where partially frozen soils are common e g ollesch et al 2006 panuska and karthikeyan 2010 frozen soils which reduce soil permeability and increase runoff and often lead to significant nutrient losses during snowmelt e g ollesch et al 2006 panuska and karthikeyan 2010 are also ignored in most of the models reviewed except for annagnps research shows that the relationship between the dissolved particulate fraction and extent of soil frost is complex varies during the melt period and depends on soil frost dynamics as melt starts restricted infiltration caused by soil frost reduces soil erodibility which results in lower particulate p than dissolved p losses e g tiessen et al 2010 freeze thaw cycles reduce the cohesive strength of soils increasing erodibility which intensifies sediment transport in early snowmelt e g edwards et al 1995 however this central aspect of erosion in cold regions is neglected in the models 4 1 5 nutrient pools biogeochemical cycles model selection should depend on regional dominant transformation processes this section focuses on the modelling of biogeochemical processes however it is well known that the criteria often used for model selection is strongly based on hydrological proceses while in many cases these are needed due to lack of representation of important processes in some models e g rain on snow is only simulated by hspf and annagnps the ability of models to represent biogeochemical cycling in ways that capture key regional biogeochemical processes in meaningful ways should receive careful consideration in order to maximize the outcome of the modelling effort as well as its acceptance by different stakeholders and scientific communities e g hydrologists and biogeochemists most nutrient pools and biogeochemical processes are directly or indirectly simulated by all models despite differences in the way the different mineral or organic nutrient species are grouped into shared pools see tables 3 and 4 for n and p respectively some differences however may be relevant for some model applications for example don and dop are ignored in all models except hype although this does not affect the capacity of models to capture the dominant n species exported in agricultural systems it limits their ability to address nutrient species of growing concern such as urea e g donald et al 2013 2011 which are raising growing concerns likewise the simulation of din by some models without differentiating between no 3 and nh 4 i e hype and annagnps hides the different controls on the fate of the different n species represented by din namely nitrification denitrification and nh 4 volatilization which are directly computed from din or a hard coded fraction of it finally the processing of pon into don and pop into dop all organic nutrient pools are only computed in hype and hspf with inca indirectly considering this process through sink terms for both no 3 and nh 4 in reality however the equilibrium concentration of po 4 in soil solution depends on both desorption and dissolution of inorganic p as well as mineralization of organic p condron et al 2005 environmental factors mediate the rate of biogeochemical transformations but there are differences in the controls characterized in the models while the impact of temperature on n and p transformations rates is considered in all models the effect of soil moisture is only more broadly included in the simulation of the n cycle the impact of substrate limitation is only accounted for in hype for all n and p reactions via half saturation constants it is also considered in swat for denitrification and annagnps for p mineralization and desorption via fixed organic carbon concentrations or fractions however the temporal spatial dynamics of such substrates may result in feedback processes via coupled cycles for example organic carbon may change over time in soil and riverbed sediments e g seasonally causing denitrification rates to change e g bijay singh et al 1988 pfenning and mcmahon 1997 an effect that is not captured in any of the models all models compute sorption desorption of dop into partp based on the sediment properties through the freundlich isotherm or other methods annagnps simulates transformations within mineral partp or minerpsol into srp or organic popres into popact or popstab and popstab into popact species in greater detail than the remaining models see table 4 which may be more suitable for agricultural regions where the historical use of fertilizers may have lead to nutrient accumulation in soils i e nutrient legacy here soil temperature soil moisture soil ph levels and caco 3 and organic carbon concentrations are selectively combined piece wise functions to compute the decomposition rates we argue that this is an essential step towards more robust conceptual models since as opposed to the traditional basic first order kinetic model typically used such approaches may account for the sensitivity of soil organic matter decomposition to temperature and the quality of the substrate which has been extensively observed e g cotrufo et al 1994 hartley and ineson 2008 the enhacement of snowmelt nutrient release from plants subject to freeze thaw e g liu et al 2019a cober et al 2018 liu et al 2013a macrae et al 2010 see section 2 is not included in any of the models which only allow for pre determined constant plant n and p loads see suplementary material however recent progress in the modelling of these processes has been reported and should be considered for integration into catchment models costa et al 2019a for example performed a series of lab experiments where alfafa was subject to different freeze thaw treatments and used the results to develop the first process based model for prediction of the temporal dynamics of n and p release from plants during snowmelt the coupling of biogeochemical cycles and reaction networks can lead to high complexity a problem that is exacerated by the mix and match of hydrological and nutrient cycles between models while this problem does not seem to have a simple solution we argue that the key regional processes should be identified first and used to select the simplest model possible that is able to capture the dominant processes identified 4 1 6 limitations for long term simulations accumulation of organic immobile pools need more attention this review shows that while mineralization processes i e from organic to mineral are simulated in all models synthesis return of organic n or organic p i e labile pon pop into refractory don dop tables 3 and 4 are only simulated in hspf and annagnps with hspf being the only model to account for the sorption immobilization of mineral n into pon or particulate bound nh 4 this finding suggests that the primary emphasis of the reviewed models as a representative sample of the current modelling capacity is on the short term prediction of readily available mineral n and srp as a response to fertilizer manure applications this is accomplished either through direct mineral fertilizer inputs or through decomposition row f within organic n species table 3 followed or directly by mineralization of organic n and p species row e table 3 originating for instance from manure applications conversely the simulation of n and p accumulation in soils through synthesis return of labile pon pop is mostly ignored except for the most sophisticated models annagnps and hspf this suggests that simpler models are particularly suited for short term intra annual simulations to capture the impact of fertilizer use and plant residue on runoff n exports but they may be less appropriate for long term inter annual simulations where soil n and p accumulation return and legacies may be relevant e g van meter et al 2016 mclauchlan 2006 simulataneously the selection of the model should take into account the availability of data e g type of data data gaps and analytical issues for model set up and verification 4 2 appropriateness of scale scope and complexity further considerations for model selection focus on biogeochemistry the various nutrient species are grouped into shared nutrient pools differently depending on the model with substantial implications on the type and number of transformation pathways that can be explicitly simulated in part this informs the scope of the model as it dictates how complex biogeochemical processes and interactions are translated into mathematical systems however increasing the complexity of biogeochemical models and the number of nutrient pools has numerous challenges more nutrient pools involve new biogeochemical pathways that need to be characterized and parameterized a process that frequently is performed with little supporting field data this suggests that simpler models like inca and hype that require fewer parameters to calibrate may be recommended for applied research in data poor environments however the nature of simpler models makes them unavoidably more deeply dependent on non physical and unobservable parameters this increases the reliance of these models on calibration in turn more detailed representation of processes in complex models like hspf can make them arguably better suited for research studies where detailed process es dynamics are needed they can also possibly be more effective as interdisciplinary collaboration tools since there is a better match between observed and modelled nutrient cycles which may increase the perceived valued of and trust in the model amongst collaborators and help to reduce communication barriers across disciplines e g modellers hydrologists agrologists biogeochemists soil scientists however meaningful model structures with observable and transferable parameters that match the type of primary biophysical and chemical processes occurring over cold regions catchments and scales temporal and spatial are needed such that catchment discretization is meaningful and used to improve model performance and integrity without excessive reliance on parameter calibration also except for hspf the models reviewed have been primarily designed for daily time steps which means that they cannot possibly generate hydrographs for small catchments from snowmelt events that have high temporal variability and that occur over a few days costa et al 2017 2019b while it is true that most models can be applied at different timesteps than the daily default and some can receive inputs at subdaily time intervals the representation of crucial periods where subdaily variation is important remains inadequate for example the snowmelt period can have major changes in hydrology on timesteps of hours which also affect nutrient sources and concentrations the use of daily temperature index snowmelt models precludes any information on sub daily melt rates hence although many models use approaches such as direct linear scaling of processes and rates key processes which change over short timescales are often not adequately represented examining the scalability of the simulation methods deployed in the models and shifting to approaches that minimize scale dependencies e g snowpack energy balance instead of the temperature index method for snowmelt calculations is therefore a recommendation of this study 5 conclusions the adequacy of process representations and model structural uncertainty of five popular catchment scale hydrological nutrient models suitable for cold regions hype inca swat hspf and annagnps has been examined to inform criteria for model selection discuss the appropriateness of scale scope and complexity and provide recommendations for future research the study involved examining the methods used for prediction of processes of general hydrology cold regions hydrology and n and p biogeochemical cycling it was found that the hydrology of these models is often largely based on simplified methods and relies heavily on parameter calibration this points at the need for meaningful model structures with observable and transferable parameters that match the type of primary biophysical and chemical processes occurring over cold regions catchments and scales temporal and spatial this is important to ensure that the discretization of the catchment domain is meaningful and used to improve model performance and integrity without excessive reliance on parameter calibration the following cold regions processes were poorly represented or completely missing in most models and should be addressed in future research and model developments 1 snow redistribution and sublimation by wind including chemical transformations during transport 2 energetics of snowmelt and areal snowcover depletion 3 snowpack physics and chemistry of flow including preferential elution 4 basal ice layer formation and impacts on runoff soil interaction 5 soil and water freeze thaw effects on nutrient release 6 infiltration into unsaturated frozen soils including preferential infiltration of ions 7 thaw of saturated and unsaturated frozen soils and implications for erodibility and solute mixing 8 shallow subsurface flow mechanisms such as cracks and tile drainage 9 rain on snow and its impact on early nutrient transport chemistry and velocity of detention flow and 10 ponding this study suggests that highly stratified soils arising from agricultural practices may be difficult to represent adequately in such models because the vertical discretization of the soils is sometimes limited to two vertical layers most nutrient pools and biogeochemical processes are directly or indirectly simulated by all models despite differences in the way the different mineral or organic nutrient species are grouped into share nutrient pools some differences however may be relevant for some model applications don and dop are neglected in most models and soil temperature is the main factor used across models to limit the rate of all transformations between different species in both the n and p cycle other environmental factors such as soil moisture are mainly used for the simulation of n transformations and the effect of substrate limitation and ph are generally neglected also the accumulation of organic immobile nutrient pools in soils due to historical use of fertilizers needs more attention it was found that most catchment scale models are particularly suited for short term intra annual studies of the impact of fertilizer manure applications on runoff nutrient export since the n and p biogeochemical models focus primarily on decomposition and mineralization with accumulation return processes being largely neglected except for more sophisticated and customizable biogeochemical models such as found in hspf the selection of the appropriate conceptual model should therefore depend on the regional dominant transformation processes to avoid excessive model uncertainty arising from model parameter non identifiability it is highlighted that simpler biogeochemical models like those in inca and hype which require fewer parameters to calibrate may be more suitable for seasonal concentration estimates in data poor environments if there is sufficient data to calibrate their parameters more sophisticated models like hspf and annagnps involving more challenging input data assimilation may however potentially be used in more challenging applications to support basic research and have more identifiable parameters declaration of competing interest the authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest such as honoraria educational grants participation in speakers bureaus membership employment consultancies stock ownership or other equity interest and expert testimony or patent licensing arrangements or non fi financial interest such as personal or professional relationships affiliations knowledge or beliefs in the subject matter or materials discussed in this manuscript acknowledgments the authors would like to thank the global water futures programme the canada excellence research chair in water security the canada research chair in water resources and climate change the canadian water network and the natural sciences and engineering research council nserc through its create in water security and discovery grants 463960 2015 for financial support appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104586 
26079,uncertainty and sensitivity analysis ua sa aid in assessing whether model complexity is warranted and under what conditions to support these analyses a variety of software tools have been developed to provide ua sa methods and approaches in a more accessible manner this paper applies a hybrid bibliometric approach using 11 625 publications sourced from the web of science database to identify software packages for ua sa used within the environmental sciences and to synthesize evidence of general research trends and directions use of local sensitivity approaches was determined to be prevalent although adoption of global sensitivity analysis approaches is increasing we find that interest in uncertainty management is also increasing particularly in improving the reliability and effectiveness of ua sa although available software is typically open source and freely available uptake of software tools is apparently slow or their use is otherwise under reported longevity is also an issue with many of the identified software appearing to be unmaintained improving the general usability and accessibility of ua sa tools may help to increase software longevity and the awareness and adoption of purpose appropriate methods usability should be improved so as to lower the cost of adoption of incorporating the software in the modelling workflow an overview of available software is provided to aid modelers in choosing an appropriate software tool for their purposes code and representative data used for this analysis can be found at https github com frog7 uasa trends 10 5281 zenodo 3406946 keywords uncertainty analysis sensitivity analysis analysis software packages bibliometric trends environmental modeling 1 introduction computational modeling has become a key activity in many areas of research in the environmental sciences the amount of available computational power and speed has led to the development of environmental models with ever increasing level of detail and complexity in this context complexity is reflected by the number of parameters a model incorporates as inputs these parameters may also be referred to as parameter factors factors or simply inputs in the literature norton 2015 increasing the number of parameters allows for a more detailed representation of the investigated system while also increasing computational cost and model complexity at an exponential rate increased detail and thus complexity may reduce the identifiability of parameters the ability to apportion model results to specific parameter values but is not always justified or necessary with respect to the aims of the modeling exercise increased complexity has led modelers to better appreciate the issue of model identifiability guillaume et al 2019 and to recognize the importance of understanding the contribution of model inputs with respect to model performance and purpose uncertainty and sensitivity analysis ua sa refer to the methods and approaches used to help researchers better understand the relative importance of each parameter factor within a given problem context put simply s ensitivity analysis assesses how variations in input parameters model parameters or boundary conditions affect the model output bennett et al 2013 with these approaches it is possible to better understand how sensitive model results are to parameter factors and how uncertain the model results are saltelli et al 2019 saltelli and annoni 2010 individual parameter factors may influence one or more outputs and could conditionally affect the importance of other factors referred to as parameter interaction the practice of analyzing uncertainty and sensitivity is now considered standard modeling practice the interested reader is directed to bennett et al 2013 norton 2015 pianosi et al 2016 razavi and gupta 2015 for introductory overviews and further information understanding the relative sensitivity of parameters can aid in the development of better monitoring strategies and experiment design for example indicating the priority and amount of data to be collected saltelli and tarantola 2002 the practice of sa can also help to constrain the parameter space by identifying parameters that may be insensitive or inactive having little to no effect on model results at least for the purpose of the modeling identifying such parameters can help constrain model complexity which in turn eases the computational cost of model evaluations for example to facilitate uncertainty analysis and the development of surrogate models in recent years a wide variety of software tools to support ua sa processes have become available that make such analyses more accessible to modelers to gain an overview of the available methods and tools we applied a hybrid bibliometric approach using publications from the web of science database while reviews of sensitivity analysis practice have been published see for example ferretti et al 2016 saltelli et al 2019 and comparisons between ua sa methods conducted for example gan et al 2014 sun et al 2012 to our knowledge there does not appear to be an overview of the available ua sa tools currently in use across different platforms and programming languages this paper follows on from and is distinguished from existing reviews such as matott et al 2009 refsgaard et al 2007 as it surveys ua sa in environmental modeling with a specific focus on sa we then provide information on the available tools as revealed through the bibliometric analysis and expert knowledge including implemented ua sa methods programming language and software features the aim here is then to provide 1 a brief introduction to the field of ua sa and its relevance to environmental modeling for those new to the field 2 an overview of ua sa research trends and 3 a guide to the development trends of ua sa tools their availability and relevance 2 key ua sa terminologies and methods often the first hurdle for those new to a research area is to grasp the multitude of acronyms and terms used in this section we briefly outline some common terminology ua sa methods and relevant publications for further reference these are provided here to contextualize the analysis and discussion later in this paper the information provided in this section is not exhaustive interested readers are directed to norton 2015 for a more thorough introduction to ua sa the descriptions of sensitivity analysis methods in pianosi et al 2016 the citations in table 1 and the citations in bennett et al 2013 p 3 pianosi et al 2015 identify three stages in a sensitivity analysis selecting a sample of input values from the variability space running a model evaluation against these input values and applying a sensitivity analysis method to the input output samples to compute sensitivity indices i e values which indicate each parameter s sensitivity for more information about the calculations for various sensitivity indices see norton 2015 here the variability space refers to all possible combinations of values that can be assigned to a model s input parameter set by running the model with the values sampled from the variability space and taking note of the resultant outputs analyses can be conducted to calculate the influence that a specific input or set of inputs may have i e their sensitivities the focus of this paper is on providing an overview of tools that aid in conducting these analyses methods to select the sample of input values are often characterized as being either local or global global methods gsa consider all dimensions of a model in one grand exercise leamer 1985 achieved by varying all parameter values at the same time gsa methods are themselves commonly categorized as being statistical derivative or variance based statistical methods use statistical analysis of the parameter space as a measure of sensitivity pianosi et al 2016 derivative based methods provide indices which characterize the distributional properties of partial derivatives razavi et al 2019 variance based approaches determine how different factors contribute to model variance by analyzing and decomposing the variance in model outputs razavi et al 2019 for brevity a full exploration of these methods is not provided here but a brief overview with references to relevant papers is given in table 1 the strength of gsa methods is that they provide a more robust depiction of model uncertainty by comprehensively accounting for parameter interactions saltelli and annoni 2010 such approaches assume a random distribution of output values in the parameter space and that such a distribution is plausible gsa methods can also be computationally expensive to perform as the parameter space being explored can be very large sampling methods schemes are used to aid in limiting the number of model runs involved whilst adequately representing the parameter space the computational cost of applying gsa methods may explain at least in part why their use is relatively uncommon compared to their local counterparts local sa methods lsa are anchored around a particular point in the parameter space with analysis involving comparisons against a known baseline output razavi and gupta 2015 the simplest most naïve and most common method of sa is one at a time oat as the name suggests this approach involves changing the value of a single parameter factor at a time referred to as perturbing whilst keeping all other parameters constant at their nominal values this approach could be described as taking samples along a single dimension with the changes to the output then attributed to the factor that was modified there are different approaches to how much the parameter value is perturbed but often a proportional increment is used e g increase or decrease a parameter by 10 of the nominal value up to and including a given bound razavi and gupta 2015 other lsa methods examine the partial derivatives of output with respect to each input parameter these are computed at one point in the sample space to determine sensitivity indices the simplicity of the procedure is advantageous as well as being computationally inexpensive for first order derivatives as they often do not require a formal sampling approach monte carlo mc a simple random sampling is commonly used although it offers a limited representation of the total parameter space gan et al 2014 the downside is that lsa only provides a robust indication of sensitivity for linear or additive models saltelli and annoni 2010 they do not account for parameter interactions and become computationally expensive when higher order and non linear effects are considered to resolve this issue several other sampling approaches have been developed and applied given each method and approach have their pros and cons multiple methods could be applied to obtain complementary results à la ensemble analysis sagi and rokach 2018 and should be considered where appropriate sun et al 2012 brief descriptions of commonly employed methods are given in table 1 methods are taken to be common where they are indicated to be so in recent review papers specifically gan et al 2014 pianosi et al 2016 the references found within these and those found within the identified corpora detailed in the next section 3 method the hybrid bibliometric approach to conduct this bibliometric review a collection of publications the corpora was gathered from clarivate analytics web of science wos database using the available web based application programming interface api use of the api enabled programmatic access to the publication data and metadata including titles abstract text author supplied keywords and dois data was retrieved with the use of wosis web of science analysis a python package developed to simplify the process of querying the wos database and aid in data analysis and visualization iwanaga and douglas smith 2019 publications in the resulting corpora were taken to represent the field of uncertainty and sensitivity analysis in the overarching field of environmental modeling to ensure as much transparency as possible much of the data collection and subsequent analysis was conducted programmatically in the python programming language the complete dataset cannot be made available as it is subject to clarivate analytics license terms representative datasets are provided instead along with the code developed for the analysis these can be viewed as a collection of jupyter notebooks and associated files at https github com frog7 uasa trends douglas smith and iwanaga 2019 names of specific notebooks will be referred to throughout this text where further detail can be found the corpora was iteratively and incrementally refined through a semi autonomous process of topic identification keyword search and subsequent manual analysis of the publications with the aid of key phrase extraction topic modeling briefly described in section 3 2 was used to aid in identifying a collection of papers relevant to uncertainty and sensitivity analysis and their overarching focus be it an application of or guiding frameworks for ua sa the publication and citation trends within these topic areas were then analyzed additional topic modeling complemented by a keyword search process was used to identify papers related to the use of ua sa software these were manually combed through with the aid of an automated key phrase identifier that helped to reduce the amount of text to be examined a subset of these papers were investigated for mention of software tools and packages the general search and analysis approach is depicted in fig 1 with further detail on topic modeling and key phrase identification provided within this section 3 1 initial search the initial corpora for the analysis was identified by specifying the search phrase with search fields bolded ts sensitivity analysis or uncertainty analysis or uncertainty quantification or uncertainty propagation or local sensitivity analysis or lsa or one at a time or exploratory modeling or oat or global sensitivity analysis or gsa or all at a time or aat and wc environmental sciences or water resources or engineering environmental or interdisciplinary applications this returns publications that use at least one of the specified terms those listed for the ts field within the title abstract or author supplied keywords for publications in the the wos defined subject areas specified for the wc field the raw search string is supplied for transparency and can be used to obtain the corpora from wos only english language publications between 2000 and 2017 were considered for this study with the ending year selected as the data request occurred in december of 2018 the approach taken at the time was to include full year datasets only the final search phrase applied with the specified time frame reduced the number of matches from over 500 000 to 11 718 publications the number of results obtained through the unrestricted search were far too many to comprehensively review at least in a timely manner the initial corpora for this study of 11 718 publications were then further constrained through the process depicted in fig 1 and is described in more detail below 3 2 topic identification a key focus in this study is the software tools and packages available to support ua sa processes the methods they implement and the trends of these to this end topic modeling was applied to constrain the corpora to relevant publications for further consideration topic models attempt to cluster texts into similar or related topics based on commonly occurring words and can aid in identifying new and emerging fields whilst also reducing the likelihood of bias and the required hours for a systematic review achakulvisut et al 2016 westgate et al 2018 topic modeling has been applied before to reduce the time and difficulties encountered when conducting systematic reviews westgate and lindenmayer 2017 however their use is still relatively limited and perhaps underutilized although software is available to aid in these bibliometric approaches currently no single software package provides all necessary functionality to conduct end to end systematic mapping the classification of articles based on their contents of research literature from data collection through to summarization and visualization arguably the conjunctive application of systematic mapping and bibliometric analysis is still in its infancy as evidenced by nakagawa et al 2018 topics are identified by the common co occurrence of semantics within a discipline for example sensitivity in the context of sa would conceptually be expected to appear in texts containing words such as analysis uncertainty and modeling the term sensitivity may also appear in relation to physical psychological response to stimuli in which case the term will appear alongside terms associated with the medical and therapy fields topics can be identified and represented through their common semantics the topic modeling approach provided within wosis non negative matrix factorization nmf is implemented with the scikit learn python package pedregosa et al 2011 the approach allows publications to be assigned to one or more topics arora et al 2012 and has been shown to be appropriate for collections of short texts chen et al 2019 this process was complemented with a traditional keyword search to help identify publications related to specific subjects tokens meaning specific words or terms for topic modeling consisted of the text found in the document titles abstracts and keywords the top 1000 tokens found within the corpora based on term frequency inverse document frequency tf idf rankings were selected for topic modeling tf idf is a common ranking method used in text mining beel et al 2016 a high tf idf score indicates that the word token has a high frequency within specific document s but a low number of occurrences within the entire corpora weighting the score in such a manner has the effect of filtering out commonly used tokens which may not have high semantic importance 3 3 key phrase identification once a topic area is identified the resulting sub corpora can be further constrained through automated key phrase identification the approach summarizes text aiding reviewers to identify irrelevant publications by reducing the amount of text for manual review the implemented approach attempts to identify these phrases of interest by scoring sentences based on their similarity with other sentences throughout the abstract text to elaborate each sentence s i is compared with other sentences in the abstract s y which are initially filtered based on the presence of a root token which is taken to be the token that appears in the middle of s i this root token selection approach is used in rabby et al 2018 for its simplicity and computational efficiency the similarity between s i and s y is then scored based on the ratio of the intersection of the two sentences sentences with three or less tokens i e words numbers or other counted by splitting the text on individual spaces are ignored the approach assumes that important features of the publication such as its key findings will be repeated throughout the considered fields title abstract and author supplied keywords these may for example be introduced or alluded to framed and the implications discussed the implemented approach is therefore dependent on the abstract length with longer texts preferred poor performance can be expected for very short abstracts e g 3 sentences or less and these were ignored for the purpose of this study comparisons with an established key phrase identification approach rake rapid automatic keyword extraction rose et al 2010 implemented through the rake nltk python package indicate that the above approach produces subjectively key phrases that were more useful for the purpose of this study see table 2 3 4 citation and trend analysis citation analysis indicates the papers being referred to by other papers within the corpora as well as the overall number of citations the given publication has received the assumption here being that impactful papers are more likely to be cited the number of citations is then used to indicate papers that are of high importance to the subject at hand both the total number of citations and the average citations since publication were used in the analysis publication trends within topic areas aided in identifying the general focus and direction taken by the research community plotted publication trends were used for this purpose 4 results ua sa packages of particular interest to this paper were the trends of software packages implementing ua sa methods and these are discussed here the final corpora was broadly categorized into two topics applications and frameworks using the topic model described in the method section publications focused on ua sa frameworks and guidelines were placed into the frameworks sub corpora while applications included those taken to be focused on the application of ua sa methods from each of these a keyword search was applied to identify publications related to model sensitivity optimization uncertainty quantification or toolboxes in order to build a sub corpora related to the software manually sorting the identified publications with the aid of the automated key phrase extraction tool reduced the corpora to 193 papers referred to as the software corpora see notebook 5c software packages analysis papers were regarded as relevant if they included direct reference to ua sa or optimization software packages were theory review or framework papers that recommended software implementation to a given field or referred to other methods and packages of interest to expert opinion further detail and a general bibliometric overview are provided in a later subsection there does not appear to be a strong correlation between the applications and software corpora fig 2 the software corpora has a stable publication trend relative to those focusing on applications over the surveyed timeframe a spike in publications in 2007 proportional to the full final corpora can be seen fig 3 while publications on the software for ua sa have been increasing see fig 4 the trend relative to the applications corpora and the full corpora could be indicative of 1 a general ambivalence towards reporting use or development of general ua sa software 2 a common set of ua sa software 3 a reliance on self coded analysis software or 4 increased tendency to release software in a directly citable manner e g with an attached doi which the wos database does not include but this is considered unlikely however in the authors opinion the slow uptake of software packages relative to the applications corpora could also be due to 1 a lack of documentation for beginner users and 2 a lack of awareness of available software packages in the first case beginner users may not use software that requires significant learning time for effective use especially when no clear user guide examples to draw from or community to engage with exists in the latter case modelers should be made aware of the available software that can reduce the time required to conduct ua sa and promote better practices in ua sa the software evident in the literature range from those specific to a field general purpose packages to custom made code fields such as hydrology climate chemistry and more general environmental modeling and engineering used field specific packages a complete list of reviewed software publications and their related software packages can be found in notebook 5a finding software packages by keyphrase extraction the most common analysis method provided by ua sa software was found to be sobol with the r sensitivity package providing the widest mix of methods see table 3 surveyed software tools typically did not provide oat analysis perhaps due to its simplicity or a sign of its decline publication of software related papers is relatively stable with a proportional spike in 2007 fig 3 software for the development of emulators did not feature heavily within the software corpora although they are present the hdmr method being one example described later on software to develop emulators include chaospy feinberg and langtangen 2015 the prism uncertainty quantification framework hunt et al 2015 gtapprox belyaev et al 2016 and uq pyl wang et al 2016 a collection of functions presented as a matlab toolbox is also introduced in vu bac et al 2016 all of these with the exception of vu bac et al 2016 were developed in the python programming language application of artificial neural networks and similar approaches did appear in the corpora but is not a topic of focus here as aforementioned current trends have shown an increased interest in best practices three sa packages released within the past five years reflect these changing attitudes psuade gan et al 2014 safe pianosi et al 2015 and vars tool razavi et al 2019 psuade a problem solving environment for uncertainty analysis and design exploration provides users with implementations of uq methods including sampling techniques and sa methods both local and global the package has had general application to various modeling scenarios safe sensitivity analysis for everybody provides users with implementations of global sa methods with the ability to perform multiple sas robustness assessment and convergence analysis without further model runs as reflected in its name this package was designed to allow global sa to be accessible to a more general audience the most recently released package in the survey vars tool provides implementations of sampling techniques and global sa methods including derivative variance and variogram based which can all be performed from a single sample the variogram approach to sa reportedly links both local and global approaches 4 1 survey of packages in common programming languages brief descriptions of software found in the corpora are provided here categorized by their implementation language some packages may be listed more than once as various implementations may exist or interoperability between languages is supported we decided to categorize the packages based on the implementation languages as most packages are not standalone tools with user interfaces ready to be used and are often provided as a library to be incorporated programmatically indicating the implementation language also allows readers to identify packages in a familiar language for potential adoption very few packages were found to provide a graphical user interface gui so some amount of programming ability and experience is the baseline expectation in the vast majority of cases users are expected to have a passing familiarity with the ua sa methods being applied as very little protection against improper use is provided a further brief discussion is in the recent developments section table 3 and 4 provide summary overviews of the software and packages 4 1 1 fortran fortran was one of the earliest programming languages available and arguably still dominates the scientific programming landscape fortran modules from the surveyed literature are jupiter api and ucode there is also a fortran repository of ua sa functions supported by the joint research centre pianosi et al 2015 the jupiter api joint universal parameter identification and evaluation of reliability application programming interface attempts to provide a standard set of programmatic functions for developing ua sa software and serves as the underlying engine for other ua sa packages ucode 2005 2014 were developed on top of this api the provided modules are developed in fortran 90 and support parallelization and local derivative sensitivity analysis jupiter api was first released in 2006 and its latest release was 2013 its affiliated webpage was last updated in 2016 suggesting an active community it is provided freely and under an open source license with a user manual and examples of applications first released in 1998 ucode universal inverse code was developed in fortran90 fortran95 and perl it originally implemented inverse modeling methods and by its first revision 2005 consisted of post processing modules for and not limited to sa calibration and ua the second revision 2014 included mcmc in the ua module and made the platform more compatible with models developed in matlab or using a gui this can be viewed as a response to changing trends in model development particularly the proliferation of matlab based models user documentation is available for download although the software is still available for download its development has ceased 4 1 2 c c surveyed software available in c c include dakota psuade pest and vars tool the dakota toolkit had its initial release in 1994 to provide optimization tools for engineers with further development it now includes sampling methods global sa methods parameter estimation and uq the software can be tightly semi or loosely coupled to the target model requiring the user in the first two cases to modify their code or use a direct interface the package is presented as being accessible to beginners and involves advanced features for more competent users it operates on linux windows and unix parallelization is possible and there is a gui option it is freely available for academic use and open source a user community exists including mailing lists and interaction with developers documentation includes user manuals examples and release notes dakota is well maintained its most recent release and webpage update being in 2018 it is an example of software that has kept up to date with the latest trends in ua sa and software implementation psuade problem solving environment for uncertainty analysis and design exploration can link to simulation code in any language it provides users with 14 sampling methods and 12 sa methods both local and global sa it was developed for large complex systems models and has been applied to various fields the software has a free public license and is open source a collaborative user community exists the software and documentation a user manual are available for web download the package is well maintained with its latest release and update in 2018 pest parameter estimation toolkit is designed primarily for model calibration originally released in 2003 and with its most recent release in 2019 it has remained up to date with the latest research in environmental modeling the current package provides parameter estimation and uncertainty analysis including monte carlo analysis and has parallelization capabilities the software is designed for complex environmental models and other models models written in c c fortran and python have interoperable interfaces available it is free although the license does not appear to be specified and well documented for ease of use developer user interaction is encouraged and training courses are offered 4 1 3 matlab identified packages of interest written in matlab are simlab mcat gui hdmr uqlab safe and vars tool simlab is a package for monte carlo based sa written in matlab and supplied by the joint research centre initially released in 1985 its latest release was 2008 and its associated webpage was last updated in 2016 it provides monte carlo and other random sampling methods test functions for educational purposes and gsa correlation regression and variance based the sa follows a loosely coupled approach requiring only the model output to be fed in it is freely available for academic use and open source no user community appears to exist the documentation consists of a reference manual and the software is available for web download mcat monte carlo analysis toolbox implements monte carlo sa its first release was 2001 and a companion paper highlighting the importance of best practices in sa was released in 2007 however no further research appears to have been conducted since this time and links to software download provided in the companion paper have expired this package is of interest as an example of software tooling designed to promote modeling sa best practices the software provides implementations of ua sa methods including regional sa monte carlo analysis and glue a gui was developed for it in 2007 the package is free and open source and documentation includes a manual and examples no user community appears to exist however there is an unofficial github page see table 4 gui hdmr graphical user interface high dimensional model representation provides hdmr a variance based sa method which the developers advertise as an alternative to other contemporary sa methods the user must supply an appropriate sample of the model output there is a complementary package rs hdmr random sampling hdmr for this purpose users have the choice of using a gui or a script based interface the software is reportedly user friendly and has been applied to various fields it is freely available for academic use but not open source the software and user documentation are available for web download although the related publication is highly cited this software appears to be abandoned having its first and last release in 2008 a lack of user community and implementation of a single sa method could be a cause for this uqlab uncertainty quantification laboratory provides among other tools for uq tools for statistical analysis such as sampling and global sa global sa methods are supplied through a linkage with the r sensitivity package parallelization is supported the package is user friendly and adaptable to various levels of computational experience collaboration amongst users is encouraged and users can contribute to code with revision by the major developers it is portable between operating systems and freely available for academic use however documentation is not freely available the software is well maintained with its latest release and update in 2018 safe sensitivity analysis for everyone is compatible with the gnu octave environment and a version implemented in r exists making it the most openly accessible of all the surveyed matlab packages it runs on any operating system the toolbox was designed to make global sa accessible to users with limited knowledge of global sa or matlab whilst also allowing more advanced users to explore research and better understand sa users are provided with various sampling methods local and global sa methods and a gui see table 3 although there appears to be no collaborative user community user developer interaction is possible via email the software is freely available for academic use and is open source documentation includes the companion paper pianosi et al 2015 and additional information provided in workflow scripts there have been no recent releases however the website is maintained last update 2018 vars tool is also available in c and ostrich a user independent interface it features off line and on line mode options for running models in any language or operating system numerous sampling and sa methods are supplied including vars it is said to be user friendly and accessible to various levels it appears to operate as a command line interface without a gui although recently developed there is no collaborative community the software is freely available for non commercial use and is open source there are capacities for parallelization and reporting and visualization tools its documentation consists of a manual 4 1 4 r statistical language the main sa package for the r language is the r sensitivity package like python the r language is widely used in the sciences and so many of the tooling support interoperability with r see the section on python below and table 4 the r sensitivity package supplies various sa and sampling methods it offers loose coupling with models implemented in other languages as well as in r test cases are supplied for research and comparison purposes the package requires knowledge of r which itself is portable between operating systems and freely available a developer community exists and the available documentation consists of a reference manual since its initial release in 2006 more recently developed methods have been implemented and included in its latest release in 2018 4 1 5 python as with r users of python have a large assortment of options generally due to python being a general purpose language often used for interoperability across languages see table 4 the principal sa package developed in python appears to be salib sensitivity analysis library which provides global sampling and analysis methods and is distributed under a free public license model runs can be invoked directly or separately offline salib is most applicable to systems modeling and knowledge of python is assumed it is a freely available open source package with a collaborative user community salib is well documented and well maintained documentation includes an installation guide basic usage guide a complete module reference and release notes its latest release was 2018 salib supports visualization of morris results only although this feature appears to be under documented a separate visualization tool is available for analysis of sobol results called savvy hough et al 2016 however this package was not examined in depth 4 1 6 java there appears to be limited sa packages implemented in java at least in the reviewed corpora a response to this limitation is the mouse model optimization uncertainty and sensitivity analysis package this is an implementation of mcat and optas model calibration software for modelers using java it is indicative of the continued influence of the packages mcat and optas its first release was in 2014 and was last updated in 2016 although claiming to be free and open source we could not find relevant information to access the package 4 1 7 julia mads model analysis decision support is an sa package available for the julia programming language the analyses it supports can be tightly or loosely coupled with an existing model in the module documentation extensive information is provided for all functions included in the main module mads jl the documentation details modules and examples and although extensive was found not to be user friendly with functions and methods often lacking meaningful descriptions mads is said to support use in high performance computing hpc environments it is a freely available open source package with a collaborative user community an inherent advantage of mads is the relative youth of the julia language with v1 0 released in 2018 due to its relative youth it leverages lessons learnt in older programming languages and was developed with modern computational architecture in mind this means that concurrent and parallel programs are relatively easy to develop in julia bezanson et al 2017 and it has had demonstrable success on hpc platforms see for example regier et al 2019 the disadvantage of this youth however is that the user community while growing quickly is still relatively small compared to that of established languages as such the language ecosystem is undergoing continual development and may still be immature 4 2 active use and development to gauge the level of support and active development occurring for each software tool we attempted to identify websites evidence of userbases public code repositories journal publications which specifically mention the software tool and other indications of activity through this process we found that many of the packages present in the literature are no longer under active development although the code and software may still be available for use a key issue in developing software for ua sa is longevity we find that those packages that are currently used and under active development and maintenance have the advantages of being open source well documented for transparency and ease of use have an active user community and offer implementations of a range of ua sa methods for general purpose application as opposed to providing a specific method for a specific model packages that have fallen into disuse may still be useful with the caveat that there is no supportive community to rely on for bug fixes troubleshooting user support and so on table 3 provides an overview of the available ua sa methods in the surveyed packages while details of the software can be found in table 4 4 4 bibliometric overview the initial corpora from wos consisted of 11 718 publications from which journals deemed to be unrelated to the topic areas of interest as specified by the search terms used journals with less than three identified publications and those without a valid doi were removed the final corpora consisted of 11 625 publications knowing that researchers build on prior work and given the exponential growth of published material bornmann and mutz 2015 haddaway and westgate 2018 we assume in this analysis that the identified corpora is representative of the ua sa field full details of this process can be found in notebook 2 create filtered corpora the number of publications in the environmental ua sa field have been increasing at an exponential rate depicted in fig 5 with journal of hydrology having the most publications overall and experiencing the largest year on year gain within the analyzed time frame fig 6 to facilitate analysis the final corpora was broadly categorized into two topic sub corpora applications and frameworks using the topic model as a reminder the final corpora represents a collection of ua sa research publications focused on ua sa frameworks and guidelines were placed into the frameworks sub corpora while applications included those taken to be focused on the application of ua sa methods the topic model was iteratively applied and key phrases from top cited papers were qualitatively examined to determine the focus of the publications the specifics of the undertaken process can be seen in notebook 4 uasa topic modeling a keyword search was applied within these topic corpora to sort publications further into those relevant to uncertainty quantification uq ua and sa the resulting collections contained 1 940 2 751 and 1 360 publications respectively to distinguish between lsa and gsa methods specific keywords were searched for in the combined corpora including for example local sensitivity oat one at a time for local methods and global sensitivity and gsa to indicate global methods in addition to these newer sa methods identified through manual inspection of the corpora were also searched for such as active subspaces and variograms 4 4 1 trends and directions as suggested by the general publication trends in fig 7 all topics ua sa frameworks and applications saw large increases in the absolute number of publications over the 2000 2017 timeframe within the same time period the proportional share of the filtered corpora has declined for sa by 4 5 while ua has increased by 5 which may indicate a gradual shift towards being more inclusive of uncertainty related matters in analyses as well as a general need for uncertainty guidelines in environmental modeling see notebook 4 uasa topic modelling the five most active journals in the frameworks sub corpora were structural and multidisciplinary optimization journal of computational physics environmental modeling software and journal of hydrology see fig 8 the 10 most cited papers from across these top five journals came from environmental modeling software 2 structural and multidisciplinary optimization 3 journal of hydrology 2 journal of computational physics 1 and computer methods in applied mechanics and engineering 2 and are detailed in table 5 under supplementary material the top cited framework related papers from these journals table 7 showcase a range of issues but particularly address the lack of uniformity in the ua sa approaches used in their respective fields these fields include environmental modeling evaluating performance bennett et al 2013 improving confidence in model outcomes and handling uncertainty bennett et al 2013 kuczera et al 2006 refsgaard et al 2007 ua for hydrological swat models yang et al 2008 optimization topology optimization sigmund and maute 2013 finite element methods blatman and sudret 2011 moens and vandepitte 2005 level set methods for structural topology optimization van dijk et al 2013 high dimensional computationally expensive black box problems shan and wang 2010 and scientific computing handling uncertainty roy and oberkampf 2011 outlines of procedures guidelines comparisons of methods and suggestions for future research resolve the issues raised in these papers these papers are highly cited indicating that they have had an impact on the research community at least within their respective fields it should be noted here that existence of highly cited papers itself does not indicate widespread application of suggested good or best practice and should not be taken as evidence the review conducted by saltelli et al 2019 concludes that there is a worrying lack of standards and good practices although it is acknowledged that the review focuses on older papers and may not capture recent trends certainly awareness appears to have increased if not adoption of practices similarly a keyword search for best practices identified 132 papers across the surveyed period by best practices we refer to practices in modeling and uncertainty management that promote transparency and reliability of results the high citation counts of papers relating to frameworks table 7 and the growth in best practices publications in absolute terms fig 9 suggest increasing interest in uncertainty management particularly improving the reliability and effectiveness of ua sa whether the modelers take up the suggestions in these papers is yet to be seen modelers can be encouraged to follow guidelines for reliable and effective treatment of ua sa if the available software implementing ua sa is designed in accordance with these guidelines and if modelers make use of such software 4 4 2 recent developments recent impactful publications in sensitivity analysis suggest a shift away from local sensitivity methods prior to 2010 one factor at a time oat local sa was the most prevalent practice in the literature saltelli and annoni 2010 with a later revisit indicating that while this was still the case for papers published in science and nature gsa methods were gaining traction ferretti et al 2016 a more recent bibliometric review conducted by saltelli et al 2019 comes to a similar conclusion across 19 subject areas in which modeling features heavily although the growth of oat related publications is shown to significantly out pace gsa related publications within the presented corpora publications with oat related keywords do decrease slightly over the past two decades down roughly 1 compared to the entire corpora with an uptick in the absolute number of publications post 2010 see fig 10 although oat is said to be a common method see for example shin et al 2013 it may not have featured heavily prior to 2010 due to 1 researchers not reporting oat use 2 modelers using custom implementations of oat and 3 the software surveyed in our analysis did not support oat which discourages modelers from using this method i e they select from available methods analysis conducted here indicates an increase in reported gsa keywords post 2010 fig 11 after the publication of how to avoid a perfunctory sensitivity analysis saltelli and annoni 2010 this paper was identified as a highly cited publication in the initial corpora table 6 in the supplementary material a key contribution being the demonstrated inefficacy of oat analyses using a geometric proof the uptick in publications with the oat related keywords appears to correlate with the number of papers citing the paper by saltelli and annoni 2010 shown in fig 12 this may contribute to the rise in publications with oat related keywords in the corpora and as identified by saltelli et al 2019 the detected increase in gsa papers may reflect the start of changing attitudes towards sa in recognition of the importance of global sensitivity analyses increased awareness in the past decade has led to the use and development of more efficient and comprehensive ua sa techniques and approaches improved approaches put forth in the past decade attempt to enhance the computational efficiency of generating a global sensitivity measure or range of measures as the case may be from a single sample set itself said to be more representative of the possible parameter space e g razavi et al 2019 in particular there has been a renewed interest in gsa based on statistical design of experiment approaches as these methods are capable of producing global sensitivity measures at an acceptable computational cost gan et al 2014 saltelli 2017 such approaches refer to methods that utilize a deterministic sample set for example the aforementioned sobol latin hypercube and morris methods saltelli 2017 despite the increased interest in gsa evidenced by the bibliometric analysis local sa and oat methods are still in widespread use if any sa is conducted at all shin et al 2013 for example found that only 7 11 of 164 of papers surveyed conducted any sa of which five applied oat it is difficult to ascertain the full extent of oat analysis through keyword analysis as researchers applying this technique may not make explicit reference to this form of analysis possible reasons for the relatively slow uptake of gsa methods are listed in ferretti et al 2016 including perceived complexity in the application of gsa modelers were characterized as being hesitant due to a lack of experience with gsa methods we also find in the literature a prevalence of self implemented ua sa that is modelers using their own code in place of existing and often open source software tools not using or otherwise contributing to readily available widely used and well tested software represents a duplication of work this can be somewhat alleviated by greater awareness of and access to the available software tools that simplify the application and use of such analyses those developing tools and methods for their part could strive to improve ease of use and lower the technical and conceptual barriers to uptake of their software pianosi et al 2016 outline three principles of good practice for a sensitivity analysis package 1 the ability to apply multiple sensitivity analyses to one sample 2 provision of tools to assess and revise user choices and 3 inclusion of visualization tools regarding point 1 early software releases tended to be platform method or model specific see table 4 for specific examples in recent years the available software has been made for more general purpose use offering a more comprehensive approach to ua sa with multiple methods supported the lack of collaborative development is also reportedly an issue with researchers preferring to develop their own toolset and as a consequence siloing advances at least in the short to medium term usability especially for novices is an ongoing concern while many sampling and analysis approaches are amenable to cross use e g a mix and match approach there is often no limitation in the application of methods within the packages which safeguards a user against inappropriate and incompatible mixes e g sobol analysis on a latin hypercube sample efforts to address these issues and criticisms are evident in the various communities however with later packages often offering detailed documentation including usage examples and tutorials see previous section well known test functions such as the ishigami function ishigami and homma 1990 sobol g function saltelli and sobol 1995 the example lake problem hadka et al 2015 as well as case studies for research and educational purposes are often included pest doherty 2018 for example provides a ua tutorial including two worked examples of hydrological models another example is safe pianosi et al 2015 which by providing commented code in workflow scripts allows beginner users to implement ua sa more easily and advanced users to improve their methodology the packages in our survey did not appear to provide guidance through ua sa theory outside of extensive reading lists although it is acknowledged that this may be out of scope for those maintaining the tool a lack of guidance as such may hinder uptake by practitioners of both the software and gsa in general generally the available packages still operate on an understanding that users have appropriate background knowledge of or experience with ua sa many of the packages identified in this review appear to have been abandoned this perhaps indicates the importance of an active user community to share knowledge and update code the corpora give evidence to newer methods that are in development and reflects a continued interest in improving ua sa examples of more recently developed techniques are vars and active subspaces developed in 2016 and 2011 respectively vars variogram analysis of response surfaces uses variograms as a measure of sensitivity a variogram is a function describing the spatial dependence of in the case of sa the parameter space that is how much the variance in parameter values is dependent on the distance between parameters in parameter space variogram related publications began in 2001 however those specifically relating to the application of variograms to sa only appear from 2016 there are five publications relevant to variogram based sa in the corpora totaling 74 citations and the highest citation average is 13 razavi and gupta 2016 the top cited variogram paper razavi and gupta 2016 presents the method as a linkage between existing derivative and variance based gsa methods and demonstrates that the approach reduces computational cost over approximately 20 000 and 100 000 model runs the vars sensitivity estimates had less uncertainty than sobol and morris indices these relatively new methods though currently lacking citations do appear to be methods with development potential due to for example current user interest and improvements to the efficiency and comprehensibility of ua sa methods active subspaces is a dimension reduction technique that identifies directions in parameter space that have a greater influence on the model output these directions are described as being active and their identification aids in reducing the dimensionality of a model by avoiding perturbations across inactive areas of parameter space thereby reducing computational cost constantine et al 2015 through this method parameters of importance and their rankings can be obtained jefferson et al 2015 papers relating to active subspaces first appear in 2015 there are eight in total in the corpora citation analysis does not indicate particularly that this new method is being taken up quickly total citations for all publications was 83 and the publication with the highest citation average had an average of 7 67 constantine et al 2015 the top cited active subspaces paper constantine et al 2015 details an application of the method to numerical simulation and an implementation may be found in the effective quadratures package for python seshadri and parks 2017 another technique of interest is hdmr high dimensional model reduction the companion paper ziehn and tomlin 2009 for the method and supporting software came through as a highly cited publication in this analysis see table 7 hdmr is an emulation method that improves variance based sa methods such as the sobol method citing articles for ziehn and tomlin 2009 continue up to 2019 identified through manual processes in fact 7 of the 32 returned publications in the corpora were published in 2017 indicating a continued interest in the method in the corpora the publication with the most citations has 158 alış and rabitz 2001 and the highest citation average is 14 ziehn and tomlin 2009 furthermore alternative methods for handling uncertainty have been developed especially to handle scenarios in which there is large uncertainty but in which accurate predictions are necessary for future policy making software for these alternate methods is deemed out of scope for this study but for completeness sake one such proposed approach is exploratory modeling and analysis rather than simply minimizing uncertainty in an attempt to produce an accurate or precise prediction uncertainty is treated as inevitable decision making processes are guided through the exploration of possible outcomes generated through computational experiments and responses planned eker et al 2018 kwakkel and pruyt 2013 5 limitations the bibliometric analysis presented here is limited by the scope of the wos database the specific search terms used the initial time frame and the included fields of study with the analysis focused on applications in environmental modeling search query results may also differ over time due to indexing artefacts with implications for the resulting trend and citation analysis a bias towards open source software literature may be perceived as these were the easiest to analyze that said it is not claimed that the analysis conducted herein uncovered all software packages currently in use or the full extent to which they are being used a known issue is the lack of attributions citations and reporting of software used for research making it difficult to find their mention especially when the analysis relied on abstract text other software may not be referenced simply because their use is taken to be a fundamental part of the programming language ecosystem for example the r sensitivity package or sci kit learn for python it was also difficult to search within the corpora for packages with names common to other applications taking as a particularly difficult example the r sensitivity package in our own process of sorting the generated database decisions whilst manually sorting and choosing the software collection papers were subject to inherent bias although this process was kept as transparent and objective as possible see notebook 5a finding software packages by keyphrase extraction another process which limited the generality of our findings was that of refining the search terms and results limiting the scope of the results was necessary to facilitate analysis of the most relevant publications iterative use of the topic model achieved this however it is entirely possible that relevant publications will have been removed fig 13 of particular note is the possible under representation of articles on emulators and surrogate modeling within the software corpora omitted publications were assumed to be irrelevant or that relevant issues were captured by the papers that remained in the desired corpora for more information see notebook 4 uasa topic modeling 6 conclusion and future directions the analysis presented here indicates that ua considerations are increasingly included in the published literature with a slight decrease in the reported use of oat methods the identified literature reflects greater attention paid to guidelines for the use of ua sa over the past decade itself perhaps indicating advances in the application of ua sa greater interest in the use of ua sa for rigorous model testing is apparent although whether modelers embrace and adopt the suggested guidelines towards the treatment assessment and analysis of ua sa e g as discussed in eker et al 2018 saltelli et al 2019 remains to be seen the literature also suggests that a wide variety of software has become available in the past two decades aimed at both non programmatic audiences and for specific programming languages the majority of these identified software packages does not support local oat analyses which may indicate a general move away from depending on local sa more recently developed software packages that implement multiple methods with open source code and documentation with little restriction in terms of software licencing to the end user are becoming the prevalent distribution format while there is a variety of software tools available the trend of publications on ua sa tooling has remained largely flat this trend may be due to the relative infancy of the available tools or a perceived complexity in their application for one while many of the surveyed software provide usage examples and documentation their use typically assumes 1 experience with the underlying programming language or 2 intimate familiarity with the methods provided their pros and cons and contextual suitability little guidance is available aside from extensive reading lists the indicated lack of uptake in this analysis may also be because software specific publications have been largely filtered out from the corpora these relevant publications may be concentrated within conference proceedings which were removed from the corpora or other topic areas not included in the initial publication search publications that are application method focused may not explicitly mention the software used in the abstract for these reasons it is difficult to concretely conclude whether those involved in environmental modeling are embracing the available ua sa software tools or if custom home grown solutions are preferred itself indicating perhaps a lack of awareness of the available software packages that said usability and user friendliness were found to be a general issue users are expected to be adept and experienced enough to produce and interpret results themselves even in cases where visualization processes are provided users may require a different approach for their analyses in the case of novices interpreting provided analyses requires first understanding a body of work usually provided in the form of a often large reading list of relevant papers this may explain in part a preference for custom home grown solutions where the developers write tools specific to their needs to avoid adoption cost time needed to learn how to use an existing tool effectively the complexity of existing tools real or perceived may contribute to the issue of lack of uptake in cases where the perceived cost of adoption is high the prospective user may find it easier to apply oat or otherwise implement their own custom solution to perform common ua sa methods which amounts to duplication of effort across the scientific community this then raises the question of what constitutes a thorough ua sa package in this survey the most comprehensive software r sensitivity simlab and salib provide users with the widest assortment of ua sa methods with limited visualization capability and test functions these target languages prevalent in the sciences r matlab and python respectively that are supported by an active community which may explain their longevity and or popularity the prevalence of open source community led efforts evident in more recent software tools suggests that an open development culture is a prerequisite to widespread adoption perhaps an unremarkable observation due to the scientific context and focus of ua sa research developers and maintainers of ua sa tools could support and encourage wider application of gsa processes by moving towards 1 an open development process 2 placing further attention on expanding documentation preferably in an easily digestible form and 3 improving usage guidelines and promoting user centric interfaces and workflows point 1 is to encourage the sharing of knowledge and experience across the disciplines that rely on modeling to leverage expertise and experience globally rather than siloing advances on point 2 ua sa software developers could further leverage the open collaboration model and re use explanations and examples from one another examples of both simple and complex workflows could be given e g in a cookbook or recipe documentation style point 3 should not be taken to mean that all packages should provide a gui rather general purpose ua sa tools should have processes in place to prevent or limit unintentional or ill informed analyses from occurring a particular pain point is the ability to mix and match sampling and analysis methods regardless of whether it makes sense to do so while ua sa tools have largely addressed the three steps defined by pianosi et al 2015 sample parameter space run model analyze results the workflow that is implicit or explicit steps in the use and application of the software could be improved so that modelers are able to move from each step without issue recently developed packages indicate that such improvements to the workflow are being made with attention to usability open source code and tools for analyzing results researchers and modelers particularly those new to ua sa need software designed with usability in mind it is expected that such software will support ua sa in more areas and encourage rigorous and reliable ua sa which will in turn allow for more informed decision making software availability code and representative data used for this analysis can be found at https github com frog7 uasa trends 10 5281 zenodo 3406946 software used to support analysis can be found at https github com connectedsystems wosis 10 5281 zenodo 3406947 declaration of competing interest the second author has contributed usability and performance improvements to the salib python library all other authors declare no potential sources of conflict acknowledgements the corresponding author second in the author list is supported through an australian government research training program agrtp scholarship and a top up scholarship from the hilda john endowment fund the authors would like to thank and acknowledge joseph guillaume barbara robson and the anonymous reviewers for their highly valued comments and suggestions the use of work by titipat achakulvisut author of the wos parser python package https github com titipata wos parser and enrico bacis author of the wos client package https github com enricobacis wos is acknowledged we would also thank and acknowledge clarivate analytics for providing access and use of the web of science database and to their staff for providing clarifications to technical details of the available api without which this work would not have been possible appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104588 
26079,uncertainty and sensitivity analysis ua sa aid in assessing whether model complexity is warranted and under what conditions to support these analyses a variety of software tools have been developed to provide ua sa methods and approaches in a more accessible manner this paper applies a hybrid bibliometric approach using 11 625 publications sourced from the web of science database to identify software packages for ua sa used within the environmental sciences and to synthesize evidence of general research trends and directions use of local sensitivity approaches was determined to be prevalent although adoption of global sensitivity analysis approaches is increasing we find that interest in uncertainty management is also increasing particularly in improving the reliability and effectiveness of ua sa although available software is typically open source and freely available uptake of software tools is apparently slow or their use is otherwise under reported longevity is also an issue with many of the identified software appearing to be unmaintained improving the general usability and accessibility of ua sa tools may help to increase software longevity and the awareness and adoption of purpose appropriate methods usability should be improved so as to lower the cost of adoption of incorporating the software in the modelling workflow an overview of available software is provided to aid modelers in choosing an appropriate software tool for their purposes code and representative data used for this analysis can be found at https github com frog7 uasa trends 10 5281 zenodo 3406946 keywords uncertainty analysis sensitivity analysis analysis software packages bibliometric trends environmental modeling 1 introduction computational modeling has become a key activity in many areas of research in the environmental sciences the amount of available computational power and speed has led to the development of environmental models with ever increasing level of detail and complexity in this context complexity is reflected by the number of parameters a model incorporates as inputs these parameters may also be referred to as parameter factors factors or simply inputs in the literature norton 2015 increasing the number of parameters allows for a more detailed representation of the investigated system while also increasing computational cost and model complexity at an exponential rate increased detail and thus complexity may reduce the identifiability of parameters the ability to apportion model results to specific parameter values but is not always justified or necessary with respect to the aims of the modeling exercise increased complexity has led modelers to better appreciate the issue of model identifiability guillaume et al 2019 and to recognize the importance of understanding the contribution of model inputs with respect to model performance and purpose uncertainty and sensitivity analysis ua sa refer to the methods and approaches used to help researchers better understand the relative importance of each parameter factor within a given problem context put simply s ensitivity analysis assesses how variations in input parameters model parameters or boundary conditions affect the model output bennett et al 2013 with these approaches it is possible to better understand how sensitive model results are to parameter factors and how uncertain the model results are saltelli et al 2019 saltelli and annoni 2010 individual parameter factors may influence one or more outputs and could conditionally affect the importance of other factors referred to as parameter interaction the practice of analyzing uncertainty and sensitivity is now considered standard modeling practice the interested reader is directed to bennett et al 2013 norton 2015 pianosi et al 2016 razavi and gupta 2015 for introductory overviews and further information understanding the relative sensitivity of parameters can aid in the development of better monitoring strategies and experiment design for example indicating the priority and amount of data to be collected saltelli and tarantola 2002 the practice of sa can also help to constrain the parameter space by identifying parameters that may be insensitive or inactive having little to no effect on model results at least for the purpose of the modeling identifying such parameters can help constrain model complexity which in turn eases the computational cost of model evaluations for example to facilitate uncertainty analysis and the development of surrogate models in recent years a wide variety of software tools to support ua sa processes have become available that make such analyses more accessible to modelers to gain an overview of the available methods and tools we applied a hybrid bibliometric approach using publications from the web of science database while reviews of sensitivity analysis practice have been published see for example ferretti et al 2016 saltelli et al 2019 and comparisons between ua sa methods conducted for example gan et al 2014 sun et al 2012 to our knowledge there does not appear to be an overview of the available ua sa tools currently in use across different platforms and programming languages this paper follows on from and is distinguished from existing reviews such as matott et al 2009 refsgaard et al 2007 as it surveys ua sa in environmental modeling with a specific focus on sa we then provide information on the available tools as revealed through the bibliometric analysis and expert knowledge including implemented ua sa methods programming language and software features the aim here is then to provide 1 a brief introduction to the field of ua sa and its relevance to environmental modeling for those new to the field 2 an overview of ua sa research trends and 3 a guide to the development trends of ua sa tools their availability and relevance 2 key ua sa terminologies and methods often the first hurdle for those new to a research area is to grasp the multitude of acronyms and terms used in this section we briefly outline some common terminology ua sa methods and relevant publications for further reference these are provided here to contextualize the analysis and discussion later in this paper the information provided in this section is not exhaustive interested readers are directed to norton 2015 for a more thorough introduction to ua sa the descriptions of sensitivity analysis methods in pianosi et al 2016 the citations in table 1 and the citations in bennett et al 2013 p 3 pianosi et al 2015 identify three stages in a sensitivity analysis selecting a sample of input values from the variability space running a model evaluation against these input values and applying a sensitivity analysis method to the input output samples to compute sensitivity indices i e values which indicate each parameter s sensitivity for more information about the calculations for various sensitivity indices see norton 2015 here the variability space refers to all possible combinations of values that can be assigned to a model s input parameter set by running the model with the values sampled from the variability space and taking note of the resultant outputs analyses can be conducted to calculate the influence that a specific input or set of inputs may have i e their sensitivities the focus of this paper is on providing an overview of tools that aid in conducting these analyses methods to select the sample of input values are often characterized as being either local or global global methods gsa consider all dimensions of a model in one grand exercise leamer 1985 achieved by varying all parameter values at the same time gsa methods are themselves commonly categorized as being statistical derivative or variance based statistical methods use statistical analysis of the parameter space as a measure of sensitivity pianosi et al 2016 derivative based methods provide indices which characterize the distributional properties of partial derivatives razavi et al 2019 variance based approaches determine how different factors contribute to model variance by analyzing and decomposing the variance in model outputs razavi et al 2019 for brevity a full exploration of these methods is not provided here but a brief overview with references to relevant papers is given in table 1 the strength of gsa methods is that they provide a more robust depiction of model uncertainty by comprehensively accounting for parameter interactions saltelli and annoni 2010 such approaches assume a random distribution of output values in the parameter space and that such a distribution is plausible gsa methods can also be computationally expensive to perform as the parameter space being explored can be very large sampling methods schemes are used to aid in limiting the number of model runs involved whilst adequately representing the parameter space the computational cost of applying gsa methods may explain at least in part why their use is relatively uncommon compared to their local counterparts local sa methods lsa are anchored around a particular point in the parameter space with analysis involving comparisons against a known baseline output razavi and gupta 2015 the simplest most naïve and most common method of sa is one at a time oat as the name suggests this approach involves changing the value of a single parameter factor at a time referred to as perturbing whilst keeping all other parameters constant at their nominal values this approach could be described as taking samples along a single dimension with the changes to the output then attributed to the factor that was modified there are different approaches to how much the parameter value is perturbed but often a proportional increment is used e g increase or decrease a parameter by 10 of the nominal value up to and including a given bound razavi and gupta 2015 other lsa methods examine the partial derivatives of output with respect to each input parameter these are computed at one point in the sample space to determine sensitivity indices the simplicity of the procedure is advantageous as well as being computationally inexpensive for first order derivatives as they often do not require a formal sampling approach monte carlo mc a simple random sampling is commonly used although it offers a limited representation of the total parameter space gan et al 2014 the downside is that lsa only provides a robust indication of sensitivity for linear or additive models saltelli and annoni 2010 they do not account for parameter interactions and become computationally expensive when higher order and non linear effects are considered to resolve this issue several other sampling approaches have been developed and applied given each method and approach have their pros and cons multiple methods could be applied to obtain complementary results à la ensemble analysis sagi and rokach 2018 and should be considered where appropriate sun et al 2012 brief descriptions of commonly employed methods are given in table 1 methods are taken to be common where they are indicated to be so in recent review papers specifically gan et al 2014 pianosi et al 2016 the references found within these and those found within the identified corpora detailed in the next section 3 method the hybrid bibliometric approach to conduct this bibliometric review a collection of publications the corpora was gathered from clarivate analytics web of science wos database using the available web based application programming interface api use of the api enabled programmatic access to the publication data and metadata including titles abstract text author supplied keywords and dois data was retrieved with the use of wosis web of science analysis a python package developed to simplify the process of querying the wos database and aid in data analysis and visualization iwanaga and douglas smith 2019 publications in the resulting corpora were taken to represent the field of uncertainty and sensitivity analysis in the overarching field of environmental modeling to ensure as much transparency as possible much of the data collection and subsequent analysis was conducted programmatically in the python programming language the complete dataset cannot be made available as it is subject to clarivate analytics license terms representative datasets are provided instead along with the code developed for the analysis these can be viewed as a collection of jupyter notebooks and associated files at https github com frog7 uasa trends douglas smith and iwanaga 2019 names of specific notebooks will be referred to throughout this text where further detail can be found the corpora was iteratively and incrementally refined through a semi autonomous process of topic identification keyword search and subsequent manual analysis of the publications with the aid of key phrase extraction topic modeling briefly described in section 3 2 was used to aid in identifying a collection of papers relevant to uncertainty and sensitivity analysis and their overarching focus be it an application of or guiding frameworks for ua sa the publication and citation trends within these topic areas were then analyzed additional topic modeling complemented by a keyword search process was used to identify papers related to the use of ua sa software these were manually combed through with the aid of an automated key phrase identifier that helped to reduce the amount of text to be examined a subset of these papers were investigated for mention of software tools and packages the general search and analysis approach is depicted in fig 1 with further detail on topic modeling and key phrase identification provided within this section 3 1 initial search the initial corpora for the analysis was identified by specifying the search phrase with search fields bolded ts sensitivity analysis or uncertainty analysis or uncertainty quantification or uncertainty propagation or local sensitivity analysis or lsa or one at a time or exploratory modeling or oat or global sensitivity analysis or gsa or all at a time or aat and wc environmental sciences or water resources or engineering environmental or interdisciplinary applications this returns publications that use at least one of the specified terms those listed for the ts field within the title abstract or author supplied keywords for publications in the the wos defined subject areas specified for the wc field the raw search string is supplied for transparency and can be used to obtain the corpora from wos only english language publications between 2000 and 2017 were considered for this study with the ending year selected as the data request occurred in december of 2018 the approach taken at the time was to include full year datasets only the final search phrase applied with the specified time frame reduced the number of matches from over 500 000 to 11 718 publications the number of results obtained through the unrestricted search were far too many to comprehensively review at least in a timely manner the initial corpora for this study of 11 718 publications were then further constrained through the process depicted in fig 1 and is described in more detail below 3 2 topic identification a key focus in this study is the software tools and packages available to support ua sa processes the methods they implement and the trends of these to this end topic modeling was applied to constrain the corpora to relevant publications for further consideration topic models attempt to cluster texts into similar or related topics based on commonly occurring words and can aid in identifying new and emerging fields whilst also reducing the likelihood of bias and the required hours for a systematic review achakulvisut et al 2016 westgate et al 2018 topic modeling has been applied before to reduce the time and difficulties encountered when conducting systematic reviews westgate and lindenmayer 2017 however their use is still relatively limited and perhaps underutilized although software is available to aid in these bibliometric approaches currently no single software package provides all necessary functionality to conduct end to end systematic mapping the classification of articles based on their contents of research literature from data collection through to summarization and visualization arguably the conjunctive application of systematic mapping and bibliometric analysis is still in its infancy as evidenced by nakagawa et al 2018 topics are identified by the common co occurrence of semantics within a discipline for example sensitivity in the context of sa would conceptually be expected to appear in texts containing words such as analysis uncertainty and modeling the term sensitivity may also appear in relation to physical psychological response to stimuli in which case the term will appear alongside terms associated with the medical and therapy fields topics can be identified and represented through their common semantics the topic modeling approach provided within wosis non negative matrix factorization nmf is implemented with the scikit learn python package pedregosa et al 2011 the approach allows publications to be assigned to one or more topics arora et al 2012 and has been shown to be appropriate for collections of short texts chen et al 2019 this process was complemented with a traditional keyword search to help identify publications related to specific subjects tokens meaning specific words or terms for topic modeling consisted of the text found in the document titles abstracts and keywords the top 1000 tokens found within the corpora based on term frequency inverse document frequency tf idf rankings were selected for topic modeling tf idf is a common ranking method used in text mining beel et al 2016 a high tf idf score indicates that the word token has a high frequency within specific document s but a low number of occurrences within the entire corpora weighting the score in such a manner has the effect of filtering out commonly used tokens which may not have high semantic importance 3 3 key phrase identification once a topic area is identified the resulting sub corpora can be further constrained through automated key phrase identification the approach summarizes text aiding reviewers to identify irrelevant publications by reducing the amount of text for manual review the implemented approach attempts to identify these phrases of interest by scoring sentences based on their similarity with other sentences throughout the abstract text to elaborate each sentence s i is compared with other sentences in the abstract s y which are initially filtered based on the presence of a root token which is taken to be the token that appears in the middle of s i this root token selection approach is used in rabby et al 2018 for its simplicity and computational efficiency the similarity between s i and s y is then scored based on the ratio of the intersection of the two sentences sentences with three or less tokens i e words numbers or other counted by splitting the text on individual spaces are ignored the approach assumes that important features of the publication such as its key findings will be repeated throughout the considered fields title abstract and author supplied keywords these may for example be introduced or alluded to framed and the implications discussed the implemented approach is therefore dependent on the abstract length with longer texts preferred poor performance can be expected for very short abstracts e g 3 sentences or less and these were ignored for the purpose of this study comparisons with an established key phrase identification approach rake rapid automatic keyword extraction rose et al 2010 implemented through the rake nltk python package indicate that the above approach produces subjectively key phrases that were more useful for the purpose of this study see table 2 3 4 citation and trend analysis citation analysis indicates the papers being referred to by other papers within the corpora as well as the overall number of citations the given publication has received the assumption here being that impactful papers are more likely to be cited the number of citations is then used to indicate papers that are of high importance to the subject at hand both the total number of citations and the average citations since publication were used in the analysis publication trends within topic areas aided in identifying the general focus and direction taken by the research community plotted publication trends were used for this purpose 4 results ua sa packages of particular interest to this paper were the trends of software packages implementing ua sa methods and these are discussed here the final corpora was broadly categorized into two topics applications and frameworks using the topic model described in the method section publications focused on ua sa frameworks and guidelines were placed into the frameworks sub corpora while applications included those taken to be focused on the application of ua sa methods from each of these a keyword search was applied to identify publications related to model sensitivity optimization uncertainty quantification or toolboxes in order to build a sub corpora related to the software manually sorting the identified publications with the aid of the automated key phrase extraction tool reduced the corpora to 193 papers referred to as the software corpora see notebook 5c software packages analysis papers were regarded as relevant if they included direct reference to ua sa or optimization software packages were theory review or framework papers that recommended software implementation to a given field or referred to other methods and packages of interest to expert opinion further detail and a general bibliometric overview are provided in a later subsection there does not appear to be a strong correlation between the applications and software corpora fig 2 the software corpora has a stable publication trend relative to those focusing on applications over the surveyed timeframe a spike in publications in 2007 proportional to the full final corpora can be seen fig 3 while publications on the software for ua sa have been increasing see fig 4 the trend relative to the applications corpora and the full corpora could be indicative of 1 a general ambivalence towards reporting use or development of general ua sa software 2 a common set of ua sa software 3 a reliance on self coded analysis software or 4 increased tendency to release software in a directly citable manner e g with an attached doi which the wos database does not include but this is considered unlikely however in the authors opinion the slow uptake of software packages relative to the applications corpora could also be due to 1 a lack of documentation for beginner users and 2 a lack of awareness of available software packages in the first case beginner users may not use software that requires significant learning time for effective use especially when no clear user guide examples to draw from or community to engage with exists in the latter case modelers should be made aware of the available software that can reduce the time required to conduct ua sa and promote better practices in ua sa the software evident in the literature range from those specific to a field general purpose packages to custom made code fields such as hydrology climate chemistry and more general environmental modeling and engineering used field specific packages a complete list of reviewed software publications and their related software packages can be found in notebook 5a finding software packages by keyphrase extraction the most common analysis method provided by ua sa software was found to be sobol with the r sensitivity package providing the widest mix of methods see table 3 surveyed software tools typically did not provide oat analysis perhaps due to its simplicity or a sign of its decline publication of software related papers is relatively stable with a proportional spike in 2007 fig 3 software for the development of emulators did not feature heavily within the software corpora although they are present the hdmr method being one example described later on software to develop emulators include chaospy feinberg and langtangen 2015 the prism uncertainty quantification framework hunt et al 2015 gtapprox belyaev et al 2016 and uq pyl wang et al 2016 a collection of functions presented as a matlab toolbox is also introduced in vu bac et al 2016 all of these with the exception of vu bac et al 2016 were developed in the python programming language application of artificial neural networks and similar approaches did appear in the corpora but is not a topic of focus here as aforementioned current trends have shown an increased interest in best practices three sa packages released within the past five years reflect these changing attitudes psuade gan et al 2014 safe pianosi et al 2015 and vars tool razavi et al 2019 psuade a problem solving environment for uncertainty analysis and design exploration provides users with implementations of uq methods including sampling techniques and sa methods both local and global the package has had general application to various modeling scenarios safe sensitivity analysis for everybody provides users with implementations of global sa methods with the ability to perform multiple sas robustness assessment and convergence analysis without further model runs as reflected in its name this package was designed to allow global sa to be accessible to a more general audience the most recently released package in the survey vars tool provides implementations of sampling techniques and global sa methods including derivative variance and variogram based which can all be performed from a single sample the variogram approach to sa reportedly links both local and global approaches 4 1 survey of packages in common programming languages brief descriptions of software found in the corpora are provided here categorized by their implementation language some packages may be listed more than once as various implementations may exist or interoperability between languages is supported we decided to categorize the packages based on the implementation languages as most packages are not standalone tools with user interfaces ready to be used and are often provided as a library to be incorporated programmatically indicating the implementation language also allows readers to identify packages in a familiar language for potential adoption very few packages were found to provide a graphical user interface gui so some amount of programming ability and experience is the baseline expectation in the vast majority of cases users are expected to have a passing familiarity with the ua sa methods being applied as very little protection against improper use is provided a further brief discussion is in the recent developments section table 3 and 4 provide summary overviews of the software and packages 4 1 1 fortran fortran was one of the earliest programming languages available and arguably still dominates the scientific programming landscape fortran modules from the surveyed literature are jupiter api and ucode there is also a fortran repository of ua sa functions supported by the joint research centre pianosi et al 2015 the jupiter api joint universal parameter identification and evaluation of reliability application programming interface attempts to provide a standard set of programmatic functions for developing ua sa software and serves as the underlying engine for other ua sa packages ucode 2005 2014 were developed on top of this api the provided modules are developed in fortran 90 and support parallelization and local derivative sensitivity analysis jupiter api was first released in 2006 and its latest release was 2013 its affiliated webpage was last updated in 2016 suggesting an active community it is provided freely and under an open source license with a user manual and examples of applications first released in 1998 ucode universal inverse code was developed in fortran90 fortran95 and perl it originally implemented inverse modeling methods and by its first revision 2005 consisted of post processing modules for and not limited to sa calibration and ua the second revision 2014 included mcmc in the ua module and made the platform more compatible with models developed in matlab or using a gui this can be viewed as a response to changing trends in model development particularly the proliferation of matlab based models user documentation is available for download although the software is still available for download its development has ceased 4 1 2 c c surveyed software available in c c include dakota psuade pest and vars tool the dakota toolkit had its initial release in 1994 to provide optimization tools for engineers with further development it now includes sampling methods global sa methods parameter estimation and uq the software can be tightly semi or loosely coupled to the target model requiring the user in the first two cases to modify their code or use a direct interface the package is presented as being accessible to beginners and involves advanced features for more competent users it operates on linux windows and unix parallelization is possible and there is a gui option it is freely available for academic use and open source a user community exists including mailing lists and interaction with developers documentation includes user manuals examples and release notes dakota is well maintained its most recent release and webpage update being in 2018 it is an example of software that has kept up to date with the latest trends in ua sa and software implementation psuade problem solving environment for uncertainty analysis and design exploration can link to simulation code in any language it provides users with 14 sampling methods and 12 sa methods both local and global sa it was developed for large complex systems models and has been applied to various fields the software has a free public license and is open source a collaborative user community exists the software and documentation a user manual are available for web download the package is well maintained with its latest release and update in 2018 pest parameter estimation toolkit is designed primarily for model calibration originally released in 2003 and with its most recent release in 2019 it has remained up to date with the latest research in environmental modeling the current package provides parameter estimation and uncertainty analysis including monte carlo analysis and has parallelization capabilities the software is designed for complex environmental models and other models models written in c c fortran and python have interoperable interfaces available it is free although the license does not appear to be specified and well documented for ease of use developer user interaction is encouraged and training courses are offered 4 1 3 matlab identified packages of interest written in matlab are simlab mcat gui hdmr uqlab safe and vars tool simlab is a package for monte carlo based sa written in matlab and supplied by the joint research centre initially released in 1985 its latest release was 2008 and its associated webpage was last updated in 2016 it provides monte carlo and other random sampling methods test functions for educational purposes and gsa correlation regression and variance based the sa follows a loosely coupled approach requiring only the model output to be fed in it is freely available for academic use and open source no user community appears to exist the documentation consists of a reference manual and the software is available for web download mcat monte carlo analysis toolbox implements monte carlo sa its first release was 2001 and a companion paper highlighting the importance of best practices in sa was released in 2007 however no further research appears to have been conducted since this time and links to software download provided in the companion paper have expired this package is of interest as an example of software tooling designed to promote modeling sa best practices the software provides implementations of ua sa methods including regional sa monte carlo analysis and glue a gui was developed for it in 2007 the package is free and open source and documentation includes a manual and examples no user community appears to exist however there is an unofficial github page see table 4 gui hdmr graphical user interface high dimensional model representation provides hdmr a variance based sa method which the developers advertise as an alternative to other contemporary sa methods the user must supply an appropriate sample of the model output there is a complementary package rs hdmr random sampling hdmr for this purpose users have the choice of using a gui or a script based interface the software is reportedly user friendly and has been applied to various fields it is freely available for academic use but not open source the software and user documentation are available for web download although the related publication is highly cited this software appears to be abandoned having its first and last release in 2008 a lack of user community and implementation of a single sa method could be a cause for this uqlab uncertainty quantification laboratory provides among other tools for uq tools for statistical analysis such as sampling and global sa global sa methods are supplied through a linkage with the r sensitivity package parallelization is supported the package is user friendly and adaptable to various levels of computational experience collaboration amongst users is encouraged and users can contribute to code with revision by the major developers it is portable between operating systems and freely available for academic use however documentation is not freely available the software is well maintained with its latest release and update in 2018 safe sensitivity analysis for everyone is compatible with the gnu octave environment and a version implemented in r exists making it the most openly accessible of all the surveyed matlab packages it runs on any operating system the toolbox was designed to make global sa accessible to users with limited knowledge of global sa or matlab whilst also allowing more advanced users to explore research and better understand sa users are provided with various sampling methods local and global sa methods and a gui see table 3 although there appears to be no collaborative user community user developer interaction is possible via email the software is freely available for academic use and is open source documentation includes the companion paper pianosi et al 2015 and additional information provided in workflow scripts there have been no recent releases however the website is maintained last update 2018 vars tool is also available in c and ostrich a user independent interface it features off line and on line mode options for running models in any language or operating system numerous sampling and sa methods are supplied including vars it is said to be user friendly and accessible to various levels it appears to operate as a command line interface without a gui although recently developed there is no collaborative community the software is freely available for non commercial use and is open source there are capacities for parallelization and reporting and visualization tools its documentation consists of a manual 4 1 4 r statistical language the main sa package for the r language is the r sensitivity package like python the r language is widely used in the sciences and so many of the tooling support interoperability with r see the section on python below and table 4 the r sensitivity package supplies various sa and sampling methods it offers loose coupling with models implemented in other languages as well as in r test cases are supplied for research and comparison purposes the package requires knowledge of r which itself is portable between operating systems and freely available a developer community exists and the available documentation consists of a reference manual since its initial release in 2006 more recently developed methods have been implemented and included in its latest release in 2018 4 1 5 python as with r users of python have a large assortment of options generally due to python being a general purpose language often used for interoperability across languages see table 4 the principal sa package developed in python appears to be salib sensitivity analysis library which provides global sampling and analysis methods and is distributed under a free public license model runs can be invoked directly or separately offline salib is most applicable to systems modeling and knowledge of python is assumed it is a freely available open source package with a collaborative user community salib is well documented and well maintained documentation includes an installation guide basic usage guide a complete module reference and release notes its latest release was 2018 salib supports visualization of morris results only although this feature appears to be under documented a separate visualization tool is available for analysis of sobol results called savvy hough et al 2016 however this package was not examined in depth 4 1 6 java there appears to be limited sa packages implemented in java at least in the reviewed corpora a response to this limitation is the mouse model optimization uncertainty and sensitivity analysis package this is an implementation of mcat and optas model calibration software for modelers using java it is indicative of the continued influence of the packages mcat and optas its first release was in 2014 and was last updated in 2016 although claiming to be free and open source we could not find relevant information to access the package 4 1 7 julia mads model analysis decision support is an sa package available for the julia programming language the analyses it supports can be tightly or loosely coupled with an existing model in the module documentation extensive information is provided for all functions included in the main module mads jl the documentation details modules and examples and although extensive was found not to be user friendly with functions and methods often lacking meaningful descriptions mads is said to support use in high performance computing hpc environments it is a freely available open source package with a collaborative user community an inherent advantage of mads is the relative youth of the julia language with v1 0 released in 2018 due to its relative youth it leverages lessons learnt in older programming languages and was developed with modern computational architecture in mind this means that concurrent and parallel programs are relatively easy to develop in julia bezanson et al 2017 and it has had demonstrable success on hpc platforms see for example regier et al 2019 the disadvantage of this youth however is that the user community while growing quickly is still relatively small compared to that of established languages as such the language ecosystem is undergoing continual development and may still be immature 4 2 active use and development to gauge the level of support and active development occurring for each software tool we attempted to identify websites evidence of userbases public code repositories journal publications which specifically mention the software tool and other indications of activity through this process we found that many of the packages present in the literature are no longer under active development although the code and software may still be available for use a key issue in developing software for ua sa is longevity we find that those packages that are currently used and under active development and maintenance have the advantages of being open source well documented for transparency and ease of use have an active user community and offer implementations of a range of ua sa methods for general purpose application as opposed to providing a specific method for a specific model packages that have fallen into disuse may still be useful with the caveat that there is no supportive community to rely on for bug fixes troubleshooting user support and so on table 3 provides an overview of the available ua sa methods in the surveyed packages while details of the software can be found in table 4 4 4 bibliometric overview the initial corpora from wos consisted of 11 718 publications from which journals deemed to be unrelated to the topic areas of interest as specified by the search terms used journals with less than three identified publications and those without a valid doi were removed the final corpora consisted of 11 625 publications knowing that researchers build on prior work and given the exponential growth of published material bornmann and mutz 2015 haddaway and westgate 2018 we assume in this analysis that the identified corpora is representative of the ua sa field full details of this process can be found in notebook 2 create filtered corpora the number of publications in the environmental ua sa field have been increasing at an exponential rate depicted in fig 5 with journal of hydrology having the most publications overall and experiencing the largest year on year gain within the analyzed time frame fig 6 to facilitate analysis the final corpora was broadly categorized into two topic sub corpora applications and frameworks using the topic model as a reminder the final corpora represents a collection of ua sa research publications focused on ua sa frameworks and guidelines were placed into the frameworks sub corpora while applications included those taken to be focused on the application of ua sa methods the topic model was iteratively applied and key phrases from top cited papers were qualitatively examined to determine the focus of the publications the specifics of the undertaken process can be seen in notebook 4 uasa topic modeling a keyword search was applied within these topic corpora to sort publications further into those relevant to uncertainty quantification uq ua and sa the resulting collections contained 1 940 2 751 and 1 360 publications respectively to distinguish between lsa and gsa methods specific keywords were searched for in the combined corpora including for example local sensitivity oat one at a time for local methods and global sensitivity and gsa to indicate global methods in addition to these newer sa methods identified through manual inspection of the corpora were also searched for such as active subspaces and variograms 4 4 1 trends and directions as suggested by the general publication trends in fig 7 all topics ua sa frameworks and applications saw large increases in the absolute number of publications over the 2000 2017 timeframe within the same time period the proportional share of the filtered corpora has declined for sa by 4 5 while ua has increased by 5 which may indicate a gradual shift towards being more inclusive of uncertainty related matters in analyses as well as a general need for uncertainty guidelines in environmental modeling see notebook 4 uasa topic modelling the five most active journals in the frameworks sub corpora were structural and multidisciplinary optimization journal of computational physics environmental modeling software and journal of hydrology see fig 8 the 10 most cited papers from across these top five journals came from environmental modeling software 2 structural and multidisciplinary optimization 3 journal of hydrology 2 journal of computational physics 1 and computer methods in applied mechanics and engineering 2 and are detailed in table 5 under supplementary material the top cited framework related papers from these journals table 7 showcase a range of issues but particularly address the lack of uniformity in the ua sa approaches used in their respective fields these fields include environmental modeling evaluating performance bennett et al 2013 improving confidence in model outcomes and handling uncertainty bennett et al 2013 kuczera et al 2006 refsgaard et al 2007 ua for hydrological swat models yang et al 2008 optimization topology optimization sigmund and maute 2013 finite element methods blatman and sudret 2011 moens and vandepitte 2005 level set methods for structural topology optimization van dijk et al 2013 high dimensional computationally expensive black box problems shan and wang 2010 and scientific computing handling uncertainty roy and oberkampf 2011 outlines of procedures guidelines comparisons of methods and suggestions for future research resolve the issues raised in these papers these papers are highly cited indicating that they have had an impact on the research community at least within their respective fields it should be noted here that existence of highly cited papers itself does not indicate widespread application of suggested good or best practice and should not be taken as evidence the review conducted by saltelli et al 2019 concludes that there is a worrying lack of standards and good practices although it is acknowledged that the review focuses on older papers and may not capture recent trends certainly awareness appears to have increased if not adoption of practices similarly a keyword search for best practices identified 132 papers across the surveyed period by best practices we refer to practices in modeling and uncertainty management that promote transparency and reliability of results the high citation counts of papers relating to frameworks table 7 and the growth in best practices publications in absolute terms fig 9 suggest increasing interest in uncertainty management particularly improving the reliability and effectiveness of ua sa whether the modelers take up the suggestions in these papers is yet to be seen modelers can be encouraged to follow guidelines for reliable and effective treatment of ua sa if the available software implementing ua sa is designed in accordance with these guidelines and if modelers make use of such software 4 4 2 recent developments recent impactful publications in sensitivity analysis suggest a shift away from local sensitivity methods prior to 2010 one factor at a time oat local sa was the most prevalent practice in the literature saltelli and annoni 2010 with a later revisit indicating that while this was still the case for papers published in science and nature gsa methods were gaining traction ferretti et al 2016 a more recent bibliometric review conducted by saltelli et al 2019 comes to a similar conclusion across 19 subject areas in which modeling features heavily although the growth of oat related publications is shown to significantly out pace gsa related publications within the presented corpora publications with oat related keywords do decrease slightly over the past two decades down roughly 1 compared to the entire corpora with an uptick in the absolute number of publications post 2010 see fig 10 although oat is said to be a common method see for example shin et al 2013 it may not have featured heavily prior to 2010 due to 1 researchers not reporting oat use 2 modelers using custom implementations of oat and 3 the software surveyed in our analysis did not support oat which discourages modelers from using this method i e they select from available methods analysis conducted here indicates an increase in reported gsa keywords post 2010 fig 11 after the publication of how to avoid a perfunctory sensitivity analysis saltelli and annoni 2010 this paper was identified as a highly cited publication in the initial corpora table 6 in the supplementary material a key contribution being the demonstrated inefficacy of oat analyses using a geometric proof the uptick in publications with the oat related keywords appears to correlate with the number of papers citing the paper by saltelli and annoni 2010 shown in fig 12 this may contribute to the rise in publications with oat related keywords in the corpora and as identified by saltelli et al 2019 the detected increase in gsa papers may reflect the start of changing attitudes towards sa in recognition of the importance of global sensitivity analyses increased awareness in the past decade has led to the use and development of more efficient and comprehensive ua sa techniques and approaches improved approaches put forth in the past decade attempt to enhance the computational efficiency of generating a global sensitivity measure or range of measures as the case may be from a single sample set itself said to be more representative of the possible parameter space e g razavi et al 2019 in particular there has been a renewed interest in gsa based on statistical design of experiment approaches as these methods are capable of producing global sensitivity measures at an acceptable computational cost gan et al 2014 saltelli 2017 such approaches refer to methods that utilize a deterministic sample set for example the aforementioned sobol latin hypercube and morris methods saltelli 2017 despite the increased interest in gsa evidenced by the bibliometric analysis local sa and oat methods are still in widespread use if any sa is conducted at all shin et al 2013 for example found that only 7 11 of 164 of papers surveyed conducted any sa of which five applied oat it is difficult to ascertain the full extent of oat analysis through keyword analysis as researchers applying this technique may not make explicit reference to this form of analysis possible reasons for the relatively slow uptake of gsa methods are listed in ferretti et al 2016 including perceived complexity in the application of gsa modelers were characterized as being hesitant due to a lack of experience with gsa methods we also find in the literature a prevalence of self implemented ua sa that is modelers using their own code in place of existing and often open source software tools not using or otherwise contributing to readily available widely used and well tested software represents a duplication of work this can be somewhat alleviated by greater awareness of and access to the available software tools that simplify the application and use of such analyses those developing tools and methods for their part could strive to improve ease of use and lower the technical and conceptual barriers to uptake of their software pianosi et al 2016 outline three principles of good practice for a sensitivity analysis package 1 the ability to apply multiple sensitivity analyses to one sample 2 provision of tools to assess and revise user choices and 3 inclusion of visualization tools regarding point 1 early software releases tended to be platform method or model specific see table 4 for specific examples in recent years the available software has been made for more general purpose use offering a more comprehensive approach to ua sa with multiple methods supported the lack of collaborative development is also reportedly an issue with researchers preferring to develop their own toolset and as a consequence siloing advances at least in the short to medium term usability especially for novices is an ongoing concern while many sampling and analysis approaches are amenable to cross use e g a mix and match approach there is often no limitation in the application of methods within the packages which safeguards a user against inappropriate and incompatible mixes e g sobol analysis on a latin hypercube sample efforts to address these issues and criticisms are evident in the various communities however with later packages often offering detailed documentation including usage examples and tutorials see previous section well known test functions such as the ishigami function ishigami and homma 1990 sobol g function saltelli and sobol 1995 the example lake problem hadka et al 2015 as well as case studies for research and educational purposes are often included pest doherty 2018 for example provides a ua tutorial including two worked examples of hydrological models another example is safe pianosi et al 2015 which by providing commented code in workflow scripts allows beginner users to implement ua sa more easily and advanced users to improve their methodology the packages in our survey did not appear to provide guidance through ua sa theory outside of extensive reading lists although it is acknowledged that this may be out of scope for those maintaining the tool a lack of guidance as such may hinder uptake by practitioners of both the software and gsa in general generally the available packages still operate on an understanding that users have appropriate background knowledge of or experience with ua sa many of the packages identified in this review appear to have been abandoned this perhaps indicates the importance of an active user community to share knowledge and update code the corpora give evidence to newer methods that are in development and reflects a continued interest in improving ua sa examples of more recently developed techniques are vars and active subspaces developed in 2016 and 2011 respectively vars variogram analysis of response surfaces uses variograms as a measure of sensitivity a variogram is a function describing the spatial dependence of in the case of sa the parameter space that is how much the variance in parameter values is dependent on the distance between parameters in parameter space variogram related publications began in 2001 however those specifically relating to the application of variograms to sa only appear from 2016 there are five publications relevant to variogram based sa in the corpora totaling 74 citations and the highest citation average is 13 razavi and gupta 2016 the top cited variogram paper razavi and gupta 2016 presents the method as a linkage between existing derivative and variance based gsa methods and demonstrates that the approach reduces computational cost over approximately 20 000 and 100 000 model runs the vars sensitivity estimates had less uncertainty than sobol and morris indices these relatively new methods though currently lacking citations do appear to be methods with development potential due to for example current user interest and improvements to the efficiency and comprehensibility of ua sa methods active subspaces is a dimension reduction technique that identifies directions in parameter space that have a greater influence on the model output these directions are described as being active and their identification aids in reducing the dimensionality of a model by avoiding perturbations across inactive areas of parameter space thereby reducing computational cost constantine et al 2015 through this method parameters of importance and their rankings can be obtained jefferson et al 2015 papers relating to active subspaces first appear in 2015 there are eight in total in the corpora citation analysis does not indicate particularly that this new method is being taken up quickly total citations for all publications was 83 and the publication with the highest citation average had an average of 7 67 constantine et al 2015 the top cited active subspaces paper constantine et al 2015 details an application of the method to numerical simulation and an implementation may be found in the effective quadratures package for python seshadri and parks 2017 another technique of interest is hdmr high dimensional model reduction the companion paper ziehn and tomlin 2009 for the method and supporting software came through as a highly cited publication in this analysis see table 7 hdmr is an emulation method that improves variance based sa methods such as the sobol method citing articles for ziehn and tomlin 2009 continue up to 2019 identified through manual processes in fact 7 of the 32 returned publications in the corpora were published in 2017 indicating a continued interest in the method in the corpora the publication with the most citations has 158 alış and rabitz 2001 and the highest citation average is 14 ziehn and tomlin 2009 furthermore alternative methods for handling uncertainty have been developed especially to handle scenarios in which there is large uncertainty but in which accurate predictions are necessary for future policy making software for these alternate methods is deemed out of scope for this study but for completeness sake one such proposed approach is exploratory modeling and analysis rather than simply minimizing uncertainty in an attempt to produce an accurate or precise prediction uncertainty is treated as inevitable decision making processes are guided through the exploration of possible outcomes generated through computational experiments and responses planned eker et al 2018 kwakkel and pruyt 2013 5 limitations the bibliometric analysis presented here is limited by the scope of the wos database the specific search terms used the initial time frame and the included fields of study with the analysis focused on applications in environmental modeling search query results may also differ over time due to indexing artefacts with implications for the resulting trend and citation analysis a bias towards open source software literature may be perceived as these were the easiest to analyze that said it is not claimed that the analysis conducted herein uncovered all software packages currently in use or the full extent to which they are being used a known issue is the lack of attributions citations and reporting of software used for research making it difficult to find their mention especially when the analysis relied on abstract text other software may not be referenced simply because their use is taken to be a fundamental part of the programming language ecosystem for example the r sensitivity package or sci kit learn for python it was also difficult to search within the corpora for packages with names common to other applications taking as a particularly difficult example the r sensitivity package in our own process of sorting the generated database decisions whilst manually sorting and choosing the software collection papers were subject to inherent bias although this process was kept as transparent and objective as possible see notebook 5a finding software packages by keyphrase extraction another process which limited the generality of our findings was that of refining the search terms and results limiting the scope of the results was necessary to facilitate analysis of the most relevant publications iterative use of the topic model achieved this however it is entirely possible that relevant publications will have been removed fig 13 of particular note is the possible under representation of articles on emulators and surrogate modeling within the software corpora omitted publications were assumed to be irrelevant or that relevant issues were captured by the papers that remained in the desired corpora for more information see notebook 4 uasa topic modeling 6 conclusion and future directions the analysis presented here indicates that ua considerations are increasingly included in the published literature with a slight decrease in the reported use of oat methods the identified literature reflects greater attention paid to guidelines for the use of ua sa over the past decade itself perhaps indicating advances in the application of ua sa greater interest in the use of ua sa for rigorous model testing is apparent although whether modelers embrace and adopt the suggested guidelines towards the treatment assessment and analysis of ua sa e g as discussed in eker et al 2018 saltelli et al 2019 remains to be seen the literature also suggests that a wide variety of software has become available in the past two decades aimed at both non programmatic audiences and for specific programming languages the majority of these identified software packages does not support local oat analyses which may indicate a general move away from depending on local sa more recently developed software packages that implement multiple methods with open source code and documentation with little restriction in terms of software licencing to the end user are becoming the prevalent distribution format while there is a variety of software tools available the trend of publications on ua sa tooling has remained largely flat this trend may be due to the relative infancy of the available tools or a perceived complexity in their application for one while many of the surveyed software provide usage examples and documentation their use typically assumes 1 experience with the underlying programming language or 2 intimate familiarity with the methods provided their pros and cons and contextual suitability little guidance is available aside from extensive reading lists the indicated lack of uptake in this analysis may also be because software specific publications have been largely filtered out from the corpora these relevant publications may be concentrated within conference proceedings which were removed from the corpora or other topic areas not included in the initial publication search publications that are application method focused may not explicitly mention the software used in the abstract for these reasons it is difficult to concretely conclude whether those involved in environmental modeling are embracing the available ua sa software tools or if custom home grown solutions are preferred itself indicating perhaps a lack of awareness of the available software packages that said usability and user friendliness were found to be a general issue users are expected to be adept and experienced enough to produce and interpret results themselves even in cases where visualization processes are provided users may require a different approach for their analyses in the case of novices interpreting provided analyses requires first understanding a body of work usually provided in the form of a often large reading list of relevant papers this may explain in part a preference for custom home grown solutions where the developers write tools specific to their needs to avoid adoption cost time needed to learn how to use an existing tool effectively the complexity of existing tools real or perceived may contribute to the issue of lack of uptake in cases where the perceived cost of adoption is high the prospective user may find it easier to apply oat or otherwise implement their own custom solution to perform common ua sa methods which amounts to duplication of effort across the scientific community this then raises the question of what constitutes a thorough ua sa package in this survey the most comprehensive software r sensitivity simlab and salib provide users with the widest assortment of ua sa methods with limited visualization capability and test functions these target languages prevalent in the sciences r matlab and python respectively that are supported by an active community which may explain their longevity and or popularity the prevalence of open source community led efforts evident in more recent software tools suggests that an open development culture is a prerequisite to widespread adoption perhaps an unremarkable observation due to the scientific context and focus of ua sa research developers and maintainers of ua sa tools could support and encourage wider application of gsa processes by moving towards 1 an open development process 2 placing further attention on expanding documentation preferably in an easily digestible form and 3 improving usage guidelines and promoting user centric interfaces and workflows point 1 is to encourage the sharing of knowledge and experience across the disciplines that rely on modeling to leverage expertise and experience globally rather than siloing advances on point 2 ua sa software developers could further leverage the open collaboration model and re use explanations and examples from one another examples of both simple and complex workflows could be given e g in a cookbook or recipe documentation style point 3 should not be taken to mean that all packages should provide a gui rather general purpose ua sa tools should have processes in place to prevent or limit unintentional or ill informed analyses from occurring a particular pain point is the ability to mix and match sampling and analysis methods regardless of whether it makes sense to do so while ua sa tools have largely addressed the three steps defined by pianosi et al 2015 sample parameter space run model analyze results the workflow that is implicit or explicit steps in the use and application of the software could be improved so that modelers are able to move from each step without issue recently developed packages indicate that such improvements to the workflow are being made with attention to usability open source code and tools for analyzing results researchers and modelers particularly those new to ua sa need software designed with usability in mind it is expected that such software will support ua sa in more areas and encourage rigorous and reliable ua sa which will in turn allow for more informed decision making software availability code and representative data used for this analysis can be found at https github com frog7 uasa trends 10 5281 zenodo 3406946 software used to support analysis can be found at https github com connectedsystems wosis 10 5281 zenodo 3406947 declaration of competing interest the second author has contributed usability and performance improvements to the salib python library all other authors declare no potential sources of conflict acknowledgements the corresponding author second in the author list is supported through an australian government research training program agrtp scholarship and a top up scholarship from the hilda john endowment fund the authors would like to thank and acknowledge joseph guillaume barbara robson and the anonymous reviewers for their highly valued comments and suggestions the use of work by titipat achakulvisut author of the wos parser python package https github com titipata wos parser and enrico bacis author of the wos client package https github com enricobacis wos is acknowledged we would also thank and acknowledge clarivate analytics for providing access and use of the web of science database and to their staff for providing clarifications to technical details of the available api without which this work would not have been possible appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104588 
