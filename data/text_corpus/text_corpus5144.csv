index,text
25720,the study of land change within social ecological systems ses is of great interest and increasingly makes use of remote sensing rs imagery to scale inferences up through space and time however spatial analysis using dense time series of rs data poses technical hurdles for non expert users to broaden the community of ses researchers using rs we present a simple tool for mapping land change at local to regional scales the python implementation of the noise insensitive trajectory algorithm pynita is accessed through a streamlined graphical user interface and requires minimal user parameterization to generate long term trends and identify key dates of significant change i e disturbance events based on time series of landsat or sentinel 2 data in this paper we introduce the pynita software explain the underlying algorithm analyze key parameter sensitivities and summarize methods and results from three ses case studies of land change keywords time series landsat social ecological land cover change software 1 introduction many environmental problems and conflicts center on disruptions mediated by social ecological systems sess collections of human and ecological agents and processes bound together by interaction and interdependence dietz 2017 lambin and meyfroidt 2010 deforestation flood and fire risk drought pressures land degradation and other issues emerge where human activity and ecosystem dynamics converge often when powerful human actors and institutions disrupt or pressure practices of people on the ground andersson et al 2011 pellow and nyseth brehm 2013 robbins 2004 to address such problems scientists governments and advocates have reason to seek deepened understandings of ses processes and accessible ways to monitor land change the tools of land change science turner and robbins 2008 which have grounded our insights into ses now make available high frequency remote sensing data and methods for interpreting them we have crafted such a tool pynita python noise insensitive trajectory algorithm an open source software with an easy to use graphical user interface gui for quantifying and mapping land cover change over time in this paper we introduce pynita and demonstrate its utility through several case studies in the last two decades mapping land cover dynamics with moderate resolution imagery has increasingly supported monitoring social ecological processes as they unfold on the landscape rather than simply detecting the end result of that process land cover change monitoring is commonly conducted through multitemporal satellite remote sensing rs rs platforms such as landsat and sentinel 2 systematically collect data relevant for characterizing land cover condition type extent and pattern at moderate spatial resolution i e 10 30 m across broad spatial extents at sub weekly through sub monthly time steps over many years li and roy 2017 earth observing satellites have provided revolutionary data for understanding ses processes such as deforestation and forest degradation hansen et al 2013 sedano et al 2020 agricultural intensification or abandonment alcantara et al 2012 seasonal flooding xiao et al 2005 and urbanization liu et al 2018 there is both increasing recognition and analytical capability by ses scholars that satellite rs data analysis has more to offer than visual interpretation of imagery or a before after i e bi temporal change comparison e g gillanders et al 2008 continuous repeated collection of rs data has allowed ses researchers to keep pace with the increasing speed of and spatial scale of social ecological change mace et al 2014 and to more systematically address core ses themes such as feedback loops lambin and meyfroidt 2010 system vulnerability and resilience that may play out over decades turner and robbins 2008 perhaps the most promising rs contributions to ses research have come through improved exploitation of the ever expanding satellite data temporal archives gorelick et al 2017 woodcock 2008 and more frequent new image acquisition claverie et al 2018 the base of users of rs data has increased substantially in recent years due to improved data availability computational processing resources and a move towards interdisciplinary science teams satellite data availability has increased massively in the last 10 15 years with the opening of the united states geological survey usgs nasa landsat archive going back to 1972 in 2008 woodcock 2008 the launch of landsat 8 in 2013 which offers nominal global coverage every 16 days at 30 m roy et al 2014 the launch of several european space agency esa sentinel earth observing satellites since 2014 berger et al 2012 and spectral harmonization among satellite sensors claverie et al 2018 li and roy 2017 similarly algorithms and image processing tools have become much more accessible due to the access of cloud computing resources e g google earth engine gorelick et al 2017 open source software and scripting languages e g r python and automated disturbance detection algorithms such as landtrendr kennedy et al 2018 and breaks for additive seasonal and trend bfast hamunyela et al 2020 verbesselt et al 2010 despite the many advances in image processing and opening of analytical frameworks several issues still impede widespread adoption of rs techniques for understanding specific ses processes pricope et al 2020 first there has long been an uneven distribution of rs research favoring protected forest ecosystems botkin and nisbet 1992 temperate zones whitcraft et al 2015 and wealthier countries martin et al 2012 fragmented and unprotected landscapes and non forested ecosystems particularly in developing countries remain understudied with rs despite their ecosystem vulnerability and need for conservation intervention trimble and van aarde 2012 second the majority of time series change detection methods for optical e g landsat and sentinel 2 satellite data verbesselt et al 2010 require time series data that are consistently collected and relatively cloud free conditions which are not often available in polar and tropical regions whitcraft et al 2015 including substantial portions of the global south third technical overhead may impede ses researchers without requisite training in analysis and interpretation of satellite and particularly time series data young et al 2017 similarly some rs researchers without theoretical grounding may be challenged to effectively relate land change information to the social ecological dynamics in question rindfuss et al 2004 fourth it is not a trivial task to integrate spatially continuous satellite data with spatially sparse or clustered ses field data e g interviews census socioeconomic data hecht 2014 steger et al 2020 in an attempt to broaden the community of ses researchers using rs data and diversify the impact of rs on ses research we present a new tool for satellite time series analysis our primary objective was to create a simple to use open source tool with low barriers to entry for researchers with limited rs experience the python implementation of the noise insensitive trajectory algorithm pynita adapted from alonzo et al 2016 is accessed through a graphical user interface gui and requires minimal user parameterization to generate long term trends and identify key dates and locations of significant change i e disturbance events based on a time series of multispectral data in this paper we introduce the pynita software explain the underlying algorithm analyze key parameter sensitivities and summarize methods and results from three distinct case studies of land change 1 differentiation of agricultural land uses in the colombian amazon 2 land cover classification for knowledge co production in ethiopia and 3 quantification of forest conversion to impervious surfaces for regional planning in maryland usa the pynita tool and the three case studies resulted from a collaborative data intensive pursuit facilitated and funded by the national socio environmental synthesis center sesync 2 background 2 1 remote sensing time series change detection algorithms the state of the art rs time series algorithms for terrestrial change analysis characterize multi year trends as well as breaks from those trends at the pixel level fig 1 huang et al 2010 kennedy et al 2010 with some algorithms additionally accounting for intra annual vegetation phenology verbesselt et al 2010 zhu et al 2012 the most common use of rs time series algorithms is for tracking land cover disturbances i e a significant change in land cover condition resulting from natural or anthropogenic factors particularly in forested ecosystems where an assumption of forest persistence and relative spectral stability year after year can aid the detection of prominent changes devries et al 2015 hansen et al 2013 murillo sandoval et al 2020 zhu et al 2020 current change detection algorithms are becoming more user oriented for instance landtrendr was originally only accessible as an interactive data language idl script kennedy et al 2010 but has recently been ported to google earth engine kennedy et al 2018 while landtrendr offers a set of spectral temporal segmentation algorithms finding the appropriate parameters for linear segmentation may pose a challenge to non expert users in contrast other algorithms available in gee such as continuous degradation detection coded bullock et al 2020 continuous change detection and classification zhu and woodcock 2014 and bfast hamunyela et al 2020 require fewer parameters to obtain useable results still these algorithms require computational resources and rs expertise there remains a need for a time series analysis tool that provides robust results and remains useable to the broader community who may lack an rs background establishing spectral trajectories from which one can assess inter and intra annual land cover dynamics for each pixel is also valuable for land cover classification at a single point in time several land cover classes may exhibit significant spectral similarity making them difficult to differentiate gómez et al 2016 for example primary forest and post clearing regrowth differ substantially in terms of disturbance history and ecosystem function but can appear very similar in a single date normalized difference vegetation index ndvi measurement a time series of ndvi however will capture disturbance history and can be distilled into useful metrics for augmenting a classification approach franklin et al 2015 showed that inclusion of normalized burn ratio nbr key and benson 2006 time series information such as overall trend persistence of largest disturbance and post disturbance duration helped raise the accuracy of a single year land cover classification by 6 4 compared to single year spectral metrics intra annual time series information is also useful for classification bargiel et al 2017 enhanced map classification accuracy of a northern european agricultural landscape by leveraging crop phenological information stored in a dense time series of sentinel 2 imagery for example summer barley classification accuracy average of users and producer was 51 5 with single date imagery and 74 when incorporating spectral crop cycle information 2 3 approaches to algorithm development the development of time series algorithms for use on rs data has to our knowledge been entirely in the domain of remote sensing computer or data scientists this leads to a limited view of the purpose of time series algorithms and whom they should ultimately serve thereby restricting uptake by disciplinarily diverse ses users this is in effect a top down approach to algorithm r d that relies on confidence in the credentials of the developer and the assumption of inherent data quality crampton 2011 such technical siloing limits the perceived value of rs data for non practitioners and can reduce their inclination to venture outside of their own disciplinary methods to investigate the benefits of rs data liverman et al 1998 by contrast a bottom up participatory design approach puts geospatial technology in the hands of the domain expert i e the person closest to the problem and best positioned to evaluate the solution goodchild 2007 steger et al 2020 in contrast to a top down approach bottom up collaboration with respect to algorithm development has multiple benefits design phase involvement of ses researchers immediately focuses the development effort on meeting on the ground needs rather than prioritizing incremental model improvement ses researchers are not monolithic and bring a diversity of viewpoints and visions for how time series of rs can help them extend their work spatially and temporally this diversity not only pushes against stubbornly ingrained disciplinary boundaries in traditional rs algorithmic design but may lead to a more useful product for a larger population working or collaborative groups tend to form based on member similarity proximity or familiarity leading to low information diversity i e diversity of knowledge bases and perspectives jehn et al 1999 cross functional teams such as those comprising rs scientists and ses domain experts have been shown to yield higher informational diversity and the potential to elevate group performance jehn et al 1999 early and steady participation in the algorithm design process also leads to end user buy in and ultimately more widespread adoption given that user needs are explicitly considered and addressed from the outset kushniruk and nohr 2016 moreover algorithmic usability and simplicity will likely be prioritized over complexity given the limited interest of ses collaborators in internal technical details of models 3 pynita methods and user workflow in the following paragraphs we briefly present the goal conceptual design workflow e g parameterization graphical user interface and key metric image outputs i e the distillations of pixel trajectories into meaningful statistics of pynita as well as our methods for conducting a sensitivity analysis comparing the python v3 7 implementation to a previous matlab 2015b implementation detailed in alonzo et al 2016 3 1 pynita algorithm pynita is a temporal segmentation algorithm tool for estimating per pixel land cover change trajectories at annual or sub annual time steps using rs time series data pynita can detect both abrupt and gradual changes using spectral indices such as ndvi or nbr and is highly flexible in response to user parameterization e g jamali et al 2015 pynita does not currently account for vegetation seasonality like ccdc zhu and woodcock 2014 bfast verbesselt et al 2010 or beast zhao et al 2019 in some contexts this is a limitation but the algorithm was designed for application in cloudy areas where modeling phenology is often infeasible because of data scarcity this development legacy gave rise to the noise insensitive moniker as we found that the influence of downwardly biased ndvi values due to atmospheric contamination could be minimized with appropriate parameterization pynita has two main subroutines build and subtract during build at each pixel a piecewise time series is fit to the data up to a user specified number of linear segments time series breakpoints are added based on the position of maximum orthogonal error calculated from the previous iteration until nbreakpoints nsegments max 1 subtract then iteratively removes segments to achieve a parsimonious fit e g fig 1 black line based on minimization of the bayesian information criterion alonzo et al 2016 3 2 pynita user workflow the pynita application can run on any commonly used operating system it is available for download https github com malonzo47 pynita gui as a binary release for windows and mac os which means that the user simply double clicks an executable file to run the software no knowledge of python or any other scripting languages is required advanced users may run source code either containerized in a docker environment to minimize system conflicts or locally the user has the opportunity to input either point or image data analysis of a spectral index e g ndvi time series is generally done first at a small set of calibration validation cal val locations where the researcher is familiar with land cover change history potentially including disturbance event timing for example the researcher may know based on fieldwork that the location point which translates here to a single image pixel depicted in fig 1 above was cleared of trees in november of 2005 the user then compares pynita s disturbance date against the known date i e reference data and updates the software s parameterization as needed once satisfied with pynita performance at their cal val points the user can then run the algorithm over an entire input image stack i e a chronologically ordered array of georeferenced satellite images to create metric image outputs fig 2 the spectral data from cal val locations and image stack files for use in pynita can be generated in google earth engine gee and include all necessary metadata image stacks may also be loaded without use of gee gee scripts shapefile examples and an instructional video can be found in the supplementary material as well as on the algorithm github page https github com malonzo47 pynita gui the gui structures the processing workflow through four tabs steps 1 2 shown in fig 3 accessed by the user in succession see basic workflow video 1 in supplementary materials in step 1 the user selects the directories for inputs and outputs and establishes a suite of 15 parameters including six critical parameters discussed below that underpin the pynita disturbance detection approach table 1 and that may be revised based on visualization or optimization results from step 2 the user first specifies their user vi with any spectral index si but commonly a vegetation index vi such as ndvi or nbr the user then defines a study period date limits and chooses day of year doy constraints i e the annual time range of analysis doy limits properly constraining doy is important to avoid modeling vegetation phenology as an annual disturbance doy limits is also a useful parameter on which to experiment because not all meaningful changes are best modeled during peak greenness which is commonly used to make annual composite images see video 3 in supplementary materials next the user specifies parameters that are geared towards modeling gradual and abrupt changes in light of varying degrees of atmospheric contamination or missing image dates for example filt dist controls the number of image dates over which error from iterative fitting is smoothed before further analysis for data with limited noise and acute disturbances followed by rapid recovery a filt dist of 3 would be reasonable while for noisy data with limited abrupt disturbances a value of 7 would be more appropriate fig 3 several parameters relate to fit complexity 1 bail thresh is the threshold at which pynita bails out of the build phase and returns only a single line fit appropriate for linear trends noisy data or computationally inexpensive initial data exploration see video 4 in supplementary materials 2 penalty is a component of the bayesian information criterion bic evaluation of model complexity during the pynita subtract phase where higher penalty values result in more parsimonious fits and 3 min complex sets a low bound on the number of segments to retain parameter superseded by bail thresh which is useful in places with known disturbances and generally not problematic for modeling undisturbed landscapes also at this point the user specifies the loss of si vi value vi change thresh that must be achieved to qualify as a disturbance as well as disturbance duration run thresh that distinguishes disturbance from decline in step 2 the user optimizes the time series model fit initialized in step 1 the user identifies a set of cal val locations steps 2a b fig 3a and views the model outputs at these sites additionally an automated optimization can be run to select the set of parameters that statistically minimizes the root mean squared error rmse between a small set of hand drawn trajectories and those that were automatically drawn see video 5 in supplementary materials in step 2c the user loads and can adjust a set of parameters see parameter set in fig 3a step2c which can be adjusted in search of the optimal parameterization that minimizes rmse once an optimized parameter set has been selected the user proceeds to steps 3 and 4 in which the full image stack is processed figure s1 in step 3 the user simply loads data chooses the number of processors over which to parallelize the processing task and runs the full analysis following processing in step 4 the output metric images e g disturbance date total si vi value change can be viewed and saved table 2 the user can interactively explore the trajectory underlying each metric image value by clicking on a pixel in any of the images fig 4 3 3 sensitivity analysis in order to ensure continuity of results after updating the algorithm and porting from matlab to python we performed a small sensitivity analysis on simulated data as in alonzo et al 2016 we simulated data based on 100 trajectories representative of a tropical forested riparian study area in papua indonesia at each trajectory we randomly generated data 100 times with realistic image date missingness temporal autocorrelation and downwardly biased error following alonzo et al 2016 algorithm parameterizations for matlab nita hereafter nita and python pynita implementations were identical we compared the deviations of nita and pynita estimated trajectories from simulated trajectories based on five key metric images disturbance date total value change value change pre 1998 value change post 1998 1998 being a pivotal year for landscape change in the papua case study and recovery four years after disturbance we also report omission and commission error for all disturbances as well as only for substantial disturbances where vi value dropped more than 0 7 which is typical for forest loss in the papua study area 4 results 4 1 sensitivity analysis while the nita and pynita implementations are nearly identical in their algorithmic underpinnings several changes were made in adherence with python coding best practices and to improve computational performance thus we sought to confirm that the outputs for key metric images were similar with respect to deviation from simulated results median deviation of pynita disturbance date from simulated disturbance data was 135 7 days while nita deviated by 274 8 days table 3 note that this timing bias in disturbance detection can be improved with additional user parameterization regarding which part of a pixel trajectory truly represents the date of disturbance parameter 16 fig 3a other value change and recovery metrics e g value change were broadly similar with no clear trend in terms of greater or lesser deviation from simulated data nita did perform slightly better in terms of lower disturbance detection commission error but both nita and pynita provided results that are within the range of existing time series disturbance algorithms table 3 zhu et al 2020 5 case studies here we present the results of three pynita case studies implemented by members of our sesync collaboration across a range of study topics and geographies fig 5 1 differentiation of coca plantation from cattle pasture following deforestation in colombia hereafter colombia 2 incorporation of pynita metric images in a land cover classification in support of knowledge co production in the ethiopian highlands ethiopia and 3 disturbance dating in suburban maryland usa to track the correspondence between increased impervious surface area and flooding md these case studies differ substantially in their biophysical climatic contexts and social ecological drivers of land change the colombia study site is between the andes montane and amazon forest biomes and characterized by gradual forest disturbance driven by a combination of human encroachment armed conflict and illicit land use activities the ethiopia site while also in the tropics is at high elevation 3200 to 3700 m above sea level and characterized by a patchwork of farmland grassland shrubs and forest that respond differently to intra and interannual variability in rainfall and human activity the md site offers a strong contrast to the others by virtue of its temperate seasonality and the dominance of the built up impervious land cover resulting from residential and commercial development 5 1 land activities in colombian protected areas colombia in the picachos tinigua macarena protected areas pas in southern colombia coca farming and cattle pasturing have brought widespread changes to this protected forest biodiversity hotspot since 1970 etter et al 2006 though documented anecdotally and intermittently across the pas these two land uses have thus far defied accurate rs detection in part because of the persistent cloud cover across the region to identify coca plantation and cattle pasture we 1 parameterized pynita to detect forest disturbances 2 classified annual landsat imagery using pynita and other metric image outputs and a random forest rf classifier breiman 2001 and 3 assessed the accuracy of classified forest coca and pasture pynita was parameterized to detect disturbances using a normalized difference moisture index ndmi time series through purposive sampling of 160 locations with known disturbance events we selected ndmi because of demonstrated high accuracy for tracking tropical deforestation devries et al 2015 schultz et al 2016 and limited the study period to after 1997 when landsat scene availability in the study area improved compared to the 1980s and early 1990s using the pynita gui we narrowed the doy limits to the range of january 1 to march 20 see table 4 coinciding with the dry season filt dist was set to 3 vi change thresh to 0 1 and filter opt to sgolay in order to maximize detection of small acute disturbance and recovery processes pct was set at 60 since downward biases common due to atmospheric noise in ndvi are less problematic with indices that do not include visible wavelengths of light e g ndmi we used four ndmi time series metrics output by pynita as inputs to our rf classifier disturbance magnitude distmag recovery two years after disturbance recovery2 post disturbance magnitude postdistmag and total value change valuechange table 2 these metrics are useful to capture abrupt and large scale land use conversion e g forest to pasture or forest to coca as well as partial forest alteration e g coca dynamics recovery is useful to identify coca plots because they are commonly abandoned leading to mixed shrub and secondary forest regrowth postdistmag allowed for tracking the spectral signal immediately after clearing while valuechange captured the ndmi trend over time we also used the annual median value for each landsat spectral band and the elevation and slope based on a 30 m digital elevation model as inputs to our classifier we trained our model by sampling the above metrics at training sites that were labeled as coca pasture or forest yearly from 2010 to 2018 our field data for coca was obtained from integrated illicit crops monitoring system through visual delineation on satellite imagery and confirmation by aerial inspection unodc simci 2018 pasture field data came from previous published research and the corine land cover dataset murillo sandoval et al 2018 we ran the classifier on landsat time series imagery from 2010 to 2018 and assessed the accuracy of the output land cover maps using an out of bag oob estimator field maps and classified maps based on pynita metrics were generally in high agreement for detecting changes associated with the expansion of coca and pasture fig 6 a even when both land covers are located in close proximity they were effectively discriminated by the classifier fig 6b however isolated coca patches were sometimes confounded with small pasture plots and coca suffered from higher commission and omission errors i e 25 compared to forest 12 or pasture 7 fig 6c these results benefited from the inclusion of pynita metrics which decreased the oob error rate from 25 without pynita metrics to 20 including pynita metrics the five most important variables based on gini impurity driving classification accuracy in order of importance included two landsat spectral bands swir1 and nir as well as three pynita metrics based on ndmi valuechange postdistmag and distmag in particular postdistmag effectively distinguished coca from pasture since the metric is generally higher and more variable for pasture than coca these results indicate that long term trends and specific time series temporal metrics output by pynita facilitated the accurate discrimination of coca and pasture murillo sandoval et al 2018 shimizu et al 2017 while random forest classification of the raw time series of spectral values may have similarly improved classification accuracy our method additionally provided interpretable metric images with physical or social ecological meaning hermosilla et al 2015 senf et al 2015 this small case study shows that pasture is more spectrally diverse over space and time with patches of bare soil and periods of rapid pasture growth by contrast coca is relative spectrally stable in this tropical region cultivated continuously over the same locations future studies will benefit from analyzing the spatial context of pixel based pynita outputs e g shape based and deep learning approaches to better disentangle the occurrence of these two important land covers in colombia 5 2 land cover classification using temporal metrics in ethiopia ethiopia knowledge co production that draws on local and indigenous knowledge as well as scientific knowledge e g satellite rs presents opportunities for developing a more holistic understanding of environmental change and improved management of ses dietz et al 2003 folke 2004 mclain and lee 1996 we applied a knowledge co production approach tengö et al 2014 to investigate the causes and consequences of environmental change in a community protected grassland and its surrounding landscape in the ethiopian highlands fig 7 a and b we sought to first understand how local residents perceived the landscape i e what land use cover classes they considered useful and how they perceived them changing over time and then used rs to calculate the spatial extent of those classes across the study area and quantify how vegetation quality has changed over time based on nine locally relevant land cover classes articulated by residents i e plantation forest grazing land farmland stone bare land shrubland native forest protected grassland and water we trained a rf classifier using metrics derived from a december 10 2016 landsat 8 surface reflectance image and pynita metrics based on the normalized burn ratio nbr from all available images collected in the wet doy limits 9 129 and dry doy limits 200 314 seasons from 1985 to 2019 date limits of 9999 9999 so as to include all available data wet and dry season boundaries were defined by local participants according to their management activities and the dates of the ethiopian calendar in this environment with high cloud cover and missing data before 1999 we used a higher filter distance filt dist of 7 table 4 for all parameters which helped us to more robustly capture broad trends albeit at the expense of making it more difficult to capture acute disturbances accordingly min complex was set low 2 for wet season and 1 for dry season as capturing 3 part disturbance dynamics was not relevant in this landscape ultimately we input 25 variables into the land cover classification algorithm including 12 metric images generated by pynita 6 wet season and 6 dry season linear error linerror normalized linear error bailcut total value change valuechange and three value change periods of particular interest to the local communities based on changing political and management regimes 1991 1991 2003 2004 2012 valuechange single date variables included 2016 30 m optical landsat 8 image bands and three derived tasseled cap composites brightness greenness wetness kauth and thomas 1976 as well as elevation aspect and slope derived from a 30 m aster global dem abrams et al 2020 the rf classifier was calibrated and validated against a dataset of 3244 data points compiled from field visits participatory mapping exercises and high resolution imagery across nine land cover land use classes defined by participants we achieved a total accuracy of 85 7 kappa 0 84 which is markedly better than the 80 5 total accuracy kappa 0 78 measured without using pynita time series metrics with only single date and topographic inputs to the classifier the accuracy of five classes i e water protected grassland bare land grazing land and plantation forest accuracy exceeded 80 with the inclusion of pynita time series metrics eight of nine classes exceeded 80 average accuracy with particularly large percentage point pp improvements for plantation forest 7 7 pp farmland 11 4 pp and stone 12 6 pp the culturally and ecologically important land cover of protected grassland was also more accurately classified using pynita time series metrics with an increase of 2 8 pp to 92 average class accuracy the most important variables based on mean decrease in accuracy upon exclusion across classes were elevation pynita linerror fig 7d and greenness highlighting the importance of including multiple sources of data with complementary spectral temporal and physiographic information content farmland accuracy increased dramatically 11 4 pp with wet season pynita linear error most responsible for the improvement previously differentiation of farmland and stone relied heavily on variation in greenness which is only adequate for fields that are under cultivation at the time of imaging compared to stone and bare land pynita linear error for farmland is high because crop phenology and rotation cannot be adequately modeled by a simple linear trend plantation forest classification accuracy increased largely because of information encapsulated by nbr value change during the dry season fig 7c these results illustrate the importance of algorithmic flexibility with respect to targeting specific doy ranges as well as the value of drawing on local knowledge for selecting meaningful ranges here wet season nbr values were slightly increasing over time but largely stable and in line with the wet season trajectory of native forest by contrast the dry season nbr trajectory requires a piecewise fit to model eucalyptus and cypress planting that was widespread in and around 2005 these introduced species tend to retain greenness even during the dry season while native species do not protected grassland shrubland and grazing lands were all somewhat difficult to discriminate on the basis of single date metrics due to their spectral and structural similarity as well as their patchiness at sub landsat pixel spatial scale distinguishing between these classes is of significant cultural and economic importance for the local case study and other areas of the ethiopian highlands as shrub encroachment increasingly threatens the sustainability of afroalpine grasslands steger 2020 here we found that classifying these challenging cover types required a wide range of data most importantly information about vegetation and soil moisture content from shortwave infrared bands elevation pynita linear error and dry season change in nbr this work has laid a foundation for reaching a common understanding of current land use cover among members of the collaborative management team who have very different worldviews and approaches e g local farmers scientists conservation managers having an agreed upon map provides a reference for understanding the manifestations of past change in the region and a launching point for examining future change in this area specifically relating to shrub encroachment and climate driven alterations to agricultural productivity 5 3 dating land conversion to impervious cover in a high flood risk area md ellicott city maryland md usa experienced two 1000 year rainfall events within a two year period 2016 and 2018 producing devastating flooding property damage and fatalities the rapid and widespread development of roads and housing over the second half of the 20th century has increased the proportion of impervious surfaces largely at the expense of forest in the hills above the city increasing lowland flood risk dramatically in response the county government has been strengthening requirements for stormwater management by new properties with key improvements implemented in 1985 2002 and 2010 given increased flood risk from the construction boom and decreased flood risk coming simultaneously through implementation of regulations and retention facilities it is useful to assess land cover changes more comprehensively for the time period corresponding to each mitigation strategy here we used pynita to quantify the extent of land cover change within the three key periods 1985 2002 2002 2010 2010 2019 by estimating the date of each pixel s disturbance and by examining long term loss of vegetation pynita parameters were selected through automated optimization fig 3 step 2c video 5 supplementary materials using ndvi imagery 869 images with 80 cloud cover from 1984 to 2019 and hand drawn trajectories of disturbance pixels and stable vegetation pixels we assessed the quality of our parameter set table 4 through comparison with disturbance dates estimated at 60 hand selected validation points the validation points were selected based on the ability to determine the disturbance year within 2 years using historical high resolution imagery available through google earth doy limits were set for northern hemisphere summer to dampen the influence of vegetation phenology we selected a small filt dist of 3 because data quality was high and missingness was low as is typical over the continental united states and in contrast to the two case studies above this ultimately allows for greater precision in disturbance dating min complex was set to 5 since model parsimony was less important than precise disturbance characterization once a trajectory was generated we allowed the time over which a decline could qualify as disturbance to be high at 3650 days 10 years this was warranted for our application because we are most interested in land transitions within regulatory periods of 8 years fig 8 disturbance dates estimated from pynita trajectories were accurate within 2 years of the validation point dates 93 and 1 year 89 of the time no false positives were detected on stable landscapes such as persistent forest or impervious cover from development prior to 1984 in our tiber sub watershed study area fig 9 we found that 3 16 km2 33 of land was disturbed within our study period in our first 1987 2001 second 2002 2010 and third 2010 2019 regulatory periods 0 96 km2 10 2 0 km2 21 and 0 20 km2 2 were disturbed respectively indicating rapid residential development in the first decade of the new millennium while pynita provides an accurate representation of disturbance that corresponds well with both large scale e g industrial and smaller scale e g single family residential development this acute assessment may not ultimately be of interest to flood planners industrial and institutional disturbance likely correspond well with persistent increase in impervious cover but residential disturbance does not due to planting of lawns and other vegetation beyond disturbance with pynita one can also report on valuechange table 2 of the spectral index over any set of time periods this value change indicator is agnostic to the driver of change and is not sensitive to disturbance detection parameters we found that 31 of land area decreased in ndvi by greater than 0 1 indicating substantial decrease in vegetation cover over the three regulatory periods ndvi decreased more than 0 1 in 1 33 km2 13 8 1 08 km2 11 2 and 0 58 km2 6 0 respectively fig 9 this finding highlights similar losses between the two presented methods disturbance detection and long term value change due to the user selected vi change threshold of 0 1 if this threshold were empirically determined to be more appropriate at for example 0 2 the total loss area is substantially reduced from 31 to 14 1 in its support of multiple approaches to documenting land cover change these results illustrate the contributions of pynita in both a policy making context and as part of more in depth social ecological research on flooding in ellicott city 6 synthesis and conclusions the three case studies above suggest the range of applications and the benefits of flexibly implemented rs time series as is now well established in the literature there is a clear need for high frequency observation bi temporal analysis could not adequately match the temporal resolution of the social ecological processes under study gillanders et al 2008 reynolds hogland and mitchell 2007 in colombia pynita allowed for ingestion of sub annual ndmi data necessary to capture change following for instance a 2016 peace agreement that resulted in increased cattle ranching and coca cultivation in ethiopia high frequency data evaluated at multiple expert defined doy ranges revealed differing trajectories of plantation forest structure and moisture depending on time of year md made use of all available landsat data in pynita to both accurately date urban development and to estimate overall downward trends in vegetation cover low frequency data availability would not allow for comparison of land cover transitions with specific stormwater mitigation efforts in this urbanizing context while all case studies were reliant on high temporal resolution satellite data achieving the most useful pixel histories in these dense time stacks additionally demanded distinct parameterization for each a goal in the development of pynita was to demand minimal parameter adjustment from end users while allowing for substantial modification as warranted the case studies demonstrate that there are broad similarities among the implemented parameter sets but important differences based on goals and data availability and quality table 4 the choice of spectral index is a critical decision for any univariate time series analysis zhu 2017 in our case studies colombia and ethiopia captured abrupt or subtle changes in vegetation structure and moisture with nir and swir bands while md highlighted stark transitions from vegetation to impervious surface in md it is likely that any standard vegetation index would suffice due to the extreme nature of the changes to the landscape users are encouraged to experiment with adjusting the doy limits parameter based on their understandings of local phenology cloud cover or other seasonally important ecosystem dynamics in ethiopia the researchers gathered expert knowledge from local ethiopian residents regarding the typical doy ranges for the wet and dry season ultimately leveraging seasonality information to improve classification results in md the research team sought to minimize the influence of phenology through selection of only temperate summer doy the ease with which doy can be adjusted and the results visually confirmed in the pynita gui is a feature of this software but in cases where broader scale mapping is desired or expert knowledge is unavailable algorithms such as bfast or ccdc are likely preferred as they are more fully data driven verbesselt et al 2010 zhu and woodcock 2014 the decision of whether to prioritize disturbance detection or more gradual changes is reflected in multiple parameters but notably in filt dist and filter opt colombia and md sought to characterize disturbance timing and magnitude albeit under very different environmental conditions and thus selected a narrow 1 d filtering kernel filt dist 3 paired with the savitzky golay sg filter opt sg filtering has proven useful in smoothing while preserving higher statistical moments of data in rs time series chen et al 2004 particularly for md where images were consistently available fig 9 and disturbance date accuracy was paramount these filter options were critical for estimating disturbance date within 1 year of the observed high resolution land cover transition by contrast ethiopia did not have a primary goal of disturbance discovery and the ethiopian study location was subject to data issues from persistent cloud cover and missing images due to limited data downlink capabilities in the 1980s and 1990s wulder et al 2016 therefore a filt dist of 7 and a movcv moving coefficient of variation filter opt were appropriate to capture broad trends aligning with political and management changes steger et al 2020 while remaining relatively insensitive to noise from imperfect cloud masking the ability to detect both acute disturbances and gradual changes has been an important advance in rs time series methods as it reflects the multiple modalities of environmental change that both may for instance underpin national policy e g reduced emissions from deforestation and degradation redd goetz et al 2015 souza et al 2013 or shed new light on more difficult to detect anthropogenic drivers of forest change such as charcoal harvesting sedano et al 2020 and illicit land activities tellman et al 2020 since the landsat archive was opened for free use in 2008 many change detection algorithms have been developed so what is pynita s particular contribution following the time series algorithm taxonomy of zhu et al 2017 pynita would be considered a segmentation algorithm making use of high frequency e g ccdc zhu and woodcock 2014 univariate e g spectral index pixel scale data in an offline mode capable of detecting both abrupt and gradual change it may then be considered most akin to the detecting breakpoints and estimating segments in trend dbest jamali et al 2015 algorithm specifically with respect to its focus on user parameterization flexibility and lack of a seasonal modeling component we thus put forth pynita not in an attempt to win an algorithm comparison contest but to lower the barrier to ses researcher adoption through improved usability to this end we benefited from an algorithm development process incorporating the stated needs and real time feedback of the 15 ses researchers in our sesync pursuit annapolis md 2018 2019 in the pynita case we primarily lower the barrier through implementation of a graphical user interface and easy visualization and interaction with data this priority is consistent with recent efforts to bring remote sensing to a broader audience through for instance publishing a survival guide to landsat preprocessing young et al 2017 and the introduction of google earth engine to improve accessibility to cloud based remote sensing image analysis at planetary scale gorelick et al 2017 despite much progress there are still barriers to non expert entry in the field of rs time series analysis fundamentally it remains challenging to non specialists and frequently specialists to link unique observable patterns from satellite data with social ecological drivers of land cover or land use change on the ground i e socializing the pixel geoghegan et al 1998 while these cognitive translations remain an area for continued research pynita can serve to test hypothesized patterns of landscape change after simple model parameterization in the spectral spatial domain pynita s metric image output currently offers something of a cognitive bridge between spectral and social ecological information for instance steger et al 2020 found that continuous rs map products offered comprehensible points of entry for interviews with residents about the causes and consequences of regional land change moreover pynita allows users to manually draw a set of well understood temporal trajectories e g researcher field sites or cultural landmarks using them as the basis for an automated parameter optimization procedure step 2b fig 3 the rationale is that this manual delineation acts as a mental stepping stone between user knowledge of a land cover change process and the representation of the process in spectral space conceptually analogous to timesync validation cohen et al 2010 as of this writing this optimization procedure is in its infancy and its full utility will be a product of future work still technical barriers to use of pynita have not entirely been eliminated the most prominent hurdle is data acquisition and preprocessing collection of calibration and validation data in the field notwithstanding while we have sought to make the software itself easy to use it is non trivial to produce a set of spectral index point data or an image stack suitable for analysis currently we provide straightforward code for data extraction in google earth engine code and video 2 in supplementary materials but fully acknowledge that this may not be considered straightforward for the uninitiated pynita is most suitable for local e g md to regional scale e g ethiopia analysis of phenomena that can be represented by either abrupt or gradual changes on the landscape in the case studies above we show relatively simple analyses leveraging both types of image information the selected case studies are also purposefully applied rather than theoretical or methodological in nature following these examples we envision that pynita will be best used by ses researchers looking for a straightforward way to apply their expert knowledge over broader spatial and temporal domains however we also seek tool adoption by a wider range of stakeholders including activists alonzo et al 2016 environmental monitors gaveau et al 2021 obida et al 2021 or even undergraduate students going forward we hope to maintain the collaborative spirit that led to the development of this tool by inviting further input from the community in the form of contributions to the open source code https github com malonzo47 pynita gui software availability software name pynita developer michael alonzo year first official release 2020 os requirements mac or pc for binary installation but software can be run from source in containerized environment program language python program size 83 mb availability https github com malonzo47 pynita gui documentation readme md on github page with links to workflow videos and extensive documentation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national socio environmental synthesis center sesync under funding received from the national science foundation dbi 1052875 the authors would further like to thank ken conca nabil ahmed and all sesync pursuit participants for their contributions to this project appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105179 
25720,the study of land change within social ecological systems ses is of great interest and increasingly makes use of remote sensing rs imagery to scale inferences up through space and time however spatial analysis using dense time series of rs data poses technical hurdles for non expert users to broaden the community of ses researchers using rs we present a simple tool for mapping land change at local to regional scales the python implementation of the noise insensitive trajectory algorithm pynita is accessed through a streamlined graphical user interface and requires minimal user parameterization to generate long term trends and identify key dates of significant change i e disturbance events based on time series of landsat or sentinel 2 data in this paper we introduce the pynita software explain the underlying algorithm analyze key parameter sensitivities and summarize methods and results from three ses case studies of land change keywords time series landsat social ecological land cover change software 1 introduction many environmental problems and conflicts center on disruptions mediated by social ecological systems sess collections of human and ecological agents and processes bound together by interaction and interdependence dietz 2017 lambin and meyfroidt 2010 deforestation flood and fire risk drought pressures land degradation and other issues emerge where human activity and ecosystem dynamics converge often when powerful human actors and institutions disrupt or pressure practices of people on the ground andersson et al 2011 pellow and nyseth brehm 2013 robbins 2004 to address such problems scientists governments and advocates have reason to seek deepened understandings of ses processes and accessible ways to monitor land change the tools of land change science turner and robbins 2008 which have grounded our insights into ses now make available high frequency remote sensing data and methods for interpreting them we have crafted such a tool pynita python noise insensitive trajectory algorithm an open source software with an easy to use graphical user interface gui for quantifying and mapping land cover change over time in this paper we introduce pynita and demonstrate its utility through several case studies in the last two decades mapping land cover dynamics with moderate resolution imagery has increasingly supported monitoring social ecological processes as they unfold on the landscape rather than simply detecting the end result of that process land cover change monitoring is commonly conducted through multitemporal satellite remote sensing rs rs platforms such as landsat and sentinel 2 systematically collect data relevant for characterizing land cover condition type extent and pattern at moderate spatial resolution i e 10 30 m across broad spatial extents at sub weekly through sub monthly time steps over many years li and roy 2017 earth observing satellites have provided revolutionary data for understanding ses processes such as deforestation and forest degradation hansen et al 2013 sedano et al 2020 agricultural intensification or abandonment alcantara et al 2012 seasonal flooding xiao et al 2005 and urbanization liu et al 2018 there is both increasing recognition and analytical capability by ses scholars that satellite rs data analysis has more to offer than visual interpretation of imagery or a before after i e bi temporal change comparison e g gillanders et al 2008 continuous repeated collection of rs data has allowed ses researchers to keep pace with the increasing speed of and spatial scale of social ecological change mace et al 2014 and to more systematically address core ses themes such as feedback loops lambin and meyfroidt 2010 system vulnerability and resilience that may play out over decades turner and robbins 2008 perhaps the most promising rs contributions to ses research have come through improved exploitation of the ever expanding satellite data temporal archives gorelick et al 2017 woodcock 2008 and more frequent new image acquisition claverie et al 2018 the base of users of rs data has increased substantially in recent years due to improved data availability computational processing resources and a move towards interdisciplinary science teams satellite data availability has increased massively in the last 10 15 years with the opening of the united states geological survey usgs nasa landsat archive going back to 1972 in 2008 woodcock 2008 the launch of landsat 8 in 2013 which offers nominal global coverage every 16 days at 30 m roy et al 2014 the launch of several european space agency esa sentinel earth observing satellites since 2014 berger et al 2012 and spectral harmonization among satellite sensors claverie et al 2018 li and roy 2017 similarly algorithms and image processing tools have become much more accessible due to the access of cloud computing resources e g google earth engine gorelick et al 2017 open source software and scripting languages e g r python and automated disturbance detection algorithms such as landtrendr kennedy et al 2018 and breaks for additive seasonal and trend bfast hamunyela et al 2020 verbesselt et al 2010 despite the many advances in image processing and opening of analytical frameworks several issues still impede widespread adoption of rs techniques for understanding specific ses processes pricope et al 2020 first there has long been an uneven distribution of rs research favoring protected forest ecosystems botkin and nisbet 1992 temperate zones whitcraft et al 2015 and wealthier countries martin et al 2012 fragmented and unprotected landscapes and non forested ecosystems particularly in developing countries remain understudied with rs despite their ecosystem vulnerability and need for conservation intervention trimble and van aarde 2012 second the majority of time series change detection methods for optical e g landsat and sentinel 2 satellite data verbesselt et al 2010 require time series data that are consistently collected and relatively cloud free conditions which are not often available in polar and tropical regions whitcraft et al 2015 including substantial portions of the global south third technical overhead may impede ses researchers without requisite training in analysis and interpretation of satellite and particularly time series data young et al 2017 similarly some rs researchers without theoretical grounding may be challenged to effectively relate land change information to the social ecological dynamics in question rindfuss et al 2004 fourth it is not a trivial task to integrate spatially continuous satellite data with spatially sparse or clustered ses field data e g interviews census socioeconomic data hecht 2014 steger et al 2020 in an attempt to broaden the community of ses researchers using rs data and diversify the impact of rs on ses research we present a new tool for satellite time series analysis our primary objective was to create a simple to use open source tool with low barriers to entry for researchers with limited rs experience the python implementation of the noise insensitive trajectory algorithm pynita adapted from alonzo et al 2016 is accessed through a graphical user interface gui and requires minimal user parameterization to generate long term trends and identify key dates and locations of significant change i e disturbance events based on a time series of multispectral data in this paper we introduce the pynita software explain the underlying algorithm analyze key parameter sensitivities and summarize methods and results from three distinct case studies of land change 1 differentiation of agricultural land uses in the colombian amazon 2 land cover classification for knowledge co production in ethiopia and 3 quantification of forest conversion to impervious surfaces for regional planning in maryland usa the pynita tool and the three case studies resulted from a collaborative data intensive pursuit facilitated and funded by the national socio environmental synthesis center sesync 2 background 2 1 remote sensing time series change detection algorithms the state of the art rs time series algorithms for terrestrial change analysis characterize multi year trends as well as breaks from those trends at the pixel level fig 1 huang et al 2010 kennedy et al 2010 with some algorithms additionally accounting for intra annual vegetation phenology verbesselt et al 2010 zhu et al 2012 the most common use of rs time series algorithms is for tracking land cover disturbances i e a significant change in land cover condition resulting from natural or anthropogenic factors particularly in forested ecosystems where an assumption of forest persistence and relative spectral stability year after year can aid the detection of prominent changes devries et al 2015 hansen et al 2013 murillo sandoval et al 2020 zhu et al 2020 current change detection algorithms are becoming more user oriented for instance landtrendr was originally only accessible as an interactive data language idl script kennedy et al 2010 but has recently been ported to google earth engine kennedy et al 2018 while landtrendr offers a set of spectral temporal segmentation algorithms finding the appropriate parameters for linear segmentation may pose a challenge to non expert users in contrast other algorithms available in gee such as continuous degradation detection coded bullock et al 2020 continuous change detection and classification zhu and woodcock 2014 and bfast hamunyela et al 2020 require fewer parameters to obtain useable results still these algorithms require computational resources and rs expertise there remains a need for a time series analysis tool that provides robust results and remains useable to the broader community who may lack an rs background establishing spectral trajectories from which one can assess inter and intra annual land cover dynamics for each pixel is also valuable for land cover classification at a single point in time several land cover classes may exhibit significant spectral similarity making them difficult to differentiate gómez et al 2016 for example primary forest and post clearing regrowth differ substantially in terms of disturbance history and ecosystem function but can appear very similar in a single date normalized difference vegetation index ndvi measurement a time series of ndvi however will capture disturbance history and can be distilled into useful metrics for augmenting a classification approach franklin et al 2015 showed that inclusion of normalized burn ratio nbr key and benson 2006 time series information such as overall trend persistence of largest disturbance and post disturbance duration helped raise the accuracy of a single year land cover classification by 6 4 compared to single year spectral metrics intra annual time series information is also useful for classification bargiel et al 2017 enhanced map classification accuracy of a northern european agricultural landscape by leveraging crop phenological information stored in a dense time series of sentinel 2 imagery for example summer barley classification accuracy average of users and producer was 51 5 with single date imagery and 74 when incorporating spectral crop cycle information 2 3 approaches to algorithm development the development of time series algorithms for use on rs data has to our knowledge been entirely in the domain of remote sensing computer or data scientists this leads to a limited view of the purpose of time series algorithms and whom they should ultimately serve thereby restricting uptake by disciplinarily diverse ses users this is in effect a top down approach to algorithm r d that relies on confidence in the credentials of the developer and the assumption of inherent data quality crampton 2011 such technical siloing limits the perceived value of rs data for non practitioners and can reduce their inclination to venture outside of their own disciplinary methods to investigate the benefits of rs data liverman et al 1998 by contrast a bottom up participatory design approach puts geospatial technology in the hands of the domain expert i e the person closest to the problem and best positioned to evaluate the solution goodchild 2007 steger et al 2020 in contrast to a top down approach bottom up collaboration with respect to algorithm development has multiple benefits design phase involvement of ses researchers immediately focuses the development effort on meeting on the ground needs rather than prioritizing incremental model improvement ses researchers are not monolithic and bring a diversity of viewpoints and visions for how time series of rs can help them extend their work spatially and temporally this diversity not only pushes against stubbornly ingrained disciplinary boundaries in traditional rs algorithmic design but may lead to a more useful product for a larger population working or collaborative groups tend to form based on member similarity proximity or familiarity leading to low information diversity i e diversity of knowledge bases and perspectives jehn et al 1999 cross functional teams such as those comprising rs scientists and ses domain experts have been shown to yield higher informational diversity and the potential to elevate group performance jehn et al 1999 early and steady participation in the algorithm design process also leads to end user buy in and ultimately more widespread adoption given that user needs are explicitly considered and addressed from the outset kushniruk and nohr 2016 moreover algorithmic usability and simplicity will likely be prioritized over complexity given the limited interest of ses collaborators in internal technical details of models 3 pynita methods and user workflow in the following paragraphs we briefly present the goal conceptual design workflow e g parameterization graphical user interface and key metric image outputs i e the distillations of pixel trajectories into meaningful statistics of pynita as well as our methods for conducting a sensitivity analysis comparing the python v3 7 implementation to a previous matlab 2015b implementation detailed in alonzo et al 2016 3 1 pynita algorithm pynita is a temporal segmentation algorithm tool for estimating per pixel land cover change trajectories at annual or sub annual time steps using rs time series data pynita can detect both abrupt and gradual changes using spectral indices such as ndvi or nbr and is highly flexible in response to user parameterization e g jamali et al 2015 pynita does not currently account for vegetation seasonality like ccdc zhu and woodcock 2014 bfast verbesselt et al 2010 or beast zhao et al 2019 in some contexts this is a limitation but the algorithm was designed for application in cloudy areas where modeling phenology is often infeasible because of data scarcity this development legacy gave rise to the noise insensitive moniker as we found that the influence of downwardly biased ndvi values due to atmospheric contamination could be minimized with appropriate parameterization pynita has two main subroutines build and subtract during build at each pixel a piecewise time series is fit to the data up to a user specified number of linear segments time series breakpoints are added based on the position of maximum orthogonal error calculated from the previous iteration until nbreakpoints nsegments max 1 subtract then iteratively removes segments to achieve a parsimonious fit e g fig 1 black line based on minimization of the bayesian information criterion alonzo et al 2016 3 2 pynita user workflow the pynita application can run on any commonly used operating system it is available for download https github com malonzo47 pynita gui as a binary release for windows and mac os which means that the user simply double clicks an executable file to run the software no knowledge of python or any other scripting languages is required advanced users may run source code either containerized in a docker environment to minimize system conflicts or locally the user has the opportunity to input either point or image data analysis of a spectral index e g ndvi time series is generally done first at a small set of calibration validation cal val locations where the researcher is familiar with land cover change history potentially including disturbance event timing for example the researcher may know based on fieldwork that the location point which translates here to a single image pixel depicted in fig 1 above was cleared of trees in november of 2005 the user then compares pynita s disturbance date against the known date i e reference data and updates the software s parameterization as needed once satisfied with pynita performance at their cal val points the user can then run the algorithm over an entire input image stack i e a chronologically ordered array of georeferenced satellite images to create metric image outputs fig 2 the spectral data from cal val locations and image stack files for use in pynita can be generated in google earth engine gee and include all necessary metadata image stacks may also be loaded without use of gee gee scripts shapefile examples and an instructional video can be found in the supplementary material as well as on the algorithm github page https github com malonzo47 pynita gui the gui structures the processing workflow through four tabs steps 1 2 shown in fig 3 accessed by the user in succession see basic workflow video 1 in supplementary materials in step 1 the user selects the directories for inputs and outputs and establishes a suite of 15 parameters including six critical parameters discussed below that underpin the pynita disturbance detection approach table 1 and that may be revised based on visualization or optimization results from step 2 the user first specifies their user vi with any spectral index si but commonly a vegetation index vi such as ndvi or nbr the user then defines a study period date limits and chooses day of year doy constraints i e the annual time range of analysis doy limits properly constraining doy is important to avoid modeling vegetation phenology as an annual disturbance doy limits is also a useful parameter on which to experiment because not all meaningful changes are best modeled during peak greenness which is commonly used to make annual composite images see video 3 in supplementary materials next the user specifies parameters that are geared towards modeling gradual and abrupt changes in light of varying degrees of atmospheric contamination or missing image dates for example filt dist controls the number of image dates over which error from iterative fitting is smoothed before further analysis for data with limited noise and acute disturbances followed by rapid recovery a filt dist of 3 would be reasonable while for noisy data with limited abrupt disturbances a value of 7 would be more appropriate fig 3 several parameters relate to fit complexity 1 bail thresh is the threshold at which pynita bails out of the build phase and returns only a single line fit appropriate for linear trends noisy data or computationally inexpensive initial data exploration see video 4 in supplementary materials 2 penalty is a component of the bayesian information criterion bic evaluation of model complexity during the pynita subtract phase where higher penalty values result in more parsimonious fits and 3 min complex sets a low bound on the number of segments to retain parameter superseded by bail thresh which is useful in places with known disturbances and generally not problematic for modeling undisturbed landscapes also at this point the user specifies the loss of si vi value vi change thresh that must be achieved to qualify as a disturbance as well as disturbance duration run thresh that distinguishes disturbance from decline in step 2 the user optimizes the time series model fit initialized in step 1 the user identifies a set of cal val locations steps 2a b fig 3a and views the model outputs at these sites additionally an automated optimization can be run to select the set of parameters that statistically minimizes the root mean squared error rmse between a small set of hand drawn trajectories and those that were automatically drawn see video 5 in supplementary materials in step 2c the user loads and can adjust a set of parameters see parameter set in fig 3a step2c which can be adjusted in search of the optimal parameterization that minimizes rmse once an optimized parameter set has been selected the user proceeds to steps 3 and 4 in which the full image stack is processed figure s1 in step 3 the user simply loads data chooses the number of processors over which to parallelize the processing task and runs the full analysis following processing in step 4 the output metric images e g disturbance date total si vi value change can be viewed and saved table 2 the user can interactively explore the trajectory underlying each metric image value by clicking on a pixel in any of the images fig 4 3 3 sensitivity analysis in order to ensure continuity of results after updating the algorithm and porting from matlab to python we performed a small sensitivity analysis on simulated data as in alonzo et al 2016 we simulated data based on 100 trajectories representative of a tropical forested riparian study area in papua indonesia at each trajectory we randomly generated data 100 times with realistic image date missingness temporal autocorrelation and downwardly biased error following alonzo et al 2016 algorithm parameterizations for matlab nita hereafter nita and python pynita implementations were identical we compared the deviations of nita and pynita estimated trajectories from simulated trajectories based on five key metric images disturbance date total value change value change pre 1998 value change post 1998 1998 being a pivotal year for landscape change in the papua case study and recovery four years after disturbance we also report omission and commission error for all disturbances as well as only for substantial disturbances where vi value dropped more than 0 7 which is typical for forest loss in the papua study area 4 results 4 1 sensitivity analysis while the nita and pynita implementations are nearly identical in their algorithmic underpinnings several changes were made in adherence with python coding best practices and to improve computational performance thus we sought to confirm that the outputs for key metric images were similar with respect to deviation from simulated results median deviation of pynita disturbance date from simulated disturbance data was 135 7 days while nita deviated by 274 8 days table 3 note that this timing bias in disturbance detection can be improved with additional user parameterization regarding which part of a pixel trajectory truly represents the date of disturbance parameter 16 fig 3a other value change and recovery metrics e g value change were broadly similar with no clear trend in terms of greater or lesser deviation from simulated data nita did perform slightly better in terms of lower disturbance detection commission error but both nita and pynita provided results that are within the range of existing time series disturbance algorithms table 3 zhu et al 2020 5 case studies here we present the results of three pynita case studies implemented by members of our sesync collaboration across a range of study topics and geographies fig 5 1 differentiation of coca plantation from cattle pasture following deforestation in colombia hereafter colombia 2 incorporation of pynita metric images in a land cover classification in support of knowledge co production in the ethiopian highlands ethiopia and 3 disturbance dating in suburban maryland usa to track the correspondence between increased impervious surface area and flooding md these case studies differ substantially in their biophysical climatic contexts and social ecological drivers of land change the colombia study site is between the andes montane and amazon forest biomes and characterized by gradual forest disturbance driven by a combination of human encroachment armed conflict and illicit land use activities the ethiopia site while also in the tropics is at high elevation 3200 to 3700 m above sea level and characterized by a patchwork of farmland grassland shrubs and forest that respond differently to intra and interannual variability in rainfall and human activity the md site offers a strong contrast to the others by virtue of its temperate seasonality and the dominance of the built up impervious land cover resulting from residential and commercial development 5 1 land activities in colombian protected areas colombia in the picachos tinigua macarena protected areas pas in southern colombia coca farming and cattle pasturing have brought widespread changes to this protected forest biodiversity hotspot since 1970 etter et al 2006 though documented anecdotally and intermittently across the pas these two land uses have thus far defied accurate rs detection in part because of the persistent cloud cover across the region to identify coca plantation and cattle pasture we 1 parameterized pynita to detect forest disturbances 2 classified annual landsat imagery using pynita and other metric image outputs and a random forest rf classifier breiman 2001 and 3 assessed the accuracy of classified forest coca and pasture pynita was parameterized to detect disturbances using a normalized difference moisture index ndmi time series through purposive sampling of 160 locations with known disturbance events we selected ndmi because of demonstrated high accuracy for tracking tropical deforestation devries et al 2015 schultz et al 2016 and limited the study period to after 1997 when landsat scene availability in the study area improved compared to the 1980s and early 1990s using the pynita gui we narrowed the doy limits to the range of january 1 to march 20 see table 4 coinciding with the dry season filt dist was set to 3 vi change thresh to 0 1 and filter opt to sgolay in order to maximize detection of small acute disturbance and recovery processes pct was set at 60 since downward biases common due to atmospheric noise in ndvi are less problematic with indices that do not include visible wavelengths of light e g ndmi we used four ndmi time series metrics output by pynita as inputs to our rf classifier disturbance magnitude distmag recovery two years after disturbance recovery2 post disturbance magnitude postdistmag and total value change valuechange table 2 these metrics are useful to capture abrupt and large scale land use conversion e g forest to pasture or forest to coca as well as partial forest alteration e g coca dynamics recovery is useful to identify coca plots because they are commonly abandoned leading to mixed shrub and secondary forest regrowth postdistmag allowed for tracking the spectral signal immediately after clearing while valuechange captured the ndmi trend over time we also used the annual median value for each landsat spectral band and the elevation and slope based on a 30 m digital elevation model as inputs to our classifier we trained our model by sampling the above metrics at training sites that were labeled as coca pasture or forest yearly from 2010 to 2018 our field data for coca was obtained from integrated illicit crops monitoring system through visual delineation on satellite imagery and confirmation by aerial inspection unodc simci 2018 pasture field data came from previous published research and the corine land cover dataset murillo sandoval et al 2018 we ran the classifier on landsat time series imagery from 2010 to 2018 and assessed the accuracy of the output land cover maps using an out of bag oob estimator field maps and classified maps based on pynita metrics were generally in high agreement for detecting changes associated with the expansion of coca and pasture fig 6 a even when both land covers are located in close proximity they were effectively discriminated by the classifier fig 6b however isolated coca patches were sometimes confounded with small pasture plots and coca suffered from higher commission and omission errors i e 25 compared to forest 12 or pasture 7 fig 6c these results benefited from the inclusion of pynita metrics which decreased the oob error rate from 25 without pynita metrics to 20 including pynita metrics the five most important variables based on gini impurity driving classification accuracy in order of importance included two landsat spectral bands swir1 and nir as well as three pynita metrics based on ndmi valuechange postdistmag and distmag in particular postdistmag effectively distinguished coca from pasture since the metric is generally higher and more variable for pasture than coca these results indicate that long term trends and specific time series temporal metrics output by pynita facilitated the accurate discrimination of coca and pasture murillo sandoval et al 2018 shimizu et al 2017 while random forest classification of the raw time series of spectral values may have similarly improved classification accuracy our method additionally provided interpretable metric images with physical or social ecological meaning hermosilla et al 2015 senf et al 2015 this small case study shows that pasture is more spectrally diverse over space and time with patches of bare soil and periods of rapid pasture growth by contrast coca is relative spectrally stable in this tropical region cultivated continuously over the same locations future studies will benefit from analyzing the spatial context of pixel based pynita outputs e g shape based and deep learning approaches to better disentangle the occurrence of these two important land covers in colombia 5 2 land cover classification using temporal metrics in ethiopia ethiopia knowledge co production that draws on local and indigenous knowledge as well as scientific knowledge e g satellite rs presents opportunities for developing a more holistic understanding of environmental change and improved management of ses dietz et al 2003 folke 2004 mclain and lee 1996 we applied a knowledge co production approach tengö et al 2014 to investigate the causes and consequences of environmental change in a community protected grassland and its surrounding landscape in the ethiopian highlands fig 7 a and b we sought to first understand how local residents perceived the landscape i e what land use cover classes they considered useful and how they perceived them changing over time and then used rs to calculate the spatial extent of those classes across the study area and quantify how vegetation quality has changed over time based on nine locally relevant land cover classes articulated by residents i e plantation forest grazing land farmland stone bare land shrubland native forest protected grassland and water we trained a rf classifier using metrics derived from a december 10 2016 landsat 8 surface reflectance image and pynita metrics based on the normalized burn ratio nbr from all available images collected in the wet doy limits 9 129 and dry doy limits 200 314 seasons from 1985 to 2019 date limits of 9999 9999 so as to include all available data wet and dry season boundaries were defined by local participants according to their management activities and the dates of the ethiopian calendar in this environment with high cloud cover and missing data before 1999 we used a higher filter distance filt dist of 7 table 4 for all parameters which helped us to more robustly capture broad trends albeit at the expense of making it more difficult to capture acute disturbances accordingly min complex was set low 2 for wet season and 1 for dry season as capturing 3 part disturbance dynamics was not relevant in this landscape ultimately we input 25 variables into the land cover classification algorithm including 12 metric images generated by pynita 6 wet season and 6 dry season linear error linerror normalized linear error bailcut total value change valuechange and three value change periods of particular interest to the local communities based on changing political and management regimes 1991 1991 2003 2004 2012 valuechange single date variables included 2016 30 m optical landsat 8 image bands and three derived tasseled cap composites brightness greenness wetness kauth and thomas 1976 as well as elevation aspect and slope derived from a 30 m aster global dem abrams et al 2020 the rf classifier was calibrated and validated against a dataset of 3244 data points compiled from field visits participatory mapping exercises and high resolution imagery across nine land cover land use classes defined by participants we achieved a total accuracy of 85 7 kappa 0 84 which is markedly better than the 80 5 total accuracy kappa 0 78 measured without using pynita time series metrics with only single date and topographic inputs to the classifier the accuracy of five classes i e water protected grassland bare land grazing land and plantation forest accuracy exceeded 80 with the inclusion of pynita time series metrics eight of nine classes exceeded 80 average accuracy with particularly large percentage point pp improvements for plantation forest 7 7 pp farmland 11 4 pp and stone 12 6 pp the culturally and ecologically important land cover of protected grassland was also more accurately classified using pynita time series metrics with an increase of 2 8 pp to 92 average class accuracy the most important variables based on mean decrease in accuracy upon exclusion across classes were elevation pynita linerror fig 7d and greenness highlighting the importance of including multiple sources of data with complementary spectral temporal and physiographic information content farmland accuracy increased dramatically 11 4 pp with wet season pynita linear error most responsible for the improvement previously differentiation of farmland and stone relied heavily on variation in greenness which is only adequate for fields that are under cultivation at the time of imaging compared to stone and bare land pynita linear error for farmland is high because crop phenology and rotation cannot be adequately modeled by a simple linear trend plantation forest classification accuracy increased largely because of information encapsulated by nbr value change during the dry season fig 7c these results illustrate the importance of algorithmic flexibility with respect to targeting specific doy ranges as well as the value of drawing on local knowledge for selecting meaningful ranges here wet season nbr values were slightly increasing over time but largely stable and in line with the wet season trajectory of native forest by contrast the dry season nbr trajectory requires a piecewise fit to model eucalyptus and cypress planting that was widespread in and around 2005 these introduced species tend to retain greenness even during the dry season while native species do not protected grassland shrubland and grazing lands were all somewhat difficult to discriminate on the basis of single date metrics due to their spectral and structural similarity as well as their patchiness at sub landsat pixel spatial scale distinguishing between these classes is of significant cultural and economic importance for the local case study and other areas of the ethiopian highlands as shrub encroachment increasingly threatens the sustainability of afroalpine grasslands steger 2020 here we found that classifying these challenging cover types required a wide range of data most importantly information about vegetation and soil moisture content from shortwave infrared bands elevation pynita linear error and dry season change in nbr this work has laid a foundation for reaching a common understanding of current land use cover among members of the collaborative management team who have very different worldviews and approaches e g local farmers scientists conservation managers having an agreed upon map provides a reference for understanding the manifestations of past change in the region and a launching point for examining future change in this area specifically relating to shrub encroachment and climate driven alterations to agricultural productivity 5 3 dating land conversion to impervious cover in a high flood risk area md ellicott city maryland md usa experienced two 1000 year rainfall events within a two year period 2016 and 2018 producing devastating flooding property damage and fatalities the rapid and widespread development of roads and housing over the second half of the 20th century has increased the proportion of impervious surfaces largely at the expense of forest in the hills above the city increasing lowland flood risk dramatically in response the county government has been strengthening requirements for stormwater management by new properties with key improvements implemented in 1985 2002 and 2010 given increased flood risk from the construction boom and decreased flood risk coming simultaneously through implementation of regulations and retention facilities it is useful to assess land cover changes more comprehensively for the time period corresponding to each mitigation strategy here we used pynita to quantify the extent of land cover change within the three key periods 1985 2002 2002 2010 2010 2019 by estimating the date of each pixel s disturbance and by examining long term loss of vegetation pynita parameters were selected through automated optimization fig 3 step 2c video 5 supplementary materials using ndvi imagery 869 images with 80 cloud cover from 1984 to 2019 and hand drawn trajectories of disturbance pixels and stable vegetation pixels we assessed the quality of our parameter set table 4 through comparison with disturbance dates estimated at 60 hand selected validation points the validation points were selected based on the ability to determine the disturbance year within 2 years using historical high resolution imagery available through google earth doy limits were set for northern hemisphere summer to dampen the influence of vegetation phenology we selected a small filt dist of 3 because data quality was high and missingness was low as is typical over the continental united states and in contrast to the two case studies above this ultimately allows for greater precision in disturbance dating min complex was set to 5 since model parsimony was less important than precise disturbance characterization once a trajectory was generated we allowed the time over which a decline could qualify as disturbance to be high at 3650 days 10 years this was warranted for our application because we are most interested in land transitions within regulatory periods of 8 years fig 8 disturbance dates estimated from pynita trajectories were accurate within 2 years of the validation point dates 93 and 1 year 89 of the time no false positives were detected on stable landscapes such as persistent forest or impervious cover from development prior to 1984 in our tiber sub watershed study area fig 9 we found that 3 16 km2 33 of land was disturbed within our study period in our first 1987 2001 second 2002 2010 and third 2010 2019 regulatory periods 0 96 km2 10 2 0 km2 21 and 0 20 km2 2 were disturbed respectively indicating rapid residential development in the first decade of the new millennium while pynita provides an accurate representation of disturbance that corresponds well with both large scale e g industrial and smaller scale e g single family residential development this acute assessment may not ultimately be of interest to flood planners industrial and institutional disturbance likely correspond well with persistent increase in impervious cover but residential disturbance does not due to planting of lawns and other vegetation beyond disturbance with pynita one can also report on valuechange table 2 of the spectral index over any set of time periods this value change indicator is agnostic to the driver of change and is not sensitive to disturbance detection parameters we found that 31 of land area decreased in ndvi by greater than 0 1 indicating substantial decrease in vegetation cover over the three regulatory periods ndvi decreased more than 0 1 in 1 33 km2 13 8 1 08 km2 11 2 and 0 58 km2 6 0 respectively fig 9 this finding highlights similar losses between the two presented methods disturbance detection and long term value change due to the user selected vi change threshold of 0 1 if this threshold were empirically determined to be more appropriate at for example 0 2 the total loss area is substantially reduced from 31 to 14 1 in its support of multiple approaches to documenting land cover change these results illustrate the contributions of pynita in both a policy making context and as part of more in depth social ecological research on flooding in ellicott city 6 synthesis and conclusions the three case studies above suggest the range of applications and the benefits of flexibly implemented rs time series as is now well established in the literature there is a clear need for high frequency observation bi temporal analysis could not adequately match the temporal resolution of the social ecological processes under study gillanders et al 2008 reynolds hogland and mitchell 2007 in colombia pynita allowed for ingestion of sub annual ndmi data necessary to capture change following for instance a 2016 peace agreement that resulted in increased cattle ranching and coca cultivation in ethiopia high frequency data evaluated at multiple expert defined doy ranges revealed differing trajectories of plantation forest structure and moisture depending on time of year md made use of all available landsat data in pynita to both accurately date urban development and to estimate overall downward trends in vegetation cover low frequency data availability would not allow for comparison of land cover transitions with specific stormwater mitigation efforts in this urbanizing context while all case studies were reliant on high temporal resolution satellite data achieving the most useful pixel histories in these dense time stacks additionally demanded distinct parameterization for each a goal in the development of pynita was to demand minimal parameter adjustment from end users while allowing for substantial modification as warranted the case studies demonstrate that there are broad similarities among the implemented parameter sets but important differences based on goals and data availability and quality table 4 the choice of spectral index is a critical decision for any univariate time series analysis zhu 2017 in our case studies colombia and ethiopia captured abrupt or subtle changes in vegetation structure and moisture with nir and swir bands while md highlighted stark transitions from vegetation to impervious surface in md it is likely that any standard vegetation index would suffice due to the extreme nature of the changes to the landscape users are encouraged to experiment with adjusting the doy limits parameter based on their understandings of local phenology cloud cover or other seasonally important ecosystem dynamics in ethiopia the researchers gathered expert knowledge from local ethiopian residents regarding the typical doy ranges for the wet and dry season ultimately leveraging seasonality information to improve classification results in md the research team sought to minimize the influence of phenology through selection of only temperate summer doy the ease with which doy can be adjusted and the results visually confirmed in the pynita gui is a feature of this software but in cases where broader scale mapping is desired or expert knowledge is unavailable algorithms such as bfast or ccdc are likely preferred as they are more fully data driven verbesselt et al 2010 zhu and woodcock 2014 the decision of whether to prioritize disturbance detection or more gradual changes is reflected in multiple parameters but notably in filt dist and filter opt colombia and md sought to characterize disturbance timing and magnitude albeit under very different environmental conditions and thus selected a narrow 1 d filtering kernel filt dist 3 paired with the savitzky golay sg filter opt sg filtering has proven useful in smoothing while preserving higher statistical moments of data in rs time series chen et al 2004 particularly for md where images were consistently available fig 9 and disturbance date accuracy was paramount these filter options were critical for estimating disturbance date within 1 year of the observed high resolution land cover transition by contrast ethiopia did not have a primary goal of disturbance discovery and the ethiopian study location was subject to data issues from persistent cloud cover and missing images due to limited data downlink capabilities in the 1980s and 1990s wulder et al 2016 therefore a filt dist of 7 and a movcv moving coefficient of variation filter opt were appropriate to capture broad trends aligning with political and management changes steger et al 2020 while remaining relatively insensitive to noise from imperfect cloud masking the ability to detect both acute disturbances and gradual changes has been an important advance in rs time series methods as it reflects the multiple modalities of environmental change that both may for instance underpin national policy e g reduced emissions from deforestation and degradation redd goetz et al 2015 souza et al 2013 or shed new light on more difficult to detect anthropogenic drivers of forest change such as charcoal harvesting sedano et al 2020 and illicit land activities tellman et al 2020 since the landsat archive was opened for free use in 2008 many change detection algorithms have been developed so what is pynita s particular contribution following the time series algorithm taxonomy of zhu et al 2017 pynita would be considered a segmentation algorithm making use of high frequency e g ccdc zhu and woodcock 2014 univariate e g spectral index pixel scale data in an offline mode capable of detecting both abrupt and gradual change it may then be considered most akin to the detecting breakpoints and estimating segments in trend dbest jamali et al 2015 algorithm specifically with respect to its focus on user parameterization flexibility and lack of a seasonal modeling component we thus put forth pynita not in an attempt to win an algorithm comparison contest but to lower the barrier to ses researcher adoption through improved usability to this end we benefited from an algorithm development process incorporating the stated needs and real time feedback of the 15 ses researchers in our sesync pursuit annapolis md 2018 2019 in the pynita case we primarily lower the barrier through implementation of a graphical user interface and easy visualization and interaction with data this priority is consistent with recent efforts to bring remote sensing to a broader audience through for instance publishing a survival guide to landsat preprocessing young et al 2017 and the introduction of google earth engine to improve accessibility to cloud based remote sensing image analysis at planetary scale gorelick et al 2017 despite much progress there are still barriers to non expert entry in the field of rs time series analysis fundamentally it remains challenging to non specialists and frequently specialists to link unique observable patterns from satellite data with social ecological drivers of land cover or land use change on the ground i e socializing the pixel geoghegan et al 1998 while these cognitive translations remain an area for continued research pynita can serve to test hypothesized patterns of landscape change after simple model parameterization in the spectral spatial domain pynita s metric image output currently offers something of a cognitive bridge between spectral and social ecological information for instance steger et al 2020 found that continuous rs map products offered comprehensible points of entry for interviews with residents about the causes and consequences of regional land change moreover pynita allows users to manually draw a set of well understood temporal trajectories e g researcher field sites or cultural landmarks using them as the basis for an automated parameter optimization procedure step 2b fig 3 the rationale is that this manual delineation acts as a mental stepping stone between user knowledge of a land cover change process and the representation of the process in spectral space conceptually analogous to timesync validation cohen et al 2010 as of this writing this optimization procedure is in its infancy and its full utility will be a product of future work still technical barriers to use of pynita have not entirely been eliminated the most prominent hurdle is data acquisition and preprocessing collection of calibration and validation data in the field notwithstanding while we have sought to make the software itself easy to use it is non trivial to produce a set of spectral index point data or an image stack suitable for analysis currently we provide straightforward code for data extraction in google earth engine code and video 2 in supplementary materials but fully acknowledge that this may not be considered straightforward for the uninitiated pynita is most suitable for local e g md to regional scale e g ethiopia analysis of phenomena that can be represented by either abrupt or gradual changes on the landscape in the case studies above we show relatively simple analyses leveraging both types of image information the selected case studies are also purposefully applied rather than theoretical or methodological in nature following these examples we envision that pynita will be best used by ses researchers looking for a straightforward way to apply their expert knowledge over broader spatial and temporal domains however we also seek tool adoption by a wider range of stakeholders including activists alonzo et al 2016 environmental monitors gaveau et al 2021 obida et al 2021 or even undergraduate students going forward we hope to maintain the collaborative spirit that led to the development of this tool by inviting further input from the community in the form of contributions to the open source code https github com malonzo47 pynita gui software availability software name pynita developer michael alonzo year first official release 2020 os requirements mac or pc for binary installation but software can be run from source in containerized environment program language python program size 83 mb availability https github com malonzo47 pynita gui documentation readme md on github page with links to workflow videos and extensive documentation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national socio environmental synthesis center sesync under funding received from the national science foundation dbi 1052875 the authors would further like to thank ken conca nabil ahmed and all sesync pursuit participants for their contributions to this project appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105179 
25721,this paper presents an innovative deep learning dl framework to a automatically identify river geometry and flood extent and b predict river flooding depth to do that u net an advanced convolutional neural network cnn was modified and given the designation of u netriver with the modification the model received an input composite image with two bands of ground elevation and flooding discharge and the output was water depth the model was trained and validated based on the outputs from iric a two dimensional hydraulic model for a segment of the green river in the state of utah the results showed that the u netriver could identify the river shape and wetted areas for flooded regions automatically the maximum difference of predicted river depth obtained from u netriver and the one obtained from the hydraulic model was 2 7 m this result suggests a 29 improvement in prediction of the maximum flood depth in the river keywords flood inundation modeling river hydraulics machine learning deep learning convolutional neural networks u net 1 introduction future flooding disasters are expected to increase in both frequency and magnitude due to climate change h hu et al 2019 leandro et al 2020 pathan and agnihotri 2021 tang et al 2020 wang et al 2019 management and mitigation of such future global challenges demand a substantial allocation of planning and preparation now and this would require wide ranging research in this field currently further studies are needed to quantify large scale fine resolution flood hazards and such studies should account for flooding extent over multiple countries shao et al 2017b moreover faced with increased global flooding decision makers and engineers must devote themselves to developing new sets of tools and efficient models for large scale flood simulations for multiple countries or continents large scale flood inundation maps can successfully demonstrate flooding risk to residents shao et al 2017a moreover quick access to flood extent mapping would be crucial for emergency responses gebrehiwot et al 2019 however large scale both spatial and temporal flood resilient analysis is not now conveniently available due to limitations in current large scale flood modeling wang et al 2019 moreover even if such maps are available in large scale formats they need to be updated very frequently to present accurate information shao et al 2017a flood risk assessment is becoming an urgent priority for many countries and this calls for a range of rapid versatile and efficient approaches yang et al 2011 along with models that can easily be exchanged between users leskens et al 2014 rapid prediction of flood extent is necessary for flood emergency services but such predictions are usually limited due to technical constraints such as model inflexibility long and expensive computation time and organizational constraints which do not allow the knowledge of the model to be well circulated leskens et al 2014 the usefulness of a hydraulic model for flood risk assessment depends on 1 the quality of the outputs and 2 the quickness and versatility of the model leskens et al 2014 as such the information and knowledge gained from a complex hydraulic model may not be incorporated into flood risk management due to the model s sophistication uncertainty and large computational expenses this means that many hydraulic models may not be useful for flood risk assessments specifically when there is an urgent need for a quick flood simulation leskens et al 2014 physics based hydraulic models have been traditionally used for simulating the flood extent and depth such models solve a set of partial differential equations associated with conservation of mass and momentum called navier stokes equations ns or simplified versions of ns such as shallow water equations janna 1993 leandro et al 2020 wang et al 2019 solving such equations for a large scale area specifically for two or three dimensional 2d 3d flow is extremely time consuming and a prohibitively expensive undertaking chu et al 2020 hosseiny et al 2020 liu and pender 2015 moreover addressing uncertainty in flood inundation maps is a laborious task as it requires a sequential run of the model chu et al 2020 leskens et al 2014 zarzar et al 2018 as such most of the previous studies on flood inundation risk and uncertainty analysis are limited and are difficult to adopt for other scenarios h hu et al 2019 recent advancements in computer science and vision coupled with a substantial increase in computing power have enabled researchers to explore the ability of machine learning ml approaches to solve their longstanding scientific problems although the field of water resources engineering has benefited somewhat from the applications of ml it has lagged behind some other fields of research in areas such as remote sensing natural language processing and the field of medicine ma et al 2019 pouyanfar et al 2018 there is an increasing need among decision makers to have ready to use hydraulic based flood simulation models advancements in computer power leskens et al 2014 and the applications of ml tools have shown promise for flood depth prediction for instance several studies have used ml tools for urban pluvial flood predictions ke et al 2020 moy de vitry et al 2019 moy de vitry and leitão 2020 sun and scanlon 2019 yan et al 2018 in spite of the comparable societal damage and cost of riverine and pluvial flooding jiang et al 2018 moy de vitry et al 2019 much less attention has been paid to applications of ml to river flood modeling among those few studies hosseiny et al 2020 introduced a framework that used two ml tools for predicting river flood extent and depth the first tool used a random forest model breiman 2001 as a binary classifier to identify the extent of wet or dry areas in a riverine flooding event the second utilized an artificial neural network ann model haykin 2008 as a regressor to estimate the flooding depth in wet areas hosseiny et al 2020 reported that the maximum error of predicted flood depth in a river took place at the river bend their results showed that the difference between the depths obtained from the hydraulic model and the ann was up to 3 8 m similarly chu et al 2020 used a hydrodynamic model to train an ann based emulation model for flood depth prediction they reported that the root mean squared error rmse of the predicted depth of the test data varied from 0 17 to 0 3 they concluded that predicted depths by an ann model were greater than the ones obtained from the hydraulic model by 2 4 15 8 folds while both studies reported that the ann models were remarkably faster than the hydraulic models their reported errors in predicting river depth suggest the limitations of ann models for efficient learning from a training dataset while both studies suggested that the ann model needed to be improved the spatial distribution of the errors in the study by hosseiny et al 2020 suggests that the ann model could not capture the relation between a river s geometry including the pattern bend shape and bathymetry and river depth the applications of ml in flood modeling are moving slowly from simple techniques such as ann toward more advanced deep learning dl techniques dikshit et al 2020 convolutional neural networks cnn is one of the most substantial and successfully applied dl models ma et al 2019 pouyanfar et al 2018 among the cnn based models u net ronneberger et al 2015 has shown an exceptional ability to extract features and to learn the patterns within data even with minimum data for training jakovljevic et al 2020 liu et al 2020 ronneberger et al 2015 ml and dl can be of great benefit to water resources engineering and specifically flood inundation modeling because they can 1 extract features and information from large amounts of data 2 handle large scale data which would be more practical using interdisciplinary approaches and 3 discover new approaches to help resolve previously challenging tasks sun and scanlon 2019 the hybrid of hydraulic models and ml tools can generate surrogate efficient models for large scale flooding dikshit et al 2020 hosseiny et al 2020 xie et al 2020 heretofore little attention has been given to spatio temporal predictions of flooding r hu et al 2019 moving toward dl models makes much more feasible the formulation of such predictions dikshit et al 2020 presently the applications of dl models in predicting the hydraulics of flooding such as flooding depth have not been investigated to fill the gap in applications of machine learning ml and deep learning dl in flood inundation modeling this paper introduces a pioneer deep learning hydraulic based model designated as u netriver which is a modification of the u net model ronneberger et al 2015 u netriver is a novel framework for predicting flood inundation extent and flooding depth in rivers hosseiny et al 2020 proposed the application of artificial intelligence to river flood predictions the author s hypothesized that their hybrid model which combined traditional hydraulic modeling with artificial neural networks would improve the speed efficiency and efficacy of modeling would lower computational costs would be more versatile and would increase abilities for predicting future flood scenarios this paper proposes to take the hybrid model a step further by applying deep learning to 1 enhance the accuracy of the hybrid model and to 2 make the ml model more objective for the sake of comparison the proposed model in this work is trained with the same data and applied to the same study area as those used by hosseiny et al 2020 the input to the u netriver takes the form of a composite image with two channels bands of elevation and discharge the output of the model is a single band image for depth this research introduces an innovative application of an advanced cnn model designated as u netriver in flood inundation modeling laying the foundations for efficient accurate large scale flood simulations the structure of this paper includes presenting the methodology in section 2 study area hydraulic model and deep learning model results and discussion in section 3 and limitations and conclusion in sections 4 and 5 respectively 2 methodology this section describes the location and the data compiled for the study area along with the concepts structure and methods used for developing both hydraulic and dl models 2 1 study area the river segment selected for this study is 3 5 km of the green river located in the united states in the northeast corner of the state of utah close to the border of the state of colorado fig 1 shows the location of the study area in utah along with an aerial view of a segment of the river which contained the study area the selected segment of the green river for this study meanders downstream of the flaming gorge dam and has a river width varying from 100 to 150 m depending on its lithology hosseiny et al 2020 the presence of sediment island and meanders in the selected segment of the river for this research ensures water depth variations along and across the river such variations are desired characteristics for testing the performance of the model in depth predictions all data required for hydraulic modeling of this segment of the river including topographic data and measured water surface elevation are publicly available from the united states geological survey usgs nelson 2019 2 2 hydraulic model the international river interface cooperative iric was used to generate training and testing datasets for this research 2 2 1 iric iric is a numerical platform for hydraulic simulations and includes a variety of solvers for predicting flooding rainfall and runoff geomorphological alterations and other phenomena the selected solver for this study was the program flow and sediment transport with morphological evolution of channel fastmech fastmech is a 2d hydraulic model for flow and sediment transport in rivers and is capable of addressing morphological evolution of channels nelson 2019 fig 2 shows the setup of the model in iric 2 2 2 calibration the hydraulic model was calibrated by changing the roughness coefficient which includes both skin and form roughness values within the model in the form of drag coefficient so that the model could accurately predict the measured water surface elevation for a discharge of 247 m3 s a drag coefficient of 0 008 was used for the computational domain which resulted in an rmse of 0 05 an exception was an area where an island in the middle of the domain splits the river for which the drag coefficient was set to 0 013 that resulted in a decrease in rmse from 0 05 to 0 03 the steps procedures and data for the calibration of the green river model are well documented by the usgs and are publicly available on the iric website nelson 2019 more information and description about model calibration can be found in the paper by hosseiny et al 2020 2 2 3 discharge data the daily discharge data of the green river in the study area from 1999 to 2019 was obtained from the usgs gage number 09234500 near greendale as in the study by hosseiny et al 2020 seven discharges of 10 50 95 120 150 300 and 400 m3 s were selected for further analysis the hydraulic model was run for selected discharges and outputs were obtained for mesh coordinates ground elevation water depth and water surface elevation the selected outputs of the hydraulic model for the seven discharges were used for training and validating the dl model further for testing the dl model five discharges of 20 30 45 225 and 350 m3 s were selected similar to the testing discharges used by hosseiny et al 2020 it is important to note that the outputs for testing discharges were not used in the training process the output of the hydraulic model for both the training and testing discharges included 47 047 data points and this was associated with the mesh size of 5 5 m in the iric model 2 3 deep learning model u net is an advanced dl algorithm and a model structure based on the concept of the cnn this section describes the structure of the cnn and u netriver and the framework that was developed for predicting flood depth and extent in rivers 2 3 1 cnn convolutional neural networks cnn are amongst the most popular and most substantial deep learning algorithms due to their ability to extract features from data with minimum human effort and knowledge pouyanfar et al 2018 making the feature selection less subjective to the user s choice is extremely important as different feature selection algorithms may result in a totally different outcome of the model yang et al 2011 cnn is a feed forward model where the weights and biases get updated by backpropagation through a supervised learning algorithm gebrehiwot et al 2019 in a two dimensional image the convolution is defined as 1 x i j u 1 m v 1 n x i u 1 j v 1 f u v b where x i j is the two dimensional image array 1 i m 1 j n f u v is the convolution filter 1 u m 1 v n and b is the bias liu et al 2020 sun and scanlon 2019 an activation function is then usually applied to the output of the cnn which enables the model to capture non linear behavior in the data gebrehiwot et al 2019 cnn has shown excellent performance in feature extraction and image segmentation especially for large scale image analysis each convolutional layer in cnn extracts descriptive features such as edges or textures in the data by generating a hierarchical descriptive feature map gebrehiwot et al 2019 sun and scanlon 2019 this descriptive feature map enables the cnn to learn local patterns and share them across other locations within an image flood et al 2019 2 3 2 u netriver u net is a specific structure of a cnn and it was introduced in a world wide image processing contest when it outperformed all the previous best models in image segmentation and needed very few images for training ronneberger et al 2015 the model has a symmetrical structure similar to the letter u and includes a contracting path left side of the u called the encoder or down sampling which produces a low resolution accurate semantic feature map jakovljevic et al 2019 to extract the texture the expanding path right side of the u called the decoder or up sampling retrieves fine spatial information jakovljevic et al 2019 along with capturing the local information in the image ronneberger et al 2015 the dl models that are based on cnn including u net mimic the human visual cortex by applying different filters and learning automatically from features generated from the hierarchical structure of the model lecun et al 1998 the hierarchy of the model includes multiple sequential layers of convolution pooling and possibly fully connected layers ma et al 2019 each convolution layer extracts a feature map by convolving the input array with a set of filters and adding the bias a nonlinear transformation which is usually called an activation function then is applied to the output of each layer any filter in the model with the dimension of n n covers an n n window of cells in an image array the filter slides along the image and aggregates the values of the covered cells by the dot product pouyanfar et al 2018 the convolution conceptually takes into account the effects of the neighboring cells in segments of an image pouyanfar et al 2018 in other words the filter technique enables the cnn to capture the texture and geometry in the array of an image as such the filters make the model sensitive to a section of the image rather than the whole image in a fully connected ann model and extract local correlations among the input data pouyanfar et al 2018 this is a conceptually critical quality of an ml model when it comes to river flood modeling where the river curves and patterns within the topographic data are influential this potential ability of the u net model to learn from geometry makes the u net a perfect candidate for this research the dl model developed for this research u netriver is a modified version of the original u net model the input to the u netriver is a composite image array of 128 128 pixels with two channels similar to an rgb image but with only two colors as opposed to 572 572 pixels and one channel in the u net model this number of pixels 128 128 for the input to the u netriver was selected based on the dimensions of the topographic raster file for the study area 260 251 this divided the study area into 4 regions for further analysis fig 1 the output of the u net model is composed of a two channel image array with the dimensions of 388 388 pixels the dimensions of the output array from the u netriver remains the same as the input 128 128 pixels with only one channel this modification is made so that the u netriver receives both topographic data and discharge data and predicts the depth at the output similar to the hydraulic model the first and second channel in the input composite image represent ground elevation and flood discharge respectively the output of the model is a one channel image of river depth the contracting path of the u netriver is composed of the following four cascading steps 1 a 3 3 convolution 2 a dropout ratio of 0 1 of the output data to prevent overfitting xie et al 2015 in the model 3 a 3 3 convolution 4 selecting the maximum number resulting from the previous steps maxpooling this sequence of actions generates low level features qamar et al 2020 and changes the dimension of the input array from 128 128 2 to 8 8 256 where 2 and 256 denote the number of the channels bands the expanding path of the u netriver is symmetrical to the contracting path while the number of the channels decreases through five cascading steps 1 a 2 2 convolution up convolution 2 concatenate features from the contracting path into the expanding path 3 a 3 3 convolution 4 a dropout ratio of 0 1 of the output 5 a 3 3 convolution the expanding path generates high level features qamar et al 2020 at the end of the path the feature map takes the dimensions of 128 128 1 the concatenation skip connections in u net preserves features and boundaries in the encoder in parallel with the convolutions khanna et al 2020 and transfers them directly into the decoder moy de vitry et al 2019 qamar et al 2020 the final output of the u netriver is an image array with the pixel dimension similar to the input tile but with one channel 128 128 1 representing the estimated depth of the river fig 3 shows the structures of the u netriver each convolution naturally reduces the dimensions of the image depending on the size of the filter to maintain the same size for the images after the convolution some extra pixels with zero values padding were added around the edges of the image moreover the strides number of cells to skip in x or y direction when the filter moves along the image in the contracting path were set to 1 in both directions this caused the model to maintain the image dimensions after the convolution the up convolution in the expanding path of the u netriver however had a filter size of 2 2 and a stride of 2 2 which resulted in doubling the number of the channels at each step the final convolution filter and stride of 1 1 changed the feature map channels from 16 to 1 fig 4 shows a schematic of the mapped features and band evolutions within the model the activation function in the cnn models similar to that in the ann models adds non linearity to the output of each convolution and enables the model to capture non linear relationships between variables hosseiny et al 2020 pouyanfar et al 2018 all the activation functions in u netriver were set to the rectified linear unit relu which is defined as 2 relu x max x 0 where x is the input to the function song et al 2019 the relu function is a common choice for dl models song et al 2019 tien bui et al 2020 wieland and martinis 2019 unlike hyperbolic tangent or sigmoid activation functions the relu is composed of two linear parts with two constant slopes of zero and one which makes it a non linear function as such the computation of the relu function and its derivatives are significantly fast which makes the learning in the training process more efficient jakovljevic et al 2020 2 3 3 data generation the outputs of the hydraulic model iric for different training discharges were exported into arcgis pro esri arcgis pro 2020 to generate the input and output array of images for training the u netriver model for the input data to the u netriver model raster files of the ground elevation and flooding discharge were made separately the discharge raster file included constant pixel values associated with each training discharge all across the domain fig 5 2 the elevation raster file was stacked on top of the discharge raster file by composite bands tools in arcgis pro for the output of the model a single band image associated with each discharge was made all the raster files had the resolution of 8 3 8 3 m which resulted in 260 251 column row of pixels fig 5 shows input and output images generated for a discharge of 20 m3 s to set the dimensions of the images conformable to the u netriver input 128 128 and to increase the number of the images for training 300 images with the size of 128 128 pixels were randomly selected from different locations within the 260 251 images such a selection of the images could potentially help the model to eventually learn the geometry of the river regardless of the location of the segment of the river within the image as a result the number of the arrays in the training dataset reached 2100 7 discharges 300 sampled images with the dimensions of the rows and columns being 128 128 pixels as such the input array to the u netriver for model training was composed of a matrix with 2100 arrays in which each array had the dimensions of 128 image width 128 image height 2 channels finally both input data to the model discharge elevation and target data depth were normalized by the minimum and maximum of their corresponding data used in the training process 2 3 4 train and test the u netriver model was trained by minimizing the mean squared error in predicted river depth utilizing 2100 training images eighty percent of the images were used for training and twenty percent for model validation fig 6 shows the performance of the model for training and validation with a learning rate of 0 1 after about 600 cycles epochs both training loss and validation loss remained relatively constant this indicates that the model had sufficiently learned the pattern and texture from the data moreover the convergence or proximity of the data for both training loss 0 0006 and validation loss 0 0012 indicated that the model performed well without the likelihood of overfitting to test the performance of the u netriver model it was run for different numbers of randomly selected sectors with the dimensions of 128 128 pixels within the testing image for each discharge fig 7 shows the performance of the model in predicting river depth it is important to note that the testing images were not used in the process of training the model fig 7 shows that the model performance including mean squared error mse and mean absolute error mae did not vary significantly with changes in the number of the images used for testing it should be noted that the reported mse and mae are based on the normalized input data varying between zero and one 3 results and discussion the u netriver model was trained and validated with 2100 image arrays and was tested with 1500 unseen images this section describes the performance of the model for flood depth prediction 3 1 river depth and flood extent predictions for the purpose of illustration three images with pixel sizes of 128 128 were randomly selected to show the predicted river depth in the upstream midstream and downstream of the river fig 8 shows a comparison between predicted river depth by u netriver and the iric hydraulic model for the discharges of 20 45 and 350 m3 s respectively fig 8 shows that the u netriver model was fully capable of detecting the river geometry within the image that means that the model could thoroughly identify the river width shape curves and even the river splits around the island regardless of the location of the river in the images this also demonstrates that the model was highly proficient at identifying the wet areas for flood simulations in the river prediction of inundated areas has been traditionally done by developing a demanding classification model such as random forest as suggested by hosseiny et al 2020 however the u netriver did not require any further classification for the wet dry areas detection of the river geometry and wet areas within the river were embedded in the depth estimations by the model as such the dry areas outside of the river were assigned a depth value of zero while the wet areas within the river had non zero depth values as such the proposed u netriver model has the advantage of detecting the wet areas automatically without a need for any classification model this is an important development for ml based flood simulations since it minimizes human involvement and makes simulations more objective in their methodology fig 8 also shows that the absolute difference in predicted river depth by the u netriver and iric for the selected images varied from 1 to 2 5 m this is a substantial improvement in river depth prediction compared to the ann model reported by hosseiny et al 2020 which showed an error in depth from 0 to 3 8 m the results show that the maximum positive error in depth prediction is likely to take place at the edges between water and dryland on the banks in such areas the u netriver predicted greater depth than the iric yellow color in fig 8 column c this indicates that in the edges the water depths obtained by the iric are close to zero while the depths resulted from the u netriver are not similar to the work by hosseiny et al 2020 the difference in predicted river depth obtained from u netriver and iric generally increased with an increase in river discharge which might be related to the distribution of the discharges used for training selected discharges for model training are relatively skewed toward smaller discharges as such more training data with a uniform distribution from low to high discharges may improve the accuracy of the model 3 2 error analysis estimated error in predicting river depth was calculated by subtracting the iric model depth results from the depth estimations of u netriver to further investigate the error in river depth prediction by the u netriver and the spatial distribution of error along the study area it was necessary to select input images without any overlap for this purpose original input images to the model with the dimension of 261 251 were converted to images measuring 256 256 by removing 5 column edges from each image and adding four row edges of zeros then the 256 256 images were each broken into four images of 128 128 pixels fig 9 to conform with the u netriver model input the input images of 128 128 were generated utilizing 5 testing discharges resulting in a total of 20 images finally the trained u netriver model was run for these 20 images and the differences between the two model depth values were estimated fig 9 which is for the discharge of 350 m3 s shows how the depth predictions obtained from u netriver differed from the ones obtained from iric the mse and mae for the total 20 images were 0 00096 and 0 0077 respectively table 1 shows the spatial distribution of the error along with variations in the error with increases in river discharge table 1 shows that absolute maximum error in river depth prediction by the u netriver varied from 0 to 2 7 m the standard deviations in 4 regions showed that the greatest variations in depth predictions took place in region 1 of the downstream and region 4 of the upstream the absolute error in depth prediction by the ann model in the previous work hosseiny et al 2020 was reported as up to 3 8 m this suggests that the u netriver model was capable of reducing the error up to 29 the maximum positive error of 1 6 m took place in the downstream areas region 1 while the maximum negative error 2 7 m occurred in a river bend in the upstream area region 4 for the river bend downstream around the island the u netriver was able to capture the depth close to the iric fig 8 showed that the trained model performs better in lower depths values lower discharges the separation of the flow around the island reduces the water depth as a result the error in depth prediction reduces around sediment island in opposite the outer part of the bend carries deeper water as such the error in depth prediction in upstream bend is higher due to the performance of the model for higher depths values fig 8 that implies that the u netriver performance for sharp bends may improve with further detailed training datasets both of these extreme errors were associated with the maximum discharge of 350 m3 s more inclusive training processes with higher discharge data are likely to enhance the model performance 4 limitations the main limitation of the developed model similar to other data driven approaches is the availability of measured data including flooding depth and extent for further model training and verification in addition applying the developed model to another study area requires further training with the local data including flooding depth and extent for a variety of discharges the applications of satellite images for model training and verification can be pursued in the future when obtaining such data with an acceptable spatial resolution is not technically limited the proposed model in this paper has shown promise in learning the physics behind the flooding and therefore can be used in the future along with measured data including satellite images for further training and verification depending upon the availability of such data another limitation of this study was the computational resources which helped to select a relatively small study area for this research the performance of the model in larger scales can be pursued in the future when the computational resources are available 5 conclusion the research outlined in this paper presents an innovative framework that incorporates a deep learning dl model for flood depth prediction in rivers to do that u net which is based on the convolutional neural networks cnn algorithm was modified and designated as u netriver the u netriver model was trained 1 to identify a river s geometry and wet areas in its domain automatically and 2 to estimate the flooding depth in wet areas for comparison purposes the model was trained with the same data from the iric model a 2d hydraulic model and applied to the same area as the ones in the study by hosseiny et al 2020 the input to the u netriver model was a composite image array of 128 128 pixels with two channels similar to an rgb image but with only two colors as opposed to the u net model which used 572 572 pixels and one channel the first and second channels in the input composite image represented ground elevation and flooding discharge respectively the output of the model was a one channel image of river depth training the model was carried out over 2100 composite normalized between zero and one image arrays that were based on discharges from 10 to 400 m3 s eighty percent of the images were used for training and twenty percent for model validation the model was trained over 600 epochs which yielded a training loss of 0 0006 and a validation loss of 0 0012 further the model was tested with some unseen data with discharges varying from 20 to 350 m3 s the analysis showed that the absolute difference between the depths obtained from the u netriver and the ones from iric which was defined as the error varied from 0 to 2 7 m this was a 29 improvement compared to the error from an ann model reported by hosseiny et al 2020 testing different numbers of images yielded a mean squared error varying from 0 0005 to 0 0007 training the model with more data specifically for higher discharges is expected to increase the accuracy of the model this can be pursued in the future depending upon the availability of enhanced computational power the proposed u netriver model identifies and accounts for the effects of river geometry on the river depth furthermore the identification of the wet areas is carried out automatically and therefore the u netriver model is substantially more objective than the previous models that required a wet dry classification selected by user choice funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the author declares that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the author would like to sincerely thank dr kamran makarian dr ali memariani dr nima ekhtari dr foad nazari dr virginia smith and ms sama shadnia for sharing their insight with the author the author would also like to thank mr ward barnes for his editorial assistance during the creation of this manuscript 
25721,this paper presents an innovative deep learning dl framework to a automatically identify river geometry and flood extent and b predict river flooding depth to do that u net an advanced convolutional neural network cnn was modified and given the designation of u netriver with the modification the model received an input composite image with two bands of ground elevation and flooding discharge and the output was water depth the model was trained and validated based on the outputs from iric a two dimensional hydraulic model for a segment of the green river in the state of utah the results showed that the u netriver could identify the river shape and wetted areas for flooded regions automatically the maximum difference of predicted river depth obtained from u netriver and the one obtained from the hydraulic model was 2 7 m this result suggests a 29 improvement in prediction of the maximum flood depth in the river keywords flood inundation modeling river hydraulics machine learning deep learning convolutional neural networks u net 1 introduction future flooding disasters are expected to increase in both frequency and magnitude due to climate change h hu et al 2019 leandro et al 2020 pathan and agnihotri 2021 tang et al 2020 wang et al 2019 management and mitigation of such future global challenges demand a substantial allocation of planning and preparation now and this would require wide ranging research in this field currently further studies are needed to quantify large scale fine resolution flood hazards and such studies should account for flooding extent over multiple countries shao et al 2017b moreover faced with increased global flooding decision makers and engineers must devote themselves to developing new sets of tools and efficient models for large scale flood simulations for multiple countries or continents large scale flood inundation maps can successfully demonstrate flooding risk to residents shao et al 2017a moreover quick access to flood extent mapping would be crucial for emergency responses gebrehiwot et al 2019 however large scale both spatial and temporal flood resilient analysis is not now conveniently available due to limitations in current large scale flood modeling wang et al 2019 moreover even if such maps are available in large scale formats they need to be updated very frequently to present accurate information shao et al 2017a flood risk assessment is becoming an urgent priority for many countries and this calls for a range of rapid versatile and efficient approaches yang et al 2011 along with models that can easily be exchanged between users leskens et al 2014 rapid prediction of flood extent is necessary for flood emergency services but such predictions are usually limited due to technical constraints such as model inflexibility long and expensive computation time and organizational constraints which do not allow the knowledge of the model to be well circulated leskens et al 2014 the usefulness of a hydraulic model for flood risk assessment depends on 1 the quality of the outputs and 2 the quickness and versatility of the model leskens et al 2014 as such the information and knowledge gained from a complex hydraulic model may not be incorporated into flood risk management due to the model s sophistication uncertainty and large computational expenses this means that many hydraulic models may not be useful for flood risk assessments specifically when there is an urgent need for a quick flood simulation leskens et al 2014 physics based hydraulic models have been traditionally used for simulating the flood extent and depth such models solve a set of partial differential equations associated with conservation of mass and momentum called navier stokes equations ns or simplified versions of ns such as shallow water equations janna 1993 leandro et al 2020 wang et al 2019 solving such equations for a large scale area specifically for two or three dimensional 2d 3d flow is extremely time consuming and a prohibitively expensive undertaking chu et al 2020 hosseiny et al 2020 liu and pender 2015 moreover addressing uncertainty in flood inundation maps is a laborious task as it requires a sequential run of the model chu et al 2020 leskens et al 2014 zarzar et al 2018 as such most of the previous studies on flood inundation risk and uncertainty analysis are limited and are difficult to adopt for other scenarios h hu et al 2019 recent advancements in computer science and vision coupled with a substantial increase in computing power have enabled researchers to explore the ability of machine learning ml approaches to solve their longstanding scientific problems although the field of water resources engineering has benefited somewhat from the applications of ml it has lagged behind some other fields of research in areas such as remote sensing natural language processing and the field of medicine ma et al 2019 pouyanfar et al 2018 there is an increasing need among decision makers to have ready to use hydraulic based flood simulation models advancements in computer power leskens et al 2014 and the applications of ml tools have shown promise for flood depth prediction for instance several studies have used ml tools for urban pluvial flood predictions ke et al 2020 moy de vitry et al 2019 moy de vitry and leitão 2020 sun and scanlon 2019 yan et al 2018 in spite of the comparable societal damage and cost of riverine and pluvial flooding jiang et al 2018 moy de vitry et al 2019 much less attention has been paid to applications of ml to river flood modeling among those few studies hosseiny et al 2020 introduced a framework that used two ml tools for predicting river flood extent and depth the first tool used a random forest model breiman 2001 as a binary classifier to identify the extent of wet or dry areas in a riverine flooding event the second utilized an artificial neural network ann model haykin 2008 as a regressor to estimate the flooding depth in wet areas hosseiny et al 2020 reported that the maximum error of predicted flood depth in a river took place at the river bend their results showed that the difference between the depths obtained from the hydraulic model and the ann was up to 3 8 m similarly chu et al 2020 used a hydrodynamic model to train an ann based emulation model for flood depth prediction they reported that the root mean squared error rmse of the predicted depth of the test data varied from 0 17 to 0 3 they concluded that predicted depths by an ann model were greater than the ones obtained from the hydraulic model by 2 4 15 8 folds while both studies reported that the ann models were remarkably faster than the hydraulic models their reported errors in predicting river depth suggest the limitations of ann models for efficient learning from a training dataset while both studies suggested that the ann model needed to be improved the spatial distribution of the errors in the study by hosseiny et al 2020 suggests that the ann model could not capture the relation between a river s geometry including the pattern bend shape and bathymetry and river depth the applications of ml in flood modeling are moving slowly from simple techniques such as ann toward more advanced deep learning dl techniques dikshit et al 2020 convolutional neural networks cnn is one of the most substantial and successfully applied dl models ma et al 2019 pouyanfar et al 2018 among the cnn based models u net ronneberger et al 2015 has shown an exceptional ability to extract features and to learn the patterns within data even with minimum data for training jakovljevic et al 2020 liu et al 2020 ronneberger et al 2015 ml and dl can be of great benefit to water resources engineering and specifically flood inundation modeling because they can 1 extract features and information from large amounts of data 2 handle large scale data which would be more practical using interdisciplinary approaches and 3 discover new approaches to help resolve previously challenging tasks sun and scanlon 2019 the hybrid of hydraulic models and ml tools can generate surrogate efficient models for large scale flooding dikshit et al 2020 hosseiny et al 2020 xie et al 2020 heretofore little attention has been given to spatio temporal predictions of flooding r hu et al 2019 moving toward dl models makes much more feasible the formulation of such predictions dikshit et al 2020 presently the applications of dl models in predicting the hydraulics of flooding such as flooding depth have not been investigated to fill the gap in applications of machine learning ml and deep learning dl in flood inundation modeling this paper introduces a pioneer deep learning hydraulic based model designated as u netriver which is a modification of the u net model ronneberger et al 2015 u netriver is a novel framework for predicting flood inundation extent and flooding depth in rivers hosseiny et al 2020 proposed the application of artificial intelligence to river flood predictions the author s hypothesized that their hybrid model which combined traditional hydraulic modeling with artificial neural networks would improve the speed efficiency and efficacy of modeling would lower computational costs would be more versatile and would increase abilities for predicting future flood scenarios this paper proposes to take the hybrid model a step further by applying deep learning to 1 enhance the accuracy of the hybrid model and to 2 make the ml model more objective for the sake of comparison the proposed model in this work is trained with the same data and applied to the same study area as those used by hosseiny et al 2020 the input to the u netriver takes the form of a composite image with two channels bands of elevation and discharge the output of the model is a single band image for depth this research introduces an innovative application of an advanced cnn model designated as u netriver in flood inundation modeling laying the foundations for efficient accurate large scale flood simulations the structure of this paper includes presenting the methodology in section 2 study area hydraulic model and deep learning model results and discussion in section 3 and limitations and conclusion in sections 4 and 5 respectively 2 methodology this section describes the location and the data compiled for the study area along with the concepts structure and methods used for developing both hydraulic and dl models 2 1 study area the river segment selected for this study is 3 5 km of the green river located in the united states in the northeast corner of the state of utah close to the border of the state of colorado fig 1 shows the location of the study area in utah along with an aerial view of a segment of the river which contained the study area the selected segment of the green river for this study meanders downstream of the flaming gorge dam and has a river width varying from 100 to 150 m depending on its lithology hosseiny et al 2020 the presence of sediment island and meanders in the selected segment of the river for this research ensures water depth variations along and across the river such variations are desired characteristics for testing the performance of the model in depth predictions all data required for hydraulic modeling of this segment of the river including topographic data and measured water surface elevation are publicly available from the united states geological survey usgs nelson 2019 2 2 hydraulic model the international river interface cooperative iric was used to generate training and testing datasets for this research 2 2 1 iric iric is a numerical platform for hydraulic simulations and includes a variety of solvers for predicting flooding rainfall and runoff geomorphological alterations and other phenomena the selected solver for this study was the program flow and sediment transport with morphological evolution of channel fastmech fastmech is a 2d hydraulic model for flow and sediment transport in rivers and is capable of addressing morphological evolution of channels nelson 2019 fig 2 shows the setup of the model in iric 2 2 2 calibration the hydraulic model was calibrated by changing the roughness coefficient which includes both skin and form roughness values within the model in the form of drag coefficient so that the model could accurately predict the measured water surface elevation for a discharge of 247 m3 s a drag coefficient of 0 008 was used for the computational domain which resulted in an rmse of 0 05 an exception was an area where an island in the middle of the domain splits the river for which the drag coefficient was set to 0 013 that resulted in a decrease in rmse from 0 05 to 0 03 the steps procedures and data for the calibration of the green river model are well documented by the usgs and are publicly available on the iric website nelson 2019 more information and description about model calibration can be found in the paper by hosseiny et al 2020 2 2 3 discharge data the daily discharge data of the green river in the study area from 1999 to 2019 was obtained from the usgs gage number 09234500 near greendale as in the study by hosseiny et al 2020 seven discharges of 10 50 95 120 150 300 and 400 m3 s were selected for further analysis the hydraulic model was run for selected discharges and outputs were obtained for mesh coordinates ground elevation water depth and water surface elevation the selected outputs of the hydraulic model for the seven discharges were used for training and validating the dl model further for testing the dl model five discharges of 20 30 45 225 and 350 m3 s were selected similar to the testing discharges used by hosseiny et al 2020 it is important to note that the outputs for testing discharges were not used in the training process the output of the hydraulic model for both the training and testing discharges included 47 047 data points and this was associated with the mesh size of 5 5 m in the iric model 2 3 deep learning model u net is an advanced dl algorithm and a model structure based on the concept of the cnn this section describes the structure of the cnn and u netriver and the framework that was developed for predicting flood depth and extent in rivers 2 3 1 cnn convolutional neural networks cnn are amongst the most popular and most substantial deep learning algorithms due to their ability to extract features from data with minimum human effort and knowledge pouyanfar et al 2018 making the feature selection less subjective to the user s choice is extremely important as different feature selection algorithms may result in a totally different outcome of the model yang et al 2011 cnn is a feed forward model where the weights and biases get updated by backpropagation through a supervised learning algorithm gebrehiwot et al 2019 in a two dimensional image the convolution is defined as 1 x i j u 1 m v 1 n x i u 1 j v 1 f u v b where x i j is the two dimensional image array 1 i m 1 j n f u v is the convolution filter 1 u m 1 v n and b is the bias liu et al 2020 sun and scanlon 2019 an activation function is then usually applied to the output of the cnn which enables the model to capture non linear behavior in the data gebrehiwot et al 2019 cnn has shown excellent performance in feature extraction and image segmentation especially for large scale image analysis each convolutional layer in cnn extracts descriptive features such as edges or textures in the data by generating a hierarchical descriptive feature map gebrehiwot et al 2019 sun and scanlon 2019 this descriptive feature map enables the cnn to learn local patterns and share them across other locations within an image flood et al 2019 2 3 2 u netriver u net is a specific structure of a cnn and it was introduced in a world wide image processing contest when it outperformed all the previous best models in image segmentation and needed very few images for training ronneberger et al 2015 the model has a symmetrical structure similar to the letter u and includes a contracting path left side of the u called the encoder or down sampling which produces a low resolution accurate semantic feature map jakovljevic et al 2019 to extract the texture the expanding path right side of the u called the decoder or up sampling retrieves fine spatial information jakovljevic et al 2019 along with capturing the local information in the image ronneberger et al 2015 the dl models that are based on cnn including u net mimic the human visual cortex by applying different filters and learning automatically from features generated from the hierarchical structure of the model lecun et al 1998 the hierarchy of the model includes multiple sequential layers of convolution pooling and possibly fully connected layers ma et al 2019 each convolution layer extracts a feature map by convolving the input array with a set of filters and adding the bias a nonlinear transformation which is usually called an activation function then is applied to the output of each layer any filter in the model with the dimension of n n covers an n n window of cells in an image array the filter slides along the image and aggregates the values of the covered cells by the dot product pouyanfar et al 2018 the convolution conceptually takes into account the effects of the neighboring cells in segments of an image pouyanfar et al 2018 in other words the filter technique enables the cnn to capture the texture and geometry in the array of an image as such the filters make the model sensitive to a section of the image rather than the whole image in a fully connected ann model and extract local correlations among the input data pouyanfar et al 2018 this is a conceptually critical quality of an ml model when it comes to river flood modeling where the river curves and patterns within the topographic data are influential this potential ability of the u net model to learn from geometry makes the u net a perfect candidate for this research the dl model developed for this research u netriver is a modified version of the original u net model the input to the u netriver is a composite image array of 128 128 pixels with two channels similar to an rgb image but with only two colors as opposed to 572 572 pixels and one channel in the u net model this number of pixels 128 128 for the input to the u netriver was selected based on the dimensions of the topographic raster file for the study area 260 251 this divided the study area into 4 regions for further analysis fig 1 the output of the u net model is composed of a two channel image array with the dimensions of 388 388 pixels the dimensions of the output array from the u netriver remains the same as the input 128 128 pixels with only one channel this modification is made so that the u netriver receives both topographic data and discharge data and predicts the depth at the output similar to the hydraulic model the first and second channel in the input composite image represent ground elevation and flood discharge respectively the output of the model is a one channel image of river depth the contracting path of the u netriver is composed of the following four cascading steps 1 a 3 3 convolution 2 a dropout ratio of 0 1 of the output data to prevent overfitting xie et al 2015 in the model 3 a 3 3 convolution 4 selecting the maximum number resulting from the previous steps maxpooling this sequence of actions generates low level features qamar et al 2020 and changes the dimension of the input array from 128 128 2 to 8 8 256 where 2 and 256 denote the number of the channels bands the expanding path of the u netriver is symmetrical to the contracting path while the number of the channels decreases through five cascading steps 1 a 2 2 convolution up convolution 2 concatenate features from the contracting path into the expanding path 3 a 3 3 convolution 4 a dropout ratio of 0 1 of the output 5 a 3 3 convolution the expanding path generates high level features qamar et al 2020 at the end of the path the feature map takes the dimensions of 128 128 1 the concatenation skip connections in u net preserves features and boundaries in the encoder in parallel with the convolutions khanna et al 2020 and transfers them directly into the decoder moy de vitry et al 2019 qamar et al 2020 the final output of the u netriver is an image array with the pixel dimension similar to the input tile but with one channel 128 128 1 representing the estimated depth of the river fig 3 shows the structures of the u netriver each convolution naturally reduces the dimensions of the image depending on the size of the filter to maintain the same size for the images after the convolution some extra pixels with zero values padding were added around the edges of the image moreover the strides number of cells to skip in x or y direction when the filter moves along the image in the contracting path were set to 1 in both directions this caused the model to maintain the image dimensions after the convolution the up convolution in the expanding path of the u netriver however had a filter size of 2 2 and a stride of 2 2 which resulted in doubling the number of the channels at each step the final convolution filter and stride of 1 1 changed the feature map channels from 16 to 1 fig 4 shows a schematic of the mapped features and band evolutions within the model the activation function in the cnn models similar to that in the ann models adds non linearity to the output of each convolution and enables the model to capture non linear relationships between variables hosseiny et al 2020 pouyanfar et al 2018 all the activation functions in u netriver were set to the rectified linear unit relu which is defined as 2 relu x max x 0 where x is the input to the function song et al 2019 the relu function is a common choice for dl models song et al 2019 tien bui et al 2020 wieland and martinis 2019 unlike hyperbolic tangent or sigmoid activation functions the relu is composed of two linear parts with two constant slopes of zero and one which makes it a non linear function as such the computation of the relu function and its derivatives are significantly fast which makes the learning in the training process more efficient jakovljevic et al 2020 2 3 3 data generation the outputs of the hydraulic model iric for different training discharges were exported into arcgis pro esri arcgis pro 2020 to generate the input and output array of images for training the u netriver model for the input data to the u netriver model raster files of the ground elevation and flooding discharge were made separately the discharge raster file included constant pixel values associated with each training discharge all across the domain fig 5 2 the elevation raster file was stacked on top of the discharge raster file by composite bands tools in arcgis pro for the output of the model a single band image associated with each discharge was made all the raster files had the resolution of 8 3 8 3 m which resulted in 260 251 column row of pixels fig 5 shows input and output images generated for a discharge of 20 m3 s to set the dimensions of the images conformable to the u netriver input 128 128 and to increase the number of the images for training 300 images with the size of 128 128 pixels were randomly selected from different locations within the 260 251 images such a selection of the images could potentially help the model to eventually learn the geometry of the river regardless of the location of the segment of the river within the image as a result the number of the arrays in the training dataset reached 2100 7 discharges 300 sampled images with the dimensions of the rows and columns being 128 128 pixels as such the input array to the u netriver for model training was composed of a matrix with 2100 arrays in which each array had the dimensions of 128 image width 128 image height 2 channels finally both input data to the model discharge elevation and target data depth were normalized by the minimum and maximum of their corresponding data used in the training process 2 3 4 train and test the u netriver model was trained by minimizing the mean squared error in predicted river depth utilizing 2100 training images eighty percent of the images were used for training and twenty percent for model validation fig 6 shows the performance of the model for training and validation with a learning rate of 0 1 after about 600 cycles epochs both training loss and validation loss remained relatively constant this indicates that the model had sufficiently learned the pattern and texture from the data moreover the convergence or proximity of the data for both training loss 0 0006 and validation loss 0 0012 indicated that the model performed well without the likelihood of overfitting to test the performance of the u netriver model it was run for different numbers of randomly selected sectors with the dimensions of 128 128 pixels within the testing image for each discharge fig 7 shows the performance of the model in predicting river depth it is important to note that the testing images were not used in the process of training the model fig 7 shows that the model performance including mean squared error mse and mean absolute error mae did not vary significantly with changes in the number of the images used for testing it should be noted that the reported mse and mae are based on the normalized input data varying between zero and one 3 results and discussion the u netriver model was trained and validated with 2100 image arrays and was tested with 1500 unseen images this section describes the performance of the model for flood depth prediction 3 1 river depth and flood extent predictions for the purpose of illustration three images with pixel sizes of 128 128 were randomly selected to show the predicted river depth in the upstream midstream and downstream of the river fig 8 shows a comparison between predicted river depth by u netriver and the iric hydraulic model for the discharges of 20 45 and 350 m3 s respectively fig 8 shows that the u netriver model was fully capable of detecting the river geometry within the image that means that the model could thoroughly identify the river width shape curves and even the river splits around the island regardless of the location of the river in the images this also demonstrates that the model was highly proficient at identifying the wet areas for flood simulations in the river prediction of inundated areas has been traditionally done by developing a demanding classification model such as random forest as suggested by hosseiny et al 2020 however the u netriver did not require any further classification for the wet dry areas detection of the river geometry and wet areas within the river were embedded in the depth estimations by the model as such the dry areas outside of the river were assigned a depth value of zero while the wet areas within the river had non zero depth values as such the proposed u netriver model has the advantage of detecting the wet areas automatically without a need for any classification model this is an important development for ml based flood simulations since it minimizes human involvement and makes simulations more objective in their methodology fig 8 also shows that the absolute difference in predicted river depth by the u netriver and iric for the selected images varied from 1 to 2 5 m this is a substantial improvement in river depth prediction compared to the ann model reported by hosseiny et al 2020 which showed an error in depth from 0 to 3 8 m the results show that the maximum positive error in depth prediction is likely to take place at the edges between water and dryland on the banks in such areas the u netriver predicted greater depth than the iric yellow color in fig 8 column c this indicates that in the edges the water depths obtained by the iric are close to zero while the depths resulted from the u netriver are not similar to the work by hosseiny et al 2020 the difference in predicted river depth obtained from u netriver and iric generally increased with an increase in river discharge which might be related to the distribution of the discharges used for training selected discharges for model training are relatively skewed toward smaller discharges as such more training data with a uniform distribution from low to high discharges may improve the accuracy of the model 3 2 error analysis estimated error in predicting river depth was calculated by subtracting the iric model depth results from the depth estimations of u netriver to further investigate the error in river depth prediction by the u netriver and the spatial distribution of error along the study area it was necessary to select input images without any overlap for this purpose original input images to the model with the dimension of 261 251 were converted to images measuring 256 256 by removing 5 column edges from each image and adding four row edges of zeros then the 256 256 images were each broken into four images of 128 128 pixels fig 9 to conform with the u netriver model input the input images of 128 128 were generated utilizing 5 testing discharges resulting in a total of 20 images finally the trained u netriver model was run for these 20 images and the differences between the two model depth values were estimated fig 9 which is for the discharge of 350 m3 s shows how the depth predictions obtained from u netriver differed from the ones obtained from iric the mse and mae for the total 20 images were 0 00096 and 0 0077 respectively table 1 shows the spatial distribution of the error along with variations in the error with increases in river discharge table 1 shows that absolute maximum error in river depth prediction by the u netriver varied from 0 to 2 7 m the standard deviations in 4 regions showed that the greatest variations in depth predictions took place in region 1 of the downstream and region 4 of the upstream the absolute error in depth prediction by the ann model in the previous work hosseiny et al 2020 was reported as up to 3 8 m this suggests that the u netriver model was capable of reducing the error up to 29 the maximum positive error of 1 6 m took place in the downstream areas region 1 while the maximum negative error 2 7 m occurred in a river bend in the upstream area region 4 for the river bend downstream around the island the u netriver was able to capture the depth close to the iric fig 8 showed that the trained model performs better in lower depths values lower discharges the separation of the flow around the island reduces the water depth as a result the error in depth prediction reduces around sediment island in opposite the outer part of the bend carries deeper water as such the error in depth prediction in upstream bend is higher due to the performance of the model for higher depths values fig 8 that implies that the u netriver performance for sharp bends may improve with further detailed training datasets both of these extreme errors were associated with the maximum discharge of 350 m3 s more inclusive training processes with higher discharge data are likely to enhance the model performance 4 limitations the main limitation of the developed model similar to other data driven approaches is the availability of measured data including flooding depth and extent for further model training and verification in addition applying the developed model to another study area requires further training with the local data including flooding depth and extent for a variety of discharges the applications of satellite images for model training and verification can be pursued in the future when obtaining such data with an acceptable spatial resolution is not technically limited the proposed model in this paper has shown promise in learning the physics behind the flooding and therefore can be used in the future along with measured data including satellite images for further training and verification depending upon the availability of such data another limitation of this study was the computational resources which helped to select a relatively small study area for this research the performance of the model in larger scales can be pursued in the future when the computational resources are available 5 conclusion the research outlined in this paper presents an innovative framework that incorporates a deep learning dl model for flood depth prediction in rivers to do that u net which is based on the convolutional neural networks cnn algorithm was modified and designated as u netriver the u netriver model was trained 1 to identify a river s geometry and wet areas in its domain automatically and 2 to estimate the flooding depth in wet areas for comparison purposes the model was trained with the same data from the iric model a 2d hydraulic model and applied to the same area as the ones in the study by hosseiny et al 2020 the input to the u netriver model was a composite image array of 128 128 pixels with two channels similar to an rgb image but with only two colors as opposed to the u net model which used 572 572 pixels and one channel the first and second channels in the input composite image represented ground elevation and flooding discharge respectively the output of the model was a one channel image of river depth training the model was carried out over 2100 composite normalized between zero and one image arrays that were based on discharges from 10 to 400 m3 s eighty percent of the images were used for training and twenty percent for model validation the model was trained over 600 epochs which yielded a training loss of 0 0006 and a validation loss of 0 0012 further the model was tested with some unseen data with discharges varying from 20 to 350 m3 s the analysis showed that the absolute difference between the depths obtained from the u netriver and the ones from iric which was defined as the error varied from 0 to 2 7 m this was a 29 improvement compared to the error from an ann model reported by hosseiny et al 2020 testing different numbers of images yielded a mean squared error varying from 0 0005 to 0 0007 training the model with more data specifically for higher discharges is expected to increase the accuracy of the model this can be pursued in the future depending upon the availability of enhanced computational power the proposed u netriver model identifies and accounts for the effects of river geometry on the river depth furthermore the identification of the wet areas is carried out automatically and therefore the u netriver model is substantially more objective than the previous models that required a wet dry classification selected by user choice funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the author declares that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the author would like to sincerely thank dr kamran makarian dr ali memariani dr nima ekhtari dr foad nazari dr virginia smith and ms sama shadnia for sharing their insight with the author the author would also like to thank mr ward barnes for his editorial assistance during the creation of this manuscript 
25722,the selection of flood events and determination of flood characteristics e g start and end dates peak discharge volume and duration are the first critical steps for flood analyses to obtain the key flood information accurately we used the automatic peak over threshold pot model for flood sampling and proposed an automatic approach to determine flood characteristics using the master recession curve analysis mrc method we further developed a graphical user interface gui and toolbox for this procedure in matlab model parameter estimation experiment mopex data from 423 stations were used to evaluate the proposed method our results suggest that the proposed procedure performs well for watersheds with diverse characteristics the developed toolbox can be conveniently applied to other watersheds for flood sampling and the characterisation of flood events thus helping reduce the uncertainty in subsequent flood analyses such as multivariable flood frequency and trend analyses graphical abstract image 1 keywords peak over threshold pot matlab flood characteristics master recession curve baseflow separation flood hydrograph 1 introduction floods are serious natural hazards causing significant damage and affecting millions of people worldwide kauffeldt et al 2016 bruijn et al 2019 qiu et al 2017 moreover with the increase in atmospheric water holding capacity due to global warming extreme weather events especially flood events occur more frequently blöschl et al 2019 adhikari et al 2010 tanoue et al 2016 to reduce flood damage and economic losses it is essential to accurately investigate the variability of flood events and assess flood risks zeng et al 2020 it is well known that the precise selection of flood events and identification of flood characteristics from daily streamflow data is the most critical steps for subsequent flood related research e g multivariable flood frequency analysis and trend analysis karahacane et al 2020 however such work yet lacks comprehensive and feasible approaches karahacane et al 2020 thus highlighting the importance of developing a framework for selecting flood events and identifying flood characteristics flooding is a multivariate stochastic phenomenon generally described by variables such as flood peak flood volume and flood duration mediero et al 2010 the determination of flood characteristics has not yet been comprehensively studied although various simplified methods have been used to obtain them in previous studies vittal et al 2015 jeong et al 2014 nadarajah and shiau 2005 three main challenges should be considered for the accurate and objective identification of flood characteristics the selection of suitable flood samples accurate identification of the start and end dates of flood events and extension of the recession process to separate flood events from the observed discharge the annual maximum series ams approach and the pot method are widely used for flood sampling s solari and losada 2012 the pot method which can capture more information about flood processes extracted from the daily streamflow data and reduce the uncertainty of flood frequency analysis lang et al 1999 has been widely used for flood risk estimation in the past few decades durocher et al 2018 durocher et al 2019 aissia et al 2012 however the flood samples obtained using the pot model largely depended on the selection of the threshold reliable threshold selection methods such as the fixed quantile jeong et al 2014 s solari and losada 2012 mean number of over threshold events brunner et al 2018 2019 mean exceedance above the threshold davison and smith 1990 lang et al 1999 and the automatic threshold selection method liang et al 2019 s solari and losada 2012 can result in a suitable flood sample automatic procedures which determine the threshold automatically according to the degree of fit between the hypothetical distribution and the flood sample have proven to be effective solari et al 2017 durocher et al 2018 s solari and losada 2012 durocher et al 2018 compared the efficiency of different threshold selection methods and indicated that automatic threshold selection based on the goodness of fit gof test can obtain a reasonable optimal threshold more objectively the determination of the start and end dates of flood events is the key to characterise the flood process one of the widely used ways to address this issue is the conceptual graphical approach in which the start date is usually marked by an abrupt increase in the hydrograph and the end date can be determined by the flattening of the hydrograph s recession limb tosunoglu et al 2020 y r liu et al 2020 sheng yue 2000 however there is a certain degree of subjectivity involved in identifying the start and end dates to reduce the influence of human decisions a large number of relevant studies have used a simplified approach vittal et al 2015 brunner et al 2019 mediero et al 2010 for example vittal et al 2015 suggested that the intersections of the threshold line and flow hydrograph correspond to the start and end points of a flood event to date there is no objective method for addressing this problem without human intervention due to the complexity of flooding events furthermore to determine the flood volume and hydrograph flood events need to be extracted from the flow hydrograph using baseflow separation methods smakhtin 2001 sujono et al 2004 tallaksen 1995 generally simplified methods can be used to separate flood hydrographs for example the flood volume was simply estimated using a straight line to separate direct runoff from baseflow tosunoglu et al 2020 aissia et al 2012 yue 2000 in addition the master recession curve a graphical method has been widely used to describe the discharge storage relationship of watersheds and some researchers have proposed different functional models for obtaining the master recession curve such as linear sujono et al 2004 and power function relation carlotto and chaffe 2019 furthermore this method allows the extraction of multiple flood events over a long period by extending the recession process beven et al 2011 lamb and keith 1997 sujono et al 2004 however it only finds scarce use in characterising flood events such as calculating flood volume due to its complexity both flood sampling and identification of flood characteristics are often performed manually which is tedious and requires expertise it also gives rise to large uncertainties in subsequent flood analyses moreover the manual method would be very inefficient when applied to a large amount of data solari et al 2017 carlotto and chaffe 2019 arciniega esparza et al 2017 durocher et al 2018 therefore it is necessary to develop an automatic generic procedure for selecting flood events and identifying flood characteristics with minimal human intervention that can be applied efficiently to extensive data hence we developed an automatic generic procedure to objectively select the threshold for flood sampling based on the pot model and identify flood characteristics according to the master recession curve method which is a generalised standardised approach that can be applied to all watersheds with diverse characteristics we also construct a graphical user interface gui for this procedure in matlab we tested and validated the proposed procedure using mopex data duan et al 2006 we expect to improve the selection of flood events and the determination of flood characteristics by using our automatic procedure thereby reducing uncertainties in subsequent flood analyses such as multivariable flood frequency and flood trend analyses 2 methods a flowchart of the automatic generic procedure for selecting flood events and identifying flood characteristics is shown in fig 1 the details of each method are described in subsequent sections 2 1 pot model the pot method has a long history of application in hydrological analyses such as flood frequency and trend analysis via accurate selection of flood events ribatet et al 2007 mostofi zadeh et al 2019 önöz and bayazit 2001 lang et al 1999 has provided detailed operational guidelines for the pot model however there are two issues that must be solved before using this model 1 the independence of the selected peak series and 2 the selection of an appropriate threshold mazas and hamm 2011 bernardara et al 2014 the following subsection presents the approaches used in this study to address these two problems 2 1 1 independence criteria in this study the widely used declustering method proposed by the united states water resources council in 1976 was used to examine the independence of the peak series s solari and losada 2012 durocher et al 2018 for this the successive flood events must meet two prerequisites 1 θ 5 ln a 1 609 2 a n d x m i n 3 4 m i n q 1 q 2 where θ denotes the interval time between two consecutive peaks days a is the basin area k m 2 q 1 and q 2 are two consecutive peak flows m3 s and x m i n is the minimum intermediate flow m3 s 2 1 2 automatic threshold selection to reduce the subjectivity and uncertainty of the threshold selection solari et al 2017 proposed an automatic threshold selection approach based on a gof test which performs efficient hydrological analyses on large datasets hence this method was applied to select the optimal threshold of the pot model it is well known that the pot selected extreme series should follow the generalised pareto distribution bernardara et al 2014 mazas 2019 cunnane 1979 therefore different thresholds were chosen to obtain different flood samples a generalised pareto distribution was used to fit each extreme sequence and the sample that gave the best performance corresponded to the optimal threshold the cumulative distribution function of the generalised pareto distribution can be given as 2 f n x 1 1 k x u σ 1 k i f k 0 1 exp x u σ i f k 0 where f n x is the cumulative distribution function k is the shape parameter σ is the scale parameter and u is the location parameter the distribution is valid in the range x u if k 0 and in the range 0 x u σ 1 if k 0 when k 0 the generalised pareto distribution becomes an exponential distribution the maximum likelihood method was used to estimate the parameters harville 1977 it is important to describe the degree of fit between the hypothesis distribution and the extreme series for the selection of the optimal threshold previous studies solari et al 2017 durocher et al 2018 have used gof tests extensively to select optimal thresholds in this study the anderson darling ad test was adopted to identify the optimal threshold the ad test is a type of gof test and is based on the empirical distribution function statistic a 2 which measures the distance between the empirical distribution obtained from the sample and the hypothesised distribution choulakian and stephens 2001 heo et al 2013 3 a 2 n f n x f x 2 ω x d f x here a 2 is the ad test value and the closer the ad value is to 0 the closer the corresponding peak series is to the hypothesised distribution n is the number of data points in the sample f n x is a hypothesised distribution i e generalised pareto distribution in this study f x is the empirical cumulative distribution function and ω x is a weighting function over the ordered sample values x 1 x 2 x n 4 ω x f x 1 f x 1 the automatic procedure for selecting the threshold used in this study is as follows 1 the candidate flood peak series was initially extracted by exceeding the 80 percentile threshold of the discharge series this percentile value was considered suitable for identifying all flood events for basins with different characteristics according to previous studies brunner et al 2019 solari et al 2017 the independence of the flood peaks was examined using the independence criteria given in equation 1 subsequently a series of independent peaks p i i 1 n p were sorted in ascending order i e p 1 p n p 2 from the peak series p i a sub series p u k 1 n u i e candidate thresholds were obtained by excluding repeated values n u n p a generalised pareto distribution was used to fit the sample corresponding to each candidate threshold and the ad statistic was calculated for each threshold the threshold corresponding to the minimum value of the ad statistics was selected as the optimal threshold according to previous studies the length of the determined flood sample corresponding to the optimal threshold should be longer than the number of years recorded lang et al 1999 durocher et al 2018 2 1 3 validation methods to verify the acceptability of the flood peak series selected using the above methods kendall s τ test and dispersion index i d the ratio of the variance to the mean was adopted cunnane 1979 lang et al 1999 kendall s τ test also called the kendall rank correlation coefficient was used to validate whether the peak series was independent for the peak series p i i 1 n p the test was applied to detect the first order serial dependence kendall s τ statistics are defined as 5 τ 2 i 1 n p 2 j i 1 n p 1 s i g n p i p j s i g n p i 1 p j 1 n p 1 n p 2 where s i g n is the sign function the values of τ statistics can range from 1 to 1 larger absolute τ statistics values indicate a stronger dependence on the peak series the distribution of the number of peaks per year ppy is usually assumed to follow a poisson distribution cunnane 1979 recommended the use of a dispersion index to test the adequacy of the poisson distribution since the mean and variance are equal in the poisson distribution the ratio of observed variance to the observed mean of the number of ppy can be used as a test statistic i e dispersion index for the annual number of floods m i i 1 y the dispersion index is defined as 6 i d i 1 y m i 1 y i 1 y m i 2 i 1 y m i where y is the number of record years and i d is the dispersion index where the closer its value is to 1 the closer the sequence is to the poisson distribution 2 2 determination of flood characteristics to obtain the flood duration series we developed an automatic generic procedure to identify the flood start and end dates with minimal human intervention the master recession curve method was adopted for obtaining the flood volume and hydrograph carlotto and chaffe 2019 the following subsection describes the process of determining flood characteristics using the aforementioned methods and fig 2 shows schematic diagrams for the determination of the flood characteristics start end dates peak volume and duration 2 2 1 determination of start and end dates in this study we developed a general approach for determining the start and end dates of flood episodes with reference to previous studies nadarajah and shiau 2005 vittal et al 2015 claps and laio 2003 ombadi et al 2018 sheng yue 2000 considering all flows within b times the minimum interval before the peak the start date of a flood event is defined as the time when the discharge falls below the a times peak discharge for the first time and is the local minimum fig 2a 7 q s a q p a n d t p t s b 5 ln a 1 609 2 where q p and q s are the discharge of the flood peak and the flow corresponding to the flood start date respectively t p and t s are the date of flood peak and the start date of the flood event respectively a and b are the empirical parameters and a must be in the range 0 1 and b should be in the range 1 2 the width of the interval between the peak and start points should be as short as possible and less than b times the minimum interval time of keeping two consecutive peaks independent according to previous studies and vast practical attempts a 0 5 and b 1 5 in equation 7 are suitable for determining the start point of a flood event for basins with various characteristics brunner et al 2019 the 1 5 parameter ensures that the true start points of the vast majority of flood events are included in the time period and can be retrieved using equation 7 the flood end date is determined using a procedure similar to that for the start date 8 q e a q p a n d t e t p b 5 ln a 1 609 2 where t e and q e are the end date of the flood and the corresponding discharge respectively a few key points need to be addressed regarding the aforementioned approach for determining the start and end dates of flood events first it must be emphasised that the method is applicable regardless of whether the flood is single peaked or multi peaked in fig 2a for example the start and end points obtained can also be accurately identified for multi peaked floods second if the flow hydrograph is strictly monotonic over a minimum interval time of 1 5 then the start end point is defined as the point of minimum flow 2 2 2 master recession curve analysis after the start and end dates of the flood events were determined the original flood hydrographs were extracted from the observed discharge hydrograph however the recession process of a flood event does not always subside to the baseflow i e the second flood will occur before the recession process of the first flood declines to baseflow see fig 2c this indicates that t e may not be the real end date of a flood event hence to address this issue and obtain the real end date and other flood characteristics such as flood volume some methods should be adopted to separate the flood hydrograph in this study the master recession curve mrc method was adopted which is a conceptual catchment storage model with a physical mechanism for responding to the discharge storage relationship the mrc analysis is a graphical method for hydrograph separation which overcomes the variability of each individual recession period since it attempts to extract several curves over an extended period duncan 2019 tallaksen 1995 to determine the mrc the matching strip method was adopted which is a graphical method based on the overlap of curves in which individual recessions are plotted and adjusted to form a single recession curve representative of a long data series beven et al 2011 lamb and keith 1997 carlotto and chaffe 2019 moreover an automatic method was adopted to obtain mrc without human intervention nathan and mcmahon 1990 carlotto and chaffe 2019 the following four mrc methods were used to obtain recession curves 1 maillet r hall 1968 assumes that the discharge storage relationship is linear s k q so that the recession curve can be described by an exponential model as follows 9 q t q 0 e t k where q t is the discharge at time t q 0 is the initial discharge and k is the parameter 2 coutagne 1948 considers that d q d t a q b whose analytical solution when b 1 is given by 10 q t q 0 1 b 1 b a t 1 1 b where a b are the parameters to be determined when b 1 the analytical solution is the same as in equation 9 3 wittenberg 1999 assumes that s a q b and that when b 1 the analytical solution can be expressed by 11 q t q 0 1 1 b q 0 1 b a b t 1 b 1 where a b are the parameters to be determined when b 1 equation 9 can be used 4 in addition to the three parametric methods a non parametric approach that uses cubic spline interpolation for mrc data was also adopted in this study which does not require the storage discharge relationship to be assumed in advance carlotto and chaffe 2019 in general non parametric methods fit the recession process better than parametric methods but the critical drawback is their inability to extrapolate mrc data 2 2 3 definition of deep baseflow baseflow is an important genetic component of streamflow during the yearly dry season the streamflow discharge is composed entirely of baseflow tallaksen 1995 smakhtin 2001 however it is difficult to define baseflow precisely or to separate it reliably from streamflow nathan and mcmahon 1990 duncan 2019 current segmentation methods are often not objective because baseflows cannot be measured directly duncan 2019 instead the deep baseflow was defined as the mean value of the discharge in the dry period of one year which is a static value the deep baseflow can be obtained by calculating the average value of the minimum of a yearly 60 day moving window from the daily streamflow data fig 2b 2 2 4 separation of the flood hydrograph the flood characteristics peak volume and duration can subsequently be determined by separating the flood hydrograph in fig 2c the solid green line represents the observed discharge the solid blue line indicates the extended recession process using the mrc method and the brown line indicates the deep baseflow for the first flood in the diagram a is the starting point c is the raw end point g is the modified end point curve ah is the extended recession process of the last flood and cg is the regression process of the current flood q n is the peak of the flood v n is the flood volume which is the area surrounded by abcgha the yellow hatched area and the flood duration is expressed by d n the equations are as follows 12 q n q n q b 13 d n e t n s t n 14 v n t c t a q t q b d t t a t h r t q b d t t c t g r t q b d t where q t is the observed discharge and r t is the mrc 3 development of the gui in this study a matlab toolbox i e sfe ifc was developed to perform the automatic procedure and the app designer embedded in matlab was used to build the gui the gui for selecting flood events and identifying the flood characteristics is shown in fig 3 the functions and procedures of the sfe ifc toolbox are presented in detail in this section the sfe ifc toolbox contains four main components selecting flood event panels determining flood characteristics panels graph windows and table windows each item in the toolbox is described below the left side of the toolbox is the functional area and the right side is the result presentation area the flood event selection module includes three parts data input sampling method and pot parameters a select data file you can select a streamflow file in any folder on your computer which must be in xlsx or xls format streamflow data will subsequently appear on the screen b sample method two sampling methods were selected the pot method is described in section 2 1 since the ams method is relatively simple and this work is mainly based on the pot model the ams method has not been introduced here c parameters of threshold when you press the calculate parameters button of the pot parameters panel the optimal threshold will be selected and the corresponding parameters will be presented in the panel which includes the ppy the optimal threshold and the ad statistic value d independence criteria the minimal interval days that make flood peaks independent are presented in the panel and the corresponding method is introduced in section 2 1 1 e validation result the dispersion index mentioned in section 2 1 3 was calculated after determining the optimal threshold by pressing the select events button the start end dates of flood events mentioned in section 2 2 1 are calculated and the raw flood characteristics applying the conventional method described in section 4 1 are presented in the table on the upper right part of the toolbox after selecting flood events using the pot model the corresponding flood characteristics were identified using the method mentioned in section 2 2 the identified flood characteristics panel of the sfe ifc toolbox fig 3 contains four main components extract recessions mrc methods deep baseflow and save the result f extract recessions recessions should be extracted from the streamflow data series before applying the mrc method in this toolbox an automatic method was adopted to extract recessions carlotto and chaffe 2019 to suppress the noise of the streamflow data a moving average of 3 points was applied the usage number of the 3 point moving average method can be pre set the default value is 1 the minimum duration for recessions can be defined based on the behaviour of the observed streamflow data by referring to carlotto and chaffe 2019 in this study the minimum duration of recession is defined as the minimum value that ensures that the amount of recession is less than 100 this value must be greater than or equal to five days g recession analysis methods in this panel the four mrc methods introduced in section 2 2 2 can be chosen below this is the corresponding method parameter which is estimated using the maximum likelihood theory h deep baseflow the deep baseflow can be obtained by applying the approach mentioned in section 2 2 3 after all the parameters have been determined the identity flood characteristics button can be clicked to separate the flood hydrograph and the results will be shown in the table of the upper right part of the toolbox the results can subsequently be saved in the current folder as an excel file by clicking the save flood events button the results such as the flood characteristics table and flood hydrograph are presented on the right side of the sfe ifc toolbox i table window the table presents the results of the flood characteristics which include the number of flood events start end dates peak volume and duration the corresponding flood hydrograph is drawn in the graph window when you click on a row in the table j graph window this is the main graphic output window k graphic box settings different y axis scale types linear and log can be chosen for the recession curves after clicking the clean graphic box button the graph window is cleaned l gof this panel presents the results of the gof of the different recession curves for mrc discrete data including the sum of squares due to error sse the values of the determination coefficient r square and root mean squared error rmse carlotto and chaffe 2019 4 case study 4 1 materials and validation methods the model parameter estimation experiment mopex dataset https www nws noaa gov oh mopex mo datasets htm duan et al 2006 was employed to evaluate the proposed method described in section 2 daily streamflow data are available for 438 catchments ranging from 67 to 10329 km2 across the united states we selected 423 stations with 15 years of data to validate the method in order to verify the adequacy of this procedure for basins with diverse characteristics the 423 stations were grouped into four based on the drainage area 2000 km2 or 2000 km2 and precipitation 1000 mm or 1000 mm the abbreviations lm sm la and sa indicate basin groups with large areas and moist climates small areas and moist climates large areas and arid climates small areas and arid climates respectively meanwhile four representative stations were selected from the four groups to drive the sfe ifc toolbox fig 4 to validate the accuracy of the automatic threshold based pot model s output kendall s τ test was used to assess the independence of this series and the dispersion index i d was adopted to test whether the number of ppy follows a poisson distribution as mentioned in section 2 1 3 moreover the confidence interval of τ and i d was calculated at a 5 significance level and the percentage of stations where the null hypothesis of the two indices is accepted can be obtained furthermore to verify the performance of the automatic threshold selection the proposed method was compared with a widely used conventional method i e the mean ppy method claps and laio 2003 durocher et al 2019 lang et al 1999 the mean number of ppy was set to equal 1 2 and 3 abbreviation ppy1 ppy2 ppy3 respectively according to previous studies j liu and zhang 2017 durocher et al 2018 the ad statistics were subsequently obtained after the peak series corresponding to different methods was determined for the flood characteristics it is widely accepted that the peak volume and duration are interdependent serinaldi and kilsby 2013 aissia et al 2012 mediero et al 2010 the pearson correlation coefficient between different flood characteristics for the same series were obtained to examine the flood characteristics which were subsequently compared with the correlation gained by the conventional flood characteristics in fig 2c for the conventional method the raw peak is q n and the raw duration is d n according to yue et al 1999 the raw volume can be expressed by equation 15 which ignores the recession of flood events and deep baseflow 15 v n t c t a q t d t here q t is the observed discharge and t a and t c are the raw start and end dates respectively finally to examine whether there are significant differences between different data series obtained from various groups or methods the two sample kolmogorov smirnov ks test was employed massey 1951 moreover the p value of the ks test was calculated to obtain a test decision for the null hypothesis stating that the various data series are from the same distribution greenland et al 2016 wasserstein and lazar 2016 ionides et al 2017 4 2 validation results the ad statistics and mean number of ppy obtained using the pot model are shown in fig 5 as shown in fig 5a the ad values from the automatic method were less than 0 4 for most stations and they exhibited no significant difference among the four groups fig 4 furthermore we calculated the p value of the ad test and found that the values of all stations were greater than 0 1 indicating that the peak series can be described by the generalised pareto distribution at the 10 significance level this method was then compared with the mean ppy method mentioned in subsection 4 1 the differences between our automatic method and the other three methods were significant the ad statistics value of the automatic method was closer to 0 and had a smaller variance while there was no significant difference between the four groups the above results show that the peak series selected by the automatic threshold selection method can be better described by the generalised pareto distribution as compared to the other methods the mean number of ppy was mostly between 1 5 and 2 5 fig 5b for the large basins the mean number was relatively smaller and had a smaller variance than that of the small basins indicating that basin sizes had significant effects on the number of ppy furthermore we calculated kendall s τ test and the dispersion index the kendall s τ values of most stations were in the range of 0 1 to 0 1 in which there was no significant difference among the four groups fig 6 a moreover the dispersion indices of the moist basins were close to 1 with low variation fig 6b the ks test results showed that there was a significant difference between the moist and arid basins dispersion indices the percentage of corresponding group stations where the null hypothesis of the index is accepted at a 5 significance level is shown at the top of this figure for kendall s τ test the null hypothesis indicates that the peak series are independent and for the dispersion index it indicates that the number of flood events approximately follow a poisson distribution for both indices 80 of the stations are located at the confidence interval indicating that the peak series of 80 of the stations are independent and that the number of flood events follows a poisson distribution the dispersion index values of almost all moist basins were within the 95 confidence intervals fig 6 we also examined flood characteristics determined using the proposed method the mean correlation coefficient between the flood characteristics calculated by the proposed method was higher than that calculated by the conventional method for all groups fig 7 the peak volume correlation coefficients obtained by the new method were approximately consistent with those obtained using the conventional method however the peak duration and volume duration correlation coefficients obtained by the new method were significantly higher this may be due to the fact that the duration of floods obtained using the proposed method was longer because the flood recession was considered fig 8 d and fig s1d s3d our results show that the automatic pot model used in this study can suitably be applied to watersheds with diverse characteristics especially moist basins and the peak series selected by the automatic threshold selection method can be better described by the generalised pareto distribution compared to other methods the results of the correlation analysis reveal that the flood characteristics determined by the mrc method are reasonable and consistent with the conventional method 4 3 specific analysis of representative stations to further demonstrate the application of this procedure in individual watersheds with different basin characteristics we selected four stations from the corresponding four groups fig 4 to drive the sfe ifc toolbox for all four stations the i d and τ indices failed to reject the null hypothesis at the 5 significance level table 1 indicating that the flood events are independent and that the generalised pareto distribution is a good fit for the peak series of the four stations fig 8 displays the results from the station no 2475000 driving the sfe ifc toolbox and the other three stations are displayed in the supplementary material figs s1 s3 the process of selecting an optimal threshold is illustrated in fig 8a with an increase in the candidate threshold the ad statistical value exhibited complex changes and the ppy value decreased the red hollow circle represents the optimal threshold fig 8b shows the peak series obtained using the pot model in this procedure to obtain relatively accurate flood characteristics the automatic mrc method was used to segment the flood hydrograph fig 8c shows the mrc obtained using the matching strip method and the analytical expressions described by maillet tallaksen 1995 the red hollow circles represent the mrc discrete data from the matching strip method and the blue line represents the mrc of the maillet method the values of the determination coefficient for the four stations were all greater than 0 9 and those of the two arid basins were greater than 0 97 thus indicating that the mrc can suitably fit the recession process fig 8c figs s1c s3c the results of the other methods mentioned in subsection 2 2 2 are presented in the supplementary material fig s4 and readers can choose different mrc methods that fit the mrc discrete data according to the gof displayed at the lower right part of the gui furthermore the pearson correlation coefficient between different flood characteristics was calculated and compared with the conventional correlation coefficient fig 8d displays scatter plots of the flood characteristics the blue hollow dots represent the flood characteristics obtained by the proposed method introduced in section 2 2 and the orange dots correspond to the conventional method the correlation coefficient of the blue dots is higher than that of the orange dots due to the consideration of the flood recession process in the proposed method figs s1 s3 the treated flood hydrograph was obtained from the output of the sfe ifc toolbox two flood events per station were randomly selected for the four basins fig 9 from these flood hydrographs it was observed that the identified start date of flood events generally corresponded to the flood rising point and that the mrc obtained via the analytical expressions described by maillet 1905 fit the recession process satisfactorily the proposed method performed well for both single and multi peaked floods compared to the other three methods the recession process obtained with the maillet method faded more quickly especially for wet basins figs s4 s5 overall these results suggest that the automatic procedure in this study can effectively capture flood hydrographs and further obtain more accurate flood characteristics 5 discussion as the pot model can capture more information about flood processes than the ams method it is widely used in flood sampling lang et al 1999 in this study the ppy values fluctuated around 2 and exhibited smaller variance for the large basins which is consistent with the results from previous literature claps and laio 2003 as for the dispersion index the value of moist basins was closer to 1 and lay within the 95 confidence intervals for almost all moist basins indicating that the method outlined in this procedure works better in moist basins than in arid basins furthermore the acceptance rate of the generalised pareto distribution at the 5 significance level was consistent with previous studies implying that the generalised pareto distribution can accurately describe the peak series from the automatic pot model durocher et al 2018 lang et al 1999 in this study we proposed an automatic generic method to determine flood characteristics the results of the correlational analysis of flood characteristics fig 8d and figs s1d s3d and the examples of hydrographs for flood events fig 9 proved that the general determination method in this study is effective and reasonable correlation analysis was an alternative method used to examine flood characteristics due to the difficulty in obtaining the actual flood characteristics compared with other methods described by researchers nadarajah and shiau 2005 daneshkhah et al 2016 y r liu et al 2020 sheng yue and rasmussen 2002 this method takes flood recession and deep baseflow into consideration as shown in fig 9 and fig s5 the identified start date of the flood event generally corresponds to the flood rising point and the mrc method fits the recession process well indicating that the proposed determination method is suitable for basins with diverse characteristics moreover it is worth noting that the developed automatic procedure is currently only applicable to daily scale data however mechanistically the method can also be extended to an hourly scale which will be addressed in future work baseflow separation is challenging because it cannot be directly measured and it can mean different things to different interest groups smakhtin 2001 therefore we did not delve into the issue in this study alternatively we defined the deep baseflow as the mean value of the discharge in the dry period of one year which is a static value the results from the correlation analysis fig 7 and the flood hydrograph fig 9 revealed that this simplification did not have a significant impact on the flood characteristic results and subsequent flood analysis one striking finding in this study was that the flood duration could last more than two months nearly twice as long as the one obtained with the conventional approach fig 9c this result seems to be inconsistent with our intuitive understanding of flood events however for large moist basins flood events can last more than 100 days najibi et al 2017 ward et al 2016 for example most of the large amazonian rivers experience a single flood period in one year and this seasonal flood lasts for nearly half a year getirana and rodrigo 2013 towner et al 2020 objectively the recession process of the flood is slow and many flood episodes in large basins do not subside to the baseflow duncan 2019 hence we considered the final flood duration thus obtained to be suitable 6 conclusions in this study we constructed a generic framework of automatic procedures for selecting flood events and identifying flood characteristics using an automatic threshold based pot model for flood sampling more importantly we proposed a generic automatic approach to determine flood characteristics using the mrc analysis methods furthermore we developed a gui i e the sfe ifc toolbox based on the matlab app designer we validated the proposed method using 423 mopex dataset stations we subsequently concluded that the pot model used in this study has good applicability to watersheds with diverse characteristics especially moist basins the peak series obtained from the automatic threshold selection method was better described by the generalised pareto distribution compared with other methods e g the mean ppy method we also reveal that the flood characteristics duration and volume determined by the proposed general method were reasonable when compared with the conventional method as the proposed method could accurately capture the flood hydrographs and further obtain additional objective flood characteristics moreover this was a general and comprehensive research on flood sampling and determination of flood characteristics which will reduce the uncertainty of future analyses such as multivariable flood frequency and flood trend analyses software and data availability toolbox name sfe ifc select flood events and identify flood characteristics software required matlab r2019a and above program language matlab contact email zhangqinwhu whu edu cn trial version link https github com zhang qin 0925 sfe ifc toolbox find main validation data the model parameter estimation experiment mopex dataset https www nws noaa gov oh mopex mo datasets htm declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the national key research and development program of china no 2017yfa0603704 major projects of national natural science foundation of china no 41890824 the strategic priority research program of the chinese academy of sciences grant no xda23040103 the strategic priority research program of the chinese academy of sciences grant no xda23040500 the national natural science foundation of china no 51809008 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105180 
25722,the selection of flood events and determination of flood characteristics e g start and end dates peak discharge volume and duration are the first critical steps for flood analyses to obtain the key flood information accurately we used the automatic peak over threshold pot model for flood sampling and proposed an automatic approach to determine flood characteristics using the master recession curve analysis mrc method we further developed a graphical user interface gui and toolbox for this procedure in matlab model parameter estimation experiment mopex data from 423 stations were used to evaluate the proposed method our results suggest that the proposed procedure performs well for watersheds with diverse characteristics the developed toolbox can be conveniently applied to other watersheds for flood sampling and the characterisation of flood events thus helping reduce the uncertainty in subsequent flood analyses such as multivariable flood frequency and trend analyses graphical abstract image 1 keywords peak over threshold pot matlab flood characteristics master recession curve baseflow separation flood hydrograph 1 introduction floods are serious natural hazards causing significant damage and affecting millions of people worldwide kauffeldt et al 2016 bruijn et al 2019 qiu et al 2017 moreover with the increase in atmospheric water holding capacity due to global warming extreme weather events especially flood events occur more frequently blöschl et al 2019 adhikari et al 2010 tanoue et al 2016 to reduce flood damage and economic losses it is essential to accurately investigate the variability of flood events and assess flood risks zeng et al 2020 it is well known that the precise selection of flood events and identification of flood characteristics from daily streamflow data is the most critical steps for subsequent flood related research e g multivariable flood frequency analysis and trend analysis karahacane et al 2020 however such work yet lacks comprehensive and feasible approaches karahacane et al 2020 thus highlighting the importance of developing a framework for selecting flood events and identifying flood characteristics flooding is a multivariate stochastic phenomenon generally described by variables such as flood peak flood volume and flood duration mediero et al 2010 the determination of flood characteristics has not yet been comprehensively studied although various simplified methods have been used to obtain them in previous studies vittal et al 2015 jeong et al 2014 nadarajah and shiau 2005 three main challenges should be considered for the accurate and objective identification of flood characteristics the selection of suitable flood samples accurate identification of the start and end dates of flood events and extension of the recession process to separate flood events from the observed discharge the annual maximum series ams approach and the pot method are widely used for flood sampling s solari and losada 2012 the pot method which can capture more information about flood processes extracted from the daily streamflow data and reduce the uncertainty of flood frequency analysis lang et al 1999 has been widely used for flood risk estimation in the past few decades durocher et al 2018 durocher et al 2019 aissia et al 2012 however the flood samples obtained using the pot model largely depended on the selection of the threshold reliable threshold selection methods such as the fixed quantile jeong et al 2014 s solari and losada 2012 mean number of over threshold events brunner et al 2018 2019 mean exceedance above the threshold davison and smith 1990 lang et al 1999 and the automatic threshold selection method liang et al 2019 s solari and losada 2012 can result in a suitable flood sample automatic procedures which determine the threshold automatically according to the degree of fit between the hypothetical distribution and the flood sample have proven to be effective solari et al 2017 durocher et al 2018 s solari and losada 2012 durocher et al 2018 compared the efficiency of different threshold selection methods and indicated that automatic threshold selection based on the goodness of fit gof test can obtain a reasonable optimal threshold more objectively the determination of the start and end dates of flood events is the key to characterise the flood process one of the widely used ways to address this issue is the conceptual graphical approach in which the start date is usually marked by an abrupt increase in the hydrograph and the end date can be determined by the flattening of the hydrograph s recession limb tosunoglu et al 2020 y r liu et al 2020 sheng yue 2000 however there is a certain degree of subjectivity involved in identifying the start and end dates to reduce the influence of human decisions a large number of relevant studies have used a simplified approach vittal et al 2015 brunner et al 2019 mediero et al 2010 for example vittal et al 2015 suggested that the intersections of the threshold line and flow hydrograph correspond to the start and end points of a flood event to date there is no objective method for addressing this problem without human intervention due to the complexity of flooding events furthermore to determine the flood volume and hydrograph flood events need to be extracted from the flow hydrograph using baseflow separation methods smakhtin 2001 sujono et al 2004 tallaksen 1995 generally simplified methods can be used to separate flood hydrographs for example the flood volume was simply estimated using a straight line to separate direct runoff from baseflow tosunoglu et al 2020 aissia et al 2012 yue 2000 in addition the master recession curve a graphical method has been widely used to describe the discharge storage relationship of watersheds and some researchers have proposed different functional models for obtaining the master recession curve such as linear sujono et al 2004 and power function relation carlotto and chaffe 2019 furthermore this method allows the extraction of multiple flood events over a long period by extending the recession process beven et al 2011 lamb and keith 1997 sujono et al 2004 however it only finds scarce use in characterising flood events such as calculating flood volume due to its complexity both flood sampling and identification of flood characteristics are often performed manually which is tedious and requires expertise it also gives rise to large uncertainties in subsequent flood analyses moreover the manual method would be very inefficient when applied to a large amount of data solari et al 2017 carlotto and chaffe 2019 arciniega esparza et al 2017 durocher et al 2018 therefore it is necessary to develop an automatic generic procedure for selecting flood events and identifying flood characteristics with minimal human intervention that can be applied efficiently to extensive data hence we developed an automatic generic procedure to objectively select the threshold for flood sampling based on the pot model and identify flood characteristics according to the master recession curve method which is a generalised standardised approach that can be applied to all watersheds with diverse characteristics we also construct a graphical user interface gui for this procedure in matlab we tested and validated the proposed procedure using mopex data duan et al 2006 we expect to improve the selection of flood events and the determination of flood characteristics by using our automatic procedure thereby reducing uncertainties in subsequent flood analyses such as multivariable flood frequency and flood trend analyses 2 methods a flowchart of the automatic generic procedure for selecting flood events and identifying flood characteristics is shown in fig 1 the details of each method are described in subsequent sections 2 1 pot model the pot method has a long history of application in hydrological analyses such as flood frequency and trend analysis via accurate selection of flood events ribatet et al 2007 mostofi zadeh et al 2019 önöz and bayazit 2001 lang et al 1999 has provided detailed operational guidelines for the pot model however there are two issues that must be solved before using this model 1 the independence of the selected peak series and 2 the selection of an appropriate threshold mazas and hamm 2011 bernardara et al 2014 the following subsection presents the approaches used in this study to address these two problems 2 1 1 independence criteria in this study the widely used declustering method proposed by the united states water resources council in 1976 was used to examine the independence of the peak series s solari and losada 2012 durocher et al 2018 for this the successive flood events must meet two prerequisites 1 θ 5 ln a 1 609 2 a n d x m i n 3 4 m i n q 1 q 2 where θ denotes the interval time between two consecutive peaks days a is the basin area k m 2 q 1 and q 2 are two consecutive peak flows m3 s and x m i n is the minimum intermediate flow m3 s 2 1 2 automatic threshold selection to reduce the subjectivity and uncertainty of the threshold selection solari et al 2017 proposed an automatic threshold selection approach based on a gof test which performs efficient hydrological analyses on large datasets hence this method was applied to select the optimal threshold of the pot model it is well known that the pot selected extreme series should follow the generalised pareto distribution bernardara et al 2014 mazas 2019 cunnane 1979 therefore different thresholds were chosen to obtain different flood samples a generalised pareto distribution was used to fit each extreme sequence and the sample that gave the best performance corresponded to the optimal threshold the cumulative distribution function of the generalised pareto distribution can be given as 2 f n x 1 1 k x u σ 1 k i f k 0 1 exp x u σ i f k 0 where f n x is the cumulative distribution function k is the shape parameter σ is the scale parameter and u is the location parameter the distribution is valid in the range x u if k 0 and in the range 0 x u σ 1 if k 0 when k 0 the generalised pareto distribution becomes an exponential distribution the maximum likelihood method was used to estimate the parameters harville 1977 it is important to describe the degree of fit between the hypothesis distribution and the extreme series for the selection of the optimal threshold previous studies solari et al 2017 durocher et al 2018 have used gof tests extensively to select optimal thresholds in this study the anderson darling ad test was adopted to identify the optimal threshold the ad test is a type of gof test and is based on the empirical distribution function statistic a 2 which measures the distance between the empirical distribution obtained from the sample and the hypothesised distribution choulakian and stephens 2001 heo et al 2013 3 a 2 n f n x f x 2 ω x d f x here a 2 is the ad test value and the closer the ad value is to 0 the closer the corresponding peak series is to the hypothesised distribution n is the number of data points in the sample f n x is a hypothesised distribution i e generalised pareto distribution in this study f x is the empirical cumulative distribution function and ω x is a weighting function over the ordered sample values x 1 x 2 x n 4 ω x f x 1 f x 1 the automatic procedure for selecting the threshold used in this study is as follows 1 the candidate flood peak series was initially extracted by exceeding the 80 percentile threshold of the discharge series this percentile value was considered suitable for identifying all flood events for basins with different characteristics according to previous studies brunner et al 2019 solari et al 2017 the independence of the flood peaks was examined using the independence criteria given in equation 1 subsequently a series of independent peaks p i i 1 n p were sorted in ascending order i e p 1 p n p 2 from the peak series p i a sub series p u k 1 n u i e candidate thresholds were obtained by excluding repeated values n u n p a generalised pareto distribution was used to fit the sample corresponding to each candidate threshold and the ad statistic was calculated for each threshold the threshold corresponding to the minimum value of the ad statistics was selected as the optimal threshold according to previous studies the length of the determined flood sample corresponding to the optimal threshold should be longer than the number of years recorded lang et al 1999 durocher et al 2018 2 1 3 validation methods to verify the acceptability of the flood peak series selected using the above methods kendall s τ test and dispersion index i d the ratio of the variance to the mean was adopted cunnane 1979 lang et al 1999 kendall s τ test also called the kendall rank correlation coefficient was used to validate whether the peak series was independent for the peak series p i i 1 n p the test was applied to detect the first order serial dependence kendall s τ statistics are defined as 5 τ 2 i 1 n p 2 j i 1 n p 1 s i g n p i p j s i g n p i 1 p j 1 n p 1 n p 2 where s i g n is the sign function the values of τ statistics can range from 1 to 1 larger absolute τ statistics values indicate a stronger dependence on the peak series the distribution of the number of peaks per year ppy is usually assumed to follow a poisson distribution cunnane 1979 recommended the use of a dispersion index to test the adequacy of the poisson distribution since the mean and variance are equal in the poisson distribution the ratio of observed variance to the observed mean of the number of ppy can be used as a test statistic i e dispersion index for the annual number of floods m i i 1 y the dispersion index is defined as 6 i d i 1 y m i 1 y i 1 y m i 2 i 1 y m i where y is the number of record years and i d is the dispersion index where the closer its value is to 1 the closer the sequence is to the poisson distribution 2 2 determination of flood characteristics to obtain the flood duration series we developed an automatic generic procedure to identify the flood start and end dates with minimal human intervention the master recession curve method was adopted for obtaining the flood volume and hydrograph carlotto and chaffe 2019 the following subsection describes the process of determining flood characteristics using the aforementioned methods and fig 2 shows schematic diagrams for the determination of the flood characteristics start end dates peak volume and duration 2 2 1 determination of start and end dates in this study we developed a general approach for determining the start and end dates of flood episodes with reference to previous studies nadarajah and shiau 2005 vittal et al 2015 claps and laio 2003 ombadi et al 2018 sheng yue 2000 considering all flows within b times the minimum interval before the peak the start date of a flood event is defined as the time when the discharge falls below the a times peak discharge for the first time and is the local minimum fig 2a 7 q s a q p a n d t p t s b 5 ln a 1 609 2 where q p and q s are the discharge of the flood peak and the flow corresponding to the flood start date respectively t p and t s are the date of flood peak and the start date of the flood event respectively a and b are the empirical parameters and a must be in the range 0 1 and b should be in the range 1 2 the width of the interval between the peak and start points should be as short as possible and less than b times the minimum interval time of keeping two consecutive peaks independent according to previous studies and vast practical attempts a 0 5 and b 1 5 in equation 7 are suitable for determining the start point of a flood event for basins with various characteristics brunner et al 2019 the 1 5 parameter ensures that the true start points of the vast majority of flood events are included in the time period and can be retrieved using equation 7 the flood end date is determined using a procedure similar to that for the start date 8 q e a q p a n d t e t p b 5 ln a 1 609 2 where t e and q e are the end date of the flood and the corresponding discharge respectively a few key points need to be addressed regarding the aforementioned approach for determining the start and end dates of flood events first it must be emphasised that the method is applicable regardless of whether the flood is single peaked or multi peaked in fig 2a for example the start and end points obtained can also be accurately identified for multi peaked floods second if the flow hydrograph is strictly monotonic over a minimum interval time of 1 5 then the start end point is defined as the point of minimum flow 2 2 2 master recession curve analysis after the start and end dates of the flood events were determined the original flood hydrographs were extracted from the observed discharge hydrograph however the recession process of a flood event does not always subside to the baseflow i e the second flood will occur before the recession process of the first flood declines to baseflow see fig 2c this indicates that t e may not be the real end date of a flood event hence to address this issue and obtain the real end date and other flood characteristics such as flood volume some methods should be adopted to separate the flood hydrograph in this study the master recession curve mrc method was adopted which is a conceptual catchment storage model with a physical mechanism for responding to the discharge storage relationship the mrc analysis is a graphical method for hydrograph separation which overcomes the variability of each individual recession period since it attempts to extract several curves over an extended period duncan 2019 tallaksen 1995 to determine the mrc the matching strip method was adopted which is a graphical method based on the overlap of curves in which individual recessions are plotted and adjusted to form a single recession curve representative of a long data series beven et al 2011 lamb and keith 1997 carlotto and chaffe 2019 moreover an automatic method was adopted to obtain mrc without human intervention nathan and mcmahon 1990 carlotto and chaffe 2019 the following four mrc methods were used to obtain recession curves 1 maillet r hall 1968 assumes that the discharge storage relationship is linear s k q so that the recession curve can be described by an exponential model as follows 9 q t q 0 e t k where q t is the discharge at time t q 0 is the initial discharge and k is the parameter 2 coutagne 1948 considers that d q d t a q b whose analytical solution when b 1 is given by 10 q t q 0 1 b 1 b a t 1 1 b where a b are the parameters to be determined when b 1 the analytical solution is the same as in equation 9 3 wittenberg 1999 assumes that s a q b and that when b 1 the analytical solution can be expressed by 11 q t q 0 1 1 b q 0 1 b a b t 1 b 1 where a b are the parameters to be determined when b 1 equation 9 can be used 4 in addition to the three parametric methods a non parametric approach that uses cubic spline interpolation for mrc data was also adopted in this study which does not require the storage discharge relationship to be assumed in advance carlotto and chaffe 2019 in general non parametric methods fit the recession process better than parametric methods but the critical drawback is their inability to extrapolate mrc data 2 2 3 definition of deep baseflow baseflow is an important genetic component of streamflow during the yearly dry season the streamflow discharge is composed entirely of baseflow tallaksen 1995 smakhtin 2001 however it is difficult to define baseflow precisely or to separate it reliably from streamflow nathan and mcmahon 1990 duncan 2019 current segmentation methods are often not objective because baseflows cannot be measured directly duncan 2019 instead the deep baseflow was defined as the mean value of the discharge in the dry period of one year which is a static value the deep baseflow can be obtained by calculating the average value of the minimum of a yearly 60 day moving window from the daily streamflow data fig 2b 2 2 4 separation of the flood hydrograph the flood characteristics peak volume and duration can subsequently be determined by separating the flood hydrograph in fig 2c the solid green line represents the observed discharge the solid blue line indicates the extended recession process using the mrc method and the brown line indicates the deep baseflow for the first flood in the diagram a is the starting point c is the raw end point g is the modified end point curve ah is the extended recession process of the last flood and cg is the regression process of the current flood q n is the peak of the flood v n is the flood volume which is the area surrounded by abcgha the yellow hatched area and the flood duration is expressed by d n the equations are as follows 12 q n q n q b 13 d n e t n s t n 14 v n t c t a q t q b d t t a t h r t q b d t t c t g r t q b d t where q t is the observed discharge and r t is the mrc 3 development of the gui in this study a matlab toolbox i e sfe ifc was developed to perform the automatic procedure and the app designer embedded in matlab was used to build the gui the gui for selecting flood events and identifying the flood characteristics is shown in fig 3 the functions and procedures of the sfe ifc toolbox are presented in detail in this section the sfe ifc toolbox contains four main components selecting flood event panels determining flood characteristics panels graph windows and table windows each item in the toolbox is described below the left side of the toolbox is the functional area and the right side is the result presentation area the flood event selection module includes three parts data input sampling method and pot parameters a select data file you can select a streamflow file in any folder on your computer which must be in xlsx or xls format streamflow data will subsequently appear on the screen b sample method two sampling methods were selected the pot method is described in section 2 1 since the ams method is relatively simple and this work is mainly based on the pot model the ams method has not been introduced here c parameters of threshold when you press the calculate parameters button of the pot parameters panel the optimal threshold will be selected and the corresponding parameters will be presented in the panel which includes the ppy the optimal threshold and the ad statistic value d independence criteria the minimal interval days that make flood peaks independent are presented in the panel and the corresponding method is introduced in section 2 1 1 e validation result the dispersion index mentioned in section 2 1 3 was calculated after determining the optimal threshold by pressing the select events button the start end dates of flood events mentioned in section 2 2 1 are calculated and the raw flood characteristics applying the conventional method described in section 4 1 are presented in the table on the upper right part of the toolbox after selecting flood events using the pot model the corresponding flood characteristics were identified using the method mentioned in section 2 2 the identified flood characteristics panel of the sfe ifc toolbox fig 3 contains four main components extract recessions mrc methods deep baseflow and save the result f extract recessions recessions should be extracted from the streamflow data series before applying the mrc method in this toolbox an automatic method was adopted to extract recessions carlotto and chaffe 2019 to suppress the noise of the streamflow data a moving average of 3 points was applied the usage number of the 3 point moving average method can be pre set the default value is 1 the minimum duration for recessions can be defined based on the behaviour of the observed streamflow data by referring to carlotto and chaffe 2019 in this study the minimum duration of recession is defined as the minimum value that ensures that the amount of recession is less than 100 this value must be greater than or equal to five days g recession analysis methods in this panel the four mrc methods introduced in section 2 2 2 can be chosen below this is the corresponding method parameter which is estimated using the maximum likelihood theory h deep baseflow the deep baseflow can be obtained by applying the approach mentioned in section 2 2 3 after all the parameters have been determined the identity flood characteristics button can be clicked to separate the flood hydrograph and the results will be shown in the table of the upper right part of the toolbox the results can subsequently be saved in the current folder as an excel file by clicking the save flood events button the results such as the flood characteristics table and flood hydrograph are presented on the right side of the sfe ifc toolbox i table window the table presents the results of the flood characteristics which include the number of flood events start end dates peak volume and duration the corresponding flood hydrograph is drawn in the graph window when you click on a row in the table j graph window this is the main graphic output window k graphic box settings different y axis scale types linear and log can be chosen for the recession curves after clicking the clean graphic box button the graph window is cleaned l gof this panel presents the results of the gof of the different recession curves for mrc discrete data including the sum of squares due to error sse the values of the determination coefficient r square and root mean squared error rmse carlotto and chaffe 2019 4 case study 4 1 materials and validation methods the model parameter estimation experiment mopex dataset https www nws noaa gov oh mopex mo datasets htm duan et al 2006 was employed to evaluate the proposed method described in section 2 daily streamflow data are available for 438 catchments ranging from 67 to 10329 km2 across the united states we selected 423 stations with 15 years of data to validate the method in order to verify the adequacy of this procedure for basins with diverse characteristics the 423 stations were grouped into four based on the drainage area 2000 km2 or 2000 km2 and precipitation 1000 mm or 1000 mm the abbreviations lm sm la and sa indicate basin groups with large areas and moist climates small areas and moist climates large areas and arid climates small areas and arid climates respectively meanwhile four representative stations were selected from the four groups to drive the sfe ifc toolbox fig 4 to validate the accuracy of the automatic threshold based pot model s output kendall s τ test was used to assess the independence of this series and the dispersion index i d was adopted to test whether the number of ppy follows a poisson distribution as mentioned in section 2 1 3 moreover the confidence interval of τ and i d was calculated at a 5 significance level and the percentage of stations where the null hypothesis of the two indices is accepted can be obtained furthermore to verify the performance of the automatic threshold selection the proposed method was compared with a widely used conventional method i e the mean ppy method claps and laio 2003 durocher et al 2019 lang et al 1999 the mean number of ppy was set to equal 1 2 and 3 abbreviation ppy1 ppy2 ppy3 respectively according to previous studies j liu and zhang 2017 durocher et al 2018 the ad statistics were subsequently obtained after the peak series corresponding to different methods was determined for the flood characteristics it is widely accepted that the peak volume and duration are interdependent serinaldi and kilsby 2013 aissia et al 2012 mediero et al 2010 the pearson correlation coefficient between different flood characteristics for the same series were obtained to examine the flood characteristics which were subsequently compared with the correlation gained by the conventional flood characteristics in fig 2c for the conventional method the raw peak is q n and the raw duration is d n according to yue et al 1999 the raw volume can be expressed by equation 15 which ignores the recession of flood events and deep baseflow 15 v n t c t a q t d t here q t is the observed discharge and t a and t c are the raw start and end dates respectively finally to examine whether there are significant differences between different data series obtained from various groups or methods the two sample kolmogorov smirnov ks test was employed massey 1951 moreover the p value of the ks test was calculated to obtain a test decision for the null hypothesis stating that the various data series are from the same distribution greenland et al 2016 wasserstein and lazar 2016 ionides et al 2017 4 2 validation results the ad statistics and mean number of ppy obtained using the pot model are shown in fig 5 as shown in fig 5a the ad values from the automatic method were less than 0 4 for most stations and they exhibited no significant difference among the four groups fig 4 furthermore we calculated the p value of the ad test and found that the values of all stations were greater than 0 1 indicating that the peak series can be described by the generalised pareto distribution at the 10 significance level this method was then compared with the mean ppy method mentioned in subsection 4 1 the differences between our automatic method and the other three methods were significant the ad statistics value of the automatic method was closer to 0 and had a smaller variance while there was no significant difference between the four groups the above results show that the peak series selected by the automatic threshold selection method can be better described by the generalised pareto distribution as compared to the other methods the mean number of ppy was mostly between 1 5 and 2 5 fig 5b for the large basins the mean number was relatively smaller and had a smaller variance than that of the small basins indicating that basin sizes had significant effects on the number of ppy furthermore we calculated kendall s τ test and the dispersion index the kendall s τ values of most stations were in the range of 0 1 to 0 1 in which there was no significant difference among the four groups fig 6 a moreover the dispersion indices of the moist basins were close to 1 with low variation fig 6b the ks test results showed that there was a significant difference between the moist and arid basins dispersion indices the percentage of corresponding group stations where the null hypothesis of the index is accepted at a 5 significance level is shown at the top of this figure for kendall s τ test the null hypothesis indicates that the peak series are independent and for the dispersion index it indicates that the number of flood events approximately follow a poisson distribution for both indices 80 of the stations are located at the confidence interval indicating that the peak series of 80 of the stations are independent and that the number of flood events follows a poisson distribution the dispersion index values of almost all moist basins were within the 95 confidence intervals fig 6 we also examined flood characteristics determined using the proposed method the mean correlation coefficient between the flood characteristics calculated by the proposed method was higher than that calculated by the conventional method for all groups fig 7 the peak volume correlation coefficients obtained by the new method were approximately consistent with those obtained using the conventional method however the peak duration and volume duration correlation coefficients obtained by the new method were significantly higher this may be due to the fact that the duration of floods obtained using the proposed method was longer because the flood recession was considered fig 8 d and fig s1d s3d our results show that the automatic pot model used in this study can suitably be applied to watersheds with diverse characteristics especially moist basins and the peak series selected by the automatic threshold selection method can be better described by the generalised pareto distribution compared to other methods the results of the correlation analysis reveal that the flood characteristics determined by the mrc method are reasonable and consistent with the conventional method 4 3 specific analysis of representative stations to further demonstrate the application of this procedure in individual watersheds with different basin characteristics we selected four stations from the corresponding four groups fig 4 to drive the sfe ifc toolbox for all four stations the i d and τ indices failed to reject the null hypothesis at the 5 significance level table 1 indicating that the flood events are independent and that the generalised pareto distribution is a good fit for the peak series of the four stations fig 8 displays the results from the station no 2475000 driving the sfe ifc toolbox and the other three stations are displayed in the supplementary material figs s1 s3 the process of selecting an optimal threshold is illustrated in fig 8a with an increase in the candidate threshold the ad statistical value exhibited complex changes and the ppy value decreased the red hollow circle represents the optimal threshold fig 8b shows the peak series obtained using the pot model in this procedure to obtain relatively accurate flood characteristics the automatic mrc method was used to segment the flood hydrograph fig 8c shows the mrc obtained using the matching strip method and the analytical expressions described by maillet tallaksen 1995 the red hollow circles represent the mrc discrete data from the matching strip method and the blue line represents the mrc of the maillet method the values of the determination coefficient for the four stations were all greater than 0 9 and those of the two arid basins were greater than 0 97 thus indicating that the mrc can suitably fit the recession process fig 8c figs s1c s3c the results of the other methods mentioned in subsection 2 2 2 are presented in the supplementary material fig s4 and readers can choose different mrc methods that fit the mrc discrete data according to the gof displayed at the lower right part of the gui furthermore the pearson correlation coefficient between different flood characteristics was calculated and compared with the conventional correlation coefficient fig 8d displays scatter plots of the flood characteristics the blue hollow dots represent the flood characteristics obtained by the proposed method introduced in section 2 2 and the orange dots correspond to the conventional method the correlation coefficient of the blue dots is higher than that of the orange dots due to the consideration of the flood recession process in the proposed method figs s1 s3 the treated flood hydrograph was obtained from the output of the sfe ifc toolbox two flood events per station were randomly selected for the four basins fig 9 from these flood hydrographs it was observed that the identified start date of flood events generally corresponded to the flood rising point and that the mrc obtained via the analytical expressions described by maillet 1905 fit the recession process satisfactorily the proposed method performed well for both single and multi peaked floods compared to the other three methods the recession process obtained with the maillet method faded more quickly especially for wet basins figs s4 s5 overall these results suggest that the automatic procedure in this study can effectively capture flood hydrographs and further obtain more accurate flood characteristics 5 discussion as the pot model can capture more information about flood processes than the ams method it is widely used in flood sampling lang et al 1999 in this study the ppy values fluctuated around 2 and exhibited smaller variance for the large basins which is consistent with the results from previous literature claps and laio 2003 as for the dispersion index the value of moist basins was closer to 1 and lay within the 95 confidence intervals for almost all moist basins indicating that the method outlined in this procedure works better in moist basins than in arid basins furthermore the acceptance rate of the generalised pareto distribution at the 5 significance level was consistent with previous studies implying that the generalised pareto distribution can accurately describe the peak series from the automatic pot model durocher et al 2018 lang et al 1999 in this study we proposed an automatic generic method to determine flood characteristics the results of the correlational analysis of flood characteristics fig 8d and figs s1d s3d and the examples of hydrographs for flood events fig 9 proved that the general determination method in this study is effective and reasonable correlation analysis was an alternative method used to examine flood characteristics due to the difficulty in obtaining the actual flood characteristics compared with other methods described by researchers nadarajah and shiau 2005 daneshkhah et al 2016 y r liu et al 2020 sheng yue and rasmussen 2002 this method takes flood recession and deep baseflow into consideration as shown in fig 9 and fig s5 the identified start date of the flood event generally corresponds to the flood rising point and the mrc method fits the recession process well indicating that the proposed determination method is suitable for basins with diverse characteristics moreover it is worth noting that the developed automatic procedure is currently only applicable to daily scale data however mechanistically the method can also be extended to an hourly scale which will be addressed in future work baseflow separation is challenging because it cannot be directly measured and it can mean different things to different interest groups smakhtin 2001 therefore we did not delve into the issue in this study alternatively we defined the deep baseflow as the mean value of the discharge in the dry period of one year which is a static value the results from the correlation analysis fig 7 and the flood hydrograph fig 9 revealed that this simplification did not have a significant impact on the flood characteristic results and subsequent flood analysis one striking finding in this study was that the flood duration could last more than two months nearly twice as long as the one obtained with the conventional approach fig 9c this result seems to be inconsistent with our intuitive understanding of flood events however for large moist basins flood events can last more than 100 days najibi et al 2017 ward et al 2016 for example most of the large amazonian rivers experience a single flood period in one year and this seasonal flood lasts for nearly half a year getirana and rodrigo 2013 towner et al 2020 objectively the recession process of the flood is slow and many flood episodes in large basins do not subside to the baseflow duncan 2019 hence we considered the final flood duration thus obtained to be suitable 6 conclusions in this study we constructed a generic framework of automatic procedures for selecting flood events and identifying flood characteristics using an automatic threshold based pot model for flood sampling more importantly we proposed a generic automatic approach to determine flood characteristics using the mrc analysis methods furthermore we developed a gui i e the sfe ifc toolbox based on the matlab app designer we validated the proposed method using 423 mopex dataset stations we subsequently concluded that the pot model used in this study has good applicability to watersheds with diverse characteristics especially moist basins the peak series obtained from the automatic threshold selection method was better described by the generalised pareto distribution compared with other methods e g the mean ppy method we also reveal that the flood characteristics duration and volume determined by the proposed general method were reasonable when compared with the conventional method as the proposed method could accurately capture the flood hydrographs and further obtain additional objective flood characteristics moreover this was a general and comprehensive research on flood sampling and determination of flood characteristics which will reduce the uncertainty of future analyses such as multivariable flood frequency and flood trend analyses software and data availability toolbox name sfe ifc select flood events and identify flood characteristics software required matlab r2019a and above program language matlab contact email zhangqinwhu whu edu cn trial version link https github com zhang qin 0925 sfe ifc toolbox find main validation data the model parameter estimation experiment mopex dataset https www nws noaa gov oh mopex mo datasets htm declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the national key research and development program of china no 2017yfa0603704 major projects of national natural science foundation of china no 41890824 the strategic priority research program of the chinese academy of sciences grant no xda23040103 the strategic priority research program of the chinese academy of sciences grant no xda23040500 the national natural science foundation of china no 51809008 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105180 
25723,regional climate models are expected to exhibit improved skill at finer spatial resolutions due to improved representation of land surface heterogeneity however at spatial scales between 1 and 10 km grey scales these improvements are often illusive due to the competing benefits from spatial resolution and cumulus parameterization this study provides insights into the impact of model resolution and cumulus parameterization on precipitation prediction in the central great plains by using an object based evaluation method our results show limited improvement solely from finer resolution but larger improvement without using the cumulus scheme at a 4 km resolution compared to traditional evaluation methods the object based analysis shows that without the cumulus scheme the spatial properties of precipitation are better represented in contrast all model configurations show a dry bias in precipitation days and a tendency to produce widespread precipitation but with fewer hours with precipitation which indicates other shortcomings in the model keywords nu wrf model resolution cumulus parameterization central great plains 1 introduction with the increasing need to understand and simulate weather and climate at regional scales downscaling global climate models gcms has become an important topic both types of downscaling methods statistical downscaling and dynamic downscaling have their own advantages and drawbacks maraun et al 2010 based on the premise that there is an underlying relationship with predictor and predictand statistical downscaling provides a way to predict unknown variables such as precipitation with relatively low computational demand the downside to statistical downscaling is that it relies on the assumption that the observed statistical relationships will be the same in the future as it was in the past on the contrary dynamic downscaling is based on physical mechanism which simulates the interactions between the land and atmosphere and has the potential to change the relationship between the predictor and predictand based on changes in the modeling processes yet dynamical downscaling is more computational and time intensive which precludes it from use in many long term climate studies however the increase in high performance computation resources in recent years has enabled high resolution models 4 km to better resolve physical processes e g topographic effects local circulation cloud formation etc despite this there is still a great need to evaluate the utility and uncertainty in dynamic downscaling previous studies de sales and xue 2011 misra et al 2013 torma et al 2015 showed improved skill from using dynamical downscaling with regional climate models rcms as compared to gcms or reanalysis data there are also several programs such as coordinated regional climate downscaling experiment cordex giorgi et al 2009 and north american regional climate change assessment program narccap mearns and team 2009 that aim to investigate local scale physical processes and provide finer rcm simulations under these programs the research indicates better prediction in precipitation extremes for specific climate regions from the rcms than their coarser counterparts or their driving gcms especially in complex orographic regions elguindi and grundstein 2013 park et al 2016 prein et al 2016 this research also found better prediction of extreme precipitation than mean precipitation which indicates an added value from finer resolution rcm due to more accurate representation of topography and surface conditions however these rcm simulations are still fairly coarse at a 50 km spatial resolution from a theoretical perspective higher model skill is expected with increasing model resolution due to a more complete representation of the land surface heterogeneity nevertheless this may not be true for all situations for instance prein et al 2016 found improved precipitation in higher resolution rcms than their coarser resolution counterparts especially in mountainous regions whereas other research shows the improvements are limited mass et al 2002 li et al 2014 lee et al 2017 mass et al 2002 found the improved model skill with increasing resolution is significant at coarser above 10 km modeling simulations but less visible in finer below 10 km simulations similarly results from tripathi and dominguez 2013 showed that finer resolution models 10 km improved prediction of individual storms in summer in the southwestern us but showed little improvement for the winter months it is clear that model improvement due to increasing resolution varies by model location and season past grey zone 1 10 km grid resolution studies prein et al 2015 wootten et al 2016 have shown that both models with and without cumulus parameterization can produce comparative precipitation cumulus parameterization is used to compensate the sub grid convective processes not resolved at coarse grid scale simulations this compensation is expected to be turned off at fine resolutions due to the model s ability to resolve sub grid processes and some assumptions of the cumulus schemes becoming invalid however models may not fully resolve the convection in the grey zone for instance yu and lee 2010 found an effectively resolved resolution of 3 km in the psu ncar pennsylvania state university national center for atmospheric research mesoscale model known as mm5 for a convection band event and comparable convection structure in models with and without a modified kain fritsch kf cumulus scheme at a 6 km resolution in addition modified schemes introducing scale dependency into the convection dynamic processes e g sub grid mass updraft dynamic adjustment time scale entrainment process etc and extending the use of cumulus schemes into various resolutions are ongoing research topics zheng et al 2016 these cumulus schemes are called scale aware schemes e g grell freitas scheme multi scale kain fritsch newer tiedtke etc and are found to largely improve original schemes e g reduce excessive precipitation wet bias and present better diurnal cycle and provide comparative precipitation as models without a cumulus scheme at grey scales zheng et al 2016 gao et al 2017 jeworrek et al 2019 the grell 3 scheme used in this study is not a strict scale aware scheme but it can be applied on fine resolution models 10 km allowing subsidence spreading skamarock et al 2008 as this scheme is still widely used it is important to check and quantify the advantages of using the cumulus scheme in this grey zone there are some recent projects focusing on dynamic downscaling impact in these intermediate resolutions such as the intra center downscaling project conducted by nasa ferraro et al 2017 to test the nasa unified weather research and forecast nu wrf model peters lidard et al 2015 employing three resolution runs at 4 km 12 km and 24 km and a downscaling project conducted by ncar liu et al 2017 with wrf runs at 4 km for two 13 year simulations however the ncar project only focused on 4 km simulations and the nasa project only used cumulus parameterization for all nu wrf runs including the 4 km simulation there are studies from the nasa project demonstrating some forecast skill in precipitation iguchi et al 2017 tian et al 2017 kim et al 2018 loikith et al 2018 but these evaluations are mainly relying on a traditional grid to grid comparison framework grid to grid comparison e g pearson correlation rmse contingency table skill scores which is analogous to point to point verification is a traditional verification method widely used in model evaluation due to its simplicity it assumes that the model forecast perfectly match the observations in their spatial and temporal extent with a fine resolution model a grid value can be compared to its grid counterpart or to gauge data which locates inside the grid however due to the spatial heterogeneity and the inaccuracy and uncertainty from model simulations e g initial conditions boundary forcing and parameterization of internal model the forecast spatial distribution could be very different from observation the low spatial tolerance in grid to grid metrics may produce low skill and misleading results gilleland et al 2009 thus a more complete evaluation method that allows for small spatial and temporal displacements between the model and the observations is necessary for a comprehensive evaluation especially for fine resolution models gilleland et al 2009 summarized four commonly used types of spatial verification methods in model simulation neighborhood scale separation feature object based and field deformation these four methods have their own advantages that stress different interests and should be carefully chosen for different evaluation purposes the neighborhood method e g fractions skill score is used to evaluate properties of a spatial smoothed field similar to grid to grid metrics it allows for some spatial error but fails to reveal the spatial structure in the field the scale separation method decomposes the field into multi scale features and is useful for scale dependent evaluations field deformation methods work similar to feature object based methods which aim to match spatial features between observations and model fields however the field deformation method cannot identify object features and only works on the entire field considering the goal of evaluating the spatial pattern we chose a feature based method i e object based diagnostic evaluation which can provide more spatial information the aim of this research is to provide a more comprehensive comparison of the impact of both model resolution and configuration i e cumulus parameterization this is done by evaluating the simulated precipitation from the nasa downscaling project and from two additional simulations with and without cumulus parameterization over a region in the central us using an object based method this paper provides a different aspect especially the spatial pattern of the precipitation forecast skill of nu wrf and provides a better understanding of nu wrf performance that can be used in future studies 2 data and methods 2 1 datasets 2 1 1 nu wrf the nu wrf peters lidard et al 2015 modeling framework is based on the ncar advanced research wrf arw model skamarock et al 2008 and couples it to the land information system lis kumar et al 2006 to provide a better representation of the land surface there are two sets of nu wrf simulations used in this study 1 nu wrf runs at 4 km 12 km and 24 km spatial resolutions with cumulus scheme turned on over conus from the nasa downscaling project and 2 nu wrf runs at 4 km in a smaller region in central us with both cumulus scheme turned on cu on and off cu off spectral nudging is applied to all model runs in this study for wind geopotential height and temperature spectral nudging is a technique to assimilate upper air variables for self defined large scale in spectrum while keeping model ability to develop small scale features waldron et al 1996 von storch et al 2000 due to the different domain sizes models in 1 were nudged at 600 km and models in 2 were nudged at 800 km however this small difference in nudging should not impact the overall results lee et al 2017 except for the cumulus scheme and nudging other configurations are the same in all simulations table 1 the atmospheric initial and boundary conditions are forced by merra 2 reanalysis data with 6 h intervals grell 3d g3d grell and dévényi 2002 and uw park and bretherton 2009 schemes are used as deep and shallow cumulus scheme hereafter refer to gw scheme except in cu off simulation mellor yamada janjic myj janjić 1990 1994 2002 scheme is used for boundary layer parameterization the land surface is initialized with 10 years 1990 1999 spin up period for 1 and an even longer time from 1990 to may 31 at 2002 2004 for models in 2 in both cases the lis modeling framework was used with the noah 3 3 chen and dudhia 2001 ek et al 2003 as the land surface model three summers jja from 2002 to 2004 are chosen for analysis to make the two sets of nu wrf results comparable only eastern kansas and western missouri hereafter referred to as roi4 24 for models in 1 fig 1 are used in analysis more details of the model configuration can be found in the nasa downscaling project report ferraro et al 2017 2 1 2 stage iv stage iv hourly precipitation lin 2011 is used in this study for validation stage iv data hourly 6 hourly is a mosaicked 4 km precipitation estimate based on radar and gauge measurements and is produced by the 12 national weather service nws river forecast centers rfcs over conus any time with missing values that cover more than 15 of the study region are not included in the validation this cut off leads to 6575 valid hours 99 26 of total hours during the three summers used in the analysis 2 2 methods this study focuses on evaluating the impact of model resolution and cumulus parameterization in nu wrf in eastern kansas and western missouri 36 3⁰ n to 41 5⁰ n 100⁰ w to 92 5⁰ w top plot in fig 1 from june to august in 2002 2004 with five model runs roi4 24 cu on and cu off precipitation from nu wrf was verified using the stage iv data set we chose the 4 km map of cu on and cu off as the standard resolution map 23368 grids in roi to ensure a larger number of sample grids in the roi all other datasets were resampled to this map projection before analysis we use the nearest neighbour method for downscaling and unweighted averaging for upscaling to evaluate model precipitation p we first analyzed model skill based on grid to grid comparisons precipitation frequency bias and select skill scores sect 2 2 1 to extract a clear skill signal hourly precipitation data are summed into daily precipitation these commonly used grid to grid metrics provide a basic understanding of model skill to account for spatial patterns the method for object based diagnostic evaluation mode is used to verify the spatial patterns of precipitation considering geometry and location of precipitation events a brief description of mode is given in sect 2 2 2 the spatial verification was assessed only in cu on and cu off with hourly data to keep the accuracy of spatial distributions since the mode analysis is sensitive to validating hours in model and observation a 6 to 6 h time lag analysis is carried out to check the timing of predicted precipitation 2 2 1 skill scores used in the study there are three skill scores used in this study heidke skill score hss gilbert skill score gss and modified taylor skill score mtss they are used to assess the model skill on grid to grid comparison over the region of interest roi both hss and gss wilks 2011 measure the model s ability to produce a correct prediction compared to a random reference forecast and are easily calculated from a contingency table skill scores like the hss and gss provide a meaningful benchmark as positive values indicate more skill than a random forecast hss ranges from 1 to 1 with a perfect forecast skill score of 1 the hss measures model skill based on correctly predicting both precipitation and no precipitation cases and can be calculated from eq 1 below 1 h s s a d n a b a c b d c d n 2 1 a b a c b d c d n 2 where a is the number of times the model successfully predicts observational precipitation hit b is the count when model predicts precipitation when it was not observed false alarm c is the number of times the model predicts no precipitation in precipitation days miss d is the number of times that no precipitation is observed in both the model and the reality also considered a hit and n is the total number of predictions evaluated in contrasts the gss evaluates corrected hits which only considers event hits exclude null events this is particularly useful for evaluating rare events like precipitation the gss ranges from 1 3 to 1 and it can be calculated from eqs 2 and 3 below where the variables are the same as in equation 1 2 g s s a a b c a r e f a b c 1 a r e f a b c a a r e f a b c a r e f 3 a r e f a b a c n different from hss and gss the mtss is a skill metric based on correlation and variance of the model compared to observation mtss is based on the taylor skill score tss taylor 2001 but assumes that the maximum correlation attainable by the model is 1 and is defined by eq 4 4 m t s s 1 r 4 4 n s t d 1 n s t d 2 where r is the pearson correlation between model and observation and n s t d is the normalized standard deviation of model standard deviation of model divided by the standard deviation of observation and is 1 when the model variance equals the observational variance the mtss score ranges from 0 to 1 where 1 indicates a perfect match between the model and observations 2 2 2 mode application the method for object based diagnostic evaluation mode is based on research by davis et al 2006a 2006b and brown et al 2007 to verify field e g precipitation spatially in a way closer to human s visual judgement the main idea of this method is to measure the matching pattern in both model and observational fields in this study we use the model evaluation tools met brown et al 2021 v5 1 developed by developmental testbed center dtc in the u s to apply mode for analyzing precipitation met includes several modules such as regridding grid to point and grid to grid verification measures mode fields decomposition etc to help process and evaluate model predictions originally designed for the wrf model but can be applied to other models in this research the process of implementing the mode for evaluating model precipitation is broken up into four key steps that are visually depicted in fig 1 1 smoothing both forecast cu on and cu off and observational precipitation stage iv were firstly smoothed ps area weighted average with a circular radius in mode we chose a 4 grid square radius as in davis et al 2006b to show the main signal of precipitation distribution 2 object identification considering three different precipitation thresholds 1 mm h 1 5 mm h 1 and 10 mm h 1 event fields were identified for both model and observational data each identified separate precipitation area is then called a simple object 3 merging and matching all simple objects were then used to calculate a total interest for all pairs of objects between model and observation based on their attributes i e centroid distance boundary distance angle difference area ratio and intersection area ratio we then use the total interest threshold of 0 7 to define matched objects as suggested in brown et al 2007 as a result an object could match more than one object in the other field which are merged meanwhile to consider the possible objects caused by the same convective system we merged any objects enclosed by the same precipitation contour 70 of thresholds in step 2 which is 0 7 mm h 1 3 5 mm h 1 and 7 mm h 1 again as a cluster to simplify the later analysis to clarify the term used in this paper hereafter any matched object s is are called a cluster and the word object only refers to the simple objects before any merging takes place more details of matching algorithm can be found in met user guide https dtcenter org community code model evaluation tools met 4 spatial and time lag analysis matching results from mode step 1 3 are then analyzed to verify the model skill spatially by comparing the match miss and false alarm location and spatial extent of matched precipitation clusters to account for a potential time lag between the model and observations which could impact the interpretation this spatial analysis was repeated 13 times with time lags that range from 6 to 6 h 3 results 3 1 grid to grid comparison 3 1 1 skill scores all model and observational data were accumulated into daily precipitation days with p 1 mm d 1 are considered as precipitation events and used to summarize the contingency table of hit false alarm misses and no precipitation p 1 mm d 1 and the three corresponding skill scores hss gss and mtss are calculated table 2 presents a summary of these metrics the general precipitation hit rate is not high 20 and is nearly the same as the false alarm rate 20 for all models most non precipitation days are predicted correctly which accounts for more than half of the three summers and explains the higher hss as compared to the gss both hss and gss are larger than 0 2 in all models showing that the model skill is better than a random forecast considering the impact of the cumulus scheme cu off shows a better estimation of precipitation frequency table 2 shows that the all models with g3d scheme overestimate precipitation frequency see frequency bias by 56 67 whereas cu off only underestimates precipitation frequency by 12 9 this shows the large overestimation of precipitation frequency in g3d scheme model simulations and a slight underestimation of frequency in cu off specifically cu on shows a higher precipitation hit rate and false alarm whereas the rate of misses and no precipitation hit is higher in cu off this difference indicates the spatial error in cu off should account for its lower precipitation hit rate with a more realistic precipitation frequency although the precipitation hit is lower in cu off both its hss and gss are higher than cu on the increase in the hss and gss is mainly a result of the reduced false alarm rate which is only 7 9 in cu off but is over 19 for all models with cumulus parameterization on this indicates that cu off has better overall model skill for correctly distinguishing the occurrence of precipitation and no precipitation days despite its lower precipitation hit rate on the contrary mtss shows an opposite result with a higher skill score in cu on this is not too surprising considering the factors pearson correlation and nstd determining the mtss as it describes different aspect of model skill rather than the frequency based skill scores like hss and gss the median of pearson correlation in cu on is a bit higher 0 415 compared to 0 397 in cu off but the median of nstd is closer to 1 in cu off 1 022 than in cu on 1 099 the small difference in correlation nstd and the overall difference in mtss of 0 007 imply no clear superiority in either of the two models the contrasting results of hss gss and mtss indicate the necessity of multi aspect assessment to get the whole picture of the model performance unlike the larger differences in precipitation frequency between cu on and cu off the difference among roi4 24 is small table 2 the largest difference among roi4 24 frequency is below 1 6 and the differences of hss and gss are below 0 013 as compared to at least 5 4 differences in frequency and at least 0 02 difference in hss and gss between cu on and cu off this indicates a smaller influence of model resolution than cumulus scheme roi4 does show the highest value in all three skill scores indicating an improved prediction in the 4 km spatial resolution simulation but this improvement is not consistent with the increase in model resolution this inconsistent change in increasing model resolution could be caused by resampling uncertainty since all models are compared at 4 km resolution to validate the impact of map resolution all three skill scores was calculated at 12 km and 24 km map resolution as well the results fig 2 showed a similar comparison in that roi4 outperformed roi12 24 furthermore roi12 has lower skill scores than roi24 in nearly all map resolutions an increased difference in skill scores is found between roi4 and roi12 roi24 with the increasing map resolution which implies that the increased skill of the 4 km model is partially caused by its better representation of land surface heterogeneity and thus results in better skill scores based on a grid to grid comparison this is further supported by the difference between roi12 and roi24 which decreases with increasing map resolution in hss and roi12 even overtakes roi24 in mtss when evaluating at a 24 km resolution therefore despite the inconsistent comparison between roi12 and roi24 roi4 does show some improvement over the coarser model simulations 3 1 2 precipitation bias to present the precipitation bias clearly a probability of bias bins is calculated in fig 3 considering three cases precipitation days pobs 1 mm d 1 no precipitation days pobs 1 mm d 1 and all days the plot is created by first categorizing daily grid bias into different bins and then calculating the portion probability of each bin all models show the largest probabilities for bias 0 0 01 0 01 in all days fig 3c due to the large portion of no precipitation hits fig 3b for precipitation days on the contrary all models have a bias all the time probability of 99 9 in fig 3a it is clear that the bias distribution is different depending on whether it rains or not there is a slight wet bias 0 2 mm in nearly all models for no precipitation days fig 3b but a slight dry bias 4 0 mm in precipitation days fig 3a the overall performance is different for each model fig 3c all models with gw scheme cu on and roi4 24 show an overall wet bias 17 22 which is mainly due to the wet bias in no precipitation pobs 1 mm d 1 events that account for 76 47 of the time on the contrary the positive and negative bias are more evenly distributed in cu off the difference between cu on and cu off indicates that gw scheme produces slightly more precipitation for about 8 of time in our study region therefore cu off has better skill with less precipitation bias compared to cu on the relative suppression in cu off is consistent with its frequency analysis table 2 which has less false alarms more misses and no precipitation days there is no improvement in the bias for finer resolution models unlike the skill scores the bias increases from roi24 to roi4 based on the decreasing no bias probability this unexpected result may be caused by using gw scheme whose assumptions may not be appropriate for finer resolutions nevertheless the total difference of bias probability bias not equal to zero between roi4 24 less than 4 in fig 3c is much smaller than the difference between cu on and cu off 13 which indicates there is less impact due to model resolution as compared to cumulus parameterization the gw scheme explains the small wet bias in no precipitation days but the cause of the mutual small dry bias in precipitation days is unknown moreover all models show similar probabilities of extreme bias 10 mm or 10 mm which are even larger than the probabilities of small bias 2 0 mm and 0 2 mm in precipitation days fig 3a only about 4 of positive or negative extreme bias in precipitation days are explained by the use of gw scheme fig 4 shows a spatial distribution of days with extreme bias when precipitation occurs pobs 1 mm d 1 in cu on the extreme bias is widely distributed and does not spatially compensate between wet days and dry days all models not shown produced a similar distribution of the extremely dry bias with cores count 20 days in north kansas midwest missouri and northeastern oklahoma consistent with our previous analysis cu off shows a suppression of precipitation spatially compared to cu on with more days of extreme dry bias than days of extreme wet bias 3 2 object based analysis 3 2 1 spatial analysis the previous grid to grid analysis shows very limited difference of model skill among roi4 24 which may be caused by the same configuration except for their resolutions therefore the spatial and time lag analyses sect 3 2 2 are only conducted on the cu on and cu off model simulations to assess the spatial matching performance hourly simple objects identified from mode are analyzed to get the probability of match miss and false alarm in table 3 this table is however different from a normal contingency table which gives a total probability of 1 based on objects the counts of three measurements are defined as 1 match the minimum number of objects between matched clusters 2 miss the number of unmatched objects in observation 3 false alarm number of additional objects in the model compared to matched objects the probability of the three measurements in table 3 are then calculated differently match and miss probability are the portion of matched or missed objects in observation match or miss counts total observational objects which give the sum of two probabilities equal to 1 as a contrast the probability of false alarm is referenced to model objects false alarm counts total model objects the reason for choosing simple objects instead of matched clusters to count the probability is due to the matching process in mode the mode treats all unmatched objects as a whole cluster even though they may belong to different events the incapability of recognizing multiple unmatched events clusters by the algorithm makes standard use of the cluster method inappropriate to assess the probability of matching from table 3 both cu on and cu off have a high miss rate 60 and a large false alarm rate 70 especially for intense precipitation events indicating spatial or and temporal mismatch of precipitation events to better understand the cause of the low matching rate the total number of objects and a ratio of event hours hours with objects in the model hours with objects in the observations table 4 are used to show the event frequency in the model except in cu on when ps 1 mm h 1 both models in all precipitation intensities produced overall equivalent or less objects than observation which seems to contradict the high false alarm rate however nearly all ratios of event hours are less than 1 which indicates widespread precipitation more objects also confirmed by the area ratio much larger than 1 in fig 7 with less temporal frequency fewer hours with precipitation produced by the model a more detailed event comparison can be seen from fig 5 both models overestimate events objm objo and this overestimation decreases with increasing intensity from 30 50 to 10 20 this decreasing trend is opposite to the increasing false alarm rate with more intense events table 3 and again strengthens the idea that both models produce more precipitation events in a short time period especially for high intensity events in contrast both models tend to produce fewer events 55 in rainy hours objo 0 which explains the high miss rate in table 3 thus the high miss rate and false alarm rate are caused in different ways despite the large portion of no precipitation hit hours objm objo is more than 59 of time in fig 5b when comparing the two models there is a large difference in matching probability table 3 and time components fig 5 of light precipitation events for ps 1 mm h 1 observational events fig 5a cu on overestimated precipitation objects for most of the time 53 whereas cu off only overestimated for 30 of the time similarly for no observational events fig 5b cu on overestimated 41 of the time which is 17 more time than cu off this large difference is only found in light rain events 1 mm h 1 ps 5 mm h 1 whereas both models have nearly the same matching probability and time components for ps 5 mm h 1 and ps 10 mm h 1 although cu on overestimated light rain events 20 more often than cu off fig 5 it produced a 10 5 higher matching probability table 3 this seemingly contradiction could be explained by the different reference that matching probability is compared to observational objects while the false alarm rate is reference to model objects the largely overproduced objects almost the same number of observational objects in table 4 for light precipitation in cu on not only increased the denominator total model objects of false alarm rate but also increased the matching observational objects as well in addition to evaluate matching rate based on simple objects it is also informative to further assess the matching quality based on clusters before digging into the quality evaluation fig 6 provides a histogram of event scale area for all matched clusters for three different intensity thresholds clearly both the number of events and precipitation area decrease with intensity the logarithmic area distribution shows the general precipitation scale smaller than 10000 km2 50 percentile for ps 1 mm h 1 for cu on and 5700 km2 for cu off although cu off produced smaller scale precipitation compared to cu on fig 6a the scale of intense precipitation is larger fig 6c ps 10 mm h 1 similarly cu off has less matched precipitation clusters at low intensities but more for intense events this indicates a preference of light yet larger scale precipitation in cu on whereas cu off produced relatively more intense but smaller scale precipitation by checking the intensity ratios of matched clusters table s1 we found that the cu on tends to overestimate light rain intensity while underestimating heavy rain intensity the cu off generally produces rain intensity closer to observation but it overestimates heavy rain intensity in order to assess the matching quality of the spatial pattern we use spatial error distance between mass centers and area ratio area of model cluster am area of observational cluster ao of matched clusters to imply spatial accuracy it should be noted that the area ratio here is different from the definition in met the area ratio in met is defined as the lesser object area over the larger object area which is less informative for comparative analysis as the reference changes based on which object is larger to present the main performance of the two models 10 of the data are removed from one side centroid distance or both sides area ratio to exclude extreme values fig 7 both cu on and cu off have a large range of spatial error with the mean around 58 70 km the cu off seems to show a better forecast in location with a shorter mean distance but the mean difference between two models is small 4 6 km smaller in cu off results in area ratio fig 7b shows that both models overestimate precipitation scale all mean area ratio are at least 2 times greater unlike the spatial error not all mean area ratios are closer to 1 in cu off especially for ps 10 mm h 1 although cu off does not predict better precipitation scale than cu on in all intensities it shows a smaller variation in both spatial measurements indicating a more stable performance of model skill in the case of different intensities the inconsistent change with intensity in cu off contrasts that from cu on which is found to reduce the matching distance and area ratio for higher intensity events this difference may relate to the use of the cumulus scheme however with only one specific cumulus scheme gw used in this study the relationship between spatial prediction and precipitation intensity while using the cumulus scheme is not certain further research using other cumulus schemes to test the trend is left for future work 3 2 2 time lag analysis the results above are sensitive to time lag which could mislead the interpretation of model skill especially with at least 10 of the time when both cu on and cu off have missed or overproduced the precipitation events ratio of event hours in table 4 to assess the temporal difference we analyzed the time lag influence by repeating the spatial matching process 13 times with up to 6 h lags of both prior negative and subsequent positive forecasts a comparison of spatial error and area ratio of matched clusters is present in fig 8 for low intensity events ps 1 mm h 1 both models show clear decreasing trends in matching distance as the time lag moves towards 0 fig 8a similarly the area ratio is generally lower around lag 0 and increases with larger time lags when ps 1 mm h 1 although the smallest value of cu on occurs at time lag of 1 h the little difference in matching distance between lag 1 h and lag 0 h could be caused by its larger number of simple objects in matched clusters with largely overproduced precipitation events of ps 1 mm h 1 table 4 the number of simple objects formed a cluster in cu on is more than its counterparts in cu off the more simple objects in the cluster the more uncertain the spatial error of the cluster is shifted meanwhile more simple objects in a cluster also leads to a larger cluster area which results in a larger area ratio this explains the better performance of cu off which shows a 5 4 km shorter mean matching distance and 3 1 smaller area ratio than cu on fig 8a nevertheless all clear trends and comparison along the time lag axis disappear in moderate to intense precipitation events fig 8b c indicating poor skill in both models to predict spatial and temporal patterns for these events these clear trends are also found in 5th and 95th percentile data fig s1 although the matching distance and area ratio have a large variation can be seen from fig 7 the overall trend shows the peak and trough shortest distance and area ratio closest to 1 around lag 0 likewise the time dependency of the 5th and 95th percentile decreases as precipitation intensity increases 4 discussion the improvement in model skill caused by finer resolution is very limited in our study although improvement is not found from 24 km to 12 km roi4 shows slightly better prediction in frequency table 2 in all three skill scores the worse bias probability fig 3 in roi4 could be explained by gw scheme which was designed to operate at coarse resolutions similarly an hourly precipitation analysis by lee et al 2017 shows both larger diurnal bias and better overlap of joint probability distribution function jpdf between duration and peak intensity in 4 km nu wrf model compared to the coarser models in central plains the contrasting results of both our daily analysis and lee s hourly analysis point out the limited value added by finer resolution of nu wrf models in this region one of the possible reasons is that the spatial heterogeneity in central plain is less than other regions such as rockies where there is a larger elevation expanse as mentioned in mass et al 2002 there is a point of diminishing returns in forecast models and it varies from finer resolution 10 15 km in western u s to a coarser resolution 20 40 km in the eastern part where mesoscale features are already well resolved in coarser resolution because of the flatter terrain thus the small added value from surface heterogeneity in our research area competes with error caused by the gw scheme being applied at finer resolutions in addition as spectral nudging is used in this study deficiencies in merra 2 could be introduced to impact the physical solution of the model and hence reduced the improvement by the finer resolution models this study also shows some interesting findings there is a slightly dry bias and about 35 extreme bias p 10 mm d 1 and p 10 mm d 1 in precipitation days in all five models fig 3 these mutual biases in all five models indicate dominate factors other than cumulus parameterization and model resolution one possible cause could be from the merra 2 boundary condition to analyze this we use model precipitation not gauge corrected from merra 2 as a reference instead of gauge corrected precipitation to compare the spatial similarity between the nu wrf and merra 2 if there is a similar pattern between merra 2 and our result we are not able exclude the possibility that the boundary condition might be a reason for the bias fig 9 shows the hovmöller diagram of precipitation mean bias for merra 2 and the cu on and cu off model runs in june 2004 the full three summer performance is not shown here but it provides similar results there is not a strong trend for dry or wet bias in merra 2 the same is true for the eastward propagation besides the larger and wider wet bias in cu on the spatial patterns of bias in cu on and cu off are quite similar as in merra 2 from a coarser temporal scale especially for the extreme bias this similarity supports the idea that merra 2 boundary conditions and nudging may be the cause for the overall bias in all model runs however to fully understand the cause of this bias requires more analysis including considering other boundary forcings which is beyond the scope of this study besides the boundary condition other model parameterizations such as radiation pbl and the land surface all play important roles in predicting precipitation and may also contribute to the bias li et al 2014 pei et al 2014 hirsch et al 2019 it should be noted that the impact from these parameterizations may vary with domain model and the research period in addition the lack of convection propagation liu et al 2017 and the time inconsistency between model and observation can also result in bias although the contribution from these factors were not checked in this study they provide potential causes that can be explored for future studies to better understand shortcomings in precipitation prediction our study not only analyzed the spatial pattern but also revealed an imperfect pattern of forming precipitation in cu on and cu off models the low matching rate and the poor matching quality in both models show the large mismatch of precipitation events in terms of spatial pattern although there is no consistent time lag in cu on and cu off the large spatial discrepancy could be partly caused by the model patterns the largely overestimated objects table 4 and 55 overestimation time fig 5 in cu on for light rain events p 1 mm h 1 contrasts with its intense precipitation cases and indicate an easy triggering for shallow convection in cu on with a larger rain area fig 7 this is consistent with research of yu and lee 2010 who found the excessive cumulus rain region reduced after modified trigger function in kain fritsch kf scheme however for more intense precipitation and cu off simulations fewer hours with events are produced indicating a possible obstacle to initializing precipitation although the generated precipitation area is still larger than observation fig 7 this may relate to the deep convection in the model which contributes more intense precipitation interestingly this pattern is clearer in cu on with even less precipitation events for p 10 mm h 1 than cu off because the cumulus precipitation can be an adding component representing sub grid processes the less intense events in cu on indicates insufficient ability to develop deep convection using g3d scheme only 15 of time both models correctly predict event counts fig 5a this time portion is stable despite the model and precipitation intensities which points the problem towards the other aspects of the nu wrf model rather than the use of cumulus parameterization there are some uncertainties in this study such as the uncertainty of resampling the choice of observational dataset and the probability of object detection near the edges however our preliminary tests fig 2 show small resampling impact the change of map resolution does not change the relative position among models in nearly all three skill scores the variation in three resolution standards is within 14 with an average of 6 7 and 10 for hss gss and mtss respectively all 4 km models roi4 cu on and cu off show higher skill scores than roi12 24 in all three resolutions roi12 has lower skill scores than roi24 in most map resolutions but the difference between them is less than 0 01 therefore the decision of using 4 km map as the standard projection in this study should not make a big impact on our analysis as for the uncertainty from observational dataset we processed a similar analysis in preliminary test not shown using livneh daily precipitation with a smaller roi in five julys from 2000 to 2004 again the impact of model resolution is separate from the use of cumulus parameterization results from the test are similar to this study the overall wet bias in all times the slightly dry bias and more than 28 probability of extreme bias in precipitation days are all reproduced the improvement in finer resolution model is small mtss of roi4 roi12 and roi24 are 0 166 0 151 and 0 151 respectively at 0 0625 degree map the test used a simplified version of mode analysis which only considered centroid distance the results also showed a general shorter matching distance in cu off compared to cu on while both models overpredicted the precipitation spatial extent therefore we think the choice of observational data does not change our conclusion the uncertainty of object detection relies on the domain size the location of the object and the searching distance for possible matching objects to evaluate this uncertainty we calculated a probability of detection supplementary material and found our study has at least a 76 67 of probability to detect 90 of the matched objects based on this we think the results presented in this paper are overall reliable measures of the model skill it is known that precipitation forecast is sensitive to the different cumulus schemes applied because of their different closure assumptions and scale considerations wang and seaman 1997 the uw scheme a convective inhibition based scheme is found to produce deeper daily pbl growth triggers shallower development of clouds compared to zhang mcfarlane scheme a convective available potential energy based scheme williams 2019 and reduce the precipitation compared to native shallow scheme in g3d iguchi et al 2017 research of iguchi et al 2017 also shows more negative daily precipitation bias in gw model compared to models using the new simplified arakawa schubert scheme nsas han and pan 2011 new kain fritsch scheme nkf kain 2004 and betts miller janjic scheme bmj in summer central plain this indicates that excessive precipitation is a common problem with these cumulus schemes applied in the central plains and thus may aggravate the wet bias in this region however there are also other scale aware cumulus schemes under development that provide promising results kwon and hong 2017 introduced three aspects of scale aware dependency into simplified arakawa schubert scheme gsas and found it outperforms precipitation prediction in no cumulus simulation at both 3 km and 1 km scale in south korea the main contribution of the improvement in their study comes from including scale dependency into cloud base mass flux therefore it is still probable to see improvements using scale aware cumulus schemes in the future 5 conclusion this study evaluates precipitation performance in five nu wrf simulations although there is little improvement found with increasing model resolution it may be due to the domain region small elevation variation and model configuration the use of gw scheme and nudging which could restrict or reduce the difference it is important to stress that the resolution impact analysis is from roi4 24 models which all use gw scheme this is different from some previous studies that turned off cumulus scheme at 4 km and resulted in more obvious improvement analogous to comparison between cu off and roi12 24 here we separated the two impacts and found that the cumulus parameterization is more influential on model prediction than model resolution although cu off shows better precipitation pattern than cu on it still has large inaccuracy especially in a grid to grid comparison the advantage of the no cumulus scheme model should become more obvious and more accurately represent the land surface spatial heterogeneity with finer resolutions such as 1 km although our study shows the model without cumulus parameterization outperforms the model with gw scheme at 4 km resolution it may still be possible to see comparative model skill with the use of a scale aware scheme in the future software availability model nu wrf developer nasa gsfc and affiliate contact toshi matsui toshihisa matsui 1 nasa gov and carlos cruz carlos a cruz nasa gov availability nu wrf model is not publicly available but the source code and data can be requested via contact more information is at https nuwrf gsfc nasa gov software first released year 2011 software model evaluation tools met developer developmental testbed center dtc availability download from https www dtcenter org community code model evaluation tools met cost free first released year 2008 data availability stage iv precipitation can be accessed from ncar eol website https data eol ucar edu dataset 21 093 nu wrf and met mode configuration files used in this study can be found at https hydrology ku edu data html merra 2 data is available at https gmao gsfc nasa gov reanalysis merra 2 livneh data is available at https psl noaa gov data gridded data livneh html funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the hydrological sciences laboratory of nasa goddard space flight center for providing their conus nu wrf data and ncar eol https data eol ucar edu for providing stage iv data under the sponsorship of the national science foundation appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105184 
25723,regional climate models are expected to exhibit improved skill at finer spatial resolutions due to improved representation of land surface heterogeneity however at spatial scales between 1 and 10 km grey scales these improvements are often illusive due to the competing benefits from spatial resolution and cumulus parameterization this study provides insights into the impact of model resolution and cumulus parameterization on precipitation prediction in the central great plains by using an object based evaluation method our results show limited improvement solely from finer resolution but larger improvement without using the cumulus scheme at a 4 km resolution compared to traditional evaluation methods the object based analysis shows that without the cumulus scheme the spatial properties of precipitation are better represented in contrast all model configurations show a dry bias in precipitation days and a tendency to produce widespread precipitation but with fewer hours with precipitation which indicates other shortcomings in the model keywords nu wrf model resolution cumulus parameterization central great plains 1 introduction with the increasing need to understand and simulate weather and climate at regional scales downscaling global climate models gcms has become an important topic both types of downscaling methods statistical downscaling and dynamic downscaling have their own advantages and drawbacks maraun et al 2010 based on the premise that there is an underlying relationship with predictor and predictand statistical downscaling provides a way to predict unknown variables such as precipitation with relatively low computational demand the downside to statistical downscaling is that it relies on the assumption that the observed statistical relationships will be the same in the future as it was in the past on the contrary dynamic downscaling is based on physical mechanism which simulates the interactions between the land and atmosphere and has the potential to change the relationship between the predictor and predictand based on changes in the modeling processes yet dynamical downscaling is more computational and time intensive which precludes it from use in many long term climate studies however the increase in high performance computation resources in recent years has enabled high resolution models 4 km to better resolve physical processes e g topographic effects local circulation cloud formation etc despite this there is still a great need to evaluate the utility and uncertainty in dynamic downscaling previous studies de sales and xue 2011 misra et al 2013 torma et al 2015 showed improved skill from using dynamical downscaling with regional climate models rcms as compared to gcms or reanalysis data there are also several programs such as coordinated regional climate downscaling experiment cordex giorgi et al 2009 and north american regional climate change assessment program narccap mearns and team 2009 that aim to investigate local scale physical processes and provide finer rcm simulations under these programs the research indicates better prediction in precipitation extremes for specific climate regions from the rcms than their coarser counterparts or their driving gcms especially in complex orographic regions elguindi and grundstein 2013 park et al 2016 prein et al 2016 this research also found better prediction of extreme precipitation than mean precipitation which indicates an added value from finer resolution rcm due to more accurate representation of topography and surface conditions however these rcm simulations are still fairly coarse at a 50 km spatial resolution from a theoretical perspective higher model skill is expected with increasing model resolution due to a more complete representation of the land surface heterogeneity nevertheless this may not be true for all situations for instance prein et al 2016 found improved precipitation in higher resolution rcms than their coarser resolution counterparts especially in mountainous regions whereas other research shows the improvements are limited mass et al 2002 li et al 2014 lee et al 2017 mass et al 2002 found the improved model skill with increasing resolution is significant at coarser above 10 km modeling simulations but less visible in finer below 10 km simulations similarly results from tripathi and dominguez 2013 showed that finer resolution models 10 km improved prediction of individual storms in summer in the southwestern us but showed little improvement for the winter months it is clear that model improvement due to increasing resolution varies by model location and season past grey zone 1 10 km grid resolution studies prein et al 2015 wootten et al 2016 have shown that both models with and without cumulus parameterization can produce comparative precipitation cumulus parameterization is used to compensate the sub grid convective processes not resolved at coarse grid scale simulations this compensation is expected to be turned off at fine resolutions due to the model s ability to resolve sub grid processes and some assumptions of the cumulus schemes becoming invalid however models may not fully resolve the convection in the grey zone for instance yu and lee 2010 found an effectively resolved resolution of 3 km in the psu ncar pennsylvania state university national center for atmospheric research mesoscale model known as mm5 for a convection band event and comparable convection structure in models with and without a modified kain fritsch kf cumulus scheme at a 6 km resolution in addition modified schemes introducing scale dependency into the convection dynamic processes e g sub grid mass updraft dynamic adjustment time scale entrainment process etc and extending the use of cumulus schemes into various resolutions are ongoing research topics zheng et al 2016 these cumulus schemes are called scale aware schemes e g grell freitas scheme multi scale kain fritsch newer tiedtke etc and are found to largely improve original schemes e g reduce excessive precipitation wet bias and present better diurnal cycle and provide comparative precipitation as models without a cumulus scheme at grey scales zheng et al 2016 gao et al 2017 jeworrek et al 2019 the grell 3 scheme used in this study is not a strict scale aware scheme but it can be applied on fine resolution models 10 km allowing subsidence spreading skamarock et al 2008 as this scheme is still widely used it is important to check and quantify the advantages of using the cumulus scheme in this grey zone there are some recent projects focusing on dynamic downscaling impact in these intermediate resolutions such as the intra center downscaling project conducted by nasa ferraro et al 2017 to test the nasa unified weather research and forecast nu wrf model peters lidard et al 2015 employing three resolution runs at 4 km 12 km and 24 km and a downscaling project conducted by ncar liu et al 2017 with wrf runs at 4 km for two 13 year simulations however the ncar project only focused on 4 km simulations and the nasa project only used cumulus parameterization for all nu wrf runs including the 4 km simulation there are studies from the nasa project demonstrating some forecast skill in precipitation iguchi et al 2017 tian et al 2017 kim et al 2018 loikith et al 2018 but these evaluations are mainly relying on a traditional grid to grid comparison framework grid to grid comparison e g pearson correlation rmse contingency table skill scores which is analogous to point to point verification is a traditional verification method widely used in model evaluation due to its simplicity it assumes that the model forecast perfectly match the observations in their spatial and temporal extent with a fine resolution model a grid value can be compared to its grid counterpart or to gauge data which locates inside the grid however due to the spatial heterogeneity and the inaccuracy and uncertainty from model simulations e g initial conditions boundary forcing and parameterization of internal model the forecast spatial distribution could be very different from observation the low spatial tolerance in grid to grid metrics may produce low skill and misleading results gilleland et al 2009 thus a more complete evaluation method that allows for small spatial and temporal displacements between the model and the observations is necessary for a comprehensive evaluation especially for fine resolution models gilleland et al 2009 summarized four commonly used types of spatial verification methods in model simulation neighborhood scale separation feature object based and field deformation these four methods have their own advantages that stress different interests and should be carefully chosen for different evaluation purposes the neighborhood method e g fractions skill score is used to evaluate properties of a spatial smoothed field similar to grid to grid metrics it allows for some spatial error but fails to reveal the spatial structure in the field the scale separation method decomposes the field into multi scale features and is useful for scale dependent evaluations field deformation methods work similar to feature object based methods which aim to match spatial features between observations and model fields however the field deformation method cannot identify object features and only works on the entire field considering the goal of evaluating the spatial pattern we chose a feature based method i e object based diagnostic evaluation which can provide more spatial information the aim of this research is to provide a more comprehensive comparison of the impact of both model resolution and configuration i e cumulus parameterization this is done by evaluating the simulated precipitation from the nasa downscaling project and from two additional simulations with and without cumulus parameterization over a region in the central us using an object based method this paper provides a different aspect especially the spatial pattern of the precipitation forecast skill of nu wrf and provides a better understanding of nu wrf performance that can be used in future studies 2 data and methods 2 1 datasets 2 1 1 nu wrf the nu wrf peters lidard et al 2015 modeling framework is based on the ncar advanced research wrf arw model skamarock et al 2008 and couples it to the land information system lis kumar et al 2006 to provide a better representation of the land surface there are two sets of nu wrf simulations used in this study 1 nu wrf runs at 4 km 12 km and 24 km spatial resolutions with cumulus scheme turned on over conus from the nasa downscaling project and 2 nu wrf runs at 4 km in a smaller region in central us with both cumulus scheme turned on cu on and off cu off spectral nudging is applied to all model runs in this study for wind geopotential height and temperature spectral nudging is a technique to assimilate upper air variables for self defined large scale in spectrum while keeping model ability to develop small scale features waldron et al 1996 von storch et al 2000 due to the different domain sizes models in 1 were nudged at 600 km and models in 2 were nudged at 800 km however this small difference in nudging should not impact the overall results lee et al 2017 except for the cumulus scheme and nudging other configurations are the same in all simulations table 1 the atmospheric initial and boundary conditions are forced by merra 2 reanalysis data with 6 h intervals grell 3d g3d grell and dévényi 2002 and uw park and bretherton 2009 schemes are used as deep and shallow cumulus scheme hereafter refer to gw scheme except in cu off simulation mellor yamada janjic myj janjić 1990 1994 2002 scheme is used for boundary layer parameterization the land surface is initialized with 10 years 1990 1999 spin up period for 1 and an even longer time from 1990 to may 31 at 2002 2004 for models in 2 in both cases the lis modeling framework was used with the noah 3 3 chen and dudhia 2001 ek et al 2003 as the land surface model three summers jja from 2002 to 2004 are chosen for analysis to make the two sets of nu wrf results comparable only eastern kansas and western missouri hereafter referred to as roi4 24 for models in 1 fig 1 are used in analysis more details of the model configuration can be found in the nasa downscaling project report ferraro et al 2017 2 1 2 stage iv stage iv hourly precipitation lin 2011 is used in this study for validation stage iv data hourly 6 hourly is a mosaicked 4 km precipitation estimate based on radar and gauge measurements and is produced by the 12 national weather service nws river forecast centers rfcs over conus any time with missing values that cover more than 15 of the study region are not included in the validation this cut off leads to 6575 valid hours 99 26 of total hours during the three summers used in the analysis 2 2 methods this study focuses on evaluating the impact of model resolution and cumulus parameterization in nu wrf in eastern kansas and western missouri 36 3⁰ n to 41 5⁰ n 100⁰ w to 92 5⁰ w top plot in fig 1 from june to august in 2002 2004 with five model runs roi4 24 cu on and cu off precipitation from nu wrf was verified using the stage iv data set we chose the 4 km map of cu on and cu off as the standard resolution map 23368 grids in roi to ensure a larger number of sample grids in the roi all other datasets were resampled to this map projection before analysis we use the nearest neighbour method for downscaling and unweighted averaging for upscaling to evaluate model precipitation p we first analyzed model skill based on grid to grid comparisons precipitation frequency bias and select skill scores sect 2 2 1 to extract a clear skill signal hourly precipitation data are summed into daily precipitation these commonly used grid to grid metrics provide a basic understanding of model skill to account for spatial patterns the method for object based diagnostic evaluation mode is used to verify the spatial patterns of precipitation considering geometry and location of precipitation events a brief description of mode is given in sect 2 2 2 the spatial verification was assessed only in cu on and cu off with hourly data to keep the accuracy of spatial distributions since the mode analysis is sensitive to validating hours in model and observation a 6 to 6 h time lag analysis is carried out to check the timing of predicted precipitation 2 2 1 skill scores used in the study there are three skill scores used in this study heidke skill score hss gilbert skill score gss and modified taylor skill score mtss they are used to assess the model skill on grid to grid comparison over the region of interest roi both hss and gss wilks 2011 measure the model s ability to produce a correct prediction compared to a random reference forecast and are easily calculated from a contingency table skill scores like the hss and gss provide a meaningful benchmark as positive values indicate more skill than a random forecast hss ranges from 1 to 1 with a perfect forecast skill score of 1 the hss measures model skill based on correctly predicting both precipitation and no precipitation cases and can be calculated from eq 1 below 1 h s s a d n a b a c b d c d n 2 1 a b a c b d c d n 2 where a is the number of times the model successfully predicts observational precipitation hit b is the count when model predicts precipitation when it was not observed false alarm c is the number of times the model predicts no precipitation in precipitation days miss d is the number of times that no precipitation is observed in both the model and the reality also considered a hit and n is the total number of predictions evaluated in contrasts the gss evaluates corrected hits which only considers event hits exclude null events this is particularly useful for evaluating rare events like precipitation the gss ranges from 1 3 to 1 and it can be calculated from eqs 2 and 3 below where the variables are the same as in equation 1 2 g s s a a b c a r e f a b c 1 a r e f a b c a a r e f a b c a r e f 3 a r e f a b a c n different from hss and gss the mtss is a skill metric based on correlation and variance of the model compared to observation mtss is based on the taylor skill score tss taylor 2001 but assumes that the maximum correlation attainable by the model is 1 and is defined by eq 4 4 m t s s 1 r 4 4 n s t d 1 n s t d 2 where r is the pearson correlation between model and observation and n s t d is the normalized standard deviation of model standard deviation of model divided by the standard deviation of observation and is 1 when the model variance equals the observational variance the mtss score ranges from 0 to 1 where 1 indicates a perfect match between the model and observations 2 2 2 mode application the method for object based diagnostic evaluation mode is based on research by davis et al 2006a 2006b and brown et al 2007 to verify field e g precipitation spatially in a way closer to human s visual judgement the main idea of this method is to measure the matching pattern in both model and observational fields in this study we use the model evaluation tools met brown et al 2021 v5 1 developed by developmental testbed center dtc in the u s to apply mode for analyzing precipitation met includes several modules such as regridding grid to point and grid to grid verification measures mode fields decomposition etc to help process and evaluate model predictions originally designed for the wrf model but can be applied to other models in this research the process of implementing the mode for evaluating model precipitation is broken up into four key steps that are visually depicted in fig 1 1 smoothing both forecast cu on and cu off and observational precipitation stage iv were firstly smoothed ps area weighted average with a circular radius in mode we chose a 4 grid square radius as in davis et al 2006b to show the main signal of precipitation distribution 2 object identification considering three different precipitation thresholds 1 mm h 1 5 mm h 1 and 10 mm h 1 event fields were identified for both model and observational data each identified separate precipitation area is then called a simple object 3 merging and matching all simple objects were then used to calculate a total interest for all pairs of objects between model and observation based on their attributes i e centroid distance boundary distance angle difference area ratio and intersection area ratio we then use the total interest threshold of 0 7 to define matched objects as suggested in brown et al 2007 as a result an object could match more than one object in the other field which are merged meanwhile to consider the possible objects caused by the same convective system we merged any objects enclosed by the same precipitation contour 70 of thresholds in step 2 which is 0 7 mm h 1 3 5 mm h 1 and 7 mm h 1 again as a cluster to simplify the later analysis to clarify the term used in this paper hereafter any matched object s is are called a cluster and the word object only refers to the simple objects before any merging takes place more details of matching algorithm can be found in met user guide https dtcenter org community code model evaluation tools met 4 spatial and time lag analysis matching results from mode step 1 3 are then analyzed to verify the model skill spatially by comparing the match miss and false alarm location and spatial extent of matched precipitation clusters to account for a potential time lag between the model and observations which could impact the interpretation this spatial analysis was repeated 13 times with time lags that range from 6 to 6 h 3 results 3 1 grid to grid comparison 3 1 1 skill scores all model and observational data were accumulated into daily precipitation days with p 1 mm d 1 are considered as precipitation events and used to summarize the contingency table of hit false alarm misses and no precipitation p 1 mm d 1 and the three corresponding skill scores hss gss and mtss are calculated table 2 presents a summary of these metrics the general precipitation hit rate is not high 20 and is nearly the same as the false alarm rate 20 for all models most non precipitation days are predicted correctly which accounts for more than half of the three summers and explains the higher hss as compared to the gss both hss and gss are larger than 0 2 in all models showing that the model skill is better than a random forecast considering the impact of the cumulus scheme cu off shows a better estimation of precipitation frequency table 2 shows that the all models with g3d scheme overestimate precipitation frequency see frequency bias by 56 67 whereas cu off only underestimates precipitation frequency by 12 9 this shows the large overestimation of precipitation frequency in g3d scheme model simulations and a slight underestimation of frequency in cu off specifically cu on shows a higher precipitation hit rate and false alarm whereas the rate of misses and no precipitation hit is higher in cu off this difference indicates the spatial error in cu off should account for its lower precipitation hit rate with a more realistic precipitation frequency although the precipitation hit is lower in cu off both its hss and gss are higher than cu on the increase in the hss and gss is mainly a result of the reduced false alarm rate which is only 7 9 in cu off but is over 19 for all models with cumulus parameterization on this indicates that cu off has better overall model skill for correctly distinguishing the occurrence of precipitation and no precipitation days despite its lower precipitation hit rate on the contrary mtss shows an opposite result with a higher skill score in cu on this is not too surprising considering the factors pearson correlation and nstd determining the mtss as it describes different aspect of model skill rather than the frequency based skill scores like hss and gss the median of pearson correlation in cu on is a bit higher 0 415 compared to 0 397 in cu off but the median of nstd is closer to 1 in cu off 1 022 than in cu on 1 099 the small difference in correlation nstd and the overall difference in mtss of 0 007 imply no clear superiority in either of the two models the contrasting results of hss gss and mtss indicate the necessity of multi aspect assessment to get the whole picture of the model performance unlike the larger differences in precipitation frequency between cu on and cu off the difference among roi4 24 is small table 2 the largest difference among roi4 24 frequency is below 1 6 and the differences of hss and gss are below 0 013 as compared to at least 5 4 differences in frequency and at least 0 02 difference in hss and gss between cu on and cu off this indicates a smaller influence of model resolution than cumulus scheme roi4 does show the highest value in all three skill scores indicating an improved prediction in the 4 km spatial resolution simulation but this improvement is not consistent with the increase in model resolution this inconsistent change in increasing model resolution could be caused by resampling uncertainty since all models are compared at 4 km resolution to validate the impact of map resolution all three skill scores was calculated at 12 km and 24 km map resolution as well the results fig 2 showed a similar comparison in that roi4 outperformed roi12 24 furthermore roi12 has lower skill scores than roi24 in nearly all map resolutions an increased difference in skill scores is found between roi4 and roi12 roi24 with the increasing map resolution which implies that the increased skill of the 4 km model is partially caused by its better representation of land surface heterogeneity and thus results in better skill scores based on a grid to grid comparison this is further supported by the difference between roi12 and roi24 which decreases with increasing map resolution in hss and roi12 even overtakes roi24 in mtss when evaluating at a 24 km resolution therefore despite the inconsistent comparison between roi12 and roi24 roi4 does show some improvement over the coarser model simulations 3 1 2 precipitation bias to present the precipitation bias clearly a probability of bias bins is calculated in fig 3 considering three cases precipitation days pobs 1 mm d 1 no precipitation days pobs 1 mm d 1 and all days the plot is created by first categorizing daily grid bias into different bins and then calculating the portion probability of each bin all models show the largest probabilities for bias 0 0 01 0 01 in all days fig 3c due to the large portion of no precipitation hits fig 3b for precipitation days on the contrary all models have a bias all the time probability of 99 9 in fig 3a it is clear that the bias distribution is different depending on whether it rains or not there is a slight wet bias 0 2 mm in nearly all models for no precipitation days fig 3b but a slight dry bias 4 0 mm in precipitation days fig 3a the overall performance is different for each model fig 3c all models with gw scheme cu on and roi4 24 show an overall wet bias 17 22 which is mainly due to the wet bias in no precipitation pobs 1 mm d 1 events that account for 76 47 of the time on the contrary the positive and negative bias are more evenly distributed in cu off the difference between cu on and cu off indicates that gw scheme produces slightly more precipitation for about 8 of time in our study region therefore cu off has better skill with less precipitation bias compared to cu on the relative suppression in cu off is consistent with its frequency analysis table 2 which has less false alarms more misses and no precipitation days there is no improvement in the bias for finer resolution models unlike the skill scores the bias increases from roi24 to roi4 based on the decreasing no bias probability this unexpected result may be caused by using gw scheme whose assumptions may not be appropriate for finer resolutions nevertheless the total difference of bias probability bias not equal to zero between roi4 24 less than 4 in fig 3c is much smaller than the difference between cu on and cu off 13 which indicates there is less impact due to model resolution as compared to cumulus parameterization the gw scheme explains the small wet bias in no precipitation days but the cause of the mutual small dry bias in precipitation days is unknown moreover all models show similar probabilities of extreme bias 10 mm or 10 mm which are even larger than the probabilities of small bias 2 0 mm and 0 2 mm in precipitation days fig 3a only about 4 of positive or negative extreme bias in precipitation days are explained by the use of gw scheme fig 4 shows a spatial distribution of days with extreme bias when precipitation occurs pobs 1 mm d 1 in cu on the extreme bias is widely distributed and does not spatially compensate between wet days and dry days all models not shown produced a similar distribution of the extremely dry bias with cores count 20 days in north kansas midwest missouri and northeastern oklahoma consistent with our previous analysis cu off shows a suppression of precipitation spatially compared to cu on with more days of extreme dry bias than days of extreme wet bias 3 2 object based analysis 3 2 1 spatial analysis the previous grid to grid analysis shows very limited difference of model skill among roi4 24 which may be caused by the same configuration except for their resolutions therefore the spatial and time lag analyses sect 3 2 2 are only conducted on the cu on and cu off model simulations to assess the spatial matching performance hourly simple objects identified from mode are analyzed to get the probability of match miss and false alarm in table 3 this table is however different from a normal contingency table which gives a total probability of 1 based on objects the counts of three measurements are defined as 1 match the minimum number of objects between matched clusters 2 miss the number of unmatched objects in observation 3 false alarm number of additional objects in the model compared to matched objects the probability of the three measurements in table 3 are then calculated differently match and miss probability are the portion of matched or missed objects in observation match or miss counts total observational objects which give the sum of two probabilities equal to 1 as a contrast the probability of false alarm is referenced to model objects false alarm counts total model objects the reason for choosing simple objects instead of matched clusters to count the probability is due to the matching process in mode the mode treats all unmatched objects as a whole cluster even though they may belong to different events the incapability of recognizing multiple unmatched events clusters by the algorithm makes standard use of the cluster method inappropriate to assess the probability of matching from table 3 both cu on and cu off have a high miss rate 60 and a large false alarm rate 70 especially for intense precipitation events indicating spatial or and temporal mismatch of precipitation events to better understand the cause of the low matching rate the total number of objects and a ratio of event hours hours with objects in the model hours with objects in the observations table 4 are used to show the event frequency in the model except in cu on when ps 1 mm h 1 both models in all precipitation intensities produced overall equivalent or less objects than observation which seems to contradict the high false alarm rate however nearly all ratios of event hours are less than 1 which indicates widespread precipitation more objects also confirmed by the area ratio much larger than 1 in fig 7 with less temporal frequency fewer hours with precipitation produced by the model a more detailed event comparison can be seen from fig 5 both models overestimate events objm objo and this overestimation decreases with increasing intensity from 30 50 to 10 20 this decreasing trend is opposite to the increasing false alarm rate with more intense events table 3 and again strengthens the idea that both models produce more precipitation events in a short time period especially for high intensity events in contrast both models tend to produce fewer events 55 in rainy hours objo 0 which explains the high miss rate in table 3 thus the high miss rate and false alarm rate are caused in different ways despite the large portion of no precipitation hit hours objm objo is more than 59 of time in fig 5b when comparing the two models there is a large difference in matching probability table 3 and time components fig 5 of light precipitation events for ps 1 mm h 1 observational events fig 5a cu on overestimated precipitation objects for most of the time 53 whereas cu off only overestimated for 30 of the time similarly for no observational events fig 5b cu on overestimated 41 of the time which is 17 more time than cu off this large difference is only found in light rain events 1 mm h 1 ps 5 mm h 1 whereas both models have nearly the same matching probability and time components for ps 5 mm h 1 and ps 10 mm h 1 although cu on overestimated light rain events 20 more often than cu off fig 5 it produced a 10 5 higher matching probability table 3 this seemingly contradiction could be explained by the different reference that matching probability is compared to observational objects while the false alarm rate is reference to model objects the largely overproduced objects almost the same number of observational objects in table 4 for light precipitation in cu on not only increased the denominator total model objects of false alarm rate but also increased the matching observational objects as well in addition to evaluate matching rate based on simple objects it is also informative to further assess the matching quality based on clusters before digging into the quality evaluation fig 6 provides a histogram of event scale area for all matched clusters for three different intensity thresholds clearly both the number of events and precipitation area decrease with intensity the logarithmic area distribution shows the general precipitation scale smaller than 10000 km2 50 percentile for ps 1 mm h 1 for cu on and 5700 km2 for cu off although cu off produced smaller scale precipitation compared to cu on fig 6a the scale of intense precipitation is larger fig 6c ps 10 mm h 1 similarly cu off has less matched precipitation clusters at low intensities but more for intense events this indicates a preference of light yet larger scale precipitation in cu on whereas cu off produced relatively more intense but smaller scale precipitation by checking the intensity ratios of matched clusters table s1 we found that the cu on tends to overestimate light rain intensity while underestimating heavy rain intensity the cu off generally produces rain intensity closer to observation but it overestimates heavy rain intensity in order to assess the matching quality of the spatial pattern we use spatial error distance between mass centers and area ratio area of model cluster am area of observational cluster ao of matched clusters to imply spatial accuracy it should be noted that the area ratio here is different from the definition in met the area ratio in met is defined as the lesser object area over the larger object area which is less informative for comparative analysis as the reference changes based on which object is larger to present the main performance of the two models 10 of the data are removed from one side centroid distance or both sides area ratio to exclude extreme values fig 7 both cu on and cu off have a large range of spatial error with the mean around 58 70 km the cu off seems to show a better forecast in location with a shorter mean distance but the mean difference between two models is small 4 6 km smaller in cu off results in area ratio fig 7b shows that both models overestimate precipitation scale all mean area ratio are at least 2 times greater unlike the spatial error not all mean area ratios are closer to 1 in cu off especially for ps 10 mm h 1 although cu off does not predict better precipitation scale than cu on in all intensities it shows a smaller variation in both spatial measurements indicating a more stable performance of model skill in the case of different intensities the inconsistent change with intensity in cu off contrasts that from cu on which is found to reduce the matching distance and area ratio for higher intensity events this difference may relate to the use of the cumulus scheme however with only one specific cumulus scheme gw used in this study the relationship between spatial prediction and precipitation intensity while using the cumulus scheme is not certain further research using other cumulus schemes to test the trend is left for future work 3 2 2 time lag analysis the results above are sensitive to time lag which could mislead the interpretation of model skill especially with at least 10 of the time when both cu on and cu off have missed or overproduced the precipitation events ratio of event hours in table 4 to assess the temporal difference we analyzed the time lag influence by repeating the spatial matching process 13 times with up to 6 h lags of both prior negative and subsequent positive forecasts a comparison of spatial error and area ratio of matched clusters is present in fig 8 for low intensity events ps 1 mm h 1 both models show clear decreasing trends in matching distance as the time lag moves towards 0 fig 8a similarly the area ratio is generally lower around lag 0 and increases with larger time lags when ps 1 mm h 1 although the smallest value of cu on occurs at time lag of 1 h the little difference in matching distance between lag 1 h and lag 0 h could be caused by its larger number of simple objects in matched clusters with largely overproduced precipitation events of ps 1 mm h 1 table 4 the number of simple objects formed a cluster in cu on is more than its counterparts in cu off the more simple objects in the cluster the more uncertain the spatial error of the cluster is shifted meanwhile more simple objects in a cluster also leads to a larger cluster area which results in a larger area ratio this explains the better performance of cu off which shows a 5 4 km shorter mean matching distance and 3 1 smaller area ratio than cu on fig 8a nevertheless all clear trends and comparison along the time lag axis disappear in moderate to intense precipitation events fig 8b c indicating poor skill in both models to predict spatial and temporal patterns for these events these clear trends are also found in 5th and 95th percentile data fig s1 although the matching distance and area ratio have a large variation can be seen from fig 7 the overall trend shows the peak and trough shortest distance and area ratio closest to 1 around lag 0 likewise the time dependency of the 5th and 95th percentile decreases as precipitation intensity increases 4 discussion the improvement in model skill caused by finer resolution is very limited in our study although improvement is not found from 24 km to 12 km roi4 shows slightly better prediction in frequency table 2 in all three skill scores the worse bias probability fig 3 in roi4 could be explained by gw scheme which was designed to operate at coarse resolutions similarly an hourly precipitation analysis by lee et al 2017 shows both larger diurnal bias and better overlap of joint probability distribution function jpdf between duration and peak intensity in 4 km nu wrf model compared to the coarser models in central plains the contrasting results of both our daily analysis and lee s hourly analysis point out the limited value added by finer resolution of nu wrf models in this region one of the possible reasons is that the spatial heterogeneity in central plain is less than other regions such as rockies where there is a larger elevation expanse as mentioned in mass et al 2002 there is a point of diminishing returns in forecast models and it varies from finer resolution 10 15 km in western u s to a coarser resolution 20 40 km in the eastern part where mesoscale features are already well resolved in coarser resolution because of the flatter terrain thus the small added value from surface heterogeneity in our research area competes with error caused by the gw scheme being applied at finer resolutions in addition as spectral nudging is used in this study deficiencies in merra 2 could be introduced to impact the physical solution of the model and hence reduced the improvement by the finer resolution models this study also shows some interesting findings there is a slightly dry bias and about 35 extreme bias p 10 mm d 1 and p 10 mm d 1 in precipitation days in all five models fig 3 these mutual biases in all five models indicate dominate factors other than cumulus parameterization and model resolution one possible cause could be from the merra 2 boundary condition to analyze this we use model precipitation not gauge corrected from merra 2 as a reference instead of gauge corrected precipitation to compare the spatial similarity between the nu wrf and merra 2 if there is a similar pattern between merra 2 and our result we are not able exclude the possibility that the boundary condition might be a reason for the bias fig 9 shows the hovmöller diagram of precipitation mean bias for merra 2 and the cu on and cu off model runs in june 2004 the full three summer performance is not shown here but it provides similar results there is not a strong trend for dry or wet bias in merra 2 the same is true for the eastward propagation besides the larger and wider wet bias in cu on the spatial patterns of bias in cu on and cu off are quite similar as in merra 2 from a coarser temporal scale especially for the extreme bias this similarity supports the idea that merra 2 boundary conditions and nudging may be the cause for the overall bias in all model runs however to fully understand the cause of this bias requires more analysis including considering other boundary forcings which is beyond the scope of this study besides the boundary condition other model parameterizations such as radiation pbl and the land surface all play important roles in predicting precipitation and may also contribute to the bias li et al 2014 pei et al 2014 hirsch et al 2019 it should be noted that the impact from these parameterizations may vary with domain model and the research period in addition the lack of convection propagation liu et al 2017 and the time inconsistency between model and observation can also result in bias although the contribution from these factors were not checked in this study they provide potential causes that can be explored for future studies to better understand shortcomings in precipitation prediction our study not only analyzed the spatial pattern but also revealed an imperfect pattern of forming precipitation in cu on and cu off models the low matching rate and the poor matching quality in both models show the large mismatch of precipitation events in terms of spatial pattern although there is no consistent time lag in cu on and cu off the large spatial discrepancy could be partly caused by the model patterns the largely overestimated objects table 4 and 55 overestimation time fig 5 in cu on for light rain events p 1 mm h 1 contrasts with its intense precipitation cases and indicate an easy triggering for shallow convection in cu on with a larger rain area fig 7 this is consistent with research of yu and lee 2010 who found the excessive cumulus rain region reduced after modified trigger function in kain fritsch kf scheme however for more intense precipitation and cu off simulations fewer hours with events are produced indicating a possible obstacle to initializing precipitation although the generated precipitation area is still larger than observation fig 7 this may relate to the deep convection in the model which contributes more intense precipitation interestingly this pattern is clearer in cu on with even less precipitation events for p 10 mm h 1 than cu off because the cumulus precipitation can be an adding component representing sub grid processes the less intense events in cu on indicates insufficient ability to develop deep convection using g3d scheme only 15 of time both models correctly predict event counts fig 5a this time portion is stable despite the model and precipitation intensities which points the problem towards the other aspects of the nu wrf model rather than the use of cumulus parameterization there are some uncertainties in this study such as the uncertainty of resampling the choice of observational dataset and the probability of object detection near the edges however our preliminary tests fig 2 show small resampling impact the change of map resolution does not change the relative position among models in nearly all three skill scores the variation in three resolution standards is within 14 with an average of 6 7 and 10 for hss gss and mtss respectively all 4 km models roi4 cu on and cu off show higher skill scores than roi12 24 in all three resolutions roi12 has lower skill scores than roi24 in most map resolutions but the difference between them is less than 0 01 therefore the decision of using 4 km map as the standard projection in this study should not make a big impact on our analysis as for the uncertainty from observational dataset we processed a similar analysis in preliminary test not shown using livneh daily precipitation with a smaller roi in five julys from 2000 to 2004 again the impact of model resolution is separate from the use of cumulus parameterization results from the test are similar to this study the overall wet bias in all times the slightly dry bias and more than 28 probability of extreme bias in precipitation days are all reproduced the improvement in finer resolution model is small mtss of roi4 roi12 and roi24 are 0 166 0 151 and 0 151 respectively at 0 0625 degree map the test used a simplified version of mode analysis which only considered centroid distance the results also showed a general shorter matching distance in cu off compared to cu on while both models overpredicted the precipitation spatial extent therefore we think the choice of observational data does not change our conclusion the uncertainty of object detection relies on the domain size the location of the object and the searching distance for possible matching objects to evaluate this uncertainty we calculated a probability of detection supplementary material and found our study has at least a 76 67 of probability to detect 90 of the matched objects based on this we think the results presented in this paper are overall reliable measures of the model skill it is known that precipitation forecast is sensitive to the different cumulus schemes applied because of their different closure assumptions and scale considerations wang and seaman 1997 the uw scheme a convective inhibition based scheme is found to produce deeper daily pbl growth triggers shallower development of clouds compared to zhang mcfarlane scheme a convective available potential energy based scheme williams 2019 and reduce the precipitation compared to native shallow scheme in g3d iguchi et al 2017 research of iguchi et al 2017 also shows more negative daily precipitation bias in gw model compared to models using the new simplified arakawa schubert scheme nsas han and pan 2011 new kain fritsch scheme nkf kain 2004 and betts miller janjic scheme bmj in summer central plain this indicates that excessive precipitation is a common problem with these cumulus schemes applied in the central plains and thus may aggravate the wet bias in this region however there are also other scale aware cumulus schemes under development that provide promising results kwon and hong 2017 introduced three aspects of scale aware dependency into simplified arakawa schubert scheme gsas and found it outperforms precipitation prediction in no cumulus simulation at both 3 km and 1 km scale in south korea the main contribution of the improvement in their study comes from including scale dependency into cloud base mass flux therefore it is still probable to see improvements using scale aware cumulus schemes in the future 5 conclusion this study evaluates precipitation performance in five nu wrf simulations although there is little improvement found with increasing model resolution it may be due to the domain region small elevation variation and model configuration the use of gw scheme and nudging which could restrict or reduce the difference it is important to stress that the resolution impact analysis is from roi4 24 models which all use gw scheme this is different from some previous studies that turned off cumulus scheme at 4 km and resulted in more obvious improvement analogous to comparison between cu off and roi12 24 here we separated the two impacts and found that the cumulus parameterization is more influential on model prediction than model resolution although cu off shows better precipitation pattern than cu on it still has large inaccuracy especially in a grid to grid comparison the advantage of the no cumulus scheme model should become more obvious and more accurately represent the land surface spatial heterogeneity with finer resolutions such as 1 km although our study shows the model without cumulus parameterization outperforms the model with gw scheme at 4 km resolution it may still be possible to see comparative model skill with the use of a scale aware scheme in the future software availability model nu wrf developer nasa gsfc and affiliate contact toshi matsui toshihisa matsui 1 nasa gov and carlos cruz carlos a cruz nasa gov availability nu wrf model is not publicly available but the source code and data can be requested via contact more information is at https nuwrf gsfc nasa gov software first released year 2011 software model evaluation tools met developer developmental testbed center dtc availability download from https www dtcenter org community code model evaluation tools met cost free first released year 2008 data availability stage iv precipitation can be accessed from ncar eol website https data eol ucar edu dataset 21 093 nu wrf and met mode configuration files used in this study can be found at https hydrology ku edu data html merra 2 data is available at https gmao gsfc nasa gov reanalysis merra 2 livneh data is available at https psl noaa gov data gridded data livneh html funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the hydrological sciences laboratory of nasa goddard space flight center for providing their conus nu wrf data and ncar eol https data eol ucar edu for providing stage iv data under the sponsorship of the national science foundation appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105184 
25724,this study evaluates a numerical weather prediction model as a tool for wind resource assessment in complex terrain and how the simulations are affected by the selection of initial and boundary conditions and various grid resolutions two global reanalyses era interim and era5 and four grid resolutions 27 km 9 km 3 km and 1 km have been considered the simulations have been compared to hub height wind measurements the era5 forced simulations were found to provide improved wind speed results the simulations show clear improvements in terms of lower error when the grid spacing is reduced from 27 km via 9 km to 3 km from 3 km to 1 km the error is not further reduced however the 1 km simulations tend to better reproduce the mean wind features and show defined areas with higher and lower wind speeds providing useful information for wind resource mapping in complex terrain keywords wind resource assessment wrf model era5 era interim complex terrain grid resolution 1 introduction a mesoscale numerical weather prediction nwp model is a mathematical representation of the physical and dynamical processes in the atmosphere on scales ranging from a few kilometres and up to a couple of thousands kilometres jacobson 2005 the models were originally developed for weather forecasting applications but have also emerged as a useful tool for the wind power sector as it can provide the users with high resolution wind climatology over large areas jacobson 2005 carvalho et al 2012b mughal et al 2017 carvalho et al 2014 wind data retrieved from the mesoscale nwp model simulations can assist wind power agents in identifying areas with favourable wind conditions and accompany local wind measurement campaigns to determine long term climatology carvalho et al 2012b mughal et al 2017 the nwp models represent an approximation of the real state of the atmosphere the size of the deviation between the real and the simulated atmosphere depends e g on the model configurations such as the choice of parametrization schemes for unresolved physical processes terrain and vegetation representation definition of the model domain size location and spatial resolution numerical options and initial and boundary conditions awan et al 2011 through a careful model configuration the user is able to optimise the model for one specific area or weather event for wind energy purposes it is especially important to limit and identify how the model deviates from the real atmosphere as a small error in the wind speed will induce a large error in the wind energy estimate in particular in our study special attention will be given to the dependency on the input atmospheric data and the model resolution both are expected to have a crucial impact on the ability of the model to reproduce the wind in the selected study area characterised by a coastal and mountainous terrain located within the arctic region for mesoscale nwp models the input atmospheric data give initialisation fields and boundary conditions throughout the simulation time and are typically provided by global reanalysis data carvalho et al 2014 the global reanalyses are obtained through global nwp models that optimally combine meteorological observations with model forecasts through an assimilation process into a physical coherent description of the evolution of the atmosphere dee et al 2011 hersbach et al 2020 the arctic region is particularly challenging for global reanalyses due to few observations from weather stations available for assimilation and evaluation wesslén et al 2014 inoue et al 2015 in addition the parametrization in the forecasting model is not tuned for phenomena unique to the arctic wesslén et al 2014 a careful evaluation of the global reanalysis used to force the simulations are therefore considered to be especially important for the area of interest in this paper two global reanalysis products will be considered the era interim era i and the era5 both produced by the european centre for medium range weather forecasts ecmwf several energy related studies have evaluated and compared era i and other global reanalyses as input data for mesoscale simulations and found that era i forced simulations tend to provide a better representation of the wind field mughal et al 2017 carvalho et al 2014 fernández gonzález et al 2018 menéndez et al 2011 carvalho et al 2012a the most recent global reanalysis by the ecmwf era5 benefits from 10 years of development in the forecasting model and the data assimilation method hersbach et al 2020 in addition era5 also typically assimilate a higher number of observations than era i the great strength of era5 in comparison with era i is expected to be due to the increased spatial and temporal resolution hersbach et al 2020 a horizontal resolution of 31 km 137 vertical layers and an hourly data output realize a more detailed description of the atmosphere and its evolution hersbach et al 2020 in comparison to the 80 km horizontal resolution 60 vertical layers and the 6 h output resolution of era i dee et al 2011 several studies report that era5 outperforms era i and several other global reanalyses in the representation of surface wind both over land olauson 2018 ramon et al 2019 and over oceans belmonte rivas and stoffelen 2019 particularly encouraging in relation to our study are the results provided by graham et al 2019 where wind data from five atmospheric reanalyses including era i and era5 were evaluated with respect to wind speed measurements in the arctic it was found that era5 provided the best results for wind speed with the highest correlation and smallest bias and root mean square error rmse the second essential factor for mesoscale wind simulations that will be considered in this study is the model resolution the study area is characterised by steep mountains fjords and numerous islands within the field of numerical weather prediction this terrain is typically described as complex and numerical simulations over such areas remain challenging carvalho et al 2012b mughal et al 2017 carvalho et al 2014 krieger et al 2009 the air flow in coastal zones is typically strongly affected by the abrupt change in temperature and frictional drag between land and sea areas national research council 1992 depending on several factors such as the stability of the atmosphere and the wind speed the coastal orography may give rise to complex flow patterns such as blocking strong gap winds and mountain waves with downslope wind storms national research council 1992 the model representation of the terrain is considered to be an important key factor for achieving a good representation of the wind flow in complex terrain carvalho et al 2012b mughal et al 2017 previous studies indicate that higher resolution tends to give better wind simulations results in complex terrain carvalho et al 2012b mughal et al 2017 fernández gonzález et al 2018 carvalho et al 2012a samuelsen 2007 valkonen et al 2020 the drawback of higher resolution is the computational cost and also the possibility of phase errors mass et al 2002 this study will assess the possible improvement in the wind speed simulations when wrf is forced by era5 compared to when it is forced by era i a second focus will be on how different model resolutions affect the ability to reproduce the wind field over the area of interest the wind simulations will be compared to hub height measurements at three different locations the rest of the paper is structured as follows section 2 describes the method of the study including a description of the measurements the configuration of the wrf model as well as the statistical metric used for evaluation the results and the discussion are presented in section 3 while section 4 provides a concluding summary 2 method 2 1 study area and field measurements the area considered in this study is located along the coast in the northern part of norway as illustrated by the red square in fig 1 a large seasonal variations in the temperature over northern scandinavia give rise to a monsoon like wind pattern on the coast of norway svendsen 1995 during the winter season cold air over land and warm air over the ocean due to the north atlantic current sets up a pressure gradient in east west direction with high pressure over land and low pressure over the ocean during the summer season the temperature and pressure gradients are often reversed this results in a main wind direction from the southeast se during the winter and from northeast ne during the summer svendsen 1995 in addition to the pressure gradient due to the difference between temperature over land and over ocean the polar front is a source of eastward moving low pressure systems especially during the winter season when the horizontal temperature gradient across the polar front is largest leading to baroclinic instability and frequently occurring strong low pressure systems there will be large variations in wind speed and directions in this area the wind patterns will also be affected by the local topography and lead to topographically induced variations in the main wind directions to illustrate that the seasonal variations in pressure and wind patterns also occur during the selected study period from 1 september 2014 to 30 august 2015 the mean sea level pressure mslp for the winter months september 2014 march 2015 and the summer months april 2015 august 2015 are presented in figure a1 in the appendix the relief map in fig 1b shows an overview of the terrain in the study area with mountainous islands and peninsulas separated by long fjords and sounds the mountains in the area range from sea level and up to about 1800 m above sea level masl the wind measurements used to analyse the model performance were collected from three different coastal locations referred to as location a b and c with mean measured wind speeds during the study period of 7 38 ms 1 7 10 ms 1 and 7 72 ms 1 respectively the coordinates the terrain ruggedness index rix bowen et al 1996 and the elevation above sea level of the locations are summarized in table 1 the rix describes the fraction of the area within a 2 km radius with slopes steeper than 30 berge et al 2006 the rix presented in this study have been collected from a wind resource report provided by the norwegian water resources and energy directorate nve byrkjedal and åkervik 2009 fig 1d shows a close up map with two dots where the left dot indicates location a and the right dot location b the two masts in location a and b are situated only 3 5 km apart on two rather flat mountain peaks at 550 masl and 505 masl respectively the two mountains are separated by a narrow valley with elevations below 200 masl the masts at a and b have similar configurations with measurements 80 m above ground level magl the data sets from location a b spanning a full year of measurements consists of 95 4 98 9 data relative to a full time series in the same period with the same sampling frequency at both a and b all measurements are recorded during the wind resource assessment phase before any construction of the wind farm and are therefore free of any wake disturbances from wind turbines mast c depicted by a black dot in fig 1c is situated at 68 masl on a rather small island the large mountains 5 km west of the mast reach up to 1000 masl and may partially work as an orographic blockage of westerly winds the lyngen alps a 90 km long mountain range separating two long fjords just south of location c is also expected to have a large impact on the wind at this location the wind measurements are done at 80 magl and has a data availability higher than 99 9 the mast is situated nearby an operational wind farm consisting of 18 wind turbines with hub height of 80 m and rotor diameter of 90 m the main wind direction is from the se and all the turbines are located north and west of the mast the measured data used in this study have a temporal resolution of 10 min the study period from 1 september 2014 to 30 august 2015 was selected because this period had the best coverage of measurements for all three locations 2 2 numerical weather model in this study the advanced weather research and forecasting wrf model version 3 9 1 is used this is a three dimensional nonhydrostatic mesoscale model developed by the national centre for atmospheric research ncar the model is widely used for research on atmospheric processes as well as for numerical weather prediction nwp a detailed description of the model may be found in skamarock et al 2008 for the purpose of this study the model is configured to include four one way nested domains d01 d02 d03 and d04 with a horizontal resolution of 27 km 9 km 3 km and 1 km respectively fig 2 polar stereographic projection has been applied as recommended for high latitude areas and the outer domain is centred at 67 56 n and 17 39 e the vertical structure of all domains consist of 50 terrain following sigma levels with an upper boundary at 50 hpa in order to describe the terrestrial fields information about the topography land water mask and land use have been interpolated to the model grid from the 20 category moderate resolution imaging spectroradiometer modis land use data and the global multi resolution terrain elevation data 2010 gmted 2010 both data sets were retrieved from the ncar database and have a 30 arc second resolution to represent the physical processes that are not explicitly resolved by the model the ncar convection permitting suite conus has been applied for all domains the conus suite consist of the thompson microphysics scheme thompson et al 2008 the mellor yamada janjic myj planetary boundary layer pbl scheme janjić 1994 the noah land surface model chen et al 1997 the rapid radiative transfer model for global applications rrtmg shortwave and longwave radiations schemes iacono et al 2008 the tiedtke cumulus scheme tiedtke 1989 zhang et al 2011 and the myj surface layer scheme janjić 1994 this combination of parametrization schemes has been developed and tested over several years until released as a physics suite in 2016 powers et al 2017 romine et al 2013 the time period considered in this study covers a full year for simulations over long time periods it is recommended to run several shorter simulations to avoid accumulation of truncation errors carvalho et al 2012b to achieve a full year of model data the wrf simulations were initialised 52 times and run for 8 days at the time hereinafter referred to as a week the first 12 h of the simulations were considered spin up time and therefore discharged the following 12 h were combined with the last 12 h from the previous week by linear interpolation to assure a smooth curve the model has been run with a time step of 81 s for d01 and reduced by a factor of 3 for each of the following nests outputs were saved every 10th minute the model was run twice first with era i and second with era5 as initial and boundary conditions the boundary conditions are updated throughout the simulation time for the simulations initialised with era i and for 35 of the 52 weeks initialised by era5 the model ran without problems however for the remaining 17 weeks initialised by era5 the model became unstable this problem was solved by either introducing damping such as time off centring vertical velocity damping and divergence damping or by running the model with adaptive time step the wind data considered in this study are retrieved in the vertical by interpolation of the model level wind to 80 magl and in the horizontal by bilinear interpolation of the grid points to the exact geographical location of the observation points 2 3 evaluation metrics in order to evaluate the performance of the different model configurations the following statistical metrics are used bias standard deviation sd mean absolute error mae root mean square error rmse centred root mean square error crmse and correlation coefficient r the bias the mae and the rmse are calculated as given in wilks 2011 while the crmse the r and the sd as given in taylor 2001 and are all repeated in this section let x i be a time series consisting of i 1 2 n data points the mean x and the sd of these time series is estimated as 1 x 1 n i 1 n x i 2 s d i 1 n x i x 2 n the sd is a measure of the dispersion of the values in a data set simulations with values close to the ones in the measurements are desired the bias describes the tendency of the model to systematically over or underestimate a parameter thus a negative value indicates that the model tends to underestimate the parameter i e the wind speed in this study and a positive value the opposite the bias between two time series x i and y i is defined as 3 d 1 n i 1 n x i y i where y i normally is taken as the true value here x and y represent the simulation and measurement data respectively the correlation coefficient indicates if there is a linear relationship between two time series and are given by the following relationship 4 r 1 n i 1 n x i x y i y s d x s d y if r equals 1 the highest possible value the pattern of the two time series is exactly the same however the amplitude might still be different i e the bias might be non zero if r equals 0 there is no linear correlation between the two data sets the mae the rmse and the crmse all describes the magnitude of the error between a measured and a simulated parameter if the pattern the phase and the amplitude of two time series are exactly identical the mae the rmse and the crmse equals zero the mae describes the total error between the measurements and the simulation without considering the sign of the error the rmse is similar to the mae however more sensitive to larger errors the crmse is different from the mae and the rmse in the way that the crmse is calculated without the tendency of the model to over or underestimate the mae the rmse and the crmse are calculated from the following equations 5 m a e 1 n x i y i i 1 n 6 r m s d 1 n i 1 n x i y i 2 1 2 7 c r m s d 1 n i 1 n x i x y i y 2 1 2 3 results and discussion 3 1 evaluation of the initial and boundary conditions the crmse the sd and the r between the measured data and the wrf simulations with different initialisation data have been summarized in taylor diagrams taylor 2001 in the left column in fig 3 for location a b and c from all the domains the statistical evaluation metrics are represented with coloured dots of different sizes wrf initialised with era i is depicted by red dots and wrf forced by era5 with green dots the smallest dot represents domain d01 while the increasing sizes represent d02 and d03 and eventually d04 by the largest dot the black dot represents the sd of the measurements in the right column of fig 3 the bias the rmse and the mae are presented for each location the solid lines represent the simulations forced by era i and the dotted lines the simulations forced by era5 for more details all the statistical evaluation metrics are also presented in the appendix table a1 at the end of this paper from the taylor diagrams in fig 3 it is evident that wrf initialised with era5 reproduces the temporal pattern of the measured wind speed better than wrf forced with the era i in terms of lower crmse and higher r for all locations and all domains also the rmse and the mae presented as graphs in the right column in fig 3 indicate that the era5 forced simulations reproduces the wind speed better than the era i driven simulations the wind speed bias of the model in this study are presented in the right column in fig 3 for location a and b the models tend to overestimate the wind speed but the bias is found to be lower for the era5 forced simulations red dotted line compared to the ones forced by era i red solid line in location c there are only small differences between the models forced by era5 relative to those forced by era i the sd of the simulations in comparison with the sd of the measurements left column fig 3 indicate how well the models are able to reproduce the wind speed variations in this study at these three locations there are little improvements in the sd when forcing the simulations with era5 rather than era i the sd is very similar for the two model configurations but appears to be sensitive to changes in the model resolution the wind roses for location b and c presented in fig 4 provide a visual comparison of the cumulative distribution of the wind speed in combination with the wind direction only the simulations retrieved from the innermost domain with the highest resolution are considered in this figure location a is not presented here as the wind roses show very similar features as the nearby location b the measured wind data for location b show a main wind direction from se with a frequency above 10 and a second main wind direction with lower frequency from the southwest sw the wind roses for both simulations forced by either era i or era5 show very similar features and are both able to reproduce the distribution of the measured wind well both wind roses show a high occurrence of wind from se and high occurrence of wind from sw both models show a low occurrence of wind with a northerly component in agreement with the measurements the wind rose for the measured data in location c shows a high occurrence of wind from two wind directions the first from se and the second from the south the measured wind from se has a high frequency of wind speed above 15 ms 1 as well as wind speeds exceeding 20 ms 1 the simulations are able to capture the main wind direction from se although slightly clockwise rotated compared to the measurements the era5 forced simulations are in better agreement with the measurements than the simulations forced by era i as it shows a higher frequency of wind from this direction including higher wind speeds above 15 ms 1 the higher frequency of wind speed above 15 ms 1 from se in the era5 forced simulations can be explained by a more detailed representation of the temperature over land in the era5 dataset in comparison to era i not shown that may give rise to more pressure driven channelling of the air out the lyngen fjord 3 2 evaluation of increasing model resolution the ability of the model to reproduce the temporal variations of the measurements as the grid resolution increases over the four domains is also presented in the taylor diagrams and as graphs in fig 3 for all locations the correlation coefficient r tends to increases slightly when the distance between the grid spacing is reduced over the four domains however the correlation between the simulations and the measurements appears to be more sensitive to the choice of initialisation data than it is to the resolution particularly for location c the crmse the rmse and the mae improve at all locations when the grid spacing is reduced from d01 via d02 to d03 in particular at locations a and b the reduction of the unsystematic error the crmse is small and similar to the correlation appears to be more sensitive to the choice of initial and boundary conditions than the resolution the wind speed simulations for location c appear to have a higher sensitivity to the resolution than the other two locations as the reduction in crmse over the first three domains is more prominent in contrast and for all locations when the resolution is increased further from 3 km in d03 to 1 km in d04 the mae the rmse and the crmse increases considerably in particular the crmse is higher in d04 than in d01 for location a and b other studies also report similar results related to the model resolution with clear improvements in the statistical evaluation metrics as the resolution increases up to a certain threshold mass et al 2002 siuta et al 2017 when the resolution is increased beyond this threshold the improvement is less evident and depends on the metrics used for evaluation as well as the location evaluated a similar behaviour can be seen for the bias for location a and b in fig 3 the positive bias decreases as the grid spacing is decreased from 27 km via 9 km 3 km when the grid spacing is further decreased to 1 km the bias increases to its highest value for the two locations at location c the d01 simulations show the largest overestimation while the lowest bias is found in d04 the sd of the simulations also presented in fig 3 for all locations are closest to the sd of the measurements in domain d01 and d04 for locations a and b the wind simulations in d04 are able to reproduce the spread in wind speeds better than the other simulations with an sd slightly larger than the sd of the measurements in location c the sd closest to the sd of the measurements is found in domain d01 in order to further assess how the change in grid spacing affects the quality of the model simulations in this study the mean features are evaluated using frequency histograms and wind roses in consideration of content limits only the results from the era5 data will be considered in this section as similar results were found for the era i data histograms for the measured and simulated wind speeds are presented in fig 5 for location b left and c right the x axis shows the different wind speed bins while the y axis shows the frequency in number of hours by comparing the histograms for location b it is evident that all domains underestimate the frequency of the wind speeds below 5 ms 1 and hence are all the histograms for the simulations shifted to the right and towards higher wind speeds this is also reflected by the positive bias for all domains at this location the first three domains d01 d02 and especially d03 overestimate the intermediate wind speeds between 5 ms 1 and 10 ms 1 d01 is able to reproduce the occurrence of the higher wind speeds above 15 ms 1 well while d02 and d03 show an underestimation of the same wind speeds this coincides well with the sd for the different domains where the low sd found for d02 and d03 indicates a low spread of the wind speeds and the higher sd for d01 indicates that the simulations are able to capture the spread of the wind speed better the shape of the histogram for d04 shows a great resemblance with the histogram of the measurements although with a shift to the right while the wind speed histograms for all the simulations at location b are skewed to the right compared to the measurements only the winds from the d01 simulations are skewed to the right for location c the histograms for the other three domains that also have a considerable lower bias than d01 show a more similar shape to the histogram of the measurements similar to the situation at location b d02 and d03 overestimate the intermediate wind speeds at location c and underestimate the higher wind speeds the histogram of the wind speeds retrieved from d04 shows a better agreement between the simulated wind speed and the measurements than the wind speeds from the simulations with a coarser resolution to also evaluate how the simulated wind speed in combination with wind direction are affected by the increased resolution wind roses for location b and c are presented in figs 6 and 7 respectively the figures include wind roses for the measurements and for the wind data retrieved from all four domains in addition the figures also include wind roses for the original 100 magl era5 wind data the wind components are retrieved from the exact locations by bilinear interpolation first the wind roses for location b in fig 6 will be considered the wind rose presenting the original era5 data shows a high occurrence of wind from se this is in agreement with the measured main wind direction from se although with a lower frequency the era5 model is not able to reproduce any of the higher wind speeds above 15 ms 1 present in the measurements this illustrates the limitations of global reanalyses like era i and era5 to reproduce local wind especially in complex terrain and why direct use of the reanalysis data should be avoided in the assessment of local wind speeds olauson 2018 ramon et al 2019 the d01 domain appears to be an improvement compared to era5 for the frequency of wind speeds above 10 ms 1 but not for the main wind direction as the wind direction is spread almost evenly over a large sector stretching from west to se the large difference between the era5 model and the selected configuration of the wrf model d01 with almost similar horizontal resolution can probably be related to differences in e g domain sizes terrain representation physiography and parametrization of physical processes a more detailed investigation of these differences although relevant for wind simulations is not further elaborated in this paper since the main focus is on the effect of the increased horizontal resolution in the wrf model the finer resolution simulations d02 d03 and d04 are able to reproduce the main wind direction from se better than d01 compared to the measurements the frequency of wind speeds between 15 ms 1 and 20 ms 1 from se is low in d02 and increases slightly in d03 d04 has a higher frequency of the same wind speeds in better agreement with the measurements and is also able to reproduce some of the wind speeds above 20 ms 1 the observed frequency of wind directions from the sw is more apparent in d03 and d04 compared to the coarse resolution models however the frequency of winds from sw are better reproduced in d04 compared to d03 the wind roses for the measurements in both location a not shown and location b show a low occurrence of wind from a sector stretching from about 225 to 255 w sw in particular there is a very low occurrence of wind speeds above 10 ms 1 one explanation for the low occurrence of wind from w sw can be orographic blocking by the steep mountains on the island senja the low occurrence of wind from w sw are not captured well in any of the simulations although slightly better in d03 and d04 in comparison with the simulations with coarser resolutions the poor representation of wind in this sector can be related to the terrain representation by the model in reality the mountains on senja reach up to a 1000 masl whereas in the four models they are represented with lower elevations for instance in d04 altitudes only up to 700 masl represent the same mountains this indicates that a better terrain representation perhaps also even higher horizontal model resolution is needed to resolve the terrain effect upstream of location a and b in fig 7 the wind data for location c is presented the era5 wind data and the two wrf simulations d01 and d02 have winds mainly coming from one large sector stretching from the west to the south d01 has compared to the era5 data and the other wrf simulations a higher frequency of all wind speeds above 10 ms 1 the two higher resolution domains d03 and d04 are able to reproduce the main wind direction from se although slightly clockwise rotated compared to the measurements d04 has a higher frequency of wind speeds between 15 ms 1 and 20 ms 1 from se than d03 and are in better agreement with the measurements none of the simulations are able to capture the highest wind speeds above 20 ms 1 present in the measurements from se in the measurements there is a high occurrence of wind directly from the south this is not represented in either d03 or d04 instead in both the finer resolution simulations there is a higher frequency of wind from sw in comparison to the measurements the behaviour of an air flow approaching a mountain barrier depends on the atmospheric stability the wind speed and the characteristics of the mountain such as the height whiteman 2000 the differences in wind patterns can therefore be related to the representation of the terrain in the models for instance the lack of wind from the south in the simulations can be related to unresolved topography in the lyngen alps see fig 1 upstream of location c that causes too much orographic blocking of the wind from the south another explanation can be related to the model terrain representations of the mountains west of location c the mountains reach up to 1000 masl and under varying atmospheric conditions winds from the west can for instance be forced around the mountains or lifted over the mountains in the d04 terrain the same mountains west of location c has a maximum elevation of 750 masl this considerably lower model terrain can give rise to a slightly different wind pattern at location c where e g less blocking of the air leads to stream across instead of around the mountains two shorter time periods have been selected to illustrate a possible explanation to the lack of winds from the south in the d04 simulations in location c in the first period selected from 19 20 december 2014 the measured wind direction is from the south and the wind speed varies between 0 ms 1 and 8 ms 1 the d04 simulation in the same time period has wind from se and the wind speed is overestimated and varies between 7 ms 1 and 15 ms 1 in the second selected time period on 13 may 2015 00 00 to 12 00 local time the measured wind direction is also from the south however the d04 simulations show wind from sw the measured wind speed varies 0 ms 1 and 7 ms 1 while the simulated wind speeds varies between 3 ms 1 and 7 ms 1 in fig 8 the d04 wind simulations from 80 magl at and around location c is presented for 20 december 2014 04 00 local time left and 13 may 03 00 local time right on the 20 december the wind field has a strong easterly component in the fjord east of locations c and a green field of wind speeds of approximately 12 ms 1 in the same location closer to location c and the nearby mountains there is a field of lower wind speeds and the wind vectors turns towards se a more accurate terrain representation may lead to more blocking of the air flow approaching from the west hence lower wind speed in location c and force more of the air flow around the mountain in a northward direction on 13 may as can be seen in fig 8 the air flow approaches the mountains nearby location c from the west the wind vectors are not affected by the mountain barrier and there is a field of higher wind speed on the mountain tops indicating that the air flow passes over the mountains a more accurate representation of the terrain with higher mountains may result in more of the air flow to be forced around instead of over the mountains west of location c and be deflected northwards on the lee side of the mountains in addition as can be seen in the wind roses in fig 7 the numerical simulations have a higher occurrence of wind directly from the west at location c compared to the measurements west of the mast in location c the mountains with elevations up to 1000 masl might cause an orographic blockage of wind from the west domain d04 is able to capture some of this orographic blockage with a low frequency of wind from this direction although with intermediate and high wind speeds present however at certain atmospheric conditions and wind speeds strong downslope winds might occur it must therefore also be taken into consideration that the measurements in particular wind speeds from the west might be disturbed by the nearby wind farm 3 2 1 case study based on the evaluation of the different horizontal model resolutions so far in this study the positive impact of an increased resolution from d03 to d04 is unclear while the wind speed histograms and the wind roses show an improved representation of the mean features in d04 compared to d03 for all locations a b and c the crmse the rmse the mae and the bias indicate the opposite a higher resolution and a more realistic representation of the terrain are e g expected to improve the simulations of terrain induced wind effects however the method of comparing point measurements with interpolated model information might be limited by timing and spatial errors of meteorological features like mountain waves and gap winds that become more prominent with increased horizontal resolution mass et al 2002 in particular when using traditional verification metrics to compare measurements and simulations in points spatial errors may be penalized both for the absence of the observed wind pattern but also for the presence of a wind pattern that is not observed at the location zingerle and nurmi 2008 this double penalty might not be seen when the resolution is lower as the wind features tend to be more smoothed zingerle and nurmi 2008 in order to further analyse the effect of increased horizontal resolution from 3 km to 1 km a case study has been carried out for location a and b fig 9 shows a close up map of the real terrain top and the model topography d04 in the middle and d03 at the bottom the maps include location a and b indicated by red dots from fig 9 it is clear that the topography is much better resolved in d04 compared to d03 when the terrain of d04 is compared to the real terrain it is clear that even at 1 km horizontal resolution considerable parts of the orography are not resolved in particular there is a height difference between the real topography and the topography of d04 of 82 m at location a and 75 m at location b table 1 fig 10 shows the wind speed and the wind direction of location a and b over a 24 h period starting at noon on the 9 october 2014 the measured wind is coming from se the main wind direction between 120 and 150 the measured wind speeds at both locations are high with long periods over 15 ms 1 at both locations and also over 20 ms 1 at location a the simulations are able to reproduce the wind direction in both d03 and d04 well while the wind speed especially from about 18 00 local time on 9 october 2014 is considerably underestimated moreover after about 20 00 only small differences can be found in the wind speed when the resolution is increased from d03 to d04 in particular during the 5 h time period from about 20 00 on the 9 october 2014 to 01 00 the following day there are observed stronger winds at location a than at location b neither the high wind speed or the difference in wind speeds between the two locations are represented by d03 or d04 in order to study this in detail fig 11 shows a close up map of the wind speed and direction 9 october at 22 40 for d03 left and d04 right d03 has small variations of the wind speeds with one large green coloured area representing wind speeds between 10 and 14 ms 1 covering both the fjord sw of location a and b and the locations of the measurements in d04 we can identify larger variations in the wind speeds location a and b are both included in a light green area with wind speeds in the interval 14 16 ms 1 and just downwind of location a we find a yellow area with wind speeds up to 20 ms 1 although this strong wind field does not include location a from the wind map in fig 11 it is evident that d04 are able to reproduce strong winds in close vicinity of location a while the strong winds are not seen in d03 the strong down slope wind seen downwind of location a can e g be a result of mountain wave activity mountain waves are reported to have the possibility to severely impact wind power production as they can cause large spatial and temporal fluctuations in wind speeds draxl et al 2021 xia et al 2021 the case study presented here exemplifies how high resolution wind simulations can provide more detailed information that are useful both for wind resource wind mapping and wind prediction in complex terrain 4 conclusion in this study wind simulations provided by the wrf model have been evaluated in an area characterized by a coastal and complex terrain located in northern norway in specific the aim of this study has been to evaluate the performance of the simulations when either forced by era i or era5 both provided by the ecmwf as well as to evaluate the impact different grid spacings has on the wind simulations the wind simulations have been compared to hub height wind measurements at three locations it was found that the era5 forced simulations provided lower mae rmse and crmse and higher correlation in comparison to the era i forced simulations the wrf simulations were run with four one way nested domains d01 d02 d03 and d04 with horizontal resolution of 27 km 9 km 3 km and 1 km respectively the results showed that when the resolution was increased from 27 km via 9 km to 3 km the mae the rmse the crmse and the bias decreased while the correlation increased when the grid spacing was further decreased from 3 km to 1 km the mae the rmse and the crmse at the three locations were impaired it is suggested that the lack of improvement from the 3 km to the 1 km simulations could be due to timing and spatial errors that occur when comparing measurements and simulations from a fixed geographical point further research should evaluate how different pbl schemes affect the results regarding increased model resolution nevertheless the wind speed histograms and the wind roses show that the d04 simulations with 1 km resolution provide an improved reproduction of the mean features of the wind and the variations in the wind speed in comparison with the simulations with lower resolution in addition a case study show that although the high wind speed events are not reproduced at the exact same locations as the measurements the highest resolution simulations show fields of higher wind speeds in nearby locations that coincides with the measured wind event whereas d03 is not able to reproduce these terrain effects this result exemplifies that although the highest resolution simulations scores low on the traditional statistical measures such as mae rmse crmse and bias the higher resolution simulations appears to reproduce local terrain effects better than the simulations run with a coarser resolution a limitation of this study is that only wind measurements at one height 80 magl is considered a further study should include measurements of the vertical wind profile and an evaluation of the ability of the wrf model to reproduce the wind shear in complex terrain in particular wind shear that might be harmful for wind turbines declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported by troms county and industry development fund under the project title renewable energy in the arctic academy and business in a joint effort rda12 46 the authors would like to thank nordlys vind and troms kraft for the meteorological data used in this work appendix table a 1 summary of the statistical measures presented in fig 3 in the bias column the bold value is the mean measured wind speed and in the sd column the bold value is the sd of the measurements table a 1 bias sd r mae rmse crmse era i era5 era i era5 era i era5 era i era5 era i era5 era i era5 a 7 86 4 98 a d01 0 75 0 55 4 2 4 15 0 53 0 58 3 51 3 25 4 56 4 27 4 5 4 24 a d02 0 31 0 09 3 85 3 77 0 55 0 59 3 33 3 15 4 32 4 12 4 31 4 12 a d03 0 3 0 08 4 03 3 96 0 56 0 61 3 29 3 06 4 3 4 04 4 29 4 04 a d04 1 83 1 6 5 2 5 1 0 57 0 63 3 83 3 39 5 07 4 63 4 73 4 34 b 7 39 4 64 b d01 1 21 1 07 4 27 4 25 0 53 0 6 3 46 3 17 4 48 4 16 4 31 4 02 b d02 0 73 0 58 3 87 3 82 0 54 0 58 3 24 3 02 4 2 3 97 4 14 3 93 b d03 0 49 0 38 3 84 3 82 0 56 0 62 3 11 2 87 4 05 3 77 4 02 3 75 b d04 1 69 1 61 4 84 4 85 0 57 0 63 3 61 3 28 4 74 4 41 4 42 4 1 c 7 82 4 69 c d01 1 16 1 13 4 48 4 54 0 43 0 5 3 93 3 67 5 03 4 75 4 9 4 61 c d02 0 36 0 43 3 75 3 8 0 45 0 51 3 39 3 21 4 52 4 28 4 51 4 26 c d03 0 79 0 69 3 47 3 53 0 49 0 59 3 28 2 96 4 34 3 92 4 27 3 86 c d04 0 01 0 17 4 17 4 23 0 49 0 58 3 43 3 13 4 5 4 11 4 5 4 1 fig a 1 top mslp during the winter months from september 2014 to march 2015 obtained from the era5 reanalysis bottom same as above but for summer months april 2015 to august 2015 fig a 1 
25724,this study evaluates a numerical weather prediction model as a tool for wind resource assessment in complex terrain and how the simulations are affected by the selection of initial and boundary conditions and various grid resolutions two global reanalyses era interim and era5 and four grid resolutions 27 km 9 km 3 km and 1 km have been considered the simulations have been compared to hub height wind measurements the era5 forced simulations were found to provide improved wind speed results the simulations show clear improvements in terms of lower error when the grid spacing is reduced from 27 km via 9 km to 3 km from 3 km to 1 km the error is not further reduced however the 1 km simulations tend to better reproduce the mean wind features and show defined areas with higher and lower wind speeds providing useful information for wind resource mapping in complex terrain keywords wind resource assessment wrf model era5 era interim complex terrain grid resolution 1 introduction a mesoscale numerical weather prediction nwp model is a mathematical representation of the physical and dynamical processes in the atmosphere on scales ranging from a few kilometres and up to a couple of thousands kilometres jacobson 2005 the models were originally developed for weather forecasting applications but have also emerged as a useful tool for the wind power sector as it can provide the users with high resolution wind climatology over large areas jacobson 2005 carvalho et al 2012b mughal et al 2017 carvalho et al 2014 wind data retrieved from the mesoscale nwp model simulations can assist wind power agents in identifying areas with favourable wind conditions and accompany local wind measurement campaigns to determine long term climatology carvalho et al 2012b mughal et al 2017 the nwp models represent an approximation of the real state of the atmosphere the size of the deviation between the real and the simulated atmosphere depends e g on the model configurations such as the choice of parametrization schemes for unresolved physical processes terrain and vegetation representation definition of the model domain size location and spatial resolution numerical options and initial and boundary conditions awan et al 2011 through a careful model configuration the user is able to optimise the model for one specific area or weather event for wind energy purposes it is especially important to limit and identify how the model deviates from the real atmosphere as a small error in the wind speed will induce a large error in the wind energy estimate in particular in our study special attention will be given to the dependency on the input atmospheric data and the model resolution both are expected to have a crucial impact on the ability of the model to reproduce the wind in the selected study area characterised by a coastal and mountainous terrain located within the arctic region for mesoscale nwp models the input atmospheric data give initialisation fields and boundary conditions throughout the simulation time and are typically provided by global reanalysis data carvalho et al 2014 the global reanalyses are obtained through global nwp models that optimally combine meteorological observations with model forecasts through an assimilation process into a physical coherent description of the evolution of the atmosphere dee et al 2011 hersbach et al 2020 the arctic region is particularly challenging for global reanalyses due to few observations from weather stations available for assimilation and evaluation wesslén et al 2014 inoue et al 2015 in addition the parametrization in the forecasting model is not tuned for phenomena unique to the arctic wesslén et al 2014 a careful evaluation of the global reanalysis used to force the simulations are therefore considered to be especially important for the area of interest in this paper two global reanalysis products will be considered the era interim era i and the era5 both produced by the european centre for medium range weather forecasts ecmwf several energy related studies have evaluated and compared era i and other global reanalyses as input data for mesoscale simulations and found that era i forced simulations tend to provide a better representation of the wind field mughal et al 2017 carvalho et al 2014 fernández gonzález et al 2018 menéndez et al 2011 carvalho et al 2012a the most recent global reanalysis by the ecmwf era5 benefits from 10 years of development in the forecasting model and the data assimilation method hersbach et al 2020 in addition era5 also typically assimilate a higher number of observations than era i the great strength of era5 in comparison with era i is expected to be due to the increased spatial and temporal resolution hersbach et al 2020 a horizontal resolution of 31 km 137 vertical layers and an hourly data output realize a more detailed description of the atmosphere and its evolution hersbach et al 2020 in comparison to the 80 km horizontal resolution 60 vertical layers and the 6 h output resolution of era i dee et al 2011 several studies report that era5 outperforms era i and several other global reanalyses in the representation of surface wind both over land olauson 2018 ramon et al 2019 and over oceans belmonte rivas and stoffelen 2019 particularly encouraging in relation to our study are the results provided by graham et al 2019 where wind data from five atmospheric reanalyses including era i and era5 were evaluated with respect to wind speed measurements in the arctic it was found that era5 provided the best results for wind speed with the highest correlation and smallest bias and root mean square error rmse the second essential factor for mesoscale wind simulations that will be considered in this study is the model resolution the study area is characterised by steep mountains fjords and numerous islands within the field of numerical weather prediction this terrain is typically described as complex and numerical simulations over such areas remain challenging carvalho et al 2012b mughal et al 2017 carvalho et al 2014 krieger et al 2009 the air flow in coastal zones is typically strongly affected by the abrupt change in temperature and frictional drag between land and sea areas national research council 1992 depending on several factors such as the stability of the atmosphere and the wind speed the coastal orography may give rise to complex flow patterns such as blocking strong gap winds and mountain waves with downslope wind storms national research council 1992 the model representation of the terrain is considered to be an important key factor for achieving a good representation of the wind flow in complex terrain carvalho et al 2012b mughal et al 2017 previous studies indicate that higher resolution tends to give better wind simulations results in complex terrain carvalho et al 2012b mughal et al 2017 fernández gonzález et al 2018 carvalho et al 2012a samuelsen 2007 valkonen et al 2020 the drawback of higher resolution is the computational cost and also the possibility of phase errors mass et al 2002 this study will assess the possible improvement in the wind speed simulations when wrf is forced by era5 compared to when it is forced by era i a second focus will be on how different model resolutions affect the ability to reproduce the wind field over the area of interest the wind simulations will be compared to hub height measurements at three different locations the rest of the paper is structured as follows section 2 describes the method of the study including a description of the measurements the configuration of the wrf model as well as the statistical metric used for evaluation the results and the discussion are presented in section 3 while section 4 provides a concluding summary 2 method 2 1 study area and field measurements the area considered in this study is located along the coast in the northern part of norway as illustrated by the red square in fig 1 a large seasonal variations in the temperature over northern scandinavia give rise to a monsoon like wind pattern on the coast of norway svendsen 1995 during the winter season cold air over land and warm air over the ocean due to the north atlantic current sets up a pressure gradient in east west direction with high pressure over land and low pressure over the ocean during the summer season the temperature and pressure gradients are often reversed this results in a main wind direction from the southeast se during the winter and from northeast ne during the summer svendsen 1995 in addition to the pressure gradient due to the difference between temperature over land and over ocean the polar front is a source of eastward moving low pressure systems especially during the winter season when the horizontal temperature gradient across the polar front is largest leading to baroclinic instability and frequently occurring strong low pressure systems there will be large variations in wind speed and directions in this area the wind patterns will also be affected by the local topography and lead to topographically induced variations in the main wind directions to illustrate that the seasonal variations in pressure and wind patterns also occur during the selected study period from 1 september 2014 to 30 august 2015 the mean sea level pressure mslp for the winter months september 2014 march 2015 and the summer months april 2015 august 2015 are presented in figure a1 in the appendix the relief map in fig 1b shows an overview of the terrain in the study area with mountainous islands and peninsulas separated by long fjords and sounds the mountains in the area range from sea level and up to about 1800 m above sea level masl the wind measurements used to analyse the model performance were collected from three different coastal locations referred to as location a b and c with mean measured wind speeds during the study period of 7 38 ms 1 7 10 ms 1 and 7 72 ms 1 respectively the coordinates the terrain ruggedness index rix bowen et al 1996 and the elevation above sea level of the locations are summarized in table 1 the rix describes the fraction of the area within a 2 km radius with slopes steeper than 30 berge et al 2006 the rix presented in this study have been collected from a wind resource report provided by the norwegian water resources and energy directorate nve byrkjedal and åkervik 2009 fig 1d shows a close up map with two dots where the left dot indicates location a and the right dot location b the two masts in location a and b are situated only 3 5 km apart on two rather flat mountain peaks at 550 masl and 505 masl respectively the two mountains are separated by a narrow valley with elevations below 200 masl the masts at a and b have similar configurations with measurements 80 m above ground level magl the data sets from location a b spanning a full year of measurements consists of 95 4 98 9 data relative to a full time series in the same period with the same sampling frequency at both a and b all measurements are recorded during the wind resource assessment phase before any construction of the wind farm and are therefore free of any wake disturbances from wind turbines mast c depicted by a black dot in fig 1c is situated at 68 masl on a rather small island the large mountains 5 km west of the mast reach up to 1000 masl and may partially work as an orographic blockage of westerly winds the lyngen alps a 90 km long mountain range separating two long fjords just south of location c is also expected to have a large impact on the wind at this location the wind measurements are done at 80 magl and has a data availability higher than 99 9 the mast is situated nearby an operational wind farm consisting of 18 wind turbines with hub height of 80 m and rotor diameter of 90 m the main wind direction is from the se and all the turbines are located north and west of the mast the measured data used in this study have a temporal resolution of 10 min the study period from 1 september 2014 to 30 august 2015 was selected because this period had the best coverage of measurements for all three locations 2 2 numerical weather model in this study the advanced weather research and forecasting wrf model version 3 9 1 is used this is a three dimensional nonhydrostatic mesoscale model developed by the national centre for atmospheric research ncar the model is widely used for research on atmospheric processes as well as for numerical weather prediction nwp a detailed description of the model may be found in skamarock et al 2008 for the purpose of this study the model is configured to include four one way nested domains d01 d02 d03 and d04 with a horizontal resolution of 27 km 9 km 3 km and 1 km respectively fig 2 polar stereographic projection has been applied as recommended for high latitude areas and the outer domain is centred at 67 56 n and 17 39 e the vertical structure of all domains consist of 50 terrain following sigma levels with an upper boundary at 50 hpa in order to describe the terrestrial fields information about the topography land water mask and land use have been interpolated to the model grid from the 20 category moderate resolution imaging spectroradiometer modis land use data and the global multi resolution terrain elevation data 2010 gmted 2010 both data sets were retrieved from the ncar database and have a 30 arc second resolution to represent the physical processes that are not explicitly resolved by the model the ncar convection permitting suite conus has been applied for all domains the conus suite consist of the thompson microphysics scheme thompson et al 2008 the mellor yamada janjic myj planetary boundary layer pbl scheme janjić 1994 the noah land surface model chen et al 1997 the rapid radiative transfer model for global applications rrtmg shortwave and longwave radiations schemes iacono et al 2008 the tiedtke cumulus scheme tiedtke 1989 zhang et al 2011 and the myj surface layer scheme janjić 1994 this combination of parametrization schemes has been developed and tested over several years until released as a physics suite in 2016 powers et al 2017 romine et al 2013 the time period considered in this study covers a full year for simulations over long time periods it is recommended to run several shorter simulations to avoid accumulation of truncation errors carvalho et al 2012b to achieve a full year of model data the wrf simulations were initialised 52 times and run for 8 days at the time hereinafter referred to as a week the first 12 h of the simulations were considered spin up time and therefore discharged the following 12 h were combined with the last 12 h from the previous week by linear interpolation to assure a smooth curve the model has been run with a time step of 81 s for d01 and reduced by a factor of 3 for each of the following nests outputs were saved every 10th minute the model was run twice first with era i and second with era5 as initial and boundary conditions the boundary conditions are updated throughout the simulation time for the simulations initialised with era i and for 35 of the 52 weeks initialised by era5 the model ran without problems however for the remaining 17 weeks initialised by era5 the model became unstable this problem was solved by either introducing damping such as time off centring vertical velocity damping and divergence damping or by running the model with adaptive time step the wind data considered in this study are retrieved in the vertical by interpolation of the model level wind to 80 magl and in the horizontal by bilinear interpolation of the grid points to the exact geographical location of the observation points 2 3 evaluation metrics in order to evaluate the performance of the different model configurations the following statistical metrics are used bias standard deviation sd mean absolute error mae root mean square error rmse centred root mean square error crmse and correlation coefficient r the bias the mae and the rmse are calculated as given in wilks 2011 while the crmse the r and the sd as given in taylor 2001 and are all repeated in this section let x i be a time series consisting of i 1 2 n data points the mean x and the sd of these time series is estimated as 1 x 1 n i 1 n x i 2 s d i 1 n x i x 2 n the sd is a measure of the dispersion of the values in a data set simulations with values close to the ones in the measurements are desired the bias describes the tendency of the model to systematically over or underestimate a parameter thus a negative value indicates that the model tends to underestimate the parameter i e the wind speed in this study and a positive value the opposite the bias between two time series x i and y i is defined as 3 d 1 n i 1 n x i y i where y i normally is taken as the true value here x and y represent the simulation and measurement data respectively the correlation coefficient indicates if there is a linear relationship between two time series and are given by the following relationship 4 r 1 n i 1 n x i x y i y s d x s d y if r equals 1 the highest possible value the pattern of the two time series is exactly the same however the amplitude might still be different i e the bias might be non zero if r equals 0 there is no linear correlation between the two data sets the mae the rmse and the crmse all describes the magnitude of the error between a measured and a simulated parameter if the pattern the phase and the amplitude of two time series are exactly identical the mae the rmse and the crmse equals zero the mae describes the total error between the measurements and the simulation without considering the sign of the error the rmse is similar to the mae however more sensitive to larger errors the crmse is different from the mae and the rmse in the way that the crmse is calculated without the tendency of the model to over or underestimate the mae the rmse and the crmse are calculated from the following equations 5 m a e 1 n x i y i i 1 n 6 r m s d 1 n i 1 n x i y i 2 1 2 7 c r m s d 1 n i 1 n x i x y i y 2 1 2 3 results and discussion 3 1 evaluation of the initial and boundary conditions the crmse the sd and the r between the measured data and the wrf simulations with different initialisation data have been summarized in taylor diagrams taylor 2001 in the left column in fig 3 for location a b and c from all the domains the statistical evaluation metrics are represented with coloured dots of different sizes wrf initialised with era i is depicted by red dots and wrf forced by era5 with green dots the smallest dot represents domain d01 while the increasing sizes represent d02 and d03 and eventually d04 by the largest dot the black dot represents the sd of the measurements in the right column of fig 3 the bias the rmse and the mae are presented for each location the solid lines represent the simulations forced by era i and the dotted lines the simulations forced by era5 for more details all the statistical evaluation metrics are also presented in the appendix table a1 at the end of this paper from the taylor diagrams in fig 3 it is evident that wrf initialised with era5 reproduces the temporal pattern of the measured wind speed better than wrf forced with the era i in terms of lower crmse and higher r for all locations and all domains also the rmse and the mae presented as graphs in the right column in fig 3 indicate that the era5 forced simulations reproduces the wind speed better than the era i driven simulations the wind speed bias of the model in this study are presented in the right column in fig 3 for location a and b the models tend to overestimate the wind speed but the bias is found to be lower for the era5 forced simulations red dotted line compared to the ones forced by era i red solid line in location c there are only small differences between the models forced by era5 relative to those forced by era i the sd of the simulations in comparison with the sd of the measurements left column fig 3 indicate how well the models are able to reproduce the wind speed variations in this study at these three locations there are little improvements in the sd when forcing the simulations with era5 rather than era i the sd is very similar for the two model configurations but appears to be sensitive to changes in the model resolution the wind roses for location b and c presented in fig 4 provide a visual comparison of the cumulative distribution of the wind speed in combination with the wind direction only the simulations retrieved from the innermost domain with the highest resolution are considered in this figure location a is not presented here as the wind roses show very similar features as the nearby location b the measured wind data for location b show a main wind direction from se with a frequency above 10 and a second main wind direction with lower frequency from the southwest sw the wind roses for both simulations forced by either era i or era5 show very similar features and are both able to reproduce the distribution of the measured wind well both wind roses show a high occurrence of wind from se and high occurrence of wind from sw both models show a low occurrence of wind with a northerly component in agreement with the measurements the wind rose for the measured data in location c shows a high occurrence of wind from two wind directions the first from se and the second from the south the measured wind from se has a high frequency of wind speed above 15 ms 1 as well as wind speeds exceeding 20 ms 1 the simulations are able to capture the main wind direction from se although slightly clockwise rotated compared to the measurements the era5 forced simulations are in better agreement with the measurements than the simulations forced by era i as it shows a higher frequency of wind from this direction including higher wind speeds above 15 ms 1 the higher frequency of wind speed above 15 ms 1 from se in the era5 forced simulations can be explained by a more detailed representation of the temperature over land in the era5 dataset in comparison to era i not shown that may give rise to more pressure driven channelling of the air out the lyngen fjord 3 2 evaluation of increasing model resolution the ability of the model to reproduce the temporal variations of the measurements as the grid resolution increases over the four domains is also presented in the taylor diagrams and as graphs in fig 3 for all locations the correlation coefficient r tends to increases slightly when the distance between the grid spacing is reduced over the four domains however the correlation between the simulations and the measurements appears to be more sensitive to the choice of initialisation data than it is to the resolution particularly for location c the crmse the rmse and the mae improve at all locations when the grid spacing is reduced from d01 via d02 to d03 in particular at locations a and b the reduction of the unsystematic error the crmse is small and similar to the correlation appears to be more sensitive to the choice of initial and boundary conditions than the resolution the wind speed simulations for location c appear to have a higher sensitivity to the resolution than the other two locations as the reduction in crmse over the first three domains is more prominent in contrast and for all locations when the resolution is increased further from 3 km in d03 to 1 km in d04 the mae the rmse and the crmse increases considerably in particular the crmse is higher in d04 than in d01 for location a and b other studies also report similar results related to the model resolution with clear improvements in the statistical evaluation metrics as the resolution increases up to a certain threshold mass et al 2002 siuta et al 2017 when the resolution is increased beyond this threshold the improvement is less evident and depends on the metrics used for evaluation as well as the location evaluated a similar behaviour can be seen for the bias for location a and b in fig 3 the positive bias decreases as the grid spacing is decreased from 27 km via 9 km 3 km when the grid spacing is further decreased to 1 km the bias increases to its highest value for the two locations at location c the d01 simulations show the largest overestimation while the lowest bias is found in d04 the sd of the simulations also presented in fig 3 for all locations are closest to the sd of the measurements in domain d01 and d04 for locations a and b the wind simulations in d04 are able to reproduce the spread in wind speeds better than the other simulations with an sd slightly larger than the sd of the measurements in location c the sd closest to the sd of the measurements is found in domain d01 in order to further assess how the change in grid spacing affects the quality of the model simulations in this study the mean features are evaluated using frequency histograms and wind roses in consideration of content limits only the results from the era5 data will be considered in this section as similar results were found for the era i data histograms for the measured and simulated wind speeds are presented in fig 5 for location b left and c right the x axis shows the different wind speed bins while the y axis shows the frequency in number of hours by comparing the histograms for location b it is evident that all domains underestimate the frequency of the wind speeds below 5 ms 1 and hence are all the histograms for the simulations shifted to the right and towards higher wind speeds this is also reflected by the positive bias for all domains at this location the first three domains d01 d02 and especially d03 overestimate the intermediate wind speeds between 5 ms 1 and 10 ms 1 d01 is able to reproduce the occurrence of the higher wind speeds above 15 ms 1 well while d02 and d03 show an underestimation of the same wind speeds this coincides well with the sd for the different domains where the low sd found for d02 and d03 indicates a low spread of the wind speeds and the higher sd for d01 indicates that the simulations are able to capture the spread of the wind speed better the shape of the histogram for d04 shows a great resemblance with the histogram of the measurements although with a shift to the right while the wind speed histograms for all the simulations at location b are skewed to the right compared to the measurements only the winds from the d01 simulations are skewed to the right for location c the histograms for the other three domains that also have a considerable lower bias than d01 show a more similar shape to the histogram of the measurements similar to the situation at location b d02 and d03 overestimate the intermediate wind speeds at location c and underestimate the higher wind speeds the histogram of the wind speeds retrieved from d04 shows a better agreement between the simulated wind speed and the measurements than the wind speeds from the simulations with a coarser resolution to also evaluate how the simulated wind speed in combination with wind direction are affected by the increased resolution wind roses for location b and c are presented in figs 6 and 7 respectively the figures include wind roses for the measurements and for the wind data retrieved from all four domains in addition the figures also include wind roses for the original 100 magl era5 wind data the wind components are retrieved from the exact locations by bilinear interpolation first the wind roses for location b in fig 6 will be considered the wind rose presenting the original era5 data shows a high occurrence of wind from se this is in agreement with the measured main wind direction from se although with a lower frequency the era5 model is not able to reproduce any of the higher wind speeds above 15 ms 1 present in the measurements this illustrates the limitations of global reanalyses like era i and era5 to reproduce local wind especially in complex terrain and why direct use of the reanalysis data should be avoided in the assessment of local wind speeds olauson 2018 ramon et al 2019 the d01 domain appears to be an improvement compared to era5 for the frequency of wind speeds above 10 ms 1 but not for the main wind direction as the wind direction is spread almost evenly over a large sector stretching from west to se the large difference between the era5 model and the selected configuration of the wrf model d01 with almost similar horizontal resolution can probably be related to differences in e g domain sizes terrain representation physiography and parametrization of physical processes a more detailed investigation of these differences although relevant for wind simulations is not further elaborated in this paper since the main focus is on the effect of the increased horizontal resolution in the wrf model the finer resolution simulations d02 d03 and d04 are able to reproduce the main wind direction from se better than d01 compared to the measurements the frequency of wind speeds between 15 ms 1 and 20 ms 1 from se is low in d02 and increases slightly in d03 d04 has a higher frequency of the same wind speeds in better agreement with the measurements and is also able to reproduce some of the wind speeds above 20 ms 1 the observed frequency of wind directions from the sw is more apparent in d03 and d04 compared to the coarse resolution models however the frequency of winds from sw are better reproduced in d04 compared to d03 the wind roses for the measurements in both location a not shown and location b show a low occurrence of wind from a sector stretching from about 225 to 255 w sw in particular there is a very low occurrence of wind speeds above 10 ms 1 one explanation for the low occurrence of wind from w sw can be orographic blocking by the steep mountains on the island senja the low occurrence of wind from w sw are not captured well in any of the simulations although slightly better in d03 and d04 in comparison with the simulations with coarser resolutions the poor representation of wind in this sector can be related to the terrain representation by the model in reality the mountains on senja reach up to a 1000 masl whereas in the four models they are represented with lower elevations for instance in d04 altitudes only up to 700 masl represent the same mountains this indicates that a better terrain representation perhaps also even higher horizontal model resolution is needed to resolve the terrain effect upstream of location a and b in fig 7 the wind data for location c is presented the era5 wind data and the two wrf simulations d01 and d02 have winds mainly coming from one large sector stretching from the west to the south d01 has compared to the era5 data and the other wrf simulations a higher frequency of all wind speeds above 10 ms 1 the two higher resolution domains d03 and d04 are able to reproduce the main wind direction from se although slightly clockwise rotated compared to the measurements d04 has a higher frequency of wind speeds between 15 ms 1 and 20 ms 1 from se than d03 and are in better agreement with the measurements none of the simulations are able to capture the highest wind speeds above 20 ms 1 present in the measurements from se in the measurements there is a high occurrence of wind directly from the south this is not represented in either d03 or d04 instead in both the finer resolution simulations there is a higher frequency of wind from sw in comparison to the measurements the behaviour of an air flow approaching a mountain barrier depends on the atmospheric stability the wind speed and the characteristics of the mountain such as the height whiteman 2000 the differences in wind patterns can therefore be related to the representation of the terrain in the models for instance the lack of wind from the south in the simulations can be related to unresolved topography in the lyngen alps see fig 1 upstream of location c that causes too much orographic blocking of the wind from the south another explanation can be related to the model terrain representations of the mountains west of location c the mountains reach up to 1000 masl and under varying atmospheric conditions winds from the west can for instance be forced around the mountains or lifted over the mountains in the d04 terrain the same mountains west of location c has a maximum elevation of 750 masl this considerably lower model terrain can give rise to a slightly different wind pattern at location c where e g less blocking of the air leads to stream across instead of around the mountains two shorter time periods have been selected to illustrate a possible explanation to the lack of winds from the south in the d04 simulations in location c in the first period selected from 19 20 december 2014 the measured wind direction is from the south and the wind speed varies between 0 ms 1 and 8 ms 1 the d04 simulation in the same time period has wind from se and the wind speed is overestimated and varies between 7 ms 1 and 15 ms 1 in the second selected time period on 13 may 2015 00 00 to 12 00 local time the measured wind direction is also from the south however the d04 simulations show wind from sw the measured wind speed varies 0 ms 1 and 7 ms 1 while the simulated wind speeds varies between 3 ms 1 and 7 ms 1 in fig 8 the d04 wind simulations from 80 magl at and around location c is presented for 20 december 2014 04 00 local time left and 13 may 03 00 local time right on the 20 december the wind field has a strong easterly component in the fjord east of locations c and a green field of wind speeds of approximately 12 ms 1 in the same location closer to location c and the nearby mountains there is a field of lower wind speeds and the wind vectors turns towards se a more accurate terrain representation may lead to more blocking of the air flow approaching from the west hence lower wind speed in location c and force more of the air flow around the mountain in a northward direction on 13 may as can be seen in fig 8 the air flow approaches the mountains nearby location c from the west the wind vectors are not affected by the mountain barrier and there is a field of higher wind speed on the mountain tops indicating that the air flow passes over the mountains a more accurate representation of the terrain with higher mountains may result in more of the air flow to be forced around instead of over the mountains west of location c and be deflected northwards on the lee side of the mountains in addition as can be seen in the wind roses in fig 7 the numerical simulations have a higher occurrence of wind directly from the west at location c compared to the measurements west of the mast in location c the mountains with elevations up to 1000 masl might cause an orographic blockage of wind from the west domain d04 is able to capture some of this orographic blockage with a low frequency of wind from this direction although with intermediate and high wind speeds present however at certain atmospheric conditions and wind speeds strong downslope winds might occur it must therefore also be taken into consideration that the measurements in particular wind speeds from the west might be disturbed by the nearby wind farm 3 2 1 case study based on the evaluation of the different horizontal model resolutions so far in this study the positive impact of an increased resolution from d03 to d04 is unclear while the wind speed histograms and the wind roses show an improved representation of the mean features in d04 compared to d03 for all locations a b and c the crmse the rmse the mae and the bias indicate the opposite a higher resolution and a more realistic representation of the terrain are e g expected to improve the simulations of terrain induced wind effects however the method of comparing point measurements with interpolated model information might be limited by timing and spatial errors of meteorological features like mountain waves and gap winds that become more prominent with increased horizontal resolution mass et al 2002 in particular when using traditional verification metrics to compare measurements and simulations in points spatial errors may be penalized both for the absence of the observed wind pattern but also for the presence of a wind pattern that is not observed at the location zingerle and nurmi 2008 this double penalty might not be seen when the resolution is lower as the wind features tend to be more smoothed zingerle and nurmi 2008 in order to further analyse the effect of increased horizontal resolution from 3 km to 1 km a case study has been carried out for location a and b fig 9 shows a close up map of the real terrain top and the model topography d04 in the middle and d03 at the bottom the maps include location a and b indicated by red dots from fig 9 it is clear that the topography is much better resolved in d04 compared to d03 when the terrain of d04 is compared to the real terrain it is clear that even at 1 km horizontal resolution considerable parts of the orography are not resolved in particular there is a height difference between the real topography and the topography of d04 of 82 m at location a and 75 m at location b table 1 fig 10 shows the wind speed and the wind direction of location a and b over a 24 h period starting at noon on the 9 october 2014 the measured wind is coming from se the main wind direction between 120 and 150 the measured wind speeds at both locations are high with long periods over 15 ms 1 at both locations and also over 20 ms 1 at location a the simulations are able to reproduce the wind direction in both d03 and d04 well while the wind speed especially from about 18 00 local time on 9 october 2014 is considerably underestimated moreover after about 20 00 only small differences can be found in the wind speed when the resolution is increased from d03 to d04 in particular during the 5 h time period from about 20 00 on the 9 october 2014 to 01 00 the following day there are observed stronger winds at location a than at location b neither the high wind speed or the difference in wind speeds between the two locations are represented by d03 or d04 in order to study this in detail fig 11 shows a close up map of the wind speed and direction 9 october at 22 40 for d03 left and d04 right d03 has small variations of the wind speeds with one large green coloured area representing wind speeds between 10 and 14 ms 1 covering both the fjord sw of location a and b and the locations of the measurements in d04 we can identify larger variations in the wind speeds location a and b are both included in a light green area with wind speeds in the interval 14 16 ms 1 and just downwind of location a we find a yellow area with wind speeds up to 20 ms 1 although this strong wind field does not include location a from the wind map in fig 11 it is evident that d04 are able to reproduce strong winds in close vicinity of location a while the strong winds are not seen in d03 the strong down slope wind seen downwind of location a can e g be a result of mountain wave activity mountain waves are reported to have the possibility to severely impact wind power production as they can cause large spatial and temporal fluctuations in wind speeds draxl et al 2021 xia et al 2021 the case study presented here exemplifies how high resolution wind simulations can provide more detailed information that are useful both for wind resource wind mapping and wind prediction in complex terrain 4 conclusion in this study wind simulations provided by the wrf model have been evaluated in an area characterized by a coastal and complex terrain located in northern norway in specific the aim of this study has been to evaluate the performance of the simulations when either forced by era i or era5 both provided by the ecmwf as well as to evaluate the impact different grid spacings has on the wind simulations the wind simulations have been compared to hub height wind measurements at three locations it was found that the era5 forced simulations provided lower mae rmse and crmse and higher correlation in comparison to the era i forced simulations the wrf simulations were run with four one way nested domains d01 d02 d03 and d04 with horizontal resolution of 27 km 9 km 3 km and 1 km respectively the results showed that when the resolution was increased from 27 km via 9 km to 3 km the mae the rmse the crmse and the bias decreased while the correlation increased when the grid spacing was further decreased from 3 km to 1 km the mae the rmse and the crmse at the three locations were impaired it is suggested that the lack of improvement from the 3 km to the 1 km simulations could be due to timing and spatial errors that occur when comparing measurements and simulations from a fixed geographical point further research should evaluate how different pbl schemes affect the results regarding increased model resolution nevertheless the wind speed histograms and the wind roses show that the d04 simulations with 1 km resolution provide an improved reproduction of the mean features of the wind and the variations in the wind speed in comparison with the simulations with lower resolution in addition a case study show that although the high wind speed events are not reproduced at the exact same locations as the measurements the highest resolution simulations show fields of higher wind speeds in nearby locations that coincides with the measured wind event whereas d03 is not able to reproduce these terrain effects this result exemplifies that although the highest resolution simulations scores low on the traditional statistical measures such as mae rmse crmse and bias the higher resolution simulations appears to reproduce local terrain effects better than the simulations run with a coarser resolution a limitation of this study is that only wind measurements at one height 80 magl is considered a further study should include measurements of the vertical wind profile and an evaluation of the ability of the wrf model to reproduce the wind shear in complex terrain in particular wind shear that might be harmful for wind turbines declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported by troms county and industry development fund under the project title renewable energy in the arctic academy and business in a joint effort rda12 46 the authors would like to thank nordlys vind and troms kraft for the meteorological data used in this work appendix table a 1 summary of the statistical measures presented in fig 3 in the bias column the bold value is the mean measured wind speed and in the sd column the bold value is the sd of the measurements table a 1 bias sd r mae rmse crmse era i era5 era i era5 era i era5 era i era5 era i era5 era i era5 a 7 86 4 98 a d01 0 75 0 55 4 2 4 15 0 53 0 58 3 51 3 25 4 56 4 27 4 5 4 24 a d02 0 31 0 09 3 85 3 77 0 55 0 59 3 33 3 15 4 32 4 12 4 31 4 12 a d03 0 3 0 08 4 03 3 96 0 56 0 61 3 29 3 06 4 3 4 04 4 29 4 04 a d04 1 83 1 6 5 2 5 1 0 57 0 63 3 83 3 39 5 07 4 63 4 73 4 34 b 7 39 4 64 b d01 1 21 1 07 4 27 4 25 0 53 0 6 3 46 3 17 4 48 4 16 4 31 4 02 b d02 0 73 0 58 3 87 3 82 0 54 0 58 3 24 3 02 4 2 3 97 4 14 3 93 b d03 0 49 0 38 3 84 3 82 0 56 0 62 3 11 2 87 4 05 3 77 4 02 3 75 b d04 1 69 1 61 4 84 4 85 0 57 0 63 3 61 3 28 4 74 4 41 4 42 4 1 c 7 82 4 69 c d01 1 16 1 13 4 48 4 54 0 43 0 5 3 93 3 67 5 03 4 75 4 9 4 61 c d02 0 36 0 43 3 75 3 8 0 45 0 51 3 39 3 21 4 52 4 28 4 51 4 26 c d03 0 79 0 69 3 47 3 53 0 49 0 59 3 28 2 96 4 34 3 92 4 27 3 86 c d04 0 01 0 17 4 17 4 23 0 49 0 58 3 43 3 13 4 5 4 11 4 5 4 1 fig a 1 top mslp during the winter months from september 2014 to march 2015 obtained from the era5 reanalysis bottom same as above but for summer months april 2015 to august 2015 fig a 1 
