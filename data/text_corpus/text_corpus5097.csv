index,text
25485,this study proposes an analytical and meshless model for three dimensional transient flow induced by multiple wells in a stream confined aquifer system with arbitrary nonuniform aquifer thickness the model involves allocating external image wells near the aquifer boundary the aquifer drawdown is defined as theis solution plus a simple series expanded by complementary error functions reflecting the boundary effects of the stream and impervious stratum the solution for stream depletion rate sdr can be derived based on the drawdown solution results suggest the dupuit assumption of no vertical flow is valid to estimate sdrs affected by the landward exponentially decreasing thickness the sdr induced by multiple wells is the same as that by a single well if the total well discharge rate and stream well distances in the multiple well case are identical to those in the single well case the present solution outperforms a finite difference solution in computational efficiency keywords analytical approach meshless method nonuniform aquifer thickness stream aquifer interaction robin boundary condition multiple wells data availability no data was used for the research described in the article 1 introduction in recent years there has been renewal of interest in dealing with the problem of stream aquifer interaction e g ou et al 2013 huang and yeh 2015 dogrul et al 2016 colin et al 2019 antonio et al 2020 huang et al 2020 groundwater extraction by wells induces stream water filtration into the adjacent aquifer and causes impacts on ecosystems with significant decrease in the stream stage the stream depletion rate sdr defined as the ratio of the stream water filtration rate to the well discharge rate is a widely used indicator huang et al 2018 provided an extensive review on existing analytical sdr solutions and categorized the solutions according to aquifer types i e confined unconfined and leaky aquifers flow dimensions i e two three and quasi three dimensions well types i e vertical horizontal and radial collector wells and stream representations i e source terms and the dirichlet and robin conditions they summarized the solutions were developed based on the assumption of a stream fully penetrating an aquifer called fully penetrating stream this assumption however causes 10 61 errors in predicting the sdr for a stream with partial penetration sophocleous et al 1995 since then xiong et al 2021 derived a laplace domain solution of sdr for a rectilinear stream shallowly penetrating a confined aquifer and found sdr predictions significantly depending on the aquifer storage below that stream regarding the problems of nonuniform aquifer thickness hantush 1962a revealed a confined aquifer of thickness linearly varying cannot be treated as a uniform thickness aquifer in simulating the flow during the entire period except at very early times this indicates the importance in dealing with spatially varying aquifer thickness in predicting sdr in reality little attention was paid to assess sdr subject to the effects of nonuniform aquifer thickness and shallow streams in the past the techniques to handle the problem of nonuniform flow domain may be classified as the analytical approach e g hantush 1962a 1962b 1962c analytic element method e g strack 2003 steward 2015 meshless method e g wu et al 2008 and numerical method such as finite difference scheme the analytical approach based on two dimensional 2d transient flow equation is applicable only under the condition of wedge shaped aquifer thickness hantush 1962a or exponentially varying thickness hantush 1962b the analytic element method with a series solution in ameli and craig 2014 applies to spatially dependent aquifer thickness while the solution satisfying the laplace equation of discharge potential is limited to three dimensional 3d steady state flow the numerical method usually takes much effort to build spatial discretization of irregular aquifer domain and involves laborious calculations for drawdown solutions at gird points the meshless method has increasingly applied to various problems in hydrogeological area such as irregular aquifer boundary e g kuo et al 1994 wave propagation e g wu et al 2008 non darcian flow e g wen et al 2014 stream aquifer interaction e g huang and yeh 2015 and aquifer heterogeneity e g huang and yeh 2016 the meshless flow domain is achieved by applying characteristic kernel functions such as the theis well function kuo et al 1994 huang and yeh 2015 2016 logarithmic function wu et al 2008 and multi quadrics basis function as well as gaussian basis function wen et al 2014 its advantages are good efficiency and accuracy over the numerical method wen et al 2014 huang and yeh 2016 wu et al 2008 developed a meshless method for solving the problem of 2d flow below a dynamic free surface what seems to be lacking however is a meshless method for solving the problem of 3d transient flow in aquifers of spatially varying thickness previously huang and yeh 2016 presented an analytical and meshless approach for 2d transient flow in a heterogeneous confined aquifer by assuming fully penetrating streams and uniform aquifer thickness they mainly handled the problem of the aquifer having a parameter zonation structure composed of five zones with different hydraulic parameters but neglected the issue of stream aquifer interaction their work inspires us to develop a new meshless model for simulating 3d transient flow in a nonuniform thickness confined aquifer connected with a shallow stream with the clogged streambed specified as the robin condition image wells hereinafter called source points are allocated outside and near the aquifer boundary for hydraulically replacing the boundary effect the drawdown solution is expressed in terms of the theis 1935 solution plus a simple series in which each term is defined as a complementary error function accounting for radial transient flow toward a source point the discharge rate for each source point is solved from a system of equations obtained by substituting the drawdown solution into the aquifer boundary conditions the sdr solution is then derived from darcy s law along with the drawdown solution a finite difference solution fds is also developed to examine the present solution a criterion for applying the present solution to various configurations of confining beds is provided temporal sdr distributions affected by nonuniform aquifer thickness are investigated in addition the effect of multiple pumping wells on sdr and aquifer drawdown is discussed the notations and abbreviations used in the text are shown in table 1 2 methodology 2 1 mathematical model fig 1 displays a pumped confined aquifer having spatially varying thickness and connecting a shallow stream with a clogged streambed in between consider the cartesian coordinates x y z the shortest distance or a selected distance between the stream and pumping well is denoted as letter a hantush 1962b defined the elevations of the lower and upper confining beds in terms of arbitrary functions as z l f 1 x y and z u f 2 x y respectively in the present meshless model the aquifer boundary is substituted by nodal points the first nodal point is allocated arbitrarily at the boundary e g the intersection of the streambed and upper confining bed the other nodal points are then successively allocated with a distance d along the boundary the source points are designated beside the nodal points for hydraulically replacing the boundary effect the distance between a source point to its nearest nodal point is l and the direction of the line across them should be normal to the boundary the governing equation of the model for 3d transient flow induced by multiple extraction wells and source points can be expressed as 1 k h 2 s x 2 k h 2 s y 2 k v 2 s z 2 s s s t k 1 m q k d k δ x x k δ y y k j 1 n q j δ x x j δ y y j δ z z j where s is aquifer drawdown t is time k h and k v are respectively the horizontal and vertical hydraulic conductivities s s is the specific storage δ is the dirac delta function the subscripts j and k respectively represent the j th source point at the location x j y j z j and k th well at x k y k d k is aquifer thickness at the k th well q k is a discharge rate of the k th well m is the number of the wells q j is a discharge rate of the j th source point if q j 0 and an injection rate if q j 0 and n is the number of the source points the potentiometric surface before pumping is at the same elevation as stream stage the initial condition is thus written as 2 s 0 a t t 0 the confining beds are under the no flow condition that 3 s n 0 a t z z u a n d z z l where n is the perpendicular direction the clogged streambed can be regarded as the robin boundary condition denoted as 4 k n s n k b s s 0 0 a t ω s where k and b are the streambed hydraulic conductivity and thickness respectively ω s is the streambed domain k n is the aquifer hydraulic conductivity normal to the streambed k n k h for a fully penetrating stream k n k v for a shallow stream s 0 is a vertical distance between the stream stage and potentiometric surface s 0 0 for the stage below the surface and s 0 0 for the opposite the sdr for narrow aquifers between a stream and an impervious stratum significantly increases at late pumping time due to that stratum miller et al 2007 the lateral aquifer boundary is therefore considered under the no flow condition expressed as 5 s n 0 a t ω note that the aquifer boundary expressed by z u z l ω s and ω can be arbitrarily distributed in space define the following dimensionless variables and parameters s 4 π k h a q s s 0 4 π k h a q s 0 x x a y y a z 1 κ z a x k x k a y k y k a q k q k q q j q j q 6 x j x j a y j y j a z j 1 κ z j a d k 1 κ d k a z u 1 κ z u a z l 1 κ z l a t k h t s s a 2 α k a k n b where κ k v k h q q 1 q 2 q m and the overbar indicates a dimensionless symbol with equation 6 equations 1 5 can be written respectively as 7 2 s x 2 2 s y 2 2 s z 2 s t k 1 m 4 π κ d k δ x x k δ y y k j 1 n 4 π q j κ δ x x j δ y y j δ z z j 8 s 0 a t t 0 9 s n 0 a t z u a n d z l 10 s n α s s 0 0 a t ω s 11 s n 0 a t ω with ω s and ω being the dimensionless domains 2 2 solution of the present model based on the principle of superposition the drawdown solution can be written as 12a s k 1 m q k κ d k w u k j 1 n q j κ r j e r f c r j 2 t 12b w u u exp v v d v 12c u k x x k 2 y y k 2 4 t 12d r j x x j 2 y y j 2 z z j 2 where w u k is the exponential integral also called theis well function and e r f c is the complementary error function the first term on the right hand side rhs of equation 12a describes the dimensionless drawdown due to extraction wells and the second term accounts for the effect of the boundary conditions equations 9 11 on the drawdown one can refer to appendix a for the derivation of equation 12a according to darcy s law sdr induced by well pumping can be estimated by integrating the flux across the streambed per unit area d a divided by the sum of the discharge rates i e q 1 q m denoted in the dimensional form as 13 s d r k q b ω s s s 0 d a a t ω s in the dimensionless form it becomes 14 s d r α 4 π ω s s s 0 d a a t ω s where s is the present solution equation 12a and d a is dimensionless unit area the implementation of the present solution involves following two steps 1 the aquifer boundary is replaced by the nodal points and the dimensionless distance defined as d between two neighboring points is at most 0 2 huang and yeh 2016 with the nodal points the locations of the source points can be defined with the dimensionless distance defined as l between a source point to tis nearest nodal point 2 the rates of q j at source points are determined by solving a system of equations based on the present solution equation 12a and boundary conditions equations 9 11 four cases of stream aquifer systems given in section 3 demonstrate the allocation of nodal and source points and the determination of the values of q 1 q 2 q n the source point far away from extraction well is ignorable to facilitate the computing efficiency of estimating q j at the early pumping period define a dimensionless radius of influence from a source point to a location at which the dimensionless drawdown s q j i e 4 π k h a s q j is 10 6 the radius is defined as r j 20 t by applying newton s method to solve s q j 10 6 with equation 12a m 0 and n 1 it can therefore be concluded that q j can be set to zero for nodal points located outside the radius i e r j 20 t in order to reduce the number of unknowns q j 2 3 finite difference solution an implicit fds is developed by employing the mathematica function nsolve to make comparison with the present solution the numerical model contains the present model equations 7 11 but excludes the term j 1 n 4 π q j κ δ x x j δ y y j δ z z j in equation 7 consider a uniform grid for the aquifer domain with the dimensionless grid size of 0 05 in the x y or z direction the dimensionless drawdowns at the grid nodes can be obtained by solving the numerical model the sdr is then approximated as 15 s d r α 4 π n 1 n a δ a s n s 0 a t ω s where s n is the n th dimensionless drawdown at the grid nodes on the streambed domain δ a 0 05 2 is a dimensionless unit area and n a is the number of the nodes 3 results and discussion four cases are chosen to demonstrate the application of the present solution case 1 is for a cuboid aquifer with a fully penetrating stream discussed in section 3 1 case 2 is for a cuboid aquifer with a shallow stream presented in section 3 2 while case 3 is for aquifers of nonuniform thicknesses given in section 3 3 case 4 is for a cuboid aquifer with multiple pumping wells beside a shallow stream demonstrated in section 3 4 the allocation of nodal points and source points for good predictions of drawdown and sdr is discussed in section 3 1 the default values are q 1 1500 m 3 d a 50 m d 1 27 38 m k h 60 m d k v 18 m d s s 10 4 m 1 d 0 2 κ 0 3 α 20 s 0 0 m 1 q q 1 d k d 1 and x k y k 0 0 3 1 case 1 cuboid aquifer in case 1 a cuboid aquifer is considered with one plane specified under the robin condition for a fully penetrating stream and the others specified under the no flow condition for impervious strata as illustrated in fig 2 this case is to examine the allocation of nodal points and source points by applying both the present solution and analytical solution of huang et al s 2014 the stream aquifer system is in the domain bounded by 1 x 4 50 m x 200 m 2 5 y 2 5 125 m y 125 m and 1 z 0 27 38 m z 0 m orthogonal and uniform nodal points on the six planes of the aquifer domain are shown in fig 2 the dimensionless coordinates of the i th nodal point are x i y i z i the dimensionless distance d between two adjacent nodal points is 0 2 source points beside the six planes are then arranged one source point is allocated near a nodal point e g p1 on each plane two source points near a nodal point e g p2 at each edge and three source points near a nodal point e g p3 at each corner the total number of the source points is therefore 1 0 2 1 5 0 2 1 4 5 0 2 1 2 2 1976 the dimensionless coordinates of x j y j z j for the source points and x i y i z i for the nodal points are provided in the supplementary material the determination of q 1 q 2 q 1976 for the source points is explained in appendix b then equation 14 can be approximated as 16 s d r α 4 π i 1 n s c d 2 s i s 0 where s i is the drawdown defined as equation 12a with replacing x y z by x i y i z i being the locations of the nodal points on the streambed plane n s 156 is the number of the nodal points and c is equal to 0 25 for the nodal points at the four corners 0 5 for the nodal points at the four sides and 1 for the nodal points inside it is of practical interest to explore the source point location represented by the dimensionless distance l between a source point and its nearest nodal point the temporal sdr distributions predicted by the present solution equation 16 with l 0 1 0 2 0 5 0 8 and 0 9 are compared with those predicted by the huang et al 2014 analytical solution for the same stream aquifer system for l 0 2 0 5 and 0 8 both solutions give close sdr predictions over the entire pumping period on the other hand the present solution significantly overestimates sdr for l 0 1 when t 2 t 12 min and gives an incorrect prediction of sdr 1 not shown when t 20 the reason for the error is due to the use of l d i e 0 1 0 2 in this case reported in wu et al 2008 for l 0 9 the sdr curve predicted by the present solution slightly fluctuates in the period of 0 1 t 1 and at t 100 for l 1 the temporal sdr distribution fluctuates dramatically after a certain t depending on the magnitude of l not shown it can be concluded that the present solution gives correct predictions when d l 0 9 and d 0 2 3 2 case 2 aquifer connected with a shallow stream in case 2 the present solution is applied to an aquifer connected with a shallow stream illustrated in fig 3 in which the stream depth is largely less than the aquifer thickness consider the aquifer in the cuboid domain bounded by 3 x 2 2 5 y 2 5 and 1 z 0 with all impervious boundaries except the one adjacent to the stream which situates in the area of 2 x 1 2 5 y 2 5 and z 0 the allocation of the nodal points and source points is the same as case 1 i e l 0 5 d 0 2 because this case has the identical size of the aquifer domain note that the nodal points and source points within the range 1 x 4 in the previous case should be changed to 3 x 2 in this case the total number of the source points is also 1976 the locations of the source points and nodal points are defined in the supplementary material the derivation of q 1 q 2 q 1976 for the source points is given in appendix b fig 3 displays time varying sdr curves plotted by the present solution i e equation 16 and fds for the aquifer with the shallow stream or a fully penetrating stream both solutions agree well to the sdr during t 10 or t 100 and the difference in the period 10 t 100 is small and acceptable there is a discrepancy in the sdrs between the streams indicating that a shallow stream cannot be considered as a fully penetrating one in estimating sdr e g sophocleous et al 1995 reeves et al 2009 xiong et al 2021 the present solution takes about 1 12 of the total computing time that the fds takes to predict the sdrs at t δ t 2 δ t 10 3 with a small dimensionless time step δ t this is because the present solution can estimate the sdr or aquifer drawdown directly at a specific dimensionless time t rather than starting from t δ t 2 δ t to that specific time it can be seen from this case that the present solution outperforms the fds in computational efficiency 3 3 case 3 aquifer of nonuniform thickness this case demonstrates two scenarios for one aquifer with exponentially varying thickness and the other one with wedge shaped thickness fig 4 a illustrates the former scenario in the domain of 1 x 14 and 2 5 y 2 5 between the confining beds located at hantush 1962c 17a z u 0 17b z l c 2 exp 2 x c 1 where c 1 10 and c 2 exp 0 2 are determined when the largest dimensionless aquifer thickness at x 1 is unity and the dimensionless distance between the stream and well is also unity shown in fig 4a note that the use of horizontally rectangular domain in this case is to simplify the problem and emphasize the capability of the present solution in handling the space varying thickness consider finite nodal points on the aquifer boundary displayed in fig 4a in the x direction the dimensionless distance between two neighboring nodal points on the lower confining bed is 0 2 the location of these nodal points starts from x 1 1 x 2 0 804 and x 76 13 948 obtained by x m 1 0 2 1 d z l d x 2 with d z l d x 2 c 2 exp 2 x m c 1 c 1 based on equation 17b the last x 77 is set to 14 the elevations of the nodal points on the upper confining bed can be defined by equation 17a with x x 1 x 2 x 77 in the y direction the location of the nodal points starts from y 2 5 to 2 5 with increment of d 0 2 in the z direction the elevation of the nodal points begins from the lower confining bed with d 0 2 the source points are then allocated beside the aquifer boundary shown in fig 4a again one source point is assigned to each nodal point e g p1 on the lateral planes and confining beds two source points to a nodal point e g p2 at each edge of the aquifer domain and three source points to a nodal point e g p3 at each corner a vector from a source point at x j y j z j to the nearest nodal point at x i y i z i should be normal to the aquifer boundary the dimensionless distance l between them is 0 2 then the location of a source point beside the confining beds can be defined as 18 x j x i l d z d x 2 d z 2 y j y i z j z i l d x d x 2 d z 2 where d x x i 1 x i d z z i 1 z i and x i 1 z i 1 counterclockwise next to x i z i notice that the location of a source point e g s1 or s2 beside a nodal point e g p3 or p4 at the corner of the aquifer domain satisfies 19 x j x i l d z d x 2 d z 2 y j y i z j z i l d x d x 2 d z 2 where d z z i z i 1 d x x i x i 1 and x i 1 z i 1 clockwise next to x i z i the total number of the source points is 4482 similarly the scenario for an aquifer with the wedge shaped thickness exhibited in fig 4b has the same procedure of allocating nodal points and source points but equation 17b is replaced by z l 2 x 1 5 1 in the aquifer domain of 1 x 4 the total number of the source points in this scenario is 9897 one can refer to the supplementary material for the locations of the nodal points and source points and appendix b for the determination of q 1 q n the effect of the nonuniform aquifer thicknesses on sdr is examined herein fig 4 shows the temporal sdr distributions predicted by the present solution equation 16 for the scenarios of the aquifers with the exponentially varying thickness and wedge shaped thickness the hantush 1962c solution was developed for the former scenario while the hunt 1999 solution was for the uniform thickness scenario the fds is applied to assess the present solution note that both the hantush and hunt solutions adopted the dupuit assumption of no vertical flow indicating a fully penetrating stream is considered sun and zhan 2007 the present solution removing the assumption by considering 3d flow agrees well with the hantush solution this indicates an insignificant influence of vertical flow in the aquifer of the exponentially decreasing thickness defined by equations 17a and 17b with c 1 10 or c 1 10 for thinner aquifers the vertical flow influence becomes minor when 10 c 1 400 and the difference in sdr between our and hantush solutions increases not shown the influence becomes again insignificant when c 1 400 and the hantush and hunt solutions give close sdrs with insignificant difference indicating that the influence of the nonuniform thickness on sdr is minor the sdr in the scenario of wedge shaped thickness is less than that of the uniform one during the period of 0 1 t 20 but a more sdr after that period due to the boundary effect at x 4 e g miller et al 2007 huang et al 2014 this can be attributed to the fact that the wedge shaped thickness is thicker and thus provides more groundwater to the pumping well at a given time in addition the difference in sdrs calculated by the fds and present solution is small for the scenario of wedge shaped thickness and acceptable for the other scenario the difference in sdrs appears in spite of using finer grids for the fds or more nodal points for the present solution such a discrepancy may be attributed to two problems one is associated with the finite difference approximation which involves truncation error and its grids not perfectly matching the confining beds the other problem may arise from the use of the finite number of the source points accounting for the boundary effect 3 4 case 4 effect of multiple well pumping on sdr only few attempts have been made to observe the behaviors of sdr and aquifer drawdown induced by multiple well pumping near a shallow stream consider the same stream aquifer system in case 2 but the stream domain is moved to 1 x 0 2 5 y 2 5 and z 0 the nodal points and source points are also allocated with l 0 5 and d 0 2 the number of the source points is 1976 the coordinates of the source points and nodal points are given in the supplementary material the derivation of q 1 q 2 q 1976 for the source points is presented in appendix b there are four scenarios for pumping well installation scenario 1 is for a single well located at x k y k 1 0 with the dimensionless discharge rate q 1 1 scenarios 2 4 all have two wells w1 and w2 the wells in both scenarios 2 and 3 are located at 1 0 and 2 0 the rates of the wells are identical i e q 1 q 2 0 5 in scenario 2 but different i e q 1 0 4 q 2 0 6 in scenario 3 scenario 4 is for the two wells at 1 8 0 and 1 2 0 with q 1 q 2 0 5 thus the dimensionless distance between the stream and each well is all unity in scenarios 1 3 however the distances for the wells in scenario 4 are different one is 0 2 i e close to the stream and the other is 1 8 i e far away from the stream fig 5 demonstrates the temporal sdr distribution and steady state dimensionless drawdown s i e t 10 3 z 0 predicted by the present solution equation 16 for the four scenarios scenarios 1 3 have the same sdr prediction curve indicating that the stream filtration water induced by the wells in scenarios 2 and 3 equals that by the single well in scenario 1 scenario 4 however produces a larger sdr than the other scenarios before the flow reaches the steady state at t 10 2 such results can be attributed to the fact that the sdr is linearly related to the well discharge rates but not to the stream well distance as indicated in equations 12a 12d and 16 scenario 4 with the near stream w2 well induces the most stream filtration water although the sum of the well discharge rates in scenario 4 is the same as those in scenarios 1 3 the identical sdr in scenarios 1 3 is due to the same stream well distance and total well discharge rate panels a d exhibit the largest dimensionless drawdown s is 10 4 0 41 m in scenario 1 5 4 at w1 or w2 in scenario 2 6 4 at w2 in scenario 3 and 7 5 at w1 in scenario 4 the dimensionless drawdown in scenario 1 is larger than those in the other scenarios indicating that a single well with a larger discharge rate produces a larger drawdown at the well the w1 well of scenario 4 is far away from the stream and induces less stream filtration water than the other wells of scenarios 2 and 4 consequently the w1 well of scenario 4 induces a larger drawdown than the other wells of scenarios 2 and 4 with the same total well discharge rate finally this case study demonstrates that the present solution can easily handle the problems of multiple well pumping in predicting accurate sdr and drawdown for uniform thickness or nonuniform thickness confined aquifers a fds however relies on inconvenient grid development with orthogonal meshes for the multiple well pumping problems 4 conclusions a new analytical and meshless model is developed for simulating 3d transient flow induced by well pumping in a nonuniform thickness confined aquifer adjacent to a stream the stream fully penetrates the aquifer or is shallow with the clogged streambed specified under the robin condition the spatial distribution of the confining beds can arbitrarily vary according to the principle of superposition the drawdown solution is in terms of the theis solution plus a simple series arising from source points accounting for the aquifer boundary effect each term of the series is defined as a complementary error function describing transient flow caused by a source point the solution of sdr can then be developed according to darcy s law in addition a fds is built for justifying the present solution the results indicate that the present solution gives close predictions with minor deviation to those predicted by the fds indicating the present solution is correctly developed based on the four case studies the conclusions can be drawn as follows 1 the present solution gives good predictions of sdr and aquifer drawdown when d l 0 9 and d 0 2 representing the quantitative criteria for allocating source points beside a cuboid aquifer connected with a fully penetrating stream 2 the present solution outperforms the fds in computational efficiency for a typical confined aquifer connected to a shallow stream significant deviation in sdr prediction appears when the stream is treated as a fully penetrating stream notice that this result is consistent with the finding of xiong et al 2021 3 the sdr is affected by the aquifer thickness exponentially decreasing or linearly increasing landward significant underestimation of sdr appears when the exponentially decreasing thickness is treated as a uniform but thicker thickness equaling the largest portion of the former thickness in contrast the linearly increasing thickness of the aquifer leads to a less transient sdr than a uniform but thinner thickness equaling the thinnest thickness of that aquifer in addition the hantush 1962c solution is applicable in estimating sdr subject to the effect of the exponentially decreasing thickness although adopting the dupuit assumption of no vertical flow 4 the present solution serves as a simple and efficient alternative to the fds in simulating 3d transient flow toward multiple wells in a uniform thickness or nonuniform thickness confined aquifer connected with a shallow stream or fully penetrating stream the fds however always confronts inconvenient grid development of using orthogonal meshes to handle the problems of multiple wells with inverse problem algorithms the present solution is applicable in identifying optimal installation of multiple wells for the near stream groundwater extraction problems e g gaur et al 2013 gaur et al 2018 5 the sdr induced by multiple wells is the same as that by a single well if the total well discharge rate and stream well distances in the multiple well cases are identical to those in the single well case contrarily the sdrs in both cases differ when one or more of the stream well distances in the multiple well case is not equal to the distance in the single well case software availability the program used in this article is written using the mathematica outlined in the methodology section and publicly available in the supplementary material declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements research leading to this paper has been supported by the national natural science foundation of china grant no 52079042 41830752 appendix c supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 multimedia component 6 multimedia component 6 multimedia component 7 multimedia component 7 multimedia component 8 multimedia component 8 multimedia component 9 multimedia component 9 multimedia component 10 multimedia component 10 multimedia component 11 multimedia component 11 multimedia component 12 multimedia component 12 multimedia component 13 multimedia component 13 multimedia component 14 multimedia component 14 multimedia component 15 multimedia component 15 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105564 appendix a derivation of equation 12a according to the principle of superposition equation 7 can be expressed in terms of two partial differential equations as a1 2 s x 2 2 s y 2 2 s z 2 s t k 1 m 4 π κ d k δ x x k δ y y k a2 2 s x 2 2 s y 2 2 s z 2 s t j 1 n 4 π q j κ δ x x j δ y y j δ z z j therefore the solution of equation 7 equals the sum of the solutions of equations a1 and a2 consider the remote boundary condition written as a3 lim ω s 0 where ω x y z the second term on the rhs of equation a1 indicates line sinks extending infinitely from z to z and provoking only horizontal flow by eliminating 2 s z 2 equation a1 becomes a4 2 s x 2 2 s y 2 s t k 1 m 4 π κ d k δ x x k δ y y k the solution satisfying equation 8 a3 and a4 can be written as the theis 1935 solution for n 1 and as the first term on the rhs of equation 12a for n 1 on the other hand equation a2 with n 1 for flow induced by a single source point can be denoted as carslaw and jaeger 1959 p 261 a5 1 r j 2 r j r j 2 s r j s t with a6 lim r j 0 r j 2 s r j q j κ note that equation a6 is a boundary condition describing flow across the surface of a sphere of infinitesimal radius the solution satisfying equation 8 a3 a5 and a6 can be written as carslaw and jaeger 1959 p 256 a7 s q j κ r j e r f c r j 2 t the solution of equation a2 for multiple source points n 1 is the second term on the rhs of equation 12a based on the principle of superposition appendix b determination of q j in equation 12a the rates of q 1 q 2 q n for source points can be obtained by solving a system of equations in which each equation is derived by substituting equation 12a into the boundary condition specified at a nodal point for case 1 the robin condition represented by equation 10 is approximated as b1 s s δ n α s s 0 a t e a c h n o d a l p o i n t o n ω s with ω s being x 1 for the streambed plane and s being the dimensionless aquifer drawdown at x i y i z i normal to the plane and separated by a dimensionless distance δ n 10 3 substituting equation 12a into equation b1 yields b2a k 1 m w u i k κ d 1 j 1 n q j κ r j i e r f c r j i 2 t k 1 m w u i k κ d 1 j 1 n q j κ r j i e r f c r j i 2 t δ n α k 1 m w u i k κ d 1 j 1 n q j κ r j i e r f c r j i 2 t s 0 a t e a c h n o d a l p o i n t o n ω s b2b u i k x i x k 2 y i y k 2 t b2c u i k x i x k 2 y i y k 2 t b2d r j i x i x j 2 y i y j 2 z i z j 2 b2e r j i x i x j 2 y i y j 2 z i z j 2 where d 1 1 and n 1976 used in case 1 rearranging equation b2a leads to b3 j 1 n q j 1 κ r j i e r f c r j i 2 t 1 δ n α 1 κ r j i e r f c r j i 2 t k 1 m 1 δ n α w u i k κ d 1 w u i k κ d 1 δ n α s 0 a t e a c h n o d a l p o i n t o n ω s with α 0 for reducing equation b1 to the no flow condition equation b3 becomes b4 j 1 n q j 1 κ r j i e r f c r j i 2 t 1 κ r j i e r f c r j i 2 t k 1 m w u i k κ d 1 w u i k κ d 1 a t e a c h n o d a l p o i n t o n ω z z u a n d z z l which equals the discretization result of substituting equation 12a into equation 9 or 11 the values of q 1 q 2 q 1976 can be determined by solving the system of equations including equation b3 specified at the nodal points on the streambed plane of x 1 and equation b4 at the other nodal points it is worth noting that a nodal point e g p4 fig 2 at the corner of the aquifer domain provides one equation b3 for the streambed plane and two equations b4 for two no flow planes e g y 2 5 and z 0 similarly a nodal point e g p3 at the corner of the three no flow planes gives three equations b4 and a nodal point e g p2 at the edge provides two equations b4 in addition a nodal point e g p5 at the edge of the aquifer domain gives one equation b3 for the streambed plane and one equation b4 for the no flow plane for case 2 the values of q 1 q 2 q 1976 can also be obtained by solving the system of equations including equation b3 specified at the nodal points on the streambed plane and equation b4 at the other nodal points on the no flow planes notice that each nodal point e g p1 fig 3 at the corners of the aquifer domain provides three equations b4 for three no flow planes e g x 3 y 2 5 and z 0 a nodal point e g p2 at line p 3 p 4 gives one equation b3 for the streambed plane and one equation b4 for the lateral no flow plane e g y 2 5 a nodal point e g p5 at the edge of the aquifer domain provides two equations b4 based on two no flow planes e g z 0 and y 2 5 for case 3 the values of q 1 q 2 q 4482 for the exponentially varying thickness scenario are acquired by solving the system of equations containing equation b3 specified at the nodal points on the streambed plane of x 1 and equation b4 at the other nodal points again a nodal point e g p3 fig 4 at the corner of the aquifer domain provides one equation b3 for the streambed plane and two equations b4 for the no flow plane of y 2 5 and the upper confining bed at z z u a nodal point e g p4 defines three equations b4 for the no flow condition similarly a nodal point e g p5 at the edge of the aquifer domain gives one equation b3 for the streambed plane and one equation b4 for the upper confining bed moreover a nodal point e g p2 requires two equations b4 for the no flow plane of y 2 5 and the upper confining bed on the other hand the magnitudes of q 1 q 2 q 9897 for the wedge shaped thickness scenario can also be acquired by a similar procedure to the other scenario similar to case 2 the values of q 1 q 2 q 1976 for case 4 can be derived by solving the system of equations containing equation b3 specified at the nodal points on the streambed domain and equation b4 at the other nodal points the number of wells defined as m in equations b3 b4 equals 1 for scenario 1 in fig 5 and 2 for the other scenarios 
25485,this study proposes an analytical and meshless model for three dimensional transient flow induced by multiple wells in a stream confined aquifer system with arbitrary nonuniform aquifer thickness the model involves allocating external image wells near the aquifer boundary the aquifer drawdown is defined as theis solution plus a simple series expanded by complementary error functions reflecting the boundary effects of the stream and impervious stratum the solution for stream depletion rate sdr can be derived based on the drawdown solution results suggest the dupuit assumption of no vertical flow is valid to estimate sdrs affected by the landward exponentially decreasing thickness the sdr induced by multiple wells is the same as that by a single well if the total well discharge rate and stream well distances in the multiple well case are identical to those in the single well case the present solution outperforms a finite difference solution in computational efficiency keywords analytical approach meshless method nonuniform aquifer thickness stream aquifer interaction robin boundary condition multiple wells data availability no data was used for the research described in the article 1 introduction in recent years there has been renewal of interest in dealing with the problem of stream aquifer interaction e g ou et al 2013 huang and yeh 2015 dogrul et al 2016 colin et al 2019 antonio et al 2020 huang et al 2020 groundwater extraction by wells induces stream water filtration into the adjacent aquifer and causes impacts on ecosystems with significant decrease in the stream stage the stream depletion rate sdr defined as the ratio of the stream water filtration rate to the well discharge rate is a widely used indicator huang et al 2018 provided an extensive review on existing analytical sdr solutions and categorized the solutions according to aquifer types i e confined unconfined and leaky aquifers flow dimensions i e two three and quasi three dimensions well types i e vertical horizontal and radial collector wells and stream representations i e source terms and the dirichlet and robin conditions they summarized the solutions were developed based on the assumption of a stream fully penetrating an aquifer called fully penetrating stream this assumption however causes 10 61 errors in predicting the sdr for a stream with partial penetration sophocleous et al 1995 since then xiong et al 2021 derived a laplace domain solution of sdr for a rectilinear stream shallowly penetrating a confined aquifer and found sdr predictions significantly depending on the aquifer storage below that stream regarding the problems of nonuniform aquifer thickness hantush 1962a revealed a confined aquifer of thickness linearly varying cannot be treated as a uniform thickness aquifer in simulating the flow during the entire period except at very early times this indicates the importance in dealing with spatially varying aquifer thickness in predicting sdr in reality little attention was paid to assess sdr subject to the effects of nonuniform aquifer thickness and shallow streams in the past the techniques to handle the problem of nonuniform flow domain may be classified as the analytical approach e g hantush 1962a 1962b 1962c analytic element method e g strack 2003 steward 2015 meshless method e g wu et al 2008 and numerical method such as finite difference scheme the analytical approach based on two dimensional 2d transient flow equation is applicable only under the condition of wedge shaped aquifer thickness hantush 1962a or exponentially varying thickness hantush 1962b the analytic element method with a series solution in ameli and craig 2014 applies to spatially dependent aquifer thickness while the solution satisfying the laplace equation of discharge potential is limited to three dimensional 3d steady state flow the numerical method usually takes much effort to build spatial discretization of irregular aquifer domain and involves laborious calculations for drawdown solutions at gird points the meshless method has increasingly applied to various problems in hydrogeological area such as irregular aquifer boundary e g kuo et al 1994 wave propagation e g wu et al 2008 non darcian flow e g wen et al 2014 stream aquifer interaction e g huang and yeh 2015 and aquifer heterogeneity e g huang and yeh 2016 the meshless flow domain is achieved by applying characteristic kernel functions such as the theis well function kuo et al 1994 huang and yeh 2015 2016 logarithmic function wu et al 2008 and multi quadrics basis function as well as gaussian basis function wen et al 2014 its advantages are good efficiency and accuracy over the numerical method wen et al 2014 huang and yeh 2016 wu et al 2008 developed a meshless method for solving the problem of 2d flow below a dynamic free surface what seems to be lacking however is a meshless method for solving the problem of 3d transient flow in aquifers of spatially varying thickness previously huang and yeh 2016 presented an analytical and meshless approach for 2d transient flow in a heterogeneous confined aquifer by assuming fully penetrating streams and uniform aquifer thickness they mainly handled the problem of the aquifer having a parameter zonation structure composed of five zones with different hydraulic parameters but neglected the issue of stream aquifer interaction their work inspires us to develop a new meshless model for simulating 3d transient flow in a nonuniform thickness confined aquifer connected with a shallow stream with the clogged streambed specified as the robin condition image wells hereinafter called source points are allocated outside and near the aquifer boundary for hydraulically replacing the boundary effect the drawdown solution is expressed in terms of the theis 1935 solution plus a simple series in which each term is defined as a complementary error function accounting for radial transient flow toward a source point the discharge rate for each source point is solved from a system of equations obtained by substituting the drawdown solution into the aquifer boundary conditions the sdr solution is then derived from darcy s law along with the drawdown solution a finite difference solution fds is also developed to examine the present solution a criterion for applying the present solution to various configurations of confining beds is provided temporal sdr distributions affected by nonuniform aquifer thickness are investigated in addition the effect of multiple pumping wells on sdr and aquifer drawdown is discussed the notations and abbreviations used in the text are shown in table 1 2 methodology 2 1 mathematical model fig 1 displays a pumped confined aquifer having spatially varying thickness and connecting a shallow stream with a clogged streambed in between consider the cartesian coordinates x y z the shortest distance or a selected distance between the stream and pumping well is denoted as letter a hantush 1962b defined the elevations of the lower and upper confining beds in terms of arbitrary functions as z l f 1 x y and z u f 2 x y respectively in the present meshless model the aquifer boundary is substituted by nodal points the first nodal point is allocated arbitrarily at the boundary e g the intersection of the streambed and upper confining bed the other nodal points are then successively allocated with a distance d along the boundary the source points are designated beside the nodal points for hydraulically replacing the boundary effect the distance between a source point to its nearest nodal point is l and the direction of the line across them should be normal to the boundary the governing equation of the model for 3d transient flow induced by multiple extraction wells and source points can be expressed as 1 k h 2 s x 2 k h 2 s y 2 k v 2 s z 2 s s s t k 1 m q k d k δ x x k δ y y k j 1 n q j δ x x j δ y y j δ z z j where s is aquifer drawdown t is time k h and k v are respectively the horizontal and vertical hydraulic conductivities s s is the specific storage δ is the dirac delta function the subscripts j and k respectively represent the j th source point at the location x j y j z j and k th well at x k y k d k is aquifer thickness at the k th well q k is a discharge rate of the k th well m is the number of the wells q j is a discharge rate of the j th source point if q j 0 and an injection rate if q j 0 and n is the number of the source points the potentiometric surface before pumping is at the same elevation as stream stage the initial condition is thus written as 2 s 0 a t t 0 the confining beds are under the no flow condition that 3 s n 0 a t z z u a n d z z l where n is the perpendicular direction the clogged streambed can be regarded as the robin boundary condition denoted as 4 k n s n k b s s 0 0 a t ω s where k and b are the streambed hydraulic conductivity and thickness respectively ω s is the streambed domain k n is the aquifer hydraulic conductivity normal to the streambed k n k h for a fully penetrating stream k n k v for a shallow stream s 0 is a vertical distance between the stream stage and potentiometric surface s 0 0 for the stage below the surface and s 0 0 for the opposite the sdr for narrow aquifers between a stream and an impervious stratum significantly increases at late pumping time due to that stratum miller et al 2007 the lateral aquifer boundary is therefore considered under the no flow condition expressed as 5 s n 0 a t ω note that the aquifer boundary expressed by z u z l ω s and ω can be arbitrarily distributed in space define the following dimensionless variables and parameters s 4 π k h a q s s 0 4 π k h a q s 0 x x a y y a z 1 κ z a x k x k a y k y k a q k q k q q j q j q 6 x j x j a y j y j a z j 1 κ z j a d k 1 κ d k a z u 1 κ z u a z l 1 κ z l a t k h t s s a 2 α k a k n b where κ k v k h q q 1 q 2 q m and the overbar indicates a dimensionless symbol with equation 6 equations 1 5 can be written respectively as 7 2 s x 2 2 s y 2 2 s z 2 s t k 1 m 4 π κ d k δ x x k δ y y k j 1 n 4 π q j κ δ x x j δ y y j δ z z j 8 s 0 a t t 0 9 s n 0 a t z u a n d z l 10 s n α s s 0 0 a t ω s 11 s n 0 a t ω with ω s and ω being the dimensionless domains 2 2 solution of the present model based on the principle of superposition the drawdown solution can be written as 12a s k 1 m q k κ d k w u k j 1 n q j κ r j e r f c r j 2 t 12b w u u exp v v d v 12c u k x x k 2 y y k 2 4 t 12d r j x x j 2 y y j 2 z z j 2 where w u k is the exponential integral also called theis well function and e r f c is the complementary error function the first term on the right hand side rhs of equation 12a describes the dimensionless drawdown due to extraction wells and the second term accounts for the effect of the boundary conditions equations 9 11 on the drawdown one can refer to appendix a for the derivation of equation 12a according to darcy s law sdr induced by well pumping can be estimated by integrating the flux across the streambed per unit area d a divided by the sum of the discharge rates i e q 1 q m denoted in the dimensional form as 13 s d r k q b ω s s s 0 d a a t ω s in the dimensionless form it becomes 14 s d r α 4 π ω s s s 0 d a a t ω s where s is the present solution equation 12a and d a is dimensionless unit area the implementation of the present solution involves following two steps 1 the aquifer boundary is replaced by the nodal points and the dimensionless distance defined as d between two neighboring points is at most 0 2 huang and yeh 2016 with the nodal points the locations of the source points can be defined with the dimensionless distance defined as l between a source point to tis nearest nodal point 2 the rates of q j at source points are determined by solving a system of equations based on the present solution equation 12a and boundary conditions equations 9 11 four cases of stream aquifer systems given in section 3 demonstrate the allocation of nodal and source points and the determination of the values of q 1 q 2 q n the source point far away from extraction well is ignorable to facilitate the computing efficiency of estimating q j at the early pumping period define a dimensionless radius of influence from a source point to a location at which the dimensionless drawdown s q j i e 4 π k h a s q j is 10 6 the radius is defined as r j 20 t by applying newton s method to solve s q j 10 6 with equation 12a m 0 and n 1 it can therefore be concluded that q j can be set to zero for nodal points located outside the radius i e r j 20 t in order to reduce the number of unknowns q j 2 3 finite difference solution an implicit fds is developed by employing the mathematica function nsolve to make comparison with the present solution the numerical model contains the present model equations 7 11 but excludes the term j 1 n 4 π q j κ δ x x j δ y y j δ z z j in equation 7 consider a uniform grid for the aquifer domain with the dimensionless grid size of 0 05 in the x y or z direction the dimensionless drawdowns at the grid nodes can be obtained by solving the numerical model the sdr is then approximated as 15 s d r α 4 π n 1 n a δ a s n s 0 a t ω s where s n is the n th dimensionless drawdown at the grid nodes on the streambed domain δ a 0 05 2 is a dimensionless unit area and n a is the number of the nodes 3 results and discussion four cases are chosen to demonstrate the application of the present solution case 1 is for a cuboid aquifer with a fully penetrating stream discussed in section 3 1 case 2 is for a cuboid aquifer with a shallow stream presented in section 3 2 while case 3 is for aquifers of nonuniform thicknesses given in section 3 3 case 4 is for a cuboid aquifer with multiple pumping wells beside a shallow stream demonstrated in section 3 4 the allocation of nodal points and source points for good predictions of drawdown and sdr is discussed in section 3 1 the default values are q 1 1500 m 3 d a 50 m d 1 27 38 m k h 60 m d k v 18 m d s s 10 4 m 1 d 0 2 κ 0 3 α 20 s 0 0 m 1 q q 1 d k d 1 and x k y k 0 0 3 1 case 1 cuboid aquifer in case 1 a cuboid aquifer is considered with one plane specified under the robin condition for a fully penetrating stream and the others specified under the no flow condition for impervious strata as illustrated in fig 2 this case is to examine the allocation of nodal points and source points by applying both the present solution and analytical solution of huang et al s 2014 the stream aquifer system is in the domain bounded by 1 x 4 50 m x 200 m 2 5 y 2 5 125 m y 125 m and 1 z 0 27 38 m z 0 m orthogonal and uniform nodal points on the six planes of the aquifer domain are shown in fig 2 the dimensionless coordinates of the i th nodal point are x i y i z i the dimensionless distance d between two adjacent nodal points is 0 2 source points beside the six planes are then arranged one source point is allocated near a nodal point e g p1 on each plane two source points near a nodal point e g p2 at each edge and three source points near a nodal point e g p3 at each corner the total number of the source points is therefore 1 0 2 1 5 0 2 1 4 5 0 2 1 2 2 1976 the dimensionless coordinates of x j y j z j for the source points and x i y i z i for the nodal points are provided in the supplementary material the determination of q 1 q 2 q 1976 for the source points is explained in appendix b then equation 14 can be approximated as 16 s d r α 4 π i 1 n s c d 2 s i s 0 where s i is the drawdown defined as equation 12a with replacing x y z by x i y i z i being the locations of the nodal points on the streambed plane n s 156 is the number of the nodal points and c is equal to 0 25 for the nodal points at the four corners 0 5 for the nodal points at the four sides and 1 for the nodal points inside it is of practical interest to explore the source point location represented by the dimensionless distance l between a source point and its nearest nodal point the temporal sdr distributions predicted by the present solution equation 16 with l 0 1 0 2 0 5 0 8 and 0 9 are compared with those predicted by the huang et al 2014 analytical solution for the same stream aquifer system for l 0 2 0 5 and 0 8 both solutions give close sdr predictions over the entire pumping period on the other hand the present solution significantly overestimates sdr for l 0 1 when t 2 t 12 min and gives an incorrect prediction of sdr 1 not shown when t 20 the reason for the error is due to the use of l d i e 0 1 0 2 in this case reported in wu et al 2008 for l 0 9 the sdr curve predicted by the present solution slightly fluctuates in the period of 0 1 t 1 and at t 100 for l 1 the temporal sdr distribution fluctuates dramatically after a certain t depending on the magnitude of l not shown it can be concluded that the present solution gives correct predictions when d l 0 9 and d 0 2 3 2 case 2 aquifer connected with a shallow stream in case 2 the present solution is applied to an aquifer connected with a shallow stream illustrated in fig 3 in which the stream depth is largely less than the aquifer thickness consider the aquifer in the cuboid domain bounded by 3 x 2 2 5 y 2 5 and 1 z 0 with all impervious boundaries except the one adjacent to the stream which situates in the area of 2 x 1 2 5 y 2 5 and z 0 the allocation of the nodal points and source points is the same as case 1 i e l 0 5 d 0 2 because this case has the identical size of the aquifer domain note that the nodal points and source points within the range 1 x 4 in the previous case should be changed to 3 x 2 in this case the total number of the source points is also 1976 the locations of the source points and nodal points are defined in the supplementary material the derivation of q 1 q 2 q 1976 for the source points is given in appendix b fig 3 displays time varying sdr curves plotted by the present solution i e equation 16 and fds for the aquifer with the shallow stream or a fully penetrating stream both solutions agree well to the sdr during t 10 or t 100 and the difference in the period 10 t 100 is small and acceptable there is a discrepancy in the sdrs between the streams indicating that a shallow stream cannot be considered as a fully penetrating one in estimating sdr e g sophocleous et al 1995 reeves et al 2009 xiong et al 2021 the present solution takes about 1 12 of the total computing time that the fds takes to predict the sdrs at t δ t 2 δ t 10 3 with a small dimensionless time step δ t this is because the present solution can estimate the sdr or aquifer drawdown directly at a specific dimensionless time t rather than starting from t δ t 2 δ t to that specific time it can be seen from this case that the present solution outperforms the fds in computational efficiency 3 3 case 3 aquifer of nonuniform thickness this case demonstrates two scenarios for one aquifer with exponentially varying thickness and the other one with wedge shaped thickness fig 4 a illustrates the former scenario in the domain of 1 x 14 and 2 5 y 2 5 between the confining beds located at hantush 1962c 17a z u 0 17b z l c 2 exp 2 x c 1 where c 1 10 and c 2 exp 0 2 are determined when the largest dimensionless aquifer thickness at x 1 is unity and the dimensionless distance between the stream and well is also unity shown in fig 4a note that the use of horizontally rectangular domain in this case is to simplify the problem and emphasize the capability of the present solution in handling the space varying thickness consider finite nodal points on the aquifer boundary displayed in fig 4a in the x direction the dimensionless distance between two neighboring nodal points on the lower confining bed is 0 2 the location of these nodal points starts from x 1 1 x 2 0 804 and x 76 13 948 obtained by x m 1 0 2 1 d z l d x 2 with d z l d x 2 c 2 exp 2 x m c 1 c 1 based on equation 17b the last x 77 is set to 14 the elevations of the nodal points on the upper confining bed can be defined by equation 17a with x x 1 x 2 x 77 in the y direction the location of the nodal points starts from y 2 5 to 2 5 with increment of d 0 2 in the z direction the elevation of the nodal points begins from the lower confining bed with d 0 2 the source points are then allocated beside the aquifer boundary shown in fig 4a again one source point is assigned to each nodal point e g p1 on the lateral planes and confining beds two source points to a nodal point e g p2 at each edge of the aquifer domain and three source points to a nodal point e g p3 at each corner a vector from a source point at x j y j z j to the nearest nodal point at x i y i z i should be normal to the aquifer boundary the dimensionless distance l between them is 0 2 then the location of a source point beside the confining beds can be defined as 18 x j x i l d z d x 2 d z 2 y j y i z j z i l d x d x 2 d z 2 where d x x i 1 x i d z z i 1 z i and x i 1 z i 1 counterclockwise next to x i z i notice that the location of a source point e g s1 or s2 beside a nodal point e g p3 or p4 at the corner of the aquifer domain satisfies 19 x j x i l d z d x 2 d z 2 y j y i z j z i l d x d x 2 d z 2 where d z z i z i 1 d x x i x i 1 and x i 1 z i 1 clockwise next to x i z i the total number of the source points is 4482 similarly the scenario for an aquifer with the wedge shaped thickness exhibited in fig 4b has the same procedure of allocating nodal points and source points but equation 17b is replaced by z l 2 x 1 5 1 in the aquifer domain of 1 x 4 the total number of the source points in this scenario is 9897 one can refer to the supplementary material for the locations of the nodal points and source points and appendix b for the determination of q 1 q n the effect of the nonuniform aquifer thicknesses on sdr is examined herein fig 4 shows the temporal sdr distributions predicted by the present solution equation 16 for the scenarios of the aquifers with the exponentially varying thickness and wedge shaped thickness the hantush 1962c solution was developed for the former scenario while the hunt 1999 solution was for the uniform thickness scenario the fds is applied to assess the present solution note that both the hantush and hunt solutions adopted the dupuit assumption of no vertical flow indicating a fully penetrating stream is considered sun and zhan 2007 the present solution removing the assumption by considering 3d flow agrees well with the hantush solution this indicates an insignificant influence of vertical flow in the aquifer of the exponentially decreasing thickness defined by equations 17a and 17b with c 1 10 or c 1 10 for thinner aquifers the vertical flow influence becomes minor when 10 c 1 400 and the difference in sdr between our and hantush solutions increases not shown the influence becomes again insignificant when c 1 400 and the hantush and hunt solutions give close sdrs with insignificant difference indicating that the influence of the nonuniform thickness on sdr is minor the sdr in the scenario of wedge shaped thickness is less than that of the uniform one during the period of 0 1 t 20 but a more sdr after that period due to the boundary effect at x 4 e g miller et al 2007 huang et al 2014 this can be attributed to the fact that the wedge shaped thickness is thicker and thus provides more groundwater to the pumping well at a given time in addition the difference in sdrs calculated by the fds and present solution is small for the scenario of wedge shaped thickness and acceptable for the other scenario the difference in sdrs appears in spite of using finer grids for the fds or more nodal points for the present solution such a discrepancy may be attributed to two problems one is associated with the finite difference approximation which involves truncation error and its grids not perfectly matching the confining beds the other problem may arise from the use of the finite number of the source points accounting for the boundary effect 3 4 case 4 effect of multiple well pumping on sdr only few attempts have been made to observe the behaviors of sdr and aquifer drawdown induced by multiple well pumping near a shallow stream consider the same stream aquifer system in case 2 but the stream domain is moved to 1 x 0 2 5 y 2 5 and z 0 the nodal points and source points are also allocated with l 0 5 and d 0 2 the number of the source points is 1976 the coordinates of the source points and nodal points are given in the supplementary material the derivation of q 1 q 2 q 1976 for the source points is presented in appendix b there are four scenarios for pumping well installation scenario 1 is for a single well located at x k y k 1 0 with the dimensionless discharge rate q 1 1 scenarios 2 4 all have two wells w1 and w2 the wells in both scenarios 2 and 3 are located at 1 0 and 2 0 the rates of the wells are identical i e q 1 q 2 0 5 in scenario 2 but different i e q 1 0 4 q 2 0 6 in scenario 3 scenario 4 is for the two wells at 1 8 0 and 1 2 0 with q 1 q 2 0 5 thus the dimensionless distance between the stream and each well is all unity in scenarios 1 3 however the distances for the wells in scenario 4 are different one is 0 2 i e close to the stream and the other is 1 8 i e far away from the stream fig 5 demonstrates the temporal sdr distribution and steady state dimensionless drawdown s i e t 10 3 z 0 predicted by the present solution equation 16 for the four scenarios scenarios 1 3 have the same sdr prediction curve indicating that the stream filtration water induced by the wells in scenarios 2 and 3 equals that by the single well in scenario 1 scenario 4 however produces a larger sdr than the other scenarios before the flow reaches the steady state at t 10 2 such results can be attributed to the fact that the sdr is linearly related to the well discharge rates but not to the stream well distance as indicated in equations 12a 12d and 16 scenario 4 with the near stream w2 well induces the most stream filtration water although the sum of the well discharge rates in scenario 4 is the same as those in scenarios 1 3 the identical sdr in scenarios 1 3 is due to the same stream well distance and total well discharge rate panels a d exhibit the largest dimensionless drawdown s is 10 4 0 41 m in scenario 1 5 4 at w1 or w2 in scenario 2 6 4 at w2 in scenario 3 and 7 5 at w1 in scenario 4 the dimensionless drawdown in scenario 1 is larger than those in the other scenarios indicating that a single well with a larger discharge rate produces a larger drawdown at the well the w1 well of scenario 4 is far away from the stream and induces less stream filtration water than the other wells of scenarios 2 and 4 consequently the w1 well of scenario 4 induces a larger drawdown than the other wells of scenarios 2 and 4 with the same total well discharge rate finally this case study demonstrates that the present solution can easily handle the problems of multiple well pumping in predicting accurate sdr and drawdown for uniform thickness or nonuniform thickness confined aquifers a fds however relies on inconvenient grid development with orthogonal meshes for the multiple well pumping problems 4 conclusions a new analytical and meshless model is developed for simulating 3d transient flow induced by well pumping in a nonuniform thickness confined aquifer adjacent to a stream the stream fully penetrates the aquifer or is shallow with the clogged streambed specified under the robin condition the spatial distribution of the confining beds can arbitrarily vary according to the principle of superposition the drawdown solution is in terms of the theis solution plus a simple series arising from source points accounting for the aquifer boundary effect each term of the series is defined as a complementary error function describing transient flow caused by a source point the solution of sdr can then be developed according to darcy s law in addition a fds is built for justifying the present solution the results indicate that the present solution gives close predictions with minor deviation to those predicted by the fds indicating the present solution is correctly developed based on the four case studies the conclusions can be drawn as follows 1 the present solution gives good predictions of sdr and aquifer drawdown when d l 0 9 and d 0 2 representing the quantitative criteria for allocating source points beside a cuboid aquifer connected with a fully penetrating stream 2 the present solution outperforms the fds in computational efficiency for a typical confined aquifer connected to a shallow stream significant deviation in sdr prediction appears when the stream is treated as a fully penetrating stream notice that this result is consistent with the finding of xiong et al 2021 3 the sdr is affected by the aquifer thickness exponentially decreasing or linearly increasing landward significant underestimation of sdr appears when the exponentially decreasing thickness is treated as a uniform but thicker thickness equaling the largest portion of the former thickness in contrast the linearly increasing thickness of the aquifer leads to a less transient sdr than a uniform but thinner thickness equaling the thinnest thickness of that aquifer in addition the hantush 1962c solution is applicable in estimating sdr subject to the effect of the exponentially decreasing thickness although adopting the dupuit assumption of no vertical flow 4 the present solution serves as a simple and efficient alternative to the fds in simulating 3d transient flow toward multiple wells in a uniform thickness or nonuniform thickness confined aquifer connected with a shallow stream or fully penetrating stream the fds however always confronts inconvenient grid development of using orthogonal meshes to handle the problems of multiple wells with inverse problem algorithms the present solution is applicable in identifying optimal installation of multiple wells for the near stream groundwater extraction problems e g gaur et al 2013 gaur et al 2018 5 the sdr induced by multiple wells is the same as that by a single well if the total well discharge rate and stream well distances in the multiple well cases are identical to those in the single well case contrarily the sdrs in both cases differ when one or more of the stream well distances in the multiple well case is not equal to the distance in the single well case software availability the program used in this article is written using the mathematica outlined in the methodology section and publicly available in the supplementary material declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements research leading to this paper has been supported by the national natural science foundation of china grant no 52079042 41830752 appendix c supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 multimedia component 6 multimedia component 6 multimedia component 7 multimedia component 7 multimedia component 8 multimedia component 8 multimedia component 9 multimedia component 9 multimedia component 10 multimedia component 10 multimedia component 11 multimedia component 11 multimedia component 12 multimedia component 12 multimedia component 13 multimedia component 13 multimedia component 14 multimedia component 14 multimedia component 15 multimedia component 15 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105564 appendix a derivation of equation 12a according to the principle of superposition equation 7 can be expressed in terms of two partial differential equations as a1 2 s x 2 2 s y 2 2 s z 2 s t k 1 m 4 π κ d k δ x x k δ y y k a2 2 s x 2 2 s y 2 2 s z 2 s t j 1 n 4 π q j κ δ x x j δ y y j δ z z j therefore the solution of equation 7 equals the sum of the solutions of equations a1 and a2 consider the remote boundary condition written as a3 lim ω s 0 where ω x y z the second term on the rhs of equation a1 indicates line sinks extending infinitely from z to z and provoking only horizontal flow by eliminating 2 s z 2 equation a1 becomes a4 2 s x 2 2 s y 2 s t k 1 m 4 π κ d k δ x x k δ y y k the solution satisfying equation 8 a3 and a4 can be written as the theis 1935 solution for n 1 and as the first term on the rhs of equation 12a for n 1 on the other hand equation a2 with n 1 for flow induced by a single source point can be denoted as carslaw and jaeger 1959 p 261 a5 1 r j 2 r j r j 2 s r j s t with a6 lim r j 0 r j 2 s r j q j κ note that equation a6 is a boundary condition describing flow across the surface of a sphere of infinitesimal radius the solution satisfying equation 8 a3 a5 and a6 can be written as carslaw and jaeger 1959 p 256 a7 s q j κ r j e r f c r j 2 t the solution of equation a2 for multiple source points n 1 is the second term on the rhs of equation 12a based on the principle of superposition appendix b determination of q j in equation 12a the rates of q 1 q 2 q n for source points can be obtained by solving a system of equations in which each equation is derived by substituting equation 12a into the boundary condition specified at a nodal point for case 1 the robin condition represented by equation 10 is approximated as b1 s s δ n α s s 0 a t e a c h n o d a l p o i n t o n ω s with ω s being x 1 for the streambed plane and s being the dimensionless aquifer drawdown at x i y i z i normal to the plane and separated by a dimensionless distance δ n 10 3 substituting equation 12a into equation b1 yields b2a k 1 m w u i k κ d 1 j 1 n q j κ r j i e r f c r j i 2 t k 1 m w u i k κ d 1 j 1 n q j κ r j i e r f c r j i 2 t δ n α k 1 m w u i k κ d 1 j 1 n q j κ r j i e r f c r j i 2 t s 0 a t e a c h n o d a l p o i n t o n ω s b2b u i k x i x k 2 y i y k 2 t b2c u i k x i x k 2 y i y k 2 t b2d r j i x i x j 2 y i y j 2 z i z j 2 b2e r j i x i x j 2 y i y j 2 z i z j 2 where d 1 1 and n 1976 used in case 1 rearranging equation b2a leads to b3 j 1 n q j 1 κ r j i e r f c r j i 2 t 1 δ n α 1 κ r j i e r f c r j i 2 t k 1 m 1 δ n α w u i k κ d 1 w u i k κ d 1 δ n α s 0 a t e a c h n o d a l p o i n t o n ω s with α 0 for reducing equation b1 to the no flow condition equation b3 becomes b4 j 1 n q j 1 κ r j i e r f c r j i 2 t 1 κ r j i e r f c r j i 2 t k 1 m w u i k κ d 1 w u i k κ d 1 a t e a c h n o d a l p o i n t o n ω z z u a n d z z l which equals the discretization result of substituting equation 12a into equation 9 or 11 the values of q 1 q 2 q 1976 can be determined by solving the system of equations including equation b3 specified at the nodal points on the streambed plane of x 1 and equation b4 at the other nodal points it is worth noting that a nodal point e g p4 fig 2 at the corner of the aquifer domain provides one equation b3 for the streambed plane and two equations b4 for two no flow planes e g y 2 5 and z 0 similarly a nodal point e g p3 at the corner of the three no flow planes gives three equations b4 and a nodal point e g p2 at the edge provides two equations b4 in addition a nodal point e g p5 at the edge of the aquifer domain gives one equation b3 for the streambed plane and one equation b4 for the no flow plane for case 2 the values of q 1 q 2 q 1976 can also be obtained by solving the system of equations including equation b3 specified at the nodal points on the streambed plane and equation b4 at the other nodal points on the no flow planes notice that each nodal point e g p1 fig 3 at the corners of the aquifer domain provides three equations b4 for three no flow planes e g x 3 y 2 5 and z 0 a nodal point e g p2 at line p 3 p 4 gives one equation b3 for the streambed plane and one equation b4 for the lateral no flow plane e g y 2 5 a nodal point e g p5 at the edge of the aquifer domain provides two equations b4 based on two no flow planes e g z 0 and y 2 5 for case 3 the values of q 1 q 2 q 4482 for the exponentially varying thickness scenario are acquired by solving the system of equations containing equation b3 specified at the nodal points on the streambed plane of x 1 and equation b4 at the other nodal points again a nodal point e g p3 fig 4 at the corner of the aquifer domain provides one equation b3 for the streambed plane and two equations b4 for the no flow plane of y 2 5 and the upper confining bed at z z u a nodal point e g p4 defines three equations b4 for the no flow condition similarly a nodal point e g p5 at the edge of the aquifer domain gives one equation b3 for the streambed plane and one equation b4 for the upper confining bed moreover a nodal point e g p2 requires two equations b4 for the no flow plane of y 2 5 and the upper confining bed on the other hand the magnitudes of q 1 q 2 q 9897 for the wedge shaped thickness scenario can also be acquired by a similar procedure to the other scenario similar to case 2 the values of q 1 q 2 q 1976 for case 4 can be derived by solving the system of equations containing equation b3 specified at the nodal points on the streambed domain and equation b4 at the other nodal points the number of wells defined as m in equations b3 b4 equals 1 for scenario 1 in fig 5 and 2 for the other scenarios 
25486,within the paris agreement s enhanced transparency framework consistent data collections are the prerequisite for a successful reporting of ghg emissions for such purposes nfis are usually the primary source of information even if they are frequently not designed for producing estimations on a yearly basis and in the form of wall to wall high resolution maps in this framework we present a new spatial model to produce yearly growing stock volume gsv above ground biomass agb and carbon stock wall to wall estimates we tested the model in italy for the period 2005 2018 obtaining a time series of yearly maps at 23 m spatial resolution results were validated against the 2015 italian nfi reaching an average rmse of 19 for aggregated areas results were also compared against data reported by the italian ghg inventory reaching an rmse of 28 and 20 for gsv and carbon stock respectively we demonstrated that the modeling approach can be successfully used for setting up a forest monitoring system to meet the interests of governments in inventories of ghg emissions and private entities in carbon offset investments keywords national forest inventory gsv carbon stock forest modeling spatial modeling italy data availability data will be made available on request 1 introduction under the enhanced transparency framework of the paris agreement each country party must report every two years an inventory of their anthropogenic greenhouse gases ghgs emissions by sources and removals by sinks following the intergovernmental panel on climate change ipcc guidelines and guidance ipcc et al 2006 the ghg emission inventory has to fulfill the ipcc key principles transparency accuracy completeness consistency and comparability while providing helpful information for assessing the climate impacts the land use land use change and forestry lulucf is exceptionally demanding dealing with natural carbon dynamics and aiming to assess emissions and removals related to the impact of anthropogenic activities the lulucf sector is responsible for significant ghg emissions globally mainly due to deforestation activities in this framework forests are pivotal ecosystems being a substantial and growing atmospheric carbon sink sellers et al 2018 forests are estimated to sequester 30 of the total global co2 released into the atmosphere annually houghton and nassikas 2017 corresponding to 7 6 gt co2 y 1 reflecting a balance between gross carbon removals and gross emissions from deforestation and other disturbances harris et al 202 xu et al 2021 increasing the carbon stored in the above and below ground forest biomass is a mitigation mechanism to fight climate change and offset anthropogenic emissions worldwide di cosmo et al 2016 despite the unfccc requirements related to the provision by parties of biennial forestry related carbon stock change many national forest inventories nfi are not designed for continuous yearly reports and cannot cope with the required reporting frequency due to longer update cycles mcroberts et al 2018 estimating carbon stock changes between consecutive nfis is a pivotal step in accomplishing the reporting requirements the methodology should be based on year to year measured forest variables or prediction models to extend nfi based estimates to assessment years rather than a simple interpolation between estimates produced by nfis at different years federici et al 2008 even if the main source of information for such reporting activities are nfis tomppo et al 2010 condés and mcroberts 2017 kulbokas et al 2019 in recent times considerable efforts have been laid out to integrate remotely sensed rs data in the process examples are available to provide spatially continuous also referred to as wall to wall maps and updated estimations of several forest variables such as the growing stock volumes gsv the above ground biomass agb kangas et al 2018 chirici et al 2020 vangi et al 2021 and the rate of forest disturbances hansen et al 2013 van der werf et al 2017 francini et al 2021 francini et al 2022a a francini et al 2022b b coupling traditional nfi information acquired in the field with such wall to wall maps based on remotely sensed data is the basis for evolving from traditional nfis to the new so called enhanced forest inventory efi framework white et al 2016 this has already been carried out by countries with a long history in nfis such as those in the scandinavian area næsset et al 2004 nord larsen and schumacher 2012 tomppo et al 2008 canada white et al 2016 austria hollaus et al 2009 and switzerland waser et al 2017 the efi approach has several benefits chirici et al 2020 it enables the estimation of forest attributes from a local to national scale to support local management and national planning it can provide estimates of forest removals due to logging and other disturbances which are essential in the context of carbon cycle assessments francini et al 2021 but evolving from traditional nfis to efis requires elaborating a huge amount of remotely sensed rs data which in turn requires investments in software and hardware resources for their processing d amico et al 2021 conversely field activities can be reduced by optimizing the sampling strategy by integrating rs data corona 2010 for example biomass density maps constructed from remotely sensed data can be used to enhance the stratification of ground inventories to supply carbon stock changes estimates in poorly sampled or unapproachable areas or for verification purposes ispra 2021a coming more specifically to the problem of how to estimate forest carbon stock changes there are at least three approaches reported in literature that are based on different level of data availability williams et al 2012 the first one is the gain loss method a process based approach which estimates emissions and removals from changes in carbon stocks due to forest land and related land use changes default data are provided in each land use category chapter to allow the estimation of biomass carbon stock changes in case of missing country specific data this is the method recommended only for countries without an nfi and poor data collection in the second approach forest carbon sinks are estimated by coupling estimates of forest age with age specific carbon sequestration models these models are derived from yield tables expressing carbon stocks as a function of age stand the third approach called the stock change approach requires bi temporal biomass carbon stock measurements therefore its application is suitable in countries having nfi systems and other land use categories where stocks of different biomass pools are surveyed with a regular frequency this method results in considerably less uncertainty mcroberts et al 2018 ispra 2021a examples of these approaches are presented by harris et al 2021 and xu et al 2021 they both integrated spatially explicit datasets and ground measured forest inventories data to provide global estimates of temporally averaged global forest carbon emissions and removals for the 21st century founding that woody carbon stocks increased slowly but significantly at a local and regional scale saatchi et al 2011 presented a benchmark map of biomass carbon content across the world s tropical forests for 2000 by combining ground data with airborne laser scanning als multispectral and radar data their map provided estimations of carbon stocks for countries where prior estimates were scarce or not complete with the stock change approach paul et al 2021 assessed the carbon stock and changes in new zealand using the nfi data from 2002 to 2014 showing that national forests are carbon neutral but with wide variation in carbon stocks between different forest categories in russia shepashenko et al 2021 used multiple rs based maps and nfi data to estimate co2 sequestration founding figures 47 higher than the national ghg inventory in italy dalponte and coomes 2016 developed an approach to map the carbon density of the italian alps through als and hyperspectral data nonini and fiala 2021 developed a model to assess the forest biomass and carbon stock at stand level with a gain loss approach in a northern region in italy federici et al 2008 developed a model to estimate carbon stock change data for the carbon pools to be reported under the forest land category in the lulucf sector in the ghg inventory the for est for est est imates is a bookkeeping model that calculates the above ground biomass pool c stock annually by adding the annual net increment and subtracting yearly losses associated with harvest industrial roundwood and fuelwood forest fires and other mortality a detailed description of the modeling approach is reported in italian ministry for the environmentland and sea 2019 section 3 3 the annual gsv is converted to agb and then to carbon stock by species specific parameters the model is currently used by italy to estimate carbon stock changes for the national ghg inventory under unfccc ispra 2021b b the aim of this study is to present the development of a spatial approach for the wall to wall estimation of gsv and carbon stock to fill the information gaps left by the long updating cycle of the periodic italian nfi under the framework for a new efi that better fits the international reporting requirements to do so we propose a new methodology to produce a yearly high resolution 23 m forest above ground carbon pools and gsv maps our approach is initiated by a 23 m resolution wall to wall gsv map of 2005 constructed by combining landsat imagery with nfi data vangi et al 2021 we then applied yearly increments with species specific growth models derived from yield tables driven by the forest gsv to estimate the annual current increment federici et al 2008 we take into account removals due to forest disturbances predicted using landsat imagery and the 3i3d forest disturbance detection algorithm francini et al 2021 francini et al 2022a a the approach was tuned against a set of independent field observations and the final pixel level estimates were aggregated at the regional level and validated against the design based estimation from the last italian nfi completed in 2015 obtaining an rmse at a regional level of 19 and 17 for gsv and carbon stock respectively our estimates were also compared with official data reported in the italian ghg inventory to the best of the author s knowledge this represents the first attempt to provide high resolution wall to wall yearly time series maps of forest growing stock volume and carbon stock in italy these new products allow the spatial analysis of the annual italian forest carbon stock changes consistently with the ipcc guidelines 2 materials 2 1 study area the study was carried out in italy covering 301 408 km2 fig 1 italy has a wide range of climatic conditions due to its proximity to the sea and the presence of two main mountain belts with elevations ranging between sea level up to 4000 m a s l italy has mainly a temperate mediterranean climate pinna 1970 according to the 2015 italian nfi infc 2021 forest vegetation and other wooded lands occupy 11 054 458 ha about 36 of the national land deciduous species cover 68 of the forest area and are represented mainly by quercus oak q petraea matt liebl q pubescens willd q robur l q cerris l and european beech fagus sylvatica l coniferous species such as norway spruce picea abies l h karst and pines pinus sylvestris l p nigra j f arnold p pinae l p pinaster aiton form vast plantations especially in the northern regions and coastal areas fig 1 italy is divided into 20 administrative regions nuts2 the nfi produces every ten years regional estimates for several variables including forest area total and average gsv and biomass with the relative associated standard errors se with a traditional design based approach according to nfis at the country level the average gsv was 121 m3 ha 1 and 135 m3 ha 1 in 2005 and 2015 respectively 2 2 growing stock volume baseline map for the assessment of forest gsv and above ground carbon stock in the years following the last nfi we used the 2005 gsv map produced by vangi et al 2021 for italy as the initial gsv baseline data gsv2005 this map consists of gsv predictions by landsat and other rs imagery at 23 23 m resolution for all forest pixels the full description of the methodology is available from chirici et al 2020 the model fitting and tuning steps were carried out using the randomforest package in the statistical software r 4 0 5 liaw and wiener 2002 https www r project org accessed on june 16th 2021 the pixel level estimations of the gsv range between 0 and 690 m3ha 1 with a mean value of 134 m3ha 1 and a standard deviation of 41 5 m3ha 1 for comparison the official nfi estimates range between 0 and 950 with a mean value of 145 and a standard deviation of 69 m3ha 1 using a model assisted estimation approach corona 2010 the 2005 growing stock volume map led to a standard error of 1 2 and 1 for the mean and total national gsv estimation respectively vangi et al 2021 2 3 forest category maps in italy a forest category map with a spatial resolution consistent with the input gsv map used in this study is not yet available for this reason the distribution of forest categories was derived from the corine land cover clc maps which are available for the reference years 2006 2012 and 2018 in italy clc is the only spatial source that provides consistent information on forest category distribution across different years on a national scale the clc project was started in 1990 by the european environmental agency büttner et al 2004 and consists of a european scale land use monitoring program with a 44 class nomenclature system produced by photointerpretation of high resolution satellite imagery clc uses a minimum mapping unit mmu of 25 ha and a minimum mapping width mmw of 100 m eea 2007 the original clc nomenclature system classifies the forest into three classes broadleaves coniferous and mixed forests in the italian implementation the clc maps produced by the istituto superiore per la protezione e la ricerca ambientale ispra classify forests into 28 classes bologna et al 2004 in this study we re classified forests into 18 classes annex i forest category maps were obtained from the original clc vector products by rasterizing at the same spatial resolution as the baseline gsv map then we masked out the non forest categories by assigning them to the non forest class our spatial approach requires for each year a newly updated forest category map since the clc project is not updated yearly we used the forest category map of 2006 for the years 2005 2009 that of 2012 for 2010 2014 and that of 2018 for 2015 2018 fig 2 this procedure was considered appropriate since the percentual change of forest area based on the clc maps is limited in the period 2006 2012 considering both increments and decrements the forest area changed only by 741 km2 the 0 74 of the forest area while in the period 2012 2018 it changed only by 705 km2 the 0 70 of the forest area 2 4 national collection of yield tables the national collection of yield tables from federici et al 2001 was used to model the current increment of forests as a function of gsv and the gsv as a function of forest age yield tables reported the gsv and current increment as a function of forest age for 27 species within 13 genera the 27 species were linked to the 18 forest categories derived from clc see section 2 3 with ad hoc harmonization bridges developed for this study bridges preserve data attributes based on different definitions allowing their comparison at a higher hierarchical level annex i 2 5 forest disturbances time series maps data on the spatial distribution of forest disturbances in the period 2004 2018 were needed to account for forest harvesting and other disturbances in gsv and the carbon stock estimation process these data were produced with the 3i3d algorithm francini et al 2021 recently implemented in google earth engine gee francini et al 2022b a cloud based platform that can process massive amounts of remotely sensed data gorelick et al 2017 3i3d is an unsupervised algorithm that predicts forest disturbances requiring no input parameters or calibration it analyses the pattern over three sequential years of three indexes 3i of photosynthetic activity used as three dimensional 3d space axes 3i3d was applied using yearly cloud free composites of landsat surface reflectance images atmospherically corrected with ledaps wolfe et al 2004 and acquired with a solar zenith angle smaller than 76 candidate images were acquired during the vegetative season between jun and aug with a cloud cover lower than 50 in the scene as a result we obtained a collection of about 800 images per year we excluded those pixels covered by clouds shadows water and snow foga et al 2017 and pixels with an opacity value greater than 0 3 for each year we then selected the best pixels among the remaining ones using the best available pixel procedure bap griffiths et al 2013 white et al 2016 obtaining a bap collection of cloud free composite for each year between 2004 and 2018 specifically the bap pixel selection is based on a set of scores among which i the sensor ii the day of the year iii the distance to cloud or cloud shadows and iv the opacity the bap was recently implemented in gee with the full code openly available a detailed description of the application guidance and suggestions on bap parameters setting is provided on github https code earthengine google com accept repo users sfrancini bap we used bap cloud free composites as input for the 3i3d algorithm to predict forest disturbances with a mmu of 500 m2 over the study period official forest data on burned areas annually produced and released for the same period by the italian forest service comando unità forestali ambientali e agroalimentari of carabinieri have been also used this dataset includes burnt areas from forest fires acquired through a ground survey with the global navigation satellite system gnss we merged the official national database of forest fires with the forest disturbances map produced by 3i3d with an or logical operator classifying forest pixels for each investigated year in disturbed or undisturbed based on these maps we finally produced the age of disturbed forests for each investigated year as the number of years since the last disturbance event ysld 2 6 calibration data to optimize and calibrate the procedure we used 9258 circular plots where the gsv was measured in the field between 2006 and 2019 in the framework of local forest inventories fig 1 the plots are distributed under different environmental conditions and forest categories over the whole country the same survey protocol of the italian nfi was adopted in all these plots the tree level gsv was determined by the allometric models used for the italian nfi tabacchi et al 2011 and then tree level data were aggregated at the plot level for this study allometric model prediction and gnss position uncertainties are expected to be negligible for the spatial resolution adopted mcroberts et al 2013 2016 2018 chirici et al 2020 the mean gsv in this calibration dataset is 216 m3 ha 1 with a maximum of 1482 4 and a standard deviation of 155 m3 ha 1 to find the most appropriate solution we evaluated the models in terms of rmse at the plot level comparing gsv estimates with the observed one 2 7 validation data to validate the results we compared aggregated regional gsv and carbon stock estimates for 2015 based on the pixel level values we produced the gsv2015 23 m resolution map with official regional estimates from the italian nfi we also compared aggregated values of gsv and carbon stock produced by our method with the official estimates reported in the italian 2006 2019 ghg inventory just as in the calibration we compared the accuracy of our results in terms of rmse calculated as the percent of rmse against the mean official values 3 methods 3 1 overview of the spatial approach for the years not covered by the periodic italian nfi our spatial approach for predicting gsv and carbon stocks was carried out differently for pixels belonging to disturbed and undisturbed forests annual stock changes were predicted for each 23 m pixel using the gsv2005 mapped for 2005 and the ysld as unique drivers the gsv2005 was selected since it is strictly related to the above ground biomass and carbon stock and it is directly measured by the nfi in the field while the ysld was selected because it can be easily obtained based on change detection algorithms such as the 3i3d and it is the primary driver of forest variables in yield tables our approach uses the first derivative of the richards function eq 1 to calculate the current increment eq 2 as a function of the gsv in undisturbed forests for each of the 18 clc forest categories federici et al 2008 the following equation defines the richards function 1 d y d t k v y 1 y a v y 0 f i r s t d e r i v a t i v e its analytical solution defines the richards growth curve 2 y a 1 e β k t 1 v where the general constraints for the parameters are a k 0 1 β v 0 the curve is bounded and monotonic highly flexible thanks to its four parameters it can be efficiently approximated to a logistic a v 0 exponential v 1 or other most used growth curves however due to the number of parameters and their high covariance the curve is difficult to fit and can cause problems during the non linear regression federici et al 2008 the current increment represents the dependent variable while the independent variable is the gsv map in disturbed forest areas different potential models were evaluated for estimating gsv as a function of ysld for each forest category using the data in the yield tables collection we tested four regression models two non parametric random forests and support vector machine svm and two parametric polynomial and linear regression the gsv represents the dependent variable while the independent variable is the forest age from yield tables the optimization was fine tuned by picking the most accurate model based on the correlation coefficient r2 all four regression models yielded comparable results with only slight differences the svm model slightly outperformed other approaches with an average r2 among the forest categories of 0 91 against 0 90 of random forests and 0 88 of polynomial and linear regression after the model fitting for each category in undisturbed and disturbed forest areas the above ground carbon stock is predicted in the five steps described below 1 starting from the initial gsv2005 map the current increment is estimated via the richards function in undisturbed forest areas for each year and for each forest category based on the relationships between gsv and current increment derived from yield tables 2 similar to point 1 gsv in disturbed forest areas is estimated for each year and forest category with the corresponding svm model based on the relationships between ysld and gsv derived from the yield tables 3 for each year and forest category the gsv is calculated as the sum of the previous year s gsv and the estimated current increment subtracting the losses due to natural mortality and adding the gsv in disturbed forest areas calculated in step 2 4 for each year and forest category the gsv m3 ha 1 is converted in above ground biomass agb mg d m ha 1 with the equation 3 a g b g s v b e f w b d where gsv is the growing stock volume calculated in step 3 bef is the category specific biomass expansion factor dimensionless and wbd is the wood basal density mg d m m 3 5 carbon stocks are derived from agb by applying the default carbon fraction factor of 0 47 ipcc et al 2006 following the ipcc good practice guidance for lulucf ipcc et al 2006 the average rate of natural mortality was set equal to 0 116 for evergreen categories 0 117 for deciduous categories and 0 1165 for mixed categories while bef and wbd are those applied by the for est model federici et al 2008 the detailed methodology is described here below 3 2 gsv estimation in undisturbed forests in undisturbed forests the gsv for the year n was computed at pixel level by adding the current increment of the year n 1 to the gsv of the year n 1 and subtracting losses due to natural mortality we used an age independent model to create annual maps of the current increment category specific growth models were constructed using the data of the national yield tables collection to calculate the current increment m3 ha 1 y 1 as a function of gsv m3 ha 1 using the first derivative of the richards function eq 1 the gsv represents the independent variable x while the dependent variable y is the correspondent current increment the forest category specific richards functions were fitted using all the fertility classes of the yield tables the parameterization was based on a 25 iterations bootstrap cross validation procedure for each bootstrap iteration and species the rmse was calculated and the model which reported the lowest rmse was chosen as the final model rmse was calculated as 4 r m s e s p i 1 n y i y i ˆ 2 n where n is the number of observations in the yield tables for the species sp y i is the current increment value reported in the yields table for the i th observation and y i ˆ is the current increment predicted from the model for the i th observation we started the process based on the gsv2005 map produced by vangi et al 2021 and we produced the updated gsv2006 map applying for each 23 23 m undisturbed forest pixel the current increment per hectare predicted for each forest category with the corresponding growth curves and subtracting the natural mortality then the process was repeated for 2007 based on gsv2006 and so on until 2018 by applying the richard first derivative approach the current increment was estimated with an average rmse as per eq 4 of 49 4 across all forest categories with significant variations among forest categories mainly due to the number of observations and fertility classes available in the yield tables some of the most frequent forest categories in italy obtained the best results such as the maple ash hornbeam mixed forests rmse 22 the mediterranean maquis rmse 13 and the chestnut forests rmse 26 which altogether represent more than 25 of the national forest area the best results were obtained by the exotic plantations category with an rmse of 3 2 most probably because of their homogeneous growth behaviour in comparison the mixed conifers category obtained the worst result with an rmse of 95 but they cover only 4 4 of the forest area most probably because of their heterogeneous composition 3 3 gsv estimation in disturbed forests we already know that forest age is not an appropriate predictor for estimating the productivity of undisturbed forests in italy since they are mainly uneven aged and are characterized by a complex mosaic of different ages or cohorts federici et al 2008 frate et al 2015 instead in most disturbed forests trees regrowing after the disturbance results in even aged stands at least for the first years after the disturbance this is particularly true for clearcuts in coppice forest chirici et al 2020 which represent the most common forest disturbance in italy based on francini et al 2022 a representing 80 of all forest loggings in italy in such a situation forest age can be used to predict gsv growth in disturbed stands using the data in the national yields table collection we used forest categories specific svm models to predict the gsv based on forest age with a radial basis kernel function svm approaches are known to be robust against outliers and overfitting and are well suited for approaching problems with a limited amount of training data these algorithms can generate non linear decision surfaces by mapping the data into a high dimensional space through non linear mapping functions called kernel functions cortes and vapnik 1995 pal and mather 2005 allowing the separation of the data through linear hyperplanes dixon and candade 2008 among the kernel functions one of the most used is the radial basis function which has two tuning parameters c regularization parameter and ɣ kernel width kavzoglu and colkesen 2009 an in depth explanation of svm based models and kernels is presented in smola and schölkopf 2004 and kavzoglu and colkesen 2009 implementations of svm models in rs can be found in mountrakis et al 2011 in this study the parameters of svm and radial kernel c ɣ were determined by bootstrap cross validation with 25 iterations using the grid search method by selecting the pairs of parameters that produce the lowest cross validation rmse among an exponentially growing sequence of the parameters c 21 2 21 25 ɣ 2 5 2 4 2 for each bootstrap iteration and species the rmse was calculated as per eq 4 and the model which obtained the lowest rmse was chosen as the final model in disturbed areas identified by the 3i3d algorithm the gsv was computed for each year and forest category by applying the category specific svm models fitted from the yield tables data the ysld for the year n was used as the independent variable to predict the gsv n in each disturbed pixel obtaining a gsv map of forest disturbances for each year between 2005 and 2018 the complete gsv n map was produced by overlaying the gsv n maps of disturbed and undisturbed forests the svm models led to an average rmse of 35 9 with a maximum of 64 for the mixed forests with the prevalence of coniferous and a minimum of 5 for the maple ash hornbeam mixed forest as for the richard models we observed significant variations depending on the number of fertility classes in yield tables 3 4 carbon stock conversion once estimated the gsv amounts of agb are consequently assessed for every forest typology starting from the gsv the agb mg d m ha 1 is calculated through equation 3 following the approach presented in federici et al 2008 carbon stock maps were derived from agb maps by applying the default factor for carbon fractions of 0 47 ipcc et al 2006 the pixel level predictions of gsv and stocked carbon were aggregated at the regional level for each year 4 results 4 1 gsv and carbon stock estimation the spatial approach for estimating annual gsv and above ground carbon pool was applied to produce 23 m resolution yearly pixel level estimates from 2005 to 2018 based on our results the gsv increased in italy by 522 million m3 moving from an average of 130 m3 ha 1 to 180 m3 ha 1 gsv and above ground carbon stocks time series are reported in annex ii carbon stock increased by 206 million of t in the same period moving from 36 9 mg c ha 1 to 59 3 mg c ha 1 with an average accumulation rate of 14 7 mln mg c y 1 regionally most of the gsv and carbon gains dominate mountain landscapes of the alps and apennines mountains in the years 2005 2018 among all forest categories beech forests accumulated the most gsv with about 3926 mln of m3 corresponding to 54 mln mg c of above ground carbon stored about 28 3 of the total carbon absorbed by national forests followed by mixed broadleaf forests 34 mln mg c about 18 of the total and the fir spruce forests 23 mln mg c 12 of the total regardless of the forest category in the study period carbon accumulation is reflected mainly in the increase of the carbon density rather than the increase of the total forest area which amounts to 145 000 ha according to the clc maps northern regions trentino alto adige piemonte lombardia veneto friuli venezia giulia have the highest gsv accumulation in terms of absolute and per hectare figures accounting for 54 of the national total other regions with significant gsv accumulation are toscana sardegna and emilia romagna contributing 20 to the national gsv growth each up to 20 mln m3 in the study period in contrast most southern regions molise campania puglia basilicata sicilia show the least accumulation of gsv less than 8 mln m3 between 2005 and 2018 carbon uptake has similar patterns exhibiting higher absolute and per hectare storage in many northern regions trentino alto adige piemonte lombardia veneto and lower in the southern ones molise puglia umbria fig 4 also at the regional level the accumulation of gsv and carbon stock is primarily driven by the increase of gsv and carbon density rather than the total forest area this is probably due to a decrease in the harvested area over the last two decades which allowed for significant growth in gsv per unit area francini et al 2022a b in fig 3 is reported the overall absolute accumulation of gsv and carbon stock at the regional level over the study period 4 2 validation and comparison of our results during the optimization phase the 23 m resolution pixel level estimations of gsv estimates obtained applying the best configuration of our models where compared against the gsv measured in the field in 9258 independent plots acquired in different years in the period 2006 2018 from such comparison the average rmse was 57 ranging between 89 6 in 2009 and 34 in 2015 fig 5 the bias across years was 3 7 m3 ha 1 with the minimum in 2010 0 2 m3 ha 1 and the maximum in 2013 and 2015 70 3 and 60 2 m3 ha 1 these values are in the range of previous experiences immitzer et al 2016 chirici et al 2020 vangi et al 2021 pixel level estimations for the year 2015 where aggregated for administrative regions and compared with the official 2015 nfi estimates infc 2021 resulting in a 6 2 and 1 1 difference at the national level for gsv and carbon stock respectively calculated as the mean value of the difference between predicted and observed results we obtained an rmse of 19 5 and 17 8 at the regional level and an r2 of 0 94 and 0 92 for gsv and carbon stock respectively fig 6 rmse was calculated as 5 r m s e n f i i 1 n y n f i i y i ˆ 2 n r g where n r g is the number of italian regions y n f i i is the official nfi value of gsv and carbon stock for the i th region and y i ˆ is the aggregated estimation of gsv and carbon stock produced by the spatial approach for the i th region the data for the comparison against the 3rd italian nfi infc 2021 are presented in annex iii finally following the same procedure our gsv predictions aggregated for italian regions were compared with official estimates of italian ghg inventories for 2005 2018 obtaining an overall rmse of 28 6 and an r2 of 0 77 with a growing trend over time here the rmse was calculated as 6 r m s e g h g i 1 n y g h g i y i ˆ 2 n r g where n r g is the number of italian regions y g h g i is the official ghg inventoriy value of gsv and carbon stock for the i th region and y i ˆ is the aggregated estimation of gsv and carbon stock produced by the spatial approach for the i th region the carbon stock was also compared against the official italian ghg inventory lulucf sector forest land remaining forest land category for the same period ispra 2021a b yielding an r2 of 0 88 and an overall rmse as per equation 6 of 23 1 and 17 2 among years and regions respectively as for the gsv consistency with official estimates has worsened over the years while at the regional level reached the minimum in piemonte rmse 2 5 and the maximum in trentino alto adige rmse 48 6 thirteen out of 20 regions showed an rmse less than 15 with seven regions less than 10 fig 7 reports the regional comparison between our results and the official estimates from the national ghg inventory ispra 2021b b regarding gsv and carbon stock 5 discussion the main objective of the study was to develop a new spatial approach for producing wall to wall high resolution yearly gsv and carbon stock predictions between consecutive nfi field measurements exploiting remotely sensed and auxiliary data which could be used for operational application and to respond to international reporting tasks empirical models developed by interpolating data from nfi field plots could produce updated estimates only for short term predictions for which growth conditions such as climate and management regimes are expected to be stable peng 2000 moreover in such models stand variables are driven by the age of the forest but in natural conditions growth is strictly related to species and local environmental conditions for these reasons our novel approach for estimating carbon stocks and changes at the national and regional level in the above ground carbon pool is driven only by nfi gsv data and yield models the yearly wall to wall maps of gsv and carbon stock can support reporting activities and forest management at any scale by aggregating pixel level predictions producing small area estimations for example using the estimators proposed by chirici et al 2020 our results agreed with those reported by the official italian ghg inventory and show an increasing trend in the above ground carbon pool that reflects both the expansion of forest areas according to nfi in the period 2005 2015 forest areas increased by 58 692 ha y 1 approximatively 0 5 of total forest area in 2015 and the increased growing stock resulting from a forest harvest rate lower than the current increment the organic carbon in the above ground biomass of the italian forest exceeded 566 million mg c in 2018 with a different contribution of regions and forest categories in terms of fixed organic carbon gsv and carbon stock distribution among forest categories comply with the 2020 italian fao fra report for 2005 and 2010 with beech and spruce fir forests accounting for 40 and 36 of the total gsv and carbon stock respectively due to their limited area nationwide the forest categories contributing the least to carbon storage less than 0 5 of the total are exotic conifers broadleaf plantations wood arboriculture and riparian formations it is worth noting that other wooded vegetation in open and shrublands has a relatively significant contribution to terrestrial carbon sinks storing more than seven mln of mg c of carbon approximately 5 of the national total large forest areas characterize regions with the most gsv and carbon stock and highly carbon storing forest categories beech mixed broadleaf fir spruce larch mediterranean pines exotic plantations despite most of these categories are subject to intense forest harvesting as resulted from 3i3d disturbances maps in this regard it is worth noting that the major disagreement between our approach and the official ispra estimates is found in the northern regions and particularly in trentino alto adige which leads to the maximum relative rmse for which the ghg inventories estimate a decrease in gsv and carbon stock over the study period this mismatch is primarily due to underestimating the number of forest disturbances especially in high forest stands of the main mountain ranges where silvicultural treatments are based on continuous canopy cover approaches that are difficult to detect by optical satellite imagery the lack of fire data compounds the underestimation of disturbances before 2007 and after 2017 moreover for the autonomous provinces of trento and bolzano the database of forest fires includes only fires greater than 20 ha potentially increasing the underestimation of the total number of disturbances without offsetting for forest harvests and forest fires the gsv builds up rapidly driven by the increase of the current increment of highly productive forest categories leading to significant overestimates and large values of rmse however in trentino alto adige the mismatch between our results and the 2015 nfi is less evident with an underestimation of only 13 and 15 for gsv and carbon stock respectively another source of uncertainty is the overestimation of the current increment with the richard first derivative compared to the 2nd nfi field measurement this finding contrasts with the underestimation reported by federici et al 2008 compared to the 1st nfi data the discrepancy between the predicted and reported current increment of nfi is likely driven by three factors i an outdated collection of national yield tables which no longer reflect the country s real average quality of forest sites ii richard s first derivatives were fitted against all species quality class data in yield tables and since species data were aggregated to match the species composition of each forest category as closely as possible the predicted current increment of less productive species can be overestimated especially for mixed forest categories iii the overestimation of the gsv in the initial 2005 gsv map used as the independent variable for richards functions this evidence was reported by vangi et al 2021 svm models generally performed better than the richards function for the same forest categories except for chestnut hygrophilous formations exotic broadleaf plantations larch and stone pine categories for those forest categories the svm model underestimates gsv for older age classes for instance over ten years for exotic broadleaf plantations and riparian formations 25 years for chestnut and more than 80 years for the remaining forest categories hence the effect of underestimation only affects categories of hygrophilous formations and non native species plantations as the years since the last disturbance can be at most equal to 13 years 2018 2005 during the optimization phase using the dataset of independent field plots the differences between pixel level measured and estimated gsv increased over time this bias with under predictions for plots with high values of gsv especially for years 2009 2012 and 2015 can be caused by the well known saturation effect especially in dense crown cover and steep terrain nilsson et al 2017 giannetti et al 2018 chirici et al 2020 such dataset was acquired for those years in forests managed for productive purposes or more in general with an average gsv much greater 420 m3 ha 1 than those measured in infc plots 140 m3 ha 1 in this phase the largest bias was observed in southern regions and in the islands that are characterized by sparse mediterranean vegetation where modeling the gsv resulted more difficult in previous experiences too d amico et al 2021 vangi et al 2021 but it is important to note that wall to wall spatial predictions from nfi field observations should never be used at the pixel level since the single pixel predictions may be affected by a consistent bias mcroberts and tomppo 2007 however when we aggregated the pixel level estimations at the regional level we obtained very satisfying results consistent both with the italian ghg inventory and the 3rd nfi infc 2021 it is worth noting that concerning the 3rd nfi in 2015 the carbon stock difference at the regional level was minor than the gsv difference fig 7 this is interesting because it proves that the species specific bef and wbd together with the spatial distribution of forest categories can compensate for the overestimation of gsv under this point of view our estimates are conservative since the approach neither overestimates increases nor underestimates decreases in carbon stocks with respect to the nfi official estimations as soon as new high resolution forest types maps will be available the use of the clc maps should be reconsidered several studies have already highlighted the limitations of clc maps in forestry primarily because of the wide mmu of 25 ha seebach et al 2012 vizzarri et al 2015 vangi et al 2021 6 conclusions to the best of our knowledge this is the first study to provide yearly high resolution gsv agb and carbon stock wall to wall time series maps for the whole national territory in italy allowing an in depth analysis of the forest carbon stock changes consistently with the ipcc guidelines the spatial nature of our results enables small scale estimates by aggregating individual pixel predictions enhancing the spatial resolution of traditional nfi design based estimates chirici et al 2020 and can be embedded into decision support systems to support sustainable forest management and precision forestry activities furthermore the knowledge of the spatial distribution of carbon among forest categories can be of fundamental importance under the climate mitigation goals of the paris agreement unfccc 2015 the growing need for new information and technological advances is driving the rapid evolution of forest monitoring and assessment however the exploitation of the latter and their implementation within international reporting processes should be evidence based corona 2018 this study provides an innovative spatial framework to track gsv and carbon stock changes between nfi surveys at local to national scales providing a reliable monitoring approach to meet the increasing interests of non government and private entities in carbon offset investments our new method incorporates forest disturbance between surveys such as forest fires and harvesting thanks to landsat based time series metrics exploited by the 3i3d unsupervised change detection algorithm which has already proved to be a better solution in the mediterranean environment than previous algorithms francini et al 2021 the 3i3d algorithm here demonstrated to be a valid solution for deriving forest age to track the gsv regrowth after disturbances by applying age dependent relationships in yield tables the approach could be improved with 2015 nfi data as well as updated information on allometric models and yield tables allowing better model calibration and quality assurance routines it is worth remembering that although ghg inventories are not measured on the ground they represent official data sources used for national and international reporting activities nevertheless the availability of ground based carbon content data to calibrate and validate the method is desirable for this reason as soon as they will be available 2015 nfi carbon stock ground data will allow the evaluation of the pixel level performance for the carbon stock map for 2015 providing insight into the method s effectiveness the availability of an official high resolution national forest map and wall to wall multitemporal als data could also be fundamental for improving the quality of gsv and carbon stock spatial estimations author s contributions conceptualization g c e v and g d methodology e v g d s f software e v validation e v formal analysis e v g d investigation e v g d data curation e v g d writing original draft preparation e v all authors have contributed to the drafting of the manuscript all the authors assisted in the quality control and revisions of the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was partially supported by the following projects multifor multi scale observations to predict forest response to pollution and climate change prin 2020 research project of national relevance funded by the italian ministry of university and research prot 2020e52ths superb systemic solutions for upscaling of urgent ecosystem restoration for forest related biodiversity and ecosystem services h2020 project funded by the european commission number 101036849 call lc gd 7 1 2020 efinet european forest information network funded by the european forest institute network fund g 01 2021 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105580 
25486,within the paris agreement s enhanced transparency framework consistent data collections are the prerequisite for a successful reporting of ghg emissions for such purposes nfis are usually the primary source of information even if they are frequently not designed for producing estimations on a yearly basis and in the form of wall to wall high resolution maps in this framework we present a new spatial model to produce yearly growing stock volume gsv above ground biomass agb and carbon stock wall to wall estimates we tested the model in italy for the period 2005 2018 obtaining a time series of yearly maps at 23 m spatial resolution results were validated against the 2015 italian nfi reaching an average rmse of 19 for aggregated areas results were also compared against data reported by the italian ghg inventory reaching an rmse of 28 and 20 for gsv and carbon stock respectively we demonstrated that the modeling approach can be successfully used for setting up a forest monitoring system to meet the interests of governments in inventories of ghg emissions and private entities in carbon offset investments keywords national forest inventory gsv carbon stock forest modeling spatial modeling italy data availability data will be made available on request 1 introduction under the enhanced transparency framework of the paris agreement each country party must report every two years an inventory of their anthropogenic greenhouse gases ghgs emissions by sources and removals by sinks following the intergovernmental panel on climate change ipcc guidelines and guidance ipcc et al 2006 the ghg emission inventory has to fulfill the ipcc key principles transparency accuracy completeness consistency and comparability while providing helpful information for assessing the climate impacts the land use land use change and forestry lulucf is exceptionally demanding dealing with natural carbon dynamics and aiming to assess emissions and removals related to the impact of anthropogenic activities the lulucf sector is responsible for significant ghg emissions globally mainly due to deforestation activities in this framework forests are pivotal ecosystems being a substantial and growing atmospheric carbon sink sellers et al 2018 forests are estimated to sequester 30 of the total global co2 released into the atmosphere annually houghton and nassikas 2017 corresponding to 7 6 gt co2 y 1 reflecting a balance between gross carbon removals and gross emissions from deforestation and other disturbances harris et al 202 xu et al 2021 increasing the carbon stored in the above and below ground forest biomass is a mitigation mechanism to fight climate change and offset anthropogenic emissions worldwide di cosmo et al 2016 despite the unfccc requirements related to the provision by parties of biennial forestry related carbon stock change many national forest inventories nfi are not designed for continuous yearly reports and cannot cope with the required reporting frequency due to longer update cycles mcroberts et al 2018 estimating carbon stock changes between consecutive nfis is a pivotal step in accomplishing the reporting requirements the methodology should be based on year to year measured forest variables or prediction models to extend nfi based estimates to assessment years rather than a simple interpolation between estimates produced by nfis at different years federici et al 2008 even if the main source of information for such reporting activities are nfis tomppo et al 2010 condés and mcroberts 2017 kulbokas et al 2019 in recent times considerable efforts have been laid out to integrate remotely sensed rs data in the process examples are available to provide spatially continuous also referred to as wall to wall maps and updated estimations of several forest variables such as the growing stock volumes gsv the above ground biomass agb kangas et al 2018 chirici et al 2020 vangi et al 2021 and the rate of forest disturbances hansen et al 2013 van der werf et al 2017 francini et al 2021 francini et al 2022a a francini et al 2022b b coupling traditional nfi information acquired in the field with such wall to wall maps based on remotely sensed data is the basis for evolving from traditional nfis to the new so called enhanced forest inventory efi framework white et al 2016 this has already been carried out by countries with a long history in nfis such as those in the scandinavian area næsset et al 2004 nord larsen and schumacher 2012 tomppo et al 2008 canada white et al 2016 austria hollaus et al 2009 and switzerland waser et al 2017 the efi approach has several benefits chirici et al 2020 it enables the estimation of forest attributes from a local to national scale to support local management and national planning it can provide estimates of forest removals due to logging and other disturbances which are essential in the context of carbon cycle assessments francini et al 2021 but evolving from traditional nfis to efis requires elaborating a huge amount of remotely sensed rs data which in turn requires investments in software and hardware resources for their processing d amico et al 2021 conversely field activities can be reduced by optimizing the sampling strategy by integrating rs data corona 2010 for example biomass density maps constructed from remotely sensed data can be used to enhance the stratification of ground inventories to supply carbon stock changes estimates in poorly sampled or unapproachable areas or for verification purposes ispra 2021a coming more specifically to the problem of how to estimate forest carbon stock changes there are at least three approaches reported in literature that are based on different level of data availability williams et al 2012 the first one is the gain loss method a process based approach which estimates emissions and removals from changes in carbon stocks due to forest land and related land use changes default data are provided in each land use category chapter to allow the estimation of biomass carbon stock changes in case of missing country specific data this is the method recommended only for countries without an nfi and poor data collection in the second approach forest carbon sinks are estimated by coupling estimates of forest age with age specific carbon sequestration models these models are derived from yield tables expressing carbon stocks as a function of age stand the third approach called the stock change approach requires bi temporal biomass carbon stock measurements therefore its application is suitable in countries having nfi systems and other land use categories where stocks of different biomass pools are surveyed with a regular frequency this method results in considerably less uncertainty mcroberts et al 2018 ispra 2021a examples of these approaches are presented by harris et al 2021 and xu et al 2021 they both integrated spatially explicit datasets and ground measured forest inventories data to provide global estimates of temporally averaged global forest carbon emissions and removals for the 21st century founding that woody carbon stocks increased slowly but significantly at a local and regional scale saatchi et al 2011 presented a benchmark map of biomass carbon content across the world s tropical forests for 2000 by combining ground data with airborne laser scanning als multispectral and radar data their map provided estimations of carbon stocks for countries where prior estimates were scarce or not complete with the stock change approach paul et al 2021 assessed the carbon stock and changes in new zealand using the nfi data from 2002 to 2014 showing that national forests are carbon neutral but with wide variation in carbon stocks between different forest categories in russia shepashenko et al 2021 used multiple rs based maps and nfi data to estimate co2 sequestration founding figures 47 higher than the national ghg inventory in italy dalponte and coomes 2016 developed an approach to map the carbon density of the italian alps through als and hyperspectral data nonini and fiala 2021 developed a model to assess the forest biomass and carbon stock at stand level with a gain loss approach in a northern region in italy federici et al 2008 developed a model to estimate carbon stock change data for the carbon pools to be reported under the forest land category in the lulucf sector in the ghg inventory the for est for est est imates is a bookkeeping model that calculates the above ground biomass pool c stock annually by adding the annual net increment and subtracting yearly losses associated with harvest industrial roundwood and fuelwood forest fires and other mortality a detailed description of the modeling approach is reported in italian ministry for the environmentland and sea 2019 section 3 3 the annual gsv is converted to agb and then to carbon stock by species specific parameters the model is currently used by italy to estimate carbon stock changes for the national ghg inventory under unfccc ispra 2021b b the aim of this study is to present the development of a spatial approach for the wall to wall estimation of gsv and carbon stock to fill the information gaps left by the long updating cycle of the periodic italian nfi under the framework for a new efi that better fits the international reporting requirements to do so we propose a new methodology to produce a yearly high resolution 23 m forest above ground carbon pools and gsv maps our approach is initiated by a 23 m resolution wall to wall gsv map of 2005 constructed by combining landsat imagery with nfi data vangi et al 2021 we then applied yearly increments with species specific growth models derived from yield tables driven by the forest gsv to estimate the annual current increment federici et al 2008 we take into account removals due to forest disturbances predicted using landsat imagery and the 3i3d forest disturbance detection algorithm francini et al 2021 francini et al 2022a a the approach was tuned against a set of independent field observations and the final pixel level estimates were aggregated at the regional level and validated against the design based estimation from the last italian nfi completed in 2015 obtaining an rmse at a regional level of 19 and 17 for gsv and carbon stock respectively our estimates were also compared with official data reported in the italian ghg inventory to the best of the author s knowledge this represents the first attempt to provide high resolution wall to wall yearly time series maps of forest growing stock volume and carbon stock in italy these new products allow the spatial analysis of the annual italian forest carbon stock changes consistently with the ipcc guidelines 2 materials 2 1 study area the study was carried out in italy covering 301 408 km2 fig 1 italy has a wide range of climatic conditions due to its proximity to the sea and the presence of two main mountain belts with elevations ranging between sea level up to 4000 m a s l italy has mainly a temperate mediterranean climate pinna 1970 according to the 2015 italian nfi infc 2021 forest vegetation and other wooded lands occupy 11 054 458 ha about 36 of the national land deciduous species cover 68 of the forest area and are represented mainly by quercus oak q petraea matt liebl q pubescens willd q robur l q cerris l and european beech fagus sylvatica l coniferous species such as norway spruce picea abies l h karst and pines pinus sylvestris l p nigra j f arnold p pinae l p pinaster aiton form vast plantations especially in the northern regions and coastal areas fig 1 italy is divided into 20 administrative regions nuts2 the nfi produces every ten years regional estimates for several variables including forest area total and average gsv and biomass with the relative associated standard errors se with a traditional design based approach according to nfis at the country level the average gsv was 121 m3 ha 1 and 135 m3 ha 1 in 2005 and 2015 respectively 2 2 growing stock volume baseline map for the assessment of forest gsv and above ground carbon stock in the years following the last nfi we used the 2005 gsv map produced by vangi et al 2021 for italy as the initial gsv baseline data gsv2005 this map consists of gsv predictions by landsat and other rs imagery at 23 23 m resolution for all forest pixels the full description of the methodology is available from chirici et al 2020 the model fitting and tuning steps were carried out using the randomforest package in the statistical software r 4 0 5 liaw and wiener 2002 https www r project org accessed on june 16th 2021 the pixel level estimations of the gsv range between 0 and 690 m3ha 1 with a mean value of 134 m3ha 1 and a standard deviation of 41 5 m3ha 1 for comparison the official nfi estimates range between 0 and 950 with a mean value of 145 and a standard deviation of 69 m3ha 1 using a model assisted estimation approach corona 2010 the 2005 growing stock volume map led to a standard error of 1 2 and 1 for the mean and total national gsv estimation respectively vangi et al 2021 2 3 forest category maps in italy a forest category map with a spatial resolution consistent with the input gsv map used in this study is not yet available for this reason the distribution of forest categories was derived from the corine land cover clc maps which are available for the reference years 2006 2012 and 2018 in italy clc is the only spatial source that provides consistent information on forest category distribution across different years on a national scale the clc project was started in 1990 by the european environmental agency büttner et al 2004 and consists of a european scale land use monitoring program with a 44 class nomenclature system produced by photointerpretation of high resolution satellite imagery clc uses a minimum mapping unit mmu of 25 ha and a minimum mapping width mmw of 100 m eea 2007 the original clc nomenclature system classifies the forest into three classes broadleaves coniferous and mixed forests in the italian implementation the clc maps produced by the istituto superiore per la protezione e la ricerca ambientale ispra classify forests into 28 classes bologna et al 2004 in this study we re classified forests into 18 classes annex i forest category maps were obtained from the original clc vector products by rasterizing at the same spatial resolution as the baseline gsv map then we masked out the non forest categories by assigning them to the non forest class our spatial approach requires for each year a newly updated forest category map since the clc project is not updated yearly we used the forest category map of 2006 for the years 2005 2009 that of 2012 for 2010 2014 and that of 2018 for 2015 2018 fig 2 this procedure was considered appropriate since the percentual change of forest area based on the clc maps is limited in the period 2006 2012 considering both increments and decrements the forest area changed only by 741 km2 the 0 74 of the forest area while in the period 2012 2018 it changed only by 705 km2 the 0 70 of the forest area 2 4 national collection of yield tables the national collection of yield tables from federici et al 2001 was used to model the current increment of forests as a function of gsv and the gsv as a function of forest age yield tables reported the gsv and current increment as a function of forest age for 27 species within 13 genera the 27 species were linked to the 18 forest categories derived from clc see section 2 3 with ad hoc harmonization bridges developed for this study bridges preserve data attributes based on different definitions allowing their comparison at a higher hierarchical level annex i 2 5 forest disturbances time series maps data on the spatial distribution of forest disturbances in the period 2004 2018 were needed to account for forest harvesting and other disturbances in gsv and the carbon stock estimation process these data were produced with the 3i3d algorithm francini et al 2021 recently implemented in google earth engine gee francini et al 2022b a cloud based platform that can process massive amounts of remotely sensed data gorelick et al 2017 3i3d is an unsupervised algorithm that predicts forest disturbances requiring no input parameters or calibration it analyses the pattern over three sequential years of three indexes 3i of photosynthetic activity used as three dimensional 3d space axes 3i3d was applied using yearly cloud free composites of landsat surface reflectance images atmospherically corrected with ledaps wolfe et al 2004 and acquired with a solar zenith angle smaller than 76 candidate images were acquired during the vegetative season between jun and aug with a cloud cover lower than 50 in the scene as a result we obtained a collection of about 800 images per year we excluded those pixels covered by clouds shadows water and snow foga et al 2017 and pixels with an opacity value greater than 0 3 for each year we then selected the best pixels among the remaining ones using the best available pixel procedure bap griffiths et al 2013 white et al 2016 obtaining a bap collection of cloud free composite for each year between 2004 and 2018 specifically the bap pixel selection is based on a set of scores among which i the sensor ii the day of the year iii the distance to cloud or cloud shadows and iv the opacity the bap was recently implemented in gee with the full code openly available a detailed description of the application guidance and suggestions on bap parameters setting is provided on github https code earthengine google com accept repo users sfrancini bap we used bap cloud free composites as input for the 3i3d algorithm to predict forest disturbances with a mmu of 500 m2 over the study period official forest data on burned areas annually produced and released for the same period by the italian forest service comando unità forestali ambientali e agroalimentari of carabinieri have been also used this dataset includes burnt areas from forest fires acquired through a ground survey with the global navigation satellite system gnss we merged the official national database of forest fires with the forest disturbances map produced by 3i3d with an or logical operator classifying forest pixels for each investigated year in disturbed or undisturbed based on these maps we finally produced the age of disturbed forests for each investigated year as the number of years since the last disturbance event ysld 2 6 calibration data to optimize and calibrate the procedure we used 9258 circular plots where the gsv was measured in the field between 2006 and 2019 in the framework of local forest inventories fig 1 the plots are distributed under different environmental conditions and forest categories over the whole country the same survey protocol of the italian nfi was adopted in all these plots the tree level gsv was determined by the allometric models used for the italian nfi tabacchi et al 2011 and then tree level data were aggregated at the plot level for this study allometric model prediction and gnss position uncertainties are expected to be negligible for the spatial resolution adopted mcroberts et al 2013 2016 2018 chirici et al 2020 the mean gsv in this calibration dataset is 216 m3 ha 1 with a maximum of 1482 4 and a standard deviation of 155 m3 ha 1 to find the most appropriate solution we evaluated the models in terms of rmse at the plot level comparing gsv estimates with the observed one 2 7 validation data to validate the results we compared aggregated regional gsv and carbon stock estimates for 2015 based on the pixel level values we produced the gsv2015 23 m resolution map with official regional estimates from the italian nfi we also compared aggregated values of gsv and carbon stock produced by our method with the official estimates reported in the italian 2006 2019 ghg inventory just as in the calibration we compared the accuracy of our results in terms of rmse calculated as the percent of rmse against the mean official values 3 methods 3 1 overview of the spatial approach for the years not covered by the periodic italian nfi our spatial approach for predicting gsv and carbon stocks was carried out differently for pixels belonging to disturbed and undisturbed forests annual stock changes were predicted for each 23 m pixel using the gsv2005 mapped for 2005 and the ysld as unique drivers the gsv2005 was selected since it is strictly related to the above ground biomass and carbon stock and it is directly measured by the nfi in the field while the ysld was selected because it can be easily obtained based on change detection algorithms such as the 3i3d and it is the primary driver of forest variables in yield tables our approach uses the first derivative of the richards function eq 1 to calculate the current increment eq 2 as a function of the gsv in undisturbed forests for each of the 18 clc forest categories federici et al 2008 the following equation defines the richards function 1 d y d t k v y 1 y a v y 0 f i r s t d e r i v a t i v e its analytical solution defines the richards growth curve 2 y a 1 e β k t 1 v where the general constraints for the parameters are a k 0 1 β v 0 the curve is bounded and monotonic highly flexible thanks to its four parameters it can be efficiently approximated to a logistic a v 0 exponential v 1 or other most used growth curves however due to the number of parameters and their high covariance the curve is difficult to fit and can cause problems during the non linear regression federici et al 2008 the current increment represents the dependent variable while the independent variable is the gsv map in disturbed forest areas different potential models were evaluated for estimating gsv as a function of ysld for each forest category using the data in the yield tables collection we tested four regression models two non parametric random forests and support vector machine svm and two parametric polynomial and linear regression the gsv represents the dependent variable while the independent variable is the forest age from yield tables the optimization was fine tuned by picking the most accurate model based on the correlation coefficient r2 all four regression models yielded comparable results with only slight differences the svm model slightly outperformed other approaches with an average r2 among the forest categories of 0 91 against 0 90 of random forests and 0 88 of polynomial and linear regression after the model fitting for each category in undisturbed and disturbed forest areas the above ground carbon stock is predicted in the five steps described below 1 starting from the initial gsv2005 map the current increment is estimated via the richards function in undisturbed forest areas for each year and for each forest category based on the relationships between gsv and current increment derived from yield tables 2 similar to point 1 gsv in disturbed forest areas is estimated for each year and forest category with the corresponding svm model based on the relationships between ysld and gsv derived from the yield tables 3 for each year and forest category the gsv is calculated as the sum of the previous year s gsv and the estimated current increment subtracting the losses due to natural mortality and adding the gsv in disturbed forest areas calculated in step 2 4 for each year and forest category the gsv m3 ha 1 is converted in above ground biomass agb mg d m ha 1 with the equation 3 a g b g s v b e f w b d where gsv is the growing stock volume calculated in step 3 bef is the category specific biomass expansion factor dimensionless and wbd is the wood basal density mg d m m 3 5 carbon stocks are derived from agb by applying the default carbon fraction factor of 0 47 ipcc et al 2006 following the ipcc good practice guidance for lulucf ipcc et al 2006 the average rate of natural mortality was set equal to 0 116 for evergreen categories 0 117 for deciduous categories and 0 1165 for mixed categories while bef and wbd are those applied by the for est model federici et al 2008 the detailed methodology is described here below 3 2 gsv estimation in undisturbed forests in undisturbed forests the gsv for the year n was computed at pixel level by adding the current increment of the year n 1 to the gsv of the year n 1 and subtracting losses due to natural mortality we used an age independent model to create annual maps of the current increment category specific growth models were constructed using the data of the national yield tables collection to calculate the current increment m3 ha 1 y 1 as a function of gsv m3 ha 1 using the first derivative of the richards function eq 1 the gsv represents the independent variable x while the dependent variable y is the correspondent current increment the forest category specific richards functions were fitted using all the fertility classes of the yield tables the parameterization was based on a 25 iterations bootstrap cross validation procedure for each bootstrap iteration and species the rmse was calculated and the model which reported the lowest rmse was chosen as the final model rmse was calculated as 4 r m s e s p i 1 n y i y i ˆ 2 n where n is the number of observations in the yield tables for the species sp y i is the current increment value reported in the yields table for the i th observation and y i ˆ is the current increment predicted from the model for the i th observation we started the process based on the gsv2005 map produced by vangi et al 2021 and we produced the updated gsv2006 map applying for each 23 23 m undisturbed forest pixel the current increment per hectare predicted for each forest category with the corresponding growth curves and subtracting the natural mortality then the process was repeated for 2007 based on gsv2006 and so on until 2018 by applying the richard first derivative approach the current increment was estimated with an average rmse as per eq 4 of 49 4 across all forest categories with significant variations among forest categories mainly due to the number of observations and fertility classes available in the yield tables some of the most frequent forest categories in italy obtained the best results such as the maple ash hornbeam mixed forests rmse 22 the mediterranean maquis rmse 13 and the chestnut forests rmse 26 which altogether represent more than 25 of the national forest area the best results were obtained by the exotic plantations category with an rmse of 3 2 most probably because of their homogeneous growth behaviour in comparison the mixed conifers category obtained the worst result with an rmse of 95 but they cover only 4 4 of the forest area most probably because of their heterogeneous composition 3 3 gsv estimation in disturbed forests we already know that forest age is not an appropriate predictor for estimating the productivity of undisturbed forests in italy since they are mainly uneven aged and are characterized by a complex mosaic of different ages or cohorts federici et al 2008 frate et al 2015 instead in most disturbed forests trees regrowing after the disturbance results in even aged stands at least for the first years after the disturbance this is particularly true for clearcuts in coppice forest chirici et al 2020 which represent the most common forest disturbance in italy based on francini et al 2022 a representing 80 of all forest loggings in italy in such a situation forest age can be used to predict gsv growth in disturbed stands using the data in the national yields table collection we used forest categories specific svm models to predict the gsv based on forest age with a radial basis kernel function svm approaches are known to be robust against outliers and overfitting and are well suited for approaching problems with a limited amount of training data these algorithms can generate non linear decision surfaces by mapping the data into a high dimensional space through non linear mapping functions called kernel functions cortes and vapnik 1995 pal and mather 2005 allowing the separation of the data through linear hyperplanes dixon and candade 2008 among the kernel functions one of the most used is the radial basis function which has two tuning parameters c regularization parameter and ɣ kernel width kavzoglu and colkesen 2009 an in depth explanation of svm based models and kernels is presented in smola and schölkopf 2004 and kavzoglu and colkesen 2009 implementations of svm models in rs can be found in mountrakis et al 2011 in this study the parameters of svm and radial kernel c ɣ were determined by bootstrap cross validation with 25 iterations using the grid search method by selecting the pairs of parameters that produce the lowest cross validation rmse among an exponentially growing sequence of the parameters c 21 2 21 25 ɣ 2 5 2 4 2 for each bootstrap iteration and species the rmse was calculated as per eq 4 and the model which obtained the lowest rmse was chosen as the final model in disturbed areas identified by the 3i3d algorithm the gsv was computed for each year and forest category by applying the category specific svm models fitted from the yield tables data the ysld for the year n was used as the independent variable to predict the gsv n in each disturbed pixel obtaining a gsv map of forest disturbances for each year between 2005 and 2018 the complete gsv n map was produced by overlaying the gsv n maps of disturbed and undisturbed forests the svm models led to an average rmse of 35 9 with a maximum of 64 for the mixed forests with the prevalence of coniferous and a minimum of 5 for the maple ash hornbeam mixed forest as for the richard models we observed significant variations depending on the number of fertility classes in yield tables 3 4 carbon stock conversion once estimated the gsv amounts of agb are consequently assessed for every forest typology starting from the gsv the agb mg d m ha 1 is calculated through equation 3 following the approach presented in federici et al 2008 carbon stock maps were derived from agb maps by applying the default factor for carbon fractions of 0 47 ipcc et al 2006 the pixel level predictions of gsv and stocked carbon were aggregated at the regional level for each year 4 results 4 1 gsv and carbon stock estimation the spatial approach for estimating annual gsv and above ground carbon pool was applied to produce 23 m resolution yearly pixel level estimates from 2005 to 2018 based on our results the gsv increased in italy by 522 million m3 moving from an average of 130 m3 ha 1 to 180 m3 ha 1 gsv and above ground carbon stocks time series are reported in annex ii carbon stock increased by 206 million of t in the same period moving from 36 9 mg c ha 1 to 59 3 mg c ha 1 with an average accumulation rate of 14 7 mln mg c y 1 regionally most of the gsv and carbon gains dominate mountain landscapes of the alps and apennines mountains in the years 2005 2018 among all forest categories beech forests accumulated the most gsv with about 3926 mln of m3 corresponding to 54 mln mg c of above ground carbon stored about 28 3 of the total carbon absorbed by national forests followed by mixed broadleaf forests 34 mln mg c about 18 of the total and the fir spruce forests 23 mln mg c 12 of the total regardless of the forest category in the study period carbon accumulation is reflected mainly in the increase of the carbon density rather than the increase of the total forest area which amounts to 145 000 ha according to the clc maps northern regions trentino alto adige piemonte lombardia veneto friuli venezia giulia have the highest gsv accumulation in terms of absolute and per hectare figures accounting for 54 of the national total other regions with significant gsv accumulation are toscana sardegna and emilia romagna contributing 20 to the national gsv growth each up to 20 mln m3 in the study period in contrast most southern regions molise campania puglia basilicata sicilia show the least accumulation of gsv less than 8 mln m3 between 2005 and 2018 carbon uptake has similar patterns exhibiting higher absolute and per hectare storage in many northern regions trentino alto adige piemonte lombardia veneto and lower in the southern ones molise puglia umbria fig 4 also at the regional level the accumulation of gsv and carbon stock is primarily driven by the increase of gsv and carbon density rather than the total forest area this is probably due to a decrease in the harvested area over the last two decades which allowed for significant growth in gsv per unit area francini et al 2022a b in fig 3 is reported the overall absolute accumulation of gsv and carbon stock at the regional level over the study period 4 2 validation and comparison of our results during the optimization phase the 23 m resolution pixel level estimations of gsv estimates obtained applying the best configuration of our models where compared against the gsv measured in the field in 9258 independent plots acquired in different years in the period 2006 2018 from such comparison the average rmse was 57 ranging between 89 6 in 2009 and 34 in 2015 fig 5 the bias across years was 3 7 m3 ha 1 with the minimum in 2010 0 2 m3 ha 1 and the maximum in 2013 and 2015 70 3 and 60 2 m3 ha 1 these values are in the range of previous experiences immitzer et al 2016 chirici et al 2020 vangi et al 2021 pixel level estimations for the year 2015 where aggregated for administrative regions and compared with the official 2015 nfi estimates infc 2021 resulting in a 6 2 and 1 1 difference at the national level for gsv and carbon stock respectively calculated as the mean value of the difference between predicted and observed results we obtained an rmse of 19 5 and 17 8 at the regional level and an r2 of 0 94 and 0 92 for gsv and carbon stock respectively fig 6 rmse was calculated as 5 r m s e n f i i 1 n y n f i i y i ˆ 2 n r g where n r g is the number of italian regions y n f i i is the official nfi value of gsv and carbon stock for the i th region and y i ˆ is the aggregated estimation of gsv and carbon stock produced by the spatial approach for the i th region the data for the comparison against the 3rd italian nfi infc 2021 are presented in annex iii finally following the same procedure our gsv predictions aggregated for italian regions were compared with official estimates of italian ghg inventories for 2005 2018 obtaining an overall rmse of 28 6 and an r2 of 0 77 with a growing trend over time here the rmse was calculated as 6 r m s e g h g i 1 n y g h g i y i ˆ 2 n r g where n r g is the number of italian regions y g h g i is the official ghg inventoriy value of gsv and carbon stock for the i th region and y i ˆ is the aggregated estimation of gsv and carbon stock produced by the spatial approach for the i th region the carbon stock was also compared against the official italian ghg inventory lulucf sector forest land remaining forest land category for the same period ispra 2021a b yielding an r2 of 0 88 and an overall rmse as per equation 6 of 23 1 and 17 2 among years and regions respectively as for the gsv consistency with official estimates has worsened over the years while at the regional level reached the minimum in piemonte rmse 2 5 and the maximum in trentino alto adige rmse 48 6 thirteen out of 20 regions showed an rmse less than 15 with seven regions less than 10 fig 7 reports the regional comparison between our results and the official estimates from the national ghg inventory ispra 2021b b regarding gsv and carbon stock 5 discussion the main objective of the study was to develop a new spatial approach for producing wall to wall high resolution yearly gsv and carbon stock predictions between consecutive nfi field measurements exploiting remotely sensed and auxiliary data which could be used for operational application and to respond to international reporting tasks empirical models developed by interpolating data from nfi field plots could produce updated estimates only for short term predictions for which growth conditions such as climate and management regimes are expected to be stable peng 2000 moreover in such models stand variables are driven by the age of the forest but in natural conditions growth is strictly related to species and local environmental conditions for these reasons our novel approach for estimating carbon stocks and changes at the national and regional level in the above ground carbon pool is driven only by nfi gsv data and yield models the yearly wall to wall maps of gsv and carbon stock can support reporting activities and forest management at any scale by aggregating pixel level predictions producing small area estimations for example using the estimators proposed by chirici et al 2020 our results agreed with those reported by the official italian ghg inventory and show an increasing trend in the above ground carbon pool that reflects both the expansion of forest areas according to nfi in the period 2005 2015 forest areas increased by 58 692 ha y 1 approximatively 0 5 of total forest area in 2015 and the increased growing stock resulting from a forest harvest rate lower than the current increment the organic carbon in the above ground biomass of the italian forest exceeded 566 million mg c in 2018 with a different contribution of regions and forest categories in terms of fixed organic carbon gsv and carbon stock distribution among forest categories comply with the 2020 italian fao fra report for 2005 and 2010 with beech and spruce fir forests accounting for 40 and 36 of the total gsv and carbon stock respectively due to their limited area nationwide the forest categories contributing the least to carbon storage less than 0 5 of the total are exotic conifers broadleaf plantations wood arboriculture and riparian formations it is worth noting that other wooded vegetation in open and shrublands has a relatively significant contribution to terrestrial carbon sinks storing more than seven mln of mg c of carbon approximately 5 of the national total large forest areas characterize regions with the most gsv and carbon stock and highly carbon storing forest categories beech mixed broadleaf fir spruce larch mediterranean pines exotic plantations despite most of these categories are subject to intense forest harvesting as resulted from 3i3d disturbances maps in this regard it is worth noting that the major disagreement between our approach and the official ispra estimates is found in the northern regions and particularly in trentino alto adige which leads to the maximum relative rmse for which the ghg inventories estimate a decrease in gsv and carbon stock over the study period this mismatch is primarily due to underestimating the number of forest disturbances especially in high forest stands of the main mountain ranges where silvicultural treatments are based on continuous canopy cover approaches that are difficult to detect by optical satellite imagery the lack of fire data compounds the underestimation of disturbances before 2007 and after 2017 moreover for the autonomous provinces of trento and bolzano the database of forest fires includes only fires greater than 20 ha potentially increasing the underestimation of the total number of disturbances without offsetting for forest harvests and forest fires the gsv builds up rapidly driven by the increase of the current increment of highly productive forest categories leading to significant overestimates and large values of rmse however in trentino alto adige the mismatch between our results and the 2015 nfi is less evident with an underestimation of only 13 and 15 for gsv and carbon stock respectively another source of uncertainty is the overestimation of the current increment with the richard first derivative compared to the 2nd nfi field measurement this finding contrasts with the underestimation reported by federici et al 2008 compared to the 1st nfi data the discrepancy between the predicted and reported current increment of nfi is likely driven by three factors i an outdated collection of national yield tables which no longer reflect the country s real average quality of forest sites ii richard s first derivatives were fitted against all species quality class data in yield tables and since species data were aggregated to match the species composition of each forest category as closely as possible the predicted current increment of less productive species can be overestimated especially for mixed forest categories iii the overestimation of the gsv in the initial 2005 gsv map used as the independent variable for richards functions this evidence was reported by vangi et al 2021 svm models generally performed better than the richards function for the same forest categories except for chestnut hygrophilous formations exotic broadleaf plantations larch and stone pine categories for those forest categories the svm model underestimates gsv for older age classes for instance over ten years for exotic broadleaf plantations and riparian formations 25 years for chestnut and more than 80 years for the remaining forest categories hence the effect of underestimation only affects categories of hygrophilous formations and non native species plantations as the years since the last disturbance can be at most equal to 13 years 2018 2005 during the optimization phase using the dataset of independent field plots the differences between pixel level measured and estimated gsv increased over time this bias with under predictions for plots with high values of gsv especially for years 2009 2012 and 2015 can be caused by the well known saturation effect especially in dense crown cover and steep terrain nilsson et al 2017 giannetti et al 2018 chirici et al 2020 such dataset was acquired for those years in forests managed for productive purposes or more in general with an average gsv much greater 420 m3 ha 1 than those measured in infc plots 140 m3 ha 1 in this phase the largest bias was observed in southern regions and in the islands that are characterized by sparse mediterranean vegetation where modeling the gsv resulted more difficult in previous experiences too d amico et al 2021 vangi et al 2021 but it is important to note that wall to wall spatial predictions from nfi field observations should never be used at the pixel level since the single pixel predictions may be affected by a consistent bias mcroberts and tomppo 2007 however when we aggregated the pixel level estimations at the regional level we obtained very satisfying results consistent both with the italian ghg inventory and the 3rd nfi infc 2021 it is worth noting that concerning the 3rd nfi in 2015 the carbon stock difference at the regional level was minor than the gsv difference fig 7 this is interesting because it proves that the species specific bef and wbd together with the spatial distribution of forest categories can compensate for the overestimation of gsv under this point of view our estimates are conservative since the approach neither overestimates increases nor underestimates decreases in carbon stocks with respect to the nfi official estimations as soon as new high resolution forest types maps will be available the use of the clc maps should be reconsidered several studies have already highlighted the limitations of clc maps in forestry primarily because of the wide mmu of 25 ha seebach et al 2012 vizzarri et al 2015 vangi et al 2021 6 conclusions to the best of our knowledge this is the first study to provide yearly high resolution gsv agb and carbon stock wall to wall time series maps for the whole national territory in italy allowing an in depth analysis of the forest carbon stock changes consistently with the ipcc guidelines the spatial nature of our results enables small scale estimates by aggregating individual pixel predictions enhancing the spatial resolution of traditional nfi design based estimates chirici et al 2020 and can be embedded into decision support systems to support sustainable forest management and precision forestry activities furthermore the knowledge of the spatial distribution of carbon among forest categories can be of fundamental importance under the climate mitigation goals of the paris agreement unfccc 2015 the growing need for new information and technological advances is driving the rapid evolution of forest monitoring and assessment however the exploitation of the latter and their implementation within international reporting processes should be evidence based corona 2018 this study provides an innovative spatial framework to track gsv and carbon stock changes between nfi surveys at local to national scales providing a reliable monitoring approach to meet the increasing interests of non government and private entities in carbon offset investments our new method incorporates forest disturbance between surveys such as forest fires and harvesting thanks to landsat based time series metrics exploited by the 3i3d unsupervised change detection algorithm which has already proved to be a better solution in the mediterranean environment than previous algorithms francini et al 2021 the 3i3d algorithm here demonstrated to be a valid solution for deriving forest age to track the gsv regrowth after disturbances by applying age dependent relationships in yield tables the approach could be improved with 2015 nfi data as well as updated information on allometric models and yield tables allowing better model calibration and quality assurance routines it is worth remembering that although ghg inventories are not measured on the ground they represent official data sources used for national and international reporting activities nevertheless the availability of ground based carbon content data to calibrate and validate the method is desirable for this reason as soon as they will be available 2015 nfi carbon stock ground data will allow the evaluation of the pixel level performance for the carbon stock map for 2015 providing insight into the method s effectiveness the availability of an official high resolution national forest map and wall to wall multitemporal als data could also be fundamental for improving the quality of gsv and carbon stock spatial estimations author s contributions conceptualization g c e v and g d methodology e v g d s f software e v validation e v formal analysis e v g d investigation e v g d data curation e v g d writing original draft preparation e v all authors have contributed to the drafting of the manuscript all the authors assisted in the quality control and revisions of the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was partially supported by the following projects multifor multi scale observations to predict forest response to pollution and climate change prin 2020 research project of national relevance funded by the italian ministry of university and research prot 2020e52ths superb systemic solutions for upscaling of urgent ecosystem restoration for forest related biodiversity and ecosystem services h2020 project funded by the european commission number 101036849 call lc gd 7 1 2020 efinet european forest information network funded by the european forest institute network fund g 01 2021 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105580 
25487,this paper presents an approach to automated model integration into the nasa land information system lis lis defines interfaces for integrating models which do not often have standardized coupling interfaces additional software development is needed when implementing models the automated approach simplifies these barriers lis standardized modeling interface lsmi is proposed as a prerequisite fortran subroutine which is directly callable by lis and calls model physics data communication between lis and a model can be connected through an lsmi the lsmi arguments can be functionally categorized taking their specifications implementation code can be generated based on code templates with programming logic and language elements incorporated an excel vba based tool lis mit has been developed for automating lis model implementation since the development of lsmi requires basic skills lis mit can significantly lower the development difficulties and efforts the proposed approach can be generalized and applied for integrating models into other modeling environments keywords automated model integration code generation model implementation nasa land information system 1 introduction the land information system lis https lis gsfc nasa gov kumar et al 2006 peters lidard et al 2007 is a software framework for high performance terrestrial hydrology modeling and data assimilation the lis software suite consists of three modeling components the first land surface data toolkit ldt arsenault et al 2018 is a formal environment that handles the data related requirements including parameter processing the second lis is the core of the framework which encapsulates physical models data assimilation algorithms optimization and uncertainty estimation algorithms and high performance computing support the third land surface verification toolkit lvt kumar et al 2012 is a formal model verification and benchmarking environment as a computational and science enabling platform many agencies and projects have adopted lis for applications involving water resources management weather and climate prediction and flood and drought simulation for example the us air force 557th weather wing and noaa s national centers for environmental prediction ncep uses lis based operational land analysis systems for weather and climate model initialization projects such as the north american land data assimilation system nldas the global land data assimilation system gldas and the famine early warning system network fews net peters lidard et al 2007 mitchell et al 2004 rodell et al 2004 mcnally et al 2017 use lis based environments to support drought monitoring and food security applications from a software engineering perspective the success of lis has been enabled by its extensible architecture which defines a suite of interfaces for adding models and algorithms these interfaces have facilitated a large set of land surface models data assimilation techniques inverse modeling approaches and flow routing models among others regarding models a set of interfaces have been developed for integrating land surface models lsms hydrological models and lake models since the start of the nasa lis project in 2002 many models including noah chen and dudhia 2001 noah mp niu et al 2011 vic liang et al 1994 mosaic koster and suarez 1996 catchment koster et al 2000 ruc smirnova et al 2000 sac htet koren et al 2004 and jules best et al 2011 have been integrated into lis with them these models enrich the capacity of lis to simulate land surface processes and allow an ensemble structure to represent the uncertainty in simulating land surface states once integrated the models can individually or collectively work with the overall facilities provided by the lis platform such as meteorological forcing datasets data assimilation techniques inverse modeling and flow routing algorithms these interoperable features have enabled lis applications in high resolution terrestrial simulations and assimilation studies kumar et al 2014 2016 2017 mcnally et al 2017 model implementation explicitly representing the software development of integrating a model into lis establishes the relevant communication and workflow between an lsm and lis which is the modeling framework for the convenience of description a modeling framework is noted as a caller hereafter on the contrary a model being integrated is noted as a callee the caller actively coordinates data exchange while the callee passively takes inputs and provides outputs in this paper model integration specifically means the software approach of making a model and a modeling framework work together here we distinguish between two types of model integration weak coupling and strong coupling the former involves separate executables that communicate through external data files the latter includes all model components in one executable and communicates data through internal subroutine calls weak coupling implies the loose integration of model components usually it can be implemented with programs such as mediators connectors and drivers dunlap et al 2013 and shell scripts that coordinate data flows among multiple executables through data files for example the data assimilation research testbed dart raeder et al 2012 is a loose coupling system developed by the national center for atmospheric research ncar it runs model components as separate executables on the other hand strong coupling means full integration and all the model components have to work at the source code level generally it is more challenging to do the strong coupling than the weak coupling from a software development perspective however we can obtain better computational performance with the strong coupling approach which helps with high resolution and large domain terrestrial simulations lis is a high performance modeling system and therefore models have to be fully integrated into it for the highest efficiency and flexibility possible in other words model integration should be at the source code level to leverage the computational subsystems in lis scientific functions such as model simulation data assimilation and inverse modeling can be performed more efficiently by having the models tightly integrated within lis lis model implementation requires a thorough knowledge about both lis and models a model implementer has to understand the programming interfaces of both sides since data communication is done through subroutine arguments except for a small portion of lis developed in c for supporting generic model interfaces lis is mainly developed in fortran 90 95 internal data exchange among lis components is based on the earth system modeling framework esmf hill et al 2004 components and subroutines mpi message passing interface is used for both point to point and collective data communication in parallel computation based on these software infrastructures lis has defined a set of programming interfaces in fortran for model integration in addition to learning the lis interfaces the model implementer has to understand the model as well since model developers own their models and make different decisions at different points in the model development process there are no standardized coupling interfaces in hydrological and land surface modeling communities therefore any model implementation is a time consuming exercise typically the model implementers have to fully understand the software protocols of the lis framework and the model besides scientific knowledge this is a challenge of model implementation though conforming to the lis lsm interfaces is necessary for a successful lsm implementation the required code restructuring is not always straightforward having an additional layer that facilitates this interface compliance would expedite the model development efforts conceptually this similar to the software layer of the national unified operational prediction capability nuopc which has been used in coupling earth system models moghimi et al 2020 sun et al 2019 developed as an add on to the standard esmf library the nuopc layer consists of generic code for building model interoperability nuopc compliance can be met through a fortran module specializing the generic nuopc model component for a particular model since the fortran module is a small software layer sitting on top of the model it is called a nuopc model cap once the cap is developed the model gains interoperability with other nuopc compliant models the nuopc software infrastructure handles all underlying data flows among component models of the coupling system a similar software infrastructure like the nuopc layer is needed for lis model implementations it would facilitate model implementation and save months to years in development time model implementation can be automated if a nuopc like layer is adaptively generated between lis and the model automated programming has been used in the software industry for decades nowadays most integrated development environments ide can generate code frameworks for graphical user interfaces gui such as the code wizards in microsoft visual studio developers only need to specify gui elements at a higher level and then the corresponding source code can be generated automatically the automated approach saves significant time for coding debugging and testing for the lis model implementation we have abstracted a set of general logic of software development and consolidated it into a tool developed with excel and its embedded vba visual basic for application environment using this tool we can implement models into lis much faster than manual conventional software development in this paper we present the approach of automated model integration at the source code level for implementing models into lis this approach is not limited to lis applications which can help model integrations in other fields in the rest of the paper the logical basis of this approach is introduced in section 2 in section 3 we define a standardized coupling interface the procedures of automated model implementation are described in section 4 in the end applications are demonstrated in section 5 2 theoretical and technical bases 2 1 mathematical abstraction of surface models all surface models including land surface models hydrological models and lake models can be described as state space systems as shown in eq 1 the state variables x k and the model outputs y k at time step k can be derived based on the meteorological variables at the current time step u k the state variables at the previous time step x k 1 and the model parameters θ 1 x k y k f u k x k 1 θ in eq 1 f represents model structure and all variable notations represent vectors instead of individual values the meteorological variables u k usually meteorological inputs represent boundary conditions of a land surface control volume the state variables x k 1 including variables such as soil moisture soil temperature and snow depth represent the initial state of the control volume model parameters θ describe the characteristics of the modeling unit soil and vegetation types can be treated as model parameters if look up tables are used for categorical parameterization model outputs variables y k are usually energy and mass fluxes getting in or out of the volume such as sensible and latent fluxes surface runoff and baseflow as well as evapotranspiration the model computes both x k and y k but x k will feedback into the model at the next time step eq 1 reveals essential aspects regarding model implementation first of all information has to flow back and forth between the modeling framework m e g lis and the model m f u x θ y m should pass u k x k 1 and θ to m and then retrieve x k and y k from m after running model physics second m is responsible for reading in u k and θ for feeding them to m last m needs to initialize m at the first time step x 0 has to be set by m either with default values cold start or with values from a spin up run restart in order to restart a model m should also be able to write model states into restart files 2 2 lis surface model interfaces lis is a highly modular flexible object oriented and component based framework fig 1 shows the lis software architecture that is related to surface model implementation the lis core manages generic utilities such as time configuration i o domain decomposition parallel computation and information logging based on user configuration it turns on or off specific functional components and controls the overall behavior of lis lis simulates terrestrial hydrology through land surface models hydrological models and lake models the basic behaviors of running a surface model in lis have been encapsulated in programming interfaces as illustrated in fig 1 a total of nine interfaces have to be implemented as fortrans subroutines for establishing data flow between lis and the surface model once registered lis uses these subroutines as callback functions to drive the surface model the read configuration subroutine is for reading simulation specifications from a lis configuration file the initialize subroutine initializes all the variables defined in a data exchange layer it allocates memory for all the allocatable variables the setup subroutine sets up time invariant model parameters from files whereas the dynamic setup subroutine sets up time varying model parameters the read restart subroutine reads model state variables from a restart file for initializing the model the write restart subroutine writes model state variables into a restart file the transfer forcing subroutine acquires meteorological variables from lis the model main is the primary connector which calls the surface model it passes configurations parameters meteorological variables and initial states from lis to the model and retrieves updated model states and model outputs back to lis for writing output files the finalize subroutine is for releasing all the memory allocated in the initialize subroutine the interfaces of read configuration initialize setup and read restart are called only once at the beginning of a lis run while the interface finalize is also called only once at the end of the lis run the interfaces of transfer forcing dynamic setup and model main are called at every time step the interface write restart is called periodically according to a restart writing time interval prescribed in the lis configuration file 2 3 lis standardized model interface there are often gaps between a model and a target framework during model integration if their individual development has not been coordinated from the very beginning typically the framework defines a set of programming interfaces to integrate land surface models hydrological models and lake models collectively called surface models hereafter in this paper since the surface models are not purposely developed to run within lis there are many structural inconsistencies between them for example lis employs a time stepping approach in which model simulations for every grid point in the domain are conducted at a given time step before proceeding to the next time step this order may be different in the native formulations of the surface models some surface models may even not have clear interfaces with sufficient separation and abstraction of i o model physics parallelization and so on the gaps can be addressed with a thin software layer building upon the interfaces of lis and the model since this layer follows the lis standards it is named the lis standardized modeling interface lsmi lsmi is similar to the concept of the nuopc cap it takes driving data from lis and sends them to model physics and then brings model outputs back to lis it tackles all the inconsistencies between lis and the model for example lsmi converts data types for variables if they are defined differently in lis and the model it also derives intermediate variables for the model if lis does not directly provide them defined as a fortran subroutine lsmi encapsulates all the model specific features inside and only exposes lis required variables in the form of subroutine arguments therefore the implementation effort is primarily for connecting lis and the lsmi since the lsmi follows all lis standards the software development for the model implementation can be automated once a set of software infrastructure is defined as code templates for generic operations required by the nine programming interfaces the integration software layer can be automatically created by customizing the code templates according to the lsmi arguments lsmi can be quickly developed by scientists instead of professional software engineers for example a model expert can develop an lsmi following given guidelines it is not necessary to learn software architecture data structure and all programming interfaces of lis after the lsmi is developed the rest of the model implementation can be done through automation with a software toolkit thus there will be much less cost of software development debug test and maintenance automated model implementation will greatly promote the application of lis 2 4 categorization of model inputs lis tackles model inputs differently according to their functional attributes model inputs can be categorized into three groups from a model implementation perspective control options physical parameters and meteorological variables a detailed categorization schematic is depicted in fig 2 the control options are enumerable numbers or texts such as start and end time the value of the time step etc for defining model behaviors meteorological variables are the upper boundary conditions of surface models usually including but not limited to air temperature air pressure wind speed air humidity and radiations physical parameters describe the characteristics of the modeling unit physical parameters can be further categorized into constant parameters lookup table parameters single level spatial parameters and multiple level spatial parameters a constant parameter has the same value over the entire simulation domain which can be either a physical constant such as the gravitational acceleration g 9 8 m s 2 or a coefficient in an empirical equation the lookup table parameters are an array of values depending on indices such as soil texture class and land use type for example the noah land surface model has a vegetation parameter table and a soil parameter table spatial parameters depend on locations and are usually represented in maps such as soil texture maps and land use maps spatial parameters are single level if there have only one map layer otherwise they are multiple level if they have multiple map layers according to temporal variability spatial parameters can be further categorized into time variant and time invariant types 2 5 data communication between lis and surface model there are three software layers in a lis model implementation the lis layer the data exchange layer and the model layer as shown in fig 3 in practice the data exchange layer is constructed with two fortran modules a domain data module and a tile data module the former is defined for the entire computation domain and the latter is defined for tiles a tile is the smallest modeling unit of lis associated with a land use type and its fraction in a grid koster and suarez 1992 there can be multiple tiles in a grid according to the distribution of land use types within it the domain data module includes all location independent values such as the constant parameters the lookup table parameters and the default values for initializing models the tile data module defines all the parameters the meteorological variables the state variables and the output variables whose values depend on spatial locations in both modules data are organized with a fortran type data structure an array of the tile data type is defined inside the domain data module to handle all tiles in the computation domain the lis layer works as a driver when running a model firstly it reads in the model parameters and the initial values of state variables these values are assigned to the model parameters and the initial state variables defined in the domain data module through the initialize interface at every time step prior to running model physics the lis layer sends values to the meteorological variables and the state variables in the tile data array of the data exchange layer these actions are taken through the interfaces transfer forcing and model main these data are then passed to the lsmi for calling model physics in the subroutine of model main after running model physics updated state variables and output variables are retrieved from the lsmi and then sent back to the data exchange layer still in the subroutine of model main there are two advantages to implementing the data exchange layer with two modules 1 memory efficiency and 2 global data accessibility the location independent variables are members of the domain data module these variables are scalars the location dependent variables are members of the tile data module these variables are arrays by separating the location independent and the location dependent variables memory is saved by not allocating large arrays containing single values and the variables in the exchange layer are accessible to all the functional components of lis such as data assimilation and flow routing therefore it is straightforward to hook the model being implemented to these lis components 3 procedures of automated model implementation 3 1 development of lsmi for surface model as mentioned above the lis interfaces are designed generally while each model s interface is specific as a software cap the lsmi is an adapter sitting between them it has to be developed case by case so being dually compatible with both sides the lsmi is typically implemented as a fortran 90 subroutine more specific guidelines are listed as follows first as illustrated in fig 2 the subroutine arguments have to be defined with intrinsic fortran 90 data types such as integer real character and logical if the model takes arguments defined in extended data types e g the fortran type they should be defined inside the lsmi subroutine their member variables should be initialized with the lsmi arguments after calling model physics corresponding lsmi arguments have to be updated for returning values to lis second multidimensional arguments have to be defined as fixed size arrays their dimensions should be arguments of the lsmi subroutine in addition to declaring these arrays shapes they are needed for allocating memory for corresponding variables defined in the data exchange layer during code generation third all subroutine arguments must have their intent attributes specified in inout or out model inputs should be in for only providing data model states should be inout for both providing and returning data model outputs should be out for only returning data with the intent attributes the lsmi subroutine is more readable in addition the intent attributes are used for determining variable categories during code generation fourth the subroutine should use a set of predefined names for meteorological variables including air temperature k air pressure pa wind speed m s specific air humidity kg kg downward longwave radiation w m 2 and shortwave radiation w m 2 and precipitation kg m 2 s these names have been registered in the model implementation tool for connecting the lis meteorological input during code generation last but not least the subroutine calls model physics therefore lis only needs to interact with the lsmi subroutine instead of the actual model once the lsmi subroutine is developed according to these guidelines the automated model implementation tool can understand it to obtain all the required information for code generation by analyzing its arguments 3 2 code generation based on lsmi two fortran modules e g the domain data module and the tile data module are generated to construct the data exchange layer 3 2 1 generation of the data exchange layer as described in section 2 4 the model inputs are categorized according to location dependence spatially distributed variables are defined for all land tiles of the simulation domain on the contrary spatially uniform variables are defined as scalar values for the entire simulation domain the meteorological variables state variables output variables and spatially distributed parameters are defined as member variables of a fortran type in the tile module the control options constant parameters and lookup table parameters are defined in another fortran type in the domain data module an array of the tile type is defined in the domain type for handling internal tile data fig 4 illustrates the two fortran types and their mapping relationships with the lsmi subroutine arguments a naming convention is created for better code readability all the modules and subroutines are named after the model itself e g modex in the domain module the fortran type is named as modex struc the model s version number is a part of the type name as a postfix for integrating multiple versions of the same model the tile type is named similarly in the tile data domain the name of the tile type is in lower case for distinguishing from the name of the domain type which is in upper case all the variables with multiple values of the same type are defined as arrays if an array has its dimension prescribed as an integer type argument of lsmi it is defined as a fixed array otherwise it is defined as an allocatable array and its size will be dynamically determined based on related arguments of lsmi fortran pointer variables are used to define arrays instead of allocatable variables either way the definition of the lsmi subroutine should have provided sufficient information for determining the dimensions of the arrays the mapping from the lsmi arguments to the member variables of the data types is illustrated in fig 4 all the member variables can be traced back to the arguments therefore it is feasible to generate code for the two fortran modules in an automated way all required information such as variable names dimensions and data types are provided by the arguments and their attributes of declaration lis supports two start modes restart and cold start every state variable is initialized uniformly all over the simulation domain when running in the cold start mode for such a case the initial state variables are independent of location therefore initial state variables are defined in the domain type for handling constant initial values on the contrary the tile type defines a set of initial state variables for initializing the model spatially from a restart file generated in a previous run 3 2 2 implementation of the lis surface model interfaces nine fortran subroutines are to be generated to implement the lis surface model interfaces the logic of code generation is based on the specifications of the lsmi subroutine s arguments as illustrated in fig 5 the subroutine readconfig implements the read configuration interface that reads the lis configuration file for setting up a simulation lis users specify control options constant parameters lookup table parameters and initial model states in the lis configuration file the code of the subroutine readconfig includes data reading and checking blocks built upon esmf subroutines for each type of lsmi argument a code template has been defined for data reading and sanity checking inside the model implementation tool therefore the code generation for the subroutine readconfig is a process of customizing the code templates according to the lsmi argument list the subroutines init and finalize implement the interfaces of initialize and finalize which allocate and release computer memory for the allocatable variables in the domain data module and the tile data module the allocatable variables and their dimensions can be determined according to the attributes of the lsmi arguments thus allocation and deallocation statements can be generated for these two subroutines the subroutines setup and dynsetup are the implementation of the interfaces of setup and dynamic setup which read spatial parameters from a netcdf rew and davis 1990 file produced with ldt all spatial parameters correspond to 2 dimensional or 3 dimensional arrays in netcdf datasets with a predefined naming convention as long as data templates are defined for handling the netcdf datasets the code of these two subroutines can be generated according to the spatial parameters in the lsmi argument list the subroutine f2t is the implementation of the transfer forcing interface which retrieves meteorological variables from lis infrastructure and sends them to the data exchange layer lis has a registered set of meteorological variables and many readers to process them from various datasets since the meteorological variables in the lsmi argument list are named consistently with the lis registered meteorological variables code templates can be defined and customized for handling the meteorological variables therefore the subroutine can be automatically generated according to the meteorological variables in the lsmi arguments the subroutines readrst and writerst implement the interfaces of read restart and write restart which reads or writes the state variables to or from a lis restart file in the netcdf format with predefined code templates and a naming convention code for both reading and writing netcdf variables can be generated automatically for all the state variables defined with the inout attribute in the lsmi arguments the main subroutine implements the model main interface the primary connector between a model and lis it grabs all model inputs from the data communication layer and runs model physics by calling the lsmi subroutine for a subdomain in the case of parallel computation after the model simulation is done for a time step the main subroutine sends all model states and outputs to lis for operations such as data assimilation flow routing and output writing the state variables in the data exchange layer are also updated for proceeding to the next time step all of these procedures correspond to the arguments of the lsmi subroutine therefore the main subroutine code can also be automatically generated by customizing predefined code templates according to the lsmi argument list overall the code generation of the lis interfaces starts from the specification of lsmi by analyzing previously implemented models code templates have been created and integrated into the implementation tool as long as the lsmi subroutine follows the guidance in section 3 1 the argument list will be sufficient input for the model implementation tool for code generation 4 the model implementation tool the lis model implementation tool lis mit has been developed for automating the software development of lis model implementation lis mit has two main functions one is to provide a set of graphical user interfaces for conveniently describing lsmi arguments the other is to generate fortran code and documentation for the nine subroutines explained in section 3 2 2 4 1 development environment lis mit is developed using visual basic for application vba within microsoft excel because it has three desirable features to meet the goals of lis mit first excel is both user friendly and well known many scientists already use it for data analysis the spreadsheet can be easily customized for describing the attributes of the lsmi arguments with simple guidance users can work with lis mit quickly the built in features of excel such as font text color and size and comments help users make notes during development this is important because the model implementation is an iterative effort it is helpful for users to improve their implementation designs based on earlier versions recorded in comments and annotations second vba has a rich set of functions for string manipulation from a programming perspective code generation is all about string operations more specifically it is the customization and assembling of general purpose code templates based on the argument list of lsmi once the code templates are embedded in lis mit the vba string functions are sufficient for automated code generation third excel is cross platform available on microsoft windows and apple macos for both operating systems excel embeds vba for macro and add in development thus vba and excel provide a familiar yet powerful programming environment accessible on most scientists personal computers for producing the complicated fortran code needed to incorporate a model into nasa s land information system 4 2 lis mit user interfaces the graphical user interface gui of lis mit includes five excel spreadsheets and thirteen command buttons in the add ins tab as illustrated in fig 6 when an excel macro enabled workbook xlsm file with lis mit code is opened the add ins tab is turned on automatically with the thirteen command buttons loaded the source files for model integration can be generated individually or altogether by clicking the command buttons 4 2 1 description worksheet the description worksheet is the starting point of a lis mit project as shown in fig 7 it asks users to input code preamble model name version number lsmi subroutine name and developer s name lis mit uses such information to name variables subroutines modules and source files lis mit does not directly analyze the source code of lsmi and the lsmi subroutine name is for generating the code of the subroutine main to call model physics lis mit produces both fortran code and documentation where the latter is in the form of comment lines usually users put institute related legal statements to the code preamble such as copyright information contact information and a code sharing policy lis mit can integrate land surface models hydrological models and lake models with slightly different code templates therefore users have to specify the type of model to be implemented in this worksheet lis mit acknowledges the developer of lsmi in the comment lines of the generated code which can be extracted and put into technical documentation 4 2 2 variable worksheet the variable worksheet as shown in fig 8 asks users to input the arguments of lsmi it prompts users to specify the names data types dimensions intent attributes categorical attributes units and descriptions of the lsmi arguments this worksheet is the main input interface of lis mit it takes the lsmi specifications as strings which can be understood and processed by lis mit as mentioned in section 3 1 all lsmi arguments must be declared with fortran intrinsic data types hence this worksheet only allows character integer and real types for the datatype column for numerical variables they have to be either scalar values or arrays in the dimension column 1 indicates a scalar value and otherwise indicates an array for example the variable sldpth has a dimension of nsoil it means that sldpth is an array of fixed size nsoil the intent column asks for the intents of arguments which supports three modes in for model inputs inout for state variables and out for output variables the attribute column asks for the categories of lsmi arguments including input state variable output and lis built in variable model inputs are further categorized into the types described in fig 2 the lis built in variables include time e g year month and day and location e g latitude and longitude which are universally applicable to all models the unit column asks for the lsmi arguments units which will be in comment lines after variable declarations similarly the texts in the description column also will be in comment lines using latexsyntax these comment lines explain the generated source code and document the variables from a scientific perspective 4 2 3 forcing worksheet the forcing worksheet as shown in fig 9 configures the meteorological variables it lists all the meteorological variables registered in lis users are required to mark only the variables required by the model being implemented for example if the air temperature tair is required the select cell should be set to 1 otherwise the cell should be set to 0 if the tair variable is optional the optional cell should be set to yes otherwise it should be set to no the rest of the columns are read only and for the users information to understand these meteorological variables 4 2 4 output worksheet the output worksheet as shown in fig 10 asks for information about output variables the columns of variable levels unit surface type and description must be consistent with corresponding columns in the description and variable worksheets the direction column should be filled with predefined directions such as up down in out if describing a flux or movement of the variable the lis moc name represents variables defined under the lis model output conversion which is explained in the lis developer s manual https lis gsfc nasa gov documentation lis 4 2 5 built in variable worksheet the built in variable worksheet as shown in fig 11 asks for the registration of lis built in variables if they are part of the lsmi arguments the built in variables are commonly used across models such as vegetation type soil texture type elevation slope leaf area index lai greenness background emissivity and surface roughness for example if vegtyp is the argument of lsmi describing vegetation type then users only need to put it in the model variable name column for the built in variables not used in lsmi users should leave the corresponding model variable name cells blank 5 applications several models have been implemented into lis using lis mit facilitated by the automated model integration approach months of development time have been saved lis mit did most of the software development therefore scientists are free from tedious learning and coding besides the code generated by lis mit strictly follows the coding standards of the nasa lis software consequently it is bug free well structured and easy to read much less time is spent on software debugging and testing than in traditional manual model implementation as mentioned above users only need to develop the lsmi subroutine after setting up a lis mit application based on the lsmi arguments all the lis model interfaces will be generated quickly fig 12 shows the line counts of the lsmi subroutines developed automatically and manually usually the automatically generated code does all the work for model implementation however if a model has a highly complicated structure users need to edit a few lines of the generated code which have placeholders and instructive comments made by lis mit fig 12 shows the automated percentage which is the ratio of the line count of the generated code and the line count of all code including the lsmi subroutine spgp sparse gaussian process model is a bayesian non linear regression model it approximates surface fluxes soil moisture and soil temperature upon meteorological inputs as a simulator of land surface models after developing a simple lsmi subroutine consisting of 36 lines lis mit generated all the implementation code consisting of 1610 lines the automated rate is about 98 sac htet and snow 17 are the operational models in the hydrology research research distributed hydrologic model hl rdhm system spies et al 2015 lee et al 2011 of the national weather service nws the former is a rainfall runoff model with frozen ground physics the latter is a conceptual model simulating snow accumulation and thawing an lsmi subroutine has been developed to combine them into a fully functioned land surface model the subroutine has 634 lines of code while the interface code generated with lis mit has 5427 lines the automated rate is about 90 flake is a 1 d lake model mironov et al 2010 kirillin et al 2011 designed for simulating lake evaporation and temperature in numerical weather predictions in its implementation the lsmi subroutine has 175 lines and the generated code has 1906 lines the automated rate is about 92 noah mp is one of the land surface models integrated into the weather research and forecasting wrf model powers et al 2017 the versions 3 6 and 4 0 1 have been respectively implemented into lis using lis mit since more components are included such as crop and groundwater noah mp version 4 0 1 has a more complicated software structure than noah mp version 3 6 therefore its lsmi subroutine has more lines the automated rates for these two versions are about 92 and 85 respectively ruc is another land surface model in wrf featured with rapid land surface status responses to meteorological inputs the ruc version 3 7 has been implemented into lis with lis mit the line count of its lsmi subroutine is 666 while there are 3057 lines of generated code the automated rate is about 82 awral is a land surface model developed for water resource assessment in australia wallace et al 2013 its model physics has been put into lis with lis mit since the model is written in the c programming language a c function has been developed in the fortran compatible manner as the lsmi subroutine the c function has 386 lines and the generated code has 2525 lines the automated rate is about 87 6 summary lis requires models to be fully integrated at the source level requiring broad and deep knowledge in addition to expert proficiency in fortran programming and software design this makes lis model implementation a technical challenge to scientists in this paper we described a software solution through automated model integration the solution has been implemented in a software toolkit lis mit lis mit relies on the lis standardized model interface lsmi which is an interface standard for land surface models hydrologic models and lake models by adopting this standard a model can directly interact with a modeling framework such as lis if a model does not follow the standard an lsmi compliant subroutine can be developed as a software cap for the model therefore the model can communicate with lis through the subroutine abstracted from the universal behaviors of surface models the concept of lsmi is valid for lis model integration and valuable for other model interoperations in applications an lsmi can be a fortran subroutine or c function with an lsmi software layer we can reduce the need to modify original code from the model sources so as to keep the model code integrity we created a categorization system for the surface models inputs from a data exchange perspective mappings have been built between the lis model interfaces and the lsmi arguments including control options constant parameters lookup table parameters spatial parameters meteorological variables state variables and outputs therefore generalized code templates are defined for these interfaces making automated code generation possible for developing corresponding fortran subroutines this is another foundation of the implementation tool lis mit developed with excel vba lis mit uses the excel worksheet as its user interface users are guided to describe model attributes and specify the lsmi arguments in five worksheets lis mit scans the lsmi arguments during code generation and then creates specific fortran data modules and subroutines following program logic imbodied in the code templates in addition to the fortran source code lis mit also generates descriptive comments for documentation a tremendous amount of time has been saved using the automated model integration approach as use cases spgp sac htet snow 17 flake noah mp version 3 6 and version 4 0 1 ruc version 3 7 and awral version 6 0 have been implemented into lis with lis mit the generated code accounts for 82 to 98 of development in terms of the line count of code more models such as crocus vionnet et al 2012 a snowpack scheme are undergoing implementation using lis mit in summary the automated model integration approach and the tool significantly lowered the technical threshold of lis model implementation it can reduce development time from months to weeks for surface models the software tool is available to partners of the nasa lis team in addition this approach can benefit a broader community the modeling framework is not limited to lis and the models are also not limited to land surface models hydrological models and lake models as long as coupling interfaces can be standardized integrating any model into a modeling framework can be automated based on a thin layer software cap similar to lsmi this can significantly save software development costs in the long run and therefore promote scientific research software and data availability as a utility toolkit of the land information system framework lisf available at https github com nasa lis lisf lis mit is released under the terms and conditions of the apache license version 2 0 see https www apache org licenses license 2 0 lis mit is open source and available at https github com nasa lis lisf tree master lis utils lis mit also included here are instructions for getting started with lis mit and for accessing its vba code readme txt along with a demonstration case noah mp version 3 64 0 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we wish to thank tatiana g smirnova cooperative institute for research in environmental sciences at noaa earth system research laboratory zhuo wang formerly with saic at nasa gsfc hydrological sciences laboratory and wendy sharples bureau of meteorology in australia for integrating and testing ruc 3 7 noah mp 4 0 1 and awral 6 0 respectively nasa center for climate simulation nccs provided computational support for this study 
25487,this paper presents an approach to automated model integration into the nasa land information system lis lis defines interfaces for integrating models which do not often have standardized coupling interfaces additional software development is needed when implementing models the automated approach simplifies these barriers lis standardized modeling interface lsmi is proposed as a prerequisite fortran subroutine which is directly callable by lis and calls model physics data communication between lis and a model can be connected through an lsmi the lsmi arguments can be functionally categorized taking their specifications implementation code can be generated based on code templates with programming logic and language elements incorporated an excel vba based tool lis mit has been developed for automating lis model implementation since the development of lsmi requires basic skills lis mit can significantly lower the development difficulties and efforts the proposed approach can be generalized and applied for integrating models into other modeling environments keywords automated model integration code generation model implementation nasa land information system 1 introduction the land information system lis https lis gsfc nasa gov kumar et al 2006 peters lidard et al 2007 is a software framework for high performance terrestrial hydrology modeling and data assimilation the lis software suite consists of three modeling components the first land surface data toolkit ldt arsenault et al 2018 is a formal environment that handles the data related requirements including parameter processing the second lis is the core of the framework which encapsulates physical models data assimilation algorithms optimization and uncertainty estimation algorithms and high performance computing support the third land surface verification toolkit lvt kumar et al 2012 is a formal model verification and benchmarking environment as a computational and science enabling platform many agencies and projects have adopted lis for applications involving water resources management weather and climate prediction and flood and drought simulation for example the us air force 557th weather wing and noaa s national centers for environmental prediction ncep uses lis based operational land analysis systems for weather and climate model initialization projects such as the north american land data assimilation system nldas the global land data assimilation system gldas and the famine early warning system network fews net peters lidard et al 2007 mitchell et al 2004 rodell et al 2004 mcnally et al 2017 use lis based environments to support drought monitoring and food security applications from a software engineering perspective the success of lis has been enabled by its extensible architecture which defines a suite of interfaces for adding models and algorithms these interfaces have facilitated a large set of land surface models data assimilation techniques inverse modeling approaches and flow routing models among others regarding models a set of interfaces have been developed for integrating land surface models lsms hydrological models and lake models since the start of the nasa lis project in 2002 many models including noah chen and dudhia 2001 noah mp niu et al 2011 vic liang et al 1994 mosaic koster and suarez 1996 catchment koster et al 2000 ruc smirnova et al 2000 sac htet koren et al 2004 and jules best et al 2011 have been integrated into lis with them these models enrich the capacity of lis to simulate land surface processes and allow an ensemble structure to represent the uncertainty in simulating land surface states once integrated the models can individually or collectively work with the overall facilities provided by the lis platform such as meteorological forcing datasets data assimilation techniques inverse modeling and flow routing algorithms these interoperable features have enabled lis applications in high resolution terrestrial simulations and assimilation studies kumar et al 2014 2016 2017 mcnally et al 2017 model implementation explicitly representing the software development of integrating a model into lis establishes the relevant communication and workflow between an lsm and lis which is the modeling framework for the convenience of description a modeling framework is noted as a caller hereafter on the contrary a model being integrated is noted as a callee the caller actively coordinates data exchange while the callee passively takes inputs and provides outputs in this paper model integration specifically means the software approach of making a model and a modeling framework work together here we distinguish between two types of model integration weak coupling and strong coupling the former involves separate executables that communicate through external data files the latter includes all model components in one executable and communicates data through internal subroutine calls weak coupling implies the loose integration of model components usually it can be implemented with programs such as mediators connectors and drivers dunlap et al 2013 and shell scripts that coordinate data flows among multiple executables through data files for example the data assimilation research testbed dart raeder et al 2012 is a loose coupling system developed by the national center for atmospheric research ncar it runs model components as separate executables on the other hand strong coupling means full integration and all the model components have to work at the source code level generally it is more challenging to do the strong coupling than the weak coupling from a software development perspective however we can obtain better computational performance with the strong coupling approach which helps with high resolution and large domain terrestrial simulations lis is a high performance modeling system and therefore models have to be fully integrated into it for the highest efficiency and flexibility possible in other words model integration should be at the source code level to leverage the computational subsystems in lis scientific functions such as model simulation data assimilation and inverse modeling can be performed more efficiently by having the models tightly integrated within lis lis model implementation requires a thorough knowledge about both lis and models a model implementer has to understand the programming interfaces of both sides since data communication is done through subroutine arguments except for a small portion of lis developed in c for supporting generic model interfaces lis is mainly developed in fortran 90 95 internal data exchange among lis components is based on the earth system modeling framework esmf hill et al 2004 components and subroutines mpi message passing interface is used for both point to point and collective data communication in parallel computation based on these software infrastructures lis has defined a set of programming interfaces in fortran for model integration in addition to learning the lis interfaces the model implementer has to understand the model as well since model developers own their models and make different decisions at different points in the model development process there are no standardized coupling interfaces in hydrological and land surface modeling communities therefore any model implementation is a time consuming exercise typically the model implementers have to fully understand the software protocols of the lis framework and the model besides scientific knowledge this is a challenge of model implementation though conforming to the lis lsm interfaces is necessary for a successful lsm implementation the required code restructuring is not always straightforward having an additional layer that facilitates this interface compliance would expedite the model development efforts conceptually this similar to the software layer of the national unified operational prediction capability nuopc which has been used in coupling earth system models moghimi et al 2020 sun et al 2019 developed as an add on to the standard esmf library the nuopc layer consists of generic code for building model interoperability nuopc compliance can be met through a fortran module specializing the generic nuopc model component for a particular model since the fortran module is a small software layer sitting on top of the model it is called a nuopc model cap once the cap is developed the model gains interoperability with other nuopc compliant models the nuopc software infrastructure handles all underlying data flows among component models of the coupling system a similar software infrastructure like the nuopc layer is needed for lis model implementations it would facilitate model implementation and save months to years in development time model implementation can be automated if a nuopc like layer is adaptively generated between lis and the model automated programming has been used in the software industry for decades nowadays most integrated development environments ide can generate code frameworks for graphical user interfaces gui such as the code wizards in microsoft visual studio developers only need to specify gui elements at a higher level and then the corresponding source code can be generated automatically the automated approach saves significant time for coding debugging and testing for the lis model implementation we have abstracted a set of general logic of software development and consolidated it into a tool developed with excel and its embedded vba visual basic for application environment using this tool we can implement models into lis much faster than manual conventional software development in this paper we present the approach of automated model integration at the source code level for implementing models into lis this approach is not limited to lis applications which can help model integrations in other fields in the rest of the paper the logical basis of this approach is introduced in section 2 in section 3 we define a standardized coupling interface the procedures of automated model implementation are described in section 4 in the end applications are demonstrated in section 5 2 theoretical and technical bases 2 1 mathematical abstraction of surface models all surface models including land surface models hydrological models and lake models can be described as state space systems as shown in eq 1 the state variables x k and the model outputs y k at time step k can be derived based on the meteorological variables at the current time step u k the state variables at the previous time step x k 1 and the model parameters θ 1 x k y k f u k x k 1 θ in eq 1 f represents model structure and all variable notations represent vectors instead of individual values the meteorological variables u k usually meteorological inputs represent boundary conditions of a land surface control volume the state variables x k 1 including variables such as soil moisture soil temperature and snow depth represent the initial state of the control volume model parameters θ describe the characteristics of the modeling unit soil and vegetation types can be treated as model parameters if look up tables are used for categorical parameterization model outputs variables y k are usually energy and mass fluxes getting in or out of the volume such as sensible and latent fluxes surface runoff and baseflow as well as evapotranspiration the model computes both x k and y k but x k will feedback into the model at the next time step eq 1 reveals essential aspects regarding model implementation first of all information has to flow back and forth between the modeling framework m e g lis and the model m f u x θ y m should pass u k x k 1 and θ to m and then retrieve x k and y k from m after running model physics second m is responsible for reading in u k and θ for feeding them to m last m needs to initialize m at the first time step x 0 has to be set by m either with default values cold start or with values from a spin up run restart in order to restart a model m should also be able to write model states into restart files 2 2 lis surface model interfaces lis is a highly modular flexible object oriented and component based framework fig 1 shows the lis software architecture that is related to surface model implementation the lis core manages generic utilities such as time configuration i o domain decomposition parallel computation and information logging based on user configuration it turns on or off specific functional components and controls the overall behavior of lis lis simulates terrestrial hydrology through land surface models hydrological models and lake models the basic behaviors of running a surface model in lis have been encapsulated in programming interfaces as illustrated in fig 1 a total of nine interfaces have to be implemented as fortrans subroutines for establishing data flow between lis and the surface model once registered lis uses these subroutines as callback functions to drive the surface model the read configuration subroutine is for reading simulation specifications from a lis configuration file the initialize subroutine initializes all the variables defined in a data exchange layer it allocates memory for all the allocatable variables the setup subroutine sets up time invariant model parameters from files whereas the dynamic setup subroutine sets up time varying model parameters the read restart subroutine reads model state variables from a restart file for initializing the model the write restart subroutine writes model state variables into a restart file the transfer forcing subroutine acquires meteorological variables from lis the model main is the primary connector which calls the surface model it passes configurations parameters meteorological variables and initial states from lis to the model and retrieves updated model states and model outputs back to lis for writing output files the finalize subroutine is for releasing all the memory allocated in the initialize subroutine the interfaces of read configuration initialize setup and read restart are called only once at the beginning of a lis run while the interface finalize is also called only once at the end of the lis run the interfaces of transfer forcing dynamic setup and model main are called at every time step the interface write restart is called periodically according to a restart writing time interval prescribed in the lis configuration file 2 3 lis standardized model interface there are often gaps between a model and a target framework during model integration if their individual development has not been coordinated from the very beginning typically the framework defines a set of programming interfaces to integrate land surface models hydrological models and lake models collectively called surface models hereafter in this paper since the surface models are not purposely developed to run within lis there are many structural inconsistencies between them for example lis employs a time stepping approach in which model simulations for every grid point in the domain are conducted at a given time step before proceeding to the next time step this order may be different in the native formulations of the surface models some surface models may even not have clear interfaces with sufficient separation and abstraction of i o model physics parallelization and so on the gaps can be addressed with a thin software layer building upon the interfaces of lis and the model since this layer follows the lis standards it is named the lis standardized modeling interface lsmi lsmi is similar to the concept of the nuopc cap it takes driving data from lis and sends them to model physics and then brings model outputs back to lis it tackles all the inconsistencies between lis and the model for example lsmi converts data types for variables if they are defined differently in lis and the model it also derives intermediate variables for the model if lis does not directly provide them defined as a fortran subroutine lsmi encapsulates all the model specific features inside and only exposes lis required variables in the form of subroutine arguments therefore the implementation effort is primarily for connecting lis and the lsmi since the lsmi follows all lis standards the software development for the model implementation can be automated once a set of software infrastructure is defined as code templates for generic operations required by the nine programming interfaces the integration software layer can be automatically created by customizing the code templates according to the lsmi arguments lsmi can be quickly developed by scientists instead of professional software engineers for example a model expert can develop an lsmi following given guidelines it is not necessary to learn software architecture data structure and all programming interfaces of lis after the lsmi is developed the rest of the model implementation can be done through automation with a software toolkit thus there will be much less cost of software development debug test and maintenance automated model implementation will greatly promote the application of lis 2 4 categorization of model inputs lis tackles model inputs differently according to their functional attributes model inputs can be categorized into three groups from a model implementation perspective control options physical parameters and meteorological variables a detailed categorization schematic is depicted in fig 2 the control options are enumerable numbers or texts such as start and end time the value of the time step etc for defining model behaviors meteorological variables are the upper boundary conditions of surface models usually including but not limited to air temperature air pressure wind speed air humidity and radiations physical parameters describe the characteristics of the modeling unit physical parameters can be further categorized into constant parameters lookup table parameters single level spatial parameters and multiple level spatial parameters a constant parameter has the same value over the entire simulation domain which can be either a physical constant such as the gravitational acceleration g 9 8 m s 2 or a coefficient in an empirical equation the lookup table parameters are an array of values depending on indices such as soil texture class and land use type for example the noah land surface model has a vegetation parameter table and a soil parameter table spatial parameters depend on locations and are usually represented in maps such as soil texture maps and land use maps spatial parameters are single level if there have only one map layer otherwise they are multiple level if they have multiple map layers according to temporal variability spatial parameters can be further categorized into time variant and time invariant types 2 5 data communication between lis and surface model there are three software layers in a lis model implementation the lis layer the data exchange layer and the model layer as shown in fig 3 in practice the data exchange layer is constructed with two fortran modules a domain data module and a tile data module the former is defined for the entire computation domain and the latter is defined for tiles a tile is the smallest modeling unit of lis associated with a land use type and its fraction in a grid koster and suarez 1992 there can be multiple tiles in a grid according to the distribution of land use types within it the domain data module includes all location independent values such as the constant parameters the lookup table parameters and the default values for initializing models the tile data module defines all the parameters the meteorological variables the state variables and the output variables whose values depend on spatial locations in both modules data are organized with a fortran type data structure an array of the tile data type is defined inside the domain data module to handle all tiles in the computation domain the lis layer works as a driver when running a model firstly it reads in the model parameters and the initial values of state variables these values are assigned to the model parameters and the initial state variables defined in the domain data module through the initialize interface at every time step prior to running model physics the lis layer sends values to the meteorological variables and the state variables in the tile data array of the data exchange layer these actions are taken through the interfaces transfer forcing and model main these data are then passed to the lsmi for calling model physics in the subroutine of model main after running model physics updated state variables and output variables are retrieved from the lsmi and then sent back to the data exchange layer still in the subroutine of model main there are two advantages to implementing the data exchange layer with two modules 1 memory efficiency and 2 global data accessibility the location independent variables are members of the domain data module these variables are scalars the location dependent variables are members of the tile data module these variables are arrays by separating the location independent and the location dependent variables memory is saved by not allocating large arrays containing single values and the variables in the exchange layer are accessible to all the functional components of lis such as data assimilation and flow routing therefore it is straightforward to hook the model being implemented to these lis components 3 procedures of automated model implementation 3 1 development of lsmi for surface model as mentioned above the lis interfaces are designed generally while each model s interface is specific as a software cap the lsmi is an adapter sitting between them it has to be developed case by case so being dually compatible with both sides the lsmi is typically implemented as a fortran 90 subroutine more specific guidelines are listed as follows first as illustrated in fig 2 the subroutine arguments have to be defined with intrinsic fortran 90 data types such as integer real character and logical if the model takes arguments defined in extended data types e g the fortran type they should be defined inside the lsmi subroutine their member variables should be initialized with the lsmi arguments after calling model physics corresponding lsmi arguments have to be updated for returning values to lis second multidimensional arguments have to be defined as fixed size arrays their dimensions should be arguments of the lsmi subroutine in addition to declaring these arrays shapes they are needed for allocating memory for corresponding variables defined in the data exchange layer during code generation third all subroutine arguments must have their intent attributes specified in inout or out model inputs should be in for only providing data model states should be inout for both providing and returning data model outputs should be out for only returning data with the intent attributes the lsmi subroutine is more readable in addition the intent attributes are used for determining variable categories during code generation fourth the subroutine should use a set of predefined names for meteorological variables including air temperature k air pressure pa wind speed m s specific air humidity kg kg downward longwave radiation w m 2 and shortwave radiation w m 2 and precipitation kg m 2 s these names have been registered in the model implementation tool for connecting the lis meteorological input during code generation last but not least the subroutine calls model physics therefore lis only needs to interact with the lsmi subroutine instead of the actual model once the lsmi subroutine is developed according to these guidelines the automated model implementation tool can understand it to obtain all the required information for code generation by analyzing its arguments 3 2 code generation based on lsmi two fortran modules e g the domain data module and the tile data module are generated to construct the data exchange layer 3 2 1 generation of the data exchange layer as described in section 2 4 the model inputs are categorized according to location dependence spatially distributed variables are defined for all land tiles of the simulation domain on the contrary spatially uniform variables are defined as scalar values for the entire simulation domain the meteorological variables state variables output variables and spatially distributed parameters are defined as member variables of a fortran type in the tile module the control options constant parameters and lookup table parameters are defined in another fortran type in the domain data module an array of the tile type is defined in the domain type for handling internal tile data fig 4 illustrates the two fortran types and their mapping relationships with the lsmi subroutine arguments a naming convention is created for better code readability all the modules and subroutines are named after the model itself e g modex in the domain module the fortran type is named as modex struc the model s version number is a part of the type name as a postfix for integrating multiple versions of the same model the tile type is named similarly in the tile data domain the name of the tile type is in lower case for distinguishing from the name of the domain type which is in upper case all the variables with multiple values of the same type are defined as arrays if an array has its dimension prescribed as an integer type argument of lsmi it is defined as a fixed array otherwise it is defined as an allocatable array and its size will be dynamically determined based on related arguments of lsmi fortran pointer variables are used to define arrays instead of allocatable variables either way the definition of the lsmi subroutine should have provided sufficient information for determining the dimensions of the arrays the mapping from the lsmi arguments to the member variables of the data types is illustrated in fig 4 all the member variables can be traced back to the arguments therefore it is feasible to generate code for the two fortran modules in an automated way all required information such as variable names dimensions and data types are provided by the arguments and their attributes of declaration lis supports two start modes restart and cold start every state variable is initialized uniformly all over the simulation domain when running in the cold start mode for such a case the initial state variables are independent of location therefore initial state variables are defined in the domain type for handling constant initial values on the contrary the tile type defines a set of initial state variables for initializing the model spatially from a restart file generated in a previous run 3 2 2 implementation of the lis surface model interfaces nine fortran subroutines are to be generated to implement the lis surface model interfaces the logic of code generation is based on the specifications of the lsmi subroutine s arguments as illustrated in fig 5 the subroutine readconfig implements the read configuration interface that reads the lis configuration file for setting up a simulation lis users specify control options constant parameters lookup table parameters and initial model states in the lis configuration file the code of the subroutine readconfig includes data reading and checking blocks built upon esmf subroutines for each type of lsmi argument a code template has been defined for data reading and sanity checking inside the model implementation tool therefore the code generation for the subroutine readconfig is a process of customizing the code templates according to the lsmi argument list the subroutines init and finalize implement the interfaces of initialize and finalize which allocate and release computer memory for the allocatable variables in the domain data module and the tile data module the allocatable variables and their dimensions can be determined according to the attributes of the lsmi arguments thus allocation and deallocation statements can be generated for these two subroutines the subroutines setup and dynsetup are the implementation of the interfaces of setup and dynamic setup which read spatial parameters from a netcdf rew and davis 1990 file produced with ldt all spatial parameters correspond to 2 dimensional or 3 dimensional arrays in netcdf datasets with a predefined naming convention as long as data templates are defined for handling the netcdf datasets the code of these two subroutines can be generated according to the spatial parameters in the lsmi argument list the subroutine f2t is the implementation of the transfer forcing interface which retrieves meteorological variables from lis infrastructure and sends them to the data exchange layer lis has a registered set of meteorological variables and many readers to process them from various datasets since the meteorological variables in the lsmi argument list are named consistently with the lis registered meteorological variables code templates can be defined and customized for handling the meteorological variables therefore the subroutine can be automatically generated according to the meteorological variables in the lsmi arguments the subroutines readrst and writerst implement the interfaces of read restart and write restart which reads or writes the state variables to or from a lis restart file in the netcdf format with predefined code templates and a naming convention code for both reading and writing netcdf variables can be generated automatically for all the state variables defined with the inout attribute in the lsmi arguments the main subroutine implements the model main interface the primary connector between a model and lis it grabs all model inputs from the data communication layer and runs model physics by calling the lsmi subroutine for a subdomain in the case of parallel computation after the model simulation is done for a time step the main subroutine sends all model states and outputs to lis for operations such as data assimilation flow routing and output writing the state variables in the data exchange layer are also updated for proceeding to the next time step all of these procedures correspond to the arguments of the lsmi subroutine therefore the main subroutine code can also be automatically generated by customizing predefined code templates according to the lsmi argument list overall the code generation of the lis interfaces starts from the specification of lsmi by analyzing previously implemented models code templates have been created and integrated into the implementation tool as long as the lsmi subroutine follows the guidance in section 3 1 the argument list will be sufficient input for the model implementation tool for code generation 4 the model implementation tool the lis model implementation tool lis mit has been developed for automating the software development of lis model implementation lis mit has two main functions one is to provide a set of graphical user interfaces for conveniently describing lsmi arguments the other is to generate fortran code and documentation for the nine subroutines explained in section 3 2 2 4 1 development environment lis mit is developed using visual basic for application vba within microsoft excel because it has three desirable features to meet the goals of lis mit first excel is both user friendly and well known many scientists already use it for data analysis the spreadsheet can be easily customized for describing the attributes of the lsmi arguments with simple guidance users can work with lis mit quickly the built in features of excel such as font text color and size and comments help users make notes during development this is important because the model implementation is an iterative effort it is helpful for users to improve their implementation designs based on earlier versions recorded in comments and annotations second vba has a rich set of functions for string manipulation from a programming perspective code generation is all about string operations more specifically it is the customization and assembling of general purpose code templates based on the argument list of lsmi once the code templates are embedded in lis mit the vba string functions are sufficient for automated code generation third excel is cross platform available on microsoft windows and apple macos for both operating systems excel embeds vba for macro and add in development thus vba and excel provide a familiar yet powerful programming environment accessible on most scientists personal computers for producing the complicated fortran code needed to incorporate a model into nasa s land information system 4 2 lis mit user interfaces the graphical user interface gui of lis mit includes five excel spreadsheets and thirteen command buttons in the add ins tab as illustrated in fig 6 when an excel macro enabled workbook xlsm file with lis mit code is opened the add ins tab is turned on automatically with the thirteen command buttons loaded the source files for model integration can be generated individually or altogether by clicking the command buttons 4 2 1 description worksheet the description worksheet is the starting point of a lis mit project as shown in fig 7 it asks users to input code preamble model name version number lsmi subroutine name and developer s name lis mit uses such information to name variables subroutines modules and source files lis mit does not directly analyze the source code of lsmi and the lsmi subroutine name is for generating the code of the subroutine main to call model physics lis mit produces both fortran code and documentation where the latter is in the form of comment lines usually users put institute related legal statements to the code preamble such as copyright information contact information and a code sharing policy lis mit can integrate land surface models hydrological models and lake models with slightly different code templates therefore users have to specify the type of model to be implemented in this worksheet lis mit acknowledges the developer of lsmi in the comment lines of the generated code which can be extracted and put into technical documentation 4 2 2 variable worksheet the variable worksheet as shown in fig 8 asks users to input the arguments of lsmi it prompts users to specify the names data types dimensions intent attributes categorical attributes units and descriptions of the lsmi arguments this worksheet is the main input interface of lis mit it takes the lsmi specifications as strings which can be understood and processed by lis mit as mentioned in section 3 1 all lsmi arguments must be declared with fortran intrinsic data types hence this worksheet only allows character integer and real types for the datatype column for numerical variables they have to be either scalar values or arrays in the dimension column 1 indicates a scalar value and otherwise indicates an array for example the variable sldpth has a dimension of nsoil it means that sldpth is an array of fixed size nsoil the intent column asks for the intents of arguments which supports three modes in for model inputs inout for state variables and out for output variables the attribute column asks for the categories of lsmi arguments including input state variable output and lis built in variable model inputs are further categorized into the types described in fig 2 the lis built in variables include time e g year month and day and location e g latitude and longitude which are universally applicable to all models the unit column asks for the lsmi arguments units which will be in comment lines after variable declarations similarly the texts in the description column also will be in comment lines using latexsyntax these comment lines explain the generated source code and document the variables from a scientific perspective 4 2 3 forcing worksheet the forcing worksheet as shown in fig 9 configures the meteorological variables it lists all the meteorological variables registered in lis users are required to mark only the variables required by the model being implemented for example if the air temperature tair is required the select cell should be set to 1 otherwise the cell should be set to 0 if the tair variable is optional the optional cell should be set to yes otherwise it should be set to no the rest of the columns are read only and for the users information to understand these meteorological variables 4 2 4 output worksheet the output worksheet as shown in fig 10 asks for information about output variables the columns of variable levels unit surface type and description must be consistent with corresponding columns in the description and variable worksheets the direction column should be filled with predefined directions such as up down in out if describing a flux or movement of the variable the lis moc name represents variables defined under the lis model output conversion which is explained in the lis developer s manual https lis gsfc nasa gov documentation lis 4 2 5 built in variable worksheet the built in variable worksheet as shown in fig 11 asks for the registration of lis built in variables if they are part of the lsmi arguments the built in variables are commonly used across models such as vegetation type soil texture type elevation slope leaf area index lai greenness background emissivity and surface roughness for example if vegtyp is the argument of lsmi describing vegetation type then users only need to put it in the model variable name column for the built in variables not used in lsmi users should leave the corresponding model variable name cells blank 5 applications several models have been implemented into lis using lis mit facilitated by the automated model integration approach months of development time have been saved lis mit did most of the software development therefore scientists are free from tedious learning and coding besides the code generated by lis mit strictly follows the coding standards of the nasa lis software consequently it is bug free well structured and easy to read much less time is spent on software debugging and testing than in traditional manual model implementation as mentioned above users only need to develop the lsmi subroutine after setting up a lis mit application based on the lsmi arguments all the lis model interfaces will be generated quickly fig 12 shows the line counts of the lsmi subroutines developed automatically and manually usually the automatically generated code does all the work for model implementation however if a model has a highly complicated structure users need to edit a few lines of the generated code which have placeholders and instructive comments made by lis mit fig 12 shows the automated percentage which is the ratio of the line count of the generated code and the line count of all code including the lsmi subroutine spgp sparse gaussian process model is a bayesian non linear regression model it approximates surface fluxes soil moisture and soil temperature upon meteorological inputs as a simulator of land surface models after developing a simple lsmi subroutine consisting of 36 lines lis mit generated all the implementation code consisting of 1610 lines the automated rate is about 98 sac htet and snow 17 are the operational models in the hydrology research research distributed hydrologic model hl rdhm system spies et al 2015 lee et al 2011 of the national weather service nws the former is a rainfall runoff model with frozen ground physics the latter is a conceptual model simulating snow accumulation and thawing an lsmi subroutine has been developed to combine them into a fully functioned land surface model the subroutine has 634 lines of code while the interface code generated with lis mit has 5427 lines the automated rate is about 90 flake is a 1 d lake model mironov et al 2010 kirillin et al 2011 designed for simulating lake evaporation and temperature in numerical weather predictions in its implementation the lsmi subroutine has 175 lines and the generated code has 1906 lines the automated rate is about 92 noah mp is one of the land surface models integrated into the weather research and forecasting wrf model powers et al 2017 the versions 3 6 and 4 0 1 have been respectively implemented into lis using lis mit since more components are included such as crop and groundwater noah mp version 4 0 1 has a more complicated software structure than noah mp version 3 6 therefore its lsmi subroutine has more lines the automated rates for these two versions are about 92 and 85 respectively ruc is another land surface model in wrf featured with rapid land surface status responses to meteorological inputs the ruc version 3 7 has been implemented into lis with lis mit the line count of its lsmi subroutine is 666 while there are 3057 lines of generated code the automated rate is about 82 awral is a land surface model developed for water resource assessment in australia wallace et al 2013 its model physics has been put into lis with lis mit since the model is written in the c programming language a c function has been developed in the fortran compatible manner as the lsmi subroutine the c function has 386 lines and the generated code has 2525 lines the automated rate is about 87 6 summary lis requires models to be fully integrated at the source level requiring broad and deep knowledge in addition to expert proficiency in fortran programming and software design this makes lis model implementation a technical challenge to scientists in this paper we described a software solution through automated model integration the solution has been implemented in a software toolkit lis mit lis mit relies on the lis standardized model interface lsmi which is an interface standard for land surface models hydrologic models and lake models by adopting this standard a model can directly interact with a modeling framework such as lis if a model does not follow the standard an lsmi compliant subroutine can be developed as a software cap for the model therefore the model can communicate with lis through the subroutine abstracted from the universal behaviors of surface models the concept of lsmi is valid for lis model integration and valuable for other model interoperations in applications an lsmi can be a fortran subroutine or c function with an lsmi software layer we can reduce the need to modify original code from the model sources so as to keep the model code integrity we created a categorization system for the surface models inputs from a data exchange perspective mappings have been built between the lis model interfaces and the lsmi arguments including control options constant parameters lookup table parameters spatial parameters meteorological variables state variables and outputs therefore generalized code templates are defined for these interfaces making automated code generation possible for developing corresponding fortran subroutines this is another foundation of the implementation tool lis mit developed with excel vba lis mit uses the excel worksheet as its user interface users are guided to describe model attributes and specify the lsmi arguments in five worksheets lis mit scans the lsmi arguments during code generation and then creates specific fortran data modules and subroutines following program logic imbodied in the code templates in addition to the fortran source code lis mit also generates descriptive comments for documentation a tremendous amount of time has been saved using the automated model integration approach as use cases spgp sac htet snow 17 flake noah mp version 3 6 and version 4 0 1 ruc version 3 7 and awral version 6 0 have been implemented into lis with lis mit the generated code accounts for 82 to 98 of development in terms of the line count of code more models such as crocus vionnet et al 2012 a snowpack scheme are undergoing implementation using lis mit in summary the automated model integration approach and the tool significantly lowered the technical threshold of lis model implementation it can reduce development time from months to weeks for surface models the software tool is available to partners of the nasa lis team in addition this approach can benefit a broader community the modeling framework is not limited to lis and the models are also not limited to land surface models hydrological models and lake models as long as coupling interfaces can be standardized integrating any model into a modeling framework can be automated based on a thin layer software cap similar to lsmi this can significantly save software development costs in the long run and therefore promote scientific research software and data availability as a utility toolkit of the land information system framework lisf available at https github com nasa lis lisf lis mit is released under the terms and conditions of the apache license version 2 0 see https www apache org licenses license 2 0 lis mit is open source and available at https github com nasa lis lisf tree master lis utils lis mit also included here are instructions for getting started with lis mit and for accessing its vba code readme txt along with a demonstration case noah mp version 3 64 0 1 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we wish to thank tatiana g smirnova cooperative institute for research in environmental sciences at noaa earth system research laboratory zhuo wang formerly with saic at nasa gsfc hydrological sciences laboratory and wendy sharples bureau of meteorology in australia for integrating and testing ruc 3 7 noah mp 4 0 1 and awral 6 0 respectively nasa center for climate simulation nccs provided computational support for this study 
25488,ensemble based approaches to prescribed fire planning cannot be supported by cfd based models like firetec and wfds because they are too computationally expensive and cannot leverage les approaches like cawfe and wrf sfire because too coarse of resolution quic fire was developed to fill this gap but it cannot currently address complex terrain typical for instance of the western united states in this paper we describe the extension of the diagnostic wind model quic urb the wind engine of quic fire to a terrain following coordinate system in particular the paper presents the mathematical derivation of the wind solver leading to a linear system of equations that are solved through the successive over relaxation method the model is validated against a standard test used in previous works the askervein hill and against a new dataset from measurements in the socorro mountains new mexico the terrain following implementation captured the correct phenomenology for the isolated askervein hill with a wind speed up at the top of the hill the model agreed well with measurements on the upwind side of the peak but overestimated speed up on the downwind side of the hill this is due to the inability of the model to generate flow separation and wake eddy dynamics on a common laptop the divergence free wind field was obtained in 6 s making the solver appealing for coupled fire atmosphere simulations the socorro mountain was highly complex with many cliff faces peaks and valleys although the model captures the magnitude and direction of inlet and outlet areas of the domain it performs rather poorly in the valley region and in the regions near the steep cliffs hence the model shows good agreement with data in areas of open sloped terrain but lacks in areas where flow separation and thermally driven effects may be present neither effect was addressed in this work results highlight that future work should focus on the implementation of parameterizations of wake eddies similar to quic urb s building parameterizations and on thermodynamic driven flow keywords complex terrain flow terrain following wind solver diagnostic wind model prescribed fire winds scales fast running wind solver data availability data will be made available on request 1 introduction mountainous terrain is well known to significantly affect the propagation of evolving wildland fires linn et al 2010 sharples et al 2012 with rate of spread rapidly increasing with slope angle silvani et al 2012 morandini et al 2018 and with fires that may spread perpendicular to the main wind direction because of the terrain slope at the flank pimont et al 2012 understanding the effect of slope is crucial for the protection of first responders in wildfire response and for avoiding escape in prescribed fires in complex terrain terrain affects fire propagation by modifying heat transfer patterns due to varying geometry by tilting the flame toward the upslope direction the flame itself becomes closer to the unburnt vegetation effectively pre heating it in addition terrain changes the two way feedback between the fire and the atmosphere by limiting fresh air entrainment downwind of the fire morandini et al 2018 such a feedback is fundamental to describe emergent fire behavior and thus an important phenomena to capture in order to correctly predict the behavior of wildland fires in general but even more in the context of prescribed fires linn and cunningham 2005 terrain also affects the trajectory of the smoke plume close to the fire front and therefore its long range transport in open terrain the smoke plumes will move upward at first under the fire generated buoyancy and eventually bend over due to the combined effect of lower updraft intensity as fresh air is entrained and the stronger upper winds however on sloped terrain the plume cannot entrain air equally from all sides of the fire the net result being that beyond a certain slope angle the smoke plume attaches itself to the terrain grumstrup et al 2017 capturing the effect of complex terrain on smoke is of growing interest because the ground concentration of fire generated pollutants may have adverse health effects in susceptible members of the public and may lead to visibility impairment which is relevant to airport operations as well as road safety goodrick et al 2012 curcio et al 2020 for these reasons in prescribed fire planning effective smoke management can make the difference between executing a burn or not complex terrain also modifies the background winds by inducing complex three dimensional wind patterns like channeling in canyons and valleys separations recirculations which induce downward momentum transport of upper winds and flow blocking due to large terrain features uchida and sugitani 2020 serafin et al 2018 terrain effects have been shown to have profound and unexpected effects on the direction of fire propagation sharples et al 2012 accounting for terrain induced mechanical effects on winds is the first step in building a coupled fire atmosphere model with sound predictive capabilities one approach to capturing the dynamic and heterogeneous influences of terrain on winds with the scope of predicting wildland fire behavior is solving the full navier stokes equations to resolve the flow at fireline scales which is done in firetec linn and cunningham 2005 and wfds mell et al 2013 models or larger scales using large eddy simulation les tools such as in cawfe coen 2013 wrf sfire mandel et al 2011 or wrf fire coen et al 2013 models like firetec and wfds are too computationally time intensive for a real time forecasting model and ensemble based prescribed fire planning the lower resolution les based tools use simplified fire spread models and vegetation representation that limit their generality and applicability for lower intensity fire scenarios typical of prescribed fires because they do not resolve the fine scale processes and three dimensional fuels structure which greatly influence fire propagation simpler and faster diagnostic wind models like windninja forthofer 2007 have not been coupled with fire models capable of addressing the complex fire spread and atmospheric response of the ignition patterns typical of prescribed fires and the multi front interaction of wildfires in complex terrain this gap has led to the development of quic fire linn et al 2020 which was intended to provide an alternative to computational fluid dynamics cfd based tools in particular quic fire was designed to be much less computationally expensive than the firetec wfds class of models and yet represent some of the processes and three dimensional structure of the fuel and winds at meter scales however the wind solver underlying quic fire the quick urban industrial complex wind solver quic urb does not currently include the influences of terrain on either the winds or the fire behavior this work is a first step towards incorporating the terrain s influence on the background wind fields in quic urb which will eventually be used with the fast running quic fire coupled fire model the complex terrain wind model presented in this paper is built upon quic urb singh et al 2008 quic urb is a diagnostic wind model that can produce mass consistent wind fields from multiple heterogeneous wind measurements over domains with sizes ranging from 1 km to 100 km at horizontal resolutions typically varying from 1 200 m singh et al 2008 prior to the terrain influences on winds presented in this manuscript background winds were specified for flat terrain scenarios the user can specify wind background profiles that follow a power law a log law or a set of measurements at multiple heights the mesoscale weather research and forecasting model wrf outputs usually at 1 5 km horizontal resolution can also be ingested kochanski et al 2015 nelson et al 2016 carvalho et al 2012 quic urb interpolates the provided wind measurements or wrf outputs over its domain using the barnes scheme barnes 1964 then the impacts of vegetative canopies and buildings are parameterized and superimposed over the background wind linn et al 2020 and finally mass consistency is imposed wind measurements at different times can be provided to follow the temporal variation of the background wind field the resulting wind field is time averaged the temporal or spatial wind variations associated with turbulent wind fluctuations are not resolved in this background wind field quic urb instead they are represented via stochastic turbulence model via the companion model quic plume a lagrangian random walk transport and dispersion model williams et al 2004 or parameterized following a shear based method in quic fire linn et al 2020 the advantages of a diagnostic wind solver over its prognostic counterparts lie in its speed and memory requirements indeed quic urb can generate wind fields for complex urban environments in less than 1 min with a common laptop i e it does not require super computing capabilities the disadvantage is that standard quic urb simulations capture only wind snapshots in time quasi steady state solutions are computed instead of the continuous evolution of the wind the quic urb framework was chosen because since 2015 it has been dynamically integrated with a fire model focusing on prescribed fire quic fire linn et al 2020 williams et al 2004 where the 3d vegetation structure is incorporated into the quic urb generated background wind field by superimposing drag affecting fire behavior therefore the addition of a complex terrain capability enhances the current predictive capabilities of wind plumes and fire in the following we will discuss the original quic urb coordinate system section 2 the complex terrain model formulation section 3 how to solve the resulting differential equation section 4 and two real world test cases of the model sections 5 and 6 this work will not address terrain induced thermodynamic effects on winds like katabatic winds 2 quic urb model description quic urb uses a cartesian coordinate system based on universal transverse mercator utm coordinates in which x and y are perpendicular and define the horizontal plane while z is vertical and positive in the direction opposite to gravity the quic urb cells are rectangular prisms in principle although usually the cell size in the x and y directions are the same in the z direction a stretched grid with smaller cells close to the ground is usually employed to capture the wind speed gradients at the surface while avoiding an unnecessary computational burden at higher heights where the wind speed does not change as much the original quic urb formulation sherman 1978 aims to minimize the integral 1 e u v w λ v α 1 2 u u 0 2 α 1 2 v v 0 2 α 2 2 w w 0 2 λ u x v y w z d x where u 0 v 0 and w 0 are the interpolated initial wind components in the x y and z directions respectively u v and w are the cartesian adjusted velocities defining a mass consistent flow field α 1 and α 2 are the gaussian precision moduli and λ is the lagrange multiplier the precision moduli define how much of the wind adjustment should be directed to changing u and v vs w hence if α 1 α 2 1 then w will change more than u and v in the original quic urb formulation α 1 α 2 since there is no reason to support weighing the adjustments differently in practice 1 expresses the concept that the desired adjusted wind field is mass consistent second term while keeping the adjustments as small as possible because the initial wind field comes from the interpolation of measurements the initial wind field typically is calculated from an inputted background wind speed direction and height recorded usually 10 m that is used to calculate the wind field as either a power law or log vertical profile the profile is fit to match the speed at the height input the vertical profile is then set for all cells in the domain the minimum of eq 1 is found by solving the euler lagrange equations 2a 2 λ 2 α 1 u 0 2b u u 0 1 2 α 1 2 λ x 2c v v 0 1 2 α 1 2 λ y 2d w w 0 1 2 α 2 2 λ z eq 2 is a system of partial differential equations for λ u v and w and u 0 u 0 v 0 w 0 in quic urb a finite difference method is used to solve equation 2 with first order central differences after discretizing the equation in λ we obtain a linear system that quic urb solves with the successive over relaxation sor method young 1954 which is a standard iterative method derived from the gauss seidel method the full derivation for flat terrain can be found in pardyjak and brown 2003 in this work we extend this method to terrain following coordinates 3 terrain following model extension to account for the effects of terrain generated winds terrain following tf coordinates are adopted denoted with x y z and defined as 3 x x 4 y y 5 z h z h x y h h x y where h is the domain height at the point of lowest elevation and h x y is the terrain elevation at x y therefore z 0 corresponds to the surface elevation z h x y solving 2 in tf coordinates to minimize 1 requires expressing the divergence and velocities in contravariant form moussiopoulos and flassak 1986 this requires using both the covariant and contravariant metric tensors for differentiation and transformations the covariant basis can be calculated by taking the derivative of the original coordinates with respect to the new coordinates 6 e i j x j x i e j and the contravariant basis can be calculated by taking the derivative of the new coordinates with respect to the original coordinates 7 e i j x i x j e j note that superscripts and subscripts will correspond to contravariant and covariant quantities respectively this results in the contravariant basis 1 1 subscripts including letters correspond to partial derivatives of the respective direction i e h x h x e 1 i e 2 j and 8 e 3 h x z h h h i h y z h h h j h h h k it is important to note that under extreme slope values the contravariant basis poorly spans the space at the surface i e e 3 e 1 0 if h x 0 in some cases it may be necessary to smooth terrain maps to ensure a solution is attainable the covariant and contravariant metric tensors can be calculated simply by 9 g i j e i e j and g i j e i e j resulting in a 3 3 matrix for each basis both metric tensors share the same determinant meaning they have the same normalization factor 10 g h h h the contravariant velocities are 11 u i u i v j w k e i resulting in u 1 u u 2 v and 12 u 3 u h x z h h h v h y z h h h w h h h with a covariant metric tensor g i j and a contravariant velocity field u i we calculate the divergence of u i to be 13 i u i 1 g i x i g u i where x i x y z this results in the divergence of the contravariant velocities having the form 14 u u 1 x u 2 y u 3 z 1 h h u 1 h x u 2 h y the contravariant velocities are transformed via u i u e i where e i are the contravariant basis using the transformations of 12 and 14 we transform 1 into contravariant terms to derive the transformed euler lagrange equations transforming 1 results in 15 e u 1 u 2 u 3 λ v α 1 2 u 1 u 0 1 2 α 1 2 u 2 u 0 2 2 α 2 2 u 3 u 0 3 h h h u 1 u 0 1 h x u 2 u 0 2 h y h z h 2 λ u 1 x u 2 y u 3 z 1 h h u 1 h x u 2 h y d x which can be written as e u 1 u 2 u 3 λ v l u 1 u 2 u 3 u 1 x u 2 y u 3 z λ d x when 15 is at its minimum energy then l f j 1 3 x j l f j 0 f j f x j where f u 1 u 2 u 3 u 1 x u 2 y u 3 z λ is any of the dependent variables the result is seven equations that can be written as 16 u 1 u 0 1 1 2 α 1 2 λ x λ z h z h h h x 17 u 2 u 0 2 1 2 α 1 2 λ y λ z h z h h h y 18 u 3 u 0 3 1 2 α 2 2 λ z h h h 2 1 2 α 1 2 h z h h h x λ x h y λ y λ z h z h h h x 2 h y 2 and the continuity equation 19 u 0 x 1 u 0 y 2 u 0 z 3 1 h h u 1 h x u 2 h y 0 eqs 16 17 and 18 are used to calculate the adjusted velocities once λ is determined by differentiating 16 17 and 18 substituting the expressions into 19 and rearranging we obtain 20 λ x x λ y y z h h h 2 h x 2 h y 2 η h h h 2 λ z z 2 z h h h h x λ x z h y λ y z z h h h h x x h y y 2 h h h x 2 h y 2 λ z 2 α 1 2 u 0 where η α 1 2 α 2 2 the laplacian of λ is i i λ 1 g i x i j g g i j λ x j resulting in 21 i i λ λ x x λ y y z h h h 2 h x 2 h y 2 h h h 2 λ z z 2 z h h h h x λ x z h y λ y z z h h h h x x h y y 2 h h h x 2 h y 2 λ z which shows we have recovered the contravariant form of 2 in 20 but with the gaussian precision moduli incorporated specifically in the form 2 λ 2 α 1 2 u 0 which is the general form of the equation solved in sherman 1978 open boundary conditions corresponding to λ 0 are implemented on the x y and top z boundaries on the surface a no flux of the wind velocity condition is enforced this boundary condition is satisfied by imposing u z 0 3 0 and enforcing that u 3 u 0 3 0 at z 0 in eq 18 this results in the neumann boundary condition 22 λ z h x λ x h y λ y h h h η h x 2 h y 2 for the terrain surface 4 computational method we solve 20 with the successive over relaxation sor method the discretization of the domain is the same as the original quic urb pardyjak and brown 2003 grid where the velocities are face centered and λ is cell centered except that it is discretized in sigma coordinate space the sigma coordinate space is a rectilinear grid that is uniform in x and y with vertically stacked horizontally uniform layers of varying thickness typically growing with height see fig 2 by choosing a maximum domain height h and discretizing the vertical grid at the same level for all x y positions a terrain following grid is produced fig 1 when z is transformed back to cartesian space by solving 5 for z the terrain following grid conforms to the topography near the surface and flattens to a horizontal plane at z h discretizing the derivatives in 20 using central difference stencils produces a sparse linear system that is solved using sor an illustration of the discretization points in the cartesian space for the stencils is in fig 2 single derivatives in the horizontal direction are discretized as λ x i j k λ i 1 j k λ i 1 j k 2 δ x second derivatives are discretized as λ x x i j k λ i 1 j k 2 λ i j k λ i 1 j k δ x 2 and cross derivatives are discretized as 23 λ x z i j k z λ x i j k 1 δ z k λ x i j k 0 5 λ x i j k 0 5 when calculating partial derivatives of λ in the x and y directions at horizontally oriented cell walls k 0 5 weighted averages based on cell height are used λ x i j k 0 5 1 δ z m k 1 z m k z k λ x i j k z k z m k 1 λ x i j k 1 for derivatives in the vertical direction non uniform cell heights must be taken into account since grid cells typically have increasing vertical size with increasing z the non uniformity is accounted for by taking the average of derivatives 24 λ z i j k 1 2 λ z i j k 0 5 λ z i j k 0 5 calculated across the top λ z i j k 0 5 λ i j k 1 λ i j k z m k 1 z m k and bottom of the cell λ z i j k 0 5 λ i j k λ i j k 1 z m k z m k 1 where z m are cell center heights for the corresponding cells the terrain surface boundary condition eq 22 must be satisfied for all bottom layer cells for λ z k 0 5 terms partial derivatives in the x and y directions are evaluated at the z 0 surface as λ x i j k 0 5 1 4 δ x λ i 1 j k 2 λ i 1 j k 2 λ i 1 j k 1 λ i 1 j k 1 which has a simple form due to both the surface cell and the under surface cell having an equal vertical cell height of δ z 1 to enforce the boundary condition λ values below the terrain surface are chosen so that 22 is satisfied to do this we enforce that there is no adjustment to u 0 3 0 at the surface using the discretized form of eq 18 setting u 3 u 0 3 0 and solving for λ i j k 1 this results in 25 λ k 1 λ k 2 δ z m k 1 h x 2 h y 2 η h h h h x 4 δ x λ i 1 k 2 λ i 1 k 2 λ i 1 k 1 λ i 1 k 1 h y 4 δ y λ j 1 k 2 λ j 1 k 2 λ j 1 k 1 λ j 1 k 1 2 2 missing i j k indices represent no shift in reference location i e λ k 2 λ i j k 2 by substituting the corresponding derivative expressions into 20 λ i j k satisfies the linear system 26 λ i j k i i 1 i 1 j j 1 j 1 k k 1 k 1 c i j k λ i j k d i j k 2 α 1 2 u 0 i j k d i j k for i j k i j k where c i j k and d i j k are the combined coefficients multiplying the corresponding λ i j k and λ i j k terms from the discretized form of 20 eq 26 is solved using the sor method which takes the form 27 λ i j k ℓ 1 1 ω λ i j k ℓ ω λ i j k where ω is the relaxation parameter and λ i j k incorporates λ values from both iteration ℓ and ℓ 1 eq 27 is iterated for all λ within the cells until convergence is reached we consider convergence reached when the average change in λ reaches less than 0 1 of the first initial iteration ϵ ℓ ϵ 1 0 001 where ϵ is defined as ϵ ℓ i j k λ i j k ℓ λ i j k ℓ 1 n cells and n cells is the number of cells used in the domain motivated by ross et al 1988 the vertical contravariant velocities above the surface are set as 28 u 0 3 h z h h h x u 0 1 h y u 0 2 to promote terrain effects on the wind this equates to w 0 which generates initial divergence in the field in the presence of terrain with w 0 everywhere in the domain it forces the solver to adjust for the no flow boundary condition at the surface the other reasonable initial vertical component is u 3 0 everywhere this results in the initial wind field being aligned with the terrain following grid everywhere already satisfying the surface boundary condition and minimal initial divergence being present u 3 z 0 this leads to a weak topographical influence seen in the resulting wind field and is not the preferred initial guess 5 isolated hill case askervein hill we test our methods on the real world terrain feature askervein hill 57 11 313 n 7 22 360 w taylor et al 1987 its simple geometry lack of vegetation and isolation from surrounding terrain means that the missing physical dynamics in quic urb i e buoyancy and momentum effects should not be a dominant factor in wind field measurements the simulation uses a similar setup as outlined in forthofer et al 2014 and wagenbrenner et al 2019 a comparison is made to measured data by using wind measurements during slightly stable atmospheric conditions richardson numbers between 0 0110 and 0 0074 along three transects at a height of 10 m that are provided in the mf03 d and tu03b datasets taylor et al 1987 sensor locations are illustrated in figs 3 and 4 topography with 23 m horizontal resolution from walmsley and taylor 1996 is used for the surface the 6 km 6 km domain is discretized into 257 257 horizontal cells a vertical extent of 760 m is discretized by a stretched grid of 18 cells growing in height with increasing z cell heights range in height from 10 m at the surface to 100 5 m at the top cell a reference velocity of 8 9 ms 1 210 from north measured at a location 3 km upstream is used to set a power law vertical wind profile to match the profile data described in forthofer et al 2014 eq 28 is used to set the contravariant vertical velocities so there is no vertical component in cartesian space above the surface using the most aggressive relaxation parameter that achieved convergence ω 1 9 349 iterations were executed on a laptop using 8 threads with openmp producing the analyzed wind field in a total of 6 s the wind speeds from the model results shown in figs 6 5 and 8 were linearly interpolated to 10 m above the surface for the comparison at the sensor locations the relative speed up is a ratio using the input reference wind speed and is calculated via relative speed up u m e a s u r e d u r e f u r e f where u m e a s u r e d is the velocity at a measurement site along a transect and u r e f is the velocity of the wind at the upwind measurement site that was used to generate the initial background wind field the quic urb model computed relative speed up vs distance near and along the hilltop transect b agrees well with measurements while the relative speed up vs distance along transects a and aa show that the model agreed well with measurements on the upwind side of the peak but overestimated speed up on the downwind side of the hill table 1 highlights the difference in accuracy between the winds calculated on the upwind and downwind sides of the peak with average percent error dropping from 28 4 to 6 83 for transect a and from 13 5 to 3 63 for transect b when winds on the lee side of the hill were omitted from error calculations the inability of the model to generate flow separation and wake eddy dynamics is most likely why wind speeds are not retarded sufficiently on the lee side of the hill transect a also contains wind direction measurements that are used to compare with quic urb model output simulation results show an average direction error of 6 80 with the largest deviation of 17 7 occurring on the leeward side of the hill in alignment with the poorer wind speed results in the same location the explanation for the differences on the downwind side of the hill is further supported by forthofer et al 2014 where it is shown that including momentum conservation to a mass conserving model improves the leeward side over prediction 6 steep complex terrain measurements from a 2019 field experiment at the energetic materials research and testing center emrtc in socorro nm 34 2 652 n 106 57 132 w are used to evaluate the performance of the terrain following algorithm in a highly complex terrain brown et al 2022 the experiment recorded wind speeds and directions over a few weeks in october and november 2019 from 16 sonic anemometers and vertical profiles from two lidars four days of the data were taken where ten minute averages were calculated from the 1 hz data to reduce turbulent signals for comparison with model output the terrain surrounding the measurement sites of the experiment lacks significant vegetation and is highly complex with many cliff faces peaks and valleys that can be seen in fig 7 measurements from one of the lidar units labeled as l1 in fig 7 are used to generate the initial wind field for the simulations by fitting a logarithmic profile to the 10 m measurement the vertical component of the initial velocity field is set to zero using eq 28 the domain for each simulation is 2 25 km 2 25 km 1 km discretized into 205 205 38 cells that grow in height with increasing z the horizontal extents of the domain encompass the sensor sites higher elevation terrain and a portion of the low lying flat section to the east fig 9 performance of the model in mountainous terrain is worse when compared to the results seen from the askervein domain results from mid day and evening measurements shown in fig 10 highlight areas where the models prediction capabilities are lacking although the model captures the magnitude and direction of inlet and outlet areas of the domain sensors 01 02 07 08 and 15 it performs rather poorly in the valley region sensors 03 10 and l2 and in the regions near the steep cliffs sensors 05 06 11 14 16 and 17 the poor performance in the cliff regions is due to the models lack of momentum conservation causing an inability to generate the wake regions on the leeward side of the steep terrain also the current lack of slope induced thermodynamically driven flows contributes to poor results in these regions although a version of the solver that includes slope induced flows is currently being worked on furthermore the model does not accurately capture the flow in the valley region because the model is unable to emulate boundary layer dynamics and thermodynamics near the surface nonetheless from a qualitative standpoint the model generates reasonable wind fields especially using only a single wind profile for input symbol description h total domain height h x y terrain height x partial derivative with respect to x x cartesian position u cartesian velocity x sigma coordinates position e i contravariant basis e i covariant basis u i contravariant velocity g i j contravariant metric tensor g i j covariant metric tensor g determinant of matrix g λ lagrange multiplier ω sor relaxation parameter α 1 α 2 gaussian precision moduli η squared ratio of α 1 over α 2 ℓ iteration number ϵ ℓ convergence metric n c e l l s number of cells used in a domain 7 conclusions current fire atmosphere coupled models cannot be used in ensemble based approaches to prescribed fire planning because they are either too computationally expensive e g firetec wfds or they are too coarse lacking the necessary level of detail e g wrf sfire cawfe the quic fire model was developed for this purpose to capture the fundamental physical phenomena while being computationally efficient and not require supercomputing capabilities however quic fire did not have complex terrain capabilities needed to address wildland fire problems in many parts of the world this paper discusses the mathematical formulation of a diagnostic wind model for flow over complex terrain using terrain following coordinates the model was implemented within quic urb which is the wind engine of quic fire and is based on the first order mechanical disturbance of the flow caused by terrain it does not account for wakes terrain blocking or thermodynamic effects the model shows strong performance in the canonical isolated hill askervein case capturing the general profile recorded by sensors it shows wind speedup at the top of the hill but overestimates wind speed on the lee side in a highly complex terrain case where measurements in the socorro mountain of new mexico were used the model struggles to capture flow dynamics near cliffs and in areas occluded by surrounding terrain however the model does capture the correct magnitude and direction of inlet and outlet areas of the domain it should be highlighted that the flow solutions were obtained quickly on a common laptop making the model appealing for coupled fire atmosphere simulations to help performance around cliffs future work includes extending the wake eddy parameterizations used in quic urb for urban environments pardyjak and brown 2003 to the terrain following coordinate system to address building like steep terrain features additional work is needed to capture flow dynamics that occur from thermodynamic effects and boundary layers although a version is currently being developed that includes slope induced flows despite these current shortcomings this work showed a promising first step into adding complex terrain winds to quic fire due to over prediction of winds on the lee side of steep terrain fire simulation results will have to be interpreted carefully in complex terrain regions before additional terrain effects are integrated into the wind solver declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments research presented in this article was supported by the laboratory directed research and development program of los alamos national laboratory under project number 20220024dr 
25488,ensemble based approaches to prescribed fire planning cannot be supported by cfd based models like firetec and wfds because they are too computationally expensive and cannot leverage les approaches like cawfe and wrf sfire because too coarse of resolution quic fire was developed to fill this gap but it cannot currently address complex terrain typical for instance of the western united states in this paper we describe the extension of the diagnostic wind model quic urb the wind engine of quic fire to a terrain following coordinate system in particular the paper presents the mathematical derivation of the wind solver leading to a linear system of equations that are solved through the successive over relaxation method the model is validated against a standard test used in previous works the askervein hill and against a new dataset from measurements in the socorro mountains new mexico the terrain following implementation captured the correct phenomenology for the isolated askervein hill with a wind speed up at the top of the hill the model agreed well with measurements on the upwind side of the peak but overestimated speed up on the downwind side of the hill this is due to the inability of the model to generate flow separation and wake eddy dynamics on a common laptop the divergence free wind field was obtained in 6 s making the solver appealing for coupled fire atmosphere simulations the socorro mountain was highly complex with many cliff faces peaks and valleys although the model captures the magnitude and direction of inlet and outlet areas of the domain it performs rather poorly in the valley region and in the regions near the steep cliffs hence the model shows good agreement with data in areas of open sloped terrain but lacks in areas where flow separation and thermally driven effects may be present neither effect was addressed in this work results highlight that future work should focus on the implementation of parameterizations of wake eddies similar to quic urb s building parameterizations and on thermodynamic driven flow keywords complex terrain flow terrain following wind solver diagnostic wind model prescribed fire winds scales fast running wind solver data availability data will be made available on request 1 introduction mountainous terrain is well known to significantly affect the propagation of evolving wildland fires linn et al 2010 sharples et al 2012 with rate of spread rapidly increasing with slope angle silvani et al 2012 morandini et al 2018 and with fires that may spread perpendicular to the main wind direction because of the terrain slope at the flank pimont et al 2012 understanding the effect of slope is crucial for the protection of first responders in wildfire response and for avoiding escape in prescribed fires in complex terrain terrain affects fire propagation by modifying heat transfer patterns due to varying geometry by tilting the flame toward the upslope direction the flame itself becomes closer to the unburnt vegetation effectively pre heating it in addition terrain changes the two way feedback between the fire and the atmosphere by limiting fresh air entrainment downwind of the fire morandini et al 2018 such a feedback is fundamental to describe emergent fire behavior and thus an important phenomena to capture in order to correctly predict the behavior of wildland fires in general but even more in the context of prescribed fires linn and cunningham 2005 terrain also affects the trajectory of the smoke plume close to the fire front and therefore its long range transport in open terrain the smoke plumes will move upward at first under the fire generated buoyancy and eventually bend over due to the combined effect of lower updraft intensity as fresh air is entrained and the stronger upper winds however on sloped terrain the plume cannot entrain air equally from all sides of the fire the net result being that beyond a certain slope angle the smoke plume attaches itself to the terrain grumstrup et al 2017 capturing the effect of complex terrain on smoke is of growing interest because the ground concentration of fire generated pollutants may have adverse health effects in susceptible members of the public and may lead to visibility impairment which is relevant to airport operations as well as road safety goodrick et al 2012 curcio et al 2020 for these reasons in prescribed fire planning effective smoke management can make the difference between executing a burn or not complex terrain also modifies the background winds by inducing complex three dimensional wind patterns like channeling in canyons and valleys separations recirculations which induce downward momentum transport of upper winds and flow blocking due to large terrain features uchida and sugitani 2020 serafin et al 2018 terrain effects have been shown to have profound and unexpected effects on the direction of fire propagation sharples et al 2012 accounting for terrain induced mechanical effects on winds is the first step in building a coupled fire atmosphere model with sound predictive capabilities one approach to capturing the dynamic and heterogeneous influences of terrain on winds with the scope of predicting wildland fire behavior is solving the full navier stokes equations to resolve the flow at fireline scales which is done in firetec linn and cunningham 2005 and wfds mell et al 2013 models or larger scales using large eddy simulation les tools such as in cawfe coen 2013 wrf sfire mandel et al 2011 or wrf fire coen et al 2013 models like firetec and wfds are too computationally time intensive for a real time forecasting model and ensemble based prescribed fire planning the lower resolution les based tools use simplified fire spread models and vegetation representation that limit their generality and applicability for lower intensity fire scenarios typical of prescribed fires because they do not resolve the fine scale processes and three dimensional fuels structure which greatly influence fire propagation simpler and faster diagnostic wind models like windninja forthofer 2007 have not been coupled with fire models capable of addressing the complex fire spread and atmospheric response of the ignition patterns typical of prescribed fires and the multi front interaction of wildfires in complex terrain this gap has led to the development of quic fire linn et al 2020 which was intended to provide an alternative to computational fluid dynamics cfd based tools in particular quic fire was designed to be much less computationally expensive than the firetec wfds class of models and yet represent some of the processes and three dimensional structure of the fuel and winds at meter scales however the wind solver underlying quic fire the quick urban industrial complex wind solver quic urb does not currently include the influences of terrain on either the winds or the fire behavior this work is a first step towards incorporating the terrain s influence on the background wind fields in quic urb which will eventually be used with the fast running quic fire coupled fire model the complex terrain wind model presented in this paper is built upon quic urb singh et al 2008 quic urb is a diagnostic wind model that can produce mass consistent wind fields from multiple heterogeneous wind measurements over domains with sizes ranging from 1 km to 100 km at horizontal resolutions typically varying from 1 200 m singh et al 2008 prior to the terrain influences on winds presented in this manuscript background winds were specified for flat terrain scenarios the user can specify wind background profiles that follow a power law a log law or a set of measurements at multiple heights the mesoscale weather research and forecasting model wrf outputs usually at 1 5 km horizontal resolution can also be ingested kochanski et al 2015 nelson et al 2016 carvalho et al 2012 quic urb interpolates the provided wind measurements or wrf outputs over its domain using the barnes scheme barnes 1964 then the impacts of vegetative canopies and buildings are parameterized and superimposed over the background wind linn et al 2020 and finally mass consistency is imposed wind measurements at different times can be provided to follow the temporal variation of the background wind field the resulting wind field is time averaged the temporal or spatial wind variations associated with turbulent wind fluctuations are not resolved in this background wind field quic urb instead they are represented via stochastic turbulence model via the companion model quic plume a lagrangian random walk transport and dispersion model williams et al 2004 or parameterized following a shear based method in quic fire linn et al 2020 the advantages of a diagnostic wind solver over its prognostic counterparts lie in its speed and memory requirements indeed quic urb can generate wind fields for complex urban environments in less than 1 min with a common laptop i e it does not require super computing capabilities the disadvantage is that standard quic urb simulations capture only wind snapshots in time quasi steady state solutions are computed instead of the continuous evolution of the wind the quic urb framework was chosen because since 2015 it has been dynamically integrated with a fire model focusing on prescribed fire quic fire linn et al 2020 williams et al 2004 where the 3d vegetation structure is incorporated into the quic urb generated background wind field by superimposing drag affecting fire behavior therefore the addition of a complex terrain capability enhances the current predictive capabilities of wind plumes and fire in the following we will discuss the original quic urb coordinate system section 2 the complex terrain model formulation section 3 how to solve the resulting differential equation section 4 and two real world test cases of the model sections 5 and 6 this work will not address terrain induced thermodynamic effects on winds like katabatic winds 2 quic urb model description quic urb uses a cartesian coordinate system based on universal transverse mercator utm coordinates in which x and y are perpendicular and define the horizontal plane while z is vertical and positive in the direction opposite to gravity the quic urb cells are rectangular prisms in principle although usually the cell size in the x and y directions are the same in the z direction a stretched grid with smaller cells close to the ground is usually employed to capture the wind speed gradients at the surface while avoiding an unnecessary computational burden at higher heights where the wind speed does not change as much the original quic urb formulation sherman 1978 aims to minimize the integral 1 e u v w λ v α 1 2 u u 0 2 α 1 2 v v 0 2 α 2 2 w w 0 2 λ u x v y w z d x where u 0 v 0 and w 0 are the interpolated initial wind components in the x y and z directions respectively u v and w are the cartesian adjusted velocities defining a mass consistent flow field α 1 and α 2 are the gaussian precision moduli and λ is the lagrange multiplier the precision moduli define how much of the wind adjustment should be directed to changing u and v vs w hence if α 1 α 2 1 then w will change more than u and v in the original quic urb formulation α 1 α 2 since there is no reason to support weighing the adjustments differently in practice 1 expresses the concept that the desired adjusted wind field is mass consistent second term while keeping the adjustments as small as possible because the initial wind field comes from the interpolation of measurements the initial wind field typically is calculated from an inputted background wind speed direction and height recorded usually 10 m that is used to calculate the wind field as either a power law or log vertical profile the profile is fit to match the speed at the height input the vertical profile is then set for all cells in the domain the minimum of eq 1 is found by solving the euler lagrange equations 2a 2 λ 2 α 1 u 0 2b u u 0 1 2 α 1 2 λ x 2c v v 0 1 2 α 1 2 λ y 2d w w 0 1 2 α 2 2 λ z eq 2 is a system of partial differential equations for λ u v and w and u 0 u 0 v 0 w 0 in quic urb a finite difference method is used to solve equation 2 with first order central differences after discretizing the equation in λ we obtain a linear system that quic urb solves with the successive over relaxation sor method young 1954 which is a standard iterative method derived from the gauss seidel method the full derivation for flat terrain can be found in pardyjak and brown 2003 in this work we extend this method to terrain following coordinates 3 terrain following model extension to account for the effects of terrain generated winds terrain following tf coordinates are adopted denoted with x y z and defined as 3 x x 4 y y 5 z h z h x y h h x y where h is the domain height at the point of lowest elevation and h x y is the terrain elevation at x y therefore z 0 corresponds to the surface elevation z h x y solving 2 in tf coordinates to minimize 1 requires expressing the divergence and velocities in contravariant form moussiopoulos and flassak 1986 this requires using both the covariant and contravariant metric tensors for differentiation and transformations the covariant basis can be calculated by taking the derivative of the original coordinates with respect to the new coordinates 6 e i j x j x i e j and the contravariant basis can be calculated by taking the derivative of the new coordinates with respect to the original coordinates 7 e i j x i x j e j note that superscripts and subscripts will correspond to contravariant and covariant quantities respectively this results in the contravariant basis 1 1 subscripts including letters correspond to partial derivatives of the respective direction i e h x h x e 1 i e 2 j and 8 e 3 h x z h h h i h y z h h h j h h h k it is important to note that under extreme slope values the contravariant basis poorly spans the space at the surface i e e 3 e 1 0 if h x 0 in some cases it may be necessary to smooth terrain maps to ensure a solution is attainable the covariant and contravariant metric tensors can be calculated simply by 9 g i j e i e j and g i j e i e j resulting in a 3 3 matrix for each basis both metric tensors share the same determinant meaning they have the same normalization factor 10 g h h h the contravariant velocities are 11 u i u i v j w k e i resulting in u 1 u u 2 v and 12 u 3 u h x z h h h v h y z h h h w h h h with a covariant metric tensor g i j and a contravariant velocity field u i we calculate the divergence of u i to be 13 i u i 1 g i x i g u i where x i x y z this results in the divergence of the contravariant velocities having the form 14 u u 1 x u 2 y u 3 z 1 h h u 1 h x u 2 h y the contravariant velocities are transformed via u i u e i where e i are the contravariant basis using the transformations of 12 and 14 we transform 1 into contravariant terms to derive the transformed euler lagrange equations transforming 1 results in 15 e u 1 u 2 u 3 λ v α 1 2 u 1 u 0 1 2 α 1 2 u 2 u 0 2 2 α 2 2 u 3 u 0 3 h h h u 1 u 0 1 h x u 2 u 0 2 h y h z h 2 λ u 1 x u 2 y u 3 z 1 h h u 1 h x u 2 h y d x which can be written as e u 1 u 2 u 3 λ v l u 1 u 2 u 3 u 1 x u 2 y u 3 z λ d x when 15 is at its minimum energy then l f j 1 3 x j l f j 0 f j f x j where f u 1 u 2 u 3 u 1 x u 2 y u 3 z λ is any of the dependent variables the result is seven equations that can be written as 16 u 1 u 0 1 1 2 α 1 2 λ x λ z h z h h h x 17 u 2 u 0 2 1 2 α 1 2 λ y λ z h z h h h y 18 u 3 u 0 3 1 2 α 2 2 λ z h h h 2 1 2 α 1 2 h z h h h x λ x h y λ y λ z h z h h h x 2 h y 2 and the continuity equation 19 u 0 x 1 u 0 y 2 u 0 z 3 1 h h u 1 h x u 2 h y 0 eqs 16 17 and 18 are used to calculate the adjusted velocities once λ is determined by differentiating 16 17 and 18 substituting the expressions into 19 and rearranging we obtain 20 λ x x λ y y z h h h 2 h x 2 h y 2 η h h h 2 λ z z 2 z h h h h x λ x z h y λ y z z h h h h x x h y y 2 h h h x 2 h y 2 λ z 2 α 1 2 u 0 where η α 1 2 α 2 2 the laplacian of λ is i i λ 1 g i x i j g g i j λ x j resulting in 21 i i λ λ x x λ y y z h h h 2 h x 2 h y 2 h h h 2 λ z z 2 z h h h h x λ x z h y λ y z z h h h h x x h y y 2 h h h x 2 h y 2 λ z which shows we have recovered the contravariant form of 2 in 20 but with the gaussian precision moduli incorporated specifically in the form 2 λ 2 α 1 2 u 0 which is the general form of the equation solved in sherman 1978 open boundary conditions corresponding to λ 0 are implemented on the x y and top z boundaries on the surface a no flux of the wind velocity condition is enforced this boundary condition is satisfied by imposing u z 0 3 0 and enforcing that u 3 u 0 3 0 at z 0 in eq 18 this results in the neumann boundary condition 22 λ z h x λ x h y λ y h h h η h x 2 h y 2 for the terrain surface 4 computational method we solve 20 with the successive over relaxation sor method the discretization of the domain is the same as the original quic urb pardyjak and brown 2003 grid where the velocities are face centered and λ is cell centered except that it is discretized in sigma coordinate space the sigma coordinate space is a rectilinear grid that is uniform in x and y with vertically stacked horizontally uniform layers of varying thickness typically growing with height see fig 2 by choosing a maximum domain height h and discretizing the vertical grid at the same level for all x y positions a terrain following grid is produced fig 1 when z is transformed back to cartesian space by solving 5 for z the terrain following grid conforms to the topography near the surface and flattens to a horizontal plane at z h discretizing the derivatives in 20 using central difference stencils produces a sparse linear system that is solved using sor an illustration of the discretization points in the cartesian space for the stencils is in fig 2 single derivatives in the horizontal direction are discretized as λ x i j k λ i 1 j k λ i 1 j k 2 δ x second derivatives are discretized as λ x x i j k λ i 1 j k 2 λ i j k λ i 1 j k δ x 2 and cross derivatives are discretized as 23 λ x z i j k z λ x i j k 1 δ z k λ x i j k 0 5 λ x i j k 0 5 when calculating partial derivatives of λ in the x and y directions at horizontally oriented cell walls k 0 5 weighted averages based on cell height are used λ x i j k 0 5 1 δ z m k 1 z m k z k λ x i j k z k z m k 1 λ x i j k 1 for derivatives in the vertical direction non uniform cell heights must be taken into account since grid cells typically have increasing vertical size with increasing z the non uniformity is accounted for by taking the average of derivatives 24 λ z i j k 1 2 λ z i j k 0 5 λ z i j k 0 5 calculated across the top λ z i j k 0 5 λ i j k 1 λ i j k z m k 1 z m k and bottom of the cell λ z i j k 0 5 λ i j k λ i j k 1 z m k z m k 1 where z m are cell center heights for the corresponding cells the terrain surface boundary condition eq 22 must be satisfied for all bottom layer cells for λ z k 0 5 terms partial derivatives in the x and y directions are evaluated at the z 0 surface as λ x i j k 0 5 1 4 δ x λ i 1 j k 2 λ i 1 j k 2 λ i 1 j k 1 λ i 1 j k 1 which has a simple form due to both the surface cell and the under surface cell having an equal vertical cell height of δ z 1 to enforce the boundary condition λ values below the terrain surface are chosen so that 22 is satisfied to do this we enforce that there is no adjustment to u 0 3 0 at the surface using the discretized form of eq 18 setting u 3 u 0 3 0 and solving for λ i j k 1 this results in 25 λ k 1 λ k 2 δ z m k 1 h x 2 h y 2 η h h h h x 4 δ x λ i 1 k 2 λ i 1 k 2 λ i 1 k 1 λ i 1 k 1 h y 4 δ y λ j 1 k 2 λ j 1 k 2 λ j 1 k 1 λ j 1 k 1 2 2 missing i j k indices represent no shift in reference location i e λ k 2 λ i j k 2 by substituting the corresponding derivative expressions into 20 λ i j k satisfies the linear system 26 λ i j k i i 1 i 1 j j 1 j 1 k k 1 k 1 c i j k λ i j k d i j k 2 α 1 2 u 0 i j k d i j k for i j k i j k where c i j k and d i j k are the combined coefficients multiplying the corresponding λ i j k and λ i j k terms from the discretized form of 20 eq 26 is solved using the sor method which takes the form 27 λ i j k ℓ 1 1 ω λ i j k ℓ ω λ i j k where ω is the relaxation parameter and λ i j k incorporates λ values from both iteration ℓ and ℓ 1 eq 27 is iterated for all λ within the cells until convergence is reached we consider convergence reached when the average change in λ reaches less than 0 1 of the first initial iteration ϵ ℓ ϵ 1 0 001 where ϵ is defined as ϵ ℓ i j k λ i j k ℓ λ i j k ℓ 1 n cells and n cells is the number of cells used in the domain motivated by ross et al 1988 the vertical contravariant velocities above the surface are set as 28 u 0 3 h z h h h x u 0 1 h y u 0 2 to promote terrain effects on the wind this equates to w 0 which generates initial divergence in the field in the presence of terrain with w 0 everywhere in the domain it forces the solver to adjust for the no flow boundary condition at the surface the other reasonable initial vertical component is u 3 0 everywhere this results in the initial wind field being aligned with the terrain following grid everywhere already satisfying the surface boundary condition and minimal initial divergence being present u 3 z 0 this leads to a weak topographical influence seen in the resulting wind field and is not the preferred initial guess 5 isolated hill case askervein hill we test our methods on the real world terrain feature askervein hill 57 11 313 n 7 22 360 w taylor et al 1987 its simple geometry lack of vegetation and isolation from surrounding terrain means that the missing physical dynamics in quic urb i e buoyancy and momentum effects should not be a dominant factor in wind field measurements the simulation uses a similar setup as outlined in forthofer et al 2014 and wagenbrenner et al 2019 a comparison is made to measured data by using wind measurements during slightly stable atmospheric conditions richardson numbers between 0 0110 and 0 0074 along three transects at a height of 10 m that are provided in the mf03 d and tu03b datasets taylor et al 1987 sensor locations are illustrated in figs 3 and 4 topography with 23 m horizontal resolution from walmsley and taylor 1996 is used for the surface the 6 km 6 km domain is discretized into 257 257 horizontal cells a vertical extent of 760 m is discretized by a stretched grid of 18 cells growing in height with increasing z cell heights range in height from 10 m at the surface to 100 5 m at the top cell a reference velocity of 8 9 ms 1 210 from north measured at a location 3 km upstream is used to set a power law vertical wind profile to match the profile data described in forthofer et al 2014 eq 28 is used to set the contravariant vertical velocities so there is no vertical component in cartesian space above the surface using the most aggressive relaxation parameter that achieved convergence ω 1 9 349 iterations were executed on a laptop using 8 threads with openmp producing the analyzed wind field in a total of 6 s the wind speeds from the model results shown in figs 6 5 and 8 were linearly interpolated to 10 m above the surface for the comparison at the sensor locations the relative speed up is a ratio using the input reference wind speed and is calculated via relative speed up u m e a s u r e d u r e f u r e f where u m e a s u r e d is the velocity at a measurement site along a transect and u r e f is the velocity of the wind at the upwind measurement site that was used to generate the initial background wind field the quic urb model computed relative speed up vs distance near and along the hilltop transect b agrees well with measurements while the relative speed up vs distance along transects a and aa show that the model agreed well with measurements on the upwind side of the peak but overestimated speed up on the downwind side of the hill table 1 highlights the difference in accuracy between the winds calculated on the upwind and downwind sides of the peak with average percent error dropping from 28 4 to 6 83 for transect a and from 13 5 to 3 63 for transect b when winds on the lee side of the hill were omitted from error calculations the inability of the model to generate flow separation and wake eddy dynamics is most likely why wind speeds are not retarded sufficiently on the lee side of the hill transect a also contains wind direction measurements that are used to compare with quic urb model output simulation results show an average direction error of 6 80 with the largest deviation of 17 7 occurring on the leeward side of the hill in alignment with the poorer wind speed results in the same location the explanation for the differences on the downwind side of the hill is further supported by forthofer et al 2014 where it is shown that including momentum conservation to a mass conserving model improves the leeward side over prediction 6 steep complex terrain measurements from a 2019 field experiment at the energetic materials research and testing center emrtc in socorro nm 34 2 652 n 106 57 132 w are used to evaluate the performance of the terrain following algorithm in a highly complex terrain brown et al 2022 the experiment recorded wind speeds and directions over a few weeks in october and november 2019 from 16 sonic anemometers and vertical profiles from two lidars four days of the data were taken where ten minute averages were calculated from the 1 hz data to reduce turbulent signals for comparison with model output the terrain surrounding the measurement sites of the experiment lacks significant vegetation and is highly complex with many cliff faces peaks and valleys that can be seen in fig 7 measurements from one of the lidar units labeled as l1 in fig 7 are used to generate the initial wind field for the simulations by fitting a logarithmic profile to the 10 m measurement the vertical component of the initial velocity field is set to zero using eq 28 the domain for each simulation is 2 25 km 2 25 km 1 km discretized into 205 205 38 cells that grow in height with increasing z the horizontal extents of the domain encompass the sensor sites higher elevation terrain and a portion of the low lying flat section to the east fig 9 performance of the model in mountainous terrain is worse when compared to the results seen from the askervein domain results from mid day and evening measurements shown in fig 10 highlight areas where the models prediction capabilities are lacking although the model captures the magnitude and direction of inlet and outlet areas of the domain sensors 01 02 07 08 and 15 it performs rather poorly in the valley region sensors 03 10 and l2 and in the regions near the steep cliffs sensors 05 06 11 14 16 and 17 the poor performance in the cliff regions is due to the models lack of momentum conservation causing an inability to generate the wake regions on the leeward side of the steep terrain also the current lack of slope induced thermodynamically driven flows contributes to poor results in these regions although a version of the solver that includes slope induced flows is currently being worked on furthermore the model does not accurately capture the flow in the valley region because the model is unable to emulate boundary layer dynamics and thermodynamics near the surface nonetheless from a qualitative standpoint the model generates reasonable wind fields especially using only a single wind profile for input symbol description h total domain height h x y terrain height x partial derivative with respect to x x cartesian position u cartesian velocity x sigma coordinates position e i contravariant basis e i covariant basis u i contravariant velocity g i j contravariant metric tensor g i j covariant metric tensor g determinant of matrix g λ lagrange multiplier ω sor relaxation parameter α 1 α 2 gaussian precision moduli η squared ratio of α 1 over α 2 ℓ iteration number ϵ ℓ convergence metric n c e l l s number of cells used in a domain 7 conclusions current fire atmosphere coupled models cannot be used in ensemble based approaches to prescribed fire planning because they are either too computationally expensive e g firetec wfds or they are too coarse lacking the necessary level of detail e g wrf sfire cawfe the quic fire model was developed for this purpose to capture the fundamental physical phenomena while being computationally efficient and not require supercomputing capabilities however quic fire did not have complex terrain capabilities needed to address wildland fire problems in many parts of the world this paper discusses the mathematical formulation of a diagnostic wind model for flow over complex terrain using terrain following coordinates the model was implemented within quic urb which is the wind engine of quic fire and is based on the first order mechanical disturbance of the flow caused by terrain it does not account for wakes terrain blocking or thermodynamic effects the model shows strong performance in the canonical isolated hill askervein case capturing the general profile recorded by sensors it shows wind speedup at the top of the hill but overestimates wind speed on the lee side in a highly complex terrain case where measurements in the socorro mountain of new mexico were used the model struggles to capture flow dynamics near cliffs and in areas occluded by surrounding terrain however the model does capture the correct magnitude and direction of inlet and outlet areas of the domain it should be highlighted that the flow solutions were obtained quickly on a common laptop making the model appealing for coupled fire atmosphere simulations to help performance around cliffs future work includes extending the wake eddy parameterizations used in quic urb for urban environments pardyjak and brown 2003 to the terrain following coordinate system to address building like steep terrain features additional work is needed to capture flow dynamics that occur from thermodynamic effects and boundary layers although a version is currently being developed that includes slope induced flows despite these current shortcomings this work showed a promising first step into adding complex terrain winds to quic fire due to over prediction of winds on the lee side of steep terrain fire simulation results will have to be interpreted carefully in complex terrain regions before additional terrain effects are integrated into the wind solver declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments research presented in this article was supported by the laboratory directed research and development program of los alamos national laboratory under project number 20220024dr 
25489,groundwater contaminant source identification based on an ensemble learning search framework associated with an auto xgboost surrogate zidong pan a b c wenxi lu a b c han wang a b c yukun bai a b c a key laboratory of groundwater resources and environment ministry of education jilin university changchun 130021 china key laboratory of groundwater resources and environment ministry of education jilin university changchun 130021 china key laboratory of groundwater resources and environment ministry of education jilin university changchun 130021 china b jilin provincial key laboratory of water resources and environment jilin university changchun 130021 china jilin provincial key laboratory of water resources and environment jilin university changchun 130021 china jilin provincial key laboratory of water resources and environment jilin university changchun 130021 china c college of new energy and environment jilin university changchun 130021 china college of new energy and environment jilin university changchun 130021 china college of new energy and environment jilin university changchun 130021 china corresponding author college of new energy and environment jilin university changchun 130021 china college of new energy and environment jilin university changchun 130021 china groundwater contaminant source identification gcsi is commonly accompanied by search process which tweaks the unknown contaminant source information to match the simulation model outputs with the measurements when solving identification task search accuracy and time cost have always been challenges that must be tackled in the present study a novel ensemble learning search framework associated with auto extreme gradient boosting tree xgboost was proposed to solve gcsi in particular auto xgboost was employed to reduce the calculation burden caused by repeatedly running simulation model to promote search efficiency boosting strategy bos was employed to sequentially concatenate iterative ensemble smoother differential evolution particle filter depf and swarm evolution algorithm the identification results indicated that 1 auto xgboost could substitute a numerical simulation model with desired accuracy and expeditious running speed 2 bos could achieve better search accuracy but with the sacrifice of infinitesimal calculated time cost when compared with bagging strategy keywords auto xgboost ensemble learning search iterative ensemble smoother particle filter differential evolution swarm evolution algorithm data availability data will be made available on request 1 introduction groundwater contaminant source identification gcsi is crucial to groundwater risk assessment and remediation the identification task commonly involves reconstructing the spatial and temporal features of contaminant source from time sequence observation data in a nutshell the identification process is a search process which aims to adjust the simulation model parameters to match simulated output with approach observed data xu et al 2022 zhou et al 2014 domain experts have summarized the inverse methods to perform gcsi into three categories simulation optimization method ayvaz 2010 ayvaz 2016 hou and lu 2018 simulation bayesian method field et al 2016 ramgraber et al 2019 yan et al 2019 zhang et al 2013 and simulation data assimilation method jiang et al 2022 li et al 2018a panzeri et al 2014 sun et al 2009 white 2018 and the three practical and powerful algorithms of these methods are swarm evolution algorithm sea ayvaz 2016 han et al 2020 yeh 2015 particle filter pf field et al 2016 ramgraber et al 2019 and iterative ensemble smoother ies yang et al 2020 respectively although these methods have achieved prominent performance on a variety of groundwater source identification tasks there are still two main challenges that must be tackled namely search accuracy and search time cost with regard to search accuracy the performance of sea deeply relies on the initial population chiu 2014 li et al 2021 pf always suffers from particle degradation pan et al 2022 schoeniger et al 2012 and ies might lose its advantage in local search stage li et al 2018b yang et al 2020 with regard to search time cost the common drawback of the aforementioned methods is the certain need of repeatedly running high calculation cost simulation model guo et al 2019 mo et al 2019 schoeniger et al 2012 to tackle the search efficiency issue ensemble learning is employed to make the best use of the advantages and bypass the disadvantages of the three classical methods ensemble learning performs the learning task by training multiple component learners cl and can be divided into two basic categories bagging and boosting dong et al 2020 guo et al 2020 webb and zheng 2004 the bagging strategy bas is a parallel method where cls can be independently generated whereas the boosting strategy bos is a concatenation generation method where cls are sequentially generated dong et al 2020 sagi and rokach 2018 the evident advantage of bas is the swift processing speed brought by parallel running cls but possibly limited by weak cl saitoh 2016 zhao et al 2010 the advantage of bos is the boosting of weak cl in other words owing to concatenation every single cl can inherit the learning experience of the former cl guryanov 2019 in the present study we aimed to explore the potential of ensemble learning to improve the search efficiency of gcsi it must be noted that the sea pf and ies algorithms share a similar concept namely parallel search to promote ergodicity in other words the conception of population in sea particle in pf and ensemble in ies are the basic search units bsu with similar function when applied in a specific search task as a result it is feasible to integrate these algorithms with consistency as a boosting block to solve search task in gcsi the sequence to concatenate bsu can evidently influence the search efficiency thus the potential combination of bsu must be considered the boosting block consists of three search stages stage 1 ies search stage of global search stage 2 pf search stage of transition search and stage 3 sea search stage of local search while pf can perform better inheriting the search process of ies sea can continue the search task with a proper initial population to tackle the particle degradation issue we proposed a differential evolution particle filter depf in particular differential evolution de algorithm was employed to replace the random exploration pattern at the state transition of traditional pf which can substantially increase the diversity of particles in general the boosting block can be summarized as an integration structure of ies depf sea to manage the high calculation cost simulation model we introduced a cheap running cost surrogate model to substitute the origin simulation model in the past decades data driven surrogate models have drawn intensive attention in the gcsi domain to our knowledge the applications of parametric surrogate methods include kernel extreme learning machine hou et al 2021 zhao et al 2020 support vector machine bashi azghadi et al 2010 hou and lu 2018 gaussian process regressor zhang et al 2016 gradient boosting regression tree barzegar et al 2021 sachdeva and kumar 2021 and deep neural network dnn although dnns have gradually replaced traditional parametric approaches when tackling complex problems alom et al 2019 shrestha and mahmood 2019 large amounts of training parameters have led dnns to overfit the problem when compared with traditional approaches shorten and khoshgoftaar 2019 srivastava et al 2014 moreover the complicated structure of the hidden layer requires a rather long period of time to design it must be noted that a well trained parametric method can also achieve similar performance to dnns different empirical experiments shows that the hyperparameters prior fixed parameter in training have evident influence on the performance of parametric method probst et al 2019 stanley et al 2019 however to the best of our knowledge the traditional operation of tuning parametric method relies on manual trial or empirical estimation lee et al 2019 which limits practicability and transferability of parametric method therefore in the present study we designed an auto well tuning and easy to perform ensemble tree named extreme gradient boosting tree xgboost to substitute the high calculation cost origin model in particular the hyperparameters of the number of estimators max depth and min child weight were optimized using sea to select the surrogate model with the best performance in other words the proposed surrogate can automatically select appropriate hyperparameters combination to achieve the best generalization performance on the whole this study presented a novel ensemble boosting search algorithm associated with an auto machine learning surrogate as an inverse framework to solve gcsi fig 1 presents the basic flowchart of gcsi using the proposed framework a linear contaminant source identification case and an areal contaminant source identification under high dimensional hydraulic conductivity field were designed to evaluate the performance of the proposed method the main contributions of this study are the simplified and refined establishment of a data driven surrogate and introduction of a novel ensemble strategy to promote search efficiency of a gcsi case 2 methodology 2 1 case overview 2 1 1 scenario 1 low dimensional inverse estimation 2 1 1 1 basic assumption of the contaminant source and aquifer a hypothetical case was designed in the present study to evaluate the effectiveness and reliability of the proposed inverse framework the potential linear contaminant source was designed based on the study by pan et al 2022 the groundwater flow was two dimensional steady flow and the transportation of contaminant was assumed as no reaction the synthetic study domain was a 2500 m 1500 m section of unconfined aquifer with flow from up left to down right fig 2 the up and down boundaries were no flow boundary and left and right boundaries were specific head boundary the hydraulic conductivity field was divided into five zones k i k ii k iii k iv and k v with values of 34 0 m 27 0 m 30 0 m 22 0 m and 18 0 m respectively seven observation wells were established to monitor the transportation of contaminant for calculation of numerical simulation model the domain was discretized bygrids with size of 100 100 2 1 1 2 known information the known information about the linear source was as follows start position starts from l 1 and potential release stress periods t 1 t 2 t 3 t 4 t 5 although there were no real observed data in the hypothetical case reference values of unknown variables were allowed to generate a reference observed data by forward running the numerical simulation model fig 3 shows the reference contaminant distribution during the stress period in the present study the peak concentrations of seven observation wells were selected as prior measurements to perform identification task the detailed hydrogeological conditions are shown in table 1 2 1 1 3 unknown variables to estimate length indicator variable was proposed to reduce the numbers of unknown variables to estimate in particular the length indicator variable 1 2 3 4 and 5 denotes the potential length l1 l1 l2 l1 l2 l3 l1 l2 l3 l4 and l1 l2 l3 l4 l5 respectively it must be noted that all potential lengths of linear source were assumed to share the same release history in general the unknown variables involved fluxes during the stress period non integer variable namely s t 1 s t 2 s t 3 s t 4 s t 5 and length indicator variable integer variable 2 1 2 scenario 2 high dimensional inverse estimation 2 1 2 1 basic assumption of the contaminant source and aquifer to further test the performance of the proposed ensemble learning inverse framework a high dimensional inverse estimation scenario was designed the flow simulation is steady state but the transport simulation is transient the study domain is a section of 400 m 300 m of an unconfined aquifer with flow from west to east the north and south boundaries are no flow boundaries whereas the west and east boundaries are specific head boundaries with heads of 76 0 m and 75 2 m respectively the contaminant areal source released the contaminant into the aquifer in four stress periods the heterogenous hydraulic conductivity can be described as a gaussian field k the correlation function c k has been adopted to describe the spatial correlation of the hydraulic conductivity zhang et al 2016 1 c k x y x y σ k 2 exp x x λ x y y λ y where x y and x y denote different grid locations distance along respective cartesian coordinate axis σ k 2 denotes the variance of the hydraulic conductivity k λ x and λ y represent the correlation length singular value decomposition svd can be adopted to describe the hydraulic conductivity k as 2 c k u ζ v u m ζ m v m 3 ζ m d i a g e v 1 e v 2 e v m 4 q 100 i 1 m e v i 2 i 1 n e v i 2 m n 5 k z k z u m ζ m ε m where n is the number of grids of the simulation model u is an upper triangular matrix v is an lower triangular matrix ζ is a diagonal matrix named singular value matrix e v represents the singular value q represents the fidelity value m is the truncation number of diag elements of ζ when q 80 u m represnts the columns matrix of u between 1 to m v m represnts the rows matrix of v between 1 to m ε m is a matrix of which elements satisfies the standard normal distribution z x y k z is the mean value of hydraulic conductivity eight observation wells fig 4 a were established to observe the transportation of contaminant the observed contaninant concentrations were shown in fig 4 b for calculation of numerical simulation model the domain was discretized by grids with size of 10 10 table 2 presents the basic description of aquifer and contaminant source in scenario 2 2 1 2 2 unknown variables to estimate in this section we aims to simultaneously estimate the hydraulic conductivity field and release history of contaminant source in particular the heterogenous hydraulic conductivity field was characterized with twelve unknown coefficients ε m ε 1 ε 2 ε 3 ε 12 the release history involves four release intensities in stress periods p p t 1 p t 2 p t 3 p t 4 in general the unknown variables to estimate includes sixteen elements 2 1 2 2 1 numerical simulation model commonly the process of groundwater contaminant transport can be described by a groundwater numerical simulation model which involves groundwater flow sub model and solute transport sub model the governing formula of groundwater flow sub model can be expressed as 6 x k h z 0 h x y k h z 0 h y s x y t μ h t where x m and y m is the cartesian coordinates of grid locations k m d 1 denotes the hydraulic conductivity h m represents the water level above the sea level z 0 m denotes the elevation of the aquifer bottom above the sea level s denotes volume flux μ is specific yield the governing formula of groundwater solute transport sub model can be expressed as 7 c t x d c y x u c r θ where c m g l 1 represents the concentration of dissolved contaminant d is coefficient of hydrodynamic dispersion u m d 1 represents average linear velocity of the groundwater flow r is concentration of the dissolved contaminant in a source or sink fluid θ denotes effective porosity of the aquifer the numerical simulation model was solved using modflow langevin et al 2000 and mt3d module zheng and wang 1999 2 2 iterative ensemble smoother the ies is a variant of ensemble kalman filter which estimates unknown variables by updating the covariance matrix to reduce the deviation from the observed value the observed value of the numerical simulation model can be expressed as 8 y o b s f v ξ v v 1 v 2 v n where y o b s represents the observed matrix f denotes the numerical simulation model v represents the unknown variable matrix n denotes the number of variable ensemble and ξ represents the observed error which satisfies a standard normal distribution the unknown variables can be sequentially updated based on covariance which reflects the sensitivity of those variables evensen 2018 9 v j v j 1 g j y o b s f v j 1 10 g j c o v v j 1 f v j 1 c o v v j 1 f v j 1 r 1 11 r ξ 2 d i a g o n e s where c o v denotes the covariance matrix d i a g indicates the diagonal matrix and j represents the iteration step 2 3 differential evolution particle filter the pf is a sequential monte carlo method which estimates the unknown parameters variables matrix v by calculating the average values of the particle set the basic conception of pf is to update v based on the corresponding weights of every single sampling particle the state transformation of unknown variable v at j 1 th iteration step can be commonly expressed as a guassian exploration pattern jouin et al 2016 12 v j 1 v j 1 1 ρ where ρ represents a random matrix with elements that satisfy a standard normal distribution determining potential search range for every particle the weight w at j 1 th iteration can be calculated as 13 l v j 1 1 y o b s f v j 1 2 14 w j 1 l v j 1 s u m l v j 1 where l represents the likelihood function that satisfies the opposite changing trend of deviation term y o b s f v j 1 the estimate value of x at j th can be updated according to the acceptance rule v j δ v j 1 1 generate a random number range from 0 to 1 2 for the i th particle if u w j 1 n replace the i th particle with the n th particle else keep the i th particle however the gaussian exploration pattern of state transformation process of classical pf might lead to the lack of diversity of particles causing particle degradation therefore in the present study we introduced de exploration pattern to replace gaussian exploration pattern equation 12 for pf to enrich the diversity of particles to promote search capacity the de state transformation can be described as ter braak 2006 15 v j 1 i v j 1 i γ v j 1 a v j 1 b 16 γ 2 38 1 2 j 1 s q r t d 17 a b r s d i f f i 1 n 2 where γ is the decay index to guarantee the convergence which declines as step j increases γ v j 1 a v j 1 b represents the search term of the de which determines the search direction and magnitude d represents the dimension of unknown variables d i f f i 1 n denotes the difference between i and 1 n and r s d i f f i 1 n 2 indicates randomly selected two different elements from the difference 2 4 swarm evolution algorithm sea is a well known population based optimization algorithm which mimics the mutation crossover and selection behavior of biological community yildiz 2013 the fitness function o b j is commonly used to evaluate similar degree between the estimated population unknown variable matrix and ideal target reference value it can be expressed as 18 o b j v y o b s f v 2 the dominant individuals with low fitness value in the population were sequentially selected to perform further reproduction of mutation and crossover the reproduction continued until maximum iteration or minimum tolerance was reached the details of mutation crossover and selection process of sea can be found in the study by katoch et al 2021 2 5 auto extreme gradient boosting tree the xgboost is a variant of gradient boosting algorithm chen and guestrin 2016 and has been widely utilized in various domains owing to its flexibility for parameter tuning and promising generalization ability torlay et al 2017 zhang et al 2018 zheng et al 2017 the performance of xgboost can be controlled by hyperparameters such as the number of estimators also known as boosting iteration which determines the learning ability of the model max depth which limits the depth of regression tree to alleviate overfitting and min child weight also known as leaf nodes which simplifies the size of regression tree the main scheme of xgboost is to calculate the additive outputs of multiple basic estimators also called as regression trees as follows chen and guestrin 2016 19 c ˆ i φ v i k 1 n u m f k v i where v i represents the samples of unknown variables c ˆ i denotes the predicted outputs of numerical simulation model φ denotes the xgboost surrogate model f k indicates the outputs of every single basic estimator and num is the number of estimators the following objective is commonly used to learn the mapping relationship between input and output bhagat et al 2021 20 l φ i 1 n l c ˆ i c i k 1 n u m ω f k 21 ω f η t 1 2 τ w 2 where l is the bias function between the prediction values and target values ω is the regularization term that limits the complexity of regression trees to alleviate overfitting t is the number of leaf nodes and η and τare the constants to control the degree of regularization in general a regression task can be described as follows 22 c ˆ φ v θ f v where c ˆ is the predicted values of surrogate model φ denotes the surrogate model f denotes the numerical simulation model θ corresponds to the hyperparameters of the number of estimators max depth and min child weight it is common knowledge that the hyperparameters have certain impact on the performance of parametric algorithm however in most of the previous studies hyperparameters had been chosen based on expert experience or trial error methods which might miss the best hyperparameter combination yielding the best performance owing to its prominent search ability sea was introduced in the present study to optimize the hyperparameter which can automatically generate xgboost structure with the best performance for training data 3 application details 3 1 surrogate instantiation of surrogate was established based on xgboost module https xgboost ai com of a python environment the following experiments were computed on a pc with intel core i5 10 400 cpu 2 90 ghz processor and 32 0 gb ram latin hypercube sampling janssen 2013 was performed for 400 times for unknown variables within the prior ranges and 400 patterns of inputs x s i m were obtained the corresponding outputs y s i m were acquired by sequentially running the numerical simulation model the inputs and outputs were concatenated as a dataset that was randomly divided into training dataset 80 and validation dataset 20 then the dataset was fed to auto xgboost to be trained as a surrogate model of numerical simulation model the performance of surrogate model can be evaluated from the generalization accuracy and calculation time of specific running times in particular the coefficient of determination r was introduced to evaluate generalization accuracy 23 r 1 i 1 n y s i m i φ x s i m i 2 y s i m i m s i m 2 where φ x s i m i is the predicted values of the surrogate model and m s i m denotes the mean values of the outputs y s i m the auto xgboost can be described by an optimization task o b j m a x i m u m r 24 s t r f θ θ θ 1 θ 2 θ 3 1 θ 1 1000 1 θ 2 10 1 θ 3 30 where θ 1 represents the number of estimators θ 2 denotes max depth and θ 3 indicates min child weight the optimal value of θ can be solved by using sea the sea was performed with geatpy module http www geatpy com on python environment the parameters of encoding nind maxgen and trapped value were set to ri 20 50 and 1e 6 respectively 3 2 boosting search algorithm to achieve a fair comparison the ensemble particle population of the three search algorithms must be of the same size e g 1000 it must be noted that the ensemble size has certain influence on the performance it can be determined by selecting the size with the best performance through performing realizations under different size levels the sequence of the basic sub search unit of boosting search algorithm was ies depf and sea respectively and the performance of the three stages was supervised by vhat 20a v h a t i 1 7 y o b s i y p r e i 2 21a y p r e φ x for ies and depf stages the tolerance of vhat and maximum iteration were 1e 3 and 100 respectively and for sea search stage the parameters of encoding nind maxgen and trapped value were set to ri 1000 100 and 1e 3 respectively it must be noted that we used the least optimal values in ensemble of bos to calculate vhat to completely explore the search space the initial ensemble size of 1000 6 was generated with latin hypercube sampling for ies janssen 2013 the update of unknown variables was performed in the ies search stage and the ensemble with the best performance was selected as the initial particle for the next depf search stage simultaneously providing the minimum and maximum values of unknown variables as search boundaries for depf the depf inherited the best ensemble from ies to perform further search process similarly the particle with the best performance was selected as the initial prophet population for sea to start with simultaneously providing the minimum and maximum values of unknown variables as search boundaries for sea finally sea performed the search task with the succession of depf providing optimal estimation of unknown variables the calculation time of bos t b o o s t can be expressed as 22a t b o o s t t i e s t d e p f t s e a the bas was conducted by parallelly running the three search algorithms and the mean values of the three estimation results were considered as the optimal value for comparison with that of bos the calculation time of bas t b a g can be expressed as 23a t b a g max t i e s t d e p f t s e a 4 results and discussion 4 1 scenario 1 4 1 1 performance of auto xgboost fig 5 shows the trace of best r score of auto xgboost auto xgboost achieved better r score of 0 99303 at the 10 th epoch when compared with that at the first epoch the optimal combination of the number of estimators max depth and min child weight was 993 3 12 respectively fig 6 presents the comparison of numerical simulation model outputs and predicted responses of the surrogate models in particular the dnn is a 6 64 64 64 7 fully connected structure with l1 regularization it reveals that a well trained auto xgboost can achieve similar performance to dnn an r score of 0 99303 indicated promising generalization of auto xgboost enabling it to substitute the high calculation cost numerical simulation model with regard to the calculation time the origin simulation model required nearly 100 000 min for computing 1000 ensembles particles individuals during 100 iterations 100 000 running whereas auto xgboost only required 1458 25 s for the same number of calculations clearly demonstrating that auto xgboost contributes significant advantages of running speed to the simulation model 4 1 2 performance of search algorithm and estimation result fig 7 presents the performance comparison of the search algorithms basic search units of bos as ies achieved relatively good global search ability it was designed to perform the first search stage of bos the search criteria of classical pf smoothly declined but presented inapparent improvement since the 10th iteration depf reached better accuracy than classical pf but with fluctuating estimation in early iteration de substantially enhanced search capacity but limited speed of convergence however the balance of search capacity and speed of convergence has always been a paradox to compromise this issue we utilized depf rather than classical pf as the second search stage of bos and selected estimation particle with the best performance during the entire iteration of depf to transmit it to the next stage of bos as sea achieved good local search ability it was set at the final search stage of bos fig 8 presents the search plot of bos including the ies depf and sea search stages in particular when compared with ies sea search pattern fig 8 a ies depf sea pattern fig 8 b evidently alleviated being trapped into a local minimum which occurred in ies sea pattern in ies depf sea pattern it can be clearly observed that v h a t of the ies search stage presented a sharp decline trend and was focused on fast and global search v h a t of the depf search stage showed intensive exploration pattern and was focused on wide search and v h a t of sea search stage achieved convergent pattern and was focused on refined and local search table 3 shows the comparison between bos and bas bos achieved better estimation results but needed longer calculation time when compared with bas fig 9 illustrates the joint distribution of the unknown variables in the ies depf and sea search stages it can be observed that the estimation results of unknown variables in ies and depf generally satisfy normal distribution whereas those in sea satisfy uniform distribution moreover the estimation results with higher frequency are closer to the reference values table 4 presents the trace of reliable search ranges of every stage of bos the search range of unknown variables clearly showed a narrowing trend and the search was stage inherited the mean absolute deviate between reference values and estimated values was no more than 0 5 the estimated values of s t 1 s t 3 were evidently higher than those of s t 4 and s t 5 which indicated that the source began to release contaminant during t1 and terminated during t3 the indicator variable was 4 which revealed that the length of linear source was l1 l4 by comparison the estimation results were confirmed to be consistent with the reference value 4 2 scenario 2 4 2 1 performance of auto xgboost surrogate fig 10 presents the performance plot of auto xgboost the r score of 0 988 fig 10 b indicates that after well tuning fig 10 a auto xgboost achieved a promising performance to substitute the original simulation model 4 2 2 inverse estimation results fig 11 presents the search trace of bos in scenario 2 it can be observed that the vhat value substantially decreases in ies search stage which performs global search furthermore vhat value declines in depf search stage which performs wide search to realize ergodicity in ranges derived by ies sea performs a local search to provide optimal estimation results of unknown variables the estimation results of contaminant source and k field are shown in table 5 the overall inverse estimation accuracy is no more than 7 4 in particular the accuracy of k field is lower than that of release intensities this was because that when compared with hydraulic conductivity the contaminant concentrations in observation wells are more sensitive to features of contaminant source such as release intensity the results of twelve unknown coefficients to describe k field can be found in table 5 it can be observed that the estimation accuracy presents a descending trend from ε 1 to ε 12 that was because according to eq 4 and eq 5 the k field were sensitive to elements of the singular value matrix ζ the former elements such as ε 1 ε 2 of ζ are more representative than the latter elements such as ε 10 ε 11 ε 12 of ζ thus the estimation accuracy of coefficients of former elements are higher than that of latter elements the mre of reference and estimated k field is 7 31 fig 12 presents the visualization comparison between reference and estimated k field in general the inverse estimation accuracy of scenario 2 shows decline tendency with increasing dimensions brought by estimation of high dimensional gaussian k field to estimate nevertheless the bos remains a desired accuracy and promising performance for gcsi 5 conclusion in the present study an auto machine learning ensemble learning search algorithm was proposed as a framework to perform gpsi the estimation results revealed that the proposed framework exhibited prominent performance and practicability for gcsi task and the following conclusions could be drawn 1 the proposed auto xgboost is an easy to perform and accurate surrogate model of high calculation cost numerical simulation model which is conducive to reduce the calculation burden of further search stage however the potential of the auto xgboost to combine with other popular inverse frameworks needs to be explored the auto machine learning can be employed in various hydrogeology scenarios such as rainfall prediction risk assessment hydraulic parameter estimation etc 2 when compared with bas bos achieved better search accuracy with the sacrifice of calculation time the search pattern of global exploration wide exploration local exploration promoted the search efficiency of gcsi the latter search stage could inherit the search experience from the former search stage which guaranteed effectiveness of the search however when the dimension of variables to estimate increase the inverse estimation accuracy inevitably presents decline tendency to some degree authors contributions zidong pan conceptualization writing original draft software methodology wenxi lu writing review editing methodology software han wang supervision validation yukun bai supervision validation funding see the acknowledgments section availability of data and materials available from the corresponding author upon reasonable request code availability available upon reasonable request ethics approval not applicable consent to participate not applicable consent for publication not applicable declaration of competing interest the authors declared that they have no conflicts of interest to this work we declare that we do not have any commercial or associative interest that represents a conflict of interest in connection with the work submitted acknowledgments this research was supported by the national natural science foundation of china no 42272283 the national key research and development program of china no 2018yfc1800405 and the graduate innovation fund of jilin university no 2022060 
25489,groundwater contaminant source identification based on an ensemble learning search framework associated with an auto xgboost surrogate zidong pan a b c wenxi lu a b c han wang a b c yukun bai a b c a key laboratory of groundwater resources and environment ministry of education jilin university changchun 130021 china key laboratory of groundwater resources and environment ministry of education jilin university changchun 130021 china key laboratory of groundwater resources and environment ministry of education jilin university changchun 130021 china b jilin provincial key laboratory of water resources and environment jilin university changchun 130021 china jilin provincial key laboratory of water resources and environment jilin university changchun 130021 china jilin provincial key laboratory of water resources and environment jilin university changchun 130021 china c college of new energy and environment jilin university changchun 130021 china college of new energy and environment jilin university changchun 130021 china college of new energy and environment jilin university changchun 130021 china corresponding author college of new energy and environment jilin university changchun 130021 china college of new energy and environment jilin university changchun 130021 china groundwater contaminant source identification gcsi is commonly accompanied by search process which tweaks the unknown contaminant source information to match the simulation model outputs with the measurements when solving identification task search accuracy and time cost have always been challenges that must be tackled in the present study a novel ensemble learning search framework associated with auto extreme gradient boosting tree xgboost was proposed to solve gcsi in particular auto xgboost was employed to reduce the calculation burden caused by repeatedly running simulation model to promote search efficiency boosting strategy bos was employed to sequentially concatenate iterative ensemble smoother differential evolution particle filter depf and swarm evolution algorithm the identification results indicated that 1 auto xgboost could substitute a numerical simulation model with desired accuracy and expeditious running speed 2 bos could achieve better search accuracy but with the sacrifice of infinitesimal calculated time cost when compared with bagging strategy keywords auto xgboost ensemble learning search iterative ensemble smoother particle filter differential evolution swarm evolution algorithm data availability data will be made available on request 1 introduction groundwater contaminant source identification gcsi is crucial to groundwater risk assessment and remediation the identification task commonly involves reconstructing the spatial and temporal features of contaminant source from time sequence observation data in a nutshell the identification process is a search process which aims to adjust the simulation model parameters to match simulated output with approach observed data xu et al 2022 zhou et al 2014 domain experts have summarized the inverse methods to perform gcsi into three categories simulation optimization method ayvaz 2010 ayvaz 2016 hou and lu 2018 simulation bayesian method field et al 2016 ramgraber et al 2019 yan et al 2019 zhang et al 2013 and simulation data assimilation method jiang et al 2022 li et al 2018a panzeri et al 2014 sun et al 2009 white 2018 and the three practical and powerful algorithms of these methods are swarm evolution algorithm sea ayvaz 2016 han et al 2020 yeh 2015 particle filter pf field et al 2016 ramgraber et al 2019 and iterative ensemble smoother ies yang et al 2020 respectively although these methods have achieved prominent performance on a variety of groundwater source identification tasks there are still two main challenges that must be tackled namely search accuracy and search time cost with regard to search accuracy the performance of sea deeply relies on the initial population chiu 2014 li et al 2021 pf always suffers from particle degradation pan et al 2022 schoeniger et al 2012 and ies might lose its advantage in local search stage li et al 2018b yang et al 2020 with regard to search time cost the common drawback of the aforementioned methods is the certain need of repeatedly running high calculation cost simulation model guo et al 2019 mo et al 2019 schoeniger et al 2012 to tackle the search efficiency issue ensemble learning is employed to make the best use of the advantages and bypass the disadvantages of the three classical methods ensemble learning performs the learning task by training multiple component learners cl and can be divided into two basic categories bagging and boosting dong et al 2020 guo et al 2020 webb and zheng 2004 the bagging strategy bas is a parallel method where cls can be independently generated whereas the boosting strategy bos is a concatenation generation method where cls are sequentially generated dong et al 2020 sagi and rokach 2018 the evident advantage of bas is the swift processing speed brought by parallel running cls but possibly limited by weak cl saitoh 2016 zhao et al 2010 the advantage of bos is the boosting of weak cl in other words owing to concatenation every single cl can inherit the learning experience of the former cl guryanov 2019 in the present study we aimed to explore the potential of ensemble learning to improve the search efficiency of gcsi it must be noted that the sea pf and ies algorithms share a similar concept namely parallel search to promote ergodicity in other words the conception of population in sea particle in pf and ensemble in ies are the basic search units bsu with similar function when applied in a specific search task as a result it is feasible to integrate these algorithms with consistency as a boosting block to solve search task in gcsi the sequence to concatenate bsu can evidently influence the search efficiency thus the potential combination of bsu must be considered the boosting block consists of three search stages stage 1 ies search stage of global search stage 2 pf search stage of transition search and stage 3 sea search stage of local search while pf can perform better inheriting the search process of ies sea can continue the search task with a proper initial population to tackle the particle degradation issue we proposed a differential evolution particle filter depf in particular differential evolution de algorithm was employed to replace the random exploration pattern at the state transition of traditional pf which can substantially increase the diversity of particles in general the boosting block can be summarized as an integration structure of ies depf sea to manage the high calculation cost simulation model we introduced a cheap running cost surrogate model to substitute the origin simulation model in the past decades data driven surrogate models have drawn intensive attention in the gcsi domain to our knowledge the applications of parametric surrogate methods include kernel extreme learning machine hou et al 2021 zhao et al 2020 support vector machine bashi azghadi et al 2010 hou and lu 2018 gaussian process regressor zhang et al 2016 gradient boosting regression tree barzegar et al 2021 sachdeva and kumar 2021 and deep neural network dnn although dnns have gradually replaced traditional parametric approaches when tackling complex problems alom et al 2019 shrestha and mahmood 2019 large amounts of training parameters have led dnns to overfit the problem when compared with traditional approaches shorten and khoshgoftaar 2019 srivastava et al 2014 moreover the complicated structure of the hidden layer requires a rather long period of time to design it must be noted that a well trained parametric method can also achieve similar performance to dnns different empirical experiments shows that the hyperparameters prior fixed parameter in training have evident influence on the performance of parametric method probst et al 2019 stanley et al 2019 however to the best of our knowledge the traditional operation of tuning parametric method relies on manual trial or empirical estimation lee et al 2019 which limits practicability and transferability of parametric method therefore in the present study we designed an auto well tuning and easy to perform ensemble tree named extreme gradient boosting tree xgboost to substitute the high calculation cost origin model in particular the hyperparameters of the number of estimators max depth and min child weight were optimized using sea to select the surrogate model with the best performance in other words the proposed surrogate can automatically select appropriate hyperparameters combination to achieve the best generalization performance on the whole this study presented a novel ensemble boosting search algorithm associated with an auto machine learning surrogate as an inverse framework to solve gcsi fig 1 presents the basic flowchart of gcsi using the proposed framework a linear contaminant source identification case and an areal contaminant source identification under high dimensional hydraulic conductivity field were designed to evaluate the performance of the proposed method the main contributions of this study are the simplified and refined establishment of a data driven surrogate and introduction of a novel ensemble strategy to promote search efficiency of a gcsi case 2 methodology 2 1 case overview 2 1 1 scenario 1 low dimensional inverse estimation 2 1 1 1 basic assumption of the contaminant source and aquifer a hypothetical case was designed in the present study to evaluate the effectiveness and reliability of the proposed inverse framework the potential linear contaminant source was designed based on the study by pan et al 2022 the groundwater flow was two dimensional steady flow and the transportation of contaminant was assumed as no reaction the synthetic study domain was a 2500 m 1500 m section of unconfined aquifer with flow from up left to down right fig 2 the up and down boundaries were no flow boundary and left and right boundaries were specific head boundary the hydraulic conductivity field was divided into five zones k i k ii k iii k iv and k v with values of 34 0 m 27 0 m 30 0 m 22 0 m and 18 0 m respectively seven observation wells were established to monitor the transportation of contaminant for calculation of numerical simulation model the domain was discretized bygrids with size of 100 100 2 1 1 2 known information the known information about the linear source was as follows start position starts from l 1 and potential release stress periods t 1 t 2 t 3 t 4 t 5 although there were no real observed data in the hypothetical case reference values of unknown variables were allowed to generate a reference observed data by forward running the numerical simulation model fig 3 shows the reference contaminant distribution during the stress period in the present study the peak concentrations of seven observation wells were selected as prior measurements to perform identification task the detailed hydrogeological conditions are shown in table 1 2 1 1 3 unknown variables to estimate length indicator variable was proposed to reduce the numbers of unknown variables to estimate in particular the length indicator variable 1 2 3 4 and 5 denotes the potential length l1 l1 l2 l1 l2 l3 l1 l2 l3 l4 and l1 l2 l3 l4 l5 respectively it must be noted that all potential lengths of linear source were assumed to share the same release history in general the unknown variables involved fluxes during the stress period non integer variable namely s t 1 s t 2 s t 3 s t 4 s t 5 and length indicator variable integer variable 2 1 2 scenario 2 high dimensional inverse estimation 2 1 2 1 basic assumption of the contaminant source and aquifer to further test the performance of the proposed ensemble learning inverse framework a high dimensional inverse estimation scenario was designed the flow simulation is steady state but the transport simulation is transient the study domain is a section of 400 m 300 m of an unconfined aquifer with flow from west to east the north and south boundaries are no flow boundaries whereas the west and east boundaries are specific head boundaries with heads of 76 0 m and 75 2 m respectively the contaminant areal source released the contaminant into the aquifer in four stress periods the heterogenous hydraulic conductivity can be described as a gaussian field k the correlation function c k has been adopted to describe the spatial correlation of the hydraulic conductivity zhang et al 2016 1 c k x y x y σ k 2 exp x x λ x y y λ y where x y and x y denote different grid locations distance along respective cartesian coordinate axis σ k 2 denotes the variance of the hydraulic conductivity k λ x and λ y represent the correlation length singular value decomposition svd can be adopted to describe the hydraulic conductivity k as 2 c k u ζ v u m ζ m v m 3 ζ m d i a g e v 1 e v 2 e v m 4 q 100 i 1 m e v i 2 i 1 n e v i 2 m n 5 k z k z u m ζ m ε m where n is the number of grids of the simulation model u is an upper triangular matrix v is an lower triangular matrix ζ is a diagonal matrix named singular value matrix e v represents the singular value q represents the fidelity value m is the truncation number of diag elements of ζ when q 80 u m represnts the columns matrix of u between 1 to m v m represnts the rows matrix of v between 1 to m ε m is a matrix of which elements satisfies the standard normal distribution z x y k z is the mean value of hydraulic conductivity eight observation wells fig 4 a were established to observe the transportation of contaminant the observed contaninant concentrations were shown in fig 4 b for calculation of numerical simulation model the domain was discretized by grids with size of 10 10 table 2 presents the basic description of aquifer and contaminant source in scenario 2 2 1 2 2 unknown variables to estimate in this section we aims to simultaneously estimate the hydraulic conductivity field and release history of contaminant source in particular the heterogenous hydraulic conductivity field was characterized with twelve unknown coefficients ε m ε 1 ε 2 ε 3 ε 12 the release history involves four release intensities in stress periods p p t 1 p t 2 p t 3 p t 4 in general the unknown variables to estimate includes sixteen elements 2 1 2 2 1 numerical simulation model commonly the process of groundwater contaminant transport can be described by a groundwater numerical simulation model which involves groundwater flow sub model and solute transport sub model the governing formula of groundwater flow sub model can be expressed as 6 x k h z 0 h x y k h z 0 h y s x y t μ h t where x m and y m is the cartesian coordinates of grid locations k m d 1 denotes the hydraulic conductivity h m represents the water level above the sea level z 0 m denotes the elevation of the aquifer bottom above the sea level s denotes volume flux μ is specific yield the governing formula of groundwater solute transport sub model can be expressed as 7 c t x d c y x u c r θ where c m g l 1 represents the concentration of dissolved contaminant d is coefficient of hydrodynamic dispersion u m d 1 represents average linear velocity of the groundwater flow r is concentration of the dissolved contaminant in a source or sink fluid θ denotes effective porosity of the aquifer the numerical simulation model was solved using modflow langevin et al 2000 and mt3d module zheng and wang 1999 2 2 iterative ensemble smoother the ies is a variant of ensemble kalman filter which estimates unknown variables by updating the covariance matrix to reduce the deviation from the observed value the observed value of the numerical simulation model can be expressed as 8 y o b s f v ξ v v 1 v 2 v n where y o b s represents the observed matrix f denotes the numerical simulation model v represents the unknown variable matrix n denotes the number of variable ensemble and ξ represents the observed error which satisfies a standard normal distribution the unknown variables can be sequentially updated based on covariance which reflects the sensitivity of those variables evensen 2018 9 v j v j 1 g j y o b s f v j 1 10 g j c o v v j 1 f v j 1 c o v v j 1 f v j 1 r 1 11 r ξ 2 d i a g o n e s where c o v denotes the covariance matrix d i a g indicates the diagonal matrix and j represents the iteration step 2 3 differential evolution particle filter the pf is a sequential monte carlo method which estimates the unknown parameters variables matrix v by calculating the average values of the particle set the basic conception of pf is to update v based on the corresponding weights of every single sampling particle the state transformation of unknown variable v at j 1 th iteration step can be commonly expressed as a guassian exploration pattern jouin et al 2016 12 v j 1 v j 1 1 ρ where ρ represents a random matrix with elements that satisfy a standard normal distribution determining potential search range for every particle the weight w at j 1 th iteration can be calculated as 13 l v j 1 1 y o b s f v j 1 2 14 w j 1 l v j 1 s u m l v j 1 where l represents the likelihood function that satisfies the opposite changing trend of deviation term y o b s f v j 1 the estimate value of x at j th can be updated according to the acceptance rule v j δ v j 1 1 generate a random number range from 0 to 1 2 for the i th particle if u w j 1 n replace the i th particle with the n th particle else keep the i th particle however the gaussian exploration pattern of state transformation process of classical pf might lead to the lack of diversity of particles causing particle degradation therefore in the present study we introduced de exploration pattern to replace gaussian exploration pattern equation 12 for pf to enrich the diversity of particles to promote search capacity the de state transformation can be described as ter braak 2006 15 v j 1 i v j 1 i γ v j 1 a v j 1 b 16 γ 2 38 1 2 j 1 s q r t d 17 a b r s d i f f i 1 n 2 where γ is the decay index to guarantee the convergence which declines as step j increases γ v j 1 a v j 1 b represents the search term of the de which determines the search direction and magnitude d represents the dimension of unknown variables d i f f i 1 n denotes the difference between i and 1 n and r s d i f f i 1 n 2 indicates randomly selected two different elements from the difference 2 4 swarm evolution algorithm sea is a well known population based optimization algorithm which mimics the mutation crossover and selection behavior of biological community yildiz 2013 the fitness function o b j is commonly used to evaluate similar degree between the estimated population unknown variable matrix and ideal target reference value it can be expressed as 18 o b j v y o b s f v 2 the dominant individuals with low fitness value in the population were sequentially selected to perform further reproduction of mutation and crossover the reproduction continued until maximum iteration or minimum tolerance was reached the details of mutation crossover and selection process of sea can be found in the study by katoch et al 2021 2 5 auto extreme gradient boosting tree the xgboost is a variant of gradient boosting algorithm chen and guestrin 2016 and has been widely utilized in various domains owing to its flexibility for parameter tuning and promising generalization ability torlay et al 2017 zhang et al 2018 zheng et al 2017 the performance of xgboost can be controlled by hyperparameters such as the number of estimators also known as boosting iteration which determines the learning ability of the model max depth which limits the depth of regression tree to alleviate overfitting and min child weight also known as leaf nodes which simplifies the size of regression tree the main scheme of xgboost is to calculate the additive outputs of multiple basic estimators also called as regression trees as follows chen and guestrin 2016 19 c ˆ i φ v i k 1 n u m f k v i where v i represents the samples of unknown variables c ˆ i denotes the predicted outputs of numerical simulation model φ denotes the xgboost surrogate model f k indicates the outputs of every single basic estimator and num is the number of estimators the following objective is commonly used to learn the mapping relationship between input and output bhagat et al 2021 20 l φ i 1 n l c ˆ i c i k 1 n u m ω f k 21 ω f η t 1 2 τ w 2 where l is the bias function between the prediction values and target values ω is the regularization term that limits the complexity of regression trees to alleviate overfitting t is the number of leaf nodes and η and τare the constants to control the degree of regularization in general a regression task can be described as follows 22 c ˆ φ v θ f v where c ˆ is the predicted values of surrogate model φ denotes the surrogate model f denotes the numerical simulation model θ corresponds to the hyperparameters of the number of estimators max depth and min child weight it is common knowledge that the hyperparameters have certain impact on the performance of parametric algorithm however in most of the previous studies hyperparameters had been chosen based on expert experience or trial error methods which might miss the best hyperparameter combination yielding the best performance owing to its prominent search ability sea was introduced in the present study to optimize the hyperparameter which can automatically generate xgboost structure with the best performance for training data 3 application details 3 1 surrogate instantiation of surrogate was established based on xgboost module https xgboost ai com of a python environment the following experiments were computed on a pc with intel core i5 10 400 cpu 2 90 ghz processor and 32 0 gb ram latin hypercube sampling janssen 2013 was performed for 400 times for unknown variables within the prior ranges and 400 patterns of inputs x s i m were obtained the corresponding outputs y s i m were acquired by sequentially running the numerical simulation model the inputs and outputs were concatenated as a dataset that was randomly divided into training dataset 80 and validation dataset 20 then the dataset was fed to auto xgboost to be trained as a surrogate model of numerical simulation model the performance of surrogate model can be evaluated from the generalization accuracy and calculation time of specific running times in particular the coefficient of determination r was introduced to evaluate generalization accuracy 23 r 1 i 1 n y s i m i φ x s i m i 2 y s i m i m s i m 2 where φ x s i m i is the predicted values of the surrogate model and m s i m denotes the mean values of the outputs y s i m the auto xgboost can be described by an optimization task o b j m a x i m u m r 24 s t r f θ θ θ 1 θ 2 θ 3 1 θ 1 1000 1 θ 2 10 1 θ 3 30 where θ 1 represents the number of estimators θ 2 denotes max depth and θ 3 indicates min child weight the optimal value of θ can be solved by using sea the sea was performed with geatpy module http www geatpy com on python environment the parameters of encoding nind maxgen and trapped value were set to ri 20 50 and 1e 6 respectively 3 2 boosting search algorithm to achieve a fair comparison the ensemble particle population of the three search algorithms must be of the same size e g 1000 it must be noted that the ensemble size has certain influence on the performance it can be determined by selecting the size with the best performance through performing realizations under different size levels the sequence of the basic sub search unit of boosting search algorithm was ies depf and sea respectively and the performance of the three stages was supervised by vhat 20a v h a t i 1 7 y o b s i y p r e i 2 21a y p r e φ x for ies and depf stages the tolerance of vhat and maximum iteration were 1e 3 and 100 respectively and for sea search stage the parameters of encoding nind maxgen and trapped value were set to ri 1000 100 and 1e 3 respectively it must be noted that we used the least optimal values in ensemble of bos to calculate vhat to completely explore the search space the initial ensemble size of 1000 6 was generated with latin hypercube sampling for ies janssen 2013 the update of unknown variables was performed in the ies search stage and the ensemble with the best performance was selected as the initial particle for the next depf search stage simultaneously providing the minimum and maximum values of unknown variables as search boundaries for depf the depf inherited the best ensemble from ies to perform further search process similarly the particle with the best performance was selected as the initial prophet population for sea to start with simultaneously providing the minimum and maximum values of unknown variables as search boundaries for sea finally sea performed the search task with the succession of depf providing optimal estimation of unknown variables the calculation time of bos t b o o s t can be expressed as 22a t b o o s t t i e s t d e p f t s e a the bas was conducted by parallelly running the three search algorithms and the mean values of the three estimation results were considered as the optimal value for comparison with that of bos the calculation time of bas t b a g can be expressed as 23a t b a g max t i e s t d e p f t s e a 4 results and discussion 4 1 scenario 1 4 1 1 performance of auto xgboost fig 5 shows the trace of best r score of auto xgboost auto xgboost achieved better r score of 0 99303 at the 10 th epoch when compared with that at the first epoch the optimal combination of the number of estimators max depth and min child weight was 993 3 12 respectively fig 6 presents the comparison of numerical simulation model outputs and predicted responses of the surrogate models in particular the dnn is a 6 64 64 64 7 fully connected structure with l1 regularization it reveals that a well trained auto xgboost can achieve similar performance to dnn an r score of 0 99303 indicated promising generalization of auto xgboost enabling it to substitute the high calculation cost numerical simulation model with regard to the calculation time the origin simulation model required nearly 100 000 min for computing 1000 ensembles particles individuals during 100 iterations 100 000 running whereas auto xgboost only required 1458 25 s for the same number of calculations clearly demonstrating that auto xgboost contributes significant advantages of running speed to the simulation model 4 1 2 performance of search algorithm and estimation result fig 7 presents the performance comparison of the search algorithms basic search units of bos as ies achieved relatively good global search ability it was designed to perform the first search stage of bos the search criteria of classical pf smoothly declined but presented inapparent improvement since the 10th iteration depf reached better accuracy than classical pf but with fluctuating estimation in early iteration de substantially enhanced search capacity but limited speed of convergence however the balance of search capacity and speed of convergence has always been a paradox to compromise this issue we utilized depf rather than classical pf as the second search stage of bos and selected estimation particle with the best performance during the entire iteration of depf to transmit it to the next stage of bos as sea achieved good local search ability it was set at the final search stage of bos fig 8 presents the search plot of bos including the ies depf and sea search stages in particular when compared with ies sea search pattern fig 8 a ies depf sea pattern fig 8 b evidently alleviated being trapped into a local minimum which occurred in ies sea pattern in ies depf sea pattern it can be clearly observed that v h a t of the ies search stage presented a sharp decline trend and was focused on fast and global search v h a t of the depf search stage showed intensive exploration pattern and was focused on wide search and v h a t of sea search stage achieved convergent pattern and was focused on refined and local search table 3 shows the comparison between bos and bas bos achieved better estimation results but needed longer calculation time when compared with bas fig 9 illustrates the joint distribution of the unknown variables in the ies depf and sea search stages it can be observed that the estimation results of unknown variables in ies and depf generally satisfy normal distribution whereas those in sea satisfy uniform distribution moreover the estimation results with higher frequency are closer to the reference values table 4 presents the trace of reliable search ranges of every stage of bos the search range of unknown variables clearly showed a narrowing trend and the search was stage inherited the mean absolute deviate between reference values and estimated values was no more than 0 5 the estimated values of s t 1 s t 3 were evidently higher than those of s t 4 and s t 5 which indicated that the source began to release contaminant during t1 and terminated during t3 the indicator variable was 4 which revealed that the length of linear source was l1 l4 by comparison the estimation results were confirmed to be consistent with the reference value 4 2 scenario 2 4 2 1 performance of auto xgboost surrogate fig 10 presents the performance plot of auto xgboost the r score of 0 988 fig 10 b indicates that after well tuning fig 10 a auto xgboost achieved a promising performance to substitute the original simulation model 4 2 2 inverse estimation results fig 11 presents the search trace of bos in scenario 2 it can be observed that the vhat value substantially decreases in ies search stage which performs global search furthermore vhat value declines in depf search stage which performs wide search to realize ergodicity in ranges derived by ies sea performs a local search to provide optimal estimation results of unknown variables the estimation results of contaminant source and k field are shown in table 5 the overall inverse estimation accuracy is no more than 7 4 in particular the accuracy of k field is lower than that of release intensities this was because that when compared with hydraulic conductivity the contaminant concentrations in observation wells are more sensitive to features of contaminant source such as release intensity the results of twelve unknown coefficients to describe k field can be found in table 5 it can be observed that the estimation accuracy presents a descending trend from ε 1 to ε 12 that was because according to eq 4 and eq 5 the k field were sensitive to elements of the singular value matrix ζ the former elements such as ε 1 ε 2 of ζ are more representative than the latter elements such as ε 10 ε 11 ε 12 of ζ thus the estimation accuracy of coefficients of former elements are higher than that of latter elements the mre of reference and estimated k field is 7 31 fig 12 presents the visualization comparison between reference and estimated k field in general the inverse estimation accuracy of scenario 2 shows decline tendency with increasing dimensions brought by estimation of high dimensional gaussian k field to estimate nevertheless the bos remains a desired accuracy and promising performance for gcsi 5 conclusion in the present study an auto machine learning ensemble learning search algorithm was proposed as a framework to perform gpsi the estimation results revealed that the proposed framework exhibited prominent performance and practicability for gcsi task and the following conclusions could be drawn 1 the proposed auto xgboost is an easy to perform and accurate surrogate model of high calculation cost numerical simulation model which is conducive to reduce the calculation burden of further search stage however the potential of the auto xgboost to combine with other popular inverse frameworks needs to be explored the auto machine learning can be employed in various hydrogeology scenarios such as rainfall prediction risk assessment hydraulic parameter estimation etc 2 when compared with bas bos achieved better search accuracy with the sacrifice of calculation time the search pattern of global exploration wide exploration local exploration promoted the search efficiency of gcsi the latter search stage could inherit the search experience from the former search stage which guaranteed effectiveness of the search however when the dimension of variables to estimate increase the inverse estimation accuracy inevitably presents decline tendency to some degree authors contributions zidong pan conceptualization writing original draft software methodology wenxi lu writing review editing methodology software han wang supervision validation yukun bai supervision validation funding see the acknowledgments section availability of data and materials available from the corresponding author upon reasonable request code availability available upon reasonable request ethics approval not applicable consent to participate not applicable consent for publication not applicable declaration of competing interest the authors declared that they have no conflicts of interest to this work we declare that we do not have any commercial or associative interest that represents a conflict of interest in connection with the work submitted acknowledgments this research was supported by the national natural science foundation of china no 42272283 the national key research and development program of china no 2018yfc1800405 and the graduate innovation fund of jilin university no 2022060 
