index,text
2645,in this study infrared thermal imaging technology is adopted to measure water content at soil surface by calibrating the linear relationship between water content at soil surface and interfacial temperature difference of a given soil type the method is suitable for the soil surface which is unsaturated first of all the theoretical considerations of the new method are introduced infrared thermal imaging technology can directly measure soil surface temperature and the difference between the soil surface temperature and the ambient temperature is named as the interfacial temperature difference for over saturated soils at a certain environmental condition the interfacial temperature difference of a given soil type is a constant value for unsaturated soils there is a linear relationship between water content at soil surface and interfacial temperature difference for the tested four types of soils with different textures all the values of r2 for the linear fitting are greater than 0 9 then the water content at soil surface can be calculated based on the surface temperature measured by the infrared camera moreover in this study a cracked soil sample with an uneven surface water content field is tested for validating the feasibility of the proposed method there is a good match between the data measured by the proposed method and traditional oven drying it indicates that the proposed method can be used to obtain the distribution of water content on the soil surface and may provide a new idea for the future research on the spatio temporal evolution characteristics of in situ soil under climate change and provide a new choice for leakage detection of structures like dams tunnels etc keywords evaporation temperature water content thermal infrared imaging data availability data will be made available on request 1 introduction water content at soil surface usually refers to the water content of soil within 2 cm depth kimura et al 2009 it plays a paramount role in the soil atmosphere continuum in many disciplines in geotechnical and geological engineering the distribution and evolution of water content at soil surface contributes to soil deformation and cracking behaviour ng and menzies 2007 tang et al 2008 li and zhang 2011 zhang et al 2021 in hydrology water content at soil surface is of particular interest with respect to evaporation infiltration and runoff which are closely related to soil erosion and river discharge huisman et al 2003 ma et al 2016 jiang et al 2019 cheng et al 2021 in agriculture water content at soil surface has a significant influence on plant growth robinson et al 2008 water content determination methods include direct method and indirect method the so called direct method is thermal drying particularly oven drying schmugge et al 1980 it measures the average water content of a soil block by sampling for soils with and without organic matter the chosen drying temperatures are different this method is simple but is not feasible for field testing in field work the alcohol burning method can be used instead but not for soils containing a high proportion of organic matter ma et al 2016 in general the direct method disturbs the field and changes the hydro mechanical properties of the soil sample as it changes the soil water content to overcome the drawbacks of direct measurement researchers have developed many indirect methods in the past few decades to achieve non destructive measurement neutron scattering and x ray computed tomography are one typical kind of indirect method to measure soil water content phogat et al 1991 sheets and hendrickx 1995 chanasyk and naeth 1996 fityus et al 2011 zhang et al 2015 balaghi et al 2018 the principle is that when ray passes through soil directly the energy will decay according to the different attenuation amounts in the solid phase and liquid phase the water content of soil can be obtained the measured water content is also the average value of soil blocks with a radius of about a few to tens of centimetres this kind of method can be used for continuous observation at a fixed point but it cannot realize long term monitoring in a large area moreover the equipment is expansive and may have a potential radiation hazard to people so it is not widely used the second kind of indirect method is to use dielectric techniques including microwave time domain transmission time domain reflectometry frequency domain reflectometry electrical resistivity tomography ground penetrating radar etc jacobsen and schjonning 1993 gardner et al 1997 topp and reynolds 1998 huisman et al 2003 kelleners et al 2004 brunet et al 2010 rao and singh 2011 zheng et al 2011 you et al 2013 zhan et al 2015 alamry et al 2017 kafarski et al 2020 bittelli et al 2021 sun et al 2021 agyeman et al 2021 the main principle is to determine soil water content indirectly by measuring the dielectric permittivity such methods have the advantages of being fast and safe and can be used to measure soil water content at different depths however the above mentioned geophysical surveys can be significantly affected by the salinity of a soil and many researches have been conducted to deal with this drawback hanson and kaita 1997 peng et al 2019 ying et al 2021 moreover the measurement results are easily affected by environmental conditions such as temperature overall there is a lack of a fast non contact non destructive and stable method for measuring the water content at soil surface infrared thermal imaging technology is a non contact and non destructive method which is able to accurately acquire a detailed temperature distribution field of the target surface in real time the camera converts the invisible infrared energy emitted by the surface of a certain objective into a 2d visual image in recent years infrared thermal imaging has been adopted for identifying and mapping cracks and fractures especially on rock masses liu et al 2016 li et al 2018 cai et al 2020 miao et al 2022 zeng et al 2022 moreover this technology has been widely used to assess different processes that occur at the soil surface level such as evaporation preferential infiltration crust formation soil water repellency assessment etc danielescu et al 2009 pfister et al 2010 shahraeeni and or 2010 soliman et al 2010 de lima and abrantes 2014 sadeghi et al 2015 dimitrov et al 2015 abrantes et al 2017 perez garcia et al 2018 abdulkareem et al 2020 moreover more and more researches have been conducted recently thanks to the easy handling and adjustment of measurement distance and scale of portable hand held thermography systems the principal objective of this study is to investigate the feasibility of applying the infrared thermal imaging technology to measure water content at soil surface the proposed method was based on a linear relationship between water content at soil surface and interfacial temperature difference which was deduced by the theoretical considerations a clayey soil was used as an example to show the calibration process then a cracked soil sample with an uneven surface water content field is tested for validating the feasibility of the proposed method 2 materials and methods 2 1 testing materials in this study a clayey soil collected from nanjing china was used it is wildly distributed in the middle and lower reaches of yangtze river its mineral composition is mainly illite and illite montmorillonite interlayer minerals the fractions of sand silt and clay are 2 76 and 22 respectively according to the unified soil classification system astm d 2487 2017 it is classified as a lean clay cl table 1 summarizes the physical properties of the tested soil 2 2 sample preparation and testing procedure the collected soil was air dried crushed and passed through a 2 mm sieve to prepare a saturated slurry the sieved soil powers were mixed with distilled water to reach a target water content of 55 a desired quantity of slurry was poured onto circular aluminium boxes with an inner diameter of 40 mm the slurry was vibrated on the shaking table for 5 min to eliminate the bubbles generated in the mixing process then the containers were sealed with a plastic membrane for at least 48 h the sample preparation procedure follows previous researches burland 1990 cotecchia and chandler 2000 liu and carter 2000 hong et al 2012 cheng et al 2020 the final settled slurry thicknesses were about 10 mm after that the soil sample was setting up in the testing apparatus and subjected to drying note that in this study two replicates were prepared and the difference between the two replicates was less than 5 this suggests that the laboratory data is reliable 2 3 testing apparatus the soil sample was placed on a balance with an accuracy of 0 01 g to measure the water content change during the drying process a thermal infrared camera flir t620 ir was fixed 50 cm above the sample surface to capture the temperature evolution with elapsed drying time adjust the lens of the camera so that the field of view is 50 mm by 50 mm slightly larger than the sample size the adopted infrared camera has a working wavelength of 7 8 14 0 μm and a spatial resolution of 640 480 pix its temperature sensitivity and measurement accuracy can reach 0 04 and 0 1 respectively fig 1 shows the schematic diagram of the experimental set up the experimental setup was placed in an environment chamber the ambient temperature and relative humidity were controlled at 30 2 and 70 3 respectively it should be noted that the sample was placed in the environment chamber and sealed for 12 h before drying so that the initial temperature of the sample is basically consistent with the ambient temperature 3 theoretical considerations of the proposed method to measure water content at soil surface 3 1 effect of water content on evaporation from bare soils based on a large amount of experimental data on evaporation yanful and choo 1997 baruah and hasegawa 2001 newson and fahey 2003 parvin et al 2017 an et al 2018 there is a consensus that at the early stage of drying the evaporation rate almost keeps constant for an initially saturated soil when air goes into the soil sample and the sample becomes unsaturated the evaporation rate falls dramatically and shows an approximately linear relationship with decreasing water content when the water content keeps decreasing to lower than the residual water content the evaporation rate tends to be zero and the water transfer is mainly due to vapour flux upward through the soil pores bittelli et al 2008 thus for the evaporation rate of unsaturated soils the following equation is proposed 1 e e 0 ω ω sat a ω b ω ω sat where e is the evaporation rate evaporation is positive and condensation is negative mm h e 0 is the evaporation rate of an saturated or over saturated soil ω is the water content of soil ω sat is the water content of soil at saturated state a and b are soil parameters using the tested soil as an example the change of evaporation rate with elapsed drying time is shown in fig 2 the evaporation rate is obtained from changes in sample weight measured by the balance the three stages of evaporation are clearly shown in the figure similar to previous studies on evaporation yanful and choo 1997 baruah and hasegawa 2001 newson and fahey 2003 parvin et al 2017 an et al 2018 stage ⅰ constant rate stage in this stage the evaporation rate of the soil sample almost remains unchanged the soil sample is still fully saturated at this stage and there is enough water in the soil to supply evaporation stage ⅱ falling rate stage in this stage the evaporation rate of the soil sample decreases dramatically with elapsed drying time the soil sample becomes unsaturated and soil suction increases during the drying process moreover the water permeability also decreases resulting in a decrease in the evaporation rate stage iii residual stage in this stage the evaporation rate of the soil sample gradually approaches zero the change of evaporation rate with decreasing water content of the tested soil is shown in fig 3 in the water content range from 55 to 22 the soil sample is saturated and the evaporation rate almost keeps constant with a small fluctuation as drying continues when the water content range decreases from 22 to the residual water content of about 13 there is a good linear relationship between the evaporation rate and the water content proving the hypothesis of eq 1 is reasonable the residual water content for the same clayey soil in the literature varies from 5 to 10 which is smaller than the value in this study this is because of the influence of soil layer thickness as evidenced by an et al 2018 the thinner the soil sample the larger the residual water content by linear fitting the values of parameters a and b are 1 33 and 0 18 respectively r square 0 9545 3 2 evolution of soil surface temperature during evaporation during the evaporation process the energy exchange between soil and air obeys the law of energy conservation if the effect of atmospheric transverse convection is assumed to be negligible the surface energy balance during evaporation from bare soil can be expressed as follows penman 1948 2 l e e r n g h where l e is the latent heat of vaporization representing the energy required for water to change from liquid to gas 2450 j g under atmospheric pressure r n is the net radiation flux which is the main source of energy flux intercepted on a surface incidence is positive and scattering is negative w m2 g is the soil heat flux the increase of soil temperature caused by energy is positive while the decrease is negative w m2 h is the sensible heat flux energy used to heat the air is positive and energy lost by air cooling is negative w m2 it should be noted that the net radiation flux r n consists of incident solar shortwave radiation atmospheric scattering and outgoing surface reflection and surface long wave radiation there is a certain proportion between the soil heat flux g and the net radiation flux r n which is related to soil properties for example the soil heat flux g of coarse sand can be approximately one fifth of the net radiation flux r n qiu et al 1998 thus the soil heat flux g can be regarded as follows 3 g γ r n where γ represents the ratio of the soil heat flux g and the net radiation flux r n the sensible heat flux h can be estimated using aerodynamic resistance as blight 1997 4 h ρ c p t s t a r a where ρ is the air density c p is the specific heat capacity of air at constant pressure t s is the temperature at soil surface t a is the air temperature at a reference height r a is the unit thermal resistance of air when combing eqs 1 4 the evaporation rate can be expressed as follows 5 e 1 γ r n l e ρ c p l e r a t s t a if the effect of atmospheric scattering is negligible and the light in the evaporation process is constant that is to say the ambient temperature and humidity and radiation intensity remain unchanged ρ c p l e is a constant then eq 4 can be simplified as 6 e α β t s t a where α is parameter related to radiation intensity and soil properties and β is parameter related to the unit thermal resistance of air it can be found that during the drying process there is a linear relationship between the evaporation rate e and the interfacial temperature difference t s t a for the tested soil the evolution of temperature field at soil surface with elapsed drying time is shown in fig 4 it can be found that at the early stage of drying soil surface temperature has little change with time with the continuous evaporation the soil surface temperature gradually increases at the end of evaporation the surface temperature of samples tends to be consistent with the ambient temperature in general at a certain time the surface temperature field of each soil sample is evenly distributed the difference between the average surface temperature and the ambient temperature which is defined as the interfacial temperature difference can be obtained the evolution of interfacial temperature difference with elapsed drying time is shown in fig 5 the variation trend of temperature difference over time is consistent with that of evaporation rate over time the specific time corresponding to the turning point of each stage is roughly the same which indicates that the change of soil surface temperature can respond to the evaporation process synchronously specifically for the tested soil sample stage ⅰ ends around 50 h and stage iii begins around 68 h this is because evaporation of soil water is an endothermic process the greater the evaporation rate the more intense the heat release and the lower corresponding surface temperature corresponding to the three stages of evaporation rate evolution the change of soil surface temperature in the drying process can be roughly divided into three stages as well stage ⅰ constant low temperature stage in this stage there is a high content of free water in the liquid phase and almost all the energy provided by radiation and heat transfer is transferred to the liquid phase in the soil resulting in the increase of entropy in the liquid phase when the entropy increases to a certain threshold value the free water in liquid phase transforms into water vapour that is soil water evaporates the loss of heat energy during phase transformation leads to a breaking of the thermodynamic balance between the liquid and solid phases in soil a thermal gradient is formed between the two phases and then the solid phase continuously supplies energy to the liquid phase as a result the heat in the solid phase of soil decreases and the soil temperature drops resulting in a relatively large interfacial temperature difference moreover with a constant evaporation rate the heat transfer between liquid and solid phase is stable thus the soil sample keeps a constant low temperature in this stage stage ⅱ increasing temperature stage with continuous drying the free water in soil decreases and the proportion of bound water increases gradually compared with free water the entropy value of bound water is lower and the energy required to excite bound water for phase transformation is higher thus under certain conditions of environmental heat radiation the amount of water evaporating and the amount of heat consumed per unit time are reduced moreover with the decrease of free water the contact area of bound water and solid soil particles with the external environment increases the heat exchange between environment and liquid phase of soil free water is gradually transformed into the heat exchange between environment and solid phase of soil bound water and soil particles therefore the surface temperature of soil in this stage gradually rises stage iii temperature stabilizing stage as the evaporation rate approaches zero in this stage the heat exchange between the environment and the soil tends to be stable and reaches thermodynamic equilibrium therefore the soil temperature tends to be consistent with the ambient temperature note that the equilibrium state can be considered to be attained when the temperature fluctuation is less than 0 1 h the relationship between evaporation rate and interfacial temperature difference of the tested soil is shown in fig 6 as can be seen from the figure there is a good linear relationship between the evaporation rate and the interfacial temperature difference this also proves the rationality of eq 6 by linear fitting the values of parameters α and β for the tested soil sample are 0 0049 and 0 062 respectively r square 0 9377 3 3 relationship between water content and temperature at soil surface based on the previous discussion on effect of water content on evaporation from bare soils as well as evolution of soil surface temperature during evaporation it can be found that for a saturated or over saturated soil water evaporation is kept highly active by the heat energy provided by the atmosphere fredlund and rahardjo 1993 in this situation both the evaporation rate and the interfacial temperature difference are constant at a given environmental condition that is to say there is a critical value of the interfacial temperature difference δ t c at a given environmental condition 7 δ t c α e 0 β when the measured interfacial temperature difference is exactly this critical value it can be judged that the soil surface is over saturated for the tested soil δ t c can be obtained from fig 5 which is around 1 7 for the soil which is unsaturated evaporation becomes increasingly difficult because the water molecules are tightly bound to the soil matrix the increase of soil temperature is due to the conduction of external heat energy to the soil solid phase gens 2010 kodikara et al 2011 there is a linear relationship between the water content and the interfacial temperature difference 8 ω α β t s t a b a note that eqs 7 and 8 are obtained by substituting eq 6 into eq 1 based on eq 8 the water content ω at soil surface can be determined by obtaining the temperature at soil surface t s using infrared thermal imaging technology and the air temperature at a reference height t a when the infrared thermal imaging technology is adopted to measure water content at soil surface there is no need to calibrate four parameters including three soil parameters α a b and one air parameter β it is possible to directly calibrate the relationship between the water content at soil surface and the interfacial temperature difference and eq 8 can be simplified as follows 9 ω m t s t a n where m is a parameter related to both soil and air n is a soil parameter the relationship between water content and interfacial temperature difference of the tested soil is shown in fig 7 for the tested soil sample in the water content range from 55 to 22 the interfacial temperature difference fluctuates around 1 7 the fluctuation is likely due to the effects of temperature change during the testing period on the evaporation in the water content range from 22 to 13 there is a good linear relationship between water content and interfacial temperature difference which proves that eq 9 is rational based on the values of a b α and β the values of m and n can be calculated as 0 047 and 0 14 respectively r square 0 9678 that is to say for the proposed method in order to measure water content at soil surface based on infrared thermal imaging technology three parameters need to be calibrated namely δ t c m and n δ t c is used to judge whether the soil surface is over saturated or unsaturated m and n are used to calculate the water content at soil surface at a given environmental condition moreover two parameters need to be measured one is the ambient temperature and the other one is the temperature at soil surface for the tested soil sample when the interfacial temperature difference of a certain area measured by the infrared camera is about 1 7 the soil surface in this area can be considered as over saturated for the areas with an interfacial temperature difference larger than this value the corresponding surface water content can be determined according to eq 9 in addition to the above mentioned lean clay sample a well graded sand a clayey sand and a fat clay were also tested under the same environmental condition and the results are shown in fig 8 the particle size distributions of the tested soils are shown in fig 8 a and the soil classification is based on the unified soil classification system astm d 2487 2017 as can be seen from the fig 8 b the linear relationship between water content and interfacial temperature difference is also well reflected in other types of soils all the four values of r2 are greater than 0 9 moreover it can be found that with an increase in the clay content the value of m decreases and the value of n increases 4 validation of the proposed surface water content measurement method in order to validate the proposed surface water content measurement method a cracked soil sample was prepared a desired amount of prepared initially saturated slurry was poured into an acrylic square mould with side length of 20 cm similarly the slurry sample was subjected to vibration and sealing as described in the previous section the final settled slurry thickness was about 4 cm then the sample was placed in the environment chamber for natural drying the ambient temperature in the environment chamber is set to a value of 22 which is deliberately set to be different from that of the sample used for calibration five hours later a camera was used to take a photo of the sample surface as shown in fig 9 a several cracks appear on the surface of the sample the development of cracks leads to some new evaporation surfaces which makes the sample has an uneven surface water content field moreover the temperature field of sample surface was photographed by the infrared camera as shown in fig 9 b it can be found that due to the presence of cracks the surface temperature of the sample is not uniform zeng et al 2022 based on the temperature field data and eq 9 the distribution of water content at soil surface can be calculated which is shown in fig 9 c since for unsaturated soils there is a linear relationship between water content and interfacial temperature difference the general pattern of the surface temperature field and the surface water content field are same to prove the accuracy of the obtained water content at soil surface using the proposed method fig 9 c is artificially divided into 5 5 grids fig 9 d the average surface water content within each grid area is calculated at this moment the actual water content at soil surface in each grid area is measured by oven drying the topsoil with a thickness of around 0 5 cm fig 9 e shows a scatterplot between the measured and calculated water contents it can be found that the calculated data is close to the measured data the calculated water contents at soil surface are a little bit larger than the measured values and the average difference is about 7 this proves the feasibility of the proposed method the possible reason for the difference between calculated data and measured data is that when measuring the water content the sample taken has a certain thickness although the sample is thin only about 0 5 cm thick there is a water content gradient along the depth direction which is not considered the method proposed in this paper has a very good application prospect in many disciplines for example it can be used to measure the distribution characteristics of soil surface water content as well as its evolution with climate change and then analyse the soil suction state and hydro mechanical behaviour of unsaturated soils moreover it can also be used for seepage detection on the surface of earthen dams and other geo structures 5 summary and conclusions in this study a rapid non contact non destructive and stable method to measure water content at soil surface based on infrared thermal imaging technology is proposed the main principle of this method is to establish the relationship between water content and interfacial temperature difference through theoretical derivation it is found that at a given environmental condition there is a critical value of the interfacial temperature difference when the measured interfacial temperature difference is around the critical value the soil surface in this specific region can be regarded as over saturated for the unsaturated soil surface there is a good linear relationship between water content and interfacial temperature difference after careful calibration of this linear relationship the water content at soil surface can be calculated based on the interfacial temperature difference data measured by the infrared camera moreover in this study a cracked soil sample with an uneven surface water content field is tested the calculated surface water contents at different locations are compared with the water content measured by oven drying results show a good match which validates the feasibility of the proposed method it should be noted that the applicable range of measured water content by the proposed method is from saturated water content to residual water content this method may provide a new idea for the future research on the spatio temporal evolution characteristics of in situ soil under climate change and provide a new choice for leakage detection of structures like dams tunnels etc credit authorship contribution statement qing cheng formal analysis validation writing original draft writing review editing chao sheng tang conceptualization resources supervision funding acquisition zong ze lin investigation methodology ben gang tian methodology bin shi writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national natural science foundation of china grant no 42172290 42230710 41902271 41925012 and natural science foundation of jiangsu province grant no bk20221250 bk20211087 
2645,in this study infrared thermal imaging technology is adopted to measure water content at soil surface by calibrating the linear relationship between water content at soil surface and interfacial temperature difference of a given soil type the method is suitable for the soil surface which is unsaturated first of all the theoretical considerations of the new method are introduced infrared thermal imaging technology can directly measure soil surface temperature and the difference between the soil surface temperature and the ambient temperature is named as the interfacial temperature difference for over saturated soils at a certain environmental condition the interfacial temperature difference of a given soil type is a constant value for unsaturated soils there is a linear relationship between water content at soil surface and interfacial temperature difference for the tested four types of soils with different textures all the values of r2 for the linear fitting are greater than 0 9 then the water content at soil surface can be calculated based on the surface temperature measured by the infrared camera moreover in this study a cracked soil sample with an uneven surface water content field is tested for validating the feasibility of the proposed method there is a good match between the data measured by the proposed method and traditional oven drying it indicates that the proposed method can be used to obtain the distribution of water content on the soil surface and may provide a new idea for the future research on the spatio temporal evolution characteristics of in situ soil under climate change and provide a new choice for leakage detection of structures like dams tunnels etc keywords evaporation temperature water content thermal infrared imaging data availability data will be made available on request 1 introduction water content at soil surface usually refers to the water content of soil within 2 cm depth kimura et al 2009 it plays a paramount role in the soil atmosphere continuum in many disciplines in geotechnical and geological engineering the distribution and evolution of water content at soil surface contributes to soil deformation and cracking behaviour ng and menzies 2007 tang et al 2008 li and zhang 2011 zhang et al 2021 in hydrology water content at soil surface is of particular interest with respect to evaporation infiltration and runoff which are closely related to soil erosion and river discharge huisman et al 2003 ma et al 2016 jiang et al 2019 cheng et al 2021 in agriculture water content at soil surface has a significant influence on plant growth robinson et al 2008 water content determination methods include direct method and indirect method the so called direct method is thermal drying particularly oven drying schmugge et al 1980 it measures the average water content of a soil block by sampling for soils with and without organic matter the chosen drying temperatures are different this method is simple but is not feasible for field testing in field work the alcohol burning method can be used instead but not for soils containing a high proportion of organic matter ma et al 2016 in general the direct method disturbs the field and changes the hydro mechanical properties of the soil sample as it changes the soil water content to overcome the drawbacks of direct measurement researchers have developed many indirect methods in the past few decades to achieve non destructive measurement neutron scattering and x ray computed tomography are one typical kind of indirect method to measure soil water content phogat et al 1991 sheets and hendrickx 1995 chanasyk and naeth 1996 fityus et al 2011 zhang et al 2015 balaghi et al 2018 the principle is that when ray passes through soil directly the energy will decay according to the different attenuation amounts in the solid phase and liquid phase the water content of soil can be obtained the measured water content is also the average value of soil blocks with a radius of about a few to tens of centimetres this kind of method can be used for continuous observation at a fixed point but it cannot realize long term monitoring in a large area moreover the equipment is expansive and may have a potential radiation hazard to people so it is not widely used the second kind of indirect method is to use dielectric techniques including microwave time domain transmission time domain reflectometry frequency domain reflectometry electrical resistivity tomography ground penetrating radar etc jacobsen and schjonning 1993 gardner et al 1997 topp and reynolds 1998 huisman et al 2003 kelleners et al 2004 brunet et al 2010 rao and singh 2011 zheng et al 2011 you et al 2013 zhan et al 2015 alamry et al 2017 kafarski et al 2020 bittelli et al 2021 sun et al 2021 agyeman et al 2021 the main principle is to determine soil water content indirectly by measuring the dielectric permittivity such methods have the advantages of being fast and safe and can be used to measure soil water content at different depths however the above mentioned geophysical surveys can be significantly affected by the salinity of a soil and many researches have been conducted to deal with this drawback hanson and kaita 1997 peng et al 2019 ying et al 2021 moreover the measurement results are easily affected by environmental conditions such as temperature overall there is a lack of a fast non contact non destructive and stable method for measuring the water content at soil surface infrared thermal imaging technology is a non contact and non destructive method which is able to accurately acquire a detailed temperature distribution field of the target surface in real time the camera converts the invisible infrared energy emitted by the surface of a certain objective into a 2d visual image in recent years infrared thermal imaging has been adopted for identifying and mapping cracks and fractures especially on rock masses liu et al 2016 li et al 2018 cai et al 2020 miao et al 2022 zeng et al 2022 moreover this technology has been widely used to assess different processes that occur at the soil surface level such as evaporation preferential infiltration crust formation soil water repellency assessment etc danielescu et al 2009 pfister et al 2010 shahraeeni and or 2010 soliman et al 2010 de lima and abrantes 2014 sadeghi et al 2015 dimitrov et al 2015 abrantes et al 2017 perez garcia et al 2018 abdulkareem et al 2020 moreover more and more researches have been conducted recently thanks to the easy handling and adjustment of measurement distance and scale of portable hand held thermography systems the principal objective of this study is to investigate the feasibility of applying the infrared thermal imaging technology to measure water content at soil surface the proposed method was based on a linear relationship between water content at soil surface and interfacial temperature difference which was deduced by the theoretical considerations a clayey soil was used as an example to show the calibration process then a cracked soil sample with an uneven surface water content field is tested for validating the feasibility of the proposed method 2 materials and methods 2 1 testing materials in this study a clayey soil collected from nanjing china was used it is wildly distributed in the middle and lower reaches of yangtze river its mineral composition is mainly illite and illite montmorillonite interlayer minerals the fractions of sand silt and clay are 2 76 and 22 respectively according to the unified soil classification system astm d 2487 2017 it is classified as a lean clay cl table 1 summarizes the physical properties of the tested soil 2 2 sample preparation and testing procedure the collected soil was air dried crushed and passed through a 2 mm sieve to prepare a saturated slurry the sieved soil powers were mixed with distilled water to reach a target water content of 55 a desired quantity of slurry was poured onto circular aluminium boxes with an inner diameter of 40 mm the slurry was vibrated on the shaking table for 5 min to eliminate the bubbles generated in the mixing process then the containers were sealed with a plastic membrane for at least 48 h the sample preparation procedure follows previous researches burland 1990 cotecchia and chandler 2000 liu and carter 2000 hong et al 2012 cheng et al 2020 the final settled slurry thicknesses were about 10 mm after that the soil sample was setting up in the testing apparatus and subjected to drying note that in this study two replicates were prepared and the difference between the two replicates was less than 5 this suggests that the laboratory data is reliable 2 3 testing apparatus the soil sample was placed on a balance with an accuracy of 0 01 g to measure the water content change during the drying process a thermal infrared camera flir t620 ir was fixed 50 cm above the sample surface to capture the temperature evolution with elapsed drying time adjust the lens of the camera so that the field of view is 50 mm by 50 mm slightly larger than the sample size the adopted infrared camera has a working wavelength of 7 8 14 0 μm and a spatial resolution of 640 480 pix its temperature sensitivity and measurement accuracy can reach 0 04 and 0 1 respectively fig 1 shows the schematic diagram of the experimental set up the experimental setup was placed in an environment chamber the ambient temperature and relative humidity were controlled at 30 2 and 70 3 respectively it should be noted that the sample was placed in the environment chamber and sealed for 12 h before drying so that the initial temperature of the sample is basically consistent with the ambient temperature 3 theoretical considerations of the proposed method to measure water content at soil surface 3 1 effect of water content on evaporation from bare soils based on a large amount of experimental data on evaporation yanful and choo 1997 baruah and hasegawa 2001 newson and fahey 2003 parvin et al 2017 an et al 2018 there is a consensus that at the early stage of drying the evaporation rate almost keeps constant for an initially saturated soil when air goes into the soil sample and the sample becomes unsaturated the evaporation rate falls dramatically and shows an approximately linear relationship with decreasing water content when the water content keeps decreasing to lower than the residual water content the evaporation rate tends to be zero and the water transfer is mainly due to vapour flux upward through the soil pores bittelli et al 2008 thus for the evaporation rate of unsaturated soils the following equation is proposed 1 e e 0 ω ω sat a ω b ω ω sat where e is the evaporation rate evaporation is positive and condensation is negative mm h e 0 is the evaporation rate of an saturated or over saturated soil ω is the water content of soil ω sat is the water content of soil at saturated state a and b are soil parameters using the tested soil as an example the change of evaporation rate with elapsed drying time is shown in fig 2 the evaporation rate is obtained from changes in sample weight measured by the balance the three stages of evaporation are clearly shown in the figure similar to previous studies on evaporation yanful and choo 1997 baruah and hasegawa 2001 newson and fahey 2003 parvin et al 2017 an et al 2018 stage ⅰ constant rate stage in this stage the evaporation rate of the soil sample almost remains unchanged the soil sample is still fully saturated at this stage and there is enough water in the soil to supply evaporation stage ⅱ falling rate stage in this stage the evaporation rate of the soil sample decreases dramatically with elapsed drying time the soil sample becomes unsaturated and soil suction increases during the drying process moreover the water permeability also decreases resulting in a decrease in the evaporation rate stage iii residual stage in this stage the evaporation rate of the soil sample gradually approaches zero the change of evaporation rate with decreasing water content of the tested soil is shown in fig 3 in the water content range from 55 to 22 the soil sample is saturated and the evaporation rate almost keeps constant with a small fluctuation as drying continues when the water content range decreases from 22 to the residual water content of about 13 there is a good linear relationship between the evaporation rate and the water content proving the hypothesis of eq 1 is reasonable the residual water content for the same clayey soil in the literature varies from 5 to 10 which is smaller than the value in this study this is because of the influence of soil layer thickness as evidenced by an et al 2018 the thinner the soil sample the larger the residual water content by linear fitting the values of parameters a and b are 1 33 and 0 18 respectively r square 0 9545 3 2 evolution of soil surface temperature during evaporation during the evaporation process the energy exchange between soil and air obeys the law of energy conservation if the effect of atmospheric transverse convection is assumed to be negligible the surface energy balance during evaporation from bare soil can be expressed as follows penman 1948 2 l e e r n g h where l e is the latent heat of vaporization representing the energy required for water to change from liquid to gas 2450 j g under atmospheric pressure r n is the net radiation flux which is the main source of energy flux intercepted on a surface incidence is positive and scattering is negative w m2 g is the soil heat flux the increase of soil temperature caused by energy is positive while the decrease is negative w m2 h is the sensible heat flux energy used to heat the air is positive and energy lost by air cooling is negative w m2 it should be noted that the net radiation flux r n consists of incident solar shortwave radiation atmospheric scattering and outgoing surface reflection and surface long wave radiation there is a certain proportion between the soil heat flux g and the net radiation flux r n which is related to soil properties for example the soil heat flux g of coarse sand can be approximately one fifth of the net radiation flux r n qiu et al 1998 thus the soil heat flux g can be regarded as follows 3 g γ r n where γ represents the ratio of the soil heat flux g and the net radiation flux r n the sensible heat flux h can be estimated using aerodynamic resistance as blight 1997 4 h ρ c p t s t a r a where ρ is the air density c p is the specific heat capacity of air at constant pressure t s is the temperature at soil surface t a is the air temperature at a reference height r a is the unit thermal resistance of air when combing eqs 1 4 the evaporation rate can be expressed as follows 5 e 1 γ r n l e ρ c p l e r a t s t a if the effect of atmospheric scattering is negligible and the light in the evaporation process is constant that is to say the ambient temperature and humidity and radiation intensity remain unchanged ρ c p l e is a constant then eq 4 can be simplified as 6 e α β t s t a where α is parameter related to radiation intensity and soil properties and β is parameter related to the unit thermal resistance of air it can be found that during the drying process there is a linear relationship between the evaporation rate e and the interfacial temperature difference t s t a for the tested soil the evolution of temperature field at soil surface with elapsed drying time is shown in fig 4 it can be found that at the early stage of drying soil surface temperature has little change with time with the continuous evaporation the soil surface temperature gradually increases at the end of evaporation the surface temperature of samples tends to be consistent with the ambient temperature in general at a certain time the surface temperature field of each soil sample is evenly distributed the difference between the average surface temperature and the ambient temperature which is defined as the interfacial temperature difference can be obtained the evolution of interfacial temperature difference with elapsed drying time is shown in fig 5 the variation trend of temperature difference over time is consistent with that of evaporation rate over time the specific time corresponding to the turning point of each stage is roughly the same which indicates that the change of soil surface temperature can respond to the evaporation process synchronously specifically for the tested soil sample stage ⅰ ends around 50 h and stage iii begins around 68 h this is because evaporation of soil water is an endothermic process the greater the evaporation rate the more intense the heat release and the lower corresponding surface temperature corresponding to the three stages of evaporation rate evolution the change of soil surface temperature in the drying process can be roughly divided into three stages as well stage ⅰ constant low temperature stage in this stage there is a high content of free water in the liquid phase and almost all the energy provided by radiation and heat transfer is transferred to the liquid phase in the soil resulting in the increase of entropy in the liquid phase when the entropy increases to a certain threshold value the free water in liquid phase transforms into water vapour that is soil water evaporates the loss of heat energy during phase transformation leads to a breaking of the thermodynamic balance between the liquid and solid phases in soil a thermal gradient is formed between the two phases and then the solid phase continuously supplies energy to the liquid phase as a result the heat in the solid phase of soil decreases and the soil temperature drops resulting in a relatively large interfacial temperature difference moreover with a constant evaporation rate the heat transfer between liquid and solid phase is stable thus the soil sample keeps a constant low temperature in this stage stage ⅱ increasing temperature stage with continuous drying the free water in soil decreases and the proportion of bound water increases gradually compared with free water the entropy value of bound water is lower and the energy required to excite bound water for phase transformation is higher thus under certain conditions of environmental heat radiation the amount of water evaporating and the amount of heat consumed per unit time are reduced moreover with the decrease of free water the contact area of bound water and solid soil particles with the external environment increases the heat exchange between environment and liquid phase of soil free water is gradually transformed into the heat exchange between environment and solid phase of soil bound water and soil particles therefore the surface temperature of soil in this stage gradually rises stage iii temperature stabilizing stage as the evaporation rate approaches zero in this stage the heat exchange between the environment and the soil tends to be stable and reaches thermodynamic equilibrium therefore the soil temperature tends to be consistent with the ambient temperature note that the equilibrium state can be considered to be attained when the temperature fluctuation is less than 0 1 h the relationship between evaporation rate and interfacial temperature difference of the tested soil is shown in fig 6 as can be seen from the figure there is a good linear relationship between the evaporation rate and the interfacial temperature difference this also proves the rationality of eq 6 by linear fitting the values of parameters α and β for the tested soil sample are 0 0049 and 0 062 respectively r square 0 9377 3 3 relationship between water content and temperature at soil surface based on the previous discussion on effect of water content on evaporation from bare soils as well as evolution of soil surface temperature during evaporation it can be found that for a saturated or over saturated soil water evaporation is kept highly active by the heat energy provided by the atmosphere fredlund and rahardjo 1993 in this situation both the evaporation rate and the interfacial temperature difference are constant at a given environmental condition that is to say there is a critical value of the interfacial temperature difference δ t c at a given environmental condition 7 δ t c α e 0 β when the measured interfacial temperature difference is exactly this critical value it can be judged that the soil surface is over saturated for the tested soil δ t c can be obtained from fig 5 which is around 1 7 for the soil which is unsaturated evaporation becomes increasingly difficult because the water molecules are tightly bound to the soil matrix the increase of soil temperature is due to the conduction of external heat energy to the soil solid phase gens 2010 kodikara et al 2011 there is a linear relationship between the water content and the interfacial temperature difference 8 ω α β t s t a b a note that eqs 7 and 8 are obtained by substituting eq 6 into eq 1 based on eq 8 the water content ω at soil surface can be determined by obtaining the temperature at soil surface t s using infrared thermal imaging technology and the air temperature at a reference height t a when the infrared thermal imaging technology is adopted to measure water content at soil surface there is no need to calibrate four parameters including three soil parameters α a b and one air parameter β it is possible to directly calibrate the relationship between the water content at soil surface and the interfacial temperature difference and eq 8 can be simplified as follows 9 ω m t s t a n where m is a parameter related to both soil and air n is a soil parameter the relationship between water content and interfacial temperature difference of the tested soil is shown in fig 7 for the tested soil sample in the water content range from 55 to 22 the interfacial temperature difference fluctuates around 1 7 the fluctuation is likely due to the effects of temperature change during the testing period on the evaporation in the water content range from 22 to 13 there is a good linear relationship between water content and interfacial temperature difference which proves that eq 9 is rational based on the values of a b α and β the values of m and n can be calculated as 0 047 and 0 14 respectively r square 0 9678 that is to say for the proposed method in order to measure water content at soil surface based on infrared thermal imaging technology three parameters need to be calibrated namely δ t c m and n δ t c is used to judge whether the soil surface is over saturated or unsaturated m and n are used to calculate the water content at soil surface at a given environmental condition moreover two parameters need to be measured one is the ambient temperature and the other one is the temperature at soil surface for the tested soil sample when the interfacial temperature difference of a certain area measured by the infrared camera is about 1 7 the soil surface in this area can be considered as over saturated for the areas with an interfacial temperature difference larger than this value the corresponding surface water content can be determined according to eq 9 in addition to the above mentioned lean clay sample a well graded sand a clayey sand and a fat clay were also tested under the same environmental condition and the results are shown in fig 8 the particle size distributions of the tested soils are shown in fig 8 a and the soil classification is based on the unified soil classification system astm d 2487 2017 as can be seen from the fig 8 b the linear relationship between water content and interfacial temperature difference is also well reflected in other types of soils all the four values of r2 are greater than 0 9 moreover it can be found that with an increase in the clay content the value of m decreases and the value of n increases 4 validation of the proposed surface water content measurement method in order to validate the proposed surface water content measurement method a cracked soil sample was prepared a desired amount of prepared initially saturated slurry was poured into an acrylic square mould with side length of 20 cm similarly the slurry sample was subjected to vibration and sealing as described in the previous section the final settled slurry thickness was about 4 cm then the sample was placed in the environment chamber for natural drying the ambient temperature in the environment chamber is set to a value of 22 which is deliberately set to be different from that of the sample used for calibration five hours later a camera was used to take a photo of the sample surface as shown in fig 9 a several cracks appear on the surface of the sample the development of cracks leads to some new evaporation surfaces which makes the sample has an uneven surface water content field moreover the temperature field of sample surface was photographed by the infrared camera as shown in fig 9 b it can be found that due to the presence of cracks the surface temperature of the sample is not uniform zeng et al 2022 based on the temperature field data and eq 9 the distribution of water content at soil surface can be calculated which is shown in fig 9 c since for unsaturated soils there is a linear relationship between water content and interfacial temperature difference the general pattern of the surface temperature field and the surface water content field are same to prove the accuracy of the obtained water content at soil surface using the proposed method fig 9 c is artificially divided into 5 5 grids fig 9 d the average surface water content within each grid area is calculated at this moment the actual water content at soil surface in each grid area is measured by oven drying the topsoil with a thickness of around 0 5 cm fig 9 e shows a scatterplot between the measured and calculated water contents it can be found that the calculated data is close to the measured data the calculated water contents at soil surface are a little bit larger than the measured values and the average difference is about 7 this proves the feasibility of the proposed method the possible reason for the difference between calculated data and measured data is that when measuring the water content the sample taken has a certain thickness although the sample is thin only about 0 5 cm thick there is a water content gradient along the depth direction which is not considered the method proposed in this paper has a very good application prospect in many disciplines for example it can be used to measure the distribution characteristics of soil surface water content as well as its evolution with climate change and then analyse the soil suction state and hydro mechanical behaviour of unsaturated soils moreover it can also be used for seepage detection on the surface of earthen dams and other geo structures 5 summary and conclusions in this study a rapid non contact non destructive and stable method to measure water content at soil surface based on infrared thermal imaging technology is proposed the main principle of this method is to establish the relationship between water content and interfacial temperature difference through theoretical derivation it is found that at a given environmental condition there is a critical value of the interfacial temperature difference when the measured interfacial temperature difference is around the critical value the soil surface in this specific region can be regarded as over saturated for the unsaturated soil surface there is a good linear relationship between water content and interfacial temperature difference after careful calibration of this linear relationship the water content at soil surface can be calculated based on the interfacial temperature difference data measured by the infrared camera moreover in this study a cracked soil sample with an uneven surface water content field is tested the calculated surface water contents at different locations are compared with the water content measured by oven drying results show a good match which validates the feasibility of the proposed method it should be noted that the applicable range of measured water content by the proposed method is from saturated water content to residual water content this method may provide a new idea for the future research on the spatio temporal evolution characteristics of in situ soil under climate change and provide a new choice for leakage detection of structures like dams tunnels etc credit authorship contribution statement qing cheng formal analysis validation writing original draft writing review editing chao sheng tang conceptualization resources supervision funding acquisition zong ze lin investigation methodology ben gang tian methodology bin shi writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the national natural science foundation of china grant no 42172290 42230710 41902271 41925012 and natural science foundation of jiangsu province grant no bk20221250 bk20211087 
2646,although flow in unconfined double porosity aquifers occurs in most part of the world such as in karst and crystalline aquifers their hydrodynamic behavior is still poorly understood especially analytical or semi analytical solutions for flow in unconfined double porosity dp aquifers are scarce in this paper analytical solutions are proposed for two configurations involving groundwater discharge to a stream adjacent to an unconfined aquifer and flow induced by pumping with constant extraction rate respectively the analytical solutions are obtained based on an h2 linearization technique of the full mathematical model and using a laplace transform the above technique has not been used before to investigate the behavior in dp aquifers the newly developed analytical solution is in agreement with numerical solutions of the full mathematical model for small to moderate drawdowns which are as high as 25 of the initial water table case of the well pumping problem this confirms on one side the correctness of the analytical solution and on the other side the reliability of the developed h2 linearization technique on the contrary the conventional confined aquifer equations approach can be used only if the drawdown does not exceed 5 of the initial head values as potential application examples the estimation of the flow rate from an aquifer to an adjacent water body and the interpretation of pumping tests in unconfined double porosity aquifers allowing to estimate the values of the models parameters are proposed keywords unconfined fractured aquifer analytical solutions aquifer stream interactions pumping test data availability no data was used for the research described in the article nomenclature a dummy variable b average saturated water depth l b dimensionless average saturated water depth eq 6h c optimization criterion for the evaluation of pumping tests see eq c1 h f water table elevation head in the fractures network see figs 1 and 2 l h f dimensionless water table elevation head in the fractures network see eq 14a h m water table elevation head in the porous matrix see figs 1 and 2 l h m dimensionless water table elevation head in the porous matrix see eq 14b h initial water depth see figs 1 and 2 l h 1 water depth at x 0 for t 0 used in the problem of one dimensional discharge to a stream see fig 1 l i o modified bessel function of the first kind and zeroth order k o modified bessel function of the second kind and zeroth order kf hydraulic conductivity of the fractures network lt 1 n number of data points from the comsol results used for calibration see appendix c n parameter for the stehfest algorithm p laplace transform variable with respect to the dimensionless time t r distance from the centre of the well see fig 2 l r dimensionless distance from the centre of the well see eq 27a r w well radius l r w dimensionless well radius see eq 36e qs specific discharge discharge per unit length from the aquifer to an adjacent stream l2t 1 q s dimensionless specific discharge see eq 19 q criterion for the calibration of the quality performance of the procedure for the evaluation of pumping tests see eq c2 q w extracted flowrate from the aquifer by the pumping process see fig 2 l3t 1 q w dimensionless form of q w see eq 27b s f drawdown in the fractures network see eq 5a l s f dimensionless drawdown in the fractures network s m drawdown in the porous matrix see eq 5b l s m dimensionless drawdown in the porous matrix s f specific yield of the fractures continuum s m specific yield of the porous matrix t time t t dimensionless time see eq 6c tf the transmissivity of the fractures network l2t 1 u f dimensionless shifted variable see eq 7a u f laplace transform of u f u m dimensionless shifted variable see eq 7b x distance cartesian coordinate l x dimensionless distance see eq 6d greek letters δ h drawdown in the stream and in the adjacent aquifer for x 0 see fig 1 l ε dimensionless drawdown in the stream and in the adjacent aquifer for x 0 see eq 6g κ exchange coefficient which describes fluid exchange between the porous matrix and the network of fractures l 1t 1 κ dimensionless form of κ see eq 6e κ depth integrated exchange coefficient used in the confined equations model t 1 λ ratio of the specific yields of the porous matrix and the fractures continuum see eq 6f ω f variable for the h2 linearization approximation procedure ω f h f 2 l2 ω f dimensionless form of ω f defined by eq 6a ω m variable for the h2 linearization approximation procedure ω m h m 2 l2 ω m dimensionless form of ω m defined by eq 6b subscripts f network of fractures m porous matrix superscripts dimensionless variable 1 introduction understanding groundwater flow in unconfined aquifers uas is of first importance in several environmental and geotechnical applications these aquifers are of ecological and economical significance as they play a major role in controlling surface subsurface water exchange alattar et al 2020 linking groundwater resources and surface water as well as in the recharge of confined aquifers and the contaminant attenuation processes uas represent an important source of freshwater and water supply şen 2015 these aquifers are often shallow and their groundwater is easily accessible they are also highly vulnerable to contamination by surface pollutants guo et al 2021 zheng et al 2021 uas are one of the main components of the aquifer thermal energy storage systems cabeza 2021 uas are also investigated in studies related to tunnel excavation and design maréchal et al 2014 sedghi and zhan 2021a as well as in studies dealing with seepage through dams zarif sanayei and javdanian 2020 among others fractured unconfined aquifers fuas are found globally in karst areas ghasemizadeh et al 2015 sanz lobón et al 2015 as for instance in italy petronici et al 2019 in iran sedghi and zhan 2021b in the usa ren et al 2018 in australia and in ireland wright 2000 and in the mediterranean basin koohbor et al 2020 hard rock aquifers like metamorphic and magmatic geological formations cover approximately 20 percent of the present land surface including large areas in africa india brazil and europe gustafson and krásný 1994 in several cases hard rock aquifers which are usually fractured occupy the upper tens of meters of the subsurface profile maréchal et al 2004 and therefore a free water surface may be present and they can often exhibit unconfined flow conditions in fuas the fracture networks provide preferential flow pathways and play an important role in the delineation of the water table level the water balance of the aquifers and in conducting groundwater in wellbores luo et al 2020 despite that fuas are widely distributed throughout the world these aquifers are rarely investigated and more attention is given in the literature to fractured confined aquifers regardless of the important role of fractures in controlling groundwater flow and water table levels many modeling based studies on fuas do not consider the fracture networks groundwater flow in fractured aquifers can be tackled using either the discrete fracture model dfm or the continuum approach ca with dfm the fractures i e their geometry and properties are described explicitly koohbor et al 2020 dfm is known to be computationally expensive especially in the case of a high number of fractures despite the progress on the development of new dfms real field applications of these models are constrained to cases with limited number of well defined fractures hirthe and graf 2015 ramasomanana et al 2018 a major drawback of the dfms is that they need detailed description of the fractures geometry which is a complex task in field applications unlike dfm with the ca the fractured domain is simulated as a porous medium with equivalent parameters this assumption allows for avoiding dense computational grids related to the fractures network as fractures are not represented explicitly the ca is applicable if the fracture density is high and when the fracture orientation is highly variable long et al 1982 however in fractured domains the matrix blocks and the fractures have different hydrodynamic properties thus it is not easy to simulate these domains with a single continuum model the concept of dual porosity dp is an improvement of the ca in which the domain is considered as a superposition of two continuums representing the porous matrix and the fractures respectively barenblatt et al 1960 hassanzadeh et al 2009 fahs et al 2014 jerbi et al 2017 with the dp model two porosities and two pressure heads representing respectively matrix rocks and fractures are associated to each point of the domain the matrix continuum can store water and exchange it with the fracture continuum the permeability of the matrix continuum often can be neglected when this permeability is considered the model is called dual permeability aguilar lópez et al 2020 the dp model cannot simulate preferential flows but it allows for reducing computational burden and does not need detailed description of the fractures topology dp models are widely used in real field studies when the dfm cannot be applied the dp approach was firstly developed to simulate fluid flow in geological oil reservoirs then it has been used to simulate water flow in confined aquifers e g streltsova 1976 moench 1984 gerke and genuchten 1993 önder 1998 noetinger and estebenet 2000 landereau et al 2001 park and zhan 2003 delay et al 2007 moutsopoulos and tsihrintzis 2009 fahs et al 2014 sanchez vila et al 2016 hayek et al 2018 and wang et al 2021 analytical solutions for confined dp aquifers have been proposed by wilson and aifantis 1982 moench 1984 önder 1998 park and zhan 2003 de smedt 2011 gonzález calderón et al 2017 dejam et al 2018 dewandel et al 2018 hayek et al 2018 asadi et al 2020 and de smedt 2022 on the contrary fewer publications have investigated the use of the dp model for fuas the most straightforward dp approach to simulate the flow behavior in fuas is based on the same equations as for the confined aquifers e g önder 1998 maréchal et al 2004 whereas one has to take into account that the storage coefficient for phreatic aquifers is roughly equal to the active porosity of each equivalent continuum and it is therefore several orders of magnitude larger than the same coefficients of confined aquifers the approach above is justified only for small drawdowns more elaborate approaches have been also proposed in the literature boulton and streltsova adams 1978 used the dp approach to investigate water extraction from a fua by a vertical fully penetrating well they considered an idealized geometry of the aquifer which was assumed to consist of identical equidistant horizontal fractures separated by identical slab shaped porous blocks they considered the effects of delayed yield which may be of importance at early times and near the well and the effects of transient matrix to fissure flow sedghi and zhan 2019 investigated interactions between a water body and unconfined single and double porosity aquifers they assumed that the aquifer was composed by identical periodical sphere shaped porous blocks separated by fractures considering an appropriate kinematic boundary condition at the water surface they could simulate flows with vertical velocity components potential drawbacks of the studies of boulton and streltsova adams 1978 and sedghi and zhan 2019 are the use of an idealized geometry of both the porous matrix consisting of either slab shaped or sphere shaped porous blocks and the fractures and the fact that in both studies the authors have not considered that the internal flow process is different in the case that the saturated porous matrix is adjacent to saturated fractures than in the case that it is adjacent to non saturated fractures kordilla et al 2012 have developed a dp model based on a generalized form of the richards equation to simulate the flow behavior in an unconfined fractured aquifer in south west germany although their approach is quite sophisticated it may not be possible to be implemented in many cases of practical interest because it involves the use of 21 parameters which are difficult to be determined furthermore their approach may not be adequate when the aperture of the fractures in the unsaturated zone is relatively large so that gravity forces prevail over the capillary ones and flow in form of films or droplets takes place in these fractures the same approach has been also adopted by other researchers e g robineau et al 2018 bresinsky et al 2021 a simpler dp model to simulate flow processes in fuas involving only 4 6 parameters has been proposed by moutsopoulos et al 2001 and moutsopoulos 2021 this model is based on two essential assumptions i discharging and not recharging flow conditions take place ii the apertures of the vertical fractures are large enough so that gravity forces prevail over capillary ones deep discussion about these assumptions can be found in moutsopoulos 2021 the latter hypothesis seems to hold for several cases as described in su et al 1999 salve et al 2004 kordilla et al 2017 noffz et al 2019 and kordilla et al 2021 advantages of this model above over both the approaches of boulton and streltsova adams 1978 and sedghi and zhan 2019 are that the fluid exchange between the porous matrix and the network of fractures is more adequately described in particular it has been taken into account that the fluid transfer process depends on whether the saturated porous matrix is adjacent to a saturated fracture or to a non saturated one and that no schematization of the pore space was assumed a substantial body of literature on the dp model is mainly based on numerical simulations where the equations are solved using numerical methods i e trottier et al 2014 fahs et al 2014 aguilar lópez et al 2020 rice et al 2021 despite the progress in numerical solutions and computational techniques there is still a need for analytical solutions due to their reduced computational costs the analytical solutions are widely used as first assessment tools in field studies despite the simplification required to obtain analytical solutions these solutions are usually used as references in validating and verifying numerical solutions and codes analytical solutions provide explicit relations between input parameters and model outputs thus they are useful for aquifer characterization with pumping tests and for conducting sensitivity or uncertainty analysis several analytical solutions have been developed for the dp model in different configurations of aquifers and pumping which as already mentioned are mainly limited to confined aquifers in contrast to confined dp aquifers analytical solutions for dp models in fuas are scarce boulton and streltsova adams 1978 and sedghi and zhan 2019 proposed analytical solutions to their models that have several limitations as discussed above while the model proposed by moutsopoulos 2021 overcomes these limitations analytical solutions of this model are still not well developed there is only one analytical solution to this model which has been developed using a regular perturbation approach this solution refers to a single problem and is valid under rather restrictive conditions because it can be used only for short time spans and a rather small range of the values of the hydrogeological parameters this study aims at developing analytical and semi analytical solutions for the dp model in unconfined aquifers by considering the mathematical model proposed by moutsopoulos 2021 this model is valid for a wide range of values of the hydrogeological parameters in this model the difficulty lies in the nonlinearity of the pressured head terms that need to be taken into account for analytical or semi analytical solutions in single porosity models a classical way to circumvent the nonlinearity and linearize the boussinesq equation is to replace the variable thickness with an average wetted thickness so that the boussinesq equation is transformed to the same form as the confined flow equation yeh and chang 2013 this same approach as already stated has been implicitly adopted for the dp model in önder 1998 and maréchal et al 2004 an alternative approach is the h2 linearization technique where the storage term of the boussinesq equation is multiplied by the ratio of the water depth to an average saturated depth hantush 1967 marino 1973 chang and yeh 2007 ilias et al 2008 teloglou and bansal 2012 this alternative approach gives better results than the straightforward transformation of the boussinesq equation to the confined flow equation upadhyaya and chauhan 1998 in this study the h2 linearization approach is used in order to obtain the semi analytical solutions two classical problems are investigated i the one dimensional discharge to a stream adjacent to an unconfined double porosity aquifer and ii the flow in an infinite unconfined double porosity aquifer with homogeneous properties induced by pumping with constant rate while the analytical solution presented in moutsopoulos 2021 developed using a regular perturbation approach is limited to the problem of discharge to a stream adjacent to an ua and is adequate only for a relatively small range of the hydrogeological parameters and small times the analytical solutions presented in this study based on the h2 linearization approach generalize the previous solution to a wider range of parameters and large durations and address also a new problem of pumping in an infinite dual porosity ua to the best of our knowledge the h2 linearization technique has not been used before to investigate the behavior in dp aquifers we discuss the validity of this technique by comparing the analytical solution to a numerical solution of the non linear model which is developed using the finite element framework comsol multi physics and also the finite volume framework openfoam by using these software tools the accuracy of the solutions obtained by using the confined aquifer equations approach see appendix a is also examined the developed analytical solutions are used to investigate the discharge to a stream adjacent to fuas and also to pumping test analysis 2 one dimensional discharge to a stream adjacent to an unconfined double porosity aquifer the classical problem of discharge to a stream adjacent to a single porosity ua has been studied by several researchers e g guo 1997 lockington 1997 upadhyaya and chauhan 1998 hayek 2019 and basha 2021 moutsopoulos 2021 investigated this problem for a dp in an ua by neglecting the hydraulic conductivity of the porous matrix and also external sink source terms caused by precipitation evapotranspiration or by fluid exchange with adjacent geological layers and assuming that the aquifers properties are isotropic homogeneous and constant the model proposed by moutsopoulos 2021 depicted in fig 1 reads 1a s f h f t k f x h f h f x κ 2 h m 2 h f 2 1b s m h m t κ 2 h m 2 h f 2 where t t is time x l and y l are the space coordinates in an horizontal plane sf and sm stand for the specific yield of the continuum composed of the fractures and of the porous matrix respectively hf l and hm l denote the water table elevation in the fractures and the porous matrix respectively kf lt 1 is the hydraulic conductivity of the fractures network and κ l 1t 1 is an exchange coefficient which describes fluid exchange between the porous matrix and the network of fractures the following initial and boundary conditions have to be considered 2a for t 0 0 x h f h m h 2b for t 0 for x 0 h f h 1 h δ h 2c for x h f h where h 1 is the water depth at x 0 for t 0 for mathematical convenience the h2 linearization technique is used to linearize this mathematical model as for the simple porosity unconfined aquifers e g hantush 1967 chang and yeh 2007 the following relations are considered 3a s f h f t s f h f b h f t 3b s m h m t s m h m b h m t where b is the average saturated water depth assumed to be known in this subsection we will adopt for the average saturated water depth b the relation proposed for this specific problem for the case of single porosity aquifers by upadhyaya and chauhan 1998 and basha 2021 4 b h h 1 2 by further introducing the relations ω f h f 2 and ω m h m 2 one gets the following linear system 5a s f ω f t b k f 2 ω f x 2 b κ ω m ω f 5b s m ω m t κ b ω m ω f by further applying the following dimensionless transformations ω f ω f h 2 ω m ω m h 2 t k f t h s f x x h κ h 2 κ k f λ s m s f ε δ h h b b h 6a h and introducing the shifted variables u f and u m defined by the relations 7a u f 1 ω f 7b u m 1 ω m one gets the following system 8a u f t b 2 u f x 2 κ b u m u f 8b λ u m t κ b u m u f associated with the following initial and boundary conditions for t 0 u f u m 0 for x 0 u f 2 ε ε 2 for x u f 0 9 considering eqs 4 and 6 it follows that b is defined by the relation 10 b 2 ε 2 by applying the laplace transforms to eqs 8a and 8b which are defined by the following relations 11a u f 0 u f e p t d t 11b u m 0 u m e p t d t and solving the resulting ordinary differential equations one obtains the following relations 12a u f 2 ε ε 2 p e x p b κ λ p λ p b κ 12b u m 2 ε ε 2 b κ p b κ λ p e x p b κ λ p λ p b κ where p is the laplace transform variable with respect to the dimensionless time t one has to consider that the non dimensional form of the heads obtained by using the h2 linearization reads in non dimensional form 13a h f 1 u f 13b h m 1 u m where 14a h f h f h 14b h m h m h fully analytical expressions for h f and h m can be obtained for both the small and large time limits for small times when the laplace variable p is large by taking advantage of the expansions 15a a x a 1 x 2 a o a 3 2 15b e x 1 x o x 2 eq 12a becomes 16 u f 2 ε ε 2 e x p b p 1 2 κ b p 3 2 x e x p b back transforming the equation above to the time domain and considering eqs 13a b 17a h f 1 2 ε ε 2 e r f c x 2 b t κ x b t π e x 2 4 b t 1 2 x 2 e r f c x 2 b t h m 1 2 ε ε 2 b κ λ π 2 b t x 2 e r f c x 2 b t 2 π b t x e x 2 4 b t 2 π b 17b 1 12 κ x e x 2 4 b t 2 b t 4 b t x 2 e x 2 4 b t π x 6 b t x 2 erfc x 2 b t π b 1 2 considering further the non dimensional form of the specific discharge or discharge per unit length see moutsopoulos 2021 18 q s 1 ε h f x x 0 where 19 q s q s h k f qs l2t 1 being the specific discharge in dimensional form therefore taking also into account eq 17a it follows that the specific discharge for small times is given by the relation 20 q s ε 2 ε 1 b κ t 2 π b t interestingly enough the relation above has similar features as equation 34 in moutsopoulos 2021 which reads 21 q s ε 1 π t κ t π ε 2 π 3 2 t in particular we want to point out that both relations suggest that in small times the leading order contribution of the outflowing water which is issued for small values of κ directly from the fractures decays as 1 t additionally there is in both relations also a contribution of the water flowing from the porous matrix to the network of fractures which grows as t and κ correspondingly for large time when the laplace variable p becomes small eqs 12a and 12b are reduced to 22 u f u m 2 ε ε 2 p e x p b 1 λ by inverting the equation above to the time domain and considering eqs 13a and 13b 23 h f h m 1 2 ε ε 2 erfc x 2 b t 1 λ the equation above suggests that for large times hydraulic equilibrium occurs where both piezometrics heads in the fractures and in the porous matrix do not depend on the exchange coefficient κ similar results have been reported for the confined aquifer case by landereau et al 2001 and moutsopoulos and tsihrintzis 2009 nevertheless solutions for the whole time range can be obtained only by numerically inverting eqs 12a and 12b with methods like the stehfest algorithm stehfest 1970 the stehfest algorithm also known as gaver stehfest algorithm is a fast and simple method for inversion of the solution of a laplace transform the algorithm is stable and popular especially in reservoir engineering and similar fields where the laplace argument must be a real number the stehfest algorithm delivers accurate results in environments of high numerical precision and it is best when dealing with smooth functions in the stehfest algorithm the integral of the laplace inversion is approximated with a sum of n products of weights and corresponding laplace solutions each laplace solution is evaluated at a different point in the laplace space a critical parameter of the stehfest algorithm is the value of n this parameter has to be an even number and its best values are considered to be within the range 10 to 20 see for example sepp and skachkov 2003 in the present work a value of n 16 was found adequate for the discharge to stream problem the stehfest algorithm was programmed in microsoft excel and it is a variant of the code used by sepp and skachkov 2003 after having inverted eqs 12a b the non dimensional heads are computed by considering eqs 13a and 13b 3 flow in an infinite unconfined double porosity aquifer induced by water extraction with constant pumping rate we investigate the classical problem of water extraction by a constant pumping rate from an aquifer of infinite extent see fig 2 this problem has been theoretically studied for both confined and unconfined flow conditions taking into account non darcian flow compressibility anisotropy and leakage effects the influence of a non zero fringe zone in moench 1984 mathias and butler 2006 tartakovsky and neuman 2007 mishra and neuman 2010 mathias and moutsopoulos 2016 lin and yeh 2021 and wang et al 2021 the solutions of this problem can be used for the interpretation of pumping test and the estimation of aquifer parameters in this paper like in other studies on unconfined dp media i e in sedghi and zhan 2019 and in the field investigation of maréchal et al 2004 it will be assumed that no capillary fringe above the saturated water table exists by considering eqs 1a and 1b and axi symmetrical conditions and using polar coordinates one gets the following relations 24a s f h f t k f r r r h f h f r κ 2 h m 2 h f 2 24b s m h m t κ 2 h m 2 h f 2 which are subject to the following initial and boundary conditions 25a for t 0 0 r h f h m h 25b for t 0 as r 0 h f h f r q w 2 π r k f 25c for r h f h where q w is the extracted flowrate from the aquifer by the pumping process in the bc eq 25b like in other studies on extracting water from dp aquifers de smedt 2011 hayek et al 2018 wang et al 2021 de smedt 2022 well bore storage and skin effects are neglected the left hand side terms of eqs 24a and 24b are replaced by eqs 3a and 3b by considering further ω f h f 2 and ω m h m 2 one gets 26a s f b ω f t k f r r r ω f r κ ω m ω f 26b s m b ω m t κ ω f ω m the dimensionless form of the governing equations can be obtained additionally to eq 6a h with the following variables 27a b r r h q w q w k f h 2 one has to take also into account that for the problem we investigate in this section there is one single constant length the initial water depth h so one has to put b h and b 1 inserting eqs 7a and 7b into the resulting equations and applying the laplace transform defined by eqs 11a and 11b one gets the following system of equations 28a p u f 1 r d d r r d u f d r κ u m u f 28b λ p u m κ u f u m associated with the boundary conditions 29a for r 0 d u f d r q w π p r 29b for r u f 0 it follows that 30a u m κ λ p κ u f and 30b d 2 u f d r 2 1 r d u f d r p λ p κ λ p κ u f the solution of eq 30b reads 31 u f c 1 i o r λ c 2 k o r λ where i o is the modified bessel function of the first kind and zeroth order k o is the modified bessel function of the second kind and zeroth order λ p λ p κ λ p κ while c1 and c 2 are coefficients to be determined by the boundary conditions considering eq 29b and the behavior of i o as r it follows that c1 0 considering further eq 29a and that for small values of x k 0 x γ ln 2 ln x o x 2 where γ is the euler s constant it follows c 2 q w π p so that eq 31 becomes 32a u f q w π p k o r p λ p κ λ p κ and subsequently 32b u m κ q w π p λ p κ k o r p λ p κ λ p κ wolfram mathematica 7 0 software was used to derive the relations above eqs 32a and 32b were numerically transformed back to the time domain using the stehfest algorithm stehfest 1970 described in the previous section however in order to avoid oscillations a value of n 12 and not n 16 as in the previous section was found to be best for this case 4 verifications of the analytical solutions and discussion of the results the analytical solutions obtained using the h2 linearization approach are compared to numerical solutions of the full equations for the problem of one dimensional discharge to a stream adjacent to an unconfined dp aquifer the numerical solutions are obtained by solving the following system of partial differential equations pdes obtained by inserting eqs 6a h and 14a b into eqs 1a b and 2a c 33a h f t x h f h f x κ 2 h m 2 h f 2 33b λ h m t κ 2 h m 2 h f 2 subject to the following ic and bcs 34a b for t 0 0 x h f h m 1 34c for x 0 t 0 h f 1 ε 34d for x t 0 h f 1 correspondingly for the problem describing the flow in an infinite unconfined double porosity aquifer induced by pumping with constant pumping rate one has to solve the following pdes 35a h f t 1 r r r h f h f r κ 2 h m 2 h f 2 35b λ h m t κ 2 h m 2 h f 2 which are subject to the following initial and boundary conditions 36a b for t 0 h f h m 1 36c for t 0 and for r r w h f h f r q w 2 π r 36d for r h f 1 where 36e r w r w h eqs 35a b and 36a d have been derived by inserting eqs 6a h 14a b and 27a b into eqs 24a b and 25a c numerical solutions are obtained with the finite element package comsol multiphysics and also by openfoam comsol is a commercial general purpose simulation software that uses the finite elements method the comsol model is developed using the mathematics toolkit for the simulations of the problem of discharge to an adjacent stream a uniform mesh with constant space steps is used in comsol with δx 0 005 the increment of t is also constant and equal to δt 0 01 but in the first time steps a smaller value of δt 0 001 is used the remaining parameters of the comsol simulation were left at their default values the pumping well problem is typically solved using the model wizard of comsol over a 1d axisymmetric field defined by 0 r 30 a uniform mesh is used with δr 0 005 the increment of t is also constant and equal to δt 0 01 the remaining parameters of the comsol simulation were left at their default values openfoam is a free and open source simulator for computational fluid dynamics the simulator uses the finite volume method for the problems studied here a custom solver was developed which is a variant of the piso pressure implicit with splitting of operators solver the piso solver is typically used to solve the navier stokes equations but in the present work it was adapted for double porosity problems as well although there is no vertical direction in the original problem the use of a quasi 2d field ensured that the generation of the mesh and the operation of the solver were correct in most simulations a constant grid distance with δx 0 005 or equivalently with δr 0 03 for the well problem was used several cases however were also run with denser meshes around x 0 using the grading factor parameter of openfoam for the time increment a value of δt 0 01 was sufficient the codes for comsol openfoam and also for the stehfest algorithm are available from the authors upon request 4 1 problem of one dimensional discharge to a stream adjacent to an unconfined dp aquifer we have tested 22 combinations of the values of the parameters ε λ and κ where ε 0 05 0 6 λ 1 500 and κ 0 0 1 some of these results are presented in figs 3 10 in each figure the upper set of curves correspond to the spatial distribution of h m and the lower ones to h f results are given at two different times comparisons to comsol show that the accuracy of the h2 linearization approximation solutions was better for small values of ε e g fig 3 where ε 0 05 rather than for large ones e g figs 4 and 5 for ε 0 4 the explanation is straightforward for the latter case of large values of ε the fluctuations of the water depths are large so that hf and hm cannot be adequately approximated by a constant mean depth b so that eqs 3a b are not valid the same conclusion holds also for the use of the confined aquifer equations solutions obtained by numerically inverting eqs a6a b and using eqs a7a b although the accuracy of the solutions obtained by the aforementioned method i e the h2 linearization approximation seems to be better than the accuracy obtained using the confined aquifer equations in all cases considered especially for h f as mentioned in the introduction for several problems related to single porosity aquifers the h2 linearization approximation provides more accurate results than the confined aquifer equations approach e g upadhyaya and chauhan 1998 additionally no assumptions and no simplifying approximation were done for the sink source term in eqs 1a b i e the term κ h m 2 h f 2 2 in the case that the h2 linearization approximation was adopted because in this case the unknown dependent variables are h f 2 and h m 2 in other words no loss of accuracy occurs for this case on the contrary for the case where the confined aquifer equations are considered for which the unknown variables are h f and h m the exchange term above is approximated by eq a2c a relation which holds only for small drawdowns when the heads of both the fractures and the matrix are approximately equal to the mean saturated thickness a further analysis of the range of validity of both approximations and also of the fully analytical solutions will be provided in section 5 1 the comparison of the numerical solution by comsol and openfoam to the full analytical solutions eqs 17a b shows satisfactory results for small times t 5 small values of κ κ 0 1 and large values of λ λ 10 as presented in figs 3 9a on the contrary for large values of κ small values of λ and large times the fully analytical eqs 17a b and those presented in moutsopoulos 2021 are not adequate to describe the flow behavior in fuas see figs 3 9b we note further that in the range of validity of eq 17a b the values of h f are independent of λ compare fig 7a where t 5 ε 0 2 κ 0 01 and λ 10 to fig 8a where t 5 ε 0 2 κ 0 01 and λ 50 as it is presented in fig 10b hydraulic equilibrium i e h m h f is achieved for sufficiently large times as predicted by eq 23 this equilibrium is obtained sooner i e for smaller times for large values of κ and small values of λ for the case of small values of κ and large values of λ eqs 17a b are still valid for large times e g t 20 as depicted in fig 9b 4 2 problem of water extraction by a constant pumping rate from an infinite dp aquifer we have considered 21 sets of the parameters q w κ and λ with q w 0 01 1 κ 0 01 1 and λ 1 10 and for a time range t 0 500 some of these results are presented in figs 11 16 we limited the range of variation of q w to 0 01 1 because convergence problems have been encountered in comsol for higher values of q w i e for q w 5 and for q w 10 because for those cases the aquifer dries out in the near well region the results are presented similarly to the results of 1d problem examined in the previous section either as cross sections of the water depth for a fixed time figs 11 and 12 or as time transients for a fixed distance r from the well figs 13 16 the latter case corresponds to the head values measured in an observation well for aquifer parameter estimation purposes in these figures we present the comparison of the semi analytical solutions obtained by considering either eqs 13a b and 32a b h2 linearization approach or eqs a7a b and eqs a11a b confined aquifer equations approach to the numerical solutions of the adequate eqs 35a b although both the h2 linearization approach and the confined aquifer equations approach are valid for relatively small drawdowns it has turned out that the former approach presented in this study is accurate for a larger range of conditions than the latter the explanation has already been provided in the previous section 4 1 subsequently while the expressions obtained by considering the confined aquifer equations approach can be used for approx h f 0 95 or equivalently when the drawdown does not exceed approx 5 of the initial water depth h the solutions issued by considering eqs 32a b can be used for approx h f 0 75 or equivalently when the drawdown does not exceed approx 25 of the initial water depth as depicted in figs 12 15 and 16 therefore the solutions proposed in this study can be used to simulate the flow behavior in an unconfined double porosity aquifer for a larger time span than the ones obtained by using eqs a7a b and a11a b nevertheless the latter expressions can also be used for large time ranges in the case of small pumping flow rates as depicted in figs 11 13 and 14 moreover the use of the confined aquifer equations clearly overestimates hf as depicted in figs 15 and 16 because this approach overestimates the internal fluid exchange between the porous matrix and the fractures network the reasons for this behaviour can be understood by comparing the sink source term in eq 24a b to the one in eq a8a b the latter is larger because the expression h f h m 2 h holds since h f h and h m h by overestimating the flow rate flowing from the matrix to the fractures one overestimates also the mean water table depth in the fractures hf like in the problem of 1d flow to an adjacent stream in the case of water extraction by a pumping well for sufficient long times hydraulic equilibrium occur where h f h m this situation occurs quasi instantaneously in the case of large values of κ and small values of λ as it is depicted in fig 14 where κ 1 on the contrary in the case of small values of κ and relatively large values of λ the relation h f h m holds for large time ranges see fig 15 where κ 10 2 and λ 10 5 application examples 5 1 estimation of the discharge from an unconfined dp aquifer to an adjacent stream the computation of the specific discharge is important for the baseflow computation and also for the estimation of aquifer parameters e g brutsaert and nieber 1977 parlange et al 2001 malvicini et al 2005 hayek 2019 in order to evaluate the derivative of h f at the origin and subsequently estimate q s using eq 18 for both the semi analytical solutions i e the h2 linearization approach and the confined aquifer equations approach and also for the numerical solution one uses the following relation 37 h f x x 0 3 h 1 4 h 2 h 3 2 δ x o δ x 2 where h 1 h 2 and h 3 stand for the values of h f at x 0 x δ x and x 2 δ x respectively the non dimensional specific discharge q s calculated with the numerical solution is compared to the semi analytical expressions issued by inverting with the stehfest algorithm with the fully analytical expression obtained with the h2 linearization eq 20 with the relation proposed by moutsopoulos 2021 and also with the data issued by the confined aquifer equations approximation the results are presented in tables 1 4 a grid spacing of δ x 0 005 was used so that the approx truncation error is 2 5 10 5 the values of q s computed on the basis of a numerical inversion of eq 12a are quite accurate for the whole range of values of the parameters κ and λ for all times t and for small to moderate values of ε say for ε 0 4 for ε 0 05 κ 0 1 and λ 1 the relative error is less than 0 5 see table 1 for ε 0 1 κ 0 2 and λ 1 the relative error is less than 1 0 the corresponding values of q s are not explicitly shown in this study for sake of brevity on the contrary the most severe errors occur for large values of ε for instance for ε 0 4 κ 0 01 and λ 10 the relative error ranges between 2 50 and 3 31 see table 2 while similar are the results for ε 0 4 κ 0 01 and λ 50 not shown in this study for sake of brevity the explanation for those facts is related to the range of validity of eqs 3a b as pointed out already in section 4 1 the solutions proposed by moutsopoulos 2021 see eq 21 provided excellent results for t 5 ε 0 4 κ 0 01 and λ 10 see tables 2 and 3 however the results issued by the h2 linearization approach introduced in this study offer reliable results for the whole time span and for a large range of the parameters κ and λ moreover the fully analytical solution eq 20 gives more accurate results than eq 21 for moderate values of κ see table 1 where ε 0 05 κ 0 1 and λ 1 and table 4 where the case of ε 0 2 κ 0 1 and λ 10 is investigated finally we point out that for all cases the results issued by adopting the confined aquifer equations approximation are much worse to those obtained by the h2 linearization approach see tables 1 4 5 2 interpretation of pumping tests in unconfined dp aquifers the calibration performance of the pumping well model was tested with artificial data as existing field data are limited and they do not guarantee the assumptions of the model a pumping test in an unconfined dp aquifer was conducted by maréchal et al 2004 the corresponding data cannot be used to validate our solutions because the head was measured inside the pumping well so that the dupuit assumption does not hold as discussed in appendix b a realistic case with parameters k f 5 10 5 m s s f 5 10 4 κ 10 8 m s 1 and s m 5 10 3 was considered two alternatives were tested for the volumetric pumping rate qw 0 1 m3 s and qw 0 05 m3 s these alternatives will be called high pumping rate and low pumping rate respectively a distance from the well axis equal to r 40 m was picked for calibration where a fictitious observation well was supposed to be positioned while the parameter h was set to 20 m all these parameter values are considered realistic for practical applications in turn the values of the parameters given above resulted to dimensionless model input parameters equal to r 2 q w 0 5 for high pumping rate and q w 0 25 for low pumping rate κ 0 08 and λ 10 these values were passed to comsol and the results of the comsol simulation were used as artificial calibration data which were supposed to emulate field data the simulated time periods were 1 day and 2 days as longer time periods of calibration data are unusual in practice these time periods correspond to t 432 for simulated time period of 1 day and t 864 for 2 days a selection time step of t 1 was adopted that is 432 and 864 data points of comsol results were selected for the two values of simulation time periods then optimization was used to find the model parameters that would give model results as close as possible to the calibration data the description of the optimization procedure is provided in appendix c two cases were examined for the limits of the model parameters as shown in table 5 these cases are called alternative and wide in the most generic case wide the range of each parameter spanned two orders of magnitude this emulates the case where there is no extra information available for the model parameters in the case called alternative both sf and sm ranges are narrow compared to the generic case wide thus emulating the case where the total porosity i e a combined porosity of fractures and matrix has been measured before the use of the model in which case it is assumed that sf would be between 1 and 10 of the total porosity for consistency in every optimization process the initial values of the parameters were set in an alternating manner i e the minimum value of kf and κ and the maximum value of sf and sm were used for practical application however several different sets of initial values should be used to ensure that the global optimum has been approached properly typical results of the calibrations are displayed in table 6 for both large and small pumping rates and for the two cases of the model parameters ranges that were mentioned above the results include both simulation time periods studied i e 1 and 2 days and both models of the present work h2 linearization and confined aquifer equations approach in every cell of the table the value outside parentheses is the optimization criterion c as defined by eq c1 in appendix c and the value inside parentheses is the calibration quality criterion q as defined by eq c2 in appendix c it is stressed here that the values of the table are just indicative of the global optima because the optimization result can t be guaranteed to be a global optimum several runs of each optimization should be performed in every case from table 6 it is clear that both cases of parameters ranges studied give similar results the calibration performance is better for small pumping rates than for large pumping rates and for the h2 linearization model compared to confined aquifer equations the optimization criterion c has similar value for 1 and 2 days simulation time in all cases with the exception of confined aquifer equations for large pumping rates especially for simulation time period of 2 days this is reasonable as the flow field in this case deviates strongly from the assumption of small drawdowns the calibration quality criterion q is better for longer simulation time period when using the h2 linearization model while the opposite holds for confined aquifer equations again this is reasonable as more data points are used for simulation time periods of 2 days and the failure of the small drawdowns hypothesis becomes more evident especially for large pumping rates overall the calibration performance of the h2 linearization model is considered satisfactory 6 summary and conclusions despite that unconfined dp aquifers are important resources of water in many parts of the world research on the efficient simulation of such systems is still rather scarce this study focusses on the development of analytical solutions of flow in unconfined fractured aquifers the mathematical model is based on the equations proposed by moutsopoulos 2021 unlike other existing models kordilla et al 2012 robineau et al 2018 bresinsky et al 2021 based on the richards equation the model considered in this paper contains only a few parameters the solutions are developed for two problems dealing respectively with discharge from an unconfined aquifer to an adjacent stream and water extraction by a constant pumping rate from an infinite aquifer accuracy and correctness of the analytical solutions are assessed by comparisons with previous analytical solutions or numerical solutions obtained by the comsol and the openfoam software the analytical and semi analytical solution issued using h2 linearization technique provides accurate solutions to the model proposed by moutsopoulos 2021 except for the case of large drawdowns in particular it offers for all cases more accurate solutions than the conventional confined aquifer equations approach it is worth noting that despite the limited range of applicability of the analytical solutions in comparison to the numerical solutions analytical solutions are easier to use and can serve as a first assessment tool they are useful in understanding the parametric trends and qualitative features of the flow kacimov 2000 for the problem of discharge to a stream the analytical solutions were used for the computation of the discharge flow rate which can be used for the recession flow analysis and determination of hydrogeological coefficients the solution of the well extraction problem was used for pumping test analysis purposes further potential applications are the computation of contaminant transport characteristics the estimation of well protection zones and the analytical solutions can also serve as a verification tool for more general numerical schemes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we like to thank frédérick delay for useful discussions and moral support we thank also the editor professor simunek the associate editor dr roubinet and the three anonymous reviewers for their useful comments and time dedicated to the improvement of our manuscript appendix a on the use of confined aquifer equations for the simulation of flow processes in unconfined double porosity equations maréchal et al 2004 w11508 sections 4 and 5 suggested that for the case of small drawdowns one may use the equations of confined double porosity equations namely the warren and root solutions similarly önder 1998 also suggested to use for fuas the equations derived for confined conditions for the case of small drawdowns this approach is similar to the one used for simple porosity aquifers e g chang and yeh 2007 assuming homogeneous properties of the aquifer these equations may be written as s f h f t t f 2 h f x 2 t f 2 h f y 2 κ h m h f a1a s m h m t κ h m h f a1b where tf is the transmissivity of the fractures defined by the relation t f k f b where b is the mean saturated thickness introduced already in section 2 and κ is the depth integrated exchange coefficient κ κ b the main difference between the use of the equations above for confined aquifers and unconfined ones case of small drawdowns respectively is the physical significance of the storage coefficients sf and sm in the former case they depend on the compressibility of water and the solid skeleton while for the latter they are roughly equal to the active porosity of the fractures and of the porous matrix in the unconfined aquifer case they are referred to as specific yield and they are some orders of magnitude larger than in the confined aquifer case it is straightforward to demonstrate that the relations above can be deduced from the equations considered in this study i e eqs 1a b of the main text by considering that for small drawdowns and for aquifers with homogeneous properties the following approximations can be used k f x h f h f x t f 2 h f x 2 a2a k f y h f h f y t f 2 h f y 2 a2b which is widely used for single porosity media by further assuming that for small drawdowns the relations h f h m b holds one gets the following relation h f h m 2 b so that κ 2 h m 2 h f 2 κ h m h f a2c introducing eqs a2a c above to eqs 1a b of the main text one recovers eqs a1a b for the case of the problem presented in section 1 eqs a1a b read s f h f t t f 2 h f x 2 κ h m h f a3a s m h m t κ h m h f a3b where the following initial and boundary conditions have to be considered for t 0 0 x h f h m h a4a b for t 0 for x 0 h f h 1 h δ h a4c for x h f h a4d by considering the following relations for the drawdowns s f h h f a5a s m h h m a5b and further inserting the relations above to eq a3a b considering eqs 6c h for non dimensionalization purposes making use of the relations s f s f h and s m s m h inverting the resulting equation into the laplace space using eqs 11a b and solving the resulting odes one gets finally the following relations s f ε p e x p b κ λ p λ p b κ a6a s m ε b κ p b κ λ p e x p b κ λ p λ p b κ a6b where s f and s m are the laplace transforms of s f and s m respectively one can calculate h f and h m by back transforming eqs a6a b into the time domain and using the relations h f 1 s f a7a h m 1 s m a7b for the case of the second problem extraction from a pumping well eqs a1a b read s f h f t t f 2 h f r 2 1 r h f r κ h m h f a8a s m h m t κ h m h f a8b which are subject to the following initial and boundary conditions for t 0 0 r h f h m h a9a b for t 0 as r 0 b h f r q w 2 π r k f a10a for r h f h a10b where as already mentioned in the main text q w is the extracted flowrate from the aquifer by the pumping process by considering eqs 6a h and 27 one can obtain the non dimensional form of the equations above i e the equations for h f and h m taking into account that for the pumping well problem as already mentioned in section 3 one has to use the relation b h considering further eqs a7a b transforming s f and s m into the laplace space and solving the resulting odes one gets the following solutions s f q w 2 π p k o r p λ p κ λ p κ a11a s m κ q w 2 π p λ p κ k o r p λ p κ λ p κ a11b one can back transform the equations above into the time domain and use eqs a7a b to estimate the non dimensional heads appendix b analysis of the data of well ifp in india maheshwaram watershed a potential site to use the procedure presented in section 5 2 is a hardrock aquifer in india maheshwaram watershed especially the aquifer area around well ifp 16 seems to be compatible with our model because there is a free water table there is both a porous matrix and a network of fractures which is dense enough that the continuum hypothesis is valid maréchal et al 2004 unfortunately the piezometer was not installed in a sufficient distance from the pumping well but inside the wellbore so that the assumption of horizontal flow hypothesis h2 in moutsopoulos 2021 is not expected to be valid i e murray and monkmeyer 1973 the most straightforward way to prove this fact is to take into account that as stated in maréchal et al 2004 see their fig 6 in relatively short time which corresponds to the duration of the pumping test say one day the theis equation gives at least a rough approximation of the head in the fractures inside the well i e for r rw 0 085 m one can use the following relation h f r s f r q w 2 π t f r for the groundwater formation around well ifp 16 qw 0 0058 m3 s tf kf h where h is the average saturated thickness i e h 21 38 m and the hydraulic conductivity is equal to kf 6 2 10 5 m s see table 2 in maréchal et al 2004 it follows that h f r r r w 8 41 so that the slope exceeds 800 and the dupuit assumption is certainly not valid inside the well ifp 16 nevertheless we did not bother to use the software mentioned above on the data of ifp 16 after estimating the values of the parameters k f s f s m and κ it turned out that the solutions issued by inverting either eqs 32a b or eqs a11a b did fit very well to the field data as well as the solution of the warren root model used in maréchal et al 2004 and also with the model of lin and yeh 2021 who used a generalized transfer term to simulate the fluid exchange term between the fractures and the porous matrix different values for kf sf sm and κ were obtained for each model although it was obvious that none of them was adequate to simulate the data inside the well ifp 16 this analysis demonstrates that great care has to be applied when interpreting field data from dp aquifers we think further that it will be interesting to put an observation well at a sufficient distance from the well ifp 16 say r 20 m and use the measured values of hf in order to estimate the parameters of the dp model by applying the procedure described in section 5 2 appendix c description of the software for the interpretation of pumping tests in unconfined dp aquifers the optimization software that was used in the present study was the solver of microsoft excel because this software is freely provided with every version of microsoft office and was found to be adequate for this study as the problem studied is highly nonlinear the evolutionary method of solver was adopted this method uses the genetic algorithm approach and is heuristic in nature which means that a global optimum is not guaranteed the optimization criterion used was c1 c i 1 n y i y p i y i 100 n where yi was the result of the comsol simulation at some simulation time i with i running from 1 to n 432 or n 864 for simulation time periods of 1 day and 2 days respectively and yp i was the prediction of the model at this simulation time that is the criterion used was an average percent absolute deviation of all data points minimization of the criterion c was sought and the parameters of the evolutionary method were set to values that would better ensure determination of an optimum as close as possible to the global optimum the convergence parameter i e the maximum percentage difference in objective values for the top 99 of the population that solver would allow in order to stop was set equal to 10 6 default is 10 4 the mutation rate i e the relative frequency with which some member of the population was altered to create a new trial solution was set equal to 0 3 default is 0 075 the population size that is the number of candidate combinations of the decision variables that the evolutionary method should maintain at any given time was set to 200 default is 100 the random seed field was left blank forcing the random number generator to use a different seed for every optimization finally the maximum time without improvement i e the maximum number of seconds during which the method would continue without a meaningful improvement in the objective value of the best solution in the population was set equal to a very large time period of 36 000 s default is 30 s leaving the decision of the method termination to the maximum number of subproblems allowed which was also set to a very large value of 100000 this number of subproblems would correspond to large clock times 5 to 6 h on a typical desktop personal computer with an intel i5 cpu and 8 gb of ram but typically the optimization process reached a value close to global optimum after 10 000 to 20 000 subproblems with these solver parameters the evolutionary optimization obtained characteristics of a monte carlo optimization method which was found to be useful for the problem studied here after completion of the optimization process a criterion of the calibration performance was evaluated as c2 q i 1 4 q i q q i q i 100 4 where qi was the value used in the comsol simulation for each of the 4 parameters kf sf κ sm and qq i was the result of the optimization process for the corresponding parameter accordingly the calibration quality criterion q was an average percent absolute deviation of the 4 model parameters from their values that were used in the comsol simulation 
2646,although flow in unconfined double porosity aquifers occurs in most part of the world such as in karst and crystalline aquifers their hydrodynamic behavior is still poorly understood especially analytical or semi analytical solutions for flow in unconfined double porosity dp aquifers are scarce in this paper analytical solutions are proposed for two configurations involving groundwater discharge to a stream adjacent to an unconfined aquifer and flow induced by pumping with constant extraction rate respectively the analytical solutions are obtained based on an h2 linearization technique of the full mathematical model and using a laplace transform the above technique has not been used before to investigate the behavior in dp aquifers the newly developed analytical solution is in agreement with numerical solutions of the full mathematical model for small to moderate drawdowns which are as high as 25 of the initial water table case of the well pumping problem this confirms on one side the correctness of the analytical solution and on the other side the reliability of the developed h2 linearization technique on the contrary the conventional confined aquifer equations approach can be used only if the drawdown does not exceed 5 of the initial head values as potential application examples the estimation of the flow rate from an aquifer to an adjacent water body and the interpretation of pumping tests in unconfined double porosity aquifers allowing to estimate the values of the models parameters are proposed keywords unconfined fractured aquifer analytical solutions aquifer stream interactions pumping test data availability no data was used for the research described in the article nomenclature a dummy variable b average saturated water depth l b dimensionless average saturated water depth eq 6h c optimization criterion for the evaluation of pumping tests see eq c1 h f water table elevation head in the fractures network see figs 1 and 2 l h f dimensionless water table elevation head in the fractures network see eq 14a h m water table elevation head in the porous matrix see figs 1 and 2 l h m dimensionless water table elevation head in the porous matrix see eq 14b h initial water depth see figs 1 and 2 l h 1 water depth at x 0 for t 0 used in the problem of one dimensional discharge to a stream see fig 1 l i o modified bessel function of the first kind and zeroth order k o modified bessel function of the second kind and zeroth order kf hydraulic conductivity of the fractures network lt 1 n number of data points from the comsol results used for calibration see appendix c n parameter for the stehfest algorithm p laplace transform variable with respect to the dimensionless time t r distance from the centre of the well see fig 2 l r dimensionless distance from the centre of the well see eq 27a r w well radius l r w dimensionless well radius see eq 36e qs specific discharge discharge per unit length from the aquifer to an adjacent stream l2t 1 q s dimensionless specific discharge see eq 19 q criterion for the calibration of the quality performance of the procedure for the evaluation of pumping tests see eq c2 q w extracted flowrate from the aquifer by the pumping process see fig 2 l3t 1 q w dimensionless form of q w see eq 27b s f drawdown in the fractures network see eq 5a l s f dimensionless drawdown in the fractures network s m drawdown in the porous matrix see eq 5b l s m dimensionless drawdown in the porous matrix s f specific yield of the fractures continuum s m specific yield of the porous matrix t time t t dimensionless time see eq 6c tf the transmissivity of the fractures network l2t 1 u f dimensionless shifted variable see eq 7a u f laplace transform of u f u m dimensionless shifted variable see eq 7b x distance cartesian coordinate l x dimensionless distance see eq 6d greek letters δ h drawdown in the stream and in the adjacent aquifer for x 0 see fig 1 l ε dimensionless drawdown in the stream and in the adjacent aquifer for x 0 see eq 6g κ exchange coefficient which describes fluid exchange between the porous matrix and the network of fractures l 1t 1 κ dimensionless form of κ see eq 6e κ depth integrated exchange coefficient used in the confined equations model t 1 λ ratio of the specific yields of the porous matrix and the fractures continuum see eq 6f ω f variable for the h2 linearization approximation procedure ω f h f 2 l2 ω f dimensionless form of ω f defined by eq 6a ω m variable for the h2 linearization approximation procedure ω m h m 2 l2 ω m dimensionless form of ω m defined by eq 6b subscripts f network of fractures m porous matrix superscripts dimensionless variable 1 introduction understanding groundwater flow in unconfined aquifers uas is of first importance in several environmental and geotechnical applications these aquifers are of ecological and economical significance as they play a major role in controlling surface subsurface water exchange alattar et al 2020 linking groundwater resources and surface water as well as in the recharge of confined aquifers and the contaminant attenuation processes uas represent an important source of freshwater and water supply şen 2015 these aquifers are often shallow and their groundwater is easily accessible they are also highly vulnerable to contamination by surface pollutants guo et al 2021 zheng et al 2021 uas are one of the main components of the aquifer thermal energy storage systems cabeza 2021 uas are also investigated in studies related to tunnel excavation and design maréchal et al 2014 sedghi and zhan 2021a as well as in studies dealing with seepage through dams zarif sanayei and javdanian 2020 among others fractured unconfined aquifers fuas are found globally in karst areas ghasemizadeh et al 2015 sanz lobón et al 2015 as for instance in italy petronici et al 2019 in iran sedghi and zhan 2021b in the usa ren et al 2018 in australia and in ireland wright 2000 and in the mediterranean basin koohbor et al 2020 hard rock aquifers like metamorphic and magmatic geological formations cover approximately 20 percent of the present land surface including large areas in africa india brazil and europe gustafson and krásný 1994 in several cases hard rock aquifers which are usually fractured occupy the upper tens of meters of the subsurface profile maréchal et al 2004 and therefore a free water surface may be present and they can often exhibit unconfined flow conditions in fuas the fracture networks provide preferential flow pathways and play an important role in the delineation of the water table level the water balance of the aquifers and in conducting groundwater in wellbores luo et al 2020 despite that fuas are widely distributed throughout the world these aquifers are rarely investigated and more attention is given in the literature to fractured confined aquifers regardless of the important role of fractures in controlling groundwater flow and water table levels many modeling based studies on fuas do not consider the fracture networks groundwater flow in fractured aquifers can be tackled using either the discrete fracture model dfm or the continuum approach ca with dfm the fractures i e their geometry and properties are described explicitly koohbor et al 2020 dfm is known to be computationally expensive especially in the case of a high number of fractures despite the progress on the development of new dfms real field applications of these models are constrained to cases with limited number of well defined fractures hirthe and graf 2015 ramasomanana et al 2018 a major drawback of the dfms is that they need detailed description of the fractures geometry which is a complex task in field applications unlike dfm with the ca the fractured domain is simulated as a porous medium with equivalent parameters this assumption allows for avoiding dense computational grids related to the fractures network as fractures are not represented explicitly the ca is applicable if the fracture density is high and when the fracture orientation is highly variable long et al 1982 however in fractured domains the matrix blocks and the fractures have different hydrodynamic properties thus it is not easy to simulate these domains with a single continuum model the concept of dual porosity dp is an improvement of the ca in which the domain is considered as a superposition of two continuums representing the porous matrix and the fractures respectively barenblatt et al 1960 hassanzadeh et al 2009 fahs et al 2014 jerbi et al 2017 with the dp model two porosities and two pressure heads representing respectively matrix rocks and fractures are associated to each point of the domain the matrix continuum can store water and exchange it with the fracture continuum the permeability of the matrix continuum often can be neglected when this permeability is considered the model is called dual permeability aguilar lópez et al 2020 the dp model cannot simulate preferential flows but it allows for reducing computational burden and does not need detailed description of the fractures topology dp models are widely used in real field studies when the dfm cannot be applied the dp approach was firstly developed to simulate fluid flow in geological oil reservoirs then it has been used to simulate water flow in confined aquifers e g streltsova 1976 moench 1984 gerke and genuchten 1993 önder 1998 noetinger and estebenet 2000 landereau et al 2001 park and zhan 2003 delay et al 2007 moutsopoulos and tsihrintzis 2009 fahs et al 2014 sanchez vila et al 2016 hayek et al 2018 and wang et al 2021 analytical solutions for confined dp aquifers have been proposed by wilson and aifantis 1982 moench 1984 önder 1998 park and zhan 2003 de smedt 2011 gonzález calderón et al 2017 dejam et al 2018 dewandel et al 2018 hayek et al 2018 asadi et al 2020 and de smedt 2022 on the contrary fewer publications have investigated the use of the dp model for fuas the most straightforward dp approach to simulate the flow behavior in fuas is based on the same equations as for the confined aquifers e g önder 1998 maréchal et al 2004 whereas one has to take into account that the storage coefficient for phreatic aquifers is roughly equal to the active porosity of each equivalent continuum and it is therefore several orders of magnitude larger than the same coefficients of confined aquifers the approach above is justified only for small drawdowns more elaborate approaches have been also proposed in the literature boulton and streltsova adams 1978 used the dp approach to investigate water extraction from a fua by a vertical fully penetrating well they considered an idealized geometry of the aquifer which was assumed to consist of identical equidistant horizontal fractures separated by identical slab shaped porous blocks they considered the effects of delayed yield which may be of importance at early times and near the well and the effects of transient matrix to fissure flow sedghi and zhan 2019 investigated interactions between a water body and unconfined single and double porosity aquifers they assumed that the aquifer was composed by identical periodical sphere shaped porous blocks separated by fractures considering an appropriate kinematic boundary condition at the water surface they could simulate flows with vertical velocity components potential drawbacks of the studies of boulton and streltsova adams 1978 and sedghi and zhan 2019 are the use of an idealized geometry of both the porous matrix consisting of either slab shaped or sphere shaped porous blocks and the fractures and the fact that in both studies the authors have not considered that the internal flow process is different in the case that the saturated porous matrix is adjacent to saturated fractures than in the case that it is adjacent to non saturated fractures kordilla et al 2012 have developed a dp model based on a generalized form of the richards equation to simulate the flow behavior in an unconfined fractured aquifer in south west germany although their approach is quite sophisticated it may not be possible to be implemented in many cases of practical interest because it involves the use of 21 parameters which are difficult to be determined furthermore their approach may not be adequate when the aperture of the fractures in the unsaturated zone is relatively large so that gravity forces prevail over the capillary ones and flow in form of films or droplets takes place in these fractures the same approach has been also adopted by other researchers e g robineau et al 2018 bresinsky et al 2021 a simpler dp model to simulate flow processes in fuas involving only 4 6 parameters has been proposed by moutsopoulos et al 2001 and moutsopoulos 2021 this model is based on two essential assumptions i discharging and not recharging flow conditions take place ii the apertures of the vertical fractures are large enough so that gravity forces prevail over capillary ones deep discussion about these assumptions can be found in moutsopoulos 2021 the latter hypothesis seems to hold for several cases as described in su et al 1999 salve et al 2004 kordilla et al 2017 noffz et al 2019 and kordilla et al 2021 advantages of this model above over both the approaches of boulton and streltsova adams 1978 and sedghi and zhan 2019 are that the fluid exchange between the porous matrix and the network of fractures is more adequately described in particular it has been taken into account that the fluid transfer process depends on whether the saturated porous matrix is adjacent to a saturated fracture or to a non saturated one and that no schematization of the pore space was assumed a substantial body of literature on the dp model is mainly based on numerical simulations where the equations are solved using numerical methods i e trottier et al 2014 fahs et al 2014 aguilar lópez et al 2020 rice et al 2021 despite the progress in numerical solutions and computational techniques there is still a need for analytical solutions due to their reduced computational costs the analytical solutions are widely used as first assessment tools in field studies despite the simplification required to obtain analytical solutions these solutions are usually used as references in validating and verifying numerical solutions and codes analytical solutions provide explicit relations between input parameters and model outputs thus they are useful for aquifer characterization with pumping tests and for conducting sensitivity or uncertainty analysis several analytical solutions have been developed for the dp model in different configurations of aquifers and pumping which as already mentioned are mainly limited to confined aquifers in contrast to confined dp aquifers analytical solutions for dp models in fuas are scarce boulton and streltsova adams 1978 and sedghi and zhan 2019 proposed analytical solutions to their models that have several limitations as discussed above while the model proposed by moutsopoulos 2021 overcomes these limitations analytical solutions of this model are still not well developed there is only one analytical solution to this model which has been developed using a regular perturbation approach this solution refers to a single problem and is valid under rather restrictive conditions because it can be used only for short time spans and a rather small range of the values of the hydrogeological parameters this study aims at developing analytical and semi analytical solutions for the dp model in unconfined aquifers by considering the mathematical model proposed by moutsopoulos 2021 this model is valid for a wide range of values of the hydrogeological parameters in this model the difficulty lies in the nonlinearity of the pressured head terms that need to be taken into account for analytical or semi analytical solutions in single porosity models a classical way to circumvent the nonlinearity and linearize the boussinesq equation is to replace the variable thickness with an average wetted thickness so that the boussinesq equation is transformed to the same form as the confined flow equation yeh and chang 2013 this same approach as already stated has been implicitly adopted for the dp model in önder 1998 and maréchal et al 2004 an alternative approach is the h2 linearization technique where the storage term of the boussinesq equation is multiplied by the ratio of the water depth to an average saturated depth hantush 1967 marino 1973 chang and yeh 2007 ilias et al 2008 teloglou and bansal 2012 this alternative approach gives better results than the straightforward transformation of the boussinesq equation to the confined flow equation upadhyaya and chauhan 1998 in this study the h2 linearization approach is used in order to obtain the semi analytical solutions two classical problems are investigated i the one dimensional discharge to a stream adjacent to an unconfined double porosity aquifer and ii the flow in an infinite unconfined double porosity aquifer with homogeneous properties induced by pumping with constant rate while the analytical solution presented in moutsopoulos 2021 developed using a regular perturbation approach is limited to the problem of discharge to a stream adjacent to an ua and is adequate only for a relatively small range of the hydrogeological parameters and small times the analytical solutions presented in this study based on the h2 linearization approach generalize the previous solution to a wider range of parameters and large durations and address also a new problem of pumping in an infinite dual porosity ua to the best of our knowledge the h2 linearization technique has not been used before to investigate the behavior in dp aquifers we discuss the validity of this technique by comparing the analytical solution to a numerical solution of the non linear model which is developed using the finite element framework comsol multi physics and also the finite volume framework openfoam by using these software tools the accuracy of the solutions obtained by using the confined aquifer equations approach see appendix a is also examined the developed analytical solutions are used to investigate the discharge to a stream adjacent to fuas and also to pumping test analysis 2 one dimensional discharge to a stream adjacent to an unconfined double porosity aquifer the classical problem of discharge to a stream adjacent to a single porosity ua has been studied by several researchers e g guo 1997 lockington 1997 upadhyaya and chauhan 1998 hayek 2019 and basha 2021 moutsopoulos 2021 investigated this problem for a dp in an ua by neglecting the hydraulic conductivity of the porous matrix and also external sink source terms caused by precipitation evapotranspiration or by fluid exchange with adjacent geological layers and assuming that the aquifers properties are isotropic homogeneous and constant the model proposed by moutsopoulos 2021 depicted in fig 1 reads 1a s f h f t k f x h f h f x κ 2 h m 2 h f 2 1b s m h m t κ 2 h m 2 h f 2 where t t is time x l and y l are the space coordinates in an horizontal plane sf and sm stand for the specific yield of the continuum composed of the fractures and of the porous matrix respectively hf l and hm l denote the water table elevation in the fractures and the porous matrix respectively kf lt 1 is the hydraulic conductivity of the fractures network and κ l 1t 1 is an exchange coefficient which describes fluid exchange between the porous matrix and the network of fractures the following initial and boundary conditions have to be considered 2a for t 0 0 x h f h m h 2b for t 0 for x 0 h f h 1 h δ h 2c for x h f h where h 1 is the water depth at x 0 for t 0 for mathematical convenience the h2 linearization technique is used to linearize this mathematical model as for the simple porosity unconfined aquifers e g hantush 1967 chang and yeh 2007 the following relations are considered 3a s f h f t s f h f b h f t 3b s m h m t s m h m b h m t where b is the average saturated water depth assumed to be known in this subsection we will adopt for the average saturated water depth b the relation proposed for this specific problem for the case of single porosity aquifers by upadhyaya and chauhan 1998 and basha 2021 4 b h h 1 2 by further introducing the relations ω f h f 2 and ω m h m 2 one gets the following linear system 5a s f ω f t b k f 2 ω f x 2 b κ ω m ω f 5b s m ω m t κ b ω m ω f by further applying the following dimensionless transformations ω f ω f h 2 ω m ω m h 2 t k f t h s f x x h κ h 2 κ k f λ s m s f ε δ h h b b h 6a h and introducing the shifted variables u f and u m defined by the relations 7a u f 1 ω f 7b u m 1 ω m one gets the following system 8a u f t b 2 u f x 2 κ b u m u f 8b λ u m t κ b u m u f associated with the following initial and boundary conditions for t 0 u f u m 0 for x 0 u f 2 ε ε 2 for x u f 0 9 considering eqs 4 and 6 it follows that b is defined by the relation 10 b 2 ε 2 by applying the laplace transforms to eqs 8a and 8b which are defined by the following relations 11a u f 0 u f e p t d t 11b u m 0 u m e p t d t and solving the resulting ordinary differential equations one obtains the following relations 12a u f 2 ε ε 2 p e x p b κ λ p λ p b κ 12b u m 2 ε ε 2 b κ p b κ λ p e x p b κ λ p λ p b κ where p is the laplace transform variable with respect to the dimensionless time t one has to consider that the non dimensional form of the heads obtained by using the h2 linearization reads in non dimensional form 13a h f 1 u f 13b h m 1 u m where 14a h f h f h 14b h m h m h fully analytical expressions for h f and h m can be obtained for both the small and large time limits for small times when the laplace variable p is large by taking advantage of the expansions 15a a x a 1 x 2 a o a 3 2 15b e x 1 x o x 2 eq 12a becomes 16 u f 2 ε ε 2 e x p b p 1 2 κ b p 3 2 x e x p b back transforming the equation above to the time domain and considering eqs 13a b 17a h f 1 2 ε ε 2 e r f c x 2 b t κ x b t π e x 2 4 b t 1 2 x 2 e r f c x 2 b t h m 1 2 ε ε 2 b κ λ π 2 b t x 2 e r f c x 2 b t 2 π b t x e x 2 4 b t 2 π b 17b 1 12 κ x e x 2 4 b t 2 b t 4 b t x 2 e x 2 4 b t π x 6 b t x 2 erfc x 2 b t π b 1 2 considering further the non dimensional form of the specific discharge or discharge per unit length see moutsopoulos 2021 18 q s 1 ε h f x x 0 where 19 q s q s h k f qs l2t 1 being the specific discharge in dimensional form therefore taking also into account eq 17a it follows that the specific discharge for small times is given by the relation 20 q s ε 2 ε 1 b κ t 2 π b t interestingly enough the relation above has similar features as equation 34 in moutsopoulos 2021 which reads 21 q s ε 1 π t κ t π ε 2 π 3 2 t in particular we want to point out that both relations suggest that in small times the leading order contribution of the outflowing water which is issued for small values of κ directly from the fractures decays as 1 t additionally there is in both relations also a contribution of the water flowing from the porous matrix to the network of fractures which grows as t and κ correspondingly for large time when the laplace variable p becomes small eqs 12a and 12b are reduced to 22 u f u m 2 ε ε 2 p e x p b 1 λ by inverting the equation above to the time domain and considering eqs 13a and 13b 23 h f h m 1 2 ε ε 2 erfc x 2 b t 1 λ the equation above suggests that for large times hydraulic equilibrium occurs where both piezometrics heads in the fractures and in the porous matrix do not depend on the exchange coefficient κ similar results have been reported for the confined aquifer case by landereau et al 2001 and moutsopoulos and tsihrintzis 2009 nevertheless solutions for the whole time range can be obtained only by numerically inverting eqs 12a and 12b with methods like the stehfest algorithm stehfest 1970 the stehfest algorithm also known as gaver stehfest algorithm is a fast and simple method for inversion of the solution of a laplace transform the algorithm is stable and popular especially in reservoir engineering and similar fields where the laplace argument must be a real number the stehfest algorithm delivers accurate results in environments of high numerical precision and it is best when dealing with smooth functions in the stehfest algorithm the integral of the laplace inversion is approximated with a sum of n products of weights and corresponding laplace solutions each laplace solution is evaluated at a different point in the laplace space a critical parameter of the stehfest algorithm is the value of n this parameter has to be an even number and its best values are considered to be within the range 10 to 20 see for example sepp and skachkov 2003 in the present work a value of n 16 was found adequate for the discharge to stream problem the stehfest algorithm was programmed in microsoft excel and it is a variant of the code used by sepp and skachkov 2003 after having inverted eqs 12a b the non dimensional heads are computed by considering eqs 13a and 13b 3 flow in an infinite unconfined double porosity aquifer induced by water extraction with constant pumping rate we investigate the classical problem of water extraction by a constant pumping rate from an aquifer of infinite extent see fig 2 this problem has been theoretically studied for both confined and unconfined flow conditions taking into account non darcian flow compressibility anisotropy and leakage effects the influence of a non zero fringe zone in moench 1984 mathias and butler 2006 tartakovsky and neuman 2007 mishra and neuman 2010 mathias and moutsopoulos 2016 lin and yeh 2021 and wang et al 2021 the solutions of this problem can be used for the interpretation of pumping test and the estimation of aquifer parameters in this paper like in other studies on unconfined dp media i e in sedghi and zhan 2019 and in the field investigation of maréchal et al 2004 it will be assumed that no capillary fringe above the saturated water table exists by considering eqs 1a and 1b and axi symmetrical conditions and using polar coordinates one gets the following relations 24a s f h f t k f r r r h f h f r κ 2 h m 2 h f 2 24b s m h m t κ 2 h m 2 h f 2 which are subject to the following initial and boundary conditions 25a for t 0 0 r h f h m h 25b for t 0 as r 0 h f h f r q w 2 π r k f 25c for r h f h where q w is the extracted flowrate from the aquifer by the pumping process in the bc eq 25b like in other studies on extracting water from dp aquifers de smedt 2011 hayek et al 2018 wang et al 2021 de smedt 2022 well bore storage and skin effects are neglected the left hand side terms of eqs 24a and 24b are replaced by eqs 3a and 3b by considering further ω f h f 2 and ω m h m 2 one gets 26a s f b ω f t k f r r r ω f r κ ω m ω f 26b s m b ω m t κ ω f ω m the dimensionless form of the governing equations can be obtained additionally to eq 6a h with the following variables 27a b r r h q w q w k f h 2 one has to take also into account that for the problem we investigate in this section there is one single constant length the initial water depth h so one has to put b h and b 1 inserting eqs 7a and 7b into the resulting equations and applying the laplace transform defined by eqs 11a and 11b one gets the following system of equations 28a p u f 1 r d d r r d u f d r κ u m u f 28b λ p u m κ u f u m associated with the boundary conditions 29a for r 0 d u f d r q w π p r 29b for r u f 0 it follows that 30a u m κ λ p κ u f and 30b d 2 u f d r 2 1 r d u f d r p λ p κ λ p κ u f the solution of eq 30b reads 31 u f c 1 i o r λ c 2 k o r λ where i o is the modified bessel function of the first kind and zeroth order k o is the modified bessel function of the second kind and zeroth order λ p λ p κ λ p κ while c1 and c 2 are coefficients to be determined by the boundary conditions considering eq 29b and the behavior of i o as r it follows that c1 0 considering further eq 29a and that for small values of x k 0 x γ ln 2 ln x o x 2 where γ is the euler s constant it follows c 2 q w π p so that eq 31 becomes 32a u f q w π p k o r p λ p κ λ p κ and subsequently 32b u m κ q w π p λ p κ k o r p λ p κ λ p κ wolfram mathematica 7 0 software was used to derive the relations above eqs 32a and 32b were numerically transformed back to the time domain using the stehfest algorithm stehfest 1970 described in the previous section however in order to avoid oscillations a value of n 12 and not n 16 as in the previous section was found to be best for this case 4 verifications of the analytical solutions and discussion of the results the analytical solutions obtained using the h2 linearization approach are compared to numerical solutions of the full equations for the problem of one dimensional discharge to a stream adjacent to an unconfined dp aquifer the numerical solutions are obtained by solving the following system of partial differential equations pdes obtained by inserting eqs 6a h and 14a b into eqs 1a b and 2a c 33a h f t x h f h f x κ 2 h m 2 h f 2 33b λ h m t κ 2 h m 2 h f 2 subject to the following ic and bcs 34a b for t 0 0 x h f h m 1 34c for x 0 t 0 h f 1 ε 34d for x t 0 h f 1 correspondingly for the problem describing the flow in an infinite unconfined double porosity aquifer induced by pumping with constant pumping rate one has to solve the following pdes 35a h f t 1 r r r h f h f r κ 2 h m 2 h f 2 35b λ h m t κ 2 h m 2 h f 2 which are subject to the following initial and boundary conditions 36a b for t 0 h f h m 1 36c for t 0 and for r r w h f h f r q w 2 π r 36d for r h f 1 where 36e r w r w h eqs 35a b and 36a d have been derived by inserting eqs 6a h 14a b and 27a b into eqs 24a b and 25a c numerical solutions are obtained with the finite element package comsol multiphysics and also by openfoam comsol is a commercial general purpose simulation software that uses the finite elements method the comsol model is developed using the mathematics toolkit for the simulations of the problem of discharge to an adjacent stream a uniform mesh with constant space steps is used in comsol with δx 0 005 the increment of t is also constant and equal to δt 0 01 but in the first time steps a smaller value of δt 0 001 is used the remaining parameters of the comsol simulation were left at their default values the pumping well problem is typically solved using the model wizard of comsol over a 1d axisymmetric field defined by 0 r 30 a uniform mesh is used with δr 0 005 the increment of t is also constant and equal to δt 0 01 the remaining parameters of the comsol simulation were left at their default values openfoam is a free and open source simulator for computational fluid dynamics the simulator uses the finite volume method for the problems studied here a custom solver was developed which is a variant of the piso pressure implicit with splitting of operators solver the piso solver is typically used to solve the navier stokes equations but in the present work it was adapted for double porosity problems as well although there is no vertical direction in the original problem the use of a quasi 2d field ensured that the generation of the mesh and the operation of the solver were correct in most simulations a constant grid distance with δx 0 005 or equivalently with δr 0 03 for the well problem was used several cases however were also run with denser meshes around x 0 using the grading factor parameter of openfoam for the time increment a value of δt 0 01 was sufficient the codes for comsol openfoam and also for the stehfest algorithm are available from the authors upon request 4 1 problem of one dimensional discharge to a stream adjacent to an unconfined dp aquifer we have tested 22 combinations of the values of the parameters ε λ and κ where ε 0 05 0 6 λ 1 500 and κ 0 0 1 some of these results are presented in figs 3 10 in each figure the upper set of curves correspond to the spatial distribution of h m and the lower ones to h f results are given at two different times comparisons to comsol show that the accuracy of the h2 linearization approximation solutions was better for small values of ε e g fig 3 where ε 0 05 rather than for large ones e g figs 4 and 5 for ε 0 4 the explanation is straightforward for the latter case of large values of ε the fluctuations of the water depths are large so that hf and hm cannot be adequately approximated by a constant mean depth b so that eqs 3a b are not valid the same conclusion holds also for the use of the confined aquifer equations solutions obtained by numerically inverting eqs a6a b and using eqs a7a b although the accuracy of the solutions obtained by the aforementioned method i e the h2 linearization approximation seems to be better than the accuracy obtained using the confined aquifer equations in all cases considered especially for h f as mentioned in the introduction for several problems related to single porosity aquifers the h2 linearization approximation provides more accurate results than the confined aquifer equations approach e g upadhyaya and chauhan 1998 additionally no assumptions and no simplifying approximation were done for the sink source term in eqs 1a b i e the term κ h m 2 h f 2 2 in the case that the h2 linearization approximation was adopted because in this case the unknown dependent variables are h f 2 and h m 2 in other words no loss of accuracy occurs for this case on the contrary for the case where the confined aquifer equations are considered for which the unknown variables are h f and h m the exchange term above is approximated by eq a2c a relation which holds only for small drawdowns when the heads of both the fractures and the matrix are approximately equal to the mean saturated thickness a further analysis of the range of validity of both approximations and also of the fully analytical solutions will be provided in section 5 1 the comparison of the numerical solution by comsol and openfoam to the full analytical solutions eqs 17a b shows satisfactory results for small times t 5 small values of κ κ 0 1 and large values of λ λ 10 as presented in figs 3 9a on the contrary for large values of κ small values of λ and large times the fully analytical eqs 17a b and those presented in moutsopoulos 2021 are not adequate to describe the flow behavior in fuas see figs 3 9b we note further that in the range of validity of eq 17a b the values of h f are independent of λ compare fig 7a where t 5 ε 0 2 κ 0 01 and λ 10 to fig 8a where t 5 ε 0 2 κ 0 01 and λ 50 as it is presented in fig 10b hydraulic equilibrium i e h m h f is achieved for sufficiently large times as predicted by eq 23 this equilibrium is obtained sooner i e for smaller times for large values of κ and small values of λ for the case of small values of κ and large values of λ eqs 17a b are still valid for large times e g t 20 as depicted in fig 9b 4 2 problem of water extraction by a constant pumping rate from an infinite dp aquifer we have considered 21 sets of the parameters q w κ and λ with q w 0 01 1 κ 0 01 1 and λ 1 10 and for a time range t 0 500 some of these results are presented in figs 11 16 we limited the range of variation of q w to 0 01 1 because convergence problems have been encountered in comsol for higher values of q w i e for q w 5 and for q w 10 because for those cases the aquifer dries out in the near well region the results are presented similarly to the results of 1d problem examined in the previous section either as cross sections of the water depth for a fixed time figs 11 and 12 or as time transients for a fixed distance r from the well figs 13 16 the latter case corresponds to the head values measured in an observation well for aquifer parameter estimation purposes in these figures we present the comparison of the semi analytical solutions obtained by considering either eqs 13a b and 32a b h2 linearization approach or eqs a7a b and eqs a11a b confined aquifer equations approach to the numerical solutions of the adequate eqs 35a b although both the h2 linearization approach and the confined aquifer equations approach are valid for relatively small drawdowns it has turned out that the former approach presented in this study is accurate for a larger range of conditions than the latter the explanation has already been provided in the previous section 4 1 subsequently while the expressions obtained by considering the confined aquifer equations approach can be used for approx h f 0 95 or equivalently when the drawdown does not exceed approx 5 of the initial water depth h the solutions issued by considering eqs 32a b can be used for approx h f 0 75 or equivalently when the drawdown does not exceed approx 25 of the initial water depth as depicted in figs 12 15 and 16 therefore the solutions proposed in this study can be used to simulate the flow behavior in an unconfined double porosity aquifer for a larger time span than the ones obtained by using eqs a7a b and a11a b nevertheless the latter expressions can also be used for large time ranges in the case of small pumping flow rates as depicted in figs 11 13 and 14 moreover the use of the confined aquifer equations clearly overestimates hf as depicted in figs 15 and 16 because this approach overestimates the internal fluid exchange between the porous matrix and the fractures network the reasons for this behaviour can be understood by comparing the sink source term in eq 24a b to the one in eq a8a b the latter is larger because the expression h f h m 2 h holds since h f h and h m h by overestimating the flow rate flowing from the matrix to the fractures one overestimates also the mean water table depth in the fractures hf like in the problem of 1d flow to an adjacent stream in the case of water extraction by a pumping well for sufficient long times hydraulic equilibrium occur where h f h m this situation occurs quasi instantaneously in the case of large values of κ and small values of λ as it is depicted in fig 14 where κ 1 on the contrary in the case of small values of κ and relatively large values of λ the relation h f h m holds for large time ranges see fig 15 where κ 10 2 and λ 10 5 application examples 5 1 estimation of the discharge from an unconfined dp aquifer to an adjacent stream the computation of the specific discharge is important for the baseflow computation and also for the estimation of aquifer parameters e g brutsaert and nieber 1977 parlange et al 2001 malvicini et al 2005 hayek 2019 in order to evaluate the derivative of h f at the origin and subsequently estimate q s using eq 18 for both the semi analytical solutions i e the h2 linearization approach and the confined aquifer equations approach and also for the numerical solution one uses the following relation 37 h f x x 0 3 h 1 4 h 2 h 3 2 δ x o δ x 2 where h 1 h 2 and h 3 stand for the values of h f at x 0 x δ x and x 2 δ x respectively the non dimensional specific discharge q s calculated with the numerical solution is compared to the semi analytical expressions issued by inverting with the stehfest algorithm with the fully analytical expression obtained with the h2 linearization eq 20 with the relation proposed by moutsopoulos 2021 and also with the data issued by the confined aquifer equations approximation the results are presented in tables 1 4 a grid spacing of δ x 0 005 was used so that the approx truncation error is 2 5 10 5 the values of q s computed on the basis of a numerical inversion of eq 12a are quite accurate for the whole range of values of the parameters κ and λ for all times t and for small to moderate values of ε say for ε 0 4 for ε 0 05 κ 0 1 and λ 1 the relative error is less than 0 5 see table 1 for ε 0 1 κ 0 2 and λ 1 the relative error is less than 1 0 the corresponding values of q s are not explicitly shown in this study for sake of brevity on the contrary the most severe errors occur for large values of ε for instance for ε 0 4 κ 0 01 and λ 10 the relative error ranges between 2 50 and 3 31 see table 2 while similar are the results for ε 0 4 κ 0 01 and λ 50 not shown in this study for sake of brevity the explanation for those facts is related to the range of validity of eqs 3a b as pointed out already in section 4 1 the solutions proposed by moutsopoulos 2021 see eq 21 provided excellent results for t 5 ε 0 4 κ 0 01 and λ 10 see tables 2 and 3 however the results issued by the h2 linearization approach introduced in this study offer reliable results for the whole time span and for a large range of the parameters κ and λ moreover the fully analytical solution eq 20 gives more accurate results than eq 21 for moderate values of κ see table 1 where ε 0 05 κ 0 1 and λ 1 and table 4 where the case of ε 0 2 κ 0 1 and λ 10 is investigated finally we point out that for all cases the results issued by adopting the confined aquifer equations approximation are much worse to those obtained by the h2 linearization approach see tables 1 4 5 2 interpretation of pumping tests in unconfined dp aquifers the calibration performance of the pumping well model was tested with artificial data as existing field data are limited and they do not guarantee the assumptions of the model a pumping test in an unconfined dp aquifer was conducted by maréchal et al 2004 the corresponding data cannot be used to validate our solutions because the head was measured inside the pumping well so that the dupuit assumption does not hold as discussed in appendix b a realistic case with parameters k f 5 10 5 m s s f 5 10 4 κ 10 8 m s 1 and s m 5 10 3 was considered two alternatives were tested for the volumetric pumping rate qw 0 1 m3 s and qw 0 05 m3 s these alternatives will be called high pumping rate and low pumping rate respectively a distance from the well axis equal to r 40 m was picked for calibration where a fictitious observation well was supposed to be positioned while the parameter h was set to 20 m all these parameter values are considered realistic for practical applications in turn the values of the parameters given above resulted to dimensionless model input parameters equal to r 2 q w 0 5 for high pumping rate and q w 0 25 for low pumping rate κ 0 08 and λ 10 these values were passed to comsol and the results of the comsol simulation were used as artificial calibration data which were supposed to emulate field data the simulated time periods were 1 day and 2 days as longer time periods of calibration data are unusual in practice these time periods correspond to t 432 for simulated time period of 1 day and t 864 for 2 days a selection time step of t 1 was adopted that is 432 and 864 data points of comsol results were selected for the two values of simulation time periods then optimization was used to find the model parameters that would give model results as close as possible to the calibration data the description of the optimization procedure is provided in appendix c two cases were examined for the limits of the model parameters as shown in table 5 these cases are called alternative and wide in the most generic case wide the range of each parameter spanned two orders of magnitude this emulates the case where there is no extra information available for the model parameters in the case called alternative both sf and sm ranges are narrow compared to the generic case wide thus emulating the case where the total porosity i e a combined porosity of fractures and matrix has been measured before the use of the model in which case it is assumed that sf would be between 1 and 10 of the total porosity for consistency in every optimization process the initial values of the parameters were set in an alternating manner i e the minimum value of kf and κ and the maximum value of sf and sm were used for practical application however several different sets of initial values should be used to ensure that the global optimum has been approached properly typical results of the calibrations are displayed in table 6 for both large and small pumping rates and for the two cases of the model parameters ranges that were mentioned above the results include both simulation time periods studied i e 1 and 2 days and both models of the present work h2 linearization and confined aquifer equations approach in every cell of the table the value outside parentheses is the optimization criterion c as defined by eq c1 in appendix c and the value inside parentheses is the calibration quality criterion q as defined by eq c2 in appendix c it is stressed here that the values of the table are just indicative of the global optima because the optimization result can t be guaranteed to be a global optimum several runs of each optimization should be performed in every case from table 6 it is clear that both cases of parameters ranges studied give similar results the calibration performance is better for small pumping rates than for large pumping rates and for the h2 linearization model compared to confined aquifer equations the optimization criterion c has similar value for 1 and 2 days simulation time in all cases with the exception of confined aquifer equations for large pumping rates especially for simulation time period of 2 days this is reasonable as the flow field in this case deviates strongly from the assumption of small drawdowns the calibration quality criterion q is better for longer simulation time period when using the h2 linearization model while the opposite holds for confined aquifer equations again this is reasonable as more data points are used for simulation time periods of 2 days and the failure of the small drawdowns hypothesis becomes more evident especially for large pumping rates overall the calibration performance of the h2 linearization model is considered satisfactory 6 summary and conclusions despite that unconfined dp aquifers are important resources of water in many parts of the world research on the efficient simulation of such systems is still rather scarce this study focusses on the development of analytical solutions of flow in unconfined fractured aquifers the mathematical model is based on the equations proposed by moutsopoulos 2021 unlike other existing models kordilla et al 2012 robineau et al 2018 bresinsky et al 2021 based on the richards equation the model considered in this paper contains only a few parameters the solutions are developed for two problems dealing respectively with discharge from an unconfined aquifer to an adjacent stream and water extraction by a constant pumping rate from an infinite aquifer accuracy and correctness of the analytical solutions are assessed by comparisons with previous analytical solutions or numerical solutions obtained by the comsol and the openfoam software the analytical and semi analytical solution issued using h2 linearization technique provides accurate solutions to the model proposed by moutsopoulos 2021 except for the case of large drawdowns in particular it offers for all cases more accurate solutions than the conventional confined aquifer equations approach it is worth noting that despite the limited range of applicability of the analytical solutions in comparison to the numerical solutions analytical solutions are easier to use and can serve as a first assessment tool they are useful in understanding the parametric trends and qualitative features of the flow kacimov 2000 for the problem of discharge to a stream the analytical solutions were used for the computation of the discharge flow rate which can be used for the recession flow analysis and determination of hydrogeological coefficients the solution of the well extraction problem was used for pumping test analysis purposes further potential applications are the computation of contaminant transport characteristics the estimation of well protection zones and the analytical solutions can also serve as a verification tool for more general numerical schemes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we like to thank frédérick delay for useful discussions and moral support we thank also the editor professor simunek the associate editor dr roubinet and the three anonymous reviewers for their useful comments and time dedicated to the improvement of our manuscript appendix a on the use of confined aquifer equations for the simulation of flow processes in unconfined double porosity equations maréchal et al 2004 w11508 sections 4 and 5 suggested that for the case of small drawdowns one may use the equations of confined double porosity equations namely the warren and root solutions similarly önder 1998 also suggested to use for fuas the equations derived for confined conditions for the case of small drawdowns this approach is similar to the one used for simple porosity aquifers e g chang and yeh 2007 assuming homogeneous properties of the aquifer these equations may be written as s f h f t t f 2 h f x 2 t f 2 h f y 2 κ h m h f a1a s m h m t κ h m h f a1b where tf is the transmissivity of the fractures defined by the relation t f k f b where b is the mean saturated thickness introduced already in section 2 and κ is the depth integrated exchange coefficient κ κ b the main difference between the use of the equations above for confined aquifers and unconfined ones case of small drawdowns respectively is the physical significance of the storage coefficients sf and sm in the former case they depend on the compressibility of water and the solid skeleton while for the latter they are roughly equal to the active porosity of the fractures and of the porous matrix in the unconfined aquifer case they are referred to as specific yield and they are some orders of magnitude larger than in the confined aquifer case it is straightforward to demonstrate that the relations above can be deduced from the equations considered in this study i e eqs 1a b of the main text by considering that for small drawdowns and for aquifers with homogeneous properties the following approximations can be used k f x h f h f x t f 2 h f x 2 a2a k f y h f h f y t f 2 h f y 2 a2b which is widely used for single porosity media by further assuming that for small drawdowns the relations h f h m b holds one gets the following relation h f h m 2 b so that κ 2 h m 2 h f 2 κ h m h f a2c introducing eqs a2a c above to eqs 1a b of the main text one recovers eqs a1a b for the case of the problem presented in section 1 eqs a1a b read s f h f t t f 2 h f x 2 κ h m h f a3a s m h m t κ h m h f a3b where the following initial and boundary conditions have to be considered for t 0 0 x h f h m h a4a b for t 0 for x 0 h f h 1 h δ h a4c for x h f h a4d by considering the following relations for the drawdowns s f h h f a5a s m h h m a5b and further inserting the relations above to eq a3a b considering eqs 6c h for non dimensionalization purposes making use of the relations s f s f h and s m s m h inverting the resulting equation into the laplace space using eqs 11a b and solving the resulting odes one gets finally the following relations s f ε p e x p b κ λ p λ p b κ a6a s m ε b κ p b κ λ p e x p b κ λ p λ p b κ a6b where s f and s m are the laplace transforms of s f and s m respectively one can calculate h f and h m by back transforming eqs a6a b into the time domain and using the relations h f 1 s f a7a h m 1 s m a7b for the case of the second problem extraction from a pumping well eqs a1a b read s f h f t t f 2 h f r 2 1 r h f r κ h m h f a8a s m h m t κ h m h f a8b which are subject to the following initial and boundary conditions for t 0 0 r h f h m h a9a b for t 0 as r 0 b h f r q w 2 π r k f a10a for r h f h a10b where as already mentioned in the main text q w is the extracted flowrate from the aquifer by the pumping process by considering eqs 6a h and 27 one can obtain the non dimensional form of the equations above i e the equations for h f and h m taking into account that for the pumping well problem as already mentioned in section 3 one has to use the relation b h considering further eqs a7a b transforming s f and s m into the laplace space and solving the resulting odes one gets the following solutions s f q w 2 π p k o r p λ p κ λ p κ a11a s m κ q w 2 π p λ p κ k o r p λ p κ λ p κ a11b one can back transform the equations above into the time domain and use eqs a7a b to estimate the non dimensional heads appendix b analysis of the data of well ifp in india maheshwaram watershed a potential site to use the procedure presented in section 5 2 is a hardrock aquifer in india maheshwaram watershed especially the aquifer area around well ifp 16 seems to be compatible with our model because there is a free water table there is both a porous matrix and a network of fractures which is dense enough that the continuum hypothesis is valid maréchal et al 2004 unfortunately the piezometer was not installed in a sufficient distance from the pumping well but inside the wellbore so that the assumption of horizontal flow hypothesis h2 in moutsopoulos 2021 is not expected to be valid i e murray and monkmeyer 1973 the most straightforward way to prove this fact is to take into account that as stated in maréchal et al 2004 see their fig 6 in relatively short time which corresponds to the duration of the pumping test say one day the theis equation gives at least a rough approximation of the head in the fractures inside the well i e for r rw 0 085 m one can use the following relation h f r s f r q w 2 π t f r for the groundwater formation around well ifp 16 qw 0 0058 m3 s tf kf h where h is the average saturated thickness i e h 21 38 m and the hydraulic conductivity is equal to kf 6 2 10 5 m s see table 2 in maréchal et al 2004 it follows that h f r r r w 8 41 so that the slope exceeds 800 and the dupuit assumption is certainly not valid inside the well ifp 16 nevertheless we did not bother to use the software mentioned above on the data of ifp 16 after estimating the values of the parameters k f s f s m and κ it turned out that the solutions issued by inverting either eqs 32a b or eqs a11a b did fit very well to the field data as well as the solution of the warren root model used in maréchal et al 2004 and also with the model of lin and yeh 2021 who used a generalized transfer term to simulate the fluid exchange term between the fractures and the porous matrix different values for kf sf sm and κ were obtained for each model although it was obvious that none of them was adequate to simulate the data inside the well ifp 16 this analysis demonstrates that great care has to be applied when interpreting field data from dp aquifers we think further that it will be interesting to put an observation well at a sufficient distance from the well ifp 16 say r 20 m and use the measured values of hf in order to estimate the parameters of the dp model by applying the procedure described in section 5 2 appendix c description of the software for the interpretation of pumping tests in unconfined dp aquifers the optimization software that was used in the present study was the solver of microsoft excel because this software is freely provided with every version of microsoft office and was found to be adequate for this study as the problem studied is highly nonlinear the evolutionary method of solver was adopted this method uses the genetic algorithm approach and is heuristic in nature which means that a global optimum is not guaranteed the optimization criterion used was c1 c i 1 n y i y p i y i 100 n where yi was the result of the comsol simulation at some simulation time i with i running from 1 to n 432 or n 864 for simulation time periods of 1 day and 2 days respectively and yp i was the prediction of the model at this simulation time that is the criterion used was an average percent absolute deviation of all data points minimization of the criterion c was sought and the parameters of the evolutionary method were set to values that would better ensure determination of an optimum as close as possible to the global optimum the convergence parameter i e the maximum percentage difference in objective values for the top 99 of the population that solver would allow in order to stop was set equal to 10 6 default is 10 4 the mutation rate i e the relative frequency with which some member of the population was altered to create a new trial solution was set equal to 0 3 default is 0 075 the population size that is the number of candidate combinations of the decision variables that the evolutionary method should maintain at any given time was set to 200 default is 100 the random seed field was left blank forcing the random number generator to use a different seed for every optimization finally the maximum time without improvement i e the maximum number of seconds during which the method would continue without a meaningful improvement in the objective value of the best solution in the population was set equal to a very large time period of 36 000 s default is 30 s leaving the decision of the method termination to the maximum number of subproblems allowed which was also set to a very large value of 100000 this number of subproblems would correspond to large clock times 5 to 6 h on a typical desktop personal computer with an intel i5 cpu and 8 gb of ram but typically the optimization process reached a value close to global optimum after 10 000 to 20 000 subproblems with these solver parameters the evolutionary optimization obtained characteristics of a monte carlo optimization method which was found to be useful for the problem studied here after completion of the optimization process a criterion of the calibration performance was evaluated as c2 q i 1 4 q i q q i q i 100 4 where qi was the value used in the comsol simulation for each of the 4 parameters kf sf κ sm and qq i was the result of the optimization process for the corresponding parameter accordingly the calibration quality criterion q was an average percent absolute deviation of the 4 model parameters from their values that were used in the comsol simulation 
2647,this paper is concerned with the formulation of an adequate likelihood function in the application of bayesian epistemology to uncertainty quantification of hydrologic models we focus our attention on a special class of likelihood functions hereinafter referred to as distribution adaptive likelihood functions which do not require prior assumptions about the expected distribution of the residuals rather inference takes place over the hypotheses model parameters and space of distribution functions our goals are threefold first we present theory of a revised implementation of the generalized likelihood gl function of schoups and vrugt 2010 wherein residual standardization precedes the treatment of serial correlation this so called gl function enjoys a solid statistical underpinning and guarantees a more robust joint inference of the autoregressive coefficients and residual properties then as secondary goal we present a further generalization of the gl function coined the universal likelihood ul function which extends applicability to highly asymmetrical lepto and platy kurtic residual distributions the ul function builds on the 5 parameter skewed generalized student s t distribution of theodossiou 2015 which makes up a large family of continuous probability distributions including but not limited to symmetric and skewed forms of the generalized normal generalized t laplace normal student s t and cauchy lorentz distributions as our third and last goal we present the use of strictly proper scoring rules to evaluate compare and rank likelihood functions these scoring rules condense the accuracy of a distribution forecast to a single value while retaining attractive statistical properties the gl and ul functions are illustrated using data of a simple autoregressive scheme and benchmarked against the gl function student t likelihood sl of scharnagl et al 2015 and normal likelihood nl for a conceptual hydrologic model using measured streamflow data our results show that i the gl function is superior to the gl function ii the active set of nuisance variables exerts a large control on the performance of the gl sl and ul functions iii the treatment of autocorrelation deteriorates the scoring rules and performance metrics of the forecast distribution iv a leptokurtic distribution is favored for discharge residuals v scoring rules are indispensable in our search for the true forecast distribution and vi the use of multiple strictly proper scoring rules turns the selection of an adequate likelihood function into a multi criteria problem keywords bayesian analysis distribution adaptive likelihood generalized likelihood universal likelihood scoring rules probabilistic forecasts data availability data will be made available on request 1 introduction and scope in the past decades much progress has been made in uncertainty quantification of the parameters state variables and simulated output of dynamic system models in all this work bayes theorem has emerged as a corner stone of modern probability theory hypothesis testing and as working paradigm for the subjectivist approach to epistemology statistics and inductive logic bayesian inference allows for an exact description of parameter uncertainty and other sources of uncertainty by treating the parameters as probabilistic variables with joint posterior probability density function pdf this so called posterior parameter distribution p θ y is the consequence of a prior distribution p θ which encodes all subjective knowledge about the parameters θ θ 1 θ 2 θ d before collection of the data y and a likelihood l θ y which was designated as mathematical quantity by fisher 1934 to quantify our degree of belief confidence in the parameter values θ in light of the observed data y bayes 1763 theorem expresses the mathematical relationship between the prior p θ likelihood l θ y and posterior updated beliefs p θ y of the parameters 1 p θ y p θ l θ y θ p θ l θ y p θ l θ y where the denominator p y θ p θ l θ y acts as a normalizing constant so that the posterior parameter distribution integrates to unity over the prior parameter space θ θ r d this so called model evidence or marginal likelihood plays a crucial role in hypothesis testing when selecting the best model among a collection of competing models volpi et al 2017 but can be ignored during parameter inference as the unnormalized posterior density p θ y p θ l θ y suffices to estimate p θ y the 100 1 α credible region c α θ of the vector valued θ θ will make up a 1 α proportion of the probability mass of p θ y at a significance level α 0 1 credible intervals c α a b of each parameter θ j j 1 2 d may be construed separately 2 a p θ 1 θ 2 θ j θ d y d θ j b p θ 1 θ 2 θ j θ d y d θ j α 2 where marginalization takes places over θ j the parameter space without θ j bayesian credible regions and or intervals are analogous to the confidence regions intervals of generalized least squares gls used in frequentist statistics lee 2012 although subtle but important differences exist in their computation and interpretation lu et al 2012 for a multi modal posterior parameter distribution confidence credible intervals are rather meaningless and we should resort instead to a highest posterior density hpd region a hpd region h α of α significance level is a 1 α region for which holds that p θ y p θ y for all θ h α and θ θ h α where a b x a and x b after we have processed the data y we can evaluate the posterior predictive pdf of the simulated quantity y θ r 3 p y y θ p y θ p θ y d θ where p y θ is the density function of the distribution of y given θ θ this predictive distribution reflects both the aleatory uncertainty in future observations and the posterior uncertainty in the parameters and is directly related to its cdf f y y 4 f y y θ f c y θ d p θ y where f c y θ is the conditional predictive distribution when θ θ equal the true parameter values greenberg 2013 gelman et al 2014 unfortunately the above integrals do not often admit a solution in closed form and thus must be approximated using some form of monte carlo simulation gelfand and smith 1990 gilks et al 1996 the likelihood function exerts a large control on the posterior parameter and predictive distributions p θ y and p y y respectively and has been the subject of much debate in the hydrologic literature beven and binley 1992 freer et al 1996 yang et al 2007 smith et al 2008 smith and marshall 2008 reichert and mieleitner 2009 vrugt et al 2009 schoups and vrugt 2010 smith et al 2010 evin et al 2013 scharnagl et al 2015 smith et al 2015 li et al 2016 mcinerney et al 2019 ammann et al 2019 these contributions may be grouped in two main camps with fundamentally different and opposing viewpoints on how to characterize uncertainty in the presence of epistemic errors and residual non stationarity the first camp believes in the power of statistical theory and treats uncertainty as aleatory in nature proponents of this camp insist that likelihood functions should have a solid theoretical underpinning but despite many efforts to understand and describe epistemic systematic uncertainty are often silent on how one should characterize and treat model structural errors and non stationary residual behavior the residuals display certain patterns and it is in the features of these patterns that we may distill a suitable likelihood function with roots in probability theory this camp of idealists is most likely to include theoreticians and hydrologists with a strong stem education contributions include the use of eq 6 and variations thereof sorooshian and dracup 1980 tasker 1980 kuczera 1982 stedinger and tasker 1985 bates and campbell 2001 smith et al 2010 evin et al 2013 mcinerney et al 2019 the application of advanced distribution adaptive likelihood functions reichert and mieleitner 2009 schoups and vrugt 2010 scharnagl et al 2015 ammann et al 2019 and implementation of set theoretic likelihood functions within the context of approximate bayesian computation sadegh and vrugt 2013 the second camp recognizes the current limitations of formal statistical theory in describing systematic uncertainty and takes on an alternative philosophy and approach to model evaluation this includes the use of so called pseudo likelihood functions within the glue methodology beven and binley 1992 freer et al 1996 beven and freer 2001 smith et al 2008 set theoretic likelihood functions within the context of limits of acceptability beven 2006 vrugt and beven 2018 and voting point likelihoods for rating curve estimation in the presence of aleatory and epistemic uncertainty mcmillan and westerberg 2015 hollaway et al 2018 this camp of pragmatists will include practicing hydrologists who seek ways to formalize their field expertise and knowledge of watershed characteristics and behavior into expectations about model performance in this paper we take a formal probabilistic viewpoint and focus our attention on so called distribution adaptive likelihood functions this special group of likelihood functions does not make prior assumptions about the expected distribution of the residuals rather inference takes place over the model parameters θ and space of distribution functions defined by the shape parameters of the likelihood function we label this group distribution adaptive likelihood functions after distribution free maximum likelihood cosslett 1983 and partially adaptive zeckhauser and thompson 1970 hansen et al 2006 estimation the goals of this paper are threefold first we present theory of a revised implementation of the generalized likelihood gl function of schoups and vrugt 2010 this new formulation referred to as the gl function rectifies a critical deficiency of the gl function pointed out by evin et al 2013 and guarantees a much more robust joint inference of the autoregressive coefficients and distribution of the partial residuals then as secondary goal we present a further generalization of the gl function coined the universal likelihood ul function the ul function builds on the skewed generalized student s t distribution of theodossiou 2015 and extends applicability to a much larger family of continuous probability distributions among which highly asymmetrical lepto and platy kurtic residual distributions this includes but is not limited to symmetric and skewed forms of the generalized normal generalized t mcdonald and newey 1988 laplace laplace 1774 normal student s t student 1908 cauchy lorentz poisson 1824 and uniform distribution finally as our third and last goal we present the use of strictly proper scoring rules to evaluate compare and rank the probabilistic forecasts of the likelihood functions scoring rules are a powerful alternative to more intuitive metrics such as the reliability coefficient of variation and coverage and condense the accuracy of a distribution forecast to a single penalty oriented value while retaining attractive statistical properties alexander et al 2022 the gl and ul functions are tested using synthetic data of a simple autoregressive scheme and benchmarked against the gl function student t likelihood sl of scharnagl et al 2015 and normal likelihood for a conceptual hydrologic model using measured streamflow data the remainder of this paper is organized as follows section 2 provides a problem statement and reviews the definition of the default normal likelihood function section 3 presents theory and a derivation of the gl function in this section we also revisit the skewed student t distribution of scharnagl et al 2015 which serves as central ingredient of the so called student likelihood sl function this is followed in section 4 by a detailed description of the ul function then section 5 presents the scoring rules used to evaluate compare and rank the predictive distributions derived from the different likelihood functions this is followed by the application of the gl and ul functions to two different case studies involving a simple autoregressive scheme and conceptual watershed model we conclude this paper in section 6 with a summary of our main findings to be complete in our reasoning and arguments and convey the subtleties involved in the application evaluation and ranking of distribution adaptive likelihood functions this paper is relatively heavy on statistics this is a necessary means to describing accurately the distribution of the residuals and prerequisite to diagnosing and detecting model structural errors in an effort to enhance our knowledge and understanding of the rainfall discharge relationship 2 problem statement and background let us consider a deterministic system s whose future state behavior and regularities may be described by one or more rules or principles of change which may be spatially coupled and explain its temporal evolution as most real world systems have an intractable complexity our understanding of the dynamics of the system is limited to one or more variables whose values have been measured in situ or sensed remotely over a period of time for simplicity we consider a single variable χ whose behavior is measured and collected in a n 1 vector y y 1 y 2 y n where the tilde operator denotes measured quantities if the real world system s is considered deterministic then we must be able to describe its behavior with explanatory laws for example those most fundamental and celebrated in physics let us suppose that the dynamic model y m θ x r d r n simulates the temporal behavior y y 1 y 2 y n of the variable χ of interest using as input arguments a d vector of unknown invariant system properties θ θ 1 θ 2 θ d with θ r d and array x of constants and input variables required under the supposition or hypothesis that they govern by causality the variable χ using the evolution rules of nature expressed in mathematical form these physical laws are the result of thought experiments mathematical analysis and derivation and laboratory and or field experimentation under idealized external boundary conditions a deterministic or metaphysical model scales up the physical laws a mixture of algebraic ordinary and partial differential equations to the domain of the real world system of interest to warrant spatiotemporal simulation of the governing variables of interest at the desired space time resolution in some cases the evolution rule can be simplified to a mathematical expression van geert 1994 but for most real world systems the dynamics are simply too complex to warrant an exact characterization by one or more mathematical functions the array x characterizes the system s initial state invariant properties and spatiotemporal control inputs forcing explanatory variables as our main focus is on the model parameters θ we suppress the symbol x in our subsequent notation and write instead y m θ for the vector valued form of the model with respect to θ a key task is now to determine suitable values of the parameters θ so that the model output y approximates as closely and consistently as possible the measured response y y 1 y 2 y n of variable χ as the data generating system s is considered deterministic and described by laws of nature there is no randomness in the evolution of future behavior any uncertainty in χ is due to measurement error only an inherent byproduct of the measurement process if we make the common assumption that the measurement errors of χ are random and additive 5 y y ϵ then the n vector y y 1 y 2 y n equals the unobserved true response of the deterministic system s which the model m is supposed to mimic this paper is concerned with a formal probabilistic description of the residuals e θ y m θ so as to help guide the inference to the unobserved true response y to quantify the level of confidence aka likelihood in the simulated outcome y t θ given observation y t we must hypothesize a distribution f y t of the measurement y t as the data generating process s is assumed deterministic there is no randomness in the system state and behavior at any moment in time then eq 5 implies that y t should follow the distribution of the measurement error ϵ t if we make the common and convenient assumption that e t θ is zero mean normally distributed with variance σ e 2 and thus e t θ n 0 σ e 2 then the distribution of the measurement f y t f n y t σ e 2 where f n a b c signifies the pdf of the normal distribution n b c with mean b and variance c evaluated at the simulated outcome a the likelihood of the parameters now equals the density of the normal pdf at the simulated outcome to yield l θ y t σ e 2 f n y t θ y t σ e 2 we can generalize the computation of the likelihood to a n vector of simulated output this leads to the well known gls likelihood function l θ y σ e and equals 6 l θ y σ e 1 2 π n σ e exp 1 2 e θ σ e 1 e θ where σ ϵ is the n n measurement error covariance matrix is the determinant operator n denotes the length of the training data record and the multiplicative term in front of the exponential function is the familiar normalizing constant of a normal distribution the maximum θ of l θ y σ e is also known as the maximum likelihood solution the expectation that the measurement errors of the data generating process will provide an exhaustive description of the residuals is flawed as complex systems do not admit a perfect characterization the residuals e θ y y θ will almost always be larger than the measurement errors ϵ of the measured response y the residuals are simply expected to absorb the consequences of model misspecification and inadequate characterization of the initial states system properties and controlling variables and behave analogously to the measurement errors of y this is equivalent to a residual covariance matrix σ e which is a multiple of the measurement error covariance matrix σ ϵ in the case of a scalar covariance matrix we can write σ e c σ ϵ 2 v where σ e 2 c σ ϵ 2 signifies the unknown variance of the residuals thus in practice we must submit instead to eq 6 the n n covariance matrix of the residuals σ e so that the standardized and decorrelated residuals satisfy the assumptions of a zero mean and constant variance of the gauss markov theorem we can slip into usage of the terminology measurement error but this characterization is imprecise in the present context e g see page 20 of beven 2006 the multivariate form of the gls likelihood in eq 6 permits treatment of residual autocorrelation through the off diagonal terms of the residual covariance matrix σ e for example for a first order autoregressive process e t θ ϕ 1 e t 1 θ ɛ t with zero mean normally distributed partial residuals ɛ t n 0 σ ɛ 2 with variance σ ɛ 2 the residual covariance matrix σ e may be factorized to a product σ e l ϕ 1 u ϕ 1 σ ɛ 2 1 of a lower and upper triangular matrix l ϕ 1 and u ϕ 1 σ ɛ 2 respectively which depend only on the autoregressive coefficient ϕ 1 and or σ ϵ 2 and the determinant σ e σ ϵ 2 n 1 ϕ 1 2 unfortunately not all surmised univariate residual distributions support a closed form multivariate expression for the joint likelihood this is particularly true for the nontraditional residual distributions discussed herein then we must treat serial correlation prior to computation of the marginal and joint likelihoods 3 generalized likelihood function the gls likelihood function has found widespread application and use yet involves the rather questionable assumption that the residuals are normally distributed power transformations such as the box and cox 1964 transform may help stabilize the variance of the data and model simulations and make their respective marginal distributions more normal distribution like yet the power and or shift parameters of this transformation are not unambiguously defined mcinerney et al 2017 and the transformation does not necessarily protect against heavy tailed residuals bates and campbell 2001 yang et al 2007 distribution adaptive likelihood functions such as the gl function of schoups and vrugt 2010 allow us to relax the assumption of normality and describe any arbitrary residual distribution with non constant variance and or different degrees of skew and kurtosis unlike the box cox transformation which simultaneously affects several moments of the residual distribution ammann et al 2019 the gl function separately treats residual heteroscedasticity skewness and kurtosis with parameters that have a direct relation to residual statistics and can be deduced from diagnostic plots schoups and vrugt 2010 this not only lets the measured data y speak for itself but also leads to a more robust characterization of model parameter and predictive uncertainty a skewed residual distribution will for instance improve the probabilistic description of truncated variables say near zero flows in a semiarid watershed schoups and vrugt 2010 in this section we present a revised formulation of the gl function of schoups and vrugt 2010 which enacts treatment of serial correlation on homogenized rather than non homogenized heteroscedastic residuals this revised formulation simplifies the joint inference of the parameters θ and probabilistic properties of the residuals the theoretical derivation below serves as precursor to the ul function which is presented in a subsequent section 3 1 the standardized skew exponential power sep density function the gl function of schoups and vrugt 2010 builds on the standardized skew exponential power sep density function which combines the distribution of subbotin 1923 also known as the exponential power ep or generalized normal distribution box and tiao 1992 7 f ep a 0 1 β ω β exp c β a 2 1 β and the template density f skew a ξ for skewed distributions of fernandez and steel 1998 8 f skew a ξ 2 ξ ξ 1 f a ξ sign a to yield see appendix a 9 f sep a 0 1 β ξ 2 σ ξ ω β ξ ξ 1 exp c β μ ξ σ ξ a ξ sign μ ξ σ ξ a 2 1 β where denotes the absolute value or modulus operator sign x x x is the signum function and the scalars c β ω β μ ξ and σ ξ are a function of the kurtosis β 1 1 and skewness ξ r using eqs a 2a a 2b a 7a and a 7b respectively appendix a completes the derivation of the sep density in schoups and vrugt 2010 with a detailed mathematical treatment of the mean μ ξ and variance σ ξ 2 of the sep distribution to provide insights into the functional shape of the standardized sep density of eq 9 please consider fig 1 which presents a graph of f sep a 0 1 β ξ for a 3 3 using different values of the a kurtosis β and b skewness ξ as is evident from the two graphs the density of the standardized sep distribution at point a is controlled by the values of β and ξ the density is symmetric for ξ 1 positively skewed for ξ 1 and negatively skewed for ξ 1 for a symmetric density that is ξ 1 a value of β 1 results in a uniform distribution β 0 produces a normal distribution and β 1 equals a double exponential or laplace distribution thus a value of β 0 1 produces a sep distribution with much heavier tails than the normal density such leptokurtic distributions help protect the model parameter estimates from outliers 3 2 treatment of residual autocorrelation the standardized sep density of eq 9 enables a fluent description of the distribution of the residuals before we can proceed however with the derivation of the sep likelihood function we are in need of an adequate description of the temporal structure of the raw residuals e θ schoups and vrugt 2010 expressed serial correlation as an ar k process of the raw residuals e θ as follows 10 e t θ j 1 k ϕ j e t j θ σ e t ɛ t where φ k ϕ 1 ϕ 2 ϕ k is a k vector of autoregressive coefficients σ e t signifies the standard deviation of the t th raw residual ɛ t θ δ denotes the ar k decorrelated residual and δ β ξ φ k is a vector of nuisance variables 1 1 a nuisance variable is fundamental to the probabilistic model of concern which is not of immediate interest but must be accounted for in the inference the entries of the n 1 vector ɛ θ δ ɛ 1 θ δ ɛ 2 θ δ ɛ n θ δ are also referred to as innovations or partial residuals the autoregressive operator in eq 10 operates on the non standardized raw residuals e θ this implementation is not recommended as the autoregressive operator of eq 10 expects use of homogenized residuals evin et al 2013 convincingly demonstrated that residual standardization should precede the treatment of serial correlation this guarantees a more robust inference of the model parameters and or nuisance variables and consequently may lead to sharper prediction intervals therefore and as in steinschneider et al 2015 and hernández lópez and francés 2017 we admit standardized raw residuals e t θ δ with unknown mean and variance σ e 2 instead 11 e t θ δ e t θ σ ϵ t t 1 2 n and conveniently limit our mathematical description of their structural dependence to an ar 2 process 12 e t θ δ ϕ 1 e t 1 θ δ ϕ 2 e t 2 θ δ ɛ t where the partial residuals ɛ t θ δ sep 0 σ ɛ 2 β ξ are assumed to be independent and zero mean sep distributed with variance σ ɛ 2 skewness β and kurtosis ξ for all t 3 4 n and the single line underneath the raw residuals articulates their standardized dimensionless counterparts e θ δ e 1 θ δ e 2 θ δ e n θ δ note that the standardized raw residuals support the use of more elaborate ar models for example with seasonally varying coefficients or ar coefficients that depend on exogenous variables ammann et al 2019 this is not uncommon in statistical hydrology and could be advised if model performance depends on time season and or the governing processes the ar 2 process of eq 12 should typically suffice in removing serial dependencies of the standardized raw residuals otherwise one could always implement a higher order ar k process with k 2 five remarks are in order first the recursion in eq 12 demands specification of so called initial conditions e 1 θ δ and or e 0 θ δ for computation of the first two partial residuals ɛ 1 θ δ and ɛ 2 θ δ these initial conditions not only determine the values of ɛ 1 θ δ and ɛ 2 θ δ but also their respective marginal distributions a second and related comment the moments of the sep innovations ɛ t θ δ are not invariant under the transformation of eq 12 thus the standardized raw residuals e t θ δ do not inherit the variance σ ɛ 2 skewness β and kurtosis ξ of the partial residuals a change of variance thus σ e 2 σ ɛ 2 is even expected with normally distributed innovations but the variant skew and kurtosis are the byproduct of the use of non gaussian innovations third eq 11 does not guarantee a zero mean and unit variance of the standardized raw residuals in fact the variance of the standardized raw residuals will almost always be larger than one as the magnitude of the e t θ δ s will exceed on average the measurement error standard deviations σ ϵ t of the training data y t for t 1 2 n we can inflate the σ ϵ t s so as to absorb all other sources of systematic and aleatory uncertainty and impose a unit variance of the standardized raw residuals σ e 2 1 in this case we should replace the σ ϵ t s with σ e t the standard deviation of the t th residual fourth it is quite common to replace the denominator of eq 11 with estimates s ϵ t of the measurement error standard deviation σ ϵ t in doing so the e t θ δ s in eq 11 should be referred to as studentized residuals 2 2 in statistics a studentized residual is equal to a residual divided by an estimate of its standard deviation if instead we divide by the population standard deviation then we speak of a standardized residual we will use the terminology studentized residual throughout the remainder of this paper as this characterizes better the application of eq 11 lastly the use of s ϵ t rather than σ ϵ t in the denominator of eq 11 associates the nuisance variables δ to the studentized raw residuals in a manner that will become explicit in section 3 4 therefore we write e θ δ e 1 θ δ e 2 θ δ e n θ δ for the studentized raw residuals next we must establish a relationship between the variance σ ɛ 2 of the innovations ɛ t θ δ and the unconditional or marginal variance σ e 2 of the studentized raw residuals the derivation of this relationship can be found in statistical text books and relies on variance decomposition of eq 12 to yield 13 σ ɛ 2 1 ϕ 2 1 ϕ 1 ϕ 2 1 ϕ 1 ϕ 2 1 ϕ 2 σ e 2 the quotient on the right hand side is on the unit interval and thus we yield that σ e 2 σ ɛ 2 unless ϕ 1 0 and ϕ 2 0 then σ ɛ 2 σ e 2 if ϕ 2 0 the above expression reduces to σ ɛ 2 1 ϕ 1 2 σ e 2 the familiar expression for the relationship between the variance of an ar 1 process σ e 2 and the variance σ ɛ 2 of the partial residuals 3 3 derivation of the sep likelihood function if we consider the values of the s nuisance variables δ β ξ φ 2 to be known a priori then the joint likelihood l θ y δ of the d model parameters θ θ 1 θ 2 θ d 14 l θ y δ p y δ θ p e δ θ equals the joint pdf of the training data y δ and θ to compute the likelihood of the n vector of raw residuals e θ we must make assumptions about the so called initial conditions of the ar 2 model in eq 12 the initial conditions e 1 θ y 1 y 1 θ and e 0 θ y 0 y 0 θ not only determine after studentization with eq 11 the values of the first two partial residuals ɛ 1 θ δ and ɛ 2 θ δ but also control the next successive entries of ɛ θ δ via the recursion in eq 12 if the measured data y 1 and y 0 and model output y 1 θ and y 0 θ are known at t 1 and t 0 immediately preceding the first entry of the training data record then the so called initial conditions equal e 1 y 1 y 1 θ and e 0 y 0 y 0 θ otherwise the user can fix the raw residuals at some default value say e 1 θ 0 and e 0 θ 0 in this latter case we do not adapt our notation and carry forward the dependence of the initial condition on the assumed parameter values the joint distribution of the raw residuals p e δ θ satisfies the following identity 15 p e θ p e θ δ e θ δ e θ where p e θ δ is the joint distribution of the studentized raw residuals and e θ δ e θ signifies the jacobian of the transformation from the studentized raw residuals e θ δ to the raw residuals e θ the joint distribution of the studentized raw residuals p e θ δ equals the product of the marginal densities p e 1 e 0 e 1 θ δ and p e 2 e 1 e 0 θ δ of e 1 θ δ and e 2 θ δ respectively and the conditional densities p e t e t 1 e t 2 θ δ of the remaining n 2 entries e 3 θ δ e n θ δ of e θ δ to yield 16 p e θ δ p e 1 e 0 e 1 θ δ p e 2 e 1 e 0 θ δ p e 3 e n e 0 e 1 θ δ p e 1 e 0 e 1 θ δ p e 2 e 1 e 0 θ δ t 3 n p e t e t 1 e 0 e 1 θ δ p e 1 e 0 e 1 θ δ p e 2 e 1 e 0 θ δ t 3 n p e t e t 1 e t 2 θ δ if we make the convenient assumption that the partial residuals ɛ θ δ follow a normal distribution with zero mean and variance σ ɛ 2 then the marginal distributions of e 1 θ δ and e 2 θ δ are gaussian as well which supports an exact closed form expression for the likelihood function of the raw residuals yet sep innovations ɛ θ δ sep 0 σ ɛ 2 β ξ do not admit a closed form description for p e 1 e 0 e 1 θ δ and p e 2 e 1 e 0 θ δ as the ar 2 process of eq 12 does not preserve higher order moments of the sep distribution such as its skew and kurtosis damsleth and el shaarawi 1989 hürlimann 2012 we may resort to the law of total cumulance to determine the expected skew and kurtosis of e 1 θ δ and e 2 θ δ yet such estimates must be turned into values of β and ξ to warrant a closed form description of p e 1 e 0 e 1 θ δ and p e 2 e 1 e 0 θ δ respectively henceforth we follow schoups and vrugt 2010 and condition on the unobserved residuals e 1 θ and e 0 θ to yield 17 p e 1 e 0 e 1 θ δ p e 2 e 1 e 0 θ δ p e 1 e 2 e 1 e 0 θ δ t 1 2 p e t e t 1 e t 2 θ δ this approximation is valid only for moderate to large training data records y if we now substitute eq 17 into 16 and enter the resulting expression into eq 15 we yield the following expression for the conditional likelihood function 18 l θ y δ e θ δ e θ t 1 n p e t e t 1 e t 2 θ δ t 1 n 1 σ ϵ t p e t e t 1 e t 2 θ δ according to the ar 2 process of eq 12 the conditional distribution of e t θ δ given e t 1 θ δ and e t 1 θ δ equals the marginal distribution of the partial residuals ɛ t θ δ for all t 1 2 n hence we can now write 19 l θ y δ t 1 n 1 σ ϵ t f sep ɛ t 0 σ ɛ 2 β ξ this formulation poses computational difficulties as we do not have available a mathematical expression for the non standardized sep distribution with variance σ ɛ 2 1 fortunately we can take advantage of the following identity 20 f sep a 0 σ a 2 β ξ 1 σ a f sep a 0 1 β ξ and thus the non standardized sep density f sep a 0 σ a 2 β ξ at point a with var a σ a 2 is equivalent to a multiple σ a 1 of the standardized sep density f sep a 0 1 β ξ evaluated at the standardized a value a a σ a if we enter eq 20 into eq 19 then we end up with the following general formulation for the sep likelihood function of the raw residuals 21 l θ y δ t 1 n 1 σ ϵ t 1 σ ɛ f sep ɛ t 0 1 β ξ using standardized partial residuals ɛ t ɛ t θ δ σ ɛ for all t 1 2 n we purposely use the terminology standardized partial residual as the denominator σ ɛ equals the theoretical standard deviation of the ar 2 innovations in eq 13 if we admit the standardized sep density f sep 0 1 β ξ of eq 9 to eq 21 then we arrive at the sep likelihood function 22 l θ y β ξ φ 2 σ ϵ 2 t 1 n 1 σ ϵ t 1 σ ɛ 2 σ ξ ω β ξ ξ 1 exp c β μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 1 β where c β ω β μ ξ and σ ξ are a function of the kurtosis β and skewness ξ according to eqs a 2a a 2b a 7a and a 7b respectively ɛ t θ δ signifies the t th standardized partial residual σ ɛ 2 is the variance of the ar 2 process of eq 12 and δ β ξ φ 2 it is important to keep in mind that the sep likelihood function is not exact but conditional upon the initial conditions e 1 θ and e 0 θ eq 22 but in modified form was coined the generalized likelihood gl function by schoups and vrugt 2010 as it extends applicability to situations wherein the raw residuals e θ deviate from normality and exhibit skew kurtosis heteroscedasticity and possibly serial correlation at one or more lags if as in eq 10 of schoups and vrugt 2010 the treatment of serial correlation precedes residual studentization then the multiplicative term 1 σ ɛ will disappear from eq 22 this original implementation is herein referred to as the gl function and the revised formulation of the sep likelihood function in eq 22 is coined the gl function finite multiplication of the n likelihoods in eq 22 is susceptible to arithmetic underflow resulting in a zero value of l θ y β ξ φ 2 σ ϵ 2 for reasons of numerical stability it is therefore more convenient to work with the sep log likelihood function l θ y β ξ φ 2 σ ϵ 2 instead 23 l θ y β ξ φ 2 σ ϵ 2 n log 2 σ ξ ω β n log ξ ξ 1 1 2 t 1 n log σ ϵ t 2 n 2 log σ ɛ 2 c β t 1 n μ ξ σ ξ ϵ t θ δ ξ sign μ ξ σ ξ ϵ t θ δ 2 1 β where σ ɛ 2 is equal to eq 13 for the ar 2 process of the standardized raw residuals 3 4 treatment of nuisance variables and measurement data errors the sep log likelihood function l θ y β ξ φ 2 σ ϵ 2 above assumes prior knowledge of the s 4 nuisance variables δ β ξ φ 2 and the n vector of measurement error variances σ ϵ 2 σ ϵ 1 2 σ ϵ 2 2 σ ϵ n 2 of the training data y y 1 y 2 y n in the absence of detailed knowledge about the expected distribution of the residuals and or measurement errors of y we can choose to estimate the nuisance variables δ along with the model parameters θ this approach does not change anything to the mathematical formulation of the gl function in eq 23 except the nuisance variables migrate to the left side of the vertical bar in the parent definition of the sep log likelihood function to read instead l θ β ξ φ 2 y σ ϵ 2 or l θ δ y σ ϵ 2 this alternative definition frees us from having to make strong and often questionable prior assumptions about the distribution of the residuals the nuisance variables let the residuals speak for themselves note that nuisance variables are also referred to as hyperparameters when they relate to the prior distribution of the model parameters we can further generalize the sep log likelihood function of eq 23 to situations without knowledge of the training data measurement errors σ ϵ 2 a pragmatic remedy is to relate the measurement error to the simulated data schoups and vrugt 2010 evin et al 2013 as follows 24 s ϵ t s 0 s 1 y t θ where s ϵ t is an estimate of the measurement error standard deviation of the t th simulated output y t θ and s 0 0 and s 1 0 1 are unknown coefficients that define the intercept and slope of the measurement error function respectively with s 1 0 the expression above reduces to a homoscedastic measurement error function with variance equal to s 0 2 in the original gl function of schoups and vrugt 2010 the slope and intercept of eq 24 were treated as two separate nuisance variables each having their own prior distribution this implementation would be fine with the application of eqs 12 and 13 so that the variance of the partial residuals σ ɛ 2 is directly proportional to the variance of the standardized raw residuals σ e 2 in the gl function we simplified the implementation to include only a single nuisance variable s 0 the slope s 1 is evoked as a phantom variable to enforce a unit variance σ e 2 1 of the studentized raw residuals e θ δ if m e and s e 2 denote the sample mean and sample variance of the studentized raw residuals respectively 25 m e 1 n t 1 n e t θ δ and s e 2 1 n 1 t 1 n e t θ δ m e 2 then the zero point of the so called residual function ħ s 1 θ δ 1 s e 2 equals the value of s 1 for which s e 2 1 the secant method can efficiently solve this root finding problem in only a handful of iterations thus for a given value of the intercept s 0 of eq 24 the secant method finds the value of the slope s 1 so that σ e 2 1 this approach increases the number of nuisance variables with only one namely δ β ξ φ 2 s 0 and the sep log likelihood function becomes 26 l θ β ξ φ 2 s 0 y n log 2 σ ξ ω β n log ξ ξ 1 t 1 n log s 0 s 1 y t θ n 2 log σ ɛ 2 c β t 1 n μ ξ σ ξ ϵ t θ δ ξ sign μ ξ σ ξ ϵ t θ δ 2 1 β as our result will demonstrate our definition of s 1 as phantom variable rather than nuisance variable is inconsequential both methods produce equivalent results the root finding procedure is not limited to measurement errors with a non constant variance but could in principle also work for homoscedastic measurement errors the value of s 0 will then determine whether the zero point of ħ s 1 θ δ is feasible or not a simpler solution in this case that avoids using any nuisance variable for the measurement error function of eq 24 after all is to set s 0 equal to the sample standard deviation of the raw residuals that is s 0 s e 2 the inference of the slope and intercept of the measurement error function is however not without problems if the model is not able to describe sufficiently closely the observed data y then joint inference of s 0 and ϕ 1 s 1 is conditioned on s 0 via the constraint that σ e 2 1 may result in unrealistically large values of s 0 and or s 1 while simultaneously promoting ϕ 1 1 so as to downplay the role of the model m θ in describing the measured data and maximize the sep log likelihood of eq 26 unfortunately there is nothing we can do about this if the raw residuals e θ are large in magnitude then s 0 and or s 1 must be large as well so as to honor the unit variance of the studentized raw residuals e θ δ instead we must find ways to remove model bias and improve the overall model data consistency forcing data errors will certainly contribute significantly to the model bias and simulation uncertainty table 1 summarizes the nuisance variables of the gl function and lists their lower and upper bounds and default values if the measurement error variances σ ϵ 2 are known a priori then the sep log likelihood function with ar 2 description of the standardized raw residuals l θ β ξ φ 2 y σ ϵ 2 is equal to eq 23 with φ 2 ϕ 1 ϕ 2 and σ ɛ 2 equal to eq 13 if the n measurement error variances σ ϵ 2 of the training data record y are unknown then we can resort to eq 26 this formulation l θ β ξ φ 2 s 0 y requires specification of the measurement error intercept s 0 in the case of heteroscedastic errors this increases the number of nuisance variables to five δ β ξ ϕ 1 ϕ 2 s 0 if an ar 1 is assumed instead then ϕ 2 0 and σ ɛ 2 1 ϕ 1 2 in eqs 23 and 26 3 5 special cases of the sep log likelihood function if so desired we can fix the skew β and kurtosis ξ of the sep log likelihood function to some default values to satisfy prior assumptions for β 0 and ξ 1 eq 26 simplifies to the well known gaussian log likelihood l θ φ 2 s 0 y 0 1 of an ar 2 process indeed without skew and excess kurtosis eqs a 2a a 2b a 7a and a 7b result in c β 1 2 ω β 2 π 1 2 μ ξ 0 and σ ξ 1 if we enter these values into the sep log likelihood function we yield 27 l θ φ 2 s 0 y β 0 ξ 1 n 2 log 2 π t 1 n log s 0 s 1 y t θ n 2 log σ ɛ 2 1 2 t 1 n ϵ t θ δ 2 which is equal to a conditional gaussian log likelihood function with constant and or nonconstant measurement errors and ar 2 process of the studentized raw residuals this expression follows directly from the gls likelihood function of eq 6 for ϕ 1 0 and ϕ 2 0 the above expression simplifies further to a normal log likelihood function conditional upon the unobserved raw residuals e 1 θ and e 0 θ and non constant variance of the raw residuals in the remainder of this paper the normal log likelihood of eq 27 will be referred to as nl function for β 1 and ξ 1 the sep log likelihood function simplifies to 28 l θ φ 2 s 0 y β 1 ξ 1 n 2 log 2 t 1 n log s 0 s 1 y t θ n 2 log σ ɛ 2 2 t 1 n ϵ t θ δ which is equal to a laplacian log likelihood l θ φ 2 s 0 y β 1 ξ 1 of an ar 2 process this concludes our description of the gl function 3 6 the skewed student s t likelihood function the gl and gl functions rely on the generalized normal or ep distribution to better characterize nontraditional residual distributions with skew kurtosis and heavier tails than the normal distribution scharnagl et al 2015 replaced the ep distribution of eq 7 with a standardized student s t density 29 f st a 0 1 ν γ ν 1 2 γ ν 2 1 π ν 2 1 a 2 ν 2 ν 1 2 where ν 2 denotes the degrees of freedom if we combine eq 29 with the skew density of eq 8 then we yield the following expression for the standardized skewed student s t sst density scharnagl et al 2015 30 f sst a 0 1 ν ξ 2 σ ξ ξ ξ 1 γ ν 1 2 γ ν 2 π ν 2 1 1 ν 2 μ ξ σ ξ a ξ sign μ ξ σ ξ a 2 ν 1 2 with skewness parameter ξ 0 and shift and scale constants μ ξ and σ ξ 2 respectively that depend on ν and ξ and standardize the sst density scharnagl et al 2015 closed form expressions for μ ξ and σ ξ 2 are presented in eqs b 7a and b 7b of appendix b fig 2 plots the density of the standardized skewed student t distribution of eq 30 for a 3 1 2 3 1 2 using different values of the a skewness ξ and b degrees of freedom ν the sst density is positively skewed for ξ 1 and negatively skewed for ξ 1 for ξ 1 the student s t distribution is symmetric and bell shaped albeit with much heavier tails than the normal distribution for small values of ν these so called leptokurtic distributions are more peaked than a normal distribution and have stronger tails for large values of ν the student s t distribution converges to the standard normal distribution zero mean and unit standard deviation thus the value of ν 2 determines the kurtosis and heaviness of the tails of the sst distribution if we assume an ar 2 process of the standardized raw residuals and treat measurement data errors as in section 3 4 then we yield the log likelihood function l θ s 0 ν ξ σ ɛ 2 y of the sst density of scharnagl et al 2015 see appendix b 31 l θ s 0 ν ξ φ 2 y n 2 log σ ɛ 2 t 1 n log s 0 s 1 y t θ n log 2 n log σ ξ n log γ ν 1 2 n log ξ ξ 1 n log γ ν 2 n 2 log π n 2 log ν 2 ν 1 2 t 1 n log 1 1 ν 2 μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 where s 1 equals the zero point of the residual function ħ s 1 θ δ 1 s e 2 so as to enforce a unit variance of the studentized raw residuals the sst log likelihood is referred to as student likelihood or sl function in the remainder of this paper table 2 summarizes the nuisance variables of the sl function the default value of the degrees of freedom ν is set equal to the sample size n minus the number d of estimated parameters θ note that for ξ 1 and ν the sl function reduces to the nl function of eq 27 4 the universal likelihood function the skewed generalized student s t sgt distribution of theodossiou 1998 is a further generalization of the sst distribution in eq 30 to a family of continuous probability distributions which includes the generalized student s t mcdonald and newey 1988 hansen 1994 error theodossiou 2015 and exponential power distributions subbotin 1923 box and tiao 1992 the skewed and symmetric laplace cauchy student s t and normal distributions see e g johnson et al 1995 and the uniform distribution the pdf of the non standardized sgt distribution is given by theodossiou 2015 32 f sgt a μ σ λ p q p 2 κ λ p q σ b 1 p q p 1 a μ μ λ p q κ λ p q σ 1 λ sign a μ μ λ p q p q 1 p where μ σ 0 and λ 1 1 are location scale and skewness parameters p 0 and q 0 control the kurtosis of the distribution μ λ p q and κ λ p q are shift and scale constants respectively that depend on λ p and q and b a b is the so called beta function or euler integral of the first kind 33 b a b 0 1 x a 1 1 x b 1 d x and is equivalent to the following expression 34 b a b γ a γ b γ a b using the gamma function γ in eq a 3 the scalars μ λ p q and κ λ p q negate changes in the mean μ and variance σ 2 of the sgt distribution imposed by the variables p q and λ in the original parameterization of theodossiou 1998 the shift and scale constants are defined as given in eqs 35a and 35b see box i and yield a sgt distribution with mean μ and variance σ 2 if p q 2 if we admit μ 0 and σ 2 1 then we yield the standardized sgt density 36 f sgt a 0 1 λ p q p 2 κ λ p q b 1 p q p 1 a μ λ p q κ λ p q 1 λ sign a μ λ p q p q 1 p the sgt distribution of eq 36 with the closed form expressions for μ λ p q and κ λ p q in eqs 35a and 35b nests a number of commonly used univariate probability distributions including but not limited to the generalized student s t distribution mcdonald and newey 1988 the generalized error distribution 3 3 also known as the generalized normal distribution or ep distribution and the symmetric and skewed student s t laplace cauchy lorentz and normal distributions see table 3 and appendix c to provide insights into the functional form of eq 36 fig 3 displays the standardized sgt density f sgt a 0 1 λ p q for 3 1 2 a 3 1 2 for different values of the a kurtosis p 0 b skewness 1 λ 1 c kurtosis q 0 and d p β and q the sgt distribution can be symmetric and highly skewed for λ 1 0 the distribution is negatively skewed to the left and for λ 0 1 the sgt distribution is positively skewed to the right for λ 0 the distribution is symmetric the parameters p 0 and q 2 control the kurtosis of the sgt distribution the smaller the values of p and q the more slender the sgt distribution will be and thus the larger its kurtosis a distribution with kurtosis that exceeds the value of three is also called leptokurtic and will have fatter tails than a normal distribution for large values of p and q the excess kurtosis of the sgt distribution disappears in lieu of a much more uniform distribution of the probability density the parameter p exerts the largest control on the kurtosis as is evident by the large variation in sgt densities for different values of p the impact of q on the sgt distribution is more subtle and its impact decreases substantially with large values of q altogether small values of p and or q produce a sgt distribution with much heavier tails than the normal density the bottom panel demonstrates the ability of the standardized sgt density to describe a wide variety of distributions the graph plots four different parameterizations color red of the standardized sgt distribution note that in each graph the mean of the sgt distribution equals zero and more difficult to see the variance equals one the bottom right graph presents eight of the special parameterizations of the sgt distribution listed in table 3 among which the ep distribution of eq 7 in red the ability of the sgt distribution to morph into a wide variety of well known probability distributions has desirable advantages for residual characterization and description henceforth the forthcoming likelihood function stemming from the sgt distribution is referred to as the universal likelihood function 4 1 derivation of sgt log likelihood function if we treat the sgt parameters λ p and q as nuisance variables and assume further that the studentized raw residuals e θ δ exhibit serial correlation according to the ar 2 scheme of eqs 12 then the sgt likelihood should follow our derivation of the sep likelihood function specifically according to eq 18 we may write 37 l θ λ p q φ 2 y σ ϵ 2 t 1 n 1 σ ϵ t p e t e t 1 e t 2 θ δ and thus we yield 38 l θ λ p q φ 2 y σ ϵ 2 t 1 n 1 σ ϵ t f sgt ɛ t θ δ 0 σ ɛ 2 λ p q t 1 n 1 σ ϵ t 1 σ ɛ f sgt ɛ t θ δ 0 1 λ p q σ ɛ n t 1 n p 2 σ ϵ t κ λ p q b 1 p q p 1 ɛ t θ δ μ λ p q κ λ p q 1 λ sign ɛ t θ δ μ λ p q p q 1 p where f sgt ɛ t θ δ 0 1 λ p q returns the density at ɛ t θ δ of the standardized sgt distribution in eq 36 ɛ θ δ ɛ 1 θ δ ɛ 2 θ δ ɛ n θ δ denotes the n vector of standardized partial residuals and σ ɛ 2 is the variance of the partial residuals ɛ θ δ ɛ 1 θ δ ɛ 2 θ δ ɛ n θ δ computed using eq 13 if we take the natural logarithm of eq 38 then we yield the sgt log likelihood function 39 l θ λ p q φ 2 y σ ϵ 2 n 2 log σ ɛ 2 t 1 n log σ ϵ t n log p n log 2 n log κ λ p q n log b 1 p q p q 1 p t 1 n log 1 ɛ t θ δ μ λ p q κ λ p q 1 λ sign ɛ t θ δ μ λ p q p where the natural logarithm of the beta function b a b follows from eq 34 40 log b a b log γ a log γ b log γ a b the sgt log likelihood functions in eqs 38 and 39 assume prior knowledge of the measurement error variances σ ϵ 2 of the training data record y if the n entries of σ ϵ 2 are unknown then we can resort to the measurement error function of eq 24 and treat the intercept s 0 as an additional nuisance variable then the sgt log likelihood function becomes 41 l θ s 0 λ p q φ 2 y n 2 log σ ɛ 2 t 1 n log s 0 s 1 y t θ n log p n log 2 n log κ λ p q n log b 1 p q p q 1 p t 1 n log 1 ɛ t θ δ μ λ p q κ λ p q 1 λ sign ɛ t θ δ μ λ p q p where s 1 is computed using root finding of the residual function ħ s 1 θ δ 1 s e 2 with newton s method for the ar 2 process of the raw studentized residuals in eq 12 the variance σ ɛ 2 of the sgt innovations equals eq 13 if the second autoregressive coefficient ϕ 2 is fixed at zero then the variance of the partial residuals ɛ θ δ simplifies to σ ɛ 2 σ e 2 1 ϕ 1 2 the variance of the innovations of an ar 1 process in the remainder of this paper we refer to eq 41 as the universal likelihood ul function table 4 summarizes the nuisance variables of the ul function including symbol units prior ranges and default values three remarks are in order first the product p q of the kurtosis parameters must always exceed two otherwise the scale constant κ λ p q is not defined second the upper bounds of p and q are arbitrary they should just be set large enough so as to benefit from the versatility of the sgt distribution third the default values of λ 0 p 2 and q 1 0 10 imply a normal distribution see table 3 4 2 special and or limiting cases of the sgt log likelihood function the two formulations of the ul function l θ λ p q φ 2 y σ ϵ 2 and l θ s 0 λ p q φ 2 y assume joint inference of the skewness λ and kurtosis p and q of the partial residuals and the model parameters θ if the partial residuals ɛ θ δ should follow some known distribution then we can set one or more nuisance variables at a default value for example if the ɛ t θ δ s are expected to follow a student s t distribution then one can fix λ 0 and p 2 see fig 3d and table 3 and infer the remaining nuisance variables q s 0 and or ϕ 1 and or ϕ 2 of the sgt log likelihood function with the d model parameters θ note that in this case the shift constant μ λ p q in eq 35a equals μ 02 q 0 the scale constant κ 02 q q 2 and the sgt log likelihood function of eq 41 reduces to the sst log likelihood of eq 31 with ξ 1 this is only one of many special cases of the sgt function as discussed in appendix c the more common case of normally distributed partial residuals deserves specific attention in appendix d in summary the sgt distribution is the key ingredient of a family of likelihood functions which includes the normal likelihood function thus the ul function makes obsolete other commonly used likelihood functions in dream suite 5 case studies in this section we illustrate the application of the gl gl sl and ul functions to two case studies involving a simple autoregressive scheme and a conceptual watershed model we first detail the methodological steps and then present the individual studies and their findings 5 1 posterior approximation unless we use a conjugate prior the gl gl sl and ul functions l θ δ y do not admit a closed form solution for the posterior parameter distribution p θ δ y p θ δ l θ δ y we therefore resort to numerical integration gelfand and smith 1990 gilks et al 1996 and infer the joint posterior distribution of the d model parameters θ and s nuisance variables δ using markov chain monte carlo simulation with dream suite a windows based implementation of the differential evolution adaptive metropolis algorithm vrugt et al 2008 2009 vrugt 2016 and variants thereof vrugt and ter braak 2011 laloy and vrugt 2012 vrugt and sadegh 2013a sadegh et al 2015 vrugt and beven 2018 the transition kernel of dream creates multiple different sequences chains of parameter vectors that are stationary and ergodic and have as joint distribution p θ δ y ter braak 2006 vrugt et al 2009 convergence of the joint chains to a stationary distribution is monitored using several different diagnostic measures as advocated by cowles and carlin 1996 this includes single chain and multi chain diagnostics among which the convergence metrics of raftery and lewis 1992 and geweke 1992 and the univariate and multivariate scale reduction factors r d and r of gelman and rubin 1992 and brooks and gelman 1998 respectively the m posterior vectors of the joint markov chains are stored column wise in a d s m matrix θ δ 5 2 performance of optimal parameter values the maximum a posteriori map parameter values θ δ equal the columns of θ δ 4 4 the semicolon implies vertical concatenation thus matrix δ is concatenated to the bottom of matrix θ which maximize the unnormalized posterior density p θ δ y the performance of the map point predictions may be quantified using different summary measures also known as scoring functions we use the root mean square error 42 rmse 1 n t 1 n y t y t θ 2 and percentage bias 43 pbias 100 t 1 n y t θ y t t 1 n y t furthermore we also perform diagnostic checks of the residuals of the map solution specifically we investigate the homoscedasticity distribution autocorrelation function and quantiles of the standardized partial residuals ɛ θ δ of the map solution the sample autocorrelation ρ ɛ k of two standardized partial residuals ɛ i θ δ and ɛ j θ δ a distance time k i j apart may be computed using 44 ρ ɛ k cov ɛ i θ δ ɛ j θ δ var ɛ i θ δ 1 n k i k 1 n ɛ i θ δ m ɛ ɛ i k θ δ m ɛ 1 n k i k 1 n ɛ i θ δ m ɛ 2 where m ɛ 1 n t 1 n ɛ t θ δ is the unitless mean of the n record of standardized partial residuals and k 1 2 n 1 5 3 parameter and predictive uncertainty parameter uncertainty is computed following eq 2 by truncating α 2 of the left and right tails of the posterior samples θ δ to yield 100 1 α credible intervals c α a b of the parameters and nuisance variables stationarity and ergodicity of the sampled chains of dream suite imply that the predictions y 1 y 2 y m of the m posterior parameter vectors θ δ have as invariant distribution the posterior predictive pdf p y y and cdf f y y in eqs 3 and 4 respectively the predictive distribution p y y of eq 3 can be obtained by marginalizing each model prediction over the posterior distribution a detailed algorithmic recipe on how to do this is provided in appendix e using the posterior realizations of the gl function this supplement also returns predictive percentiles y t α 2 and y t 1 α 2 which together define the 100 1 α prediction interval of p y t y where t 1 2 n 5 4 comparison and ranking of predictive distributions we use so called scoring rules to evaluate the quality of the predictive distributions derived from the different likelihood functions scoring rules condense the accuracy of a distribution forecast to a single penalty oriented value while retaining attractive statistical properties alexander et al 2022 if ω r denotes the set of possible values of the quantity of interest y and f is a convex class of probability distributions on ω then a scoring rule s f y is a function 45 s f ω r that assigns numerical values to pairs of forecasts f f and observations y ω 5 5 we use the terminology forecasts for the residual corrupted model output y j y j θ e j δ r n m of the posterior realizations j 1 2 m strictly speaking these outcomes are not forecasts as the simulated output y j θ assumes knowledge of the exogenous variables based on early recommendations by brier 1950 and shuford et al 1966 we restrict attention to proper scoring rules the statistical implications of which have been discussed in the literature by gneiting and raftery 2007 a scoring rule is proper relative to f if the expected score 46 s f g ω s f y d g y is minimized for f g and thus s g g s f g for all probability distributions f g f krüger et al 2021 a score rule is strictly proper relative to the class f if the above holds with equality only if f g lerch et al 2017 such strictly proper scoring rules have an important advantage over proper scoring rules in statistical jargon a strictly proper score rule is a sufficient condition whereas a proper score rule is a necessary but not sufficient condition in plain words if s f g is a strictly proper score rule then the smaller its value the closer the distribution of f will be to that of g this is not true for proper scoring rules which can attain a perfect score even if f g while it is generally agreed upon that scoring rules must at least be proper to adequately quantify the accuracy of probabilistic forecasts winkler et al 1996 gneiting and ranjan 2011 the question which ones to use remains largely open gneiting and raftery 2007 alexander et al 2022 we restrict our attention to the four scoring rules listed in table 5 these scoring rules benefit a strong mathematical underpinning dawid 2007 gneiting and raftery 2007 and are used to evaluate predictive distributions of mcmc output in probabilistic forecasting krüger et al 2021 the scoring rules are negatively oriented 6 6 if s f y s g y then g is a better probabilistic forecast of the target variable y than f and for all but the interval score is condense different aspects of a forecast distribution among which accuracy skill resolution and sharpness in short the logarithmic score ls evaluates the forecast density p y t y at the measured value y t this measure has a strong foundation in information theory shannon 1948a b kullback and leibler 1951 and is strictly local as it ignores model predicted probabilities of all non realized outcomes the continuous rank probability score crps is a quadratic measure of the difference between the predictive cdf f y t y and the cdf g t of y t this is a common measure of performance for probabilistic forecasts of a scalar observation y t and rewards predictive distributions with a mass close to the observation the pseudospherical score ps remunerates leptokurtic forecast densities as well yet this score rule acts on the predictive pdf and uses mixed powers in the numerator and denominator for ζ 2 the ps reduces to the spherical score ss as used herein friedman 1983 the interval score is is important for quantile prediction and rewards narrow prediction intervals if the observation is outside the l t u t prediction interval the score incurs a penalty the size of which depends on the significance level α note that the is evaluates only two predictive percentiles of the forecast distribution this does not necessarily require an accurate description of the distribution of the predictand y t hence in the terminology of gneiting and raftery 2007 the is is a proper but not a strictly proper scoring rule the situation is different for the ls crps and ss scoring rules which are strictly proper and thus encourage forecasters to model correctly the entire predictive distribution thus in principle any of the ls crps and ss rules will suffice to evaluate the quality of the probabilistic forecasts of the gl gl ul sl and nl functions for each scoring rule we compute a time averaged mean score 47 s f y 1 n t 1 n s f t y t where f f 1 f 2 f n is the collection of empirical cdfs derived from the m posterior realizations specifically for t 1 2 n the empirical cdf ecdf of the predicted outcomes f t is constructed from the m entries y t 1 y t 2 y t m in the t th row of the n m matrix y as follows 48 f t z 1 m j 1 m 1 z y t j where the indicator function 1 a returns 1 if a is true and zero otherwise the ecdf is assumed continuous for every θ θ and δ δ and strictly positive on the bounded interval ω as a result the density f t θ δ 49 f t z d d z f t z is continuous and strictly positive under the same support the lower l t and upper u t limits of the 100 1 α prediction interval are equal to 50 l t f t 1 α 2 and u t f t 1 1 α 2 where f t 1 z is the quantile function of f t θ δ the scoring rules of table 5 are indispensable in our search for the true forecast densities nevertheless have not yet entered mainstream use in the hydrologic community therefore we also consider other metrics of the forecast distribution hydrologists may be more familiar with see table 6 the reliability formalizes the thesis in probability theory that the sequence of probability integral transforms should consist of independent standard uniform random variables in other words if the n observations are samples of the predictive distribution f then the successive values of f t y t 51 f t y t 1 y t θ δ y t p y t θ δ θ δ p θ δ y d y t θ δ d θ δ should be uniformly distributed on the unit interval thyer et al 2009 renard et al 2011 the reliability rlbl equals the mean absolute distance 1 norm between the quantile function of the f t y t s and its counterpart of the standard uniform distribution the multiplier of 2 scales the rlbl to the unit interval between 0 poor and 1 perfect the coefficient of variation cv measures the average sharpness of the predictive distribution smaller values of the cv are preferred the coverage c equals the fraction of observations inside the prediction intervals to be statistically meaningful and robust c should equal 1 α at a significance level α the width w measures the average size of the 100 1 α prediction intervals the performance metrics of table 6 measure different and complementary aspects of the forecast distribution this complicates somewhat the ranking of the likelihood functions as we cannot combine the rlbl cv w and c into a single performance index without assigning arbitrary weights one can instead sort the performance metrics according to the pareto dominance principle and use the corresponding pareto rank of each likelihood function as overall performance index mcinerney et al 2017 2019 but the width and coefficient of variation are properties of the predictive distribution only and thus do not guarantee honest forecasts furthermore the reliability and coverage measure only two aspects of the statistical consistency between the distributional forecasts and the observations in other words these are necessary but not sufficient criteria for determining that the forecast distribution is accurate see e g hamill 2001 we therefore insist on using the strictly proper scoring rules of table 5 for likelihood function evaluation and ranking 5 5 general remarks to simplify the discussion of our findings we classify the nuisance variables of the gl gl sl and ul functions in two different groups nuisance variables that are integral part of the underlying pdf of each likelihood function are coined internal nuisance variables this includes the parameters of f sep a f sst a and f sgt a in eqs 9 30 and 36 respectively the remaining autoregressive coefficients ϕ 1 and or ϕ 2 will be referred to as external nuisance variables 5 6 case study i an ar 2 process with non gaussian innovations to benchmark the gl and ul functions our first case study considers a simple autoregressive scheme 52 y t ϕ 1 y t 1 ϕ 2 y t 2 ɛ t with ϕ 1 0 7 ϕ 2 0 2 and ɛ t s drawn from a a standardized sep distribution ɛ t sep 0 1 β ξ with β 0 5 and ξ 3 and b a standardized sgt distribution ɛ t sgt 0 1 λ p q with λ 0 5 p 1 2 and q 5 we create a synthetic training data record y y 1 y 2 y n with n 5 000 using initial conditions y 1 0 and y 0 0 fig 4 presents histograms of the marginal posterior distributions of coefficients ϕ 1 and ϕ 2 of the autoregressive model of eq 52 and the nuisance variables a β and ξ and b λ p and q of the sep and sgt distributions using the gl and ul functions of eqs 26 and 41 with s 0 1 the true values of the coefficients are separately indicated with a red cross the histograms of the sep and sgt coefficients are well described by a normal distribution with mean approximately equal to the true values of the parameters of the ar 2 training data record and a small dispersion this inspires confidence in the ability of the gl and ul functions to correctly describe the marginal distribution of the partial studentized residuals as the ar 2 operator of eq 12 does not preserve the skew and or kurtosis of the sep and sgt innovations ɛ 1 ɛ 2 ɛ n the mode of the marginal posterior distributions of the skew and or kurtosis parameters do not necessarily have to coincide with their true values but as we will show next such small discrepancies do not have practical consequences fig 5 compares the empirical density function histogram of the partial residuals ɛ 1 ɛ 2 ɛ n of eq 52 with its theoretical true counterpart solid line for the a sep and b sgt innovations respectively notice the excellent agreement between the theoretical pdf of the a sep and b sgt innovations and their empirical frequency distribution derived from mcmc simulation using dream suite vrugt 2016 the different bins of the empirical histogram of the partial residuals provide an almost perfect characterization of the true distribution of the partial residuals in fact this holds for almost any other realization from the posterior parameter distribution in other words the posterior uncertainty in the empirical distribution of the innovations is very small to understand the relationship between the length of the training data record and the posterior uncertainty of the coefficients of eq 52 please consider fig 6 which presents trace plots of the map values solid black lines of the two model parameters ϕ 1 and ϕ 2 and nuisance variables of the left sep and right sgt innovations the gray region portrays the 95 confidence intervals of the map solution the autoregressive coefficients ϕ 1 and ϕ 2 of the data generating process of eq 52 converge rapidly to their theoretical values after processing only one hundred data points the 95 confidence intervals of the two autoregressive coefficients have collapsed to a small region immediately surrounding their map values this conclusion holds for both the sep and sgt innovations most of the nuisance variables of the gl and ul functions converge at a somewhat smaller pace to their theoretical values with map values that go up and or down the prior parameter space for small training data records this is particularly true for the kurtosis parameters β and q a training record of n 2 000 data points appears long enough to sufficiently constrain all the nuisance variables of the gl and ul functions larger records hardly change the map values of the nuisance variables but further reduce the parameter uncertainty to a very small region interior to the uniform prior distribution intuitively one would expect that the highest order moments of the partial residuals are most difficult to accurately characterize thus it is no surprise that β q and or p may need a longer training data record on average than the skew parameters ξ and λ even a long training data record is no guarantee however that the skew and or kurtosis parameters of the gl and ul functions will converge exactly to their theoretical values such discrepancies do not harm the distribution of the partial residuals see fig 5 altogether the results of our first study demonstrate that the gl and ul functions correctly infer the marginal distribution of the sep and sgt innovations of the ar 2 scheme this inspires confidence in the ability of the two likelihood functions to accurately describe the distribution of the partial residuals 5 7 case study ii conceptual watershed model with measured discharge data as second case study we illustrate the application of the gl gl sl and ul functions to a parsimonious 5 parameter conceptual watershed model the hydrologic model describes the rainfall discharge relationship using five fictitious control volumes these reservoirs simulate processes such as evaporation percolation river inflow and baseflow see fig 7 hymod originates from the phd thesis of boyle 2001 and interested readers are referred to this publication for further details we embed the process formulations of fenicia et al 2018 in a mass conservative second order integration method adaptive time stepping guarantee a robust and accurate numerical solution of the simulated fluxes and state variables table 7 presents the five hymod parameters with their corresponding symbols units and lower and upper bounds we illustrate the different likelihood functions by application to hydrologic data from the leaf river near collins ms usa this medium sized watershed with a strong winter regime according to the functional classification of brunner et al 2020 has been studied extensively in the hydrologic literature we resort to the camels data set newman et al 2015 addor et al 2017 and simulate daily river discharge between 1 october 1998 and 30 september 2004 using daily estimates of catchment averaged precipitation maurer product and potential evapotranspiration derived from the formula of oudin et al 2005 daily discharge measurements of the last five years on record wy 1999 2004 are used for parameter estimation purposes using the gl gl sl and ul functions this amounts to n 1827 data points table 8 lists the scoring rules and performance metrics of tables 5 and 6 for the predictive streamflow distributions derived from the gl gl sl ul and nl functions using different active sets of nuisance variables inactive nuisance variables are set to their default values see tables 1 2 and 4 for completeness the penultimate three columns list summary statistics of the goodness of fit of the hymod map simulation finally the last column reports the pareto rank of all likelihood functions but the gl function these ranks are derived from non dominated sorting of the strictly proper scoring rules of the tabulated formulations of the ul gl sl and nl functions thus all likelihoods are ranked together treating ls crps and ss as if they are complementary non commensurate criteria rank one solutions are pareto optimal any other solution is considered inferior a priori one would advise using ϕ 1 and or ϕ 2 in the active set of nuisance variables as the discharge residuals will almost surely exhibit serial correlation and thus fail an independence test to explore to the fullest extent possible the performance of the different likelihood functions we do include formulations without the external nuisance variables ϕ 1 and or ϕ 2 the row number in the first column serves as unique identifier of each likelihood function the tabulated data may be a bit overwhelming and thus we organize and discuss our results according to the goals and or objectives of this paper comparison of gl and gl functions i the values of the scoring rules and performance metrics of the gl and gl functions are in almost perfect agreement when the active set of nuisance variables is equivalent and made up of only internal variables any small differences between the two likelihood functions are simply the result of a dissimilar treatment of the slope s 1 of the measurement error function the gl function considers s 1 a free parameter whose value is estimated jointly with the other nuisance variables the gl function on the contrary evokes the slope as a phantom variable so as to enforce a unit variance of the studentized residuals e θ δ this latter approach frees s 1 from the list of nuisance variable in the gl sl nl and ul functions this simplification is inconsequential as evidenced by the close match in the results of formulations 13 15 and 16 of the gl function and their equivalent counterparts 29 31 and 32 of the gl function respectively ii the scoring rules and performance metrics of the gl function deteriorate tremendously when the active set of nuisance variables includes one or more external variables ϕ 1 and or ϕ 2 this is an immediate consequence of the application of the ar k process of eq 12 to non homogenized residuals this flaw was identified by evin et al 2013 and explains the rather inferior and overdispersed forecast distributions of the gl function as evidenced by the exceedingly high values of the scoring rules relatively poor reliability and unexpectedly large values of the coefficient of variation width of the 95 prediction intervals and bias of the map simulation note that formulations 27 and 28 of the gl function even produce negative values of the cv this exemplifies the deficient implementation of the gl function and demonstrates that we do not truncate negative discharge forecasts to zero the gl function does not suffer these problems as residual studentization precedes the treatment of serial correlation in eq 12 this results in a much better description of the discharge forecast distribution with prediction intervals that are much sharper on average and exhibit an improved coverage and reliability performance of distribution adaptive likelihood functions i the scoring rules and performance metrics vary considerably among the tabulated formulations of the same distribution adaptive likelihood function the choice of active nuisance variables exerts a large control on the performance of the ul gl and sl functions the most advanced formulations do not necessarily yield the best overall performance in other words a larger number of active nuisance variables does not necessarily improve the performance of the ul gl and sl functions ii it is difficult if not impossible to single out a single best likelihood function and or formulation thereof whose performance is uniformly excellent across all scoring rules and or performance metrics the gl and sl functions achieve on average a somewhat lower pareto rank than the ul function suggesting that the advantages of this latter likelihood function did not fully materialize in the present example yet we should not draw generalized conclusions from this single data set and model we do not observe large differences in the performance of the gl ul sl and nl functions none of their formulations consistently places best among the different scoring rules and or performance metrics furthermore care should be exercised that each non dominated rank one likelihood function satisfies residual assumptions such diagnostic checks of the residuals will impact substantially the ranking of the likelihood functions iii the sl function has the largest number of non dominated formulations two and produces on average the lowest values of the is and width w of the 95 prediction intervals the gl function yields the highest reliability the ul function is a close competitor to the sl and gl functions and achieves the highest mean value of the log likelihood l θ δ y but at the expense of a somewhat larger rmse and pbias the ul and gl have only a single pareto optimal formulation according to the strictly proper scoring rules of the discharge forecast distribution iv the ul gl sl functions produce equivalent scoring rules performance metrics and summary statistics if the active set of nuisance variables includes only the intercept s 0 of the measurement error function of eq 24 the performance of these three respective formulations 9 17 and 25 is nearly indistinguishable and equal to that of the nl function with s 0 35 this latter finding is enforced by the use of normal default values of the inactive nuisance variables and confirms our analytic derivations v the nl function with heteroscedastic measurement error function 35 achieves a performance that is at least comparable to that of most of the tabulated formulations of the gl sl and ul functions but as this formulation does not satisfy residual assumptions shown later we should benchmark the distribution adaptive likelihood functions against the nl function with s 0 ϕ 1 34 all tabulated formulations of the ul gl and sl functions but the ul function with 3 s 0 p ϕ 1 improve upon the scoring rules and performance metrics of this normal likelihood as evidenced by their superior pareto rank vi different trials with the exact same likelihood function produce almost equal values of the scoring rules performance metrics and summary statistics see 15 20 and 34 this demonstrates that the sampled chains successively converge to the approximately same target distribution thus disparities in the tabulated statistics of the likelihood functions highlight differences between the ul gl sl and nl functions and or their selection of active nuisance variables on scoring rules and performance metrics i the scoring rules of the forecast distribution display only weak relationships see tables f 1 and f 2 in appendix f the pearson and spearman correlation coefficients r and ρ respectively of ls crps ss and is are consistently smaller than 0 5 except for r crps is 0 67 and ρ crps is 0 62 respectively the weak correlation between the strictly proper scoring rules may seem counter intuitive given that ls crps and ss measure the divergence of the predictive distribution to the true forecast density apparently this divergence can be expressed using three seemingly unrelated quantities but this does not imply that the strictly proper scoring rules are independent their low degree of correlation simply articulates a large departure on average from the true forecast densities we expect the pairwise correlations of ls crps and ss to increase when the predictive distributions approximate more closely and consistently the true forecast densities the relative independence of ls crps and ss turns the selection of an adequate likelihood into a multi criteria decision making problem ii the performance metrics of the forecast distribution do not display strong linear and monotonic relationships appendix f the exception to this are the reliability and coefficient of variation which have pearson and spearman correlation coefficients of 0 82 and 0 91 respectively these two performance metrics measure different aspects of the forecast distribution yet exhibit a relatively strong linear and rank dependence for the present data set and model iii most of the scoring rules display only a weak relationship with the tabulated performance metrics the strongest linear and rank correlation is found between the crps and width w of the forecast distribution r crps w 0 71 ρ crps w 0 87 and the spherical score ss and the reliability r ss rlbl 0 82 ρ ss rlbl 0 76 a somewhat weaker relationship is found between the ss and the coefficient of variation r ss cv 0 73 ρ ss cv 0 68 and the logarithmic score ls and the coverage c r ls c 0 68 ρ ls c 0 73 the scoring rules complement the performance metrics and help provide additional and or orthogonal information about the quality of the forecast distribution iv all tabulated formulations of the ul gl sl and nl functions achieve the desired coverage of about 0 95 at a significance level α 0 05 this performance metric does not discriminate among the different likelihood functions indeed a coverage of 1 α is simply a result of the rigorous application of probability theory note that a poor coverage alerts us to an erroneous implementation of the likelihood function inaccurate characterization of the posterior θ δ parameter distribution and or use of an improper prior distribution v the crps demonstrates a rather small variation among the different likelihood formulations this measure is commonly used when the probabilistic forecast is a cdf and the observations are scalars this amounts to an ecdf g t of each observation y t that is equal to a dirac delta function with unit integral the crps may show a larger variation among the different likelihoods if we replace this tall narrow spike with an actual cdf for each individual discharge observation this cdf may be construed from replicates of discharge time series using the approach of oliveira and vrugt 2022 alternatively one can specify a normal cdf with variance derived from eq 24 using the map value of s 0 this latter approach is not ideal as the cdf of the data will depend on the likelihood function used vi the reliability coefficient of variation width and coverage measure specific properties of the forecast distribution deemed important to represent accurately in practical application these performance metrics have the advantage of being easy to analyze and interpret but as necessary and insufficient conditions of the predictive distribution these metrics do not help in the pursuit of the true forecast density the same is true for proper scoring rules such as the interval score vii the strictly proper scoring rules provide a sufficient description of the true forecast density yet the ls crps and ss are difficult to interpret as they provide an integrated measure of the divergence of the predictive distribution from the true forecast density the treatment of residual autocorrelation i the use of external nuisance variables ϕ 1 and or ϕ 2 in the ul gl sl and nl functions enhances substantially the log likelihood of the posterior realizations but at the expense of a deterioration in most if not all of the scoring rules and performance metrics specifically the treatment of serial correlation reduces the reliability of the discharge forecast distribution and increases the width w of its 95 prediction intervals in interpreting this finding it is important to reiterate that the autoregressive operator of eq 12 is evaluated in simulation mode using the recipe detailed in algorithm e 1 if we switch to one observation ahead forecasting and submit the actual residuals to the autoregressive operator instead then the quality of the predictive discharge distribution would improve considerably beyond the scoring rules and performance metrics obtained with internal nuisance variables only however this would obfuscate the comparison of ar and non ar based formulations of the likelihood function s and thus we do not present these results herein ii the use of external nuisance variables ϕ 1 and or ϕ 2 substantially increases the rmse and percentage bias of the map simulation this is a result of model evaluation in simulation mode the map discharge simulation equals the deterministic output of hymod only thus does not include the ar extension of eq 12 iii there are no benefits to using an ar 2 model of the studentized raw residuals the value of ϕ 2 consistently converges to zero in all respective formulations higher order autoregressive models may be required for other data sets and or models to remove serial correlation among the studentized raw residuals altogether the results confirm that the gl function is superior to the gl function and has the additional benefit of requiring the estimation of one less nuisance variable the use of studentized residuals in the autoregressive model of eq 12 rectifies a critical deficiency of the gl function and guarantees an accurate implementation of the sep log likelihood of eq 26 this makes obsolete the gl function and thus we focus our attention on the ul gl and sl functions the strictly proper scoring rules facilitate a rigorous evaluation of the forecast distributions derived from the posterior realizations of the different likelihood functions this reduces the need for a visual assessment of the results the ls crps and ss and or is should not be used as sole determinant for likelihood function selection as these scoring rules do not verify whether residual assumptions are satisfied some of the likelihood functions perform better than others but no formulation consistently ranks best among the different scoring rules and or performance metrics to illustrate this in more detail please consider fig 8 which presents bivariate scatter plots of the values of the strictly proper scoring rules of the ul gl sl and nl functions of table 8 we assign a separate color to each likelihood function and use empty white circles to differentiate formulations that do not treat serial correlation from those that do filled circles the scatter diagrams are only cross sections of the three dimensional space of strictly proper scoring rules yet highlight some of the difficulties involved in the selection of a suitable likelihood function the dotty plots confirm our earlier claim that the treatment of serial correlation tends to deteriorate on average the quality of the discharge forecast distribution this testifies to the use of simulated residuals in the autoregressive operator of eq 12 the different graphs do not favor a single unique formulation of the likelihood function but rather demonstrate the presence of multiple different optimal formulations these so called pareto solutions represent trade offs among the different scoring rules and have the property that moving from one solution to another results in the improvement of one scoring rule while causing a simultaneous deterioration in one or more others this is a result of the low correlation among the strictly proper scoring rules and consequently we can treat the selection of an adequate likelihood function as a multi criteria decision making problem the spatial arrangement of the different colors in each dotty plot confirm our earlier finding that the different formulations of the sl function achieve on average the best performance followed by the gl and ul functions upon inspection of the pareto ranks of the ar based formulations of the gl ul and sl functions listed in table 8 we conclude that the ul function with 1 s 0 λ p q ϕ 1 ϕ 2 2 s 0 λ ϕ 1 4 s 0 q ϕ 1 6 s 0 p q ϕ 1 and 8 s 0 λ p q ϕ 1 the gl function with 10 s 0 β ξ ϕ 1 ϕ 2 11 s 0 β ϕ 1 12 s 0 ξ ϕ 1 14 s 0 β ξ ϕ 1 and the sl function with 18 s 0 ν ξ ϕ 1 ϕ 2 19 s 0 ν ϕ 1 20 s 0 ξ ϕ 1 and 22 s 0 ν ξ ϕ 1 achieve a performance that is superior to the nl function of eq 27 with s 0 ϕ 1 purple dot at this time it is important to reiterate that we could have also ranked the different likelihood functions according to any of the three strictly proper scoring rules then likelihood functions 18 12 and 10 would have come on top for the ls crps and ss rules respectively and the so ordered likelihood functions would significantly overlap with the pareto dominance based selection listed above of these solutions we will investigate in more detail next the ul function with 2 s 0 λ ϕ 1 the gl function with 12 s 0 β ϕ 1 and sl function with 20 s 0 ξ ϕ 1 the scoring rules and or performance metrics do not tell us whether residual assumptions are satisfied and whether hymod accurately describes the measured discharge record next we verify whether the partial residuals of the ul gl sl and nl functions satisfy assumptions of homogeneity and independence among others fig 9 presents the results of our analysis the left graph displays the studentized partial residuals as a function of the hymod simulated discharge the second graph shows the assumed solid line and actual vertical lines pdf of the studentized partial residuals the third graph presents the sample autocorrelation function of the studentized partial residuals the fourth and last graph compares theoretical and empirical quantiles of the studentized partial residuals to benchmark our findings the bottom panel visualizes the results of the nl function with s 0 35 this likelihood function does not treat serial correlation thus we expect that it will violate residual assumptions the scatter plots appear to suggest that the studentized partial residuals are not homogeneous but rather decrease in magnitude with increasing value of the simulated flows but this is an optical illusion upon close inspection of the data scattering at different flow levels we can reveal that the variance of the studentized partial residuals is rather constant and independent of the magnitude of the simulated discharge the near zero slopes of the regression lines confirm the homogeneous nature of the ɛ t θ δ s the empirical density functions of the partial residuals follow a laplacian distribution that is generally well described by the respective pdfs of the likelihood functions the characteristic peak of the partial residuals is only duplicated by the gl function note that the pdfs of the ul and sl functions exhibit a small skew to the right whereas the pdf of the gl function is symmetric around zero by definition the pdfs of the nl function are without skew autocorrelation is negligible at all lags for likelihood functions 2 12 20 and 35 this is an immediate consequence of the use of the ar operator in eq 12 the nl function in the bottom panel does not treat serial correlation hence the studentized residuals violate the independence assumption the theoretical and empirical quantiles of the studentized partial residuals are in close agreement within the interquartile range outside this interval the empirical quantiles of the likelihood functions deviate considerably from the 1 1 line indeed none of the likelihood functions characterize well the tails of the residual distribution this involves only a small number of residuals but these residuals are critically important in describing adequately the prediction limits of the simulated discharge fig 10 presents time series plots of the observed red dots and simulated discharge of the m posterior realizations for a representative 1 year period of the training record of the leaf river basin using likelihood functions a 2 b 12 c 20 and d 34 the dark and light shaded regions summarize the 95 credible intervals due to parameter and total uncertainty respectively for all four likelihood functions the hymod parameter uncertainty makes up an insignificant part of the prediction uncertainty with 95 credible intervals that collapse to a dark line with the exception of a few peak flows the discharge prediction uncertainty is largest for the gl function with 95 intervals that extent to zero for the largest streamflows this is visually not particularly pleasing the nl function with s 0 ϕ 1 provides a relatively good description of the uncertainty with map solution not shown that tracks reasonably well the measured discharge record the prediction uncertainty of the ul and sl functions depend most on the simulated flow level the discharge uncertainty is relatively small at low flows but increases substantially during rainfall events with increasing discharge the prediction uncertainty of the nl function appears more constant with a comparatively large uncertainty at low flows and comparatively small uncertainty at the peak flows altogether the prediction uncertainty of the ul function is visually most pleasing with 95 intervals that are smallest on average finally we investigate the effect of the choice of likelihood function on the posterior marginal distribution of the hymod parameters see fig 11 the hymod parameters appear well defined by the measured discharge with exception of the nl function with s 0 ϕ 1 we observe a close agreement in the marginal parameter distributions derived from the different likelihood functions this is particularly true for the ul and sl functions we see nicely bell shaped histograms with an approximately similar mean and comparably small spread variations in the mean of each hymod parameter among the different likelihood functions are relatively small certainly compared to the prior ranges of the parameters the hymod parameters exhibit a negligible correlation with exception of s u max and b of the nl function with s 0 ϕ 1 this explains their much enlarged uncertainty these results confirm that the choice of likelihood function has an effect on the posterior parameter values note in our analysis here we limit our attention to the performance of hymod during the 5 year calibration period and do not present performance statistics and or analyze results for an independent evaluation period the calibration period suffices to illustrate the application of the different likelihood functions and the comparison and or ranking of their predictive distributions furthermore for a long data set and parsimonious model such as hymod we do not expect large changes to the performance of the likelihood functions in an independent evaluation period under the stationarity thesis if model selection is of interest then the user can resort to game sampling volpi et al 2017 this method numerically integrates the posterior distribution in pursuit of the so called marginal likelihood this measure suffices to determine which model and or likelihood is most supported by the experimental discharge data 6 conclusions this paper was concerned with the formulation of herein called distribution adaptive likelihood functions in the application of bayesian epistemology to uncertainty quantification of hydrologic watershed models this class of likelihood functions does not require prior assumptions about the expected distribution of the residuals inference takes place over the model parameters and space of distribution functions defined by the nuisance variables of the likelihood function the goals of this paper were threefold first we presented a revised formulation of the generalized likelihood gl function of schoups and vrugt 2010 by enacting the treatment of serial correlation on the studentized raw residuals rather than the raw residuals this so called gl function rectifies a critical deficiency of the gl function this correction guarantees an accurate computation of the sep log likelihood and allows for a much more robust joint inference of the autoregressive coefficients and variance skew and or kurtosis of the residuals as secondary goal we presented a further generalization of the gl function coined the universal likelihood ul function this likelihood function uses as its main building block the skewed generalized student s t distribution of theodossiou 2015 and extends applicability to a much larger family of well known probability distributions as third and last goal we introduced the use of strictly proper scoring rules to evaluate compare and rank the predictive distributions of different likelihood functions the logarithmic continuous rank probability and spherical score rules condense the accuracy of a distribution forecast to a single value while retaining attractive statistical properties and incentivizing a correct modeling of the predictive distribution the power and usefulness of the gl and ul functions were demonstrated by application to two case studies the first study involved a simple autoregressive data generating process with sep and or sgt innovations and was used to investigate the convergence properties of the gl and ul functions both likelihood functions converge upon the true values of the model parameters and shape coefficients of the distribution of the innovations the uncertainty in the autoregressive coefficients and nuisance variables of the partial residuals dissipates quickly with increasing length of the training data record as the ar 2 operator does not preserve higher order moments of the sep and or sgt innovations the mode of the marginal distributions of the skew and or kurtosis parameters may not coincide exactly with their true values such discrepancies however are inconsequential as the distribution of the innovations is perfectly described the second case study considered the application of distribution adaptive likelihood functions to a simple five parameter watershed model named hymod using daily discharge measurements of the leaf river different formulations of the gl and ul functions were evaluated using the logarithmic continuous rank probability and spherical scoring rules and benchmarked against the gl function the student t likelihood sl of scharnagl et al 2015 and a normal likelihood nl with first order autoregressive model and or heteroscedastic measurement errors our most important results were as follows 1 the gl function is superior to the gl function and has the advantage of requiring the specification of one less nuisance variable 2 the performance of the gl sl and ul functions depends in large part on the choice of nuisance variables the most complex formulations of the gl sl and ul functions all nuisance variables in active set do not yield the best performance 3 the internal nuisance variables of the gl sl and ul functions enable a better description of the discharge residuals this reduces the width of the discharge prediction intervals 4 the normal likelihood achieves a performance that is fairly comparable to several formulations of the gl sl and ul functions 5 the treatment of autocorrelation deteriorates the values of the scoring rules and performance metrics of the forecast distribution if however we switch to one observation ahead forecasting and work with actual rather than simulated residuals then the use of autocorrelation improves substantially the quality of the predictive discharge distribution beyond the scoring rules and performance metrics obtained for internal nuisance variables only e g see evin et al 2014 5 scoring rules are indispensable in our search for the true forecast distribution 6 strictly proper scoring rules facilitate a statistically rigorous evaluation and ranking of different likelihood functions 7 we cannot single out a single best likelihood function whose performance is uniformly excellent across all scoring rules and performance metrics of the forecast distribution 8 the use of one strictly proper scoring rule suffices in principle for evaluation and ranking of the likelihood functions the simultaneous use of the logarithmic continuous rank probability and spherical scores converts the selection of an adequate likelihood function into a multi criteria problem care should be exercised that each likelihood function satisfies residual assumptions 9 the pareto optimal likelihood functions produce fairly similar marginal distributions of the hymod parameters altogether our results for hymod favored the use of a distribution adaptive likelihood function in describing the rainfall discharge dynamics of the leaf river a leptokurtic distribution of the partial residuals provided the overall best characterization of the measured discharge record and associated uncertainties credit authorship contribution statement jasper a vrugt conceptualization methodology software validation formal analysis investigation resources writing original draft writing review editing visualization supervision debora y de oliveira validation methodology formal analysis resources visualization gerrit schoups methodology validation investigation cees g h diks methodology validation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the comments of carlos lima and one anonymous reviewer led to a significantly improved manuscript the second author gratefully acknowledges the financial support received from the brazilian federal agency for support and evaluation of graduate education capes grant number 88881 174456 2018 01 appendix a standardized skewed exponential power sep density we revisit appendix a of schoups and vrugt 2010 and reiterate how the standardized skew exponential power density function in eq 9 can be obtained from the exponential power ep or generalized normal distribution of subbotin 1923 and box and tiao 1992 using the method of fernandez and steel 1998 the density of the standardized ep distribution zero mean and unit variance at point a may be computed as follows box and tiao 1992 a 1 f ep a 0 1 β ω β exp c β a 2 1 β where β 1 1 is a so called kurtosis parameter which controls the peakedness of the distribution 7 7 with 1 β 1 the residual norm ℓ 2 1 β ranges between ℓ 1 and ℓ and denotes the absolute value or modulus operator the values of the scalars c β and ω β depend on the kurtosis parameter and are given by box and tiao 1992 a 2a c β γ 3 1 β 2 γ 1 β 2 1 1 β a 2b ω β γ 1 2 3 1 β 2 1 β γ 3 2 1 β 2 where γ b signifies the incomplete gamma function evaluated at b a 3 γ b 0 x b 1 exp x d x b r which satisfies the recursion γ b 1 b γ b the gamma function can be approximated numerically a 4 log γ b γ b log b m 1 b m log 1 b m where γ lim m log m k 1 m 1 k 0 5772 is the so called euler mascheroni constant and log signifies the natural logarithm the ep density f a β in eq a 1 is symmetric around a 0 this symmetry impairs our ability to describe accurately skewed residual distributions with an upper or lower tail fernandez and steel 1998 developed a general solution for the treatment of skew in symmetric distributions with closed form mathematical expressions their template density f skew a ξ with skewness parameter ξ r introduces tails in an arbitrary symmetric density f as follows a 5 f skew a ξ 2 ξ ξ 1 f a ξ sign a 2 ξ ξ 1 f a ξ sign a where sign is the signum function a 6 sign x 1 if x 0 0 if x 0 1 if x 0 and the mean μ ξ and variance σ ξ 2 of a satisfy the following relationship fernandez and steel 1998 a 7a μ ξ m 1 ξ ξ 1 a 7b σ ξ 2 m 2 m 1 2 ξ 2 ξ 2 2 m 1 2 m 2 wherein m j is the j th absolute moment of the standardized ep density f ep 0 1 β of eq a 1 a 8 m j e x j x j f ep x 0 1 β d x we can now substitute the exponential power density f ep a 0 1 β of eq a 1 into the skew distribution f skew a ξ of eq a 5 the result is a non standardized skewed exponential power sep density with mean μ ξ and variance σ ξ 2 of a determined by eqs a 7a and a 7b to negate changes in the mean and variance of a imposed by the use of the skew density of eq a 5 we must scale the sep density and therefore use as its input argument μ ξ σ ξ a rather than a this leads to the following expression for the standardized sep density schoups and vrugt 2010 a 9 f sep a 0 1 β ξ 2 σ ξ ω β ξ ξ 1 exp c β μ ξ σ ξ a ξ sign μ ξ σ ξ a 2 1 β where c β ω β μ ξ and σ ξ are a function of the kurtosis β and skewness ξ using eqs a 2a a 2b a 7a and a 7b respectively the derivation of the standardized sep density function in eq a 9 would not be complete without addressing the computation of the first and second moment m 1 and m 2 in eqs a 7a and a 7b respectively as the ep distribution of subbotin 1923 is symmetric around zero eq a 8 may be written as follows a 10 m j 2 0 x j f ep x 0 1 β d x and reduces to a 11 m j 2 ω β 0 x j exp c β x 2 1 β d x for the ep distribution in eq a 1 the above integral appears in integration tables a 12 0 x j exp a x b d x 1 b a j 1 b γ j 1 b where a c β and b 2 1 β if we enter this solution in eq a 11 then we obtain the following closed form expression for the j th moment of the ep distribution a 13 m j 2 ω β c β j 1 2 1 β 2 1 β γ j 1 2 1 β ω β 1 β c β 1 2 j 1 1 β γ j 1 1 β 2 for j 1 we yield the first moment m 1 of the ep distribution a 14 m 1 ω β 1 β c β 1 β γ 1 β we can enter eqs a 2a and a 2b for ω β and c β respectively to yield a 15 m 1 γ 1 2 3 1 β 2 1 β 1 β γ 3 2 1 β 2 γ 3 1 β 2 γ 1 β 2 1 1 β 1 β γ 1 β γ 1 2 3 1 β 2 γ 3 2 1 β 2 γ 3 1 β 2 γ 1 β 2 1 γ 1 β γ 1 2 3 1 β 2 γ 3 1 β 2 γ 1 β 2 γ 3 2 1 β 2 γ 1 β and thus the first moment of the ep distribution simplifies to a 16 m 1 γ 1 β γ 1 2 3 1 β 2 γ 1 2 1 β 2 for the second moment m 2 of the ep distribution we must enter j 2 in eq a 13 a 17 m 2 ω β 1 β c β 3 2 1 β γ 3 1 β 2 after substitution of eqs a 2a and a 2b the above expression simplifies to a 18 m 2 γ 1 2 3 1 β 2 1 β 1 β γ 3 2 1 β 2 γ 3 1 β 2 γ 1 β 2 1 1 β 3 2 1 β γ 3 1 β 2 γ 3 2 3 1 β 2 γ 3 2 1 β 2 γ 3 1 β 2 γ 1 β 2 3 2 1 this concludes the derivation appendix b the student t likelihood function the gl and gl functions rely on the generalized normal or ep distribution to characterize nontraditional residual distributions with different degrees of skew and kurtosis scharnagl et al 2015 replaced the ep distribution of eq 7 with a standardized student s t density b 1 f st a 0 1 ν γ ν 1 2 γ ν 2 1 π ν 2 1 a 2 ν 2 ν 1 2 where μ a 0 σ a 2 1 and ν 2 denotes the degrees of freedom if we combine eq b 1 with the skew density of eq 8 we yield the following expression for the standardized skewed student s t sst density scharnagl et al 2015 b 2 f sst a 0 1 ν ξ 2 σ ξ ξ ξ 1 γ ν 1 2 γ ν 2 π ν 2 1 1 ν 2 μ ξ σ ξ a ξ sign μ ξ σ ξ a 2 ν 1 2 where ξ 0 is the skewness parameter and μ ξ and σ ξ 2 are shift and scale constants respectively which standardize the sst density the shift and scale constants μ ξ and σ ξ 2 respectively must satisfy eqs a 7a and a 7b b 3a μ ξ m 1 ξ ξ 1 b 3b σ ξ 2 m 2 m 1 2 ξ 2 ξ 2 2 m 1 2 m 2 wherein m j is the j th absolute moment of the standardized st density f st 0 1 ν of eq b 1 b 4 m j e x j 2 0 x j f st x 0 1 ν d x this results in kirkby et al 2019 b 5 m j γ j 1 2 γ ν j 2 ν 2 j 2 π γ ν 2 and leads to the following expressions for the first two absolute moments of the st distribution b 6 m 1 γ ν 1 2 ν 2 π γ ν 2 and m 2 γ 3 2 γ ν 2 2 ν 2 π γ ν 2 if we enter the above expressions for m 1 and m 2 into eqs b 3a and b 3b and simplify and rearrange the resulting equations we yield see also scharnagl et al 2015 b 7a μ ξ γ ν 1 2 ν 2 ξ ξ 1 π γ ν 2 b 7b σ ξ 2 μ ξ 2 ξ 2 ξ 2 1 the so obtained shift and scale constants standardize the sst distribution μ a 0 and σ a 2 1 the autoregressive model of eq 12 may be used to treat serial correlation of the studentized raw residuals e θ δ we can use eq 21 to derive the corresponding sst likelihood function b 8 l θ ν ξ σ ɛ 2 y σ ϵ 2 t 1 n 1 σ ϵ t 1 σ ɛ f sgt ɛ t θ δ ν ξ σ ɛ n t 1 n 2 σ ξ γ ν 1 2 σ ϵ t ξ ξ 1 γ ν 2 π ν 2 1 1 ν 2 μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 ν 1 2 where σ ɛ 2 equals the variance of the partial residuals in eq 13 the above formulation of the sst likelihood function l θ ν ξ σ ɛ 2 y σ ϵ 2 assumes prior knowledge of the measurement error variances σ ϵ 2 we can relax this assumption and infer the n entries of σ ϵ 2 along with the model parameters θ using the measurement error function of eq 24 the sst likelihood function then becomes b 9 l θ s 0 ν ξ σ ɛ 2 y σ ɛ n t 1 n 2 σ ξ γ ν 1 2 s 0 s 1 y t θ ξ ξ 1 γ ν 2 π ν 2 1 1 ν 2 μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 ν 1 2 where s 1 equals the zero point of the residual function ħ s 1 δ 1 g s 1 δ and satisfies the constraint explicated in eq 25 the sst log likelihood l θ s 0 ν ξ σ ɛ 2 y now becomes b 10 l θ s 0 ν ξ σ ɛ 2 y n 2 log σ ɛ 2 1 2 t 1 n log s 0 s 1 y t θ 2 n log 2 n log σ ξ n log γ ν 1 2 n log ξ ξ 1 n log γ ν 2 n 2 log π n 2 log ν 2 ν 1 2 t 1 n log 1 1 ν 2 μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 this concludes our derivation of the sst log likelihood function abbreviated as sl function appendix c special and limiting cases of the sgt distribution the skewed generalized t distribution of theodossiou 1998 defines a large family of continuous probability distributions which includes several well known distributions these special or limiting cases of the sgt distribution f sgt a μ σ λ p q may be derived by fixing one or more of its shape parameters λ p and q to preset values what follows is a summary of twelve well known probability distributions c 1 the skewed generalized error distribution the skewed generalized error sge distribution of theodossiou 2015 is defined as follows c 1 lim q f sgt a μ σ λ p q and yields the following pdf c 2 f sge a μ σ λ p p 2 v σ γ 1 p exp a μ m v σ 1 λ sign a μ m p where m 2 2 p v σ λ γ 1 2 1 p π and c 3 v π γ 1 p π 1 3 λ 2 γ 3 p 1 6 1 p λ 2 γ 1 2 1 p 2 γ 1 p affix the mean μ and variance σ 2 of the sge distribution c 2 the generalized student s t distribution the generalized student s t distribution of mcdonald and newey 1988 is defined as follows c 4 f sgt a μ σ λ 0 p q and yields the pdf c 5 f gt a μ σ p q p 2 v σ q 1 p b 1 p q a μ p q v σ p 1 1 p q where c 6 v 1 q 1 p b 1 p q b 3 p q 2 p affixes the variance σ 2 of the gt distribution c 3 the skewed student s t distribution the skewed student s t st distribution of hansen 1994 is defined as follows c 7 f sgt a μ σ λ p 2 q and yields the following pdf c 8 f st a μ σ λ q γ 1 2 q v σ π q 1 2 γ q x μ m 2 q v σ 2 λ sign x μ m 1 2 1 1 2 q where m 2 v σ λ q 1 2 γ q 1 2 π 1 2 γ q and c 9 v 1 q 1 2 3 λ 2 1 1 2 q 2 4 λ 2 π γ q 1 2 γ q 2 affix the mean μ and variance σ 2 of the st distribution c 4 the skewed laplace distribution the skewed laplace sl distribution is given by c 10 lim q f sgt a μ σ λ p 1 q and results in the following pdf c 11 f sl a μ σ λ 1 2 v σ exp a μ m v σ 1 λ sign a μ m where c 12 m 2 v σ λ and v 2 1 λ 2 1 2 affix the mean μ and variance σ 2 of the sl distribution c 5 the generalized error normal distribution the exponential power ep distribution subbotin 1923 box and tiao 1992 is defined as follows c 13 lim q f sgt a μ σ λ 0 p q and yields the following pdf c 14 f ep a μ σ p p 2 v σ γ 1 p exp x μ v σ p where c 15 v γ 1 p γ 3 p affixes the variance σ 2 of the ep distribution for μ 0 and σ 1 eq c 14 simplifies to the pdf of the standardized ep distribution in eq 7 indeed if we substitute eq c 15 into eq c 14 and rearrange the resulting expression we yield c 16 f ep a μ σ p p γ 1 2 3 p 2 σ γ 3 2 1 p exp γ 3 p γ 1 p p 2 x μ σ p for μ 0 and σ 1 this expression simplifies to eq 7 c 17 f ep a p ω p exp c p x p with constants ω p and c p c 18 c p γ 3 p γ 1 p p 2 and ω p p γ 1 2 3 p 2 γ 3 2 1 p equal to eqs a 2a and a 2b respectively and p 2 1 β c 6 the skewed normal distribution the skewed normal sn distribution is defined as follows c 19 lim q f sgt a μ σ λ p 2 q and has the following pdf c 20 f sn a μ σ λ 1 v σ π exp a μ m v σ 1 λ sign a μ m 2 where c 21 m 2 v σ λ π and v 2 π π 8 λ 2 3 π λ 2 affix the mean μ and variance σ 2 of the sn distribution c 7 the student s t distribution the student s t distribution student 1908 is given by c 22 f sgt a μ 0 σ 1 λ 0 p 2 q and yields the pdf c 23 f t a ν γ ν 1 2 π ν 1 2 γ ν 2 1 x 2 ν ν 1 2 c 8 the skewed cauchy distribution the skewed cauchy distribution is defined as follows c 24 f sgt a μ σ λ p 2 q 1 and yields the following pdf c 25 f sc a μ σ λ 1 σ π x μ 2 σ 2 λ sign x μ 1 2 1 1 c 9 the laplace distribution the laplace 1774 distribution is given by c 26 lim q f sgt a μ σ λ 0 p 1 q which results in the following pdf c 27 f l a μ σ 1 2 σ exp a μ σ where σ is the scale parameter of the laplace distribution c 10 the uniform distribution the uniform distribution is given by c 28 lim p q f sgt a μ σ λ 0 p q and results in the following pdf c 29 f u a 1 2 v σ if a μ v σ 0 otherwise the standard uniform distribution is obtained by setting μ a b 2 v 1 and σ b a 2 c 11 the normal distribution the gaussian distribution is defined as follows c 30 lim q f sgt a μ σ λ 0 p 2 q and yields the familiar pdf c 31 f n a μ σ 1 v σ π exp a μ v σ 2 where v 2 affixes the variance σ 2 of the normal distribution c 12 the cauchy lorentz distribution the cauchy lorentz cl distribution of poisson 1824 is given by c 32 f sgt a μ σ λ 0 p 2 q 1 and results in the following pdf c 33 f cl a μ σ 1 σ π x μ σ 2 1 1 this concludes appendix c appendix d the sgt likelihood with normally distributed residuals if the partial residuals ɛ t θ δ are believed to be normally distributed then we must set λ 0 p 2 and q unfortunately we cannot submit an infinite value of q to eqs 39 and or 41 as this will induce numerical problems a pragmatic remedy is to enter a large value of q instead say the default value of q 1 0 10 in table 4 this will do in practice we must still demonstrate the theoretical equivalence of the sgt log likelihood function with λ 0 p 2 and q and the nl function of eq 27 to support limit analysis we print below the sgt log likelihood function of eq 41 and label the three terms that require limit analysis d 1 l θ s 0 λ p q φ 2 y n 2 log σ ɛ 2 t 1 n log s 0 s 1 y t θ n log p n log 2 n log κ λ p q i n log b 1 p q p ii q 1 p t 1 n log 1 ɛ t θ δ μ λ p q κ λ p q 1 λ sign ɛ t θ δ μ λ p q p iii we use the labels i ii and iii in the analysis below for λ 0 p 2 and q the shift constant μ 02 in eq 35a amounts to zero for the scale constant κ 02 in eq 35b we yield eq d 2 see box ii we can take advantage of the following limit of the beta function d 3 lim b b a b γ a b a to simplify the expression of the scale constant κ 02 to yield d 4 κ 02 q γ 1 2 q 2 1 2 γ 3 2 q 2 2 3 2 if q 2 γ 1 2 q 2 1 2 γ 1 2 q 2 2 3 2 if q 2 q 2 1 2 q 2 3 2 if q q if q thus for q term i in eq d 1 is equal to n log q the limit of the beta function in eq d 3 can also be applied to term ii of eq d 1 we yield d 5 lim q n log b 1 2 q 2 n log γ 1 2 q 2 1 2 n 2 log π n 2 log q 2 n 2 log 2 π n log q thus in the limit of q term ii of eq d 1 may be replaced with n 2 log 2 π n log q this leaves us with term iii of the sgt log likelihood function in eq d 1 the following limit is well known in elementary calculus d 6 lim x 1 a x x exp a if we take the natural logarithm of both sides of this limit and then divide by p we yield the corollary d 7 lim x x p log 1 a x a p for q x p 2 and a ɛ t θ δ 2 the left hand side reduces to term iii of eq d 1 but without the sum operator as a result we yield d 8 lim q q 1 p t 1 n log 1 ɛ t θ δ 2 q ɛ 1 θ δ 2 2 ɛ 2 θ δ 2 2 ɛ n θ δ 2 2 1 2 t 1 n ɛ t θ δ 2 and thus in the limit of q term iii of eq d 1 simplifies to half the sum of squares of the standardized partial residuals if we replace terms i ii and iii in eq d 1 with their counterparts of eqs d 4 d 5 and d 8 we yield the sgt log likelihood function l θ s 0 φ 2 y λ 0 p 2 q for normally distributed partial residuals d 9 l θ s 0 φ 2 y λ 0 p 2 q n 2 log σ ɛ 2 t 1 n log s 0 s 1 y t θ n log 2 n log 2 n log q n 2 log 2 π n log q 1 2 t 1 n ϵ t θ δ 2 this expression of the sgt log likelihood function reduces to the nl function of eq 27 d 10 l θ s 0 φ 2 y λ 0 p 2 q n 2 log 2 π t 1 n log s 0 s 1 y t θ n 2 log σ ɛ 2 1 2 t 1 n ϵ t θ δ 2 with variance of the partial residuals σ ɛ 2 which is derived from eq 13 thus the sgt distribution is the key ingredient of a large family of likelihood functions which includes the normal likelihood function this concludes the derivation appendix e algorithmic recipe for the predictive distributions algorithm e 1 presents a step by step recipe for determining the predictive distribution p y t y of eq 3 from the posterior realizations sampled with dream suite this recipe is specifically written for the sep likelihood of the gl function but easy to adapt to the sl and ul functions of eqs 31 and 41 respectively the algorithmic recipe uses as input arguments i the d s m matrix θ δ of m posterior samples of d model parameters θ θ 1 θ 2 θ d concatenated vertically with the s nuisance variables δ s 0 β ξ ϕ 1 ϕ 2 of the sep likelihood function and ii the initial conditions e 1 and e 0 of the second order autoregressive scheme in eq 12 and returns n m matrices of posterior model simulations y s and predictions y predictive percentiles y t α 2 and y t 1 α 2 are readily derived from the m entries y t j r n m in the t th row of y e 1 y t α 2 y t 1 α 2 p y t y p y t α 2 y t j y t 1 α 2 y 1 α j 1 2 m by truncating α 2 of the left and right tails of the predictive distribution p y t y this must be repeated for all t 1 2 n and produces two n 1 vectors y α 2 and y 1 α 2 respectively which together define the 100 1 α prediction uncertainty if we admit instead to eq e 1 the n m matrix y s of posterior model simulations then we yield prediction percentiles due to parameter uncertainty only appendix f correlation of scoring rules and performance metrics to determine whether the scoring rules and or performance metrics measure independent information about the discharge forecast distribution we compute pairwise correlation coefficients of the different scoring rules and performance metrics of table 8 
2647,this paper is concerned with the formulation of an adequate likelihood function in the application of bayesian epistemology to uncertainty quantification of hydrologic models we focus our attention on a special class of likelihood functions hereinafter referred to as distribution adaptive likelihood functions which do not require prior assumptions about the expected distribution of the residuals rather inference takes place over the hypotheses model parameters and space of distribution functions our goals are threefold first we present theory of a revised implementation of the generalized likelihood gl function of schoups and vrugt 2010 wherein residual standardization precedes the treatment of serial correlation this so called gl function enjoys a solid statistical underpinning and guarantees a more robust joint inference of the autoregressive coefficients and residual properties then as secondary goal we present a further generalization of the gl function coined the universal likelihood ul function which extends applicability to highly asymmetrical lepto and platy kurtic residual distributions the ul function builds on the 5 parameter skewed generalized student s t distribution of theodossiou 2015 which makes up a large family of continuous probability distributions including but not limited to symmetric and skewed forms of the generalized normal generalized t laplace normal student s t and cauchy lorentz distributions as our third and last goal we present the use of strictly proper scoring rules to evaluate compare and rank likelihood functions these scoring rules condense the accuracy of a distribution forecast to a single value while retaining attractive statistical properties the gl and ul functions are illustrated using data of a simple autoregressive scheme and benchmarked against the gl function student t likelihood sl of scharnagl et al 2015 and normal likelihood nl for a conceptual hydrologic model using measured streamflow data our results show that i the gl function is superior to the gl function ii the active set of nuisance variables exerts a large control on the performance of the gl sl and ul functions iii the treatment of autocorrelation deteriorates the scoring rules and performance metrics of the forecast distribution iv a leptokurtic distribution is favored for discharge residuals v scoring rules are indispensable in our search for the true forecast distribution and vi the use of multiple strictly proper scoring rules turns the selection of an adequate likelihood function into a multi criteria problem keywords bayesian analysis distribution adaptive likelihood generalized likelihood universal likelihood scoring rules probabilistic forecasts data availability data will be made available on request 1 introduction and scope in the past decades much progress has been made in uncertainty quantification of the parameters state variables and simulated output of dynamic system models in all this work bayes theorem has emerged as a corner stone of modern probability theory hypothesis testing and as working paradigm for the subjectivist approach to epistemology statistics and inductive logic bayesian inference allows for an exact description of parameter uncertainty and other sources of uncertainty by treating the parameters as probabilistic variables with joint posterior probability density function pdf this so called posterior parameter distribution p θ y is the consequence of a prior distribution p θ which encodes all subjective knowledge about the parameters θ θ 1 θ 2 θ d before collection of the data y and a likelihood l θ y which was designated as mathematical quantity by fisher 1934 to quantify our degree of belief confidence in the parameter values θ in light of the observed data y bayes 1763 theorem expresses the mathematical relationship between the prior p θ likelihood l θ y and posterior updated beliefs p θ y of the parameters 1 p θ y p θ l θ y θ p θ l θ y p θ l θ y where the denominator p y θ p θ l θ y acts as a normalizing constant so that the posterior parameter distribution integrates to unity over the prior parameter space θ θ r d this so called model evidence or marginal likelihood plays a crucial role in hypothesis testing when selecting the best model among a collection of competing models volpi et al 2017 but can be ignored during parameter inference as the unnormalized posterior density p θ y p θ l θ y suffices to estimate p θ y the 100 1 α credible region c α θ of the vector valued θ θ will make up a 1 α proportion of the probability mass of p θ y at a significance level α 0 1 credible intervals c α a b of each parameter θ j j 1 2 d may be construed separately 2 a p θ 1 θ 2 θ j θ d y d θ j b p θ 1 θ 2 θ j θ d y d θ j α 2 where marginalization takes places over θ j the parameter space without θ j bayesian credible regions and or intervals are analogous to the confidence regions intervals of generalized least squares gls used in frequentist statistics lee 2012 although subtle but important differences exist in their computation and interpretation lu et al 2012 for a multi modal posterior parameter distribution confidence credible intervals are rather meaningless and we should resort instead to a highest posterior density hpd region a hpd region h α of α significance level is a 1 α region for which holds that p θ y p θ y for all θ h α and θ θ h α where a b x a and x b after we have processed the data y we can evaluate the posterior predictive pdf of the simulated quantity y θ r 3 p y y θ p y θ p θ y d θ where p y θ is the density function of the distribution of y given θ θ this predictive distribution reflects both the aleatory uncertainty in future observations and the posterior uncertainty in the parameters and is directly related to its cdf f y y 4 f y y θ f c y θ d p θ y where f c y θ is the conditional predictive distribution when θ θ equal the true parameter values greenberg 2013 gelman et al 2014 unfortunately the above integrals do not often admit a solution in closed form and thus must be approximated using some form of monte carlo simulation gelfand and smith 1990 gilks et al 1996 the likelihood function exerts a large control on the posterior parameter and predictive distributions p θ y and p y y respectively and has been the subject of much debate in the hydrologic literature beven and binley 1992 freer et al 1996 yang et al 2007 smith et al 2008 smith and marshall 2008 reichert and mieleitner 2009 vrugt et al 2009 schoups and vrugt 2010 smith et al 2010 evin et al 2013 scharnagl et al 2015 smith et al 2015 li et al 2016 mcinerney et al 2019 ammann et al 2019 these contributions may be grouped in two main camps with fundamentally different and opposing viewpoints on how to characterize uncertainty in the presence of epistemic errors and residual non stationarity the first camp believes in the power of statistical theory and treats uncertainty as aleatory in nature proponents of this camp insist that likelihood functions should have a solid theoretical underpinning but despite many efforts to understand and describe epistemic systematic uncertainty are often silent on how one should characterize and treat model structural errors and non stationary residual behavior the residuals display certain patterns and it is in the features of these patterns that we may distill a suitable likelihood function with roots in probability theory this camp of idealists is most likely to include theoreticians and hydrologists with a strong stem education contributions include the use of eq 6 and variations thereof sorooshian and dracup 1980 tasker 1980 kuczera 1982 stedinger and tasker 1985 bates and campbell 2001 smith et al 2010 evin et al 2013 mcinerney et al 2019 the application of advanced distribution adaptive likelihood functions reichert and mieleitner 2009 schoups and vrugt 2010 scharnagl et al 2015 ammann et al 2019 and implementation of set theoretic likelihood functions within the context of approximate bayesian computation sadegh and vrugt 2013 the second camp recognizes the current limitations of formal statistical theory in describing systematic uncertainty and takes on an alternative philosophy and approach to model evaluation this includes the use of so called pseudo likelihood functions within the glue methodology beven and binley 1992 freer et al 1996 beven and freer 2001 smith et al 2008 set theoretic likelihood functions within the context of limits of acceptability beven 2006 vrugt and beven 2018 and voting point likelihoods for rating curve estimation in the presence of aleatory and epistemic uncertainty mcmillan and westerberg 2015 hollaway et al 2018 this camp of pragmatists will include practicing hydrologists who seek ways to formalize their field expertise and knowledge of watershed characteristics and behavior into expectations about model performance in this paper we take a formal probabilistic viewpoint and focus our attention on so called distribution adaptive likelihood functions this special group of likelihood functions does not make prior assumptions about the expected distribution of the residuals rather inference takes place over the model parameters θ and space of distribution functions defined by the shape parameters of the likelihood function we label this group distribution adaptive likelihood functions after distribution free maximum likelihood cosslett 1983 and partially adaptive zeckhauser and thompson 1970 hansen et al 2006 estimation the goals of this paper are threefold first we present theory of a revised implementation of the generalized likelihood gl function of schoups and vrugt 2010 this new formulation referred to as the gl function rectifies a critical deficiency of the gl function pointed out by evin et al 2013 and guarantees a much more robust joint inference of the autoregressive coefficients and distribution of the partial residuals then as secondary goal we present a further generalization of the gl function coined the universal likelihood ul function the ul function builds on the skewed generalized student s t distribution of theodossiou 2015 and extends applicability to a much larger family of continuous probability distributions among which highly asymmetrical lepto and platy kurtic residual distributions this includes but is not limited to symmetric and skewed forms of the generalized normal generalized t mcdonald and newey 1988 laplace laplace 1774 normal student s t student 1908 cauchy lorentz poisson 1824 and uniform distribution finally as our third and last goal we present the use of strictly proper scoring rules to evaluate compare and rank the probabilistic forecasts of the likelihood functions scoring rules are a powerful alternative to more intuitive metrics such as the reliability coefficient of variation and coverage and condense the accuracy of a distribution forecast to a single penalty oriented value while retaining attractive statistical properties alexander et al 2022 the gl and ul functions are tested using synthetic data of a simple autoregressive scheme and benchmarked against the gl function student t likelihood sl of scharnagl et al 2015 and normal likelihood for a conceptual hydrologic model using measured streamflow data the remainder of this paper is organized as follows section 2 provides a problem statement and reviews the definition of the default normal likelihood function section 3 presents theory and a derivation of the gl function in this section we also revisit the skewed student t distribution of scharnagl et al 2015 which serves as central ingredient of the so called student likelihood sl function this is followed in section 4 by a detailed description of the ul function then section 5 presents the scoring rules used to evaluate compare and rank the predictive distributions derived from the different likelihood functions this is followed by the application of the gl and ul functions to two different case studies involving a simple autoregressive scheme and conceptual watershed model we conclude this paper in section 6 with a summary of our main findings to be complete in our reasoning and arguments and convey the subtleties involved in the application evaluation and ranking of distribution adaptive likelihood functions this paper is relatively heavy on statistics this is a necessary means to describing accurately the distribution of the residuals and prerequisite to diagnosing and detecting model structural errors in an effort to enhance our knowledge and understanding of the rainfall discharge relationship 2 problem statement and background let us consider a deterministic system s whose future state behavior and regularities may be described by one or more rules or principles of change which may be spatially coupled and explain its temporal evolution as most real world systems have an intractable complexity our understanding of the dynamics of the system is limited to one or more variables whose values have been measured in situ or sensed remotely over a period of time for simplicity we consider a single variable χ whose behavior is measured and collected in a n 1 vector y y 1 y 2 y n where the tilde operator denotes measured quantities if the real world system s is considered deterministic then we must be able to describe its behavior with explanatory laws for example those most fundamental and celebrated in physics let us suppose that the dynamic model y m θ x r d r n simulates the temporal behavior y y 1 y 2 y n of the variable χ of interest using as input arguments a d vector of unknown invariant system properties θ θ 1 θ 2 θ d with θ r d and array x of constants and input variables required under the supposition or hypothesis that they govern by causality the variable χ using the evolution rules of nature expressed in mathematical form these physical laws are the result of thought experiments mathematical analysis and derivation and laboratory and or field experimentation under idealized external boundary conditions a deterministic or metaphysical model scales up the physical laws a mixture of algebraic ordinary and partial differential equations to the domain of the real world system of interest to warrant spatiotemporal simulation of the governing variables of interest at the desired space time resolution in some cases the evolution rule can be simplified to a mathematical expression van geert 1994 but for most real world systems the dynamics are simply too complex to warrant an exact characterization by one or more mathematical functions the array x characterizes the system s initial state invariant properties and spatiotemporal control inputs forcing explanatory variables as our main focus is on the model parameters θ we suppress the symbol x in our subsequent notation and write instead y m θ for the vector valued form of the model with respect to θ a key task is now to determine suitable values of the parameters θ so that the model output y approximates as closely and consistently as possible the measured response y y 1 y 2 y n of variable χ as the data generating system s is considered deterministic and described by laws of nature there is no randomness in the evolution of future behavior any uncertainty in χ is due to measurement error only an inherent byproduct of the measurement process if we make the common assumption that the measurement errors of χ are random and additive 5 y y ϵ then the n vector y y 1 y 2 y n equals the unobserved true response of the deterministic system s which the model m is supposed to mimic this paper is concerned with a formal probabilistic description of the residuals e θ y m θ so as to help guide the inference to the unobserved true response y to quantify the level of confidence aka likelihood in the simulated outcome y t θ given observation y t we must hypothesize a distribution f y t of the measurement y t as the data generating process s is assumed deterministic there is no randomness in the system state and behavior at any moment in time then eq 5 implies that y t should follow the distribution of the measurement error ϵ t if we make the common and convenient assumption that e t θ is zero mean normally distributed with variance σ e 2 and thus e t θ n 0 σ e 2 then the distribution of the measurement f y t f n y t σ e 2 where f n a b c signifies the pdf of the normal distribution n b c with mean b and variance c evaluated at the simulated outcome a the likelihood of the parameters now equals the density of the normal pdf at the simulated outcome to yield l θ y t σ e 2 f n y t θ y t σ e 2 we can generalize the computation of the likelihood to a n vector of simulated output this leads to the well known gls likelihood function l θ y σ e and equals 6 l θ y σ e 1 2 π n σ e exp 1 2 e θ σ e 1 e θ where σ ϵ is the n n measurement error covariance matrix is the determinant operator n denotes the length of the training data record and the multiplicative term in front of the exponential function is the familiar normalizing constant of a normal distribution the maximum θ of l θ y σ e is also known as the maximum likelihood solution the expectation that the measurement errors of the data generating process will provide an exhaustive description of the residuals is flawed as complex systems do not admit a perfect characterization the residuals e θ y y θ will almost always be larger than the measurement errors ϵ of the measured response y the residuals are simply expected to absorb the consequences of model misspecification and inadequate characterization of the initial states system properties and controlling variables and behave analogously to the measurement errors of y this is equivalent to a residual covariance matrix σ e which is a multiple of the measurement error covariance matrix σ ϵ in the case of a scalar covariance matrix we can write σ e c σ ϵ 2 v where σ e 2 c σ ϵ 2 signifies the unknown variance of the residuals thus in practice we must submit instead to eq 6 the n n covariance matrix of the residuals σ e so that the standardized and decorrelated residuals satisfy the assumptions of a zero mean and constant variance of the gauss markov theorem we can slip into usage of the terminology measurement error but this characterization is imprecise in the present context e g see page 20 of beven 2006 the multivariate form of the gls likelihood in eq 6 permits treatment of residual autocorrelation through the off diagonal terms of the residual covariance matrix σ e for example for a first order autoregressive process e t θ ϕ 1 e t 1 θ ɛ t with zero mean normally distributed partial residuals ɛ t n 0 σ ɛ 2 with variance σ ɛ 2 the residual covariance matrix σ e may be factorized to a product σ e l ϕ 1 u ϕ 1 σ ɛ 2 1 of a lower and upper triangular matrix l ϕ 1 and u ϕ 1 σ ɛ 2 respectively which depend only on the autoregressive coefficient ϕ 1 and or σ ϵ 2 and the determinant σ e σ ϵ 2 n 1 ϕ 1 2 unfortunately not all surmised univariate residual distributions support a closed form multivariate expression for the joint likelihood this is particularly true for the nontraditional residual distributions discussed herein then we must treat serial correlation prior to computation of the marginal and joint likelihoods 3 generalized likelihood function the gls likelihood function has found widespread application and use yet involves the rather questionable assumption that the residuals are normally distributed power transformations such as the box and cox 1964 transform may help stabilize the variance of the data and model simulations and make their respective marginal distributions more normal distribution like yet the power and or shift parameters of this transformation are not unambiguously defined mcinerney et al 2017 and the transformation does not necessarily protect against heavy tailed residuals bates and campbell 2001 yang et al 2007 distribution adaptive likelihood functions such as the gl function of schoups and vrugt 2010 allow us to relax the assumption of normality and describe any arbitrary residual distribution with non constant variance and or different degrees of skew and kurtosis unlike the box cox transformation which simultaneously affects several moments of the residual distribution ammann et al 2019 the gl function separately treats residual heteroscedasticity skewness and kurtosis with parameters that have a direct relation to residual statistics and can be deduced from diagnostic plots schoups and vrugt 2010 this not only lets the measured data y speak for itself but also leads to a more robust characterization of model parameter and predictive uncertainty a skewed residual distribution will for instance improve the probabilistic description of truncated variables say near zero flows in a semiarid watershed schoups and vrugt 2010 in this section we present a revised formulation of the gl function of schoups and vrugt 2010 which enacts treatment of serial correlation on homogenized rather than non homogenized heteroscedastic residuals this revised formulation simplifies the joint inference of the parameters θ and probabilistic properties of the residuals the theoretical derivation below serves as precursor to the ul function which is presented in a subsequent section 3 1 the standardized skew exponential power sep density function the gl function of schoups and vrugt 2010 builds on the standardized skew exponential power sep density function which combines the distribution of subbotin 1923 also known as the exponential power ep or generalized normal distribution box and tiao 1992 7 f ep a 0 1 β ω β exp c β a 2 1 β and the template density f skew a ξ for skewed distributions of fernandez and steel 1998 8 f skew a ξ 2 ξ ξ 1 f a ξ sign a to yield see appendix a 9 f sep a 0 1 β ξ 2 σ ξ ω β ξ ξ 1 exp c β μ ξ σ ξ a ξ sign μ ξ σ ξ a 2 1 β where denotes the absolute value or modulus operator sign x x x is the signum function and the scalars c β ω β μ ξ and σ ξ are a function of the kurtosis β 1 1 and skewness ξ r using eqs a 2a a 2b a 7a and a 7b respectively appendix a completes the derivation of the sep density in schoups and vrugt 2010 with a detailed mathematical treatment of the mean μ ξ and variance σ ξ 2 of the sep distribution to provide insights into the functional shape of the standardized sep density of eq 9 please consider fig 1 which presents a graph of f sep a 0 1 β ξ for a 3 3 using different values of the a kurtosis β and b skewness ξ as is evident from the two graphs the density of the standardized sep distribution at point a is controlled by the values of β and ξ the density is symmetric for ξ 1 positively skewed for ξ 1 and negatively skewed for ξ 1 for a symmetric density that is ξ 1 a value of β 1 results in a uniform distribution β 0 produces a normal distribution and β 1 equals a double exponential or laplace distribution thus a value of β 0 1 produces a sep distribution with much heavier tails than the normal density such leptokurtic distributions help protect the model parameter estimates from outliers 3 2 treatment of residual autocorrelation the standardized sep density of eq 9 enables a fluent description of the distribution of the residuals before we can proceed however with the derivation of the sep likelihood function we are in need of an adequate description of the temporal structure of the raw residuals e θ schoups and vrugt 2010 expressed serial correlation as an ar k process of the raw residuals e θ as follows 10 e t θ j 1 k ϕ j e t j θ σ e t ɛ t where φ k ϕ 1 ϕ 2 ϕ k is a k vector of autoregressive coefficients σ e t signifies the standard deviation of the t th raw residual ɛ t θ δ denotes the ar k decorrelated residual and δ β ξ φ k is a vector of nuisance variables 1 1 a nuisance variable is fundamental to the probabilistic model of concern which is not of immediate interest but must be accounted for in the inference the entries of the n 1 vector ɛ θ δ ɛ 1 θ δ ɛ 2 θ δ ɛ n θ δ are also referred to as innovations or partial residuals the autoregressive operator in eq 10 operates on the non standardized raw residuals e θ this implementation is not recommended as the autoregressive operator of eq 10 expects use of homogenized residuals evin et al 2013 convincingly demonstrated that residual standardization should precede the treatment of serial correlation this guarantees a more robust inference of the model parameters and or nuisance variables and consequently may lead to sharper prediction intervals therefore and as in steinschneider et al 2015 and hernández lópez and francés 2017 we admit standardized raw residuals e t θ δ with unknown mean and variance σ e 2 instead 11 e t θ δ e t θ σ ϵ t t 1 2 n and conveniently limit our mathematical description of their structural dependence to an ar 2 process 12 e t θ δ ϕ 1 e t 1 θ δ ϕ 2 e t 2 θ δ ɛ t where the partial residuals ɛ t θ δ sep 0 σ ɛ 2 β ξ are assumed to be independent and zero mean sep distributed with variance σ ɛ 2 skewness β and kurtosis ξ for all t 3 4 n and the single line underneath the raw residuals articulates their standardized dimensionless counterparts e θ δ e 1 θ δ e 2 θ δ e n θ δ note that the standardized raw residuals support the use of more elaborate ar models for example with seasonally varying coefficients or ar coefficients that depend on exogenous variables ammann et al 2019 this is not uncommon in statistical hydrology and could be advised if model performance depends on time season and or the governing processes the ar 2 process of eq 12 should typically suffice in removing serial dependencies of the standardized raw residuals otherwise one could always implement a higher order ar k process with k 2 five remarks are in order first the recursion in eq 12 demands specification of so called initial conditions e 1 θ δ and or e 0 θ δ for computation of the first two partial residuals ɛ 1 θ δ and ɛ 2 θ δ these initial conditions not only determine the values of ɛ 1 θ δ and ɛ 2 θ δ but also their respective marginal distributions a second and related comment the moments of the sep innovations ɛ t θ δ are not invariant under the transformation of eq 12 thus the standardized raw residuals e t θ δ do not inherit the variance σ ɛ 2 skewness β and kurtosis ξ of the partial residuals a change of variance thus σ e 2 σ ɛ 2 is even expected with normally distributed innovations but the variant skew and kurtosis are the byproduct of the use of non gaussian innovations third eq 11 does not guarantee a zero mean and unit variance of the standardized raw residuals in fact the variance of the standardized raw residuals will almost always be larger than one as the magnitude of the e t θ δ s will exceed on average the measurement error standard deviations σ ϵ t of the training data y t for t 1 2 n we can inflate the σ ϵ t s so as to absorb all other sources of systematic and aleatory uncertainty and impose a unit variance of the standardized raw residuals σ e 2 1 in this case we should replace the σ ϵ t s with σ e t the standard deviation of the t th residual fourth it is quite common to replace the denominator of eq 11 with estimates s ϵ t of the measurement error standard deviation σ ϵ t in doing so the e t θ δ s in eq 11 should be referred to as studentized residuals 2 2 in statistics a studentized residual is equal to a residual divided by an estimate of its standard deviation if instead we divide by the population standard deviation then we speak of a standardized residual we will use the terminology studentized residual throughout the remainder of this paper as this characterizes better the application of eq 11 lastly the use of s ϵ t rather than σ ϵ t in the denominator of eq 11 associates the nuisance variables δ to the studentized raw residuals in a manner that will become explicit in section 3 4 therefore we write e θ δ e 1 θ δ e 2 θ δ e n θ δ for the studentized raw residuals next we must establish a relationship between the variance σ ɛ 2 of the innovations ɛ t θ δ and the unconditional or marginal variance σ e 2 of the studentized raw residuals the derivation of this relationship can be found in statistical text books and relies on variance decomposition of eq 12 to yield 13 σ ɛ 2 1 ϕ 2 1 ϕ 1 ϕ 2 1 ϕ 1 ϕ 2 1 ϕ 2 σ e 2 the quotient on the right hand side is on the unit interval and thus we yield that σ e 2 σ ɛ 2 unless ϕ 1 0 and ϕ 2 0 then σ ɛ 2 σ e 2 if ϕ 2 0 the above expression reduces to σ ɛ 2 1 ϕ 1 2 σ e 2 the familiar expression for the relationship between the variance of an ar 1 process σ e 2 and the variance σ ɛ 2 of the partial residuals 3 3 derivation of the sep likelihood function if we consider the values of the s nuisance variables δ β ξ φ 2 to be known a priori then the joint likelihood l θ y δ of the d model parameters θ θ 1 θ 2 θ d 14 l θ y δ p y δ θ p e δ θ equals the joint pdf of the training data y δ and θ to compute the likelihood of the n vector of raw residuals e θ we must make assumptions about the so called initial conditions of the ar 2 model in eq 12 the initial conditions e 1 θ y 1 y 1 θ and e 0 θ y 0 y 0 θ not only determine after studentization with eq 11 the values of the first two partial residuals ɛ 1 θ δ and ɛ 2 θ δ but also control the next successive entries of ɛ θ δ via the recursion in eq 12 if the measured data y 1 and y 0 and model output y 1 θ and y 0 θ are known at t 1 and t 0 immediately preceding the first entry of the training data record then the so called initial conditions equal e 1 y 1 y 1 θ and e 0 y 0 y 0 θ otherwise the user can fix the raw residuals at some default value say e 1 θ 0 and e 0 θ 0 in this latter case we do not adapt our notation and carry forward the dependence of the initial condition on the assumed parameter values the joint distribution of the raw residuals p e δ θ satisfies the following identity 15 p e θ p e θ δ e θ δ e θ where p e θ δ is the joint distribution of the studentized raw residuals and e θ δ e θ signifies the jacobian of the transformation from the studentized raw residuals e θ δ to the raw residuals e θ the joint distribution of the studentized raw residuals p e θ δ equals the product of the marginal densities p e 1 e 0 e 1 θ δ and p e 2 e 1 e 0 θ δ of e 1 θ δ and e 2 θ δ respectively and the conditional densities p e t e t 1 e t 2 θ δ of the remaining n 2 entries e 3 θ δ e n θ δ of e θ δ to yield 16 p e θ δ p e 1 e 0 e 1 θ δ p e 2 e 1 e 0 θ δ p e 3 e n e 0 e 1 θ δ p e 1 e 0 e 1 θ δ p e 2 e 1 e 0 θ δ t 3 n p e t e t 1 e 0 e 1 θ δ p e 1 e 0 e 1 θ δ p e 2 e 1 e 0 θ δ t 3 n p e t e t 1 e t 2 θ δ if we make the convenient assumption that the partial residuals ɛ θ δ follow a normal distribution with zero mean and variance σ ɛ 2 then the marginal distributions of e 1 θ δ and e 2 θ δ are gaussian as well which supports an exact closed form expression for the likelihood function of the raw residuals yet sep innovations ɛ θ δ sep 0 σ ɛ 2 β ξ do not admit a closed form description for p e 1 e 0 e 1 θ δ and p e 2 e 1 e 0 θ δ as the ar 2 process of eq 12 does not preserve higher order moments of the sep distribution such as its skew and kurtosis damsleth and el shaarawi 1989 hürlimann 2012 we may resort to the law of total cumulance to determine the expected skew and kurtosis of e 1 θ δ and e 2 θ δ yet such estimates must be turned into values of β and ξ to warrant a closed form description of p e 1 e 0 e 1 θ δ and p e 2 e 1 e 0 θ δ respectively henceforth we follow schoups and vrugt 2010 and condition on the unobserved residuals e 1 θ and e 0 θ to yield 17 p e 1 e 0 e 1 θ δ p e 2 e 1 e 0 θ δ p e 1 e 2 e 1 e 0 θ δ t 1 2 p e t e t 1 e t 2 θ δ this approximation is valid only for moderate to large training data records y if we now substitute eq 17 into 16 and enter the resulting expression into eq 15 we yield the following expression for the conditional likelihood function 18 l θ y δ e θ δ e θ t 1 n p e t e t 1 e t 2 θ δ t 1 n 1 σ ϵ t p e t e t 1 e t 2 θ δ according to the ar 2 process of eq 12 the conditional distribution of e t θ δ given e t 1 θ δ and e t 1 θ δ equals the marginal distribution of the partial residuals ɛ t θ δ for all t 1 2 n hence we can now write 19 l θ y δ t 1 n 1 σ ϵ t f sep ɛ t 0 σ ɛ 2 β ξ this formulation poses computational difficulties as we do not have available a mathematical expression for the non standardized sep distribution with variance σ ɛ 2 1 fortunately we can take advantage of the following identity 20 f sep a 0 σ a 2 β ξ 1 σ a f sep a 0 1 β ξ and thus the non standardized sep density f sep a 0 σ a 2 β ξ at point a with var a σ a 2 is equivalent to a multiple σ a 1 of the standardized sep density f sep a 0 1 β ξ evaluated at the standardized a value a a σ a if we enter eq 20 into eq 19 then we end up with the following general formulation for the sep likelihood function of the raw residuals 21 l θ y δ t 1 n 1 σ ϵ t 1 σ ɛ f sep ɛ t 0 1 β ξ using standardized partial residuals ɛ t ɛ t θ δ σ ɛ for all t 1 2 n we purposely use the terminology standardized partial residual as the denominator σ ɛ equals the theoretical standard deviation of the ar 2 innovations in eq 13 if we admit the standardized sep density f sep 0 1 β ξ of eq 9 to eq 21 then we arrive at the sep likelihood function 22 l θ y β ξ φ 2 σ ϵ 2 t 1 n 1 σ ϵ t 1 σ ɛ 2 σ ξ ω β ξ ξ 1 exp c β μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 1 β where c β ω β μ ξ and σ ξ are a function of the kurtosis β and skewness ξ according to eqs a 2a a 2b a 7a and a 7b respectively ɛ t θ δ signifies the t th standardized partial residual σ ɛ 2 is the variance of the ar 2 process of eq 12 and δ β ξ φ 2 it is important to keep in mind that the sep likelihood function is not exact but conditional upon the initial conditions e 1 θ and e 0 θ eq 22 but in modified form was coined the generalized likelihood gl function by schoups and vrugt 2010 as it extends applicability to situations wherein the raw residuals e θ deviate from normality and exhibit skew kurtosis heteroscedasticity and possibly serial correlation at one or more lags if as in eq 10 of schoups and vrugt 2010 the treatment of serial correlation precedes residual studentization then the multiplicative term 1 σ ɛ will disappear from eq 22 this original implementation is herein referred to as the gl function and the revised formulation of the sep likelihood function in eq 22 is coined the gl function finite multiplication of the n likelihoods in eq 22 is susceptible to arithmetic underflow resulting in a zero value of l θ y β ξ φ 2 σ ϵ 2 for reasons of numerical stability it is therefore more convenient to work with the sep log likelihood function l θ y β ξ φ 2 σ ϵ 2 instead 23 l θ y β ξ φ 2 σ ϵ 2 n log 2 σ ξ ω β n log ξ ξ 1 1 2 t 1 n log σ ϵ t 2 n 2 log σ ɛ 2 c β t 1 n μ ξ σ ξ ϵ t θ δ ξ sign μ ξ σ ξ ϵ t θ δ 2 1 β where σ ɛ 2 is equal to eq 13 for the ar 2 process of the standardized raw residuals 3 4 treatment of nuisance variables and measurement data errors the sep log likelihood function l θ y β ξ φ 2 σ ϵ 2 above assumes prior knowledge of the s 4 nuisance variables δ β ξ φ 2 and the n vector of measurement error variances σ ϵ 2 σ ϵ 1 2 σ ϵ 2 2 σ ϵ n 2 of the training data y y 1 y 2 y n in the absence of detailed knowledge about the expected distribution of the residuals and or measurement errors of y we can choose to estimate the nuisance variables δ along with the model parameters θ this approach does not change anything to the mathematical formulation of the gl function in eq 23 except the nuisance variables migrate to the left side of the vertical bar in the parent definition of the sep log likelihood function to read instead l θ β ξ φ 2 y σ ϵ 2 or l θ δ y σ ϵ 2 this alternative definition frees us from having to make strong and often questionable prior assumptions about the distribution of the residuals the nuisance variables let the residuals speak for themselves note that nuisance variables are also referred to as hyperparameters when they relate to the prior distribution of the model parameters we can further generalize the sep log likelihood function of eq 23 to situations without knowledge of the training data measurement errors σ ϵ 2 a pragmatic remedy is to relate the measurement error to the simulated data schoups and vrugt 2010 evin et al 2013 as follows 24 s ϵ t s 0 s 1 y t θ where s ϵ t is an estimate of the measurement error standard deviation of the t th simulated output y t θ and s 0 0 and s 1 0 1 are unknown coefficients that define the intercept and slope of the measurement error function respectively with s 1 0 the expression above reduces to a homoscedastic measurement error function with variance equal to s 0 2 in the original gl function of schoups and vrugt 2010 the slope and intercept of eq 24 were treated as two separate nuisance variables each having their own prior distribution this implementation would be fine with the application of eqs 12 and 13 so that the variance of the partial residuals σ ɛ 2 is directly proportional to the variance of the standardized raw residuals σ e 2 in the gl function we simplified the implementation to include only a single nuisance variable s 0 the slope s 1 is evoked as a phantom variable to enforce a unit variance σ e 2 1 of the studentized raw residuals e θ δ if m e and s e 2 denote the sample mean and sample variance of the studentized raw residuals respectively 25 m e 1 n t 1 n e t θ δ and s e 2 1 n 1 t 1 n e t θ δ m e 2 then the zero point of the so called residual function ħ s 1 θ δ 1 s e 2 equals the value of s 1 for which s e 2 1 the secant method can efficiently solve this root finding problem in only a handful of iterations thus for a given value of the intercept s 0 of eq 24 the secant method finds the value of the slope s 1 so that σ e 2 1 this approach increases the number of nuisance variables with only one namely δ β ξ φ 2 s 0 and the sep log likelihood function becomes 26 l θ β ξ φ 2 s 0 y n log 2 σ ξ ω β n log ξ ξ 1 t 1 n log s 0 s 1 y t θ n 2 log σ ɛ 2 c β t 1 n μ ξ σ ξ ϵ t θ δ ξ sign μ ξ σ ξ ϵ t θ δ 2 1 β as our result will demonstrate our definition of s 1 as phantom variable rather than nuisance variable is inconsequential both methods produce equivalent results the root finding procedure is not limited to measurement errors with a non constant variance but could in principle also work for homoscedastic measurement errors the value of s 0 will then determine whether the zero point of ħ s 1 θ δ is feasible or not a simpler solution in this case that avoids using any nuisance variable for the measurement error function of eq 24 after all is to set s 0 equal to the sample standard deviation of the raw residuals that is s 0 s e 2 the inference of the slope and intercept of the measurement error function is however not without problems if the model is not able to describe sufficiently closely the observed data y then joint inference of s 0 and ϕ 1 s 1 is conditioned on s 0 via the constraint that σ e 2 1 may result in unrealistically large values of s 0 and or s 1 while simultaneously promoting ϕ 1 1 so as to downplay the role of the model m θ in describing the measured data and maximize the sep log likelihood of eq 26 unfortunately there is nothing we can do about this if the raw residuals e θ are large in magnitude then s 0 and or s 1 must be large as well so as to honor the unit variance of the studentized raw residuals e θ δ instead we must find ways to remove model bias and improve the overall model data consistency forcing data errors will certainly contribute significantly to the model bias and simulation uncertainty table 1 summarizes the nuisance variables of the gl function and lists their lower and upper bounds and default values if the measurement error variances σ ϵ 2 are known a priori then the sep log likelihood function with ar 2 description of the standardized raw residuals l θ β ξ φ 2 y σ ϵ 2 is equal to eq 23 with φ 2 ϕ 1 ϕ 2 and σ ɛ 2 equal to eq 13 if the n measurement error variances σ ϵ 2 of the training data record y are unknown then we can resort to eq 26 this formulation l θ β ξ φ 2 s 0 y requires specification of the measurement error intercept s 0 in the case of heteroscedastic errors this increases the number of nuisance variables to five δ β ξ ϕ 1 ϕ 2 s 0 if an ar 1 is assumed instead then ϕ 2 0 and σ ɛ 2 1 ϕ 1 2 in eqs 23 and 26 3 5 special cases of the sep log likelihood function if so desired we can fix the skew β and kurtosis ξ of the sep log likelihood function to some default values to satisfy prior assumptions for β 0 and ξ 1 eq 26 simplifies to the well known gaussian log likelihood l θ φ 2 s 0 y 0 1 of an ar 2 process indeed without skew and excess kurtosis eqs a 2a a 2b a 7a and a 7b result in c β 1 2 ω β 2 π 1 2 μ ξ 0 and σ ξ 1 if we enter these values into the sep log likelihood function we yield 27 l θ φ 2 s 0 y β 0 ξ 1 n 2 log 2 π t 1 n log s 0 s 1 y t θ n 2 log σ ɛ 2 1 2 t 1 n ϵ t θ δ 2 which is equal to a conditional gaussian log likelihood function with constant and or nonconstant measurement errors and ar 2 process of the studentized raw residuals this expression follows directly from the gls likelihood function of eq 6 for ϕ 1 0 and ϕ 2 0 the above expression simplifies further to a normal log likelihood function conditional upon the unobserved raw residuals e 1 θ and e 0 θ and non constant variance of the raw residuals in the remainder of this paper the normal log likelihood of eq 27 will be referred to as nl function for β 1 and ξ 1 the sep log likelihood function simplifies to 28 l θ φ 2 s 0 y β 1 ξ 1 n 2 log 2 t 1 n log s 0 s 1 y t θ n 2 log σ ɛ 2 2 t 1 n ϵ t θ δ which is equal to a laplacian log likelihood l θ φ 2 s 0 y β 1 ξ 1 of an ar 2 process this concludes our description of the gl function 3 6 the skewed student s t likelihood function the gl and gl functions rely on the generalized normal or ep distribution to better characterize nontraditional residual distributions with skew kurtosis and heavier tails than the normal distribution scharnagl et al 2015 replaced the ep distribution of eq 7 with a standardized student s t density 29 f st a 0 1 ν γ ν 1 2 γ ν 2 1 π ν 2 1 a 2 ν 2 ν 1 2 where ν 2 denotes the degrees of freedom if we combine eq 29 with the skew density of eq 8 then we yield the following expression for the standardized skewed student s t sst density scharnagl et al 2015 30 f sst a 0 1 ν ξ 2 σ ξ ξ ξ 1 γ ν 1 2 γ ν 2 π ν 2 1 1 ν 2 μ ξ σ ξ a ξ sign μ ξ σ ξ a 2 ν 1 2 with skewness parameter ξ 0 and shift and scale constants μ ξ and σ ξ 2 respectively that depend on ν and ξ and standardize the sst density scharnagl et al 2015 closed form expressions for μ ξ and σ ξ 2 are presented in eqs b 7a and b 7b of appendix b fig 2 plots the density of the standardized skewed student t distribution of eq 30 for a 3 1 2 3 1 2 using different values of the a skewness ξ and b degrees of freedom ν the sst density is positively skewed for ξ 1 and negatively skewed for ξ 1 for ξ 1 the student s t distribution is symmetric and bell shaped albeit with much heavier tails than the normal distribution for small values of ν these so called leptokurtic distributions are more peaked than a normal distribution and have stronger tails for large values of ν the student s t distribution converges to the standard normal distribution zero mean and unit standard deviation thus the value of ν 2 determines the kurtosis and heaviness of the tails of the sst distribution if we assume an ar 2 process of the standardized raw residuals and treat measurement data errors as in section 3 4 then we yield the log likelihood function l θ s 0 ν ξ σ ɛ 2 y of the sst density of scharnagl et al 2015 see appendix b 31 l θ s 0 ν ξ φ 2 y n 2 log σ ɛ 2 t 1 n log s 0 s 1 y t θ n log 2 n log σ ξ n log γ ν 1 2 n log ξ ξ 1 n log γ ν 2 n 2 log π n 2 log ν 2 ν 1 2 t 1 n log 1 1 ν 2 μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 where s 1 equals the zero point of the residual function ħ s 1 θ δ 1 s e 2 so as to enforce a unit variance of the studentized raw residuals the sst log likelihood is referred to as student likelihood or sl function in the remainder of this paper table 2 summarizes the nuisance variables of the sl function the default value of the degrees of freedom ν is set equal to the sample size n minus the number d of estimated parameters θ note that for ξ 1 and ν the sl function reduces to the nl function of eq 27 4 the universal likelihood function the skewed generalized student s t sgt distribution of theodossiou 1998 is a further generalization of the sst distribution in eq 30 to a family of continuous probability distributions which includes the generalized student s t mcdonald and newey 1988 hansen 1994 error theodossiou 2015 and exponential power distributions subbotin 1923 box and tiao 1992 the skewed and symmetric laplace cauchy student s t and normal distributions see e g johnson et al 1995 and the uniform distribution the pdf of the non standardized sgt distribution is given by theodossiou 2015 32 f sgt a μ σ λ p q p 2 κ λ p q σ b 1 p q p 1 a μ μ λ p q κ λ p q σ 1 λ sign a μ μ λ p q p q 1 p where μ σ 0 and λ 1 1 are location scale and skewness parameters p 0 and q 0 control the kurtosis of the distribution μ λ p q and κ λ p q are shift and scale constants respectively that depend on λ p and q and b a b is the so called beta function or euler integral of the first kind 33 b a b 0 1 x a 1 1 x b 1 d x and is equivalent to the following expression 34 b a b γ a γ b γ a b using the gamma function γ in eq a 3 the scalars μ λ p q and κ λ p q negate changes in the mean μ and variance σ 2 of the sgt distribution imposed by the variables p q and λ in the original parameterization of theodossiou 1998 the shift and scale constants are defined as given in eqs 35a and 35b see box i and yield a sgt distribution with mean μ and variance σ 2 if p q 2 if we admit μ 0 and σ 2 1 then we yield the standardized sgt density 36 f sgt a 0 1 λ p q p 2 κ λ p q b 1 p q p 1 a μ λ p q κ λ p q 1 λ sign a μ λ p q p q 1 p the sgt distribution of eq 36 with the closed form expressions for μ λ p q and κ λ p q in eqs 35a and 35b nests a number of commonly used univariate probability distributions including but not limited to the generalized student s t distribution mcdonald and newey 1988 the generalized error distribution 3 3 also known as the generalized normal distribution or ep distribution and the symmetric and skewed student s t laplace cauchy lorentz and normal distributions see table 3 and appendix c to provide insights into the functional form of eq 36 fig 3 displays the standardized sgt density f sgt a 0 1 λ p q for 3 1 2 a 3 1 2 for different values of the a kurtosis p 0 b skewness 1 λ 1 c kurtosis q 0 and d p β and q the sgt distribution can be symmetric and highly skewed for λ 1 0 the distribution is negatively skewed to the left and for λ 0 1 the sgt distribution is positively skewed to the right for λ 0 the distribution is symmetric the parameters p 0 and q 2 control the kurtosis of the sgt distribution the smaller the values of p and q the more slender the sgt distribution will be and thus the larger its kurtosis a distribution with kurtosis that exceeds the value of three is also called leptokurtic and will have fatter tails than a normal distribution for large values of p and q the excess kurtosis of the sgt distribution disappears in lieu of a much more uniform distribution of the probability density the parameter p exerts the largest control on the kurtosis as is evident by the large variation in sgt densities for different values of p the impact of q on the sgt distribution is more subtle and its impact decreases substantially with large values of q altogether small values of p and or q produce a sgt distribution with much heavier tails than the normal density the bottom panel demonstrates the ability of the standardized sgt density to describe a wide variety of distributions the graph plots four different parameterizations color red of the standardized sgt distribution note that in each graph the mean of the sgt distribution equals zero and more difficult to see the variance equals one the bottom right graph presents eight of the special parameterizations of the sgt distribution listed in table 3 among which the ep distribution of eq 7 in red the ability of the sgt distribution to morph into a wide variety of well known probability distributions has desirable advantages for residual characterization and description henceforth the forthcoming likelihood function stemming from the sgt distribution is referred to as the universal likelihood function 4 1 derivation of sgt log likelihood function if we treat the sgt parameters λ p and q as nuisance variables and assume further that the studentized raw residuals e θ δ exhibit serial correlation according to the ar 2 scheme of eqs 12 then the sgt likelihood should follow our derivation of the sep likelihood function specifically according to eq 18 we may write 37 l θ λ p q φ 2 y σ ϵ 2 t 1 n 1 σ ϵ t p e t e t 1 e t 2 θ δ and thus we yield 38 l θ λ p q φ 2 y σ ϵ 2 t 1 n 1 σ ϵ t f sgt ɛ t θ δ 0 σ ɛ 2 λ p q t 1 n 1 σ ϵ t 1 σ ɛ f sgt ɛ t θ δ 0 1 λ p q σ ɛ n t 1 n p 2 σ ϵ t κ λ p q b 1 p q p 1 ɛ t θ δ μ λ p q κ λ p q 1 λ sign ɛ t θ δ μ λ p q p q 1 p where f sgt ɛ t θ δ 0 1 λ p q returns the density at ɛ t θ δ of the standardized sgt distribution in eq 36 ɛ θ δ ɛ 1 θ δ ɛ 2 θ δ ɛ n θ δ denotes the n vector of standardized partial residuals and σ ɛ 2 is the variance of the partial residuals ɛ θ δ ɛ 1 θ δ ɛ 2 θ δ ɛ n θ δ computed using eq 13 if we take the natural logarithm of eq 38 then we yield the sgt log likelihood function 39 l θ λ p q φ 2 y σ ϵ 2 n 2 log σ ɛ 2 t 1 n log σ ϵ t n log p n log 2 n log κ λ p q n log b 1 p q p q 1 p t 1 n log 1 ɛ t θ δ μ λ p q κ λ p q 1 λ sign ɛ t θ δ μ λ p q p where the natural logarithm of the beta function b a b follows from eq 34 40 log b a b log γ a log γ b log γ a b the sgt log likelihood functions in eqs 38 and 39 assume prior knowledge of the measurement error variances σ ϵ 2 of the training data record y if the n entries of σ ϵ 2 are unknown then we can resort to the measurement error function of eq 24 and treat the intercept s 0 as an additional nuisance variable then the sgt log likelihood function becomes 41 l θ s 0 λ p q φ 2 y n 2 log σ ɛ 2 t 1 n log s 0 s 1 y t θ n log p n log 2 n log κ λ p q n log b 1 p q p q 1 p t 1 n log 1 ɛ t θ δ μ λ p q κ λ p q 1 λ sign ɛ t θ δ μ λ p q p where s 1 is computed using root finding of the residual function ħ s 1 θ δ 1 s e 2 with newton s method for the ar 2 process of the raw studentized residuals in eq 12 the variance σ ɛ 2 of the sgt innovations equals eq 13 if the second autoregressive coefficient ϕ 2 is fixed at zero then the variance of the partial residuals ɛ θ δ simplifies to σ ɛ 2 σ e 2 1 ϕ 1 2 the variance of the innovations of an ar 1 process in the remainder of this paper we refer to eq 41 as the universal likelihood ul function table 4 summarizes the nuisance variables of the ul function including symbol units prior ranges and default values three remarks are in order first the product p q of the kurtosis parameters must always exceed two otherwise the scale constant κ λ p q is not defined second the upper bounds of p and q are arbitrary they should just be set large enough so as to benefit from the versatility of the sgt distribution third the default values of λ 0 p 2 and q 1 0 10 imply a normal distribution see table 3 4 2 special and or limiting cases of the sgt log likelihood function the two formulations of the ul function l θ λ p q φ 2 y σ ϵ 2 and l θ s 0 λ p q φ 2 y assume joint inference of the skewness λ and kurtosis p and q of the partial residuals and the model parameters θ if the partial residuals ɛ θ δ should follow some known distribution then we can set one or more nuisance variables at a default value for example if the ɛ t θ δ s are expected to follow a student s t distribution then one can fix λ 0 and p 2 see fig 3d and table 3 and infer the remaining nuisance variables q s 0 and or ϕ 1 and or ϕ 2 of the sgt log likelihood function with the d model parameters θ note that in this case the shift constant μ λ p q in eq 35a equals μ 02 q 0 the scale constant κ 02 q q 2 and the sgt log likelihood function of eq 41 reduces to the sst log likelihood of eq 31 with ξ 1 this is only one of many special cases of the sgt function as discussed in appendix c the more common case of normally distributed partial residuals deserves specific attention in appendix d in summary the sgt distribution is the key ingredient of a family of likelihood functions which includes the normal likelihood function thus the ul function makes obsolete other commonly used likelihood functions in dream suite 5 case studies in this section we illustrate the application of the gl gl sl and ul functions to two case studies involving a simple autoregressive scheme and a conceptual watershed model we first detail the methodological steps and then present the individual studies and their findings 5 1 posterior approximation unless we use a conjugate prior the gl gl sl and ul functions l θ δ y do not admit a closed form solution for the posterior parameter distribution p θ δ y p θ δ l θ δ y we therefore resort to numerical integration gelfand and smith 1990 gilks et al 1996 and infer the joint posterior distribution of the d model parameters θ and s nuisance variables δ using markov chain monte carlo simulation with dream suite a windows based implementation of the differential evolution adaptive metropolis algorithm vrugt et al 2008 2009 vrugt 2016 and variants thereof vrugt and ter braak 2011 laloy and vrugt 2012 vrugt and sadegh 2013a sadegh et al 2015 vrugt and beven 2018 the transition kernel of dream creates multiple different sequences chains of parameter vectors that are stationary and ergodic and have as joint distribution p θ δ y ter braak 2006 vrugt et al 2009 convergence of the joint chains to a stationary distribution is monitored using several different diagnostic measures as advocated by cowles and carlin 1996 this includes single chain and multi chain diagnostics among which the convergence metrics of raftery and lewis 1992 and geweke 1992 and the univariate and multivariate scale reduction factors r d and r of gelman and rubin 1992 and brooks and gelman 1998 respectively the m posterior vectors of the joint markov chains are stored column wise in a d s m matrix θ δ 5 2 performance of optimal parameter values the maximum a posteriori map parameter values θ δ equal the columns of θ δ 4 4 the semicolon implies vertical concatenation thus matrix δ is concatenated to the bottom of matrix θ which maximize the unnormalized posterior density p θ δ y the performance of the map point predictions may be quantified using different summary measures also known as scoring functions we use the root mean square error 42 rmse 1 n t 1 n y t y t θ 2 and percentage bias 43 pbias 100 t 1 n y t θ y t t 1 n y t furthermore we also perform diagnostic checks of the residuals of the map solution specifically we investigate the homoscedasticity distribution autocorrelation function and quantiles of the standardized partial residuals ɛ θ δ of the map solution the sample autocorrelation ρ ɛ k of two standardized partial residuals ɛ i θ δ and ɛ j θ δ a distance time k i j apart may be computed using 44 ρ ɛ k cov ɛ i θ δ ɛ j θ δ var ɛ i θ δ 1 n k i k 1 n ɛ i θ δ m ɛ ɛ i k θ δ m ɛ 1 n k i k 1 n ɛ i θ δ m ɛ 2 where m ɛ 1 n t 1 n ɛ t θ δ is the unitless mean of the n record of standardized partial residuals and k 1 2 n 1 5 3 parameter and predictive uncertainty parameter uncertainty is computed following eq 2 by truncating α 2 of the left and right tails of the posterior samples θ δ to yield 100 1 α credible intervals c α a b of the parameters and nuisance variables stationarity and ergodicity of the sampled chains of dream suite imply that the predictions y 1 y 2 y m of the m posterior parameter vectors θ δ have as invariant distribution the posterior predictive pdf p y y and cdf f y y in eqs 3 and 4 respectively the predictive distribution p y y of eq 3 can be obtained by marginalizing each model prediction over the posterior distribution a detailed algorithmic recipe on how to do this is provided in appendix e using the posterior realizations of the gl function this supplement also returns predictive percentiles y t α 2 and y t 1 α 2 which together define the 100 1 α prediction interval of p y t y where t 1 2 n 5 4 comparison and ranking of predictive distributions we use so called scoring rules to evaluate the quality of the predictive distributions derived from the different likelihood functions scoring rules condense the accuracy of a distribution forecast to a single penalty oriented value while retaining attractive statistical properties alexander et al 2022 if ω r denotes the set of possible values of the quantity of interest y and f is a convex class of probability distributions on ω then a scoring rule s f y is a function 45 s f ω r that assigns numerical values to pairs of forecasts f f and observations y ω 5 5 we use the terminology forecasts for the residual corrupted model output y j y j θ e j δ r n m of the posterior realizations j 1 2 m strictly speaking these outcomes are not forecasts as the simulated output y j θ assumes knowledge of the exogenous variables based on early recommendations by brier 1950 and shuford et al 1966 we restrict attention to proper scoring rules the statistical implications of which have been discussed in the literature by gneiting and raftery 2007 a scoring rule is proper relative to f if the expected score 46 s f g ω s f y d g y is minimized for f g and thus s g g s f g for all probability distributions f g f krüger et al 2021 a score rule is strictly proper relative to the class f if the above holds with equality only if f g lerch et al 2017 such strictly proper scoring rules have an important advantage over proper scoring rules in statistical jargon a strictly proper score rule is a sufficient condition whereas a proper score rule is a necessary but not sufficient condition in plain words if s f g is a strictly proper score rule then the smaller its value the closer the distribution of f will be to that of g this is not true for proper scoring rules which can attain a perfect score even if f g while it is generally agreed upon that scoring rules must at least be proper to adequately quantify the accuracy of probabilistic forecasts winkler et al 1996 gneiting and ranjan 2011 the question which ones to use remains largely open gneiting and raftery 2007 alexander et al 2022 we restrict our attention to the four scoring rules listed in table 5 these scoring rules benefit a strong mathematical underpinning dawid 2007 gneiting and raftery 2007 and are used to evaluate predictive distributions of mcmc output in probabilistic forecasting krüger et al 2021 the scoring rules are negatively oriented 6 6 if s f y s g y then g is a better probabilistic forecast of the target variable y than f and for all but the interval score is condense different aspects of a forecast distribution among which accuracy skill resolution and sharpness in short the logarithmic score ls evaluates the forecast density p y t y at the measured value y t this measure has a strong foundation in information theory shannon 1948a b kullback and leibler 1951 and is strictly local as it ignores model predicted probabilities of all non realized outcomes the continuous rank probability score crps is a quadratic measure of the difference between the predictive cdf f y t y and the cdf g t of y t this is a common measure of performance for probabilistic forecasts of a scalar observation y t and rewards predictive distributions with a mass close to the observation the pseudospherical score ps remunerates leptokurtic forecast densities as well yet this score rule acts on the predictive pdf and uses mixed powers in the numerator and denominator for ζ 2 the ps reduces to the spherical score ss as used herein friedman 1983 the interval score is is important for quantile prediction and rewards narrow prediction intervals if the observation is outside the l t u t prediction interval the score incurs a penalty the size of which depends on the significance level α note that the is evaluates only two predictive percentiles of the forecast distribution this does not necessarily require an accurate description of the distribution of the predictand y t hence in the terminology of gneiting and raftery 2007 the is is a proper but not a strictly proper scoring rule the situation is different for the ls crps and ss scoring rules which are strictly proper and thus encourage forecasters to model correctly the entire predictive distribution thus in principle any of the ls crps and ss rules will suffice to evaluate the quality of the probabilistic forecasts of the gl gl ul sl and nl functions for each scoring rule we compute a time averaged mean score 47 s f y 1 n t 1 n s f t y t where f f 1 f 2 f n is the collection of empirical cdfs derived from the m posterior realizations specifically for t 1 2 n the empirical cdf ecdf of the predicted outcomes f t is constructed from the m entries y t 1 y t 2 y t m in the t th row of the n m matrix y as follows 48 f t z 1 m j 1 m 1 z y t j where the indicator function 1 a returns 1 if a is true and zero otherwise the ecdf is assumed continuous for every θ θ and δ δ and strictly positive on the bounded interval ω as a result the density f t θ δ 49 f t z d d z f t z is continuous and strictly positive under the same support the lower l t and upper u t limits of the 100 1 α prediction interval are equal to 50 l t f t 1 α 2 and u t f t 1 1 α 2 where f t 1 z is the quantile function of f t θ δ the scoring rules of table 5 are indispensable in our search for the true forecast densities nevertheless have not yet entered mainstream use in the hydrologic community therefore we also consider other metrics of the forecast distribution hydrologists may be more familiar with see table 6 the reliability formalizes the thesis in probability theory that the sequence of probability integral transforms should consist of independent standard uniform random variables in other words if the n observations are samples of the predictive distribution f then the successive values of f t y t 51 f t y t 1 y t θ δ y t p y t θ δ θ δ p θ δ y d y t θ δ d θ δ should be uniformly distributed on the unit interval thyer et al 2009 renard et al 2011 the reliability rlbl equals the mean absolute distance 1 norm between the quantile function of the f t y t s and its counterpart of the standard uniform distribution the multiplier of 2 scales the rlbl to the unit interval between 0 poor and 1 perfect the coefficient of variation cv measures the average sharpness of the predictive distribution smaller values of the cv are preferred the coverage c equals the fraction of observations inside the prediction intervals to be statistically meaningful and robust c should equal 1 α at a significance level α the width w measures the average size of the 100 1 α prediction intervals the performance metrics of table 6 measure different and complementary aspects of the forecast distribution this complicates somewhat the ranking of the likelihood functions as we cannot combine the rlbl cv w and c into a single performance index without assigning arbitrary weights one can instead sort the performance metrics according to the pareto dominance principle and use the corresponding pareto rank of each likelihood function as overall performance index mcinerney et al 2017 2019 but the width and coefficient of variation are properties of the predictive distribution only and thus do not guarantee honest forecasts furthermore the reliability and coverage measure only two aspects of the statistical consistency between the distributional forecasts and the observations in other words these are necessary but not sufficient criteria for determining that the forecast distribution is accurate see e g hamill 2001 we therefore insist on using the strictly proper scoring rules of table 5 for likelihood function evaluation and ranking 5 5 general remarks to simplify the discussion of our findings we classify the nuisance variables of the gl gl sl and ul functions in two different groups nuisance variables that are integral part of the underlying pdf of each likelihood function are coined internal nuisance variables this includes the parameters of f sep a f sst a and f sgt a in eqs 9 30 and 36 respectively the remaining autoregressive coefficients ϕ 1 and or ϕ 2 will be referred to as external nuisance variables 5 6 case study i an ar 2 process with non gaussian innovations to benchmark the gl and ul functions our first case study considers a simple autoregressive scheme 52 y t ϕ 1 y t 1 ϕ 2 y t 2 ɛ t with ϕ 1 0 7 ϕ 2 0 2 and ɛ t s drawn from a a standardized sep distribution ɛ t sep 0 1 β ξ with β 0 5 and ξ 3 and b a standardized sgt distribution ɛ t sgt 0 1 λ p q with λ 0 5 p 1 2 and q 5 we create a synthetic training data record y y 1 y 2 y n with n 5 000 using initial conditions y 1 0 and y 0 0 fig 4 presents histograms of the marginal posterior distributions of coefficients ϕ 1 and ϕ 2 of the autoregressive model of eq 52 and the nuisance variables a β and ξ and b λ p and q of the sep and sgt distributions using the gl and ul functions of eqs 26 and 41 with s 0 1 the true values of the coefficients are separately indicated with a red cross the histograms of the sep and sgt coefficients are well described by a normal distribution with mean approximately equal to the true values of the parameters of the ar 2 training data record and a small dispersion this inspires confidence in the ability of the gl and ul functions to correctly describe the marginal distribution of the partial studentized residuals as the ar 2 operator of eq 12 does not preserve the skew and or kurtosis of the sep and sgt innovations ɛ 1 ɛ 2 ɛ n the mode of the marginal posterior distributions of the skew and or kurtosis parameters do not necessarily have to coincide with their true values but as we will show next such small discrepancies do not have practical consequences fig 5 compares the empirical density function histogram of the partial residuals ɛ 1 ɛ 2 ɛ n of eq 52 with its theoretical true counterpart solid line for the a sep and b sgt innovations respectively notice the excellent agreement between the theoretical pdf of the a sep and b sgt innovations and their empirical frequency distribution derived from mcmc simulation using dream suite vrugt 2016 the different bins of the empirical histogram of the partial residuals provide an almost perfect characterization of the true distribution of the partial residuals in fact this holds for almost any other realization from the posterior parameter distribution in other words the posterior uncertainty in the empirical distribution of the innovations is very small to understand the relationship between the length of the training data record and the posterior uncertainty of the coefficients of eq 52 please consider fig 6 which presents trace plots of the map values solid black lines of the two model parameters ϕ 1 and ϕ 2 and nuisance variables of the left sep and right sgt innovations the gray region portrays the 95 confidence intervals of the map solution the autoregressive coefficients ϕ 1 and ϕ 2 of the data generating process of eq 52 converge rapidly to their theoretical values after processing only one hundred data points the 95 confidence intervals of the two autoregressive coefficients have collapsed to a small region immediately surrounding their map values this conclusion holds for both the sep and sgt innovations most of the nuisance variables of the gl and ul functions converge at a somewhat smaller pace to their theoretical values with map values that go up and or down the prior parameter space for small training data records this is particularly true for the kurtosis parameters β and q a training record of n 2 000 data points appears long enough to sufficiently constrain all the nuisance variables of the gl and ul functions larger records hardly change the map values of the nuisance variables but further reduce the parameter uncertainty to a very small region interior to the uniform prior distribution intuitively one would expect that the highest order moments of the partial residuals are most difficult to accurately characterize thus it is no surprise that β q and or p may need a longer training data record on average than the skew parameters ξ and λ even a long training data record is no guarantee however that the skew and or kurtosis parameters of the gl and ul functions will converge exactly to their theoretical values such discrepancies do not harm the distribution of the partial residuals see fig 5 altogether the results of our first study demonstrate that the gl and ul functions correctly infer the marginal distribution of the sep and sgt innovations of the ar 2 scheme this inspires confidence in the ability of the two likelihood functions to accurately describe the distribution of the partial residuals 5 7 case study ii conceptual watershed model with measured discharge data as second case study we illustrate the application of the gl gl sl and ul functions to a parsimonious 5 parameter conceptual watershed model the hydrologic model describes the rainfall discharge relationship using five fictitious control volumes these reservoirs simulate processes such as evaporation percolation river inflow and baseflow see fig 7 hymod originates from the phd thesis of boyle 2001 and interested readers are referred to this publication for further details we embed the process formulations of fenicia et al 2018 in a mass conservative second order integration method adaptive time stepping guarantee a robust and accurate numerical solution of the simulated fluxes and state variables table 7 presents the five hymod parameters with their corresponding symbols units and lower and upper bounds we illustrate the different likelihood functions by application to hydrologic data from the leaf river near collins ms usa this medium sized watershed with a strong winter regime according to the functional classification of brunner et al 2020 has been studied extensively in the hydrologic literature we resort to the camels data set newman et al 2015 addor et al 2017 and simulate daily river discharge between 1 october 1998 and 30 september 2004 using daily estimates of catchment averaged precipitation maurer product and potential evapotranspiration derived from the formula of oudin et al 2005 daily discharge measurements of the last five years on record wy 1999 2004 are used for parameter estimation purposes using the gl gl sl and ul functions this amounts to n 1827 data points table 8 lists the scoring rules and performance metrics of tables 5 and 6 for the predictive streamflow distributions derived from the gl gl sl ul and nl functions using different active sets of nuisance variables inactive nuisance variables are set to their default values see tables 1 2 and 4 for completeness the penultimate three columns list summary statistics of the goodness of fit of the hymod map simulation finally the last column reports the pareto rank of all likelihood functions but the gl function these ranks are derived from non dominated sorting of the strictly proper scoring rules of the tabulated formulations of the ul gl sl and nl functions thus all likelihoods are ranked together treating ls crps and ss as if they are complementary non commensurate criteria rank one solutions are pareto optimal any other solution is considered inferior a priori one would advise using ϕ 1 and or ϕ 2 in the active set of nuisance variables as the discharge residuals will almost surely exhibit serial correlation and thus fail an independence test to explore to the fullest extent possible the performance of the different likelihood functions we do include formulations without the external nuisance variables ϕ 1 and or ϕ 2 the row number in the first column serves as unique identifier of each likelihood function the tabulated data may be a bit overwhelming and thus we organize and discuss our results according to the goals and or objectives of this paper comparison of gl and gl functions i the values of the scoring rules and performance metrics of the gl and gl functions are in almost perfect agreement when the active set of nuisance variables is equivalent and made up of only internal variables any small differences between the two likelihood functions are simply the result of a dissimilar treatment of the slope s 1 of the measurement error function the gl function considers s 1 a free parameter whose value is estimated jointly with the other nuisance variables the gl function on the contrary evokes the slope as a phantom variable so as to enforce a unit variance of the studentized residuals e θ δ this latter approach frees s 1 from the list of nuisance variable in the gl sl nl and ul functions this simplification is inconsequential as evidenced by the close match in the results of formulations 13 15 and 16 of the gl function and their equivalent counterparts 29 31 and 32 of the gl function respectively ii the scoring rules and performance metrics of the gl function deteriorate tremendously when the active set of nuisance variables includes one or more external variables ϕ 1 and or ϕ 2 this is an immediate consequence of the application of the ar k process of eq 12 to non homogenized residuals this flaw was identified by evin et al 2013 and explains the rather inferior and overdispersed forecast distributions of the gl function as evidenced by the exceedingly high values of the scoring rules relatively poor reliability and unexpectedly large values of the coefficient of variation width of the 95 prediction intervals and bias of the map simulation note that formulations 27 and 28 of the gl function even produce negative values of the cv this exemplifies the deficient implementation of the gl function and demonstrates that we do not truncate negative discharge forecasts to zero the gl function does not suffer these problems as residual studentization precedes the treatment of serial correlation in eq 12 this results in a much better description of the discharge forecast distribution with prediction intervals that are much sharper on average and exhibit an improved coverage and reliability performance of distribution adaptive likelihood functions i the scoring rules and performance metrics vary considerably among the tabulated formulations of the same distribution adaptive likelihood function the choice of active nuisance variables exerts a large control on the performance of the ul gl and sl functions the most advanced formulations do not necessarily yield the best overall performance in other words a larger number of active nuisance variables does not necessarily improve the performance of the ul gl and sl functions ii it is difficult if not impossible to single out a single best likelihood function and or formulation thereof whose performance is uniformly excellent across all scoring rules and or performance metrics the gl and sl functions achieve on average a somewhat lower pareto rank than the ul function suggesting that the advantages of this latter likelihood function did not fully materialize in the present example yet we should not draw generalized conclusions from this single data set and model we do not observe large differences in the performance of the gl ul sl and nl functions none of their formulations consistently places best among the different scoring rules and or performance metrics furthermore care should be exercised that each non dominated rank one likelihood function satisfies residual assumptions such diagnostic checks of the residuals will impact substantially the ranking of the likelihood functions iii the sl function has the largest number of non dominated formulations two and produces on average the lowest values of the is and width w of the 95 prediction intervals the gl function yields the highest reliability the ul function is a close competitor to the sl and gl functions and achieves the highest mean value of the log likelihood l θ δ y but at the expense of a somewhat larger rmse and pbias the ul and gl have only a single pareto optimal formulation according to the strictly proper scoring rules of the discharge forecast distribution iv the ul gl sl functions produce equivalent scoring rules performance metrics and summary statistics if the active set of nuisance variables includes only the intercept s 0 of the measurement error function of eq 24 the performance of these three respective formulations 9 17 and 25 is nearly indistinguishable and equal to that of the nl function with s 0 35 this latter finding is enforced by the use of normal default values of the inactive nuisance variables and confirms our analytic derivations v the nl function with heteroscedastic measurement error function 35 achieves a performance that is at least comparable to that of most of the tabulated formulations of the gl sl and ul functions but as this formulation does not satisfy residual assumptions shown later we should benchmark the distribution adaptive likelihood functions against the nl function with s 0 ϕ 1 34 all tabulated formulations of the ul gl and sl functions but the ul function with 3 s 0 p ϕ 1 improve upon the scoring rules and performance metrics of this normal likelihood as evidenced by their superior pareto rank vi different trials with the exact same likelihood function produce almost equal values of the scoring rules performance metrics and summary statistics see 15 20 and 34 this demonstrates that the sampled chains successively converge to the approximately same target distribution thus disparities in the tabulated statistics of the likelihood functions highlight differences between the ul gl sl and nl functions and or their selection of active nuisance variables on scoring rules and performance metrics i the scoring rules of the forecast distribution display only weak relationships see tables f 1 and f 2 in appendix f the pearson and spearman correlation coefficients r and ρ respectively of ls crps ss and is are consistently smaller than 0 5 except for r crps is 0 67 and ρ crps is 0 62 respectively the weak correlation between the strictly proper scoring rules may seem counter intuitive given that ls crps and ss measure the divergence of the predictive distribution to the true forecast density apparently this divergence can be expressed using three seemingly unrelated quantities but this does not imply that the strictly proper scoring rules are independent their low degree of correlation simply articulates a large departure on average from the true forecast densities we expect the pairwise correlations of ls crps and ss to increase when the predictive distributions approximate more closely and consistently the true forecast densities the relative independence of ls crps and ss turns the selection of an adequate likelihood into a multi criteria decision making problem ii the performance metrics of the forecast distribution do not display strong linear and monotonic relationships appendix f the exception to this are the reliability and coefficient of variation which have pearson and spearman correlation coefficients of 0 82 and 0 91 respectively these two performance metrics measure different aspects of the forecast distribution yet exhibit a relatively strong linear and rank dependence for the present data set and model iii most of the scoring rules display only a weak relationship with the tabulated performance metrics the strongest linear and rank correlation is found between the crps and width w of the forecast distribution r crps w 0 71 ρ crps w 0 87 and the spherical score ss and the reliability r ss rlbl 0 82 ρ ss rlbl 0 76 a somewhat weaker relationship is found between the ss and the coefficient of variation r ss cv 0 73 ρ ss cv 0 68 and the logarithmic score ls and the coverage c r ls c 0 68 ρ ls c 0 73 the scoring rules complement the performance metrics and help provide additional and or orthogonal information about the quality of the forecast distribution iv all tabulated formulations of the ul gl sl and nl functions achieve the desired coverage of about 0 95 at a significance level α 0 05 this performance metric does not discriminate among the different likelihood functions indeed a coverage of 1 α is simply a result of the rigorous application of probability theory note that a poor coverage alerts us to an erroneous implementation of the likelihood function inaccurate characterization of the posterior θ δ parameter distribution and or use of an improper prior distribution v the crps demonstrates a rather small variation among the different likelihood formulations this measure is commonly used when the probabilistic forecast is a cdf and the observations are scalars this amounts to an ecdf g t of each observation y t that is equal to a dirac delta function with unit integral the crps may show a larger variation among the different likelihoods if we replace this tall narrow spike with an actual cdf for each individual discharge observation this cdf may be construed from replicates of discharge time series using the approach of oliveira and vrugt 2022 alternatively one can specify a normal cdf with variance derived from eq 24 using the map value of s 0 this latter approach is not ideal as the cdf of the data will depend on the likelihood function used vi the reliability coefficient of variation width and coverage measure specific properties of the forecast distribution deemed important to represent accurately in practical application these performance metrics have the advantage of being easy to analyze and interpret but as necessary and insufficient conditions of the predictive distribution these metrics do not help in the pursuit of the true forecast density the same is true for proper scoring rules such as the interval score vii the strictly proper scoring rules provide a sufficient description of the true forecast density yet the ls crps and ss are difficult to interpret as they provide an integrated measure of the divergence of the predictive distribution from the true forecast density the treatment of residual autocorrelation i the use of external nuisance variables ϕ 1 and or ϕ 2 in the ul gl sl and nl functions enhances substantially the log likelihood of the posterior realizations but at the expense of a deterioration in most if not all of the scoring rules and performance metrics specifically the treatment of serial correlation reduces the reliability of the discharge forecast distribution and increases the width w of its 95 prediction intervals in interpreting this finding it is important to reiterate that the autoregressive operator of eq 12 is evaluated in simulation mode using the recipe detailed in algorithm e 1 if we switch to one observation ahead forecasting and submit the actual residuals to the autoregressive operator instead then the quality of the predictive discharge distribution would improve considerably beyond the scoring rules and performance metrics obtained with internal nuisance variables only however this would obfuscate the comparison of ar and non ar based formulations of the likelihood function s and thus we do not present these results herein ii the use of external nuisance variables ϕ 1 and or ϕ 2 substantially increases the rmse and percentage bias of the map simulation this is a result of model evaluation in simulation mode the map discharge simulation equals the deterministic output of hymod only thus does not include the ar extension of eq 12 iii there are no benefits to using an ar 2 model of the studentized raw residuals the value of ϕ 2 consistently converges to zero in all respective formulations higher order autoregressive models may be required for other data sets and or models to remove serial correlation among the studentized raw residuals altogether the results confirm that the gl function is superior to the gl function and has the additional benefit of requiring the estimation of one less nuisance variable the use of studentized residuals in the autoregressive model of eq 12 rectifies a critical deficiency of the gl function and guarantees an accurate implementation of the sep log likelihood of eq 26 this makes obsolete the gl function and thus we focus our attention on the ul gl and sl functions the strictly proper scoring rules facilitate a rigorous evaluation of the forecast distributions derived from the posterior realizations of the different likelihood functions this reduces the need for a visual assessment of the results the ls crps and ss and or is should not be used as sole determinant for likelihood function selection as these scoring rules do not verify whether residual assumptions are satisfied some of the likelihood functions perform better than others but no formulation consistently ranks best among the different scoring rules and or performance metrics to illustrate this in more detail please consider fig 8 which presents bivariate scatter plots of the values of the strictly proper scoring rules of the ul gl sl and nl functions of table 8 we assign a separate color to each likelihood function and use empty white circles to differentiate formulations that do not treat serial correlation from those that do filled circles the scatter diagrams are only cross sections of the three dimensional space of strictly proper scoring rules yet highlight some of the difficulties involved in the selection of a suitable likelihood function the dotty plots confirm our earlier claim that the treatment of serial correlation tends to deteriorate on average the quality of the discharge forecast distribution this testifies to the use of simulated residuals in the autoregressive operator of eq 12 the different graphs do not favor a single unique formulation of the likelihood function but rather demonstrate the presence of multiple different optimal formulations these so called pareto solutions represent trade offs among the different scoring rules and have the property that moving from one solution to another results in the improvement of one scoring rule while causing a simultaneous deterioration in one or more others this is a result of the low correlation among the strictly proper scoring rules and consequently we can treat the selection of an adequate likelihood function as a multi criteria decision making problem the spatial arrangement of the different colors in each dotty plot confirm our earlier finding that the different formulations of the sl function achieve on average the best performance followed by the gl and ul functions upon inspection of the pareto ranks of the ar based formulations of the gl ul and sl functions listed in table 8 we conclude that the ul function with 1 s 0 λ p q ϕ 1 ϕ 2 2 s 0 λ ϕ 1 4 s 0 q ϕ 1 6 s 0 p q ϕ 1 and 8 s 0 λ p q ϕ 1 the gl function with 10 s 0 β ξ ϕ 1 ϕ 2 11 s 0 β ϕ 1 12 s 0 ξ ϕ 1 14 s 0 β ξ ϕ 1 and the sl function with 18 s 0 ν ξ ϕ 1 ϕ 2 19 s 0 ν ϕ 1 20 s 0 ξ ϕ 1 and 22 s 0 ν ξ ϕ 1 achieve a performance that is superior to the nl function of eq 27 with s 0 ϕ 1 purple dot at this time it is important to reiterate that we could have also ranked the different likelihood functions according to any of the three strictly proper scoring rules then likelihood functions 18 12 and 10 would have come on top for the ls crps and ss rules respectively and the so ordered likelihood functions would significantly overlap with the pareto dominance based selection listed above of these solutions we will investigate in more detail next the ul function with 2 s 0 λ ϕ 1 the gl function with 12 s 0 β ϕ 1 and sl function with 20 s 0 ξ ϕ 1 the scoring rules and or performance metrics do not tell us whether residual assumptions are satisfied and whether hymod accurately describes the measured discharge record next we verify whether the partial residuals of the ul gl sl and nl functions satisfy assumptions of homogeneity and independence among others fig 9 presents the results of our analysis the left graph displays the studentized partial residuals as a function of the hymod simulated discharge the second graph shows the assumed solid line and actual vertical lines pdf of the studentized partial residuals the third graph presents the sample autocorrelation function of the studentized partial residuals the fourth and last graph compares theoretical and empirical quantiles of the studentized partial residuals to benchmark our findings the bottom panel visualizes the results of the nl function with s 0 35 this likelihood function does not treat serial correlation thus we expect that it will violate residual assumptions the scatter plots appear to suggest that the studentized partial residuals are not homogeneous but rather decrease in magnitude with increasing value of the simulated flows but this is an optical illusion upon close inspection of the data scattering at different flow levels we can reveal that the variance of the studentized partial residuals is rather constant and independent of the magnitude of the simulated discharge the near zero slopes of the regression lines confirm the homogeneous nature of the ɛ t θ δ s the empirical density functions of the partial residuals follow a laplacian distribution that is generally well described by the respective pdfs of the likelihood functions the characteristic peak of the partial residuals is only duplicated by the gl function note that the pdfs of the ul and sl functions exhibit a small skew to the right whereas the pdf of the gl function is symmetric around zero by definition the pdfs of the nl function are without skew autocorrelation is negligible at all lags for likelihood functions 2 12 20 and 35 this is an immediate consequence of the use of the ar operator in eq 12 the nl function in the bottom panel does not treat serial correlation hence the studentized residuals violate the independence assumption the theoretical and empirical quantiles of the studentized partial residuals are in close agreement within the interquartile range outside this interval the empirical quantiles of the likelihood functions deviate considerably from the 1 1 line indeed none of the likelihood functions characterize well the tails of the residual distribution this involves only a small number of residuals but these residuals are critically important in describing adequately the prediction limits of the simulated discharge fig 10 presents time series plots of the observed red dots and simulated discharge of the m posterior realizations for a representative 1 year period of the training record of the leaf river basin using likelihood functions a 2 b 12 c 20 and d 34 the dark and light shaded regions summarize the 95 credible intervals due to parameter and total uncertainty respectively for all four likelihood functions the hymod parameter uncertainty makes up an insignificant part of the prediction uncertainty with 95 credible intervals that collapse to a dark line with the exception of a few peak flows the discharge prediction uncertainty is largest for the gl function with 95 intervals that extent to zero for the largest streamflows this is visually not particularly pleasing the nl function with s 0 ϕ 1 provides a relatively good description of the uncertainty with map solution not shown that tracks reasonably well the measured discharge record the prediction uncertainty of the ul and sl functions depend most on the simulated flow level the discharge uncertainty is relatively small at low flows but increases substantially during rainfall events with increasing discharge the prediction uncertainty of the nl function appears more constant with a comparatively large uncertainty at low flows and comparatively small uncertainty at the peak flows altogether the prediction uncertainty of the ul function is visually most pleasing with 95 intervals that are smallest on average finally we investigate the effect of the choice of likelihood function on the posterior marginal distribution of the hymod parameters see fig 11 the hymod parameters appear well defined by the measured discharge with exception of the nl function with s 0 ϕ 1 we observe a close agreement in the marginal parameter distributions derived from the different likelihood functions this is particularly true for the ul and sl functions we see nicely bell shaped histograms with an approximately similar mean and comparably small spread variations in the mean of each hymod parameter among the different likelihood functions are relatively small certainly compared to the prior ranges of the parameters the hymod parameters exhibit a negligible correlation with exception of s u max and b of the nl function with s 0 ϕ 1 this explains their much enlarged uncertainty these results confirm that the choice of likelihood function has an effect on the posterior parameter values note in our analysis here we limit our attention to the performance of hymod during the 5 year calibration period and do not present performance statistics and or analyze results for an independent evaluation period the calibration period suffices to illustrate the application of the different likelihood functions and the comparison and or ranking of their predictive distributions furthermore for a long data set and parsimonious model such as hymod we do not expect large changes to the performance of the likelihood functions in an independent evaluation period under the stationarity thesis if model selection is of interest then the user can resort to game sampling volpi et al 2017 this method numerically integrates the posterior distribution in pursuit of the so called marginal likelihood this measure suffices to determine which model and or likelihood is most supported by the experimental discharge data 6 conclusions this paper was concerned with the formulation of herein called distribution adaptive likelihood functions in the application of bayesian epistemology to uncertainty quantification of hydrologic watershed models this class of likelihood functions does not require prior assumptions about the expected distribution of the residuals inference takes place over the model parameters and space of distribution functions defined by the nuisance variables of the likelihood function the goals of this paper were threefold first we presented a revised formulation of the generalized likelihood gl function of schoups and vrugt 2010 by enacting the treatment of serial correlation on the studentized raw residuals rather than the raw residuals this so called gl function rectifies a critical deficiency of the gl function this correction guarantees an accurate computation of the sep log likelihood and allows for a much more robust joint inference of the autoregressive coefficients and variance skew and or kurtosis of the residuals as secondary goal we presented a further generalization of the gl function coined the universal likelihood ul function this likelihood function uses as its main building block the skewed generalized student s t distribution of theodossiou 2015 and extends applicability to a much larger family of well known probability distributions as third and last goal we introduced the use of strictly proper scoring rules to evaluate compare and rank the predictive distributions of different likelihood functions the logarithmic continuous rank probability and spherical score rules condense the accuracy of a distribution forecast to a single value while retaining attractive statistical properties and incentivizing a correct modeling of the predictive distribution the power and usefulness of the gl and ul functions were demonstrated by application to two case studies the first study involved a simple autoregressive data generating process with sep and or sgt innovations and was used to investigate the convergence properties of the gl and ul functions both likelihood functions converge upon the true values of the model parameters and shape coefficients of the distribution of the innovations the uncertainty in the autoregressive coefficients and nuisance variables of the partial residuals dissipates quickly with increasing length of the training data record as the ar 2 operator does not preserve higher order moments of the sep and or sgt innovations the mode of the marginal distributions of the skew and or kurtosis parameters may not coincide exactly with their true values such discrepancies however are inconsequential as the distribution of the innovations is perfectly described the second case study considered the application of distribution adaptive likelihood functions to a simple five parameter watershed model named hymod using daily discharge measurements of the leaf river different formulations of the gl and ul functions were evaluated using the logarithmic continuous rank probability and spherical scoring rules and benchmarked against the gl function the student t likelihood sl of scharnagl et al 2015 and a normal likelihood nl with first order autoregressive model and or heteroscedastic measurement errors our most important results were as follows 1 the gl function is superior to the gl function and has the advantage of requiring the specification of one less nuisance variable 2 the performance of the gl sl and ul functions depends in large part on the choice of nuisance variables the most complex formulations of the gl sl and ul functions all nuisance variables in active set do not yield the best performance 3 the internal nuisance variables of the gl sl and ul functions enable a better description of the discharge residuals this reduces the width of the discharge prediction intervals 4 the normal likelihood achieves a performance that is fairly comparable to several formulations of the gl sl and ul functions 5 the treatment of autocorrelation deteriorates the values of the scoring rules and performance metrics of the forecast distribution if however we switch to one observation ahead forecasting and work with actual rather than simulated residuals then the use of autocorrelation improves substantially the quality of the predictive discharge distribution beyond the scoring rules and performance metrics obtained for internal nuisance variables only e g see evin et al 2014 5 scoring rules are indispensable in our search for the true forecast distribution 6 strictly proper scoring rules facilitate a statistically rigorous evaluation and ranking of different likelihood functions 7 we cannot single out a single best likelihood function whose performance is uniformly excellent across all scoring rules and performance metrics of the forecast distribution 8 the use of one strictly proper scoring rule suffices in principle for evaluation and ranking of the likelihood functions the simultaneous use of the logarithmic continuous rank probability and spherical scores converts the selection of an adequate likelihood function into a multi criteria problem care should be exercised that each likelihood function satisfies residual assumptions 9 the pareto optimal likelihood functions produce fairly similar marginal distributions of the hymod parameters altogether our results for hymod favored the use of a distribution adaptive likelihood function in describing the rainfall discharge dynamics of the leaf river a leptokurtic distribution of the partial residuals provided the overall best characterization of the measured discharge record and associated uncertainties credit authorship contribution statement jasper a vrugt conceptualization methodology software validation formal analysis investigation resources writing original draft writing review editing visualization supervision debora y de oliveira validation methodology formal analysis resources visualization gerrit schoups methodology validation investigation cees g h diks methodology validation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the comments of carlos lima and one anonymous reviewer led to a significantly improved manuscript the second author gratefully acknowledges the financial support received from the brazilian federal agency for support and evaluation of graduate education capes grant number 88881 174456 2018 01 appendix a standardized skewed exponential power sep density we revisit appendix a of schoups and vrugt 2010 and reiterate how the standardized skew exponential power density function in eq 9 can be obtained from the exponential power ep or generalized normal distribution of subbotin 1923 and box and tiao 1992 using the method of fernandez and steel 1998 the density of the standardized ep distribution zero mean and unit variance at point a may be computed as follows box and tiao 1992 a 1 f ep a 0 1 β ω β exp c β a 2 1 β where β 1 1 is a so called kurtosis parameter which controls the peakedness of the distribution 7 7 with 1 β 1 the residual norm ℓ 2 1 β ranges between ℓ 1 and ℓ and denotes the absolute value or modulus operator the values of the scalars c β and ω β depend on the kurtosis parameter and are given by box and tiao 1992 a 2a c β γ 3 1 β 2 γ 1 β 2 1 1 β a 2b ω β γ 1 2 3 1 β 2 1 β γ 3 2 1 β 2 where γ b signifies the incomplete gamma function evaluated at b a 3 γ b 0 x b 1 exp x d x b r which satisfies the recursion γ b 1 b γ b the gamma function can be approximated numerically a 4 log γ b γ b log b m 1 b m log 1 b m where γ lim m log m k 1 m 1 k 0 5772 is the so called euler mascheroni constant and log signifies the natural logarithm the ep density f a β in eq a 1 is symmetric around a 0 this symmetry impairs our ability to describe accurately skewed residual distributions with an upper or lower tail fernandez and steel 1998 developed a general solution for the treatment of skew in symmetric distributions with closed form mathematical expressions their template density f skew a ξ with skewness parameter ξ r introduces tails in an arbitrary symmetric density f as follows a 5 f skew a ξ 2 ξ ξ 1 f a ξ sign a 2 ξ ξ 1 f a ξ sign a where sign is the signum function a 6 sign x 1 if x 0 0 if x 0 1 if x 0 and the mean μ ξ and variance σ ξ 2 of a satisfy the following relationship fernandez and steel 1998 a 7a μ ξ m 1 ξ ξ 1 a 7b σ ξ 2 m 2 m 1 2 ξ 2 ξ 2 2 m 1 2 m 2 wherein m j is the j th absolute moment of the standardized ep density f ep 0 1 β of eq a 1 a 8 m j e x j x j f ep x 0 1 β d x we can now substitute the exponential power density f ep a 0 1 β of eq a 1 into the skew distribution f skew a ξ of eq a 5 the result is a non standardized skewed exponential power sep density with mean μ ξ and variance σ ξ 2 of a determined by eqs a 7a and a 7b to negate changes in the mean and variance of a imposed by the use of the skew density of eq a 5 we must scale the sep density and therefore use as its input argument μ ξ σ ξ a rather than a this leads to the following expression for the standardized sep density schoups and vrugt 2010 a 9 f sep a 0 1 β ξ 2 σ ξ ω β ξ ξ 1 exp c β μ ξ σ ξ a ξ sign μ ξ σ ξ a 2 1 β where c β ω β μ ξ and σ ξ are a function of the kurtosis β and skewness ξ using eqs a 2a a 2b a 7a and a 7b respectively the derivation of the standardized sep density function in eq a 9 would not be complete without addressing the computation of the first and second moment m 1 and m 2 in eqs a 7a and a 7b respectively as the ep distribution of subbotin 1923 is symmetric around zero eq a 8 may be written as follows a 10 m j 2 0 x j f ep x 0 1 β d x and reduces to a 11 m j 2 ω β 0 x j exp c β x 2 1 β d x for the ep distribution in eq a 1 the above integral appears in integration tables a 12 0 x j exp a x b d x 1 b a j 1 b γ j 1 b where a c β and b 2 1 β if we enter this solution in eq a 11 then we obtain the following closed form expression for the j th moment of the ep distribution a 13 m j 2 ω β c β j 1 2 1 β 2 1 β γ j 1 2 1 β ω β 1 β c β 1 2 j 1 1 β γ j 1 1 β 2 for j 1 we yield the first moment m 1 of the ep distribution a 14 m 1 ω β 1 β c β 1 β γ 1 β we can enter eqs a 2a and a 2b for ω β and c β respectively to yield a 15 m 1 γ 1 2 3 1 β 2 1 β 1 β γ 3 2 1 β 2 γ 3 1 β 2 γ 1 β 2 1 1 β 1 β γ 1 β γ 1 2 3 1 β 2 γ 3 2 1 β 2 γ 3 1 β 2 γ 1 β 2 1 γ 1 β γ 1 2 3 1 β 2 γ 3 1 β 2 γ 1 β 2 γ 3 2 1 β 2 γ 1 β and thus the first moment of the ep distribution simplifies to a 16 m 1 γ 1 β γ 1 2 3 1 β 2 γ 1 2 1 β 2 for the second moment m 2 of the ep distribution we must enter j 2 in eq a 13 a 17 m 2 ω β 1 β c β 3 2 1 β γ 3 1 β 2 after substitution of eqs a 2a and a 2b the above expression simplifies to a 18 m 2 γ 1 2 3 1 β 2 1 β 1 β γ 3 2 1 β 2 γ 3 1 β 2 γ 1 β 2 1 1 β 3 2 1 β γ 3 1 β 2 γ 3 2 3 1 β 2 γ 3 2 1 β 2 γ 3 1 β 2 γ 1 β 2 3 2 1 this concludes the derivation appendix b the student t likelihood function the gl and gl functions rely on the generalized normal or ep distribution to characterize nontraditional residual distributions with different degrees of skew and kurtosis scharnagl et al 2015 replaced the ep distribution of eq 7 with a standardized student s t density b 1 f st a 0 1 ν γ ν 1 2 γ ν 2 1 π ν 2 1 a 2 ν 2 ν 1 2 where μ a 0 σ a 2 1 and ν 2 denotes the degrees of freedom if we combine eq b 1 with the skew density of eq 8 we yield the following expression for the standardized skewed student s t sst density scharnagl et al 2015 b 2 f sst a 0 1 ν ξ 2 σ ξ ξ ξ 1 γ ν 1 2 γ ν 2 π ν 2 1 1 ν 2 μ ξ σ ξ a ξ sign μ ξ σ ξ a 2 ν 1 2 where ξ 0 is the skewness parameter and μ ξ and σ ξ 2 are shift and scale constants respectively which standardize the sst density the shift and scale constants μ ξ and σ ξ 2 respectively must satisfy eqs a 7a and a 7b b 3a μ ξ m 1 ξ ξ 1 b 3b σ ξ 2 m 2 m 1 2 ξ 2 ξ 2 2 m 1 2 m 2 wherein m j is the j th absolute moment of the standardized st density f st 0 1 ν of eq b 1 b 4 m j e x j 2 0 x j f st x 0 1 ν d x this results in kirkby et al 2019 b 5 m j γ j 1 2 γ ν j 2 ν 2 j 2 π γ ν 2 and leads to the following expressions for the first two absolute moments of the st distribution b 6 m 1 γ ν 1 2 ν 2 π γ ν 2 and m 2 γ 3 2 γ ν 2 2 ν 2 π γ ν 2 if we enter the above expressions for m 1 and m 2 into eqs b 3a and b 3b and simplify and rearrange the resulting equations we yield see also scharnagl et al 2015 b 7a μ ξ γ ν 1 2 ν 2 ξ ξ 1 π γ ν 2 b 7b σ ξ 2 μ ξ 2 ξ 2 ξ 2 1 the so obtained shift and scale constants standardize the sst distribution μ a 0 and σ a 2 1 the autoregressive model of eq 12 may be used to treat serial correlation of the studentized raw residuals e θ δ we can use eq 21 to derive the corresponding sst likelihood function b 8 l θ ν ξ σ ɛ 2 y σ ϵ 2 t 1 n 1 σ ϵ t 1 σ ɛ f sgt ɛ t θ δ ν ξ σ ɛ n t 1 n 2 σ ξ γ ν 1 2 σ ϵ t ξ ξ 1 γ ν 2 π ν 2 1 1 ν 2 μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 ν 1 2 where σ ɛ 2 equals the variance of the partial residuals in eq 13 the above formulation of the sst likelihood function l θ ν ξ σ ɛ 2 y σ ϵ 2 assumes prior knowledge of the measurement error variances σ ϵ 2 we can relax this assumption and infer the n entries of σ ϵ 2 along with the model parameters θ using the measurement error function of eq 24 the sst likelihood function then becomes b 9 l θ s 0 ν ξ σ ɛ 2 y σ ɛ n t 1 n 2 σ ξ γ ν 1 2 s 0 s 1 y t θ ξ ξ 1 γ ν 2 π ν 2 1 1 ν 2 μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 ν 1 2 where s 1 equals the zero point of the residual function ħ s 1 δ 1 g s 1 δ and satisfies the constraint explicated in eq 25 the sst log likelihood l θ s 0 ν ξ σ ɛ 2 y now becomes b 10 l θ s 0 ν ξ σ ɛ 2 y n 2 log σ ɛ 2 1 2 t 1 n log s 0 s 1 y t θ 2 n log 2 n log σ ξ n log γ ν 1 2 n log ξ ξ 1 n log γ ν 2 n 2 log π n 2 log ν 2 ν 1 2 t 1 n log 1 1 ν 2 μ ξ σ ξ ɛ t θ δ ξ sign μ ξ σ ξ ɛ t θ δ 2 this concludes our derivation of the sst log likelihood function abbreviated as sl function appendix c special and limiting cases of the sgt distribution the skewed generalized t distribution of theodossiou 1998 defines a large family of continuous probability distributions which includes several well known distributions these special or limiting cases of the sgt distribution f sgt a μ σ λ p q may be derived by fixing one or more of its shape parameters λ p and q to preset values what follows is a summary of twelve well known probability distributions c 1 the skewed generalized error distribution the skewed generalized error sge distribution of theodossiou 2015 is defined as follows c 1 lim q f sgt a μ σ λ p q and yields the following pdf c 2 f sge a μ σ λ p p 2 v σ γ 1 p exp a μ m v σ 1 λ sign a μ m p where m 2 2 p v σ λ γ 1 2 1 p π and c 3 v π γ 1 p π 1 3 λ 2 γ 3 p 1 6 1 p λ 2 γ 1 2 1 p 2 γ 1 p affix the mean μ and variance σ 2 of the sge distribution c 2 the generalized student s t distribution the generalized student s t distribution of mcdonald and newey 1988 is defined as follows c 4 f sgt a μ σ λ 0 p q and yields the pdf c 5 f gt a μ σ p q p 2 v σ q 1 p b 1 p q a μ p q v σ p 1 1 p q where c 6 v 1 q 1 p b 1 p q b 3 p q 2 p affixes the variance σ 2 of the gt distribution c 3 the skewed student s t distribution the skewed student s t st distribution of hansen 1994 is defined as follows c 7 f sgt a μ σ λ p 2 q and yields the following pdf c 8 f st a μ σ λ q γ 1 2 q v σ π q 1 2 γ q x μ m 2 q v σ 2 λ sign x μ m 1 2 1 1 2 q where m 2 v σ λ q 1 2 γ q 1 2 π 1 2 γ q and c 9 v 1 q 1 2 3 λ 2 1 1 2 q 2 4 λ 2 π γ q 1 2 γ q 2 affix the mean μ and variance σ 2 of the st distribution c 4 the skewed laplace distribution the skewed laplace sl distribution is given by c 10 lim q f sgt a μ σ λ p 1 q and results in the following pdf c 11 f sl a μ σ λ 1 2 v σ exp a μ m v σ 1 λ sign a μ m where c 12 m 2 v σ λ and v 2 1 λ 2 1 2 affix the mean μ and variance σ 2 of the sl distribution c 5 the generalized error normal distribution the exponential power ep distribution subbotin 1923 box and tiao 1992 is defined as follows c 13 lim q f sgt a μ σ λ 0 p q and yields the following pdf c 14 f ep a μ σ p p 2 v σ γ 1 p exp x μ v σ p where c 15 v γ 1 p γ 3 p affixes the variance σ 2 of the ep distribution for μ 0 and σ 1 eq c 14 simplifies to the pdf of the standardized ep distribution in eq 7 indeed if we substitute eq c 15 into eq c 14 and rearrange the resulting expression we yield c 16 f ep a μ σ p p γ 1 2 3 p 2 σ γ 3 2 1 p exp γ 3 p γ 1 p p 2 x μ σ p for μ 0 and σ 1 this expression simplifies to eq 7 c 17 f ep a p ω p exp c p x p with constants ω p and c p c 18 c p γ 3 p γ 1 p p 2 and ω p p γ 1 2 3 p 2 γ 3 2 1 p equal to eqs a 2a and a 2b respectively and p 2 1 β c 6 the skewed normal distribution the skewed normal sn distribution is defined as follows c 19 lim q f sgt a μ σ λ p 2 q and has the following pdf c 20 f sn a μ σ λ 1 v σ π exp a μ m v σ 1 λ sign a μ m 2 where c 21 m 2 v σ λ π and v 2 π π 8 λ 2 3 π λ 2 affix the mean μ and variance σ 2 of the sn distribution c 7 the student s t distribution the student s t distribution student 1908 is given by c 22 f sgt a μ 0 σ 1 λ 0 p 2 q and yields the pdf c 23 f t a ν γ ν 1 2 π ν 1 2 γ ν 2 1 x 2 ν ν 1 2 c 8 the skewed cauchy distribution the skewed cauchy distribution is defined as follows c 24 f sgt a μ σ λ p 2 q 1 and yields the following pdf c 25 f sc a μ σ λ 1 σ π x μ 2 σ 2 λ sign x μ 1 2 1 1 c 9 the laplace distribution the laplace 1774 distribution is given by c 26 lim q f sgt a μ σ λ 0 p 1 q which results in the following pdf c 27 f l a μ σ 1 2 σ exp a μ σ where σ is the scale parameter of the laplace distribution c 10 the uniform distribution the uniform distribution is given by c 28 lim p q f sgt a μ σ λ 0 p q and results in the following pdf c 29 f u a 1 2 v σ if a μ v σ 0 otherwise the standard uniform distribution is obtained by setting μ a b 2 v 1 and σ b a 2 c 11 the normal distribution the gaussian distribution is defined as follows c 30 lim q f sgt a μ σ λ 0 p 2 q and yields the familiar pdf c 31 f n a μ σ 1 v σ π exp a μ v σ 2 where v 2 affixes the variance σ 2 of the normal distribution c 12 the cauchy lorentz distribution the cauchy lorentz cl distribution of poisson 1824 is given by c 32 f sgt a μ σ λ 0 p 2 q 1 and results in the following pdf c 33 f cl a μ σ 1 σ π x μ σ 2 1 1 this concludes appendix c appendix d the sgt likelihood with normally distributed residuals if the partial residuals ɛ t θ δ are believed to be normally distributed then we must set λ 0 p 2 and q unfortunately we cannot submit an infinite value of q to eqs 39 and or 41 as this will induce numerical problems a pragmatic remedy is to enter a large value of q instead say the default value of q 1 0 10 in table 4 this will do in practice we must still demonstrate the theoretical equivalence of the sgt log likelihood function with λ 0 p 2 and q and the nl function of eq 27 to support limit analysis we print below the sgt log likelihood function of eq 41 and label the three terms that require limit analysis d 1 l θ s 0 λ p q φ 2 y n 2 log σ ɛ 2 t 1 n log s 0 s 1 y t θ n log p n log 2 n log κ λ p q i n log b 1 p q p ii q 1 p t 1 n log 1 ɛ t θ δ μ λ p q κ λ p q 1 λ sign ɛ t θ δ μ λ p q p iii we use the labels i ii and iii in the analysis below for λ 0 p 2 and q the shift constant μ 02 in eq 35a amounts to zero for the scale constant κ 02 in eq 35b we yield eq d 2 see box ii we can take advantage of the following limit of the beta function d 3 lim b b a b γ a b a to simplify the expression of the scale constant κ 02 to yield d 4 κ 02 q γ 1 2 q 2 1 2 γ 3 2 q 2 2 3 2 if q 2 γ 1 2 q 2 1 2 γ 1 2 q 2 2 3 2 if q 2 q 2 1 2 q 2 3 2 if q q if q thus for q term i in eq d 1 is equal to n log q the limit of the beta function in eq d 3 can also be applied to term ii of eq d 1 we yield d 5 lim q n log b 1 2 q 2 n log γ 1 2 q 2 1 2 n 2 log π n 2 log q 2 n 2 log 2 π n log q thus in the limit of q term ii of eq d 1 may be replaced with n 2 log 2 π n log q this leaves us with term iii of the sgt log likelihood function in eq d 1 the following limit is well known in elementary calculus d 6 lim x 1 a x x exp a if we take the natural logarithm of both sides of this limit and then divide by p we yield the corollary d 7 lim x x p log 1 a x a p for q x p 2 and a ɛ t θ δ 2 the left hand side reduces to term iii of eq d 1 but without the sum operator as a result we yield d 8 lim q q 1 p t 1 n log 1 ɛ t θ δ 2 q ɛ 1 θ δ 2 2 ɛ 2 θ δ 2 2 ɛ n θ δ 2 2 1 2 t 1 n ɛ t θ δ 2 and thus in the limit of q term iii of eq d 1 simplifies to half the sum of squares of the standardized partial residuals if we replace terms i ii and iii in eq d 1 with their counterparts of eqs d 4 d 5 and d 8 we yield the sgt log likelihood function l θ s 0 φ 2 y λ 0 p 2 q for normally distributed partial residuals d 9 l θ s 0 φ 2 y λ 0 p 2 q n 2 log σ ɛ 2 t 1 n log s 0 s 1 y t θ n log 2 n log 2 n log q n 2 log 2 π n log q 1 2 t 1 n ϵ t θ δ 2 this expression of the sgt log likelihood function reduces to the nl function of eq 27 d 10 l θ s 0 φ 2 y λ 0 p 2 q n 2 log 2 π t 1 n log s 0 s 1 y t θ n 2 log σ ɛ 2 1 2 t 1 n ϵ t θ δ 2 with variance of the partial residuals σ ɛ 2 which is derived from eq 13 thus the sgt distribution is the key ingredient of a large family of likelihood functions which includes the normal likelihood function this concludes the derivation appendix e algorithmic recipe for the predictive distributions algorithm e 1 presents a step by step recipe for determining the predictive distribution p y t y of eq 3 from the posterior realizations sampled with dream suite this recipe is specifically written for the sep likelihood of the gl function but easy to adapt to the sl and ul functions of eqs 31 and 41 respectively the algorithmic recipe uses as input arguments i the d s m matrix θ δ of m posterior samples of d model parameters θ θ 1 θ 2 θ d concatenated vertically with the s nuisance variables δ s 0 β ξ ϕ 1 ϕ 2 of the sep likelihood function and ii the initial conditions e 1 and e 0 of the second order autoregressive scheme in eq 12 and returns n m matrices of posterior model simulations y s and predictions y predictive percentiles y t α 2 and y t 1 α 2 are readily derived from the m entries y t j r n m in the t th row of y e 1 y t α 2 y t 1 α 2 p y t y p y t α 2 y t j y t 1 α 2 y 1 α j 1 2 m by truncating α 2 of the left and right tails of the predictive distribution p y t y this must be repeated for all t 1 2 n and produces two n 1 vectors y α 2 and y 1 α 2 respectively which together define the 100 1 α prediction uncertainty if we admit instead to eq e 1 the n m matrix y s of posterior model simulations then we yield prediction percentiles due to parameter uncertainty only appendix f correlation of scoring rules and performance metrics to determine whether the scoring rules and or performance metrics measure independent information about the discharge forecast distribution we compute pairwise correlation coefficients of the different scoring rules and performance metrics of table 8 
2648,ecosystem water use efficiency wue is an effective indicator of ecosystem functioning which is widely used to characterize the coupling relationship between terrestrial carbon and the water cycle although the linear trends in wue have been well reported the nonlinearity of the secular trends in wue and their relative contribution to the overall changes were still poorly investigated in this study we used ensemble empirical mode decomposition eemd to extract the secular trends of the wue and their relative contribution rc to the wue changes in china which represents the stability of the ecosystem functioning we found that although increasing trends dominated the secular trends of the wue nearly half of the study areas had experienced trend shifts except that deciduous forests and mixed forests were dominated by trend shifts most of the land cover types were dominated by increasing trends the rc exhibited a bimodal pattern along the elevation gradient with peaks at 20 m and 5000 m similarly the rc peaked at the east and south aspects and was the lowest for the west and north aspects in comparison the rc exhibited a u shaped pattern with the slope and was the lowest at 80 for the monotonically increasing trends and decreasing to increasing trends the high rc mainly occurred on the north china plain and loess plateau indicating ecosystem improvement and strong stability meanwhile it was low in northeastern china and southwestern china indicating unsustainableecosystem improvement the rc was always very low for increasing to decreasing trends and monotonically decreasing trends indicating low ecosystem stability with a high possibility for improvement temperature and the atmospheric carbon dioxide co2 concentration were the two main climatic factors driving the wue changes the climatic mean state rather than the climatic variability affected the ecosystem stability the results of this study highlight the relative contribution of the secular trends to wue changes and provide a better understanding of the structure and the stability of ecosystem functioning keywords water use efficiency wue ensemble empirical mode decomposition eemd nonlinear trend ecosystem stability data availability data will be made available on request 1 introduction plants take in carbon dioxide co2 through photosynthesis and generate an inevitable loss of water via transpiration guerrieri et al 2019 it has been proposed that plants control stomata to optimally satisfy the compromise between the amount of carbon gain and the amount of water loss beer et al 2009 water use efficiency wue which is characterized by the rate of carbon uptake per unit of water loss is used to quantify the trade off between carbon gain and water loss beer et al 2009 horion et al 2019 wue links biological and physical processes in terrestrial ecosystems and underlies global scale vegetation climate interactions and the terrestrial water cycle belmecheri et al 2021 cheng et al 2017 frank et al 2015 huang et al 2017 niu et al 2011 tang et al 2021 yu et al 2008 it is well known that when an ecosystem is stressed by environmental conditions its response may change due to survival strategies or direct loss of productivity hence as a vital indicator of terrestrial ecosystem functioning the change in wue is a critical parameter for assessing the hydro climatic status of terrestrial ecosystems horion et al 2019 wue changes include not only secular trends but also inter annual variability ponce campos et al 2013 affected by environmental changes wue changes are characterized by large temporal variability correct knowledge of wue trends would contribute to a deeper comprehension for ecosystem functioning assessment although the leaf level wue has been well studied via physiological processes our understanding of wue dynamics remains largely ambiguous at the ecosystem level cao et al 2020 previous studies based on flux data have found that wue has been increasing in china since the very beginning of the 21st century niu et al 2011 yu et al 2008 with the emerging satellite and earth observation technology remote sensing datasets are now available for detecting wue changes the national oceanic and atmospheric administration noaa advanced very high resolution radiometer avhrr datasets which can provide the longest record recently are helpful to achieve insights of ecosystem functioning changes recent studies based on remote sensing found that increased wue was widely distributed in high latitudes but decreased wue also occurred in low latitudes guo et al 2019 huang et al 2015 lin et al 2020 liu et al 2015 sun et al 2018 many studies have estimated the trends in wue through linear regression or piecewise linear regression however due to the effects by non stationary climate changes the trends in vegetation growth and carbon sink change always change over time and ever reverse piao et al 2020 which cannot be revealed via linear analysis other methods like piecewise linear regression models plr breaks for additive seasonal and trend analysis bfast and detecting breakpoints and estimating segments in trend analysis dbest can also been proposed to gain further insight into ecosystem functioning changes but these methods both have inevitable flaws more specifically the predetermined assumption of a constant trend makes the models not only sensitive to short term fluctuations and abrupt changes but also to the extension of time series hence it is crucial to explore the nonlinear characteristics of the secular trends to assess the terrestrial ecosystem functioning in this study an adaptive method named ensemble empirical mode decomposition eemd was used to extract the gradual and nonlinear secular trends of wue this method has been widely proven to be suitable for use in hydrological climatic and ecological research chang et al 2011 franzke 2012 liu et al 2016 wang et al 2015a wang et al 2015b wang et al 2013 yin et al 2017 as was mentioned above several researches have assessed the secular trends of wue changes and the climatic controls at both the leaf scale and the ecosystem scale lin et al 2020 niu et al 2011 sun et al 2018 wang et al 2021 zheng et al 2019 zou et al 2020 previous studies mainly focused on the types i e increasing and decreasing and the rates i e rapid and slow of the trends xu et al 2020 xu et al 2021 but little research has been conducted on the relative contribution of the trends to the total wue changes the type and rate of the trends can merely describe the direction and speed of the wue changes respectively but they can hardly illustrate the structure and stability of wue an ecosystem wue trend with a high relative contribution means that wue is dominated by secular trends and is resistant to perturbations and hence it is barely possible for trend shifts in contrast an ecosystem wue trend with a low relative contribution means that wue is dominated by inter annual variations and is vulnerable to perturbations and thus trend shifts are highly likely although some studies have explored the stability of ecosystem functioning through the coefficient of variation cv bai et al 2004 song and yu 2015 zhang et al 2022 cv cannot distinguish between the relative contribution of the secular trends and the inter annual variations to the overall changes and thus it is unable to reveal the structure of wue changes investigation of the relative contribution is not only beneficial to understand the structure of wue changes but is also helpful in evaluating the ecosystem stability and further in better projecting the future changes under the context of global warming piao et al 2020 hence it is vital to investigate the contribution of the secular trends to the total wue changes in order to assess ecosystem functioning moreover previous studies focused on the water and temperature controls on the ecosystem functioning changes but ignored the atmospheric co2 concentration which is also a determining factor of wue sun et al 2018 wang et al 2021 yu et al 2008 zhang et al 2014 zheng et al 2019 zou et al 2020 huang et al 2015 concluded that the atmospheric co2 concentration was the primary controlling factor of wue considering that environmental factors including temperature precipitation solar radiation and the atmospheric co2 concentration all affect wue it is necessary to quantify the impact of each factor on wue in this study we used well validated datasets to investigate the nonlinear trends of ecosystem wue and their relative contribution to the overall wue changes in china and more specifically 1 the spatial distribution of the nonlinear secular trends of wue 2 the relative contribution of the secular trends to the overall wue changes and 3 how climate factors affect the secular trends and their contributions to the overall changes 2 materials and methods 2 1 data the global land surface satellite glass gross primary productivity gpp datasets with a spatial resolution of 0 05 for 1982 2015 were used in this study https www glass umd edu the glass gpp product which has been widely used in global and regional ecosystem carbon cycle studies was generated using the bayesian algorithm ensemble of eight widely used light use efficiency models yuan et al 2010 and it has been validated using 155 global eddy covariance sites which covered nine major terrestrial biomes this product has been well validated and shows more accuracy than many other gpp products and thus has been applied in previous ecosystem carbon cycles studies at global and regional scale yuan et al 2010 the terrestrial evapotranspiration et datasets across china with a spatial resolution of 0 1 for 1982 2015 were obtained from national tibetan plateau data center https data tpdc ac cn this et product was simulated through a nonlinear complementary relationship model and validated against 13 eddy covariance measurements and 10 river basins measured by nash sutcliffe efficiency in the range of 0 72 0 94 ma et al 2019 hence this et product is considered more accurate than other mainstream et products the land cover types were delineated according to the moderate resolution imaging spectroradiometer modis mcd12c1 product friedl et al 2010 using the international geosphere biosphere programme igbp land cover classification scheme loveland and belward 1997 only the land cover types that remained unchanged during the period 2001 2015 were retained fig s1 including evergreen needle leaf forest enf evergreen broad leaf forest ebf deciduous needle leaf forest dnf deciduous broad leaf forest dbf mixed forest mf savannas sav grasslands gra croplands cro urban and built up ub croplands natural vegetation mosaic cnv mean air temperature tem total precipitation pre and downward shortwave radiation rad can represent the demand for heat water and energy for vegetation growth respectively and thus were used to study the effects of climate changes the terraclimate datasets which were accessed from climatology lab abatzoglou et al 2018 were adopted in this study these datasets can provide monthly climate and climatic water balance data for global terrestrial surfaces from 1958 to 2020 at a spatial resolution of 1 24 recently these dataset have been used as a high spatial resolution and time varying climatic input in ecological and hydrological studies klein et al 2022 li et al 2022 the daily global co2 concentration ca product was downloaded from jena carboscope rödenbeck et al 2018 we calculated the average value of the daily global co2 concentration and used it to calculate the annual global co2 concentration all data were unified at a spatial resolution of 0 1 0 1 using the nearest or majority methods to match with the et datasets detailed information about all of the datasets used in this study is presented in table s1 2 2 methods 2 2 1 definition of wue the ecosystem scale wue is an effective indicator of ecosystem functioning and is used to characterize the coupling relationship between the water and carbon cycle migliavacca et al 2021 reichstein et al 2014 the ecosystem wue can be calculated at the pixel scale as the ratio between annual gpp and annual et as follows 1 wue gpp et where the unit of wue is gc kg 1h2o the unit of gpp is gc m 2 a 1 and the unit of wue is kgh2o m 2 a 1 2 2 2 eemd the eemd method decomposes nonlinear and nonstationary time series data into a finite number of stationary intrinsic mode functions imf imfi i 1 2 n and a residual rn imfi is the ith imf obtained by the eemd method and rn is the residual component after n imfs are decomposed and screened each imf component contains local characteristic signals of different time scales and rn contains the secular trend of the original signal wang et al 2015a wu and huang 2009 the principle of the eemd method is to repeatedly add different uniformly distributed white noises to the raw data and then to take the ensemble mean of the imfs and rn as the final results after the sifting processes which not only alleviates the mode mixing problem but also retains the original information in the data since the added white noises can be inter neutralized when taking the ensemble mean wang et al 2015a wang et al 2015b the eemd sifting processes are conducted as follows fig 1 1 add a set of white noise series n t the amplitude was 0 2 standard deviations of the raw data in this study to the raw time series x t and obtain a new time series y t 2 y t x t n t 2 obtain the upper and lower envelope curves eu t and el t by connecting the local maxima and minima with splines respectively and then obtain the mean of the upper and lower envelope curves m t 3 m t e u t e l t 2 3 obtain h t by subtracting m t from y t and then judge whether m t satisfies the stop criterion m t is close to 0 at any point if it does h t is the first imf if it does not take h t as the new time series y t and repeat step 2 4 a 1 1 t h t y t m t 4 obtain r t by subtracting h t from y t determine whether r t has an oscillatory component if it does take r t as the new time series and repeat step 2 if it does not stop sifting 5 r 1 t y t a 1 1 t 6 r i t r i 1 t a i 1 t 5 repeat steps 1 4 m times 100 in this study and then obtain the ensemble mean of each component as the final results 7 im f i t 1 m j 1 m a i j t the relative contribution of each imf and rn can be calculated using the rate of variance of each component to the sum of the variance of all the components as follows liu et al 2018 8 r c j σ x j i 1 n 1 σ x i 100 where n is the number of imfi σxi is the variance of the corresponding imfi and σxi 1 is the variance of rn rcj represents the relative contribution of the corresponding component to the overall variation the improved typology based on the eemd trend consists of four change types table 1 the significance test approach to test the eemd trends has been provided in the previous studies pan et al 2018 xu et al 2020 the number of the white noises in the significance test approach was set as 50 000 for relatively high accuracy and relatively short calculating time 2 2 3 attribution analysis to disentangle the driving factors of the ecosystem wue we employed variation partitioning and multiple linear regression models mlr multiple linear regression model has been extensively used to assess the interlinkages between global ecosystems and climate marcolla et al 2020 wang et al 2020a yang et al 2022 as this method is one of the most simple and effective to diagnose the sensitivity of ecosystems to climate change in this study regressions between wue and temperature precipitation the amount of solar radiation and the atmospheric co2 concentration were estimated at the pixel level using the matlab function fitlm the sensitivities of wue to climate changes were separately estimated according to the following equation 9 δ w u e δ w u e tem δ w u e pre δ w u e rad δ w u e c a δ w u e δ t e m dtem dt δ w u e δ p r e dpre dt δ w u e δ r a d drad dt δ w u e δ c a dca dt where δwuetem δwuepre δwuerad and δwueca are the variations in wue contributed by temperature precipitation the amount of solar radiation and the atmospheric co2 concentration respectively δ w u e δ x is the sensitivity index si of wue to an explanatory variable x dx dt represents the secular trends of x the climatic sensitivity index of wue is estimated as the standardized regression coefficients of the multiple linear regression performed using the eemd trend of wue against the eemd trends of all listed explanatory variables for 1982 2015 marcolla et al 2020 finally the contribution of all of the explanatory variables to wue can be estimated as follows 10 r x s i x s i tem s i pre s i rad s i ca 100 where rx is the contribution of an explanatory variable x we conducted partial correlation analysis to partition the effects of the climatic conditions on the relative contribution of the secular trends partial correlation analysis refers to the process of removing the influence of the third variable when two variables are simultaneously related to the third variable and thus only analyzes the correlation between the other two variables the climatic conditions included the climatic mean state and the climatic variability the climatic mean state and climatic variability were represented by the mean value and standard deviation of each explanatory variable respectively we also explored the relative contribution of the secular trends along topographical gradients the topographical factors including elevation slope and aspect were extracted from the shuttle radar topography mission srtm digital terrain model dem in arcgis using 3d analyst tools the elevation of the study area was divided into intervals every 100 m the slope of the study area was divided into intervals every 1 from 0 m to 85 the aspect of the study area was divided into intervals every 5 from 0 to 360 and then the aspect of 45 90 135 180 225 270 and 315 reclassifies as the northeast ne east e southeast se south s southwest sw west w and northwest nw aspects respectively finally we investigated the topographical gradients through the response curves between the relative contribution of the secular trends and topographical factors the brief flow scheme of wue time series analysis is shown in fig 2 3 results 3 1 the secular trends of wue 3 1 1 temporal dynamics of ecosystem wue the national pattern of the average wue exhibited a significant increase trend with a rate of 0 0123 gc kg 1h2o yr 1 during 1982 2015 p 0 05 the eemd trend also exhibited a significant increasing trend p 0 05 over the entire period fig 3 a in addition the eemd rate of increasing trend increased from 0 0086 gc kg 1h2o yr 1 in 1983 to 0 0198 gc kg 1h2o yr 1 in 2015 nevertheless the increment of the eemd rate decreased over time among the different land cover types forests except for dnf had relatively high wue especially dbf which had the highest fig 3 b gra and ub had the lowest wue among all of the land cover types and all of the artificial vegetation respectively the spatial distribution of the multi year average wue exhibited strong spatial heterogeneity fig 3 c with higher values in northeastern and southeastern china sub humid and humid zones and lower values in northwestern china and the qinghai tibet plateau 3 1 2 spatial distribution of the types of the secular trends fig 4 shows the spatial distribution of the different trend types with the results of the significance test a total of 46 20 34 19 significant and 4 68 1 86 significant of the study area experienced monotonic increase and monotonic decrease trends of wue which indicate the improvement and degradation of the local ecosystems respectively among them the significant monotonic increase trends mainly occurred in the southern china the north china plain and the northeast china plain while the significant monotonic decrease trends were sporadically distributed in easternmost china and the sichuan basin although almost half of china exhibited increasing trends in the past few decades the eemd results revealed that trend shifts actually occurred in many regions there was a large proportion 21 52 of negative reversal trends in taiwan the inner mongolia plateau and the tibetan plateau indicating a shift from improvement to degradation fortunately only half 10 64 were statistically significant in addition positive reversal trends accounted for the second largest regions 27 60 after monotonic increase trends this type of trend was predominantly distributed in southwestern china the changbai mountain the khingan range and the loess plateau indicating a shift from degradation to improvement except northeastern china most of these regions had statistically significant trends the area percentages of each significant type of secular trends varied greatly among the land cover types fig 5 expect for the deciduous forests and mf all of the other land cover types predominantly exhibited monotonic increase trends the area percentage of cro with monotonic increase trends was much higher than other land cover types in evergreen forests negative reversal trends occupied the second largest area percentage while positive reversal trends occupied in sav gra cro ub and cnv in mf and dbf the dominant type was positive reversal while positive reversal and negative reversal occupied almost the same area percentage in dnf 3 1 3 the dominant climatic factors of the secular trends the relative contributions of the underlying climatic factors controlling the secular trends in ecosystem wue were calculated for each pixel by taking an empirical approach based on multiple linear regression we identified the relevant variables over china where the wue secular trends can be explained by the atmospheric co2 concentration 46 followed by temperature 29 solar radiation 14 and precipitation 11 fig 6 temperature dominated areas were mainly located in northeastern china and xinjiang only the north china plain exhibited significant positive relationship with temperature while other areas exhibited neutral or negative relationship fig 7 a precipitation only controlled the southwestern tibeten plateau and most was negatively influenced fig 7 b the amount of solar radiation was found to be the primary positive determining factor in the guangxi and guangdong fig 7 c co2 dependent wue secular trends were found to be widely distributed throughout the north china plain the loess plateau and southwestern china fig 7 d in most of these areas wue secular trends were synergistic with the secular trends of the atmospheric co2 concentration 3 2 relative contribution of secular trends to wue changes 3 2 1 spatial distribution of relative contribution of secular trends fig 8 shows the spatial distribution of the relative contribution rc of the secular trends the areas with a low rc 30 accounted for more than half of china and were concentrated in southwestern china the southeastern coast the changbai mountain and the inner mongolian plateau the areas with a high rc 70 only accounted for a small proportion and were concentrated in the northern china plain and the loess plateau the areas with a moderate rc 30 70 occupied slightly more than a quarter of the study area but their distribution was quite scattered and was largely concentrated in the northeast china plain and southern china in general the area percentage of rc decreased as the value of rc increased from the perspective of the land cover types fig 9 cro had the highest rc followed by cnv gra and ub cro was the only land cover type for which rc 50 of the secular trends was greater than that of the interannual variations iav rc of the secular trends was similar in sav ebf and enf rc was much lower in coniferous forests than in broadleaf forests moreover rc was much smaller in natural vegetation except for gra than in cro ub and cnv 3 2 2 relative contribution of secular trends along topographical gradients we investigated the changes in rc with three topographical factors elevation slope and aspect fig 10 rc exhibited roughly a bimodal pattern with elevation and aspect gradient but a u shaped pattern with the slope in the areas below 800 m rc had a significant negative relationship with elevation decreasing from 70 to 30 at elevations of 800 5000 m rc remained somewhat stable fluctuating between 30 and 40 fig 10 a at elevations above 5000 m rc decreased rapidly to 15 in the scarce high elevation area the relationship between rc and elevation was significantly negative compared to elevation a much more pronounced negative trend was found between rc and slope particularly in the low slope areas fig 10 b in areas with slopes 12 rc rapidly decreased from 50 to 37 nonetheless a slowing or even hiatus of this downward trend occurred in the medium slope areas interestingly rc was higher in the areas with eastern southern and southeastern aspects while it was much lower in the areas with northern western and northwestern aspects fig 10 c 3 2 3 the effects of climatic conditions on the relative contribution of the secular trends to identify the underlying factors controlling the relative contributions of the secular trends we analyzed the partial correlation coefficients between the climatic conditions and rc fig 11 we found that the coefficients between the climatic mean state and rc were generally higher than those between the climatic variability and rc the effects of the amount of solar radiation and the atmospheric co2 concentration on rc were significantly stronger than those of temperature and precipitation however their effects are opposite that is radiation had a positive effect while the atmospheric co2 concentration had a negative effect interestingly rc was positively related with the variability of the atmospheric co2 concentration especially in monotonic increase trends and positive reversal trends whereas it was negatively related with the mean state of the atmospheric co2 concentration the effects of climate changes on monotonic decrease trends and negative reversal trends were visibly weaker than those on monotonic increase trends and positive reversal trends 3 3 relative contributions of different trend types monotonic increase trends had the highest average rc followed by positive reversal trends monotonic decrease trends and negative reversal trends fig 12 in particular monotonic increase trends had very high rc values in the north china plain and the southern part of northeast china plain as well as a slightly lower rc values in southern china fig 13 a moreover according to the frequency distribution rc values of monotonic increase trends were predominantly medium values most of the areas that exhibited monotonic decrease and negative reversal had extremely low rc values fig 13 b and fig 13 c for instance the inner mongolia grasslands exhibited negative reversal trends with rc 30 the same pattern was observed in the northern part of the greater khingan range and taiwan island positive reversal trends with high rc values occurred in the loess plateau and the mountains in xinjiang fig 13 d it should be noted that although a large proportion of the southwestern china exhibited positive reversal trends their rc values were rather low the percentage of the areas that exhibited positive reversal trends with relatively high rc values was much greater the proportion that exhibited negative reversal trends with high rc values it can be seen from fig 14 that monotonic increase trends always made higher relative contribution than positive reversal trends or negative reversal trends in all of the land cover types the relative contributions of monotonic increase trends in cro and cnv was all 50 and those in gra and ub were quite close to 50 thus for these four land cover types the contribution of monotonic increase trends to the overall wue changes surpassed the contribution of the iav this indicates stable improvement in those ecosystems which was not easily affected by perturbations however the relative contributions of monotonic increase trends in dbf and mf were 30 indicating low ecosystem stability except for the artificial land cover types such as cro ub and cnv the relative contributions of positive reversal trends in the other land cover types were considerably high and was only slightly lower than that of monotonic increase trends indicating that the early degradation had changed to improvement and this improvement may not be modified easily in the future for monotonic decrease trends the relative contributions were always 25 except for cro and ub this suggests that the degradation in cro and ub will be much harder to change the relative contributions of negative reversal trends were always 20 especially in the deciduous forests thus although the trends changed from improvement to degradation the relative contribution was low indicating that the degradation can be easily reversed thus it is highly possible for ecological engineering to improve the ecosystem functioning of the deciduous forests 4 discussion 4 1 nonlinearity of the secular trends in this study we first estimated the multi year average wue at the national scale and in different land cover types we found that there were large differences in wue due to the high spatial heterogeneity wue was unexpectedly high in northeastern china indicating high ecosystem functioning the multi year mean wue of the different land cover types exhibited the following order dbf dnf ebf sav cnv mf cro enf gra this was in good agreement with the results of previous researches based on fluxnet sites tang et al 2015 and process based model simulations liu et al 2015 zhang et al 2014 wue of the forests was greater than those of the croplands and grasslands indicating that higher ecosystem functioning in the forests previous studies based on linear analysis reported that the increasing trends were widely distributed in northeastern and central china while the decreasing trends were scattered in southern china guo et al 2019 sun et al 2021 sun et al 2018 similarly we found that the types of the secular trends were dominated by predominantly monotonic increase trends with a very low area proportion of monotonic decrease trends however using eemd methods we also found that 27 60 of china exhibited positive reversal trends and 21 52 exhibited negative reversal trends this was rarely revealed by the linear analysis in the previous studies by considering the nonlinearity of the secular trends we determined the intrinsic trend shifts of wue providing a deeper understanding of the ecosystem functioning the areas with monotonic increase trends were widely distributed in the north china plain and the northeast china plain we found that monotonic increase trends in northeast china plain were positively affected by temperature fig 7 because it is located in the cold temperate zone fig s3 the northeast china plain is stressed by heat more than water increases in air temperature can not only enhance photosynthesis but also can supply more nutrients for vegetation growth by accelerating the decomposition of soil organic matter lin et al 2020 moreover warming leads to more snowmelt and higher soil temperatures piao et al 2019 therefore warming can to large extent increase vegetation productivity different from the northeast china plain which was mainly caused by temperature monotonic increase trends in the north china plain were controlled by the increase in the atmospheric co2 concentration fig 6 the increase in the atmospheric co2 concentration can not only increase vegetation productivity due to the co2 fertilization effect lin et al 2020 wang et al 2020b but can also reduce et by reducing transpiration via decreasing the stomatal conductance of leaves li et al 2010 shi et al 2013 therefore an increase in the atmospheric co2 concentration will significantly increase wue fig 7 in addition croplands are widely distributed in these areas and thus agronomic activities should be considered the effects of agronomic activities on wue have been reported deng et al 2006 ji et al 2021 jin et al 2018 improved water saving projects including water saving irrigation limited irrigation and dryland cultivation enhance wue through the precise control of the timing and quantity of irrigation water supply deng et al 2006 tian et al 2020 moreover the heavy use of fertilizers and cultivar renewal generally increase the yields of croplands ai et al 2020 lu and tian 2017 zhang et al 2013 therefore climate change and agronomic activities jointly led to the significant improvement of the cropland ecosystems positive reversal trends occupied slightly more areas than negative reversal trends the areas with significant positive reversal trends were mainly located in the loess plateau and southwestern china these areas were both ecologically fragile and economically undeveloped these dual pressures from the ecology and economy led to ecosystem degradation in the early stage since the end of the 20th century a series of government led ecological projects have been launched for example the three north shelter forest program and the grain for green project cao et al 2020 ding et al 2021 jin et al 2018 these ecological projects significantly increased the vegetation coverage and thus have promoted ecosystem functioning and have prevented desertification in the loess plateau cao et al 2020 li et al 2017 and rocky desertification in southwestern china tong et al 2018 respectively furthermore we found that wue in these areas was controlled by the atmospheric co2 concentration fig 7 therefore the ecological projects and the significant positive effects of elevated co2 concentration on photosynthesis increased wue the areas with significant negative reversal trends were sparsely distributed in the tibetan plateau and the inner mongolia plateau in these areas wue was predominantly influenced by temperature and was positively correlated with temperature and precipitation warming and the accompanying drought may reduce stomatal conductance and lead to early water saving survival strategies and lately drop in wue allen et al 2003 de boeck et al 2006 niu et al 2011 moreover although the increased temperature could enhance vegetation productivity lin et al 2020 warming had greater influences on the vegetation transpiration and soil evaporation gaertner et al 2019 huntington 2010 which may have offset its positive effect on the ecosystem productivity more attention should be paid and more actions taken to improve the ecosystem functioning of the areas with negative reversal trends 4 2 stability of the secular trends although the spatial distributions of the types of the secular trends have been determined using different methods in previous studies cao et al 2020 horion et al 2019 huang et al 2015 the relative contributions of secular trends in different trend types to wue changes remain unclear our results suggest that areas that exhibited the monotonic increase trends and positive reversal trends with high rc values were mainly distributed in the north china plain and the loess plateau this suggests that the wue changes were dominated by secular trends rather than the inter annual variations in these areas and the improvement of ecosystem functioning had strong stability the strong stability in the north china plain and the loess plateau implies the current agricultural intensification and the implementation of ambitious ecological projects would benefit future improvement however low rc with the improvement of ecosystem functioning was also found in southwestern china this implies that these areas were experiencing unsustainable improvement with poor ecosystem stability in recent years several studies have also suggested that ecological projects promoted the improvement of the ecosystem functioning cao et al 2020 feng et al 2016 zheng et al 2019 however ecological projects may not promote the stability of the improvement our results suggest that in some areas the expected improvement of ecosystem functioning will not be easy to maintain and may be easily altered by the short term external disturbance in the future afforestation in southwestern china has largely enhanced ecosystem productivity and ecosystem functioning whereas the resultant increase of evapotranspiration et may instead have limiting effects on the future increase of wue this leads to the low stability of the ecosystem improvement challenges still exist in terms of maintaining the current improvement of ecosystem functioning in these areas on the contrary although unacceptable monotonic decrease trends and negative reversal trends accounted for more than a quarter of the study areas their low rc implies that it will be somewhat easy to recover from degradation our results demonstrate that more than half of the study area exhibited a relatively low relative contribution of the secular trends to the overall changes indicating a low stability of the secular trends thus the ecosystem improvement or degradation in some local ecosystems may be limited and overstated this also means that there are risks hidden under the apparent ecosystem improvement or potential opportunities beneath the apparent ecosystem degradation we found that rc had higher sensitivities to climatic mean state rather than variability which is consistent with the results of a previous study on ecosystem resilience stability feng et al 2021 compared with temperature and precipitation the amount of solar radiation and the co2 concentration tend to have stronger effects on the secular trends of the ecosystem functioning however their effects are opposite that is radiation had a positive effect while co2 concentration had a negative effect thus although they both exhibited monotonic increase trends the north china plain and southern china generally had higher rc values than the northeast china plain and the inner mongolia plateau the high co2 concentrations led to reduced stomatal conductance and further led to photo adaptation or photosynthetic down regulation which instead alleviated the stability of ecosystem functioning changes li et al 2020 sanz sáez et al 2010 zhou et al 2013 thus in the context of global warming future higher co2 concentrations may be harmful to the stability of ecosystem functioning improvement and may even cause trend shifts occur topographical factors may affect vegetation growth by controlling the hydrothermal conditions in our study it was found that elevation had a negative effect on rc in the low altitude and high altitude areas in the low altitude areas as the elevation increased the proportion of croplands decreased rapidly the climatic condition varied intensely with increasing elevation which made the ecosystems more sensitive to external disturbances thus rc of the secular trends decreased in the high altitude areas i e the tibetan plateau as the elevation increased the environment became harsher and unsuitable for vegetation growth piao et al 2012 and ecosystems become more sensitive to short term climate change and thus rc of secular trends become much lower as the slope increased rc of the secular trends exhibited a non linear u shaped pattern rc decreased in the low slope areas 20 croplands always have relatively high rc values and were usually distributed in the low slope areas and thus the low slope areas had higher rc values nutrients and water are more difficult to preserve and external disturbances become more frequent as the slope increases thus ecosystems seem to be more fragile and more sensitive to external disturbances as the slope increases it has been reported that the grain for green project was launched in the mid slope and high slope areas aiming to change arable sloped croplands and wasted land into forests and grasslands chen et al 2015 due to the implementation of the grain for green project and the resulting wide distribution of forests and grasslands in the mid slope and high slope areas rc of the secular trends decreased at a lower rate in these areas however the low value of rc indicates that the current improvement in ecosystem functioning by ecological projects is still in low stability and thus the actions should still be paid regarding the aspects high rc of secular trends was high in the areas with east and south aspects while it was low in the areas with west aspect this suggests that ecosystems in the areas with the east and south aspects had strong ecosystem stability while the ecosystems in the areas with the west aspect were vulnerable the effect of the aspect on ecosystem stability seems to be primarily attributable to water availability especially precipitation the east asian monsoon prevails in eastern and southern of china and thus southeast east and south are the main windward directions precipitation is a limiting factor for most biomes and it can have dramatic influences on ecosystems medvigy et al 2010 ritter et al 2020 thus in the areas with the southeast east and south aspects rc was high and the ecosystems were stable due to abundant and steady precipitation due to the leeward nature the areas with the west and northwest aspects did not receive sufficient precipitation which led to vulnerable ecosystems moreover radiation may also play an important role as more sunshine duration in windward slopes thus rc was low in the areas with the west and northwest aspects our results suggest that the improvement of the ecosystem functioning with strong stability occurred in gra cro ub and cnv nevertheless the improvement of the ecosystem functioning had lower stability in the deciduous forests and the mixed forest this implies the satisfactory improvement of ecosystem functioning is still vulnerable and sensitive to external disturbance in the meantime the monotonic decrease and negative reversal trend change type also had low rc indicating the slight difficulty of ecosystem improvement for deciduous forests and mixed forests 4 3 uncertainties and limitations gpp accurately has been estimated for many years in previous studies but estimating et accurately is still a challenge li et al 2010 although the et datasets have been validated using plot scale eddy covariance observations and basin wide water balance values there are still some uncertainties due to the small amount of field data for validation gentine et al 2019 we suggest that more fieldwork is needed to reduce uncertainties in the future climate change and human activities both contribute to wue changes although four climatic variables were used in this study human activities such as irrigation are hard to quantify due to the lack of long term datasets in this study we focused primarily on the effects of climate changes and thus the results on human activities was not presented however we suggest that when long term datasets on human activities become more available future studies should pay attention to the anthropic impacts on wue changes fluctuations in ecosystem functioning depend on environmental variations we explored the climatic controls at the annual scale yet debate remains at the seasonal scale the effects of environmental factors on ecosystems in spring may differ from the effects in summer and the effects in autumn future studies can focus on the seasonal partitioning and the seasonal compensation of water and temperature controls on wue although this study has a few inevitable limitations we nevertheless believe that our results can help future researchers to better understand the nonlinearity of the secular trends in wue and the stability of the secular trends these results also can provide scientific support for the ecological protection projects 5 conclusions in this study we explored the secular trends of the ecosystem functioning and evaluated their stability using the relative contribution of secular trends to the overall changes across china during 1982 2015 we found that although 46 2 of the study area experienced monotonic increase trends almost half 49 12 of the study area experienced trend shifts analyses based on variation partitioning revealed that the atmospheric co2 concentration and temperature were the two main factors that affected the wue evolutions although ecosystem improvement widely occurred the high relative contribution of these wue increasing or positive reversal trends mainly occurred in north china plain and loess plateau ecosystem stability in artificial vegetation was generally higher than that in natural vegetation except in grassland topographical factors affected ecosystem stability the relative contribution of secular trends presented a bimodal pattern along elevation and aspect gradient but a u shaped pattern against the slope the climatic mean state had larger effects on ecosystem stability than climatic variability our study highlighted the importance of the nonlinear trends of ecosystem wue and the ecosystem stability which was neglected in former studies the findings of this study are expected to deepen our understanding of current wue evolutions and help predict future wue changes credit authorship contribution statement fusheng jiao data curation conceptualization supervision methodology writing original draft writing review editing xiaojuan xu visualization methodology writing review editing mingyang zhang funding acquisition supervision writing original draft haibo gong visualization conceptualization methodology huiyu liu methodology supervision writing original draft writing review editing funding acquisition kelin wang supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the national natural science foundation of china no 41971382 32271663 31870454 u19a2051 the key research and development program of china 2021yfb3901104 the special fund of the chinese central government for basic scientific research operations in the commonweal research institute gyzx210405 the priority academic program development of jiangsu higher education institutions 164320h116 and jiangsu education department no 1812000024462 we thank letpub for its linguistic assistance during the preparation of this manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128690 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2648,ecosystem water use efficiency wue is an effective indicator of ecosystem functioning which is widely used to characterize the coupling relationship between terrestrial carbon and the water cycle although the linear trends in wue have been well reported the nonlinearity of the secular trends in wue and their relative contribution to the overall changes were still poorly investigated in this study we used ensemble empirical mode decomposition eemd to extract the secular trends of the wue and their relative contribution rc to the wue changes in china which represents the stability of the ecosystem functioning we found that although increasing trends dominated the secular trends of the wue nearly half of the study areas had experienced trend shifts except that deciduous forests and mixed forests were dominated by trend shifts most of the land cover types were dominated by increasing trends the rc exhibited a bimodal pattern along the elevation gradient with peaks at 20 m and 5000 m similarly the rc peaked at the east and south aspects and was the lowest for the west and north aspects in comparison the rc exhibited a u shaped pattern with the slope and was the lowest at 80 for the monotonically increasing trends and decreasing to increasing trends the high rc mainly occurred on the north china plain and loess plateau indicating ecosystem improvement and strong stability meanwhile it was low in northeastern china and southwestern china indicating unsustainableecosystem improvement the rc was always very low for increasing to decreasing trends and monotonically decreasing trends indicating low ecosystem stability with a high possibility for improvement temperature and the atmospheric carbon dioxide co2 concentration were the two main climatic factors driving the wue changes the climatic mean state rather than the climatic variability affected the ecosystem stability the results of this study highlight the relative contribution of the secular trends to wue changes and provide a better understanding of the structure and the stability of ecosystem functioning keywords water use efficiency wue ensemble empirical mode decomposition eemd nonlinear trend ecosystem stability data availability data will be made available on request 1 introduction plants take in carbon dioxide co2 through photosynthesis and generate an inevitable loss of water via transpiration guerrieri et al 2019 it has been proposed that plants control stomata to optimally satisfy the compromise between the amount of carbon gain and the amount of water loss beer et al 2009 water use efficiency wue which is characterized by the rate of carbon uptake per unit of water loss is used to quantify the trade off between carbon gain and water loss beer et al 2009 horion et al 2019 wue links biological and physical processes in terrestrial ecosystems and underlies global scale vegetation climate interactions and the terrestrial water cycle belmecheri et al 2021 cheng et al 2017 frank et al 2015 huang et al 2017 niu et al 2011 tang et al 2021 yu et al 2008 it is well known that when an ecosystem is stressed by environmental conditions its response may change due to survival strategies or direct loss of productivity hence as a vital indicator of terrestrial ecosystem functioning the change in wue is a critical parameter for assessing the hydro climatic status of terrestrial ecosystems horion et al 2019 wue changes include not only secular trends but also inter annual variability ponce campos et al 2013 affected by environmental changes wue changes are characterized by large temporal variability correct knowledge of wue trends would contribute to a deeper comprehension for ecosystem functioning assessment although the leaf level wue has been well studied via physiological processes our understanding of wue dynamics remains largely ambiguous at the ecosystem level cao et al 2020 previous studies based on flux data have found that wue has been increasing in china since the very beginning of the 21st century niu et al 2011 yu et al 2008 with the emerging satellite and earth observation technology remote sensing datasets are now available for detecting wue changes the national oceanic and atmospheric administration noaa advanced very high resolution radiometer avhrr datasets which can provide the longest record recently are helpful to achieve insights of ecosystem functioning changes recent studies based on remote sensing found that increased wue was widely distributed in high latitudes but decreased wue also occurred in low latitudes guo et al 2019 huang et al 2015 lin et al 2020 liu et al 2015 sun et al 2018 many studies have estimated the trends in wue through linear regression or piecewise linear regression however due to the effects by non stationary climate changes the trends in vegetation growth and carbon sink change always change over time and ever reverse piao et al 2020 which cannot be revealed via linear analysis other methods like piecewise linear regression models plr breaks for additive seasonal and trend analysis bfast and detecting breakpoints and estimating segments in trend analysis dbest can also been proposed to gain further insight into ecosystem functioning changes but these methods both have inevitable flaws more specifically the predetermined assumption of a constant trend makes the models not only sensitive to short term fluctuations and abrupt changes but also to the extension of time series hence it is crucial to explore the nonlinear characteristics of the secular trends to assess the terrestrial ecosystem functioning in this study an adaptive method named ensemble empirical mode decomposition eemd was used to extract the gradual and nonlinear secular trends of wue this method has been widely proven to be suitable for use in hydrological climatic and ecological research chang et al 2011 franzke 2012 liu et al 2016 wang et al 2015a wang et al 2015b wang et al 2013 yin et al 2017 as was mentioned above several researches have assessed the secular trends of wue changes and the climatic controls at both the leaf scale and the ecosystem scale lin et al 2020 niu et al 2011 sun et al 2018 wang et al 2021 zheng et al 2019 zou et al 2020 previous studies mainly focused on the types i e increasing and decreasing and the rates i e rapid and slow of the trends xu et al 2020 xu et al 2021 but little research has been conducted on the relative contribution of the trends to the total wue changes the type and rate of the trends can merely describe the direction and speed of the wue changes respectively but they can hardly illustrate the structure and stability of wue an ecosystem wue trend with a high relative contribution means that wue is dominated by secular trends and is resistant to perturbations and hence it is barely possible for trend shifts in contrast an ecosystem wue trend with a low relative contribution means that wue is dominated by inter annual variations and is vulnerable to perturbations and thus trend shifts are highly likely although some studies have explored the stability of ecosystem functioning through the coefficient of variation cv bai et al 2004 song and yu 2015 zhang et al 2022 cv cannot distinguish between the relative contribution of the secular trends and the inter annual variations to the overall changes and thus it is unable to reveal the structure of wue changes investigation of the relative contribution is not only beneficial to understand the structure of wue changes but is also helpful in evaluating the ecosystem stability and further in better projecting the future changes under the context of global warming piao et al 2020 hence it is vital to investigate the contribution of the secular trends to the total wue changes in order to assess ecosystem functioning moreover previous studies focused on the water and temperature controls on the ecosystem functioning changes but ignored the atmospheric co2 concentration which is also a determining factor of wue sun et al 2018 wang et al 2021 yu et al 2008 zhang et al 2014 zheng et al 2019 zou et al 2020 huang et al 2015 concluded that the atmospheric co2 concentration was the primary controlling factor of wue considering that environmental factors including temperature precipitation solar radiation and the atmospheric co2 concentration all affect wue it is necessary to quantify the impact of each factor on wue in this study we used well validated datasets to investigate the nonlinear trends of ecosystem wue and their relative contribution to the overall wue changes in china and more specifically 1 the spatial distribution of the nonlinear secular trends of wue 2 the relative contribution of the secular trends to the overall wue changes and 3 how climate factors affect the secular trends and their contributions to the overall changes 2 materials and methods 2 1 data the global land surface satellite glass gross primary productivity gpp datasets with a spatial resolution of 0 05 for 1982 2015 were used in this study https www glass umd edu the glass gpp product which has been widely used in global and regional ecosystem carbon cycle studies was generated using the bayesian algorithm ensemble of eight widely used light use efficiency models yuan et al 2010 and it has been validated using 155 global eddy covariance sites which covered nine major terrestrial biomes this product has been well validated and shows more accuracy than many other gpp products and thus has been applied in previous ecosystem carbon cycles studies at global and regional scale yuan et al 2010 the terrestrial evapotranspiration et datasets across china with a spatial resolution of 0 1 for 1982 2015 were obtained from national tibetan plateau data center https data tpdc ac cn this et product was simulated through a nonlinear complementary relationship model and validated against 13 eddy covariance measurements and 10 river basins measured by nash sutcliffe efficiency in the range of 0 72 0 94 ma et al 2019 hence this et product is considered more accurate than other mainstream et products the land cover types were delineated according to the moderate resolution imaging spectroradiometer modis mcd12c1 product friedl et al 2010 using the international geosphere biosphere programme igbp land cover classification scheme loveland and belward 1997 only the land cover types that remained unchanged during the period 2001 2015 were retained fig s1 including evergreen needle leaf forest enf evergreen broad leaf forest ebf deciduous needle leaf forest dnf deciduous broad leaf forest dbf mixed forest mf savannas sav grasslands gra croplands cro urban and built up ub croplands natural vegetation mosaic cnv mean air temperature tem total precipitation pre and downward shortwave radiation rad can represent the demand for heat water and energy for vegetation growth respectively and thus were used to study the effects of climate changes the terraclimate datasets which were accessed from climatology lab abatzoglou et al 2018 were adopted in this study these datasets can provide monthly climate and climatic water balance data for global terrestrial surfaces from 1958 to 2020 at a spatial resolution of 1 24 recently these dataset have been used as a high spatial resolution and time varying climatic input in ecological and hydrological studies klein et al 2022 li et al 2022 the daily global co2 concentration ca product was downloaded from jena carboscope rödenbeck et al 2018 we calculated the average value of the daily global co2 concentration and used it to calculate the annual global co2 concentration all data were unified at a spatial resolution of 0 1 0 1 using the nearest or majority methods to match with the et datasets detailed information about all of the datasets used in this study is presented in table s1 2 2 methods 2 2 1 definition of wue the ecosystem scale wue is an effective indicator of ecosystem functioning and is used to characterize the coupling relationship between the water and carbon cycle migliavacca et al 2021 reichstein et al 2014 the ecosystem wue can be calculated at the pixel scale as the ratio between annual gpp and annual et as follows 1 wue gpp et where the unit of wue is gc kg 1h2o the unit of gpp is gc m 2 a 1 and the unit of wue is kgh2o m 2 a 1 2 2 2 eemd the eemd method decomposes nonlinear and nonstationary time series data into a finite number of stationary intrinsic mode functions imf imfi i 1 2 n and a residual rn imfi is the ith imf obtained by the eemd method and rn is the residual component after n imfs are decomposed and screened each imf component contains local characteristic signals of different time scales and rn contains the secular trend of the original signal wang et al 2015a wu and huang 2009 the principle of the eemd method is to repeatedly add different uniformly distributed white noises to the raw data and then to take the ensemble mean of the imfs and rn as the final results after the sifting processes which not only alleviates the mode mixing problem but also retains the original information in the data since the added white noises can be inter neutralized when taking the ensemble mean wang et al 2015a wang et al 2015b the eemd sifting processes are conducted as follows fig 1 1 add a set of white noise series n t the amplitude was 0 2 standard deviations of the raw data in this study to the raw time series x t and obtain a new time series y t 2 y t x t n t 2 obtain the upper and lower envelope curves eu t and el t by connecting the local maxima and minima with splines respectively and then obtain the mean of the upper and lower envelope curves m t 3 m t e u t e l t 2 3 obtain h t by subtracting m t from y t and then judge whether m t satisfies the stop criterion m t is close to 0 at any point if it does h t is the first imf if it does not take h t as the new time series y t and repeat step 2 4 a 1 1 t h t y t m t 4 obtain r t by subtracting h t from y t determine whether r t has an oscillatory component if it does take r t as the new time series and repeat step 2 if it does not stop sifting 5 r 1 t y t a 1 1 t 6 r i t r i 1 t a i 1 t 5 repeat steps 1 4 m times 100 in this study and then obtain the ensemble mean of each component as the final results 7 im f i t 1 m j 1 m a i j t the relative contribution of each imf and rn can be calculated using the rate of variance of each component to the sum of the variance of all the components as follows liu et al 2018 8 r c j σ x j i 1 n 1 σ x i 100 where n is the number of imfi σxi is the variance of the corresponding imfi and σxi 1 is the variance of rn rcj represents the relative contribution of the corresponding component to the overall variation the improved typology based on the eemd trend consists of four change types table 1 the significance test approach to test the eemd trends has been provided in the previous studies pan et al 2018 xu et al 2020 the number of the white noises in the significance test approach was set as 50 000 for relatively high accuracy and relatively short calculating time 2 2 3 attribution analysis to disentangle the driving factors of the ecosystem wue we employed variation partitioning and multiple linear regression models mlr multiple linear regression model has been extensively used to assess the interlinkages between global ecosystems and climate marcolla et al 2020 wang et al 2020a yang et al 2022 as this method is one of the most simple and effective to diagnose the sensitivity of ecosystems to climate change in this study regressions between wue and temperature precipitation the amount of solar radiation and the atmospheric co2 concentration were estimated at the pixel level using the matlab function fitlm the sensitivities of wue to climate changes were separately estimated according to the following equation 9 δ w u e δ w u e tem δ w u e pre δ w u e rad δ w u e c a δ w u e δ t e m dtem dt δ w u e δ p r e dpre dt δ w u e δ r a d drad dt δ w u e δ c a dca dt where δwuetem δwuepre δwuerad and δwueca are the variations in wue contributed by temperature precipitation the amount of solar radiation and the atmospheric co2 concentration respectively δ w u e δ x is the sensitivity index si of wue to an explanatory variable x dx dt represents the secular trends of x the climatic sensitivity index of wue is estimated as the standardized regression coefficients of the multiple linear regression performed using the eemd trend of wue against the eemd trends of all listed explanatory variables for 1982 2015 marcolla et al 2020 finally the contribution of all of the explanatory variables to wue can be estimated as follows 10 r x s i x s i tem s i pre s i rad s i ca 100 where rx is the contribution of an explanatory variable x we conducted partial correlation analysis to partition the effects of the climatic conditions on the relative contribution of the secular trends partial correlation analysis refers to the process of removing the influence of the third variable when two variables are simultaneously related to the third variable and thus only analyzes the correlation between the other two variables the climatic conditions included the climatic mean state and the climatic variability the climatic mean state and climatic variability were represented by the mean value and standard deviation of each explanatory variable respectively we also explored the relative contribution of the secular trends along topographical gradients the topographical factors including elevation slope and aspect were extracted from the shuttle radar topography mission srtm digital terrain model dem in arcgis using 3d analyst tools the elevation of the study area was divided into intervals every 100 m the slope of the study area was divided into intervals every 1 from 0 m to 85 the aspect of the study area was divided into intervals every 5 from 0 to 360 and then the aspect of 45 90 135 180 225 270 and 315 reclassifies as the northeast ne east e southeast se south s southwest sw west w and northwest nw aspects respectively finally we investigated the topographical gradients through the response curves between the relative contribution of the secular trends and topographical factors the brief flow scheme of wue time series analysis is shown in fig 2 3 results 3 1 the secular trends of wue 3 1 1 temporal dynamics of ecosystem wue the national pattern of the average wue exhibited a significant increase trend with a rate of 0 0123 gc kg 1h2o yr 1 during 1982 2015 p 0 05 the eemd trend also exhibited a significant increasing trend p 0 05 over the entire period fig 3 a in addition the eemd rate of increasing trend increased from 0 0086 gc kg 1h2o yr 1 in 1983 to 0 0198 gc kg 1h2o yr 1 in 2015 nevertheless the increment of the eemd rate decreased over time among the different land cover types forests except for dnf had relatively high wue especially dbf which had the highest fig 3 b gra and ub had the lowest wue among all of the land cover types and all of the artificial vegetation respectively the spatial distribution of the multi year average wue exhibited strong spatial heterogeneity fig 3 c with higher values in northeastern and southeastern china sub humid and humid zones and lower values in northwestern china and the qinghai tibet plateau 3 1 2 spatial distribution of the types of the secular trends fig 4 shows the spatial distribution of the different trend types with the results of the significance test a total of 46 20 34 19 significant and 4 68 1 86 significant of the study area experienced monotonic increase and monotonic decrease trends of wue which indicate the improvement and degradation of the local ecosystems respectively among them the significant monotonic increase trends mainly occurred in the southern china the north china plain and the northeast china plain while the significant monotonic decrease trends were sporadically distributed in easternmost china and the sichuan basin although almost half of china exhibited increasing trends in the past few decades the eemd results revealed that trend shifts actually occurred in many regions there was a large proportion 21 52 of negative reversal trends in taiwan the inner mongolia plateau and the tibetan plateau indicating a shift from improvement to degradation fortunately only half 10 64 were statistically significant in addition positive reversal trends accounted for the second largest regions 27 60 after monotonic increase trends this type of trend was predominantly distributed in southwestern china the changbai mountain the khingan range and the loess plateau indicating a shift from degradation to improvement except northeastern china most of these regions had statistically significant trends the area percentages of each significant type of secular trends varied greatly among the land cover types fig 5 expect for the deciduous forests and mf all of the other land cover types predominantly exhibited monotonic increase trends the area percentage of cro with monotonic increase trends was much higher than other land cover types in evergreen forests negative reversal trends occupied the second largest area percentage while positive reversal trends occupied in sav gra cro ub and cnv in mf and dbf the dominant type was positive reversal while positive reversal and negative reversal occupied almost the same area percentage in dnf 3 1 3 the dominant climatic factors of the secular trends the relative contributions of the underlying climatic factors controlling the secular trends in ecosystem wue were calculated for each pixel by taking an empirical approach based on multiple linear regression we identified the relevant variables over china where the wue secular trends can be explained by the atmospheric co2 concentration 46 followed by temperature 29 solar radiation 14 and precipitation 11 fig 6 temperature dominated areas were mainly located in northeastern china and xinjiang only the north china plain exhibited significant positive relationship with temperature while other areas exhibited neutral or negative relationship fig 7 a precipitation only controlled the southwestern tibeten plateau and most was negatively influenced fig 7 b the amount of solar radiation was found to be the primary positive determining factor in the guangxi and guangdong fig 7 c co2 dependent wue secular trends were found to be widely distributed throughout the north china plain the loess plateau and southwestern china fig 7 d in most of these areas wue secular trends were synergistic with the secular trends of the atmospheric co2 concentration 3 2 relative contribution of secular trends to wue changes 3 2 1 spatial distribution of relative contribution of secular trends fig 8 shows the spatial distribution of the relative contribution rc of the secular trends the areas with a low rc 30 accounted for more than half of china and were concentrated in southwestern china the southeastern coast the changbai mountain and the inner mongolian plateau the areas with a high rc 70 only accounted for a small proportion and were concentrated in the northern china plain and the loess plateau the areas with a moderate rc 30 70 occupied slightly more than a quarter of the study area but their distribution was quite scattered and was largely concentrated in the northeast china plain and southern china in general the area percentage of rc decreased as the value of rc increased from the perspective of the land cover types fig 9 cro had the highest rc followed by cnv gra and ub cro was the only land cover type for which rc 50 of the secular trends was greater than that of the interannual variations iav rc of the secular trends was similar in sav ebf and enf rc was much lower in coniferous forests than in broadleaf forests moreover rc was much smaller in natural vegetation except for gra than in cro ub and cnv 3 2 2 relative contribution of secular trends along topographical gradients we investigated the changes in rc with three topographical factors elevation slope and aspect fig 10 rc exhibited roughly a bimodal pattern with elevation and aspect gradient but a u shaped pattern with the slope in the areas below 800 m rc had a significant negative relationship with elevation decreasing from 70 to 30 at elevations of 800 5000 m rc remained somewhat stable fluctuating between 30 and 40 fig 10 a at elevations above 5000 m rc decreased rapidly to 15 in the scarce high elevation area the relationship between rc and elevation was significantly negative compared to elevation a much more pronounced negative trend was found between rc and slope particularly in the low slope areas fig 10 b in areas with slopes 12 rc rapidly decreased from 50 to 37 nonetheless a slowing or even hiatus of this downward trend occurred in the medium slope areas interestingly rc was higher in the areas with eastern southern and southeastern aspects while it was much lower in the areas with northern western and northwestern aspects fig 10 c 3 2 3 the effects of climatic conditions on the relative contribution of the secular trends to identify the underlying factors controlling the relative contributions of the secular trends we analyzed the partial correlation coefficients between the climatic conditions and rc fig 11 we found that the coefficients between the climatic mean state and rc were generally higher than those between the climatic variability and rc the effects of the amount of solar radiation and the atmospheric co2 concentration on rc were significantly stronger than those of temperature and precipitation however their effects are opposite that is radiation had a positive effect while the atmospheric co2 concentration had a negative effect interestingly rc was positively related with the variability of the atmospheric co2 concentration especially in monotonic increase trends and positive reversal trends whereas it was negatively related with the mean state of the atmospheric co2 concentration the effects of climate changes on monotonic decrease trends and negative reversal trends were visibly weaker than those on monotonic increase trends and positive reversal trends 3 3 relative contributions of different trend types monotonic increase trends had the highest average rc followed by positive reversal trends monotonic decrease trends and negative reversal trends fig 12 in particular monotonic increase trends had very high rc values in the north china plain and the southern part of northeast china plain as well as a slightly lower rc values in southern china fig 13 a moreover according to the frequency distribution rc values of monotonic increase trends were predominantly medium values most of the areas that exhibited monotonic decrease and negative reversal had extremely low rc values fig 13 b and fig 13 c for instance the inner mongolia grasslands exhibited negative reversal trends with rc 30 the same pattern was observed in the northern part of the greater khingan range and taiwan island positive reversal trends with high rc values occurred in the loess plateau and the mountains in xinjiang fig 13 d it should be noted that although a large proportion of the southwestern china exhibited positive reversal trends their rc values were rather low the percentage of the areas that exhibited positive reversal trends with relatively high rc values was much greater the proportion that exhibited negative reversal trends with high rc values it can be seen from fig 14 that monotonic increase trends always made higher relative contribution than positive reversal trends or negative reversal trends in all of the land cover types the relative contributions of monotonic increase trends in cro and cnv was all 50 and those in gra and ub were quite close to 50 thus for these four land cover types the contribution of monotonic increase trends to the overall wue changes surpassed the contribution of the iav this indicates stable improvement in those ecosystems which was not easily affected by perturbations however the relative contributions of monotonic increase trends in dbf and mf were 30 indicating low ecosystem stability except for the artificial land cover types such as cro ub and cnv the relative contributions of positive reversal trends in the other land cover types were considerably high and was only slightly lower than that of monotonic increase trends indicating that the early degradation had changed to improvement and this improvement may not be modified easily in the future for monotonic decrease trends the relative contributions were always 25 except for cro and ub this suggests that the degradation in cro and ub will be much harder to change the relative contributions of negative reversal trends were always 20 especially in the deciduous forests thus although the trends changed from improvement to degradation the relative contribution was low indicating that the degradation can be easily reversed thus it is highly possible for ecological engineering to improve the ecosystem functioning of the deciduous forests 4 discussion 4 1 nonlinearity of the secular trends in this study we first estimated the multi year average wue at the national scale and in different land cover types we found that there were large differences in wue due to the high spatial heterogeneity wue was unexpectedly high in northeastern china indicating high ecosystem functioning the multi year mean wue of the different land cover types exhibited the following order dbf dnf ebf sav cnv mf cro enf gra this was in good agreement with the results of previous researches based on fluxnet sites tang et al 2015 and process based model simulations liu et al 2015 zhang et al 2014 wue of the forests was greater than those of the croplands and grasslands indicating that higher ecosystem functioning in the forests previous studies based on linear analysis reported that the increasing trends were widely distributed in northeastern and central china while the decreasing trends were scattered in southern china guo et al 2019 sun et al 2021 sun et al 2018 similarly we found that the types of the secular trends were dominated by predominantly monotonic increase trends with a very low area proportion of monotonic decrease trends however using eemd methods we also found that 27 60 of china exhibited positive reversal trends and 21 52 exhibited negative reversal trends this was rarely revealed by the linear analysis in the previous studies by considering the nonlinearity of the secular trends we determined the intrinsic trend shifts of wue providing a deeper understanding of the ecosystem functioning the areas with monotonic increase trends were widely distributed in the north china plain and the northeast china plain we found that monotonic increase trends in northeast china plain were positively affected by temperature fig 7 because it is located in the cold temperate zone fig s3 the northeast china plain is stressed by heat more than water increases in air temperature can not only enhance photosynthesis but also can supply more nutrients for vegetation growth by accelerating the decomposition of soil organic matter lin et al 2020 moreover warming leads to more snowmelt and higher soil temperatures piao et al 2019 therefore warming can to large extent increase vegetation productivity different from the northeast china plain which was mainly caused by temperature monotonic increase trends in the north china plain were controlled by the increase in the atmospheric co2 concentration fig 6 the increase in the atmospheric co2 concentration can not only increase vegetation productivity due to the co2 fertilization effect lin et al 2020 wang et al 2020b but can also reduce et by reducing transpiration via decreasing the stomatal conductance of leaves li et al 2010 shi et al 2013 therefore an increase in the atmospheric co2 concentration will significantly increase wue fig 7 in addition croplands are widely distributed in these areas and thus agronomic activities should be considered the effects of agronomic activities on wue have been reported deng et al 2006 ji et al 2021 jin et al 2018 improved water saving projects including water saving irrigation limited irrigation and dryland cultivation enhance wue through the precise control of the timing and quantity of irrigation water supply deng et al 2006 tian et al 2020 moreover the heavy use of fertilizers and cultivar renewal generally increase the yields of croplands ai et al 2020 lu and tian 2017 zhang et al 2013 therefore climate change and agronomic activities jointly led to the significant improvement of the cropland ecosystems positive reversal trends occupied slightly more areas than negative reversal trends the areas with significant positive reversal trends were mainly located in the loess plateau and southwestern china these areas were both ecologically fragile and economically undeveloped these dual pressures from the ecology and economy led to ecosystem degradation in the early stage since the end of the 20th century a series of government led ecological projects have been launched for example the three north shelter forest program and the grain for green project cao et al 2020 ding et al 2021 jin et al 2018 these ecological projects significantly increased the vegetation coverage and thus have promoted ecosystem functioning and have prevented desertification in the loess plateau cao et al 2020 li et al 2017 and rocky desertification in southwestern china tong et al 2018 respectively furthermore we found that wue in these areas was controlled by the atmospheric co2 concentration fig 7 therefore the ecological projects and the significant positive effects of elevated co2 concentration on photosynthesis increased wue the areas with significant negative reversal trends were sparsely distributed in the tibetan plateau and the inner mongolia plateau in these areas wue was predominantly influenced by temperature and was positively correlated with temperature and precipitation warming and the accompanying drought may reduce stomatal conductance and lead to early water saving survival strategies and lately drop in wue allen et al 2003 de boeck et al 2006 niu et al 2011 moreover although the increased temperature could enhance vegetation productivity lin et al 2020 warming had greater influences on the vegetation transpiration and soil evaporation gaertner et al 2019 huntington 2010 which may have offset its positive effect on the ecosystem productivity more attention should be paid and more actions taken to improve the ecosystem functioning of the areas with negative reversal trends 4 2 stability of the secular trends although the spatial distributions of the types of the secular trends have been determined using different methods in previous studies cao et al 2020 horion et al 2019 huang et al 2015 the relative contributions of secular trends in different trend types to wue changes remain unclear our results suggest that areas that exhibited the monotonic increase trends and positive reversal trends with high rc values were mainly distributed in the north china plain and the loess plateau this suggests that the wue changes were dominated by secular trends rather than the inter annual variations in these areas and the improvement of ecosystem functioning had strong stability the strong stability in the north china plain and the loess plateau implies the current agricultural intensification and the implementation of ambitious ecological projects would benefit future improvement however low rc with the improvement of ecosystem functioning was also found in southwestern china this implies that these areas were experiencing unsustainable improvement with poor ecosystem stability in recent years several studies have also suggested that ecological projects promoted the improvement of the ecosystem functioning cao et al 2020 feng et al 2016 zheng et al 2019 however ecological projects may not promote the stability of the improvement our results suggest that in some areas the expected improvement of ecosystem functioning will not be easy to maintain and may be easily altered by the short term external disturbance in the future afforestation in southwestern china has largely enhanced ecosystem productivity and ecosystem functioning whereas the resultant increase of evapotranspiration et may instead have limiting effects on the future increase of wue this leads to the low stability of the ecosystem improvement challenges still exist in terms of maintaining the current improvement of ecosystem functioning in these areas on the contrary although unacceptable monotonic decrease trends and negative reversal trends accounted for more than a quarter of the study areas their low rc implies that it will be somewhat easy to recover from degradation our results demonstrate that more than half of the study area exhibited a relatively low relative contribution of the secular trends to the overall changes indicating a low stability of the secular trends thus the ecosystem improvement or degradation in some local ecosystems may be limited and overstated this also means that there are risks hidden under the apparent ecosystem improvement or potential opportunities beneath the apparent ecosystem degradation we found that rc had higher sensitivities to climatic mean state rather than variability which is consistent with the results of a previous study on ecosystem resilience stability feng et al 2021 compared with temperature and precipitation the amount of solar radiation and the co2 concentration tend to have stronger effects on the secular trends of the ecosystem functioning however their effects are opposite that is radiation had a positive effect while co2 concentration had a negative effect thus although they both exhibited monotonic increase trends the north china plain and southern china generally had higher rc values than the northeast china plain and the inner mongolia plateau the high co2 concentrations led to reduced stomatal conductance and further led to photo adaptation or photosynthetic down regulation which instead alleviated the stability of ecosystem functioning changes li et al 2020 sanz sáez et al 2010 zhou et al 2013 thus in the context of global warming future higher co2 concentrations may be harmful to the stability of ecosystem functioning improvement and may even cause trend shifts occur topographical factors may affect vegetation growth by controlling the hydrothermal conditions in our study it was found that elevation had a negative effect on rc in the low altitude and high altitude areas in the low altitude areas as the elevation increased the proportion of croplands decreased rapidly the climatic condition varied intensely with increasing elevation which made the ecosystems more sensitive to external disturbances thus rc of the secular trends decreased in the high altitude areas i e the tibetan plateau as the elevation increased the environment became harsher and unsuitable for vegetation growth piao et al 2012 and ecosystems become more sensitive to short term climate change and thus rc of secular trends become much lower as the slope increased rc of the secular trends exhibited a non linear u shaped pattern rc decreased in the low slope areas 20 croplands always have relatively high rc values and were usually distributed in the low slope areas and thus the low slope areas had higher rc values nutrients and water are more difficult to preserve and external disturbances become more frequent as the slope increases thus ecosystems seem to be more fragile and more sensitive to external disturbances as the slope increases it has been reported that the grain for green project was launched in the mid slope and high slope areas aiming to change arable sloped croplands and wasted land into forests and grasslands chen et al 2015 due to the implementation of the grain for green project and the resulting wide distribution of forests and grasslands in the mid slope and high slope areas rc of the secular trends decreased at a lower rate in these areas however the low value of rc indicates that the current improvement in ecosystem functioning by ecological projects is still in low stability and thus the actions should still be paid regarding the aspects high rc of secular trends was high in the areas with east and south aspects while it was low in the areas with west aspect this suggests that ecosystems in the areas with the east and south aspects had strong ecosystem stability while the ecosystems in the areas with the west aspect were vulnerable the effect of the aspect on ecosystem stability seems to be primarily attributable to water availability especially precipitation the east asian monsoon prevails in eastern and southern of china and thus southeast east and south are the main windward directions precipitation is a limiting factor for most biomes and it can have dramatic influences on ecosystems medvigy et al 2010 ritter et al 2020 thus in the areas with the southeast east and south aspects rc was high and the ecosystems were stable due to abundant and steady precipitation due to the leeward nature the areas with the west and northwest aspects did not receive sufficient precipitation which led to vulnerable ecosystems moreover radiation may also play an important role as more sunshine duration in windward slopes thus rc was low in the areas with the west and northwest aspects our results suggest that the improvement of the ecosystem functioning with strong stability occurred in gra cro ub and cnv nevertheless the improvement of the ecosystem functioning had lower stability in the deciduous forests and the mixed forest this implies the satisfactory improvement of ecosystem functioning is still vulnerable and sensitive to external disturbance in the meantime the monotonic decrease and negative reversal trend change type also had low rc indicating the slight difficulty of ecosystem improvement for deciduous forests and mixed forests 4 3 uncertainties and limitations gpp accurately has been estimated for many years in previous studies but estimating et accurately is still a challenge li et al 2010 although the et datasets have been validated using plot scale eddy covariance observations and basin wide water balance values there are still some uncertainties due to the small amount of field data for validation gentine et al 2019 we suggest that more fieldwork is needed to reduce uncertainties in the future climate change and human activities both contribute to wue changes although four climatic variables were used in this study human activities such as irrigation are hard to quantify due to the lack of long term datasets in this study we focused primarily on the effects of climate changes and thus the results on human activities was not presented however we suggest that when long term datasets on human activities become more available future studies should pay attention to the anthropic impacts on wue changes fluctuations in ecosystem functioning depend on environmental variations we explored the climatic controls at the annual scale yet debate remains at the seasonal scale the effects of environmental factors on ecosystems in spring may differ from the effects in summer and the effects in autumn future studies can focus on the seasonal partitioning and the seasonal compensation of water and temperature controls on wue although this study has a few inevitable limitations we nevertheless believe that our results can help future researchers to better understand the nonlinearity of the secular trends in wue and the stability of the secular trends these results also can provide scientific support for the ecological protection projects 5 conclusions in this study we explored the secular trends of the ecosystem functioning and evaluated their stability using the relative contribution of secular trends to the overall changes across china during 1982 2015 we found that although 46 2 of the study area experienced monotonic increase trends almost half 49 12 of the study area experienced trend shifts analyses based on variation partitioning revealed that the atmospheric co2 concentration and temperature were the two main factors that affected the wue evolutions although ecosystem improvement widely occurred the high relative contribution of these wue increasing or positive reversal trends mainly occurred in north china plain and loess plateau ecosystem stability in artificial vegetation was generally higher than that in natural vegetation except in grassland topographical factors affected ecosystem stability the relative contribution of secular trends presented a bimodal pattern along elevation and aspect gradient but a u shaped pattern against the slope the climatic mean state had larger effects on ecosystem stability than climatic variability our study highlighted the importance of the nonlinear trends of ecosystem wue and the ecosystem stability which was neglected in former studies the findings of this study are expected to deepen our understanding of current wue evolutions and help predict future wue changes credit authorship contribution statement fusheng jiao data curation conceptualization supervision methodology writing original draft writing review editing xiaojuan xu visualization methodology writing review editing mingyang zhang funding acquisition supervision writing original draft haibo gong visualization conceptualization methodology huiyu liu methodology supervision writing original draft writing review editing funding acquisition kelin wang supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the national natural science foundation of china no 41971382 32271663 31870454 u19a2051 the key research and development program of china 2021yfb3901104 the special fund of the chinese central government for basic scientific research operations in the commonweal research institute gyzx210405 the priority academic program development of jiangsu higher education institutions 164320h116 and jiangsu education department no 1812000024462 we thank letpub for its linguistic assistance during the preparation of this manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128690 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2649,statistical downscaling of daily precipitation is important for assessing the impact of climate change however some issues e g the underestimation of extremes limit statistical downscaling methods to project daily precipitation accurately this study develops an integrated hybrid statistical downscaling model combined with bias correction and bayesian model averaging named as sdbc bma for improving the ability of downscaling methods the performance of sdbc is compared with regression based statistical downscaling methods rsdm and hybrid statistical downscaling methods hsdm based on different performance metrics and extreme indices sdbc bma is applied to the amu darya river basin adrb to demonstrate its feasibility and capability where three gcms and two cmip6 scenarios are considered several findings can be summarized 1 compared to rsdm and hsdm sdbc is more robust with a higher computation ability of daily precipitation and a lower mean bias i e 0 10 mm day 2 compared with the simple model average the biases of extreme indices and mean daily precipitation obtained from sdbc bma could decrease by 49 and 88 respectively 3 the mean precipitation values would increase by 56 41 mm year under ssp245 and 126 55 mm year under ssp585 during 2076 2100 higher than mean precipitation in the historical period 1979 2005 4 extreme indices under multi gcms except for consecutive wet days would increase by 20 under ssp245 and 22 under ssp585 implying that extreme events in adrb are more frequent in the 21st century the increasing risk of precipitation extremes would exacerbate threats to agricultural and ecological security keywords amu darya river basin bayesian model averaging climate change cmip6 statistical downscaling data availability data will be made available on request 1 introduction climate change has become one of the major issues to the scientific community and the general public most regions of the world have experienced a significant impact of climate change especially extreme events for example the precipitation extreme in zhengzhou of china 201 9 mm 20 july 2021 caused over 300 people deaths and 18 billion economic damages zhang et al 2022b it is projected that the impact of climate change will become more pronounced in the future eekhout et al 2020 marras et al 2021 nie et al 2021 schoof et al 2019 the intensity and occurrence of extreme events also tend to be increased significantly in the following years around the world byun and hamlet 2018 hertig et al 2019 daily precipitation is one of the most fundamental meteorological and climatic variables which plays a dominant role in the hydrologic cycle some research works have demonstrated that the fluctuations of daily precipitation due to climate change are the primary cause of extreme events duan et al 2021a tabari et al 2021 the projection of daily precipitation is more complex than temperature due to its high spatial and temporal variabilities lim et al 2010 maraun et al 2010 for example schoof and pryor 2001 found that the accuracy of the downscaling models for temperature is superior to that for precipitation it might be partially attributed to the large number of zeros in the time series of daily precipitation furthermore precipitation is recognized as a sensitive indicator to transient local weather and climate change such as a short lived and intense precipitation caused by a violent and transient weather disturbance li et al 2022 it is essential to analyze the change of daily precipitation and extremes to formulate efficient adaptation strategies general circulation models gcms are widely used for reproducing observed precipitation data and projecting precipitation changes but cannot be directly used due to coarse spatial resolution mei et al 2021 therefore statistical downscaling and dynamic downscaling have been developed to transform gcms to local scale hou et al 2019 wang et al 2015 for dynamic downscaling gcms are used as detailed physical processes and boundary conditions to generate regional climate models at local scale iseri et al 2021 dynamic downscaling methods could achieve a higher spatial resolution based on coarse scale gcm outputs as boundary conditions and detailed physical processes statistical downscaling techniques are based on the statistical relationship between large scale atmospheric predictors and local climate predictand dynamic downscaling has shown robust performance but it requires large amount of data and is also computationally intensive which makes it beyond the scope of most studies compared with dynamic downscaling statistical downscaling techniques are more widely adopted due to their less computational effort and simplicity regression based statistical downscaling methods rsdm are the most common methods and has been developed based on machine learning and non machine learning techniques such as artificial neural network ann yang et al 2018a and multiple linear regression mlr schoof and pryor 2001 however rsdm has a tendency to underestimate precipitation amounts and overestimate precipitation occurrences to address such a shortcoming hybrid statistical downscaling methods hsdm were proposed most but not all hsdm are divided into two parts including classification and regression to calculate daily precipitation amount if the day is classified as wet tareghian and rasmussen 2013 for example the statistical downscaling model sdsm is the most common method that combines regression method and a weather generator and it has been widely applied in many studies wilby et al 2002 inspired by sdsm hessami et al 2008 proposed asd automated statistical downscaling developed under the matlab environment except the introduction of ridge regression rr the bias correction method vif variance inflation factor is used to improve the performance of asd chen et al 2010 proposed a new statistical downscaling based on svm including support vector classification svc and support vector regression svr pour et al 2016 combined random forests rf and svm for daily rainfall projection results showed that the hybrid model could downscale rainfall with nash sutcliff efficiency in the range of 0 90 0 93 in the previous studies the performance of different statistical downscaling methods for ordinary precipitation was compared nevertheless few works were reported on assessing the statistical downscaling methods for extreme events of daily precipitation kumar et al 2021 yang et al 2018a moreover since not all small scale variability can be explained by the large scale predictors predictions of precipitation especially for extreme precipitations generally have less variance than the observed local variables maraun 2013 pryor and schoof 2019 therefore statistical downscaling methods need to be improved for their ability of projecting extreme events as one of the most popular bias correction methods cumulative distribution function transform cdft has been proven as an efficient tool for extreme events grouillet et al 2016 yang et al 2019 the classification model of hsdm could also help cdft overcome the disadvantage of overestimating the probability of drizzle days however few studies focused on introducing cdft into statistical downscaling methods since statistical downscaling methods based on different techniques have diverse performance in a certain area it is important to combine the results from different methods zhang et al 2019 zhao et al 2020 liu et al 2021 there are some ensemble methods such as the simple model average sma fan et al 2021 and bayesian model average bma achieng and zhu 2019 bma can combine forecasts from each output model into a complete probability density function pdf in bma the pdf of ensemble mean is the weighted average of individual pdfs the weights are the estimated posterior probabilities of the models and reflect the relative contributions of individual models in the training periods due to the pronounced advantages the studies used bma methods are increasing in climate change evaluation and projections huang 2014 previous studies mainly focused on the application of bma on gcms and few works of employing bma to downscaling methods were reported abul basher et al 2020 yang et al 2012 the amu darya river basin ardb is one of the largest sub basins of the areal sea basin in central asia precipitation has significant effects on agriculture runoff and ecological systems in ardb making it particularly important to understand future precipitation projections and formulate adaptation planning for this region wang et al 2021 unfortunately due to the lack of observed data and coarse resolution of gcms some studies have calibrated precipitation of ardb for future periods simply by directly using gcms su et al 2021 white et al 2014 xu et al 2021 there are few studies to explore statistical downscaling methods for daily precipitation in ardb the objective of this study is to develop such a novel statistical downscaling approach named as sdbc bma and apply it to ardb the main novelty and contribution of this study are i in sdbc multiple statistical downscaling methods i e rf ann rf svr rf sca and rf mlr are introduced into bias correction bc technique to improve the simulation and prediction accuracies of precipitation and extremes ii bayesian model averaging bma technique is incorporated within the sdbc framework to produce more accurate and reliable predictions than simple multi averaging iii sdbc bma is firstly applied to adrb for assessing daily precipitation change in the 21st century under multiple gcms i e access cm2 cesm2 waccm and canesm5 and two cmip6 emission scenarios i e ssp245 and ssp585 and results are expected to support climate change adapting strategies 2 study area and data set the amu darya river basin adrb originates from the southwest of the pamir mountains then passes through hindu kush afghanistan the kara kum and kyzyl kum deserts uzbekistan flows into the aral sea salehie et al 2021 xu et al 2021 the river has a total length of 2550 km with the average flow about 75 billion m3 year the mean precipitation of the river is 464 mm year and most rainfall occurs during winter and spring november to april in summer the average temperature is 35 while in winter it falls to 8 to 20 water derived from adrb has significant impact on economy agricultural irrigation and associated livelihoods of the local population unger shayesteh et al 2013 yang et al 2021a yang et al 2021b over mountainous regions of ardb there are few meteorological stations the spatial and temporal extent of the station data is also a significant limitation of their use khan et al 2020 over the years gridded datasets have been developed and employed for climatic studies in such regions asian precipitation highly resolved observational data integration towards evaluation of water resources aphrodite is proven as a suitable precipitation forcing in such regions wang et al 2016 the threshold is set as 0 1 mm to classify wet or dry days breinl et al 2020 tabari et al 2021 the location of 15 stations and their wetness fractions from 1979 to 2005 are presented in table s1 the choice of reanalysis data is mainly related to the regional conditions there are few studies about the application of reanalysis data over adrb except the national centre for environmental prediction national centre for atmospheric research reanalysis 2 ncep2 liu et al 2021 yang et al 2020 ncep2 data is the most popular reanalysis data and it is available from the national centers for environmental prediction reanalysis dataset https psl noaa gov the future atmospheric variables from three gcms i e access cm2 cesm2 waccm and canesm5 under two scenarios i e ssp245 and ssp585 are obtained from cmip6 https esgf node llnl gov projects cmip6 only the first runs r1i1p1f1 in each model are considered in the study the above three gcms are selected due to the reasons that 1 several studies based on these three gcms were conducted for analyzing the future climate change in adrb results suggested they are reliable guo et al 2021 jiang et al 2020 2 these three gcms are conducted by different model structure and atmospheric circulation experiment i e physical parameters initialization condition and biogeochemical processes which are helpful for providing more reliable ensemble simulation the general information on selected gcms are shown in table s2 the calibration period is 1979 1996 the validation period is 1997 2005 and the projection periods are near term 2021 2050 mid term 2051 2075 and long term 2076 2100 periods of the 21st century the predictor selection is critical for developing statistical downscaling model the suitable predictors should be physically and conceptually sensible with respect to the daily precipitation and readily available from observed data and gcms however there are few studies about the daily precipitation statistical downscaling in adrb given the spatial variation of ardb in climate and topography it is desirable to select as many predictors as possible in this study 17 large scale atmospheric variables are extracted for screening potential variables including air temperature geopotential height relative humidity zonal and meridional wind component at different pressure levels as shown in table s3 partial correlation analysis paca is used to identify the sets of significant predictors for daily precipitation downscaling because paca could remove the effects of other predictors and calculate the strength of correlation between one predictor and daily precipitation the selection of too many variables can increase the computation time lead to over fitting and under fitting in model calibration and validation respectively too few variables may cause the failure of model to adequately explain the relationship between predictors and predictand according to the previous studies fan et al 2021 sachindra et al 2018 yang et al 2018a it was found that 2 7 predictors were adequate in developing a statistical downscaling model in this study the results of partial correlation coefficients and p values at the 95 confidence level suggest that 5 predictors could be used as inputs to avoid multi collinearity during calibration a detailed introduction about paca is provided in the supporting information the selected variables are standardized to reduce the instability of predictors and avoid systematic bias in this study the variables are standardized using the monthly means and standard deviations by subtracting the mean and normalizing by the standard deviations the behavior of extreme events is one of key aspects of climate change study in recent years compared with the mean precipitation the changes in the frequency and intensity of precipitation extremes are likely to have more significant impacts on ecological system economy and agricultural production in this study six extreme indices including two frequency indices i e cdd and cwd and four intensity indices i e rx1day rx5day r95p and r10mm are selected to assess the statistical downscaling methods the extreme indices are listed in table s4 3 methodology as shown in fig 1 sdbc bma is integrated by multi gcms from cmip6 hybrid statistical downscaling models combined with bias correction sdbc and bayesian model averaging bma sdbc consists of three parts 1 a classification model is developed to classify days as dry or wet 2 a regression model is developed to calculate the amount of precipitation in wet day predicted by classification model 3 bias correction model i e cdft is conducted between observed and modelled precipitation three classification models are selected and evaluated to classify wet and dry days including random forest rf multiple linear regression classification mlc and support vector classification svc four regression models are used for calculating precipitation intensity including support vector regression svr artificial neural network ann stepwise cluster analysis sca and multi linear regression mlr a brief synopsis of the above models is provided in the supporting information the cdft method was firstly proposed by michelangeli et al 2009 for wind downscaling and recently applied to precipitation yang et al 2019 compared with unconditional predictands i e wind a threshold of rainy days is used to match the downscaled data to observed data for a wet day frequency the mathematical function is then established to transform the cdf of downscaled data to observed data during the calibration period although it is assumed that the transformation function is stable over the future period the statistical properties of the statistically downscaled data are not stationary yang et al 2018b a detailed introduction about cdft is provided in the supporting information based on the above results bma is conducted for analyzing of possible future changes bma is a standard probabilistic procedure for combining posterior distributions from different statistical downscaling methods and calibrating them by the training data at the same time the output of bma is a probability density function pdf which is a weighted average of pdfs centered on the bias corrected forecasts the bma weights reflect the relationship between the results of statistical downscaling methods and training data the methods perform better the weights are higher the combined forecast pdf of a variable y is najafi and moradkhani 2015 1 p y y t k 1 k p y m k y t p m k y t where p y m k y t is the forecast pdf based on model m k alone estimated from the training data k is the number of models being combined p m k y t is the posterior probability of model m k being correct by the training data this term is computed by the support of bayes theory 2 p m k y t p y t m k p m k l 1 k p y t m l p m l considering the application of bma to bias corrected forecasts from the k models eq 1 can be rewritten as 3 p y f 1 f k y t k 1 k w k p k y f k y t where w k p k y f k y t is the bma weight for model k computed from the training data and reflects the relationship between model k and the training data the weights w k add up to 1 the conditional probabilities p k y f k y t may be interpreted as the conditional pdf of y given f k i e model k has been chosen and training data y t these conditional pdfs are assumed to be normally distributed as 4 y f k y t ñ a k b k y k σ 2 where the coefficients a k and b k are estimated from the bias correction procedures described above this means that the bma predictive distribution becomes a weighted ensemble of normal distributions with equal variances but centers at the bias corrected forecast which can also be obtained from the bma distribution using the conditional expectation of y given the forecasts 5 e y f 1 f k y t k 1 k w k a k b k y k this forecast function would be expected to be more useful than either the simple model averaging or any one member it is an automatic consensus forecast weighted by the recent performance results for the component models mustafa et al 2020 besides the maximum likelihood function is used to estimate the bma weights and variance the likelihood function is used as the probability of the training data for given parameters to be estimated the differential evolution adaptive metropolis dream markov chain monte carlo mcmc algorithm is used to calculate the bma weights and variances vrugt 2016 4 result and discussion 4 1 sdbc development and evaluation the results of selected predictor combinations for daily precipitation at each station are summarized in figure s1 in the selections for the first predictor vwnd and rhum are dominant in the basin whereas uwnd plays an important role in the northeast the selections for the second and third predictors are more complex vwnd is commonly selected shi et al 2021 also found that there is a strong correlation between vwnd and precipitation over central asia except for vwnd uwnd is often selected for the fourth predictor uwnd is the most important variable however for the fifth predictor air is commonly selected there is not a significantly similar combination of predictors for 15 stations the results may be related to the fact that the elevation of the 15 stations between 647 m and 4626 m associated with different mountains plains valleys and plateaus besides the unique geographical location of adrb is affected by several climate modes i e scandinavian and el nino southern oscillation making this region as a climate change hotspot with strong spatial variations in weather patterns de beurs et al 2018 wet spell length distribution and the accuracy of wet dry day classification are the most important frequency criteria for assessing the ability of statistical downscaling methods three classification methods i e rf svc and mlc were selected and computed in comparison with four rsdm i e svr ann sca and mlr the monthly wet spell length distribution of the observed and downscaled data during the validation period were analyzed to determine if there were any statistically significant differences using kolmogorov smirnov ks test and students t test as shown in fig 2 a and b the values mean the proportions of station with p value 5 which suggest that there are no significant differences between the observed and downscaled precipitation wet days distribution the results suggest that the classification methods perform better than rsdm there is no obvious difference between mlc and rf and their performances are better than svc results show the relatively poor performance in aug and sep at 5 significant level which may be mainly attributed to the low wetness fraction in the observed data the accuracy of dry wet day classification is calculated as the percentage of wet dry days corrected for classification summarized in fig 2 c the results imply that the accuracy of classification methods is generally higher than rsdm the total accuracy of the best classification model svc is 23 higher than the worst rsdm ann rf 73 and svc 74 are more accurate for predicting whether rain will occur on a particular day than mlc 69 results show that classification methods are more reliable and capable in reproducing wet and dry days the accuracy of rsdm is low due to the impact of drizzle days and the direct linkage between large scale predictors and precipitation amounts in general classification methods show the better performance than rsdm in wet spell length distribution and wet dry day occurrence accuracy based on the above results rf exhibits best agreement with the observations and is selected in this study after rsdm i e svr ann sca and mlr hsdm i e rf svr rf ann rf sca and rf mlr and sdbc i e rf svr cdft rf ann cdft rf sca cdft and rf mlr cdft are trained using corresponding data the statistical summary r std bias and mean bias of comparisons between downscaled and observed precipitation in validation period is computed as shown in fig 3 the results had passed the significant test the results of r indicate that rsdm has the worst performance compared with hsdm and sdbc the average r values for rsdm hsdm and sdbc are 0 49 0 52 and 0 50 respectively because the large scale predictors could not explain the variance of observed predictands there are some biases of standard deviation std and mean precipitation between observed and downscaled data the std describes the dispersion of the precipitation from the mean the negative biases of std for rsdm and hsdm indicate that the downscaled data is closer to the mean precipitation than observed data the average mean and associated std biases of rsdm hsdm and sdbc are 0 18 0 58 0 18 0 77 and 0 10 0 22 mm day the positive biases of mean precipitation indicate that statistical downscaling methods tend to overestimate the precipitation intensity the results suggest that sdbc is the best model in terms of r std and mean precipitation bias due to the modification of cdft the spatial distributions of correlation coefficients are shown in figure s2 the results indicate that the correlation coefficients in southern region are significantly lower than that in northern region it is mainly attributed to the precipitation intensity the southern region is more arid than the northern region the average annual precipitation of the southern region is 366 mm whereas the northern region is 480 mm the spatial distribution of mean figure s3 and std bias figure s4 are mixed there is not a significantly pronounced regularity it is obvious that the performance of sdbc in mean and std bias is better than rsdm and hsdm for most stations in general sdbc is the closet to the observed precipitation because the classification model and bias correction could effectively improve the ability of statistical downscaling methods among sdbc the rf svr cdft performs the comparatively better in r less bias in std and mean precipitation to quantify the difference between downscaled and observed precipitation wilcoxon rank sum test and levene s test are used for statistical testing of the equality of monthly mean precipitation and standard deviation in each month at the 5 significant level the computed test results are shown in fig 4 the values mean the proportions of stations with p value 5 it is obvious that the performances are sdbc hsdm rsdm for both mean precipitation and standard deviation the standard deviation errors of rsdm are pronounced during summer due to the lack of wet dry day classification after the modification of bias correction i e cdft the values increase significantly models based on machine learning theory i e svr and ann display the highest percentage for mean precipitation and standard deviation respectively the results further show that sdbc is robust in capturing the mean and standard deviation of the daily precipitation it also illustrates the feasibility of bias correction for downscaling models even in dry months may to october two extreme precipitation frequency indices i e cdd and cwd and four extreme intensity indices i e r10mm rx1day rx5day and r95p based on daily precipitation data are applied to assess the ability of downscaling methods in simulating the frequency and intensity of precipitation extreme events for 15 stations the biases between the downscaled and observed extreme indices in the validation period are shown in fig 5 the boxplots show the quartiles and whiskers as for rsdm the results indicate that the skill of reproducing extreme precipitation frequency indices is generally poor and inadequate most of them tend to underestimate and overestimate the value of cdd and cwd respectively sca is the best model among rsdm in reproducing extreme precipitation frequency indices due to the impact of classification model hsdm and sdbc have similar performance and perform better than rsdm for frequency indices however the negative biases for intensity indices indicate most of rsdm and hsdm consistently underestimate the extreme intensity indices it might be attributed to the short validation period that the downscaling methods could not accurately capture some abnormal and extreme values compared with other rsdm and hsdm sca and rf sca is better in terms of median and outlier of extreme intensity indices nevertheless after the modification of bias correction the distribution scope of rx1day rx5day and r95p of sca based model is not improved it might be attributed to that the structure of sca is to form a classification tree through a series of cutting or merging processes based on a given statistical standard the same values produced by sca hinder the transformation of cdft based on cumulative distribution function duan et al 2021b except sca the extreme intensity indices biases of the other rsdm are obviously improved with the modification of bias correction in general sdbc show the relatively lower biases in comparison with rsdm and hsdm to quantify the difference of extreme indices between downscaled and observed precipitation ks test at 5 significant level are used the proportions of passed ks test among 15 stations are calculated as shown in figure s5 it is evident that classification and bias correction model could effectively improve the ability in reproducing extreme indices in general machine learning methods i e ann and svr are not superior compared with non machine learning methods i e mlr and sca in rsdm most of them tend to underestimate the extreme intensity indices however after the modification of bias correction model the machine learning methods based on ann and svr perform better than non machine learning methods in terms of r mean bias std bias and extreme indices compared with rsdm and hsdm sdbc has the highest performances in wet day distribution r std and mean precipitation bias sdbc also improves upon rsdm and hsdm that often underestimate extreme intensity indices 4 2 multi model ensemble evaluation and projection the results of downscaling projection are always susceptible to uncertainties mainly arising from statistical downscaling methods and gcms in order to reduce uncertainty the comparison between bma and sma based on three gcms i e access cm2 cesm2 waccm and canesm5 and four above sdbc is evaluated through comparing the biases and rmse between downscaled data and observations in adrb table 1 provides a summarized result of model skills inter comparison between the bma and sma ensemble it shows that rmse and biases of the bma ensemble for all six extreme indices are lower than the sma ensemble whereas the results of frequency indices i e cdd cwd for bma and sma are similar due to the impact of classification models the results by bma for the extreme intensity indices are more strongly consistent with observation than by sma therefore bma could provide robust reproduction of the observed data the results also illuminate the applicability of bma in both conventional metrics and climate extremes of statistical downscaling in order to assess the projection results the projection of annual precipitation and sen s slope estimators are present in figure s6 the bold values indicate the p value 0 05 the results indicate that the annual precipitation of ssp245 is slightly higher than ssp585 during 2021 2050 however during 2051 2075 and 2076 2100 the annual precipitation of ssp585 is significantly higher than ssp245 compared with the historical period 1979 2005 the annual precipitations are projected to rise by 35 40 and 49 40 mm year for 2040 s 49 41 and 63 48 mm year for 2060 s 56 41 and 126 55 mm year for 2080 s under ssp245 and ssp585 respectively in general the annual precipitation will be increasing in 21st century the monthly mean precipitation changes by calculating the difference between future and baseline data 1979 2005 under ssp245 and ssp585 emission scenarios are presented in fig 6 the precipitation variability for ssp585 is more pronounced than ssp245 the results of two scenarios are similar and exhibit significant seasonality the increased precipitation is mainly concentrated in winter and spring it might be attributed to the esno the water vapor fluxes of winter and spring are mainly generated in indian and north atlantic oceans the precipitation is expected to increase in the range of 0 14 0 86 mm day during feb to may in 2021 2050 in the other months the precipitation tends to slightly decrease or remain almost unchanged in the mid of century 2051 2075 the trend of mean precipitation is similar to that in 2021 2050 however the overall mean precipitation change of ssp585 0 15 mm day is higher than ssp245 0 11 mm day the phenomenon is more pronounced in 2076 2100 the mean precipitation is projected to increase in most months under ssp585 the overall mean precipitation is anticipated to rise by 0 13 and 0 34 mm day during 2076 2100 under ssp245 and ssp585 respectively the difference may be partly due to the larger radiation forcing furthermore considering with the global warming the increase of mean precipitation will not significantly change the water shortage in adrb extreme events have severe direct and indirect impacts on fragile ecological systems in adrb it is essential to assess the changes of extremes in the future to effectively visualize the relative changes of precipitation extreme indices in adrb the results generated by sdbc bma are presented in fig 7 all the extreme indices are likely to significantly change but the rate of change varies with emission scenarios and time although the overall mean precipitation is projected to increase in the future the results of cdd and cwd show opposite sign with difference of 31 and 23 during 2021 2050 respectively the same phenomenon was observed by ali et al 2019 the changes may aggravate the possibility of increase in frequency of drought in the basin whereas the average values of cdd and cwd will reduce for 2051 2075 and 2076 2100 the extreme precipitation intensity indices perform reverse trends they are projected to increase for 2051 2075 and 2076 2100 compared with 2021 2050 the average changes of extreme frequency and intensity indices under ssp585 are lower than ssp245 during 2021 2050 however the average changes under ssp585 will exceed ssp245 during 2076 2100 for example the average changes of r95p under ssp245 and ssp585 during 2021 2050 are 3 7 and 1 5 while the values during 2076 2100 are 7 5 and 10 respectively table 2 shows the trends of six extreme indices based on mann kendall test and sen s slope estimator test in adrb most trends of extreme indices are not significant with p value 5 some increasing trends under ssp585 are significant with p value 5 i e r10mm in 2051 2075 rx1day rx5day r10mm in 2076 2100 r95p in 2051 2075 and 2076 2100 the results indicate that adrb would experience more extreme events by the mid and end of century under ssp585 fig 8 presents the probability density functions pdfs of precipitation extreme indices in historical period observations and simulation by sdbc bma and future period 2021 2100 obviously the pdfs of extreme indices are significantly different between historical and future period with notable shifts in the distribution comparing with the historical period the densities of cdd r95p r10mm rx1day and rx5day are obviously skewed to the right and the density of cwd is skewed to the left in both ssp245 and ssp585 the phenomenon is more obvious under ssp585 in 2076 2100 period for example in the historical period the locations of highest density of rx5day are 39 mm historical observation and 55 mm historical simulation in the future period their values would be 59 mm under ssp245 and 65 mm under ssp585 the locations of highest density of r10mm are 4 4 days 7 5 days 9 5 days and 12 3 days under observed data simulated data ssp245 and ssp585 the results suggest that the likelihood of lower cwd and higher cdd r95p r10mm rx1day and rx5day are increased in the future it is in accordance with the findings in fig 7 the occurrence of precipitation extremes would be amplified in adrb in 21st century figs 9 and 10 show the spatial distribution of the future extreme indices changes relative to 1979 2005 in adrb the results indicate cdd r10mm rx1day rx5day and r95p show positive trends for most areas the extremes increase by an average of 20 and 22 for ssp245 and ssp585 the results imply the strong spatial variation in the distribution and magnitude of changes in projected extreme indices for example r10mm shows a more pronounced increase in upstream and downstream of the river during 2076 2100 under ssp585 implying a greater possibility of high frequency precipitation the other four extreme indices are projected to increase significantly in most regions compared with northern region cdd show similar features characterized by a significant increase in the southern region rx5day as an important indicator of flood producing events presents opposite spatial pattern to cdd indicating a higher possibility of heavy rain in the northern basin compared with other indices the variation of r95p is not pronounced the results of cwd show negative trends overall however slight increase can be observed in the temporal evolution during 2021 2100 the intensification of extremes over most parts of adrb might be a reason to the increase of mean precipitation the results further indicate that the climate change may aggravate the possibility of increase in frequency of drought and severe precipitation extremes in the basin in general mean precipitation and extreme indices except for cwd indicate a positive response to increasing radiative forcing implying that they are sensitive to the global warming however water storage would not increase in the 21st century in adrb with the impact of global warming and human activity chen et al 2016 thus the limited availability of atmospheric water in adrb will not promote mean precipitation and extremes the increase of precipitation might be a consequence of atmospheric moisture convergence from remote moisture sources that are transported to adrb through large scale atmospheric flows moisture is mainly transported by midlatitude westerlies as they flow from the north atlantic and arctic oceans the past studies had suggested that enso chen et al 2018 zhao et al 2020 tpdv amv jiang et al 2021 and tp monsoon zhang et al 2022a inevitably affected precipitation and extremes in adrb it is suggested that extreme precipitation changes in adrb are not regulated by local water resources but are rather controlled by large scale atmospheric flows 5 conclusions in this study a novel statistical downscaling method named as sdbc bma has been developed to downscale large scale climatic variables from cmip6 for daily precipitation sdbc bma incorporates multi gcms sdbc and bma within a general framework sdbc is composed by classification models regression models and bias correction model the performance of sdbc has been assessed and compared with hsdm and rsdm at 15 stations in adrb then sdbc bma has been applied to adrb for assessing daily precipitation change in the 21st century where three gcms i e access cm2 cesm2 waccm and canesm5 and two emission scenarios i e ssp245 and ssp585 have been considered several findings can be concluded as follows 1 compared with rsdm classification models could better capture the wet dry day distribution more accurately 2 among all classification models rf has the best performance in the model validation 3 sdbc outperforms hsdm and rsdm in the traditional metrics and six extreme indices and the mean biases of rsdm hsdm and sdbc are 0 18 0 18 and 0 10 mm day the performance of sdbc can be improved due to the modification of classification model and bias correction cdft 4 compared with sma the result biases of sdbc bma decrease by 49 for extremes and 88 for mean daily precipitation 5 the mean precipitation of adrb would increase during 2021 2100 where the annual mean precipitation is anticipated to rise by 56 41 mm under ssp245 and 126 55 mm ssp585 during 2076 2100 6 most extreme indices except for cwd are projected to increase by an average of 20 21 under ssp245 and 22 24 under ssp585 implying that more extreme events would occur in the 21st century in general both daily precipitation and extremes would increase in the future over adrb and additional adaption measures are desired to mitigate climate change related issues credit authorship contribution statement q zhang conceptualization methodology writing original draft writing review editing y p li conceptualization funding acquisition supervision writing review editing g h huang writing review editing supervision h wang methodology writing review editing y f li writing review editing y r liu data curation writing review editing z y shen writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the strategic priority research program of the chinese academy of sciences xda20060302 the national natural science foundation of china 52279003 and 52279002 and the innovative research group of the national natural science foundation of china 52221003 the authors are grateful to the editors and the reviewers for their insightful comments and suggestions appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128730 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2649,statistical downscaling of daily precipitation is important for assessing the impact of climate change however some issues e g the underestimation of extremes limit statistical downscaling methods to project daily precipitation accurately this study develops an integrated hybrid statistical downscaling model combined with bias correction and bayesian model averaging named as sdbc bma for improving the ability of downscaling methods the performance of sdbc is compared with regression based statistical downscaling methods rsdm and hybrid statistical downscaling methods hsdm based on different performance metrics and extreme indices sdbc bma is applied to the amu darya river basin adrb to demonstrate its feasibility and capability where three gcms and two cmip6 scenarios are considered several findings can be summarized 1 compared to rsdm and hsdm sdbc is more robust with a higher computation ability of daily precipitation and a lower mean bias i e 0 10 mm day 2 compared with the simple model average the biases of extreme indices and mean daily precipitation obtained from sdbc bma could decrease by 49 and 88 respectively 3 the mean precipitation values would increase by 56 41 mm year under ssp245 and 126 55 mm year under ssp585 during 2076 2100 higher than mean precipitation in the historical period 1979 2005 4 extreme indices under multi gcms except for consecutive wet days would increase by 20 under ssp245 and 22 under ssp585 implying that extreme events in adrb are more frequent in the 21st century the increasing risk of precipitation extremes would exacerbate threats to agricultural and ecological security keywords amu darya river basin bayesian model averaging climate change cmip6 statistical downscaling data availability data will be made available on request 1 introduction climate change has become one of the major issues to the scientific community and the general public most regions of the world have experienced a significant impact of climate change especially extreme events for example the precipitation extreme in zhengzhou of china 201 9 mm 20 july 2021 caused over 300 people deaths and 18 billion economic damages zhang et al 2022b it is projected that the impact of climate change will become more pronounced in the future eekhout et al 2020 marras et al 2021 nie et al 2021 schoof et al 2019 the intensity and occurrence of extreme events also tend to be increased significantly in the following years around the world byun and hamlet 2018 hertig et al 2019 daily precipitation is one of the most fundamental meteorological and climatic variables which plays a dominant role in the hydrologic cycle some research works have demonstrated that the fluctuations of daily precipitation due to climate change are the primary cause of extreme events duan et al 2021a tabari et al 2021 the projection of daily precipitation is more complex than temperature due to its high spatial and temporal variabilities lim et al 2010 maraun et al 2010 for example schoof and pryor 2001 found that the accuracy of the downscaling models for temperature is superior to that for precipitation it might be partially attributed to the large number of zeros in the time series of daily precipitation furthermore precipitation is recognized as a sensitive indicator to transient local weather and climate change such as a short lived and intense precipitation caused by a violent and transient weather disturbance li et al 2022 it is essential to analyze the change of daily precipitation and extremes to formulate efficient adaptation strategies general circulation models gcms are widely used for reproducing observed precipitation data and projecting precipitation changes but cannot be directly used due to coarse spatial resolution mei et al 2021 therefore statistical downscaling and dynamic downscaling have been developed to transform gcms to local scale hou et al 2019 wang et al 2015 for dynamic downscaling gcms are used as detailed physical processes and boundary conditions to generate regional climate models at local scale iseri et al 2021 dynamic downscaling methods could achieve a higher spatial resolution based on coarse scale gcm outputs as boundary conditions and detailed physical processes statistical downscaling techniques are based on the statistical relationship between large scale atmospheric predictors and local climate predictand dynamic downscaling has shown robust performance but it requires large amount of data and is also computationally intensive which makes it beyond the scope of most studies compared with dynamic downscaling statistical downscaling techniques are more widely adopted due to their less computational effort and simplicity regression based statistical downscaling methods rsdm are the most common methods and has been developed based on machine learning and non machine learning techniques such as artificial neural network ann yang et al 2018a and multiple linear regression mlr schoof and pryor 2001 however rsdm has a tendency to underestimate precipitation amounts and overestimate precipitation occurrences to address such a shortcoming hybrid statistical downscaling methods hsdm were proposed most but not all hsdm are divided into two parts including classification and regression to calculate daily precipitation amount if the day is classified as wet tareghian and rasmussen 2013 for example the statistical downscaling model sdsm is the most common method that combines regression method and a weather generator and it has been widely applied in many studies wilby et al 2002 inspired by sdsm hessami et al 2008 proposed asd automated statistical downscaling developed under the matlab environment except the introduction of ridge regression rr the bias correction method vif variance inflation factor is used to improve the performance of asd chen et al 2010 proposed a new statistical downscaling based on svm including support vector classification svc and support vector regression svr pour et al 2016 combined random forests rf and svm for daily rainfall projection results showed that the hybrid model could downscale rainfall with nash sutcliff efficiency in the range of 0 90 0 93 in the previous studies the performance of different statistical downscaling methods for ordinary precipitation was compared nevertheless few works were reported on assessing the statistical downscaling methods for extreme events of daily precipitation kumar et al 2021 yang et al 2018a moreover since not all small scale variability can be explained by the large scale predictors predictions of precipitation especially for extreme precipitations generally have less variance than the observed local variables maraun 2013 pryor and schoof 2019 therefore statistical downscaling methods need to be improved for their ability of projecting extreme events as one of the most popular bias correction methods cumulative distribution function transform cdft has been proven as an efficient tool for extreme events grouillet et al 2016 yang et al 2019 the classification model of hsdm could also help cdft overcome the disadvantage of overestimating the probability of drizzle days however few studies focused on introducing cdft into statistical downscaling methods since statistical downscaling methods based on different techniques have diverse performance in a certain area it is important to combine the results from different methods zhang et al 2019 zhao et al 2020 liu et al 2021 there are some ensemble methods such as the simple model average sma fan et al 2021 and bayesian model average bma achieng and zhu 2019 bma can combine forecasts from each output model into a complete probability density function pdf in bma the pdf of ensemble mean is the weighted average of individual pdfs the weights are the estimated posterior probabilities of the models and reflect the relative contributions of individual models in the training periods due to the pronounced advantages the studies used bma methods are increasing in climate change evaluation and projections huang 2014 previous studies mainly focused on the application of bma on gcms and few works of employing bma to downscaling methods were reported abul basher et al 2020 yang et al 2012 the amu darya river basin ardb is one of the largest sub basins of the areal sea basin in central asia precipitation has significant effects on agriculture runoff and ecological systems in ardb making it particularly important to understand future precipitation projections and formulate adaptation planning for this region wang et al 2021 unfortunately due to the lack of observed data and coarse resolution of gcms some studies have calibrated precipitation of ardb for future periods simply by directly using gcms su et al 2021 white et al 2014 xu et al 2021 there are few studies to explore statistical downscaling methods for daily precipitation in ardb the objective of this study is to develop such a novel statistical downscaling approach named as sdbc bma and apply it to ardb the main novelty and contribution of this study are i in sdbc multiple statistical downscaling methods i e rf ann rf svr rf sca and rf mlr are introduced into bias correction bc technique to improve the simulation and prediction accuracies of precipitation and extremes ii bayesian model averaging bma technique is incorporated within the sdbc framework to produce more accurate and reliable predictions than simple multi averaging iii sdbc bma is firstly applied to adrb for assessing daily precipitation change in the 21st century under multiple gcms i e access cm2 cesm2 waccm and canesm5 and two cmip6 emission scenarios i e ssp245 and ssp585 and results are expected to support climate change adapting strategies 2 study area and data set the amu darya river basin adrb originates from the southwest of the pamir mountains then passes through hindu kush afghanistan the kara kum and kyzyl kum deserts uzbekistan flows into the aral sea salehie et al 2021 xu et al 2021 the river has a total length of 2550 km with the average flow about 75 billion m3 year the mean precipitation of the river is 464 mm year and most rainfall occurs during winter and spring november to april in summer the average temperature is 35 while in winter it falls to 8 to 20 water derived from adrb has significant impact on economy agricultural irrigation and associated livelihoods of the local population unger shayesteh et al 2013 yang et al 2021a yang et al 2021b over mountainous regions of ardb there are few meteorological stations the spatial and temporal extent of the station data is also a significant limitation of their use khan et al 2020 over the years gridded datasets have been developed and employed for climatic studies in such regions asian precipitation highly resolved observational data integration towards evaluation of water resources aphrodite is proven as a suitable precipitation forcing in such regions wang et al 2016 the threshold is set as 0 1 mm to classify wet or dry days breinl et al 2020 tabari et al 2021 the location of 15 stations and their wetness fractions from 1979 to 2005 are presented in table s1 the choice of reanalysis data is mainly related to the regional conditions there are few studies about the application of reanalysis data over adrb except the national centre for environmental prediction national centre for atmospheric research reanalysis 2 ncep2 liu et al 2021 yang et al 2020 ncep2 data is the most popular reanalysis data and it is available from the national centers for environmental prediction reanalysis dataset https psl noaa gov the future atmospheric variables from three gcms i e access cm2 cesm2 waccm and canesm5 under two scenarios i e ssp245 and ssp585 are obtained from cmip6 https esgf node llnl gov projects cmip6 only the first runs r1i1p1f1 in each model are considered in the study the above three gcms are selected due to the reasons that 1 several studies based on these three gcms were conducted for analyzing the future climate change in adrb results suggested they are reliable guo et al 2021 jiang et al 2020 2 these three gcms are conducted by different model structure and atmospheric circulation experiment i e physical parameters initialization condition and biogeochemical processes which are helpful for providing more reliable ensemble simulation the general information on selected gcms are shown in table s2 the calibration period is 1979 1996 the validation period is 1997 2005 and the projection periods are near term 2021 2050 mid term 2051 2075 and long term 2076 2100 periods of the 21st century the predictor selection is critical for developing statistical downscaling model the suitable predictors should be physically and conceptually sensible with respect to the daily precipitation and readily available from observed data and gcms however there are few studies about the daily precipitation statistical downscaling in adrb given the spatial variation of ardb in climate and topography it is desirable to select as many predictors as possible in this study 17 large scale atmospheric variables are extracted for screening potential variables including air temperature geopotential height relative humidity zonal and meridional wind component at different pressure levels as shown in table s3 partial correlation analysis paca is used to identify the sets of significant predictors for daily precipitation downscaling because paca could remove the effects of other predictors and calculate the strength of correlation between one predictor and daily precipitation the selection of too many variables can increase the computation time lead to over fitting and under fitting in model calibration and validation respectively too few variables may cause the failure of model to adequately explain the relationship between predictors and predictand according to the previous studies fan et al 2021 sachindra et al 2018 yang et al 2018a it was found that 2 7 predictors were adequate in developing a statistical downscaling model in this study the results of partial correlation coefficients and p values at the 95 confidence level suggest that 5 predictors could be used as inputs to avoid multi collinearity during calibration a detailed introduction about paca is provided in the supporting information the selected variables are standardized to reduce the instability of predictors and avoid systematic bias in this study the variables are standardized using the monthly means and standard deviations by subtracting the mean and normalizing by the standard deviations the behavior of extreme events is one of key aspects of climate change study in recent years compared with the mean precipitation the changes in the frequency and intensity of precipitation extremes are likely to have more significant impacts on ecological system economy and agricultural production in this study six extreme indices including two frequency indices i e cdd and cwd and four intensity indices i e rx1day rx5day r95p and r10mm are selected to assess the statistical downscaling methods the extreme indices are listed in table s4 3 methodology as shown in fig 1 sdbc bma is integrated by multi gcms from cmip6 hybrid statistical downscaling models combined with bias correction sdbc and bayesian model averaging bma sdbc consists of three parts 1 a classification model is developed to classify days as dry or wet 2 a regression model is developed to calculate the amount of precipitation in wet day predicted by classification model 3 bias correction model i e cdft is conducted between observed and modelled precipitation three classification models are selected and evaluated to classify wet and dry days including random forest rf multiple linear regression classification mlc and support vector classification svc four regression models are used for calculating precipitation intensity including support vector regression svr artificial neural network ann stepwise cluster analysis sca and multi linear regression mlr a brief synopsis of the above models is provided in the supporting information the cdft method was firstly proposed by michelangeli et al 2009 for wind downscaling and recently applied to precipitation yang et al 2019 compared with unconditional predictands i e wind a threshold of rainy days is used to match the downscaled data to observed data for a wet day frequency the mathematical function is then established to transform the cdf of downscaled data to observed data during the calibration period although it is assumed that the transformation function is stable over the future period the statistical properties of the statistically downscaled data are not stationary yang et al 2018b a detailed introduction about cdft is provided in the supporting information based on the above results bma is conducted for analyzing of possible future changes bma is a standard probabilistic procedure for combining posterior distributions from different statistical downscaling methods and calibrating them by the training data at the same time the output of bma is a probability density function pdf which is a weighted average of pdfs centered on the bias corrected forecasts the bma weights reflect the relationship between the results of statistical downscaling methods and training data the methods perform better the weights are higher the combined forecast pdf of a variable y is najafi and moradkhani 2015 1 p y y t k 1 k p y m k y t p m k y t where p y m k y t is the forecast pdf based on model m k alone estimated from the training data k is the number of models being combined p m k y t is the posterior probability of model m k being correct by the training data this term is computed by the support of bayes theory 2 p m k y t p y t m k p m k l 1 k p y t m l p m l considering the application of bma to bias corrected forecasts from the k models eq 1 can be rewritten as 3 p y f 1 f k y t k 1 k w k p k y f k y t where w k p k y f k y t is the bma weight for model k computed from the training data and reflects the relationship between model k and the training data the weights w k add up to 1 the conditional probabilities p k y f k y t may be interpreted as the conditional pdf of y given f k i e model k has been chosen and training data y t these conditional pdfs are assumed to be normally distributed as 4 y f k y t ñ a k b k y k σ 2 where the coefficients a k and b k are estimated from the bias correction procedures described above this means that the bma predictive distribution becomes a weighted ensemble of normal distributions with equal variances but centers at the bias corrected forecast which can also be obtained from the bma distribution using the conditional expectation of y given the forecasts 5 e y f 1 f k y t k 1 k w k a k b k y k this forecast function would be expected to be more useful than either the simple model averaging or any one member it is an automatic consensus forecast weighted by the recent performance results for the component models mustafa et al 2020 besides the maximum likelihood function is used to estimate the bma weights and variance the likelihood function is used as the probability of the training data for given parameters to be estimated the differential evolution adaptive metropolis dream markov chain monte carlo mcmc algorithm is used to calculate the bma weights and variances vrugt 2016 4 result and discussion 4 1 sdbc development and evaluation the results of selected predictor combinations for daily precipitation at each station are summarized in figure s1 in the selections for the first predictor vwnd and rhum are dominant in the basin whereas uwnd plays an important role in the northeast the selections for the second and third predictors are more complex vwnd is commonly selected shi et al 2021 also found that there is a strong correlation between vwnd and precipitation over central asia except for vwnd uwnd is often selected for the fourth predictor uwnd is the most important variable however for the fifth predictor air is commonly selected there is not a significantly similar combination of predictors for 15 stations the results may be related to the fact that the elevation of the 15 stations between 647 m and 4626 m associated with different mountains plains valleys and plateaus besides the unique geographical location of adrb is affected by several climate modes i e scandinavian and el nino southern oscillation making this region as a climate change hotspot with strong spatial variations in weather patterns de beurs et al 2018 wet spell length distribution and the accuracy of wet dry day classification are the most important frequency criteria for assessing the ability of statistical downscaling methods three classification methods i e rf svc and mlc were selected and computed in comparison with four rsdm i e svr ann sca and mlr the monthly wet spell length distribution of the observed and downscaled data during the validation period were analyzed to determine if there were any statistically significant differences using kolmogorov smirnov ks test and students t test as shown in fig 2 a and b the values mean the proportions of station with p value 5 which suggest that there are no significant differences between the observed and downscaled precipitation wet days distribution the results suggest that the classification methods perform better than rsdm there is no obvious difference between mlc and rf and their performances are better than svc results show the relatively poor performance in aug and sep at 5 significant level which may be mainly attributed to the low wetness fraction in the observed data the accuracy of dry wet day classification is calculated as the percentage of wet dry days corrected for classification summarized in fig 2 c the results imply that the accuracy of classification methods is generally higher than rsdm the total accuracy of the best classification model svc is 23 higher than the worst rsdm ann rf 73 and svc 74 are more accurate for predicting whether rain will occur on a particular day than mlc 69 results show that classification methods are more reliable and capable in reproducing wet and dry days the accuracy of rsdm is low due to the impact of drizzle days and the direct linkage between large scale predictors and precipitation amounts in general classification methods show the better performance than rsdm in wet spell length distribution and wet dry day occurrence accuracy based on the above results rf exhibits best agreement with the observations and is selected in this study after rsdm i e svr ann sca and mlr hsdm i e rf svr rf ann rf sca and rf mlr and sdbc i e rf svr cdft rf ann cdft rf sca cdft and rf mlr cdft are trained using corresponding data the statistical summary r std bias and mean bias of comparisons between downscaled and observed precipitation in validation period is computed as shown in fig 3 the results had passed the significant test the results of r indicate that rsdm has the worst performance compared with hsdm and sdbc the average r values for rsdm hsdm and sdbc are 0 49 0 52 and 0 50 respectively because the large scale predictors could not explain the variance of observed predictands there are some biases of standard deviation std and mean precipitation between observed and downscaled data the std describes the dispersion of the precipitation from the mean the negative biases of std for rsdm and hsdm indicate that the downscaled data is closer to the mean precipitation than observed data the average mean and associated std biases of rsdm hsdm and sdbc are 0 18 0 58 0 18 0 77 and 0 10 0 22 mm day the positive biases of mean precipitation indicate that statistical downscaling methods tend to overestimate the precipitation intensity the results suggest that sdbc is the best model in terms of r std and mean precipitation bias due to the modification of cdft the spatial distributions of correlation coefficients are shown in figure s2 the results indicate that the correlation coefficients in southern region are significantly lower than that in northern region it is mainly attributed to the precipitation intensity the southern region is more arid than the northern region the average annual precipitation of the southern region is 366 mm whereas the northern region is 480 mm the spatial distribution of mean figure s3 and std bias figure s4 are mixed there is not a significantly pronounced regularity it is obvious that the performance of sdbc in mean and std bias is better than rsdm and hsdm for most stations in general sdbc is the closet to the observed precipitation because the classification model and bias correction could effectively improve the ability of statistical downscaling methods among sdbc the rf svr cdft performs the comparatively better in r less bias in std and mean precipitation to quantify the difference between downscaled and observed precipitation wilcoxon rank sum test and levene s test are used for statistical testing of the equality of monthly mean precipitation and standard deviation in each month at the 5 significant level the computed test results are shown in fig 4 the values mean the proportions of stations with p value 5 it is obvious that the performances are sdbc hsdm rsdm for both mean precipitation and standard deviation the standard deviation errors of rsdm are pronounced during summer due to the lack of wet dry day classification after the modification of bias correction i e cdft the values increase significantly models based on machine learning theory i e svr and ann display the highest percentage for mean precipitation and standard deviation respectively the results further show that sdbc is robust in capturing the mean and standard deviation of the daily precipitation it also illustrates the feasibility of bias correction for downscaling models even in dry months may to october two extreme precipitation frequency indices i e cdd and cwd and four extreme intensity indices i e r10mm rx1day rx5day and r95p based on daily precipitation data are applied to assess the ability of downscaling methods in simulating the frequency and intensity of precipitation extreme events for 15 stations the biases between the downscaled and observed extreme indices in the validation period are shown in fig 5 the boxplots show the quartiles and whiskers as for rsdm the results indicate that the skill of reproducing extreme precipitation frequency indices is generally poor and inadequate most of them tend to underestimate and overestimate the value of cdd and cwd respectively sca is the best model among rsdm in reproducing extreme precipitation frequency indices due to the impact of classification model hsdm and sdbc have similar performance and perform better than rsdm for frequency indices however the negative biases for intensity indices indicate most of rsdm and hsdm consistently underestimate the extreme intensity indices it might be attributed to the short validation period that the downscaling methods could not accurately capture some abnormal and extreme values compared with other rsdm and hsdm sca and rf sca is better in terms of median and outlier of extreme intensity indices nevertheless after the modification of bias correction the distribution scope of rx1day rx5day and r95p of sca based model is not improved it might be attributed to that the structure of sca is to form a classification tree through a series of cutting or merging processes based on a given statistical standard the same values produced by sca hinder the transformation of cdft based on cumulative distribution function duan et al 2021b except sca the extreme intensity indices biases of the other rsdm are obviously improved with the modification of bias correction in general sdbc show the relatively lower biases in comparison with rsdm and hsdm to quantify the difference of extreme indices between downscaled and observed precipitation ks test at 5 significant level are used the proportions of passed ks test among 15 stations are calculated as shown in figure s5 it is evident that classification and bias correction model could effectively improve the ability in reproducing extreme indices in general machine learning methods i e ann and svr are not superior compared with non machine learning methods i e mlr and sca in rsdm most of them tend to underestimate the extreme intensity indices however after the modification of bias correction model the machine learning methods based on ann and svr perform better than non machine learning methods in terms of r mean bias std bias and extreme indices compared with rsdm and hsdm sdbc has the highest performances in wet day distribution r std and mean precipitation bias sdbc also improves upon rsdm and hsdm that often underestimate extreme intensity indices 4 2 multi model ensemble evaluation and projection the results of downscaling projection are always susceptible to uncertainties mainly arising from statistical downscaling methods and gcms in order to reduce uncertainty the comparison between bma and sma based on three gcms i e access cm2 cesm2 waccm and canesm5 and four above sdbc is evaluated through comparing the biases and rmse between downscaled data and observations in adrb table 1 provides a summarized result of model skills inter comparison between the bma and sma ensemble it shows that rmse and biases of the bma ensemble for all six extreme indices are lower than the sma ensemble whereas the results of frequency indices i e cdd cwd for bma and sma are similar due to the impact of classification models the results by bma for the extreme intensity indices are more strongly consistent with observation than by sma therefore bma could provide robust reproduction of the observed data the results also illuminate the applicability of bma in both conventional metrics and climate extremes of statistical downscaling in order to assess the projection results the projection of annual precipitation and sen s slope estimators are present in figure s6 the bold values indicate the p value 0 05 the results indicate that the annual precipitation of ssp245 is slightly higher than ssp585 during 2021 2050 however during 2051 2075 and 2076 2100 the annual precipitation of ssp585 is significantly higher than ssp245 compared with the historical period 1979 2005 the annual precipitations are projected to rise by 35 40 and 49 40 mm year for 2040 s 49 41 and 63 48 mm year for 2060 s 56 41 and 126 55 mm year for 2080 s under ssp245 and ssp585 respectively in general the annual precipitation will be increasing in 21st century the monthly mean precipitation changes by calculating the difference between future and baseline data 1979 2005 under ssp245 and ssp585 emission scenarios are presented in fig 6 the precipitation variability for ssp585 is more pronounced than ssp245 the results of two scenarios are similar and exhibit significant seasonality the increased precipitation is mainly concentrated in winter and spring it might be attributed to the esno the water vapor fluxes of winter and spring are mainly generated in indian and north atlantic oceans the precipitation is expected to increase in the range of 0 14 0 86 mm day during feb to may in 2021 2050 in the other months the precipitation tends to slightly decrease or remain almost unchanged in the mid of century 2051 2075 the trend of mean precipitation is similar to that in 2021 2050 however the overall mean precipitation change of ssp585 0 15 mm day is higher than ssp245 0 11 mm day the phenomenon is more pronounced in 2076 2100 the mean precipitation is projected to increase in most months under ssp585 the overall mean precipitation is anticipated to rise by 0 13 and 0 34 mm day during 2076 2100 under ssp245 and ssp585 respectively the difference may be partly due to the larger radiation forcing furthermore considering with the global warming the increase of mean precipitation will not significantly change the water shortage in adrb extreme events have severe direct and indirect impacts on fragile ecological systems in adrb it is essential to assess the changes of extremes in the future to effectively visualize the relative changes of precipitation extreme indices in adrb the results generated by sdbc bma are presented in fig 7 all the extreme indices are likely to significantly change but the rate of change varies with emission scenarios and time although the overall mean precipitation is projected to increase in the future the results of cdd and cwd show opposite sign with difference of 31 and 23 during 2021 2050 respectively the same phenomenon was observed by ali et al 2019 the changes may aggravate the possibility of increase in frequency of drought in the basin whereas the average values of cdd and cwd will reduce for 2051 2075 and 2076 2100 the extreme precipitation intensity indices perform reverse trends they are projected to increase for 2051 2075 and 2076 2100 compared with 2021 2050 the average changes of extreme frequency and intensity indices under ssp585 are lower than ssp245 during 2021 2050 however the average changes under ssp585 will exceed ssp245 during 2076 2100 for example the average changes of r95p under ssp245 and ssp585 during 2021 2050 are 3 7 and 1 5 while the values during 2076 2100 are 7 5 and 10 respectively table 2 shows the trends of six extreme indices based on mann kendall test and sen s slope estimator test in adrb most trends of extreme indices are not significant with p value 5 some increasing trends under ssp585 are significant with p value 5 i e r10mm in 2051 2075 rx1day rx5day r10mm in 2076 2100 r95p in 2051 2075 and 2076 2100 the results indicate that adrb would experience more extreme events by the mid and end of century under ssp585 fig 8 presents the probability density functions pdfs of precipitation extreme indices in historical period observations and simulation by sdbc bma and future period 2021 2100 obviously the pdfs of extreme indices are significantly different between historical and future period with notable shifts in the distribution comparing with the historical period the densities of cdd r95p r10mm rx1day and rx5day are obviously skewed to the right and the density of cwd is skewed to the left in both ssp245 and ssp585 the phenomenon is more obvious under ssp585 in 2076 2100 period for example in the historical period the locations of highest density of rx5day are 39 mm historical observation and 55 mm historical simulation in the future period their values would be 59 mm under ssp245 and 65 mm under ssp585 the locations of highest density of r10mm are 4 4 days 7 5 days 9 5 days and 12 3 days under observed data simulated data ssp245 and ssp585 the results suggest that the likelihood of lower cwd and higher cdd r95p r10mm rx1day and rx5day are increased in the future it is in accordance with the findings in fig 7 the occurrence of precipitation extremes would be amplified in adrb in 21st century figs 9 and 10 show the spatial distribution of the future extreme indices changes relative to 1979 2005 in adrb the results indicate cdd r10mm rx1day rx5day and r95p show positive trends for most areas the extremes increase by an average of 20 and 22 for ssp245 and ssp585 the results imply the strong spatial variation in the distribution and magnitude of changes in projected extreme indices for example r10mm shows a more pronounced increase in upstream and downstream of the river during 2076 2100 under ssp585 implying a greater possibility of high frequency precipitation the other four extreme indices are projected to increase significantly in most regions compared with northern region cdd show similar features characterized by a significant increase in the southern region rx5day as an important indicator of flood producing events presents opposite spatial pattern to cdd indicating a higher possibility of heavy rain in the northern basin compared with other indices the variation of r95p is not pronounced the results of cwd show negative trends overall however slight increase can be observed in the temporal evolution during 2021 2100 the intensification of extremes over most parts of adrb might be a reason to the increase of mean precipitation the results further indicate that the climate change may aggravate the possibility of increase in frequency of drought and severe precipitation extremes in the basin in general mean precipitation and extreme indices except for cwd indicate a positive response to increasing radiative forcing implying that they are sensitive to the global warming however water storage would not increase in the 21st century in adrb with the impact of global warming and human activity chen et al 2016 thus the limited availability of atmospheric water in adrb will not promote mean precipitation and extremes the increase of precipitation might be a consequence of atmospheric moisture convergence from remote moisture sources that are transported to adrb through large scale atmospheric flows moisture is mainly transported by midlatitude westerlies as they flow from the north atlantic and arctic oceans the past studies had suggested that enso chen et al 2018 zhao et al 2020 tpdv amv jiang et al 2021 and tp monsoon zhang et al 2022a inevitably affected precipitation and extremes in adrb it is suggested that extreme precipitation changes in adrb are not regulated by local water resources but are rather controlled by large scale atmospheric flows 5 conclusions in this study a novel statistical downscaling method named as sdbc bma has been developed to downscale large scale climatic variables from cmip6 for daily precipitation sdbc bma incorporates multi gcms sdbc and bma within a general framework sdbc is composed by classification models regression models and bias correction model the performance of sdbc has been assessed and compared with hsdm and rsdm at 15 stations in adrb then sdbc bma has been applied to adrb for assessing daily precipitation change in the 21st century where three gcms i e access cm2 cesm2 waccm and canesm5 and two emission scenarios i e ssp245 and ssp585 have been considered several findings can be concluded as follows 1 compared with rsdm classification models could better capture the wet dry day distribution more accurately 2 among all classification models rf has the best performance in the model validation 3 sdbc outperforms hsdm and rsdm in the traditional metrics and six extreme indices and the mean biases of rsdm hsdm and sdbc are 0 18 0 18 and 0 10 mm day the performance of sdbc can be improved due to the modification of classification model and bias correction cdft 4 compared with sma the result biases of sdbc bma decrease by 49 for extremes and 88 for mean daily precipitation 5 the mean precipitation of adrb would increase during 2021 2100 where the annual mean precipitation is anticipated to rise by 56 41 mm under ssp245 and 126 55 mm ssp585 during 2076 2100 6 most extreme indices except for cwd are projected to increase by an average of 20 21 under ssp245 and 22 24 under ssp585 implying that more extreme events would occur in the 21st century in general both daily precipitation and extremes would increase in the future over adrb and additional adaption measures are desired to mitigate climate change related issues credit authorship contribution statement q zhang conceptualization methodology writing original draft writing review editing y p li conceptualization funding acquisition supervision writing review editing g h huang writing review editing supervision h wang methodology writing review editing y f li writing review editing y r liu data curation writing review editing z y shen writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the strategic priority research program of the chinese academy of sciences xda20060302 the national natural science foundation of china 52279003 and 52279002 and the innovative research group of the national natural science foundation of china 52221003 the authors are grateful to the editors and the reviewers for their insightful comments and suggestions appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128730 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
