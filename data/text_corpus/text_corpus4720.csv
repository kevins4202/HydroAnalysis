index,text
23600,present study examines the interannual changes of significant wave height hs in shelf seas around indian mainland based on the 15 year 1998 2012 wave hindcast data obtained from numerical model validation of the hindcast data with buoy measured data shows that hindcast hs is reasonably in good agreement with the observation pearson correlation coefficient values of 0 92 0 97 annual average hs varied from 0 9 to 1 4 m and the wave heights are higher 20 in western shelf seas compared to eastern shelf seas the analysis reveals seasonal fluctuations of wave climate with a strong influence of asian summer monsoon in the western shelf seas compared to the eastern shelf seas of india maximum hs varied from 3 65 to 7 36 m and these maximum values were during the tropical cyclones during 1998 to 2012 a statistically significant positive trend of 0 8 1 4 cm yr 1 in annual mean hs is observed and the increasing trend is higher 0 7 2 5 cm yr 1 during the asian summer monsoon period june september the average trend of annual mean wind speed is also positive and is higher 1 67 cm s 1 yr 1 for the western shelf seas than that for eastern shelf seas 0 93 cm s 1 yr 1 keywords wind waves wave height inter annual variability wave hindcasts arabian sea bay of bengal north indian ocean 1 introduction the shelf seas of the indian subcontinent are being explored for oil and gas and minerals due to the rising demands of these commodities the wind generated ocean waves play a significant role in all aspects of coastal and offshore activities semedo et al 2013 waves also cause erosion of the coastlines and the coastline erosion is expected to be intensified by future sea level rise hinkel et al 2013 along with potential changes in the wave climate seneviratne et al 2012 hence it is important to know the changes in wave climate in the shelf seas of india generally significant wave height hs is used as an indicator to study the wave climate at a location and the variations in annual mean hs during different years are examined to know the change in wave climate estimating reliable trends in wave height from measured buoy data are limited because they are sparse point measurements and are not available for long period hence the long term and decadal trend of wave climate in the different parts of major oceans are studied based on voluntary observing ships global datasets produced within wave reanalysis satellite altimetry or wave model hindcasts carter and draper 1988 allan and komar 2000 gulev and grigorieva 2004 young et al 2011 semedo et al 2011 appendini et al 2014 aarnes et al 2015 the waters of the north indian ocean nio are exposed to seasonally reversing winds winds from the southwest sw during the asian summer monsoon period june to september and from the northeast ne during the winter monsoon period october to january these seasonal changes in winds produce similar changes in the surface waves in nio anoop et al 2015 during the annual cycle monsoon is the dominant mode of variability and it covers 92 of the total variability anoop et al 2015 in the nio the swell heights are greater than those associated with the wind waves and remote influences propagating from the subantarctic region of the south indian ocean are observed anoop et al 2015 the indian subcontinent divides the nio into two semi enclosed seas the arabian sea as and the bay of bengal bob as and bob are the only sea areas in the northern hemisphere with larger summer than winter energy levels due to a combination of stronger summer monsoon winds and higher swell influx during southern ocean winter and hence have large seasonality barstow et al 2011 waves in the bob are frequently influenced by tropical cyclones whereas the occurrence of the tropical cyclone in as is less compared to bob and the ratio of their frequencies is around 1 4 dube et al 1985 in the nio the long term trend in wave height is estimated based on european centre for medium range weather forecasts ecmwf global atmospheric re analysis product era i during 1979 2012 shanas and kumar 2014 2015 kumar and anoop 2015 anoop et al 2015 aarnes et al 2015 and satellite altimeter data during 1993 2010 kumar et al 2013a 1992 2012 bhaskaran et al 2014 and 1996 2012 hithin et al 2015 the study using the altimeter data for a period of 18 years in the indian ocean indicates that the variation in hs is almost negligible kumar et al 2013a young et al 2011 based on the 23 year period 1985 2008 satellite altimeter measurements indicate a weak increase 0 0 25 of annual mean value of hs in nio anoop et al 2015 study shows a negative trend of hs in a small area of the central east coast of india whereas the study of young et al 2011 for the same region shows a small positive trend aarnes et al 2015 study shows that during 1979 1991 there is an increase of hs 0 0 5 of annual mean value per year in nio and from 1992 to 2012 there is a decrease 0 0 5 the study in the central as based on satellite altimeter data shows a positive trend of 0 63 cm yr 1 in the annual mean hs hithin et al 2015 the earlier studies young et al 2011 kumar et al 2013a anoop et al 2015 kumar and anoop 2015 aarnes et al 2015 indicated that the long term trend in wave climate is different for different locations in the shelf seas around india wave hindcasts with numerical models have become a common tool to get high resolution spatial and temporal long term wave height for locations devoid of observational data or having short term measurements appendini et al 2014 as in the north indian ocean hence we have examined the trend in the estimated hs based on the numerical model over the period 1998 to 2012 at the same locations covered in the study of kumar and anoop 2015 with the advent of long marine earth observing satellite missions european remote sensing satellites ers 1 and ers 2 starting in 1991 the quality and coverage of marine wind speed and hs observations rose dramatically aarnes et al 2015 since such observations are usually assimilated into global reanalyses the quality of a reanalysis before and after the satellite era are different and hence trends in the analysis will be spuriously affected if we combine data before and after 1992 aarnes et al 2015 hence we have not carried out the trend for the longer period even though for trends on climate the rule of thumb is to use data covering 30 years in order to understand temporal variations in the wind driven hs seasonal cycle we calculate the trend of monthly average hs and trends of wind speed as well the paper is organized as follows section 2 provides information on the locations studied wave model setup and validation by buoy data section 3 presents the analysis on significant wave height trends and variability discussions are given in section 4 and conclusions of the study are given in section 5 2 materials and methods 2 1 study locations nineteen locations considered in the study of kumar and anoop 2015 in the shelf seas of india are selected for the present study considering approximately 1 5 spacing in latitude of the locations since kumar et al 2012 has shown that the average wave characteristics in the same water depth along the eastern as do not vary significantly within 200 km distance the interannual changes in hs at these locations based on the era i data are reported by kumar and anoop 2015 among these 19 locations 11 locations are in the western side and 8 locations are in the eastern side of the indian mainland fig 1 and the locations studied are in water depth more than 55 m kumar and anoop 2015 2 2 model setup the present study is based on the 15 year wave data simulated using the third generation wave model mike21 spectral waves see sørensen et al 2004 for model description for the preparation of the wave atlas of the indian coast sivakholundu et al 2014 the model is based on the wave action balance equation where the wave field is represented by the wave action density spectrum and the numerical integration of wave action balance equation is formulated in cartesian co ordinates komen et al 1994 young 1999 the discretization of the governing equation in geographical and spectral space is performed using cell centered finite volume method in the geographical domain an unstructured mesh technique is used a logarithmic spectral discretization is employed with a minimum frequency of 0 055 hz 25 frequencies and a frequency factor of 1 1 the directional discretization is done for 360 divided in 16 directions the details of the model equations and methods of solution are explained in the mike 21 user manual dhi 2011 the bathymetry of the study area was prepared using the etopo1 earth topography 1 minute amante and eakins 2009 and mike cmap data mike cmap works with data from digital nautical charts using this one can create a bathymetry of the coastal region in the world the model wind forcing is provided by ecmwf operational archive wind data trenberth 1992 at every six hours interval http browse ceda ac uk browse badc ecmwf op these data sets are produced at ecmwf as part of various global analyses and forecasts and archived in mars meteorological archival and retrieval system ecmwf analysed wind from operational archive is available with a variable grid at every six hour interval bilinear interpolation is carried out on the input wind data to match the model time step the time step of the simulation is 300 s the resolution and internal representation of the archive vary according to changes in data assimilation and forecast system in operational use at ecmwf the spatial resolution of the wind data used in the present study is 0 5 from 1 january 1998 to 31 january 2006 0 25 from 1 february 2006 to 30 january 2012 and 0 125 from 1 july 2012 to 31 december 2012 even though the wind data is with non homogenous spatial resolution the product is successfully utilised by ecmwf for various operational activities the wind data set is validated with measured data which exhibits good correlation considering the complex variation in bathymetry and the demand for a high resolution in the model extensive sensitivity analysis is carried out it is observed that inclusion of gulf of thailand and java sea significantly increases complexity and run time of the model without much improvement in the model results in nio hence the model domain is closed in the malacca strait however the red sea and the arabian gulf are included in the domain the selection of southern boundary seems to be very sensitive and is fixed at 40 s longitude varying from 20 to 120 e after extensive analysis the sensitivity analysis is carried out with varying southern boundary from 30 s to 70 s at an increment of 10 i e 30 s 40 s 50 s 60 s and 70 s the simulation is carried out 10 months from january to october 2005 with the same wind input and same model setup except the model domain the model results are validated with two deep sea observations in nio one in the as and another in the bob the model results are statistically compared with measured buoy data using pearson s linear correlation coefficient r root mean square error rmse and mean absolute error mae and the results are presented in table 1 considering the error estimates and the complexity of the southern ocean topography and time constraints the southern boundary of the model setup is fixed at 40 s 1 r i 1 n a i a b i b i 1 n a i a 2 b i b 2 2 mae 1 n i 1 n a i b i 3 rmse 1 n i 1 n a i b i 2 where ai represents the parameter based on the numerical model bi represents the measured parameter n is the number of data points and the overbar represents the mean value 2 3 model validation the measured wave data collected using a moored seatex buoy oceanor norway under the national data buoy programme premkumar et al 2000 at 1 location in the as ad02 15 00 n 69 01 e during february december 2012 and 1 location in the bob ds3 12 15 n 90 75 e during january december 2003 are used for comparison with the hindcast hs value the heave data of the buoy is recorded at 2 hz interval for 17 min duration and from the recorded heave data the wave spectrum is obtained through fast fourier transform and the hs is estimated from the zeroth spectral moment mo as hs 4 m o comparison of hindcast hs data with the measured data in as at location ad02 for the year 2012 indicates that hindcast data is reasonably in good agreement with the observation fig 2 a the scatter plots are reasonably tight with high pearson correlation coefficient r of 0 97 for the hs with bias value of 0 13 m scatter index si is also small 0 16 the monthly mean hindcast hs is within 15 error of the measured value during the year 2012 the difference between the average value of the hs between the measured and hindcast data is 0 13 m whereas the difference between the maximum hs value is high 0 55 m and the difference in the 90 percentile hs is 0 34 m fig 2b shows that the model underestimates the hs data in bob for the year 2003 fig 2b the pearson correlation coefficient for the hs is 0 92 with bias value of 0 3 m and low scatter index si values 0 23 scatter index is calculated by dividing the rmse with mean of the measured value and it gives the expected error for the hs during the year 2003 the difference between the average value of the hs between the measured and hindcast data in bob is 0 36 m whereas the difference between the maximum hs value is 0 29 m and the difference in the 90 percentile hs is 0 4 m in bob the monthly mean hindcast hs in some months deviates more than 15 of the measured value the scatter plot of the hs based on wave hind cast results obtained without closing the southern boundary and that obtained by closing the southern boundary at 40 s during february december 2012 at a location in the as ad02 15 00 n 69 01 e is presented in fig 3 closing the southern boundary at 40 s results in underestimation of the wave height rmse 0 21 m and is mainly for lower wave heights hs 2 m the influence of swells from the southern ocean on the waves in the north indian ocean can vary with location and season the comparison of the hindcast hs mean wave period and mean wave direction for vijayadurg in the coastal arabian sea and cuddalore in the coastal bay of bengal is carried out with the one year measured waverider buoy data and presented in sivakholundu et al 2014 2 4 trend estimate the trend of hs is estimated based on the slope of the linear best fit curve to the annual mean monthly mean hs for 15 years a positive slope indicates hs increase and negative slope an hs decrease the linear trends in hs for 1998 to 2012 are also calculated for summer monsoon winter monsoon and non monsoon seasons separately the significant trend of the hs is estimated by applying kendall s tau sen test sen 1968 burkey 2006 http www mathworks com matlabcentral fileexchange 3 results 3 1 variations in hs the monthly mean hs indicates that the influence of summer monsoon is high at all locations in the western shelf seas of india and is not significant at locations 12 to 15 in the eastern shelf seas of india fig 4 hence the monthly mean hs is high in july at all locations studied except at locations 12 to 15 table 2 the average hs during july along the western shelf seas of india varied from 2 1 to 2 7 m for locations 12 to 15 in the eastern shelf seas of india the hs is high in december due to ne winds of the winter monsoon the locations 12 to 15 are in the shadow regions of the southern part of indian and the sri lankan land mass and hence relatively low waves are observed at these locations during the summer monsoon anoop et al 2015 have observed that the sri lankan landmass has a major role in the wave climate of a major part of the eastern shelf seas of india the average hs during three different seasons is presented in fig 5 in the non monsoon period the waves in the shelf seas of india become low in height hs range from 0 6 to 1 2 m due to a lower wind speed over the basin anoop et al 2015 fig 5a the average hs in the shelf seas of india during summer monsoon is high hs varies from 0 6 to 2 5 m and the average hs is higher 1 8 2 5 m off the central west coast of india fig 5b during the summer monsoon due to the strong cross equatorial winds of the findlater somali jet findlater 1969 wave heights are high in the central as due to the sheltering effect of the sri lankan land mass the average hs is less than 1 m in the coastal region of the southeast coast of india the wave height in the shelf seas of india during the winter monsoon period is slightly higher than it is in the non monsoon during the winter monsoon period high waves 1 1 2 m are observed off the southern coast of india fig 5c the wave height in the bob is higher 0 8 1 2 m than that in the as 0 6 1 0 m during the winter monsoon period due to the northeast monsoon winds the mean hs during the summer monsoon is highest 2 1 m at locations 2 and 3 and is lowest 1 m at locations 14 and 15 table 3 in the study area during the summer monsoon period maximum hs 7 36 m is observed at location 2 in the western shelf seas of india fig 6 during the winter monsoon and non monsoon period seasonal average hs at all locations is in the range of 0 6 1 2 m the annual mean hs during 1998 2012 in the shelf seas of india varied from 0 9 m to 1 4 m annual mean hs is highest 1 4 m at the southern location 11 since the wave height at this location is relatively high during non monsoon and winter monsoon seasons table 3 in the western shelf seas of india the maximum hs at different locations during the study period varied from 3 65 m at location 11 to 7 36 m at location 2 at most of the locations in the as the maximum hs was in june 2007 due to the super cyclonic storm gonu which was the strongest tc on record in the as at location 5 high hs was also observed in june 2004 due to a deep depression arb02 in the eastern shelf seas of india at the southern locations 12 and 13 the maximum hs were during the tc thane december 2011 during the tc thane maximum hs of 6 m is measured off puducherry eastern shelf seas of india kumar et al 2013b and the maximum hs estimated at location 13 which is in the tc thane track is 5 86 m at this location the model estimated value is close 3 error to the measured value at locations 14 to 16 the maximum hs was during tc laila may 2010 and at the northern locations 17 19 maximum hs was during june 2007 3 2 long term variation in seasonal and annual average hs long period changes in monthly wave activity can be characterized by trends in monthly mean hs trends in monthly hs show intra annual variability but no distinct cycle in western shelf seas of india locations 1 to 7 trends are largest and positive in june to september and negative in may table 4 in the north of eastern shelf seas locations 16 to 19 also the trends are largest and positive during june to september the positive trend is relatively high at all locations in september except at locations 12 to 15 at locations 12 to 16 the positive trend is high in may long term changes in wave activity can be characterized by trends in annual mean the results from the linear trend analysis of annual averages in fig 7 reveal significant linear trends at all locations all locations show positive trend 0 82 1 47 cm yr 1 in annual average hs table 5 the estimated trends at half of the locations are statistically significant with 90 confidence limit except at 3 5 to 11 and 19 at location 1 the increasing trend based on era i data is 0 11 cm yr 1 whereas the trend based on the present data is 1 2 cm yr 1 shanas and kumar 2014 observed a negligible positive trend 0 012 cm yr 1 in annual mean hs based on era i data for shallow water location in western shelf seas the difference in the trend estimated based on the present data and the era i data kumar and anoop 2015 for the same locations is due to the fact that wave data in era i are produced on a grid with resolution of the order of 110 km and this resolution is inadequate to resolve tropical cyclones and hence high waves are underestimated kumar and naseef 2015 fig 8 shows the comparison of hs estimated based on numerical model and the era i for the locations studied at all locations except at 8 to 11 the era i hs are underestimated during the high waves due to the fact that era i underestimates the upper percentiles stopa and cheung 2014 the maximum hs at locations 8 to 11 are less than that at other locations along the western shelf seas of india and hence large underestimation is not observed in the era i hs fig 6 since the satellite data is already assimilated in the era i a separate comparison of hind cast data with satellite data is not attempted trend analysis of seasonally average hs indicates that the monsoon trends are significantly higher than the trends of annual average while non monsoon trends tend to be significantly smaller table 5 the average trend of mean hs during monsoon in western shelf seas is higher 1 8 cm yr 1 than that 1 5 cm yr 1 for the eastern shelf seas of india performing mean and trend analyses for wind speed for the 1998 to 2012 period reveals seasonal cycles with highest wind velocities in summer monsoon and also large trends in wind speed in summer monsoon annual mean wind speed shows the positive trend at some locations negative trend at some other locations the average trend of annual mean wind speed for all the locations in the western shelf seas is 1 67 cm s 1 yr 1 and that for the eastern shelf seas is 0 93 cm s 1 yr 1 table 5 4 discussions the mean hs during summer monsoon at all the locations in the western shelf seas is 2 0 m whereas that for the eastern shelf seas is 1 3 m kumar and anoop 2015 reported maximum hs of 6 3 m at locations 5 and 6 whereas the present study shows a value of 6 2 m similar to the observation of kumar and anoop 2015 the influence of summer monsoon swells at locations 1 to 7 is higher than that at other locations the mean hs at all locations in the eastern shelf seas 0 9 m are slightly higher than that in the western shelf seas 0 8 m during the winter monsoon indicating the predominance of winter monsoon in the eastern shelf seas the annual mean hs for all the locations in the western shelf seas 1 2 m is slightly higher than that for the locations in the eastern shelf seas 1 0 m due to the stronger influence of summer monsoon in the as compared to bob the measured data also indicate that the maximum hs in the eastern shelf seas are less than that in the western shelf seas except during the tropical cyclone tc period kumar et al 2010 during 1998 to 2012 the average trend of annual mean hs for all the locations in the western shelf seas is 1 14 cm yr 1 0 95 of the annual mean per year and that for all the locations in the eastern shelf seas is 1 03 cm yr 1 1 03 of the annual mean per year based on the 23 year period 1985 2008 satellite altimeter measurements young et al 2011 finds a weak increase 0 0 25 of annual mean value of hs in nio trends obtained in the present study are higher than that of young et al 2011 which probably is an effect of the different time periods used and the different data sets the increase in annual mean wind speed 0 32 of the annual mean in western shelf seas and 0 17 for the eastern shelf seas is less than that of annual mean hs the study of semedo et al 2013 shows that wave heights are highest in the indian sector of the southern ocean southwest of australia where the climatological annual mean maxima reach values higher than 5 m the increasing trend in hs in as and bob is due to the increase in hs observed by hemer et al 2010 in the indian ocean sector of southern ocean the earlier studies indicated propagation of swells from the southern ocean to nio glejin et al 2013 nayak et al 2013 since the waves in the nio are mainly the swells arriving from the south indian ocean the increase in hs observed in the present study is largely due to the increase in swells in the south india ocean bhaskaran et al 2014 reported that the maximum hs over the southern ocean increased during 1992 2012 and these swells influenced the waves in certain sectors of the nio the present study shows that the most significant rate of annual mean hs increase 1 4 cm yr 1 occurs at the locations 2 11 and 17 the error in the hs obtained from hindcast data are upto 15 since the trends observed are small 1 of the annual average value slight changes in the annual mean hs value can alter the trends the maximum value of hs measured by buoy bd11 14 2 n 82 93 e during tc thane in december 2011 is 5 21 m whereas that estimated based on the wave model used in the present study is 4 06 m fig 9 even though the correlation coefficient is 0 95 the study shows that the tc generated hs are underestimated 23 by the numerical model since the short term sub synoptic scale wind pulses and fluctuations are not likely to be captured by the numerical model to the accuracy required similarly the mean wave period tm02 is also underestimated by the model fig 9 the mean value of measured tm02 is 5 6 s whereas the model estimated value is 5 0 s amrutha et al 2014 reported maximum hs of 7 3 m in the eastern shelf seas of india off gopalpur during the tc phailin the average hs during february october off gopalpur including the tc phailin is 1 3 m and the average hs excluding the tc is 1 26 m kumar and naseef 2015 during the tc thane maximum hs of 6 m is reported off puducherry eastern shelf seas of india kumar et al 2013b the annual hs off puducherry in 2011 is 0 77 m including the tc thane and is 0 75 m excluding the tc kumar and naseef 2015 even though the change in annual average hs due to tropical cyclone is 2 4 cm it can influence the long term trend in annual average hs since the hs during tc are not estimated accurately even though the wave model hindcasts has comprehensive spatial coverage the model results heavily depend on the winds used for forcing the model a 10 error in the estimate of surface wind speed can lead to a 10 20 error in hs cavaleri 1994 any bias in the wind data used in hindcasting the waves will result wrong estimation of trends of hs for the study area it was observed that there is a bias in the ecmwf wind used to force the wave hindcast model during 1998 the average wind speed measured at buoy location ds01 15 5 n 69 2 e is 5 66 m s 1 whereas the average wind speed obtained from ecmwf for the collocated points is 5 58 m s 1 indicating underestimation in the ecmwf wind speed data by 1 4 during 2011 and 2012 the ecmwf wind data overestimated the buoy measured wind speed at location ad02 close to ds1 by 15 7 and 12 respectively table 6 the comparison shows that the bias in the ecmwf wind speed data is negative 0 08 m s 1 during 1998 and is positive 0 85 0 68 m s 1 in the recent years 2011 2012 the varying bias in the ecmwf wind speed data may be also due to the non homogenous spatial resolution the varying bias in the ecmwf wind speed data used for forcing the wave model also will contribute to the observed positive trend of hs apart from the increase caused due to the swells from the southern ocean the study shows that the hs obtained by selecting the southern boundary at 40 s is slightly less than rmse 0 21 m that obtained without any restriction on the southern boundary one might also miss some major high wave events like kallakkadal events rogue waves if the boundary is at 40 s patra and bhaskaran 2016 moreover the wave periods would be severely underestimated in this case as the model is not taking care of major swell components from southern ocean patra and bhaskaran 2016 the study by kumar and naseef 2015 shows that the annual average hs is not significantly influenced by the occurrence of one large wave events since the influence such events lasts only 3 4 days since the present study deals with only the annual average hs the rogue wave events generated below 40 s will not have significant influence on the study results 5 conclusion in the present study we analyse changes in wave height in the indian shelf seas using the hindcast data from 1998 to 2012 the annual mean hs for all the locations in the western shelf seas 1 2 m is slightly higher than that for the locations in the eastern shelf seas 1 0 m due to the stronger influence of summer monsoon in the as compared to bob from 1998 to 2012 the average trend of hs for all the locations in the western shelf seas is 1 14 cm yr 1 and that for the eastern shelf seas is 1 03 cm yr 1 even though the change in annual average hs due to tropical cyclone is 2 4 cm it will influence the long term trend in annual average hs the varying bias in the wind speed used for forcing the wave hindcast model also will contribute to the trend estimated in summary it is difficult to assess the long term trend in hs using a numerical model in a somewhat complex basin where multiple cyclones are observed annually primarily the bay of bengal the short term sub synoptic scale wind pulses and fluctuations are not likely to be captured by the numerical model to the accuracy required for examining the long term trend the present study is based on model results obtained by closing the southern boundary at 40 s and can have influence on the wave parameters at different locations in the north indian ocean in different seasons and hence the results of this study may be taken into confidence with care acknowledgments wave model data was generated under the research project technical criteria atlas tca sponsored by the ministry of earth sciences moes govt of india director csir national institute of oceanography goa and director national institute of ocean technology chennai provided encouragement to carry out the study we thank all the 3 reviewers for the critical comments which improved the manuscript this work is nio contribution number 6152 
23600,present study examines the interannual changes of significant wave height hs in shelf seas around indian mainland based on the 15 year 1998 2012 wave hindcast data obtained from numerical model validation of the hindcast data with buoy measured data shows that hindcast hs is reasonably in good agreement with the observation pearson correlation coefficient values of 0 92 0 97 annual average hs varied from 0 9 to 1 4 m and the wave heights are higher 20 in western shelf seas compared to eastern shelf seas the analysis reveals seasonal fluctuations of wave climate with a strong influence of asian summer monsoon in the western shelf seas compared to the eastern shelf seas of india maximum hs varied from 3 65 to 7 36 m and these maximum values were during the tropical cyclones during 1998 to 2012 a statistically significant positive trend of 0 8 1 4 cm yr 1 in annual mean hs is observed and the increasing trend is higher 0 7 2 5 cm yr 1 during the asian summer monsoon period june september the average trend of annual mean wind speed is also positive and is higher 1 67 cm s 1 yr 1 for the western shelf seas than that for eastern shelf seas 0 93 cm s 1 yr 1 keywords wind waves wave height inter annual variability wave hindcasts arabian sea bay of bengal north indian ocean 1 introduction the shelf seas of the indian subcontinent are being explored for oil and gas and minerals due to the rising demands of these commodities the wind generated ocean waves play a significant role in all aspects of coastal and offshore activities semedo et al 2013 waves also cause erosion of the coastlines and the coastline erosion is expected to be intensified by future sea level rise hinkel et al 2013 along with potential changes in the wave climate seneviratne et al 2012 hence it is important to know the changes in wave climate in the shelf seas of india generally significant wave height hs is used as an indicator to study the wave climate at a location and the variations in annual mean hs during different years are examined to know the change in wave climate estimating reliable trends in wave height from measured buoy data are limited because they are sparse point measurements and are not available for long period hence the long term and decadal trend of wave climate in the different parts of major oceans are studied based on voluntary observing ships global datasets produced within wave reanalysis satellite altimetry or wave model hindcasts carter and draper 1988 allan and komar 2000 gulev and grigorieva 2004 young et al 2011 semedo et al 2011 appendini et al 2014 aarnes et al 2015 the waters of the north indian ocean nio are exposed to seasonally reversing winds winds from the southwest sw during the asian summer monsoon period june to september and from the northeast ne during the winter monsoon period october to january these seasonal changes in winds produce similar changes in the surface waves in nio anoop et al 2015 during the annual cycle monsoon is the dominant mode of variability and it covers 92 of the total variability anoop et al 2015 in the nio the swell heights are greater than those associated with the wind waves and remote influences propagating from the subantarctic region of the south indian ocean are observed anoop et al 2015 the indian subcontinent divides the nio into two semi enclosed seas the arabian sea as and the bay of bengal bob as and bob are the only sea areas in the northern hemisphere with larger summer than winter energy levels due to a combination of stronger summer monsoon winds and higher swell influx during southern ocean winter and hence have large seasonality barstow et al 2011 waves in the bob are frequently influenced by tropical cyclones whereas the occurrence of the tropical cyclone in as is less compared to bob and the ratio of their frequencies is around 1 4 dube et al 1985 in the nio the long term trend in wave height is estimated based on european centre for medium range weather forecasts ecmwf global atmospheric re analysis product era i during 1979 2012 shanas and kumar 2014 2015 kumar and anoop 2015 anoop et al 2015 aarnes et al 2015 and satellite altimeter data during 1993 2010 kumar et al 2013a 1992 2012 bhaskaran et al 2014 and 1996 2012 hithin et al 2015 the study using the altimeter data for a period of 18 years in the indian ocean indicates that the variation in hs is almost negligible kumar et al 2013a young et al 2011 based on the 23 year period 1985 2008 satellite altimeter measurements indicate a weak increase 0 0 25 of annual mean value of hs in nio anoop et al 2015 study shows a negative trend of hs in a small area of the central east coast of india whereas the study of young et al 2011 for the same region shows a small positive trend aarnes et al 2015 study shows that during 1979 1991 there is an increase of hs 0 0 5 of annual mean value per year in nio and from 1992 to 2012 there is a decrease 0 0 5 the study in the central as based on satellite altimeter data shows a positive trend of 0 63 cm yr 1 in the annual mean hs hithin et al 2015 the earlier studies young et al 2011 kumar et al 2013a anoop et al 2015 kumar and anoop 2015 aarnes et al 2015 indicated that the long term trend in wave climate is different for different locations in the shelf seas around india wave hindcasts with numerical models have become a common tool to get high resolution spatial and temporal long term wave height for locations devoid of observational data or having short term measurements appendini et al 2014 as in the north indian ocean hence we have examined the trend in the estimated hs based on the numerical model over the period 1998 to 2012 at the same locations covered in the study of kumar and anoop 2015 with the advent of long marine earth observing satellite missions european remote sensing satellites ers 1 and ers 2 starting in 1991 the quality and coverage of marine wind speed and hs observations rose dramatically aarnes et al 2015 since such observations are usually assimilated into global reanalyses the quality of a reanalysis before and after the satellite era are different and hence trends in the analysis will be spuriously affected if we combine data before and after 1992 aarnes et al 2015 hence we have not carried out the trend for the longer period even though for trends on climate the rule of thumb is to use data covering 30 years in order to understand temporal variations in the wind driven hs seasonal cycle we calculate the trend of monthly average hs and trends of wind speed as well the paper is organized as follows section 2 provides information on the locations studied wave model setup and validation by buoy data section 3 presents the analysis on significant wave height trends and variability discussions are given in section 4 and conclusions of the study are given in section 5 2 materials and methods 2 1 study locations nineteen locations considered in the study of kumar and anoop 2015 in the shelf seas of india are selected for the present study considering approximately 1 5 spacing in latitude of the locations since kumar et al 2012 has shown that the average wave characteristics in the same water depth along the eastern as do not vary significantly within 200 km distance the interannual changes in hs at these locations based on the era i data are reported by kumar and anoop 2015 among these 19 locations 11 locations are in the western side and 8 locations are in the eastern side of the indian mainland fig 1 and the locations studied are in water depth more than 55 m kumar and anoop 2015 2 2 model setup the present study is based on the 15 year wave data simulated using the third generation wave model mike21 spectral waves see sørensen et al 2004 for model description for the preparation of the wave atlas of the indian coast sivakholundu et al 2014 the model is based on the wave action balance equation where the wave field is represented by the wave action density spectrum and the numerical integration of wave action balance equation is formulated in cartesian co ordinates komen et al 1994 young 1999 the discretization of the governing equation in geographical and spectral space is performed using cell centered finite volume method in the geographical domain an unstructured mesh technique is used a logarithmic spectral discretization is employed with a minimum frequency of 0 055 hz 25 frequencies and a frequency factor of 1 1 the directional discretization is done for 360 divided in 16 directions the details of the model equations and methods of solution are explained in the mike 21 user manual dhi 2011 the bathymetry of the study area was prepared using the etopo1 earth topography 1 minute amante and eakins 2009 and mike cmap data mike cmap works with data from digital nautical charts using this one can create a bathymetry of the coastal region in the world the model wind forcing is provided by ecmwf operational archive wind data trenberth 1992 at every six hours interval http browse ceda ac uk browse badc ecmwf op these data sets are produced at ecmwf as part of various global analyses and forecasts and archived in mars meteorological archival and retrieval system ecmwf analysed wind from operational archive is available with a variable grid at every six hour interval bilinear interpolation is carried out on the input wind data to match the model time step the time step of the simulation is 300 s the resolution and internal representation of the archive vary according to changes in data assimilation and forecast system in operational use at ecmwf the spatial resolution of the wind data used in the present study is 0 5 from 1 january 1998 to 31 january 2006 0 25 from 1 february 2006 to 30 january 2012 and 0 125 from 1 july 2012 to 31 december 2012 even though the wind data is with non homogenous spatial resolution the product is successfully utilised by ecmwf for various operational activities the wind data set is validated with measured data which exhibits good correlation considering the complex variation in bathymetry and the demand for a high resolution in the model extensive sensitivity analysis is carried out it is observed that inclusion of gulf of thailand and java sea significantly increases complexity and run time of the model without much improvement in the model results in nio hence the model domain is closed in the malacca strait however the red sea and the arabian gulf are included in the domain the selection of southern boundary seems to be very sensitive and is fixed at 40 s longitude varying from 20 to 120 e after extensive analysis the sensitivity analysis is carried out with varying southern boundary from 30 s to 70 s at an increment of 10 i e 30 s 40 s 50 s 60 s and 70 s the simulation is carried out 10 months from january to october 2005 with the same wind input and same model setup except the model domain the model results are validated with two deep sea observations in nio one in the as and another in the bob the model results are statistically compared with measured buoy data using pearson s linear correlation coefficient r root mean square error rmse and mean absolute error mae and the results are presented in table 1 considering the error estimates and the complexity of the southern ocean topography and time constraints the southern boundary of the model setup is fixed at 40 s 1 r i 1 n a i a b i b i 1 n a i a 2 b i b 2 2 mae 1 n i 1 n a i b i 3 rmse 1 n i 1 n a i b i 2 where ai represents the parameter based on the numerical model bi represents the measured parameter n is the number of data points and the overbar represents the mean value 2 3 model validation the measured wave data collected using a moored seatex buoy oceanor norway under the national data buoy programme premkumar et al 2000 at 1 location in the as ad02 15 00 n 69 01 e during february december 2012 and 1 location in the bob ds3 12 15 n 90 75 e during january december 2003 are used for comparison with the hindcast hs value the heave data of the buoy is recorded at 2 hz interval for 17 min duration and from the recorded heave data the wave spectrum is obtained through fast fourier transform and the hs is estimated from the zeroth spectral moment mo as hs 4 m o comparison of hindcast hs data with the measured data in as at location ad02 for the year 2012 indicates that hindcast data is reasonably in good agreement with the observation fig 2 a the scatter plots are reasonably tight with high pearson correlation coefficient r of 0 97 for the hs with bias value of 0 13 m scatter index si is also small 0 16 the monthly mean hindcast hs is within 15 error of the measured value during the year 2012 the difference between the average value of the hs between the measured and hindcast data is 0 13 m whereas the difference between the maximum hs value is high 0 55 m and the difference in the 90 percentile hs is 0 34 m fig 2b shows that the model underestimates the hs data in bob for the year 2003 fig 2b the pearson correlation coefficient for the hs is 0 92 with bias value of 0 3 m and low scatter index si values 0 23 scatter index is calculated by dividing the rmse with mean of the measured value and it gives the expected error for the hs during the year 2003 the difference between the average value of the hs between the measured and hindcast data in bob is 0 36 m whereas the difference between the maximum hs value is 0 29 m and the difference in the 90 percentile hs is 0 4 m in bob the monthly mean hindcast hs in some months deviates more than 15 of the measured value the scatter plot of the hs based on wave hind cast results obtained without closing the southern boundary and that obtained by closing the southern boundary at 40 s during february december 2012 at a location in the as ad02 15 00 n 69 01 e is presented in fig 3 closing the southern boundary at 40 s results in underestimation of the wave height rmse 0 21 m and is mainly for lower wave heights hs 2 m the influence of swells from the southern ocean on the waves in the north indian ocean can vary with location and season the comparison of the hindcast hs mean wave period and mean wave direction for vijayadurg in the coastal arabian sea and cuddalore in the coastal bay of bengal is carried out with the one year measured waverider buoy data and presented in sivakholundu et al 2014 2 4 trend estimate the trend of hs is estimated based on the slope of the linear best fit curve to the annual mean monthly mean hs for 15 years a positive slope indicates hs increase and negative slope an hs decrease the linear trends in hs for 1998 to 2012 are also calculated for summer monsoon winter monsoon and non monsoon seasons separately the significant trend of the hs is estimated by applying kendall s tau sen test sen 1968 burkey 2006 http www mathworks com matlabcentral fileexchange 3 results 3 1 variations in hs the monthly mean hs indicates that the influence of summer monsoon is high at all locations in the western shelf seas of india and is not significant at locations 12 to 15 in the eastern shelf seas of india fig 4 hence the monthly mean hs is high in july at all locations studied except at locations 12 to 15 table 2 the average hs during july along the western shelf seas of india varied from 2 1 to 2 7 m for locations 12 to 15 in the eastern shelf seas of india the hs is high in december due to ne winds of the winter monsoon the locations 12 to 15 are in the shadow regions of the southern part of indian and the sri lankan land mass and hence relatively low waves are observed at these locations during the summer monsoon anoop et al 2015 have observed that the sri lankan landmass has a major role in the wave climate of a major part of the eastern shelf seas of india the average hs during three different seasons is presented in fig 5 in the non monsoon period the waves in the shelf seas of india become low in height hs range from 0 6 to 1 2 m due to a lower wind speed over the basin anoop et al 2015 fig 5a the average hs in the shelf seas of india during summer monsoon is high hs varies from 0 6 to 2 5 m and the average hs is higher 1 8 2 5 m off the central west coast of india fig 5b during the summer monsoon due to the strong cross equatorial winds of the findlater somali jet findlater 1969 wave heights are high in the central as due to the sheltering effect of the sri lankan land mass the average hs is less than 1 m in the coastal region of the southeast coast of india the wave height in the shelf seas of india during the winter monsoon period is slightly higher than it is in the non monsoon during the winter monsoon period high waves 1 1 2 m are observed off the southern coast of india fig 5c the wave height in the bob is higher 0 8 1 2 m than that in the as 0 6 1 0 m during the winter monsoon period due to the northeast monsoon winds the mean hs during the summer monsoon is highest 2 1 m at locations 2 and 3 and is lowest 1 m at locations 14 and 15 table 3 in the study area during the summer monsoon period maximum hs 7 36 m is observed at location 2 in the western shelf seas of india fig 6 during the winter monsoon and non monsoon period seasonal average hs at all locations is in the range of 0 6 1 2 m the annual mean hs during 1998 2012 in the shelf seas of india varied from 0 9 m to 1 4 m annual mean hs is highest 1 4 m at the southern location 11 since the wave height at this location is relatively high during non monsoon and winter monsoon seasons table 3 in the western shelf seas of india the maximum hs at different locations during the study period varied from 3 65 m at location 11 to 7 36 m at location 2 at most of the locations in the as the maximum hs was in june 2007 due to the super cyclonic storm gonu which was the strongest tc on record in the as at location 5 high hs was also observed in june 2004 due to a deep depression arb02 in the eastern shelf seas of india at the southern locations 12 and 13 the maximum hs were during the tc thane december 2011 during the tc thane maximum hs of 6 m is measured off puducherry eastern shelf seas of india kumar et al 2013b and the maximum hs estimated at location 13 which is in the tc thane track is 5 86 m at this location the model estimated value is close 3 error to the measured value at locations 14 to 16 the maximum hs was during tc laila may 2010 and at the northern locations 17 19 maximum hs was during june 2007 3 2 long term variation in seasonal and annual average hs long period changes in monthly wave activity can be characterized by trends in monthly mean hs trends in monthly hs show intra annual variability but no distinct cycle in western shelf seas of india locations 1 to 7 trends are largest and positive in june to september and negative in may table 4 in the north of eastern shelf seas locations 16 to 19 also the trends are largest and positive during june to september the positive trend is relatively high at all locations in september except at locations 12 to 15 at locations 12 to 16 the positive trend is high in may long term changes in wave activity can be characterized by trends in annual mean the results from the linear trend analysis of annual averages in fig 7 reveal significant linear trends at all locations all locations show positive trend 0 82 1 47 cm yr 1 in annual average hs table 5 the estimated trends at half of the locations are statistically significant with 90 confidence limit except at 3 5 to 11 and 19 at location 1 the increasing trend based on era i data is 0 11 cm yr 1 whereas the trend based on the present data is 1 2 cm yr 1 shanas and kumar 2014 observed a negligible positive trend 0 012 cm yr 1 in annual mean hs based on era i data for shallow water location in western shelf seas the difference in the trend estimated based on the present data and the era i data kumar and anoop 2015 for the same locations is due to the fact that wave data in era i are produced on a grid with resolution of the order of 110 km and this resolution is inadequate to resolve tropical cyclones and hence high waves are underestimated kumar and naseef 2015 fig 8 shows the comparison of hs estimated based on numerical model and the era i for the locations studied at all locations except at 8 to 11 the era i hs are underestimated during the high waves due to the fact that era i underestimates the upper percentiles stopa and cheung 2014 the maximum hs at locations 8 to 11 are less than that at other locations along the western shelf seas of india and hence large underestimation is not observed in the era i hs fig 6 since the satellite data is already assimilated in the era i a separate comparison of hind cast data with satellite data is not attempted trend analysis of seasonally average hs indicates that the monsoon trends are significantly higher than the trends of annual average while non monsoon trends tend to be significantly smaller table 5 the average trend of mean hs during monsoon in western shelf seas is higher 1 8 cm yr 1 than that 1 5 cm yr 1 for the eastern shelf seas of india performing mean and trend analyses for wind speed for the 1998 to 2012 period reveals seasonal cycles with highest wind velocities in summer monsoon and also large trends in wind speed in summer monsoon annual mean wind speed shows the positive trend at some locations negative trend at some other locations the average trend of annual mean wind speed for all the locations in the western shelf seas is 1 67 cm s 1 yr 1 and that for the eastern shelf seas is 0 93 cm s 1 yr 1 table 5 4 discussions the mean hs during summer monsoon at all the locations in the western shelf seas is 2 0 m whereas that for the eastern shelf seas is 1 3 m kumar and anoop 2015 reported maximum hs of 6 3 m at locations 5 and 6 whereas the present study shows a value of 6 2 m similar to the observation of kumar and anoop 2015 the influence of summer monsoon swells at locations 1 to 7 is higher than that at other locations the mean hs at all locations in the eastern shelf seas 0 9 m are slightly higher than that in the western shelf seas 0 8 m during the winter monsoon indicating the predominance of winter monsoon in the eastern shelf seas the annual mean hs for all the locations in the western shelf seas 1 2 m is slightly higher than that for the locations in the eastern shelf seas 1 0 m due to the stronger influence of summer monsoon in the as compared to bob the measured data also indicate that the maximum hs in the eastern shelf seas are less than that in the western shelf seas except during the tropical cyclone tc period kumar et al 2010 during 1998 to 2012 the average trend of annual mean hs for all the locations in the western shelf seas is 1 14 cm yr 1 0 95 of the annual mean per year and that for all the locations in the eastern shelf seas is 1 03 cm yr 1 1 03 of the annual mean per year based on the 23 year period 1985 2008 satellite altimeter measurements young et al 2011 finds a weak increase 0 0 25 of annual mean value of hs in nio trends obtained in the present study are higher than that of young et al 2011 which probably is an effect of the different time periods used and the different data sets the increase in annual mean wind speed 0 32 of the annual mean in western shelf seas and 0 17 for the eastern shelf seas is less than that of annual mean hs the study of semedo et al 2013 shows that wave heights are highest in the indian sector of the southern ocean southwest of australia where the climatological annual mean maxima reach values higher than 5 m the increasing trend in hs in as and bob is due to the increase in hs observed by hemer et al 2010 in the indian ocean sector of southern ocean the earlier studies indicated propagation of swells from the southern ocean to nio glejin et al 2013 nayak et al 2013 since the waves in the nio are mainly the swells arriving from the south indian ocean the increase in hs observed in the present study is largely due to the increase in swells in the south india ocean bhaskaran et al 2014 reported that the maximum hs over the southern ocean increased during 1992 2012 and these swells influenced the waves in certain sectors of the nio the present study shows that the most significant rate of annual mean hs increase 1 4 cm yr 1 occurs at the locations 2 11 and 17 the error in the hs obtained from hindcast data are upto 15 since the trends observed are small 1 of the annual average value slight changes in the annual mean hs value can alter the trends the maximum value of hs measured by buoy bd11 14 2 n 82 93 e during tc thane in december 2011 is 5 21 m whereas that estimated based on the wave model used in the present study is 4 06 m fig 9 even though the correlation coefficient is 0 95 the study shows that the tc generated hs are underestimated 23 by the numerical model since the short term sub synoptic scale wind pulses and fluctuations are not likely to be captured by the numerical model to the accuracy required similarly the mean wave period tm02 is also underestimated by the model fig 9 the mean value of measured tm02 is 5 6 s whereas the model estimated value is 5 0 s amrutha et al 2014 reported maximum hs of 7 3 m in the eastern shelf seas of india off gopalpur during the tc phailin the average hs during february october off gopalpur including the tc phailin is 1 3 m and the average hs excluding the tc is 1 26 m kumar and naseef 2015 during the tc thane maximum hs of 6 m is reported off puducherry eastern shelf seas of india kumar et al 2013b the annual hs off puducherry in 2011 is 0 77 m including the tc thane and is 0 75 m excluding the tc kumar and naseef 2015 even though the change in annual average hs due to tropical cyclone is 2 4 cm it can influence the long term trend in annual average hs since the hs during tc are not estimated accurately even though the wave model hindcasts has comprehensive spatial coverage the model results heavily depend on the winds used for forcing the model a 10 error in the estimate of surface wind speed can lead to a 10 20 error in hs cavaleri 1994 any bias in the wind data used in hindcasting the waves will result wrong estimation of trends of hs for the study area it was observed that there is a bias in the ecmwf wind used to force the wave hindcast model during 1998 the average wind speed measured at buoy location ds01 15 5 n 69 2 e is 5 66 m s 1 whereas the average wind speed obtained from ecmwf for the collocated points is 5 58 m s 1 indicating underestimation in the ecmwf wind speed data by 1 4 during 2011 and 2012 the ecmwf wind data overestimated the buoy measured wind speed at location ad02 close to ds1 by 15 7 and 12 respectively table 6 the comparison shows that the bias in the ecmwf wind speed data is negative 0 08 m s 1 during 1998 and is positive 0 85 0 68 m s 1 in the recent years 2011 2012 the varying bias in the ecmwf wind speed data may be also due to the non homogenous spatial resolution the varying bias in the ecmwf wind speed data used for forcing the wave model also will contribute to the observed positive trend of hs apart from the increase caused due to the swells from the southern ocean the study shows that the hs obtained by selecting the southern boundary at 40 s is slightly less than rmse 0 21 m that obtained without any restriction on the southern boundary one might also miss some major high wave events like kallakkadal events rogue waves if the boundary is at 40 s patra and bhaskaran 2016 moreover the wave periods would be severely underestimated in this case as the model is not taking care of major swell components from southern ocean patra and bhaskaran 2016 the study by kumar and naseef 2015 shows that the annual average hs is not significantly influenced by the occurrence of one large wave events since the influence such events lasts only 3 4 days since the present study deals with only the annual average hs the rogue wave events generated below 40 s will not have significant influence on the study results 5 conclusion in the present study we analyse changes in wave height in the indian shelf seas using the hindcast data from 1998 to 2012 the annual mean hs for all the locations in the western shelf seas 1 2 m is slightly higher than that for the locations in the eastern shelf seas 1 0 m due to the stronger influence of summer monsoon in the as compared to bob from 1998 to 2012 the average trend of hs for all the locations in the western shelf seas is 1 14 cm yr 1 and that for the eastern shelf seas is 1 03 cm yr 1 even though the change in annual average hs due to tropical cyclone is 2 4 cm it will influence the long term trend in annual average hs the varying bias in the wind speed used for forcing the wave hindcast model also will contribute to the trend estimated in summary it is difficult to assess the long term trend in hs using a numerical model in a somewhat complex basin where multiple cyclones are observed annually primarily the bay of bengal the short term sub synoptic scale wind pulses and fluctuations are not likely to be captured by the numerical model to the accuracy required for examining the long term trend the present study is based on model results obtained by closing the southern boundary at 40 s and can have influence on the wave parameters at different locations in the north indian ocean in different seasons and hence the results of this study may be taken into confidence with care acknowledgments wave model data was generated under the research project technical criteria atlas tca sponsored by the ministry of earth sciences moes govt of india director csir national institute of oceanography goa and director national institute of ocean technology chennai provided encouragement to carry out the study we thank all the 3 reviewers for the critical comments which improved the manuscript this work is nio contribution number 6152 
23601,the strait of gibraltar sog is one of the principal navigation areas in the world the maritime traffic registered in the area is approximately 110 000 ship movements per year where thirty three per cent of total traffic involves roll on roll off passenger ropax ships which run scheduled voyages between ports in the area there are presently many accidents involving this type of ship being reported although these incidents have serious consequences both based on a financial scale and regarding human safety there is no formal maritime risk analysis study for this area carried out to date the aim of this paper is to present the results of a risk analysis for ropax ships operating in the sog based on accidents statistics covering the period 2000 2011 the work has been performed using the two first steps of the imo formal safety assessment methodology hazard identification and risk analysis to identify the hazards and their associated scenarios and quantify their frequencies and consequences the historical accident data analysis and expert judgement techniques were used a risk matrix has been drawn up to calculate the risk indices of the identified hazards a comparative study of the accident frequencies obtained from similar previous studies is also presented in the paper a high level model risk for collisions was established through the elaboration and quantification of an event tree calculating the individual and social risks the conclusions of this study could serve as recommendations to be used in a subsequent decision making process keywords risk analysis ropax ships maritime safety 1 introduction ships designed to carry passengers and roll on roll off cargo ropax ships are among functional types of ships the flexibility ability to integrate with other transport systems and operating speed has become extremely popular in many routes however as past accident statistics demonstrate there are numerous examples of accidents involving ropax ships consequences of these accidents include large numbers of lost lives serious damage to the environment and economic costs the capsize of the herald of free enterprise in 1987 the fire of onboard the scandinavian star in 1990 and the sinking of the estonia in 1994 are notable cases of well known and investigated maritime casualties unfortunately major maritime disasters involving ropax ships still occurring in the last decade as the sinking of the ferry al salam boccaccio 98 in february 2006 and more recently the sinking of sewol in april 2014 studies relating to shipping risk assessment have received growing interest in the last years risk assessment has been very helpful for the review and development of new rules and regulations in order to reduce accidents and improve maritime safety many methods and applications for maritime transportation risk analysis have been presented in the literature goerlandt and montewka 2015 present a complete review of scientific approaches to risk analysis focusing on applications addressing accidental risk of shipping in a sea area the review covers the period from 1970 to 2014 up to a total of 58 applications a number of risk assessments studies applied to ropax ships can be found in the literature det norske veritas dnv 1996 carried out a study on ropax ships sailing in the north west of europe focusing on the investigation of operational dangers and causes of such vessels and quantify when possible their frequency and consequences through the creation of a risk model based on event trees et van dorp et al 2001 and merrick et al 2003 carried out both risk assessments on ferries at specific geographical areas washington state and san francisco bay respectively otto et al 2002 submitted a risk analysis for these ships to study the damages produced from collision and grounding at the same time the international maritime organization imo adopted in 2002 the formal safety assessment imo 2002 a structured and systematic methodology aimed to increase maritime safety based on risk analysis accordingly we can find studies as the hazard identification related to casualties of ropax vessels antao and soares 2006 which used the fsa methodology another risk analysis study for the world wide ropax fleet konovessis and vassalos 2007 konovessis et al 2008 guarin et al 2009 was carried out as part of the activities of the safedor integrated project safedor 2005 2009 and submitted to imo imo 2008a in addition gemelos and ventikos 2008 present a risk assessment on greek passenger ships in order to estimate the safety level of greek coastal shipping more recently other studies montewka et al 2014 goerland et al 2014 are focusing in the development of a novel framework for estimating the risk and consequences resulting of open sea collisions involving a ropax using tools such as bayesian belief networks bbn to calculate the fatalities risk on this background in this paper we present results of a risk analysis study for ropax ships in a specific area the strait of gibraltar sog there is no earlier study available in the literature to the maritime risk analysis in this specific area with exception of piniella and walliser 2013 which however focused in drawing the taxonomy and distribution of maritime emergencies in the area the strait of gibraltar is one of the main shipping areas in the world it is well known for its high volume of maritime traffic which is dependent on a maritime organization system mainly controlled by a traffic separation scheme tss the aim of this tss is to organize traffic and to avoid the occurrence of maritime accidents however this area is prone to problematic conditions which lead to the sog being a hazardous area for safe maritime navigation the weather is one such troublesome element due to the continuous winds from either the east or west there is also frequent fog in the area in addition there is a limited geographical area intended for navigation aggravated by frequent geopolitical disputes between the different states who lay claim to the waters and disagree over who has control of the total traffic registered in the area approximately 110 000 vessel movements per year 33 involves ropax ships which run regular voyages between the ports in the area in 2012 35 925 movements of ropax were recorded representing an average of 98 daily movements 1 1 traffic data have been provided by spanish maritime safety agency http www salvamentomaritimo es and although these ships are extremely common in this area they are involved in a high number of accidents as a consequence of the hazards associated to this type of transport and the area in which they sail ropax ships sail in the area according to three route types connecting the following ports a route between algeciras port and ceuta port b route between algeciras port and tanger mediterraneo port c route between tarifa port and tanger city port fig 1 shows the ports located in the area as well as the ropax ship routes it is noticed that these routes cross the tts located in the area which means the ropax ships are constantly cut crossing the main sog maritime traffic increasing the risk of collision in addition the modification of the tts in 2007 on the occasion of the opening of the service tanger mediterraneo port in 2009 has contributed on an increase of cross points of conflict in the area particularly in the eastern part ropax fleet compromises of ferries and high speed craft hsc above 1000 gt during the period covered by the present study there were on average 16 ropax ships covering these services one example of a common ropax ship in the area can be seen in fig 2 the aim of this study is to carry out a risk analysis for ropax ships navigating in the sog based on accidents statistics collected for the period 2000 2011 the work has been performed in accordance with the imo fsa guidelines the first objective consists of identifying the hazards and their associated scenarios and quantifying to the extent possible their frequencies and consequences in order to do this the historical accident data analysis and expert judgement techniques were used a risk matrix has been drawn up which is supported by the opinion of experts in order to calculate the rates and levels of risk also a high level model risk for collisions will be established through the elaboration and quantification of an et it is used to determinate the safety level of ropax calculating the individual and social risks a study of the accident frequencies obtained from similar previous studies has also been carried out finally the conclusions of this study could serve as recommendations for a subsequent decision making process 2 methodology 2 1 approach adopted the fsa adopted by the international maritime organization imo in 2002 is a structured and systematic methodology aimed at enhancing maritime safety this includes the protection of life health property and the marine environment by using risk analysis and cost benefit assessment the last update of the fsa guidelines was in 2012 imo 2012 fsa consists of five steps as follow 1 identification of hazards 2 risk analysis 3 risk control options 4 cost benefit assessment and 5 recommendations for decision making this paper is focused in the step1 hazard identification and step 2 risk analysis 2 2 hazard identification the aim of this step is to identify a list of hazards and their associated scenarios prioritized by risk levels to determine these hazards standardized techniques are used these hazard identification tools give answers to questions such as what can go wrong and why the guidelines for carrying out an fsa imo 2012 recommend using a combination of creative and analytical techniques for developing the hazard identification hazid the creative element aims to ensure the process uses foresight rather than just being limited to those hazards that have materialized in the past on the other hand the analytical element guarantees that experience is used for full advantage by using the information available this study has been carried out using historical accident data analysis and expert judgement techniques it follows that the former one is analytical and makes use of the information available on accidents that have happened in the past while the latter one is creative and assures that this research will not be limited to only those hazards which have triggered accidents 2 2 1 historical accident data analysis historical analysis has been carried out using the ihs fairplay sea web database earlier known as lloyds register fairplay from 2000 to 2011 the information registered in this database has been supported by the official reports of the accidents which were produced by the different government organizations 2 2 2 expert judgment methodology expert judgement involves using the information and data given by qualified individuals in order to solve problems or make decisions in a range of areas skjong and wentworth 2001 meyer and booker 1991 an expert is defined as a person with experience in the area of study whose expertise is recognised by their peers in this field in this way expert judgement generates opinions based on the individual training and experience of those who participate the experts give information evidence opinions and assessment of the subject in our study the opinions of the experts have been extracted using the delphi method a delphi study involves choosing a group of experts who are asked their opinion about a particular matter this information is then gathered together so that subsequent questionnaires can be designed with the aim of finally reaching a consensus all the time the complete autonomy of the participants is respected astigarra 2003 it could be considered as a feedback process where a unique consensus is reached through the completion of diverse questionnaires one of the advantages of this method is that it does not require experts to come together in group sessions in this manner any view expressed will not be conditioned upon personal conflicts pressures from within the group the appearance of a leader and so forth thus the process was divided into three stages choosing the experts elicitation and aggregation phases respectively the elicitation phase refers to the process of suggesting opinions or expert judgements through special means this is usually supplied through interviews and or questionnaires the aggregation phase is the converging of the different expert opinions proposed in the study meyer and booker 1991 stage 1 choosing the experts the first step was to choose the experts who would take part in the study in this case the number of participants was 15 where the experts were 7 captains and 5 officers with experience in maritime traffic of the area 2 members of the maritime pilot s association of the bay of algeciras and 1 flag state surveyor stage 2 elicitation once the participants were chosen the process for obtaining the information was outlined the process used was the delphi method through the use of questionnaires in order to draw up the questionnaires earlier study carried out by åland sea fsa nyman et al 2010 was used as a reference based on the process outlined above a first open end questionnaire was designed with the objective of identifying a list of hazards and their associated scenarios which could result in a risk for maritime traffic in the sog the experts had to define those initial events which in their opinion could lead to undesirable events which would have negative consequences for people property and or the environment as this study focuses on risks during navigation all hazards associated with the loading and unloading of people and vehicles port manoeuvres and mooring were excluded from the study stage 3 aggregation with the results from the first questionnaire a list was drawn up which included all the identified hazards in total there were 63 hazards related to maritime traffic this list of hazards was re introduced to the participants as a closed end questionnaire so that they could rank the hazards posing the greatest threat to ropax ships during this aggregation stage the experts had to highlight the ten most significant hazards among the list of 63 hazards contained in the questionnaire 2 2 3 risk indices classification the identified hazards have been classified according to their index of risk the experts gave each hazard a rating depending on the frequency with which it occurs and the severity of the consequences for this purpose this study used the logarithmic frequency scale and logarithmic severity scale recommended by the imo fsa guidelines imo 2012 see tables 1 and 2 the combination of these scales result in a risk index represented in the form a risk matrix 2 3 risk analysis risk analysis should lead to a thorough investigation of the causes and consequences of the associated scenarios to the major hazards identified in the hazid for that probabilistic and deterministic techniques are used which solve issues as how often and what are the effects frequency is the number of occurrences of an undesirable event expressed as events per unit of time in this paper accident frequency was defined as the number of incidents divided by the number of ship years casualty statistics analysis has been carried out on the basis of historical data using the sea web database for the period 2000 2011 the fleet statistics for the same period was obtained by the spanish maritime safety agency also a high level model risk for collisions in ropax vessels under way at the sog was established through the elaboration and quantification of an et as collisions are the most frequent accident involving ropax ships according to the accident statistics analysed for the sog area et is a standard technique recommended by the imo fsa guidelines imo 2012 the assignment of branch probabilities on the event tree was done on the basis of the accident statistics of the present study for the period 2000 2011 earlier research studies dnv 1996 imo 2008a b konovessis et al 2008 and expert judgment 3 hazid results 3 1 historical accident analysis for ropax vessels in the sog according to the sea web database the accidents are classified according to six categories sinking fire or explosion collision contact grounding and damage caused to the hull or machinery accidents involving ropax ships during the period 2000 to 2011 in the sog include the following 7 accidents caused by collision 2 accidents caused by contact 2 cases of damage to the hull or machinery and 1 accident involving fire or explosion this total 12 incidents involving ropax ships given that the majority of the incidents come under the collision category this study will focus on identifying the hazards and causes associated with this type of accident with reference to table 3 it is noted that the collision between al mansour and ciudad de málaga of 18 04 2007 ocurred in the port whilst al mansour was moored hence in this respect 6 out of the 7 collisions of table 4 86 are collisions when the ship was under way and 1 collision 14 when the ship was moored 3 2 analysis of the causes of collisions in order to determine the causes of collisions which occurred in the sog during the period from 2000 to 2011 it was necessary to refer to the official accident reports drawn up by the appropriate authorities cma 2007 dgmm 2000 dgmm 2001 dgmm 2006 for the purpose of analysis of causes we will also use the accident report dgmm 2012 of a collision incident involving the ferry milleniun ii which occurred in january 2012 from the accident reports it is observed that the main causes for collision between ropax ships in the area are due to the following failure to comply with the international regulations for preventing collisions at sea colreg this mainly relates to rule 7 risk of collision and rule 8 action to avoid collision inadequate surveillance on the bridge this deals with both visual and radar surveillance this cause would also be classified as failure to comply with colreg under rule 5 look out poor visibility due to fog evidence suggests that the frequent fogs in the sog can hinder safe navigation in the area the occurrence of collisions under such circumstances would appear to be associated with a failure to comply with colreg rule 19 conduct of vessels in restricted visibility unsafe speed related to a those collisions where ships were navigating in reduced visibility and did not reduce their speed as is required by colreg rule 6 safe speed b as a consequence of the high speeds reached by high speed crafts hscs under pressure to meet schedule times poor or non existent ship to ship and ship to shore communication there are several accidents in which the ships involved have not established communication between themselves before collision this also applies to vts on shore and ships characterized by the poor insufficient or lack of information being transmitted 3 3 hazid using expert judgement with the aim of incorporating the creative analytical element in identifying the hazards which pose a threat to safe traffic of ropax ships in the sog a hazid study has been carried out using expert judgement the hazards identified by the experts are shown in table 5 sorted by number of votes obtained the table only shows those hazards which were voted by at least a third of the experts this provides a total of 13 hazards 3 4 calculation of the rates and indices of risk for the hazid so that the hazards can be classified according to their index of risk the experts gave each hazard a rating depending on the frequency with which it occurs and the severity of the consequences in this way the hazards identified in table 4 were re introduced to the experts so that they could assign a value to each hazard depending on the index of frequency and severity outlined above the results from the sum of each scale are shown in table 5 where the hazards appear in order from the highest to the lowest index of risk it also shows the risk matrix developed with reference to the risk index table 6 the different levels of the matrix represent the index of risk for each hazard both quantitatively and qualitatively from low to high risk therefore green indicates those hazards which have a low risk risk rating 2 5 yellow indicates those hazards which have a moderate risk risk rating 6 7 and red is attributed to those hazards which have a high risk index risk rating 8 9 based on the results it can be concluded that according to expert opinion the failure to report ship movements by the port of gibraltar and fatigue among crew members are the hazards which are identified as being of highest risk index regarding the failure to report ship movements by the port of gibraltar the high index of risk is due to its rate of frequency which scores between frequent and reasonably probable fatigue also borders on reasonably probable in both cases the consequences are considered significant the rest of the hazards shown in table 6 fall under the considerable risk category and ways to reduce them should also be considered in order to show uncertainty of main hazard outcomes in both severe consequences and expected frequency probability tukey box plots are performed mcgill et al 1978 fig 3 shows the distribution of the probability and consequence variables to both events 57 failure to report ship movements by the port of gibraltar and 1 fatigue among crew members in addition according to the proposal of goerlandt and reniers 2016 the qualitative strength of evidence assessment has been performed using a 3 level categorization of each applicable evidence category namely expert judgments and assumptions a traffic light symbolism is applied to rate each evidence category whereas green yellow red means strong medium red the evidence for probability and consequence is for the main risk events are performed in fig 3 for each event each evidence type is assessed using a set of evidential qualities described in table 7 see goerlandt and reniers 2016 for further details 4 risk analysis 4 1 accidents statistics and frequency analysis casualty statistics analysis has been carried out on the basis of historical data for the period 2000 2011 the fleet statistics for the same period was obtained by the spanish maritime safety agency table 8 shows the records of incidents and their frequencies two types of frequency have been calculated for this study a frequency per ship year where the number of the fleet at risk corresponds to the total number of ropax that operated in the area during the specified period b frequency per movement year where the number of the fleet at risk corresponds to the total number of records of ship movements underway in the area during the specified period previous studies for ropax ships dnv 1996 referring to north west european experience for the period 1978 1994 and imo 2008a b referring to world wide fleet for the period 1994 2004 have only calculated the frequencies per ship year however it seems appropriate to perform a calculation of frequencies by ship movements since the risk of an accident will be greater the more the ship moves as the sog is an area controlled by a vts it is possible to get the movement ship records for the established period table 8 shows the frequency of collisions in ropax ships is the highest value compared to other categories of incidents the frequency of collisions is estimated to be 3 65e 02 per ship year if the collision involving the millenium ii of 2012 is introduced the collision frequency becomes 3 84e 02 per ship year table 8 also shows that the frequency of collisions expressed in movement ships year at sog for the period 2000 2011 is estimated to be 1 83e 05 for ropax and 8 89e 06 for rest of the fleet this indicates that the frequency of collisions in ropax is higher in relation to the rest of the fleet 4 2 calculation of consequences 4 2 1 introduction to risk criteria risk acceptance criteria used in this study have been determined from the relevant literature skjong et al 2007 imo 2008a imo 2012 individual risk is the risk of death injury and illness experienced by a single individual crew member or passenger in a given time period who in our case is exposed to hazards relating to ropax operations the individual risk is usually expressed as the frequency of an individual fatality per year imo 2012 proposes a series of criteria for individual risk in shipping operations at the same level as those used by the uk health and safety executive these criteria are shown in table 9 see table 10 social risk is the average risk regarding the number of deaths experienced by a whole group of people crew member or passengers traveling on a ship social risk is usually taken as the risk of death and is typically expressed as potential loss of life pll or fn diagrams imo 2012 in this study we use pll defined as the expected number of deaths for a year 4 2 2 mortal incidents once the risk criteria have been established it is necessary to estimate the individual and social risks derived from this study there is only an accident registered in ropax ship during the period 2000 2011 where there were mortal victims the collision in the ferry ciudad de ceuta with five fatalities table 9 presents the calculation for the potential loss of life during the period 2000 2011 at the sog the pll is estimated to be 2 60e 02 per ship year 4 2 3 risk model in order to calculate a consequences level which is not only based on historical data a risk model to estimate the level of consequences has been developed a high level model risk for collisions in ropax vessels underway at the sog was established through the elaboration and quantification of an et the selection of this initiating event collision is in agreement with the outcome of the hazid work through the consideration which all identified hazards may cause one collision in addition as it can be seen from the frequency analysis this initiating event represents 58 of all incidents involving ropax ships in the sog fig 4 presents the generic collision et based on ropax fleet at the sog during the period 2000 2011 the frequency for collision incidents estimated is 3 65e 02 per ship year data used and assumptions made are the follows level 1 casualty data for ropax at the sog indicate that during the period 2000 2011 of the 7 recorded collision casualties 6 86 are collisions under way and only one collision is striking whilst at berth these percentages are used as the branch probabilities on the event tree level 2 of the total collisions under way 5 83 are recorded as minor incidents and 1 17 represent a serious casualty 2 2 a collision is considered serious if as result of it has been one or multiple fatalities or damage to the vessel that has halted service or total loss those collisions where only ship damages occurred and only reparations were needed are considered minor incidents level 3 the probability the ship being the struck or the striking ship is assumed to be 50 50 konovessis et al 2008 level 4 none of the collisions occurred during the period 2000 2011 in the sog involving ropax ships resulted in flooding or fire onboard the ship due to the limited number of collisions in the area other references based on relevant previous studies imo 2003 imo 2008a vanen and skjong 2004 were taken into account to estimate the branch probability of flooding or fire occurring on the struck ship in this respect it is considered that the probability of flooding into a struck ship after a collision is 50 it is assumed that the probability of fire is zero in the same way it is assumed that the probability of flooding on a ship striking is 5 it is understood that damage to the striking ship is limited to her bow area where the collision bulkhead is hence avoiding water ingress beyond the collision bulkhead level 5 and 6 branch probabilities in these levels are adopted from relevant results from imo 2003 imo 2008a it is noticed that we distinguish two cases for impacts one involving fatalities due the impact and the most common case of impacts not resulting in fatalities this is due to the fact that the only collision case involving fatalities in our study can be classified as an impact involving fatalities this is reflected in the et we estimate the corresponding branch probabilities in the event tree on the following assumption just in one of 5 collisions there have been produced mortal victims that means that 80 have produced no mortal impacts and 20 mortal impacts 4 2 3 1 risk calculations and reference data fort the calculation of individual risk the following is assumed average number of crew on ropax ships operating in the area is 30 average maximum carrying capacity is 780 passengers there are different fluctuations of number of passengers depending to the seasonal period according to this and to maximum capacity of 780 passengers is assumed a 30 of trips carrying full passengers load 780 passengers b 40 of trips carrying half of maximum passengers load 390 passengers c 30 of trips carrying 35 of maximum passenger load 273 passengers these percentages are based on expert opinions captain and officers currently employed by shipping companies average fatality rates used for the different potential scenarios are the same used in imo 2008a b dnv 1996 namely a 12 fatality for flooding incidents leading to slow sinking and 66 for incidents leading to rapid capsize there are not probabilities of fatalities for these cases which the ship had only impact without flood the cruise ships fsa study imo 2008c includes fatality rates for the impact scenario only due to differences between the types of ships of each study and the number of passengers it does not seem appropriate to rely on their results according to our data the following assumptions could be made a on board of ciudad de ceuta ferry only collision with fatalities had 331 people of which 5 died in the collision the averages fatality rate is 0 015 b if the average number of persons on board a ropax ship at half load in the sog is 420 people and there were seven collisions this represents 2940 persons exposed to the collision as for those people only there were 5 deaths the mortality rate will be 0 17 for all ships representing 0 02 of each ship as shown the results are so low 0 015 0 02 we could disregard this risk furthermore the assumptions are based on our registered accidents representing a very limited number of cases table 11 present the risk calculations for the outcomes of collision on the basis of the event tree of fig 4 4 2 3 2 individual risk calculation on basis collision risk model table 12 summarizes the calculations presented the individual risk calculated by the collision risk model is 1 17e 0 4 per year assuming the ship being at sea and a person being onboard for the full year to estimate of the individual risk experienced by crew members and passengers the following considerations can be made for crew members assuming that the ship operates 24 h a day of which half 12 h is in navigation and there is a crew rotating of 50 50 each day the model predicts an overall individual risk for crew of 2 92e 05 per year but usually in these vessels object of this study it is that there are several crews for one ship ensuring periods of rest holiday thus we can further assume a if there is a period of rest generated 1 1 i e one working day corresponds to a day of rest equivalent to 4 crews for one ship the overall individual risk becomes 1 46e 05 per year b if the period of rest is generated 2 1 i e one working day corresponds to a half of rest equivalent to 3 crews for one ship the overall individual risk becomes 1 95e 05 per year for passenger assuming that the average duration of a trip is 90 min so a return trip is 3 h we have a for a passenger traveling once a year round trip the average individual risk is 4 01e 08 per year b for a passenger who usually makes a trip round trip a week for a year the individual risk is 2 08e 06 per year this probability could be applied to truck drivers who regularly use these lines considering the fig 5 it can be concluded that individual risk levels during a collision are within the alarp as low as reasonably practicable region for both passenger and crew members it would be desirable to reduce that risk as long as feasible potential loss of life is 5 85e 02 ship years equivalent to 1 death per year 5 comparison with previous studies a comparison with frequencies calculated in dnv 1996 referring to north west european experience for the period 1978 1994 and imo 2008a b referring to world wide experience for the period 1994 2004 is attempted in this section the following are the points that can be made collision the frequency of collisions world wide during the period 1994 2004 was 1 25e 02 per ship year imo 2008a b from table 8 the frequency of collisions at sog for the period 2000 2011 is estimated to be 3 65e 02 per ship year this indicates a frequency 2 92 times higher in the sog if the frequency of collisions under way is compared it can be shown that the frequency of collisions under way at north west europe during the period 1978 1994 was 1 32e 02 per ship year dnv 1996 and 7 88e 03 per ship year for word wide during the period 1994 2004 imo 2008a b from table 8 and considering that collisions under way represent 86 of the total frequency as explained in section 4 1 the frequency of collisions under way at sog for the period 2000 2011 is estimated to be 3 12e 02 per ship year this indicates a frequency 2 36 times higher in the sog with respect to nw europe and 3 96 times higher with respect to world wide exposure contact the frequency of contact at north west europe during the period 1978 1994 was 4 90e 02 per ship year dnv 1996 and for world wide during the period 1994 2004 was 1 25e 02 per ship year imo 2008a b from table 8 the frequency of contact at sog for the period 2000 2011 is estimated to be 1 04e 02 per ship year this indicates a frequency 4 71 times higher in nw europe and 1 2 times higher in world wide exposure with respect to sog fire the frequency of fires at north west europe during the period 1978 1994 was 1 00e 02 per ship year dnv 1996 and for word wide during the period 1994 2004 was 8 28e 03 per ship year imo 2008a b from table 8 the frequency of fires at sog for the period 2000 2011 is estimated to be 5 21e 03 per ship year this indicates a frequency 1 92 times higher in nw europe and 1 59 times higher in world wide exposure with respect to sog overall frequency the overall frequency for all critical scenarios collisions under way groundings impacts fires and flooding at north west europe during the period 1978 1994 was 9 44e 02 per ship year dnv 1996 and for word wide during the period 1994 2004 was 4 05e 02 per ship year imo 2008a b from table 8 the overall frequency at strait of gibraltar for the period 2000 2011 is estimated to be 4 69e 02 per ship year this indicates a frequency 2 times higher in nw europe and 1 16 times lower in world wide exposure with respect to sog it is noticed that for grounding and flooding the accident frequency is estimated to be zero at the sog for the period examined it is evident from the above analysis and comparison that collision frequency in the sog area is very much higher compared with experience from north west europe dnv 199nw and world wide imo 2008a b it is significant that studying a particular geographical area such as the sog the concentration of collisions involving ropax fleet is greater in relation to another area nw europe experience however due to differences in reporting to both database and periods the differences between frequencies calculated above should be used as reference only in this respect an early study papanikolaou et al 2015 gives an overview of all basic merchant ship types in terms of accidents occurrence frequencies and consequences for the period 1990 2012 this research uses the same resource as us ihs sea web database and therefore it analyzes the same accidents type the outcomes of the research showed above indicate that the overall frequency of serious accident s occurrence in ropax ships during the period 2000 2012 was 4 39e 02 per ship year the overall frequency at strait of gibraltar for the period 2000 2011 is estimated to be 6 25e 02 per ship year this indicates a frequency 1 42 times higher in the sog in addition the frequency of collisions in ropax world fleet during the period 2000 2012 was 5 69e 03 per ship year the frequency of collisions at sog for the same period is estimated to be 3 84e 02 per ship year this indicates a frequency 6 7 times higher in the sog 6 analysis and discussion of the results based on historical accident data analysis and expert judgement the main hazards associated to the navigation of ropax ships in the sog are a communication problems ship to shore and ship to ship the lack of a formal procedure for the exchange of information between the vts in algeciras and the port of gibraltar can mean that vessels coming into or going out of the bay of algeciras do not know the exact movements of other ships that are sailing or operating in the same area in fact the failure of the gibraltar port authority to report ship movements has been identified by the experts as the hazard with the highest risk factor given its frequency rate a limited level of navigational assistance for ships has also been detected on the part of the on shore stations and ports dgmm 2012 especially in conflict situations all this is exacerbated by the lack of communication between ships that are in situations where collision is imminent b fatigue it appears to be an inconsistency that despite 100 of the participants in the expert judgement process voting fatigue as one of the main hazards affecting navigation in this area it was not contemplated as a cause in any of the official reports on the collisions that have occurred in the past 3 3 the official report on the collision between the ciudad de ceuta and the ciudad de tanger mentions fatigue as an underlying or remote cause which lessens its significance it is important to emphasise that according to the experts this hazard is classified with a high risk factor therefore action should be taken to reduce this situation c non compliance with colreg regarding this matter both studies appear to agree that failure to comply with some of the colreg rules are direct causes of collisions there was a failure to observe rules 7 and 8 in all cases if the accident occurred then it seems logical that it was because one of these rules was not followed d inadequate vigilance and distraction of oow although inadequate surveillance comes under non compliance with colreg it is worth explaining this factor in greater detail on passenger ships the usual procedure is to have surveillance on the bridge by an on duty officer and a lookout there can even be two officers in some cases however the official collision reports studied show this type of vigilance to be far from effective the lack of vigilance both visual and through navigational aids was identified as a cause of collision the routine of the journey in regular lines detracting attention due to situations of conflict the oow being distracted by other tasks during the watch stress tiredness and even the automation of the bridge equipment could be underlying causes which lead to a poor or inadequate surveillance e overconfidence on the part of the oow it is often the case that the oow has too much faith in the vessel s capabilities e g manoeuvrability or speed this excess confidence is what can often produce for example drastic manoeuvres in order to avoid collision between hscs and ships in transit which is an abuse not only of the regulations but also of good seamanship f fog poor visibility as a direct consequence of the emergence of fog is considered as a determining factor detrimental to safe navigation in this area g light pollution the difficulty of port calls in the bay of algeciras at night with poor visibility is well known this is a result of the high level of light pollution in the area due to the location of the ports of algeciras and gibraltar the refinery at san roque etc which at times can make both the visual navigational aids and the lights from other boats imperceptible this can lead to confusion on the part of the oow on some occasions this pollution to a lesser extent also affects the area to the south of the strait in the vicinity of ceuta h high presence of fishing boats in the area the existence of moroccan fishing vessels in the vicinity of the tss at the point of cape spartel often compromises the safety of the vessels in transit this is aggravated by the deployment of driftnets by these vessels for fishing these nets clog the area and compromise the safety of the vessel propulsion systems fishing vessels carrying the spanish flag do not normally cause conflict situations with the tss as these boats fish inside the traffic separation zone however the movements of these vessels to the place of fishing can pose a threat to navigation i density of traffic in the area the geographical location of the sog makes it a privileged enclave as the gateway to the mediterranean from the atlantic ocean thus it is one of the main shipping areas in the world approximately 110 000 vessel movements are recorder every year resulting in a very congested area complicating safe navigation through its waters j occupation of areas at the entrance to the bay of algeciras as commented earlier the strait is one of the main navigational areas so for this reason the ports located in the area have enhanced their provisioning and bunkering services as is the case with the ports of algeciras and gibraltar therefore the ships which arrive in the bay for provisioning often use the outer limits of the area to carry out these operations these boats are regarded as off limits the gathering of these boats which sit at the entrance to the bay impede the safe transit in this area to a large extent k competition between ferries although there is no accident report which refers to this matter the experts believe that this does occur in the area and is detrimental to safe navigation commercial pressure to meet service timetable schedules can sometimes mean that ferries from different companies compete during their voyages to see who can arrive first at port for mooring request pilotage etc in the light of the foregoing some risk control measures could be advised as follow regarding to main identified hazards related with human factors such as fatigue overconfidence and lack lookout of oow the ropax ships owners operators are recommended to review the operational procedures of the safety management system to establish clear instructions that promote safety of navigation and the environment and through appropriate supervision ensure that the operational procedures are complied with in particular the following should be addressed 1 maintaining a proper lookout at all times 2 the provision of effective bridge resource management and bridge team 3 revision of the rest hours of the crew members regarding to hazard related with technician factor such as the communication problems ship to shore and ship to ship this research suggests establishing formal links communications between the two port authorities gibraltar and algeciras to ensure a full exchange of traffic information in addition on the bases of the risk analysis results it is observe that collision accidents are the most common accident involving ropax ships also the frequencies of occurrence s collisions in the sog are greater than frequencies of previous studies two types of incident frequencies were calculated frequency per ship year and frequency per movements year however the authors consider more appropriate to perform a calculation of frequencies by ship movements whenever possible since the risk of an accident will be greater the more the ship moves finally risk model for collisions estimates that the individual risk levels for both crew and passenger are inside to the alarp region it would be desirable to reduce that risk as long as feasible 7 conclusions in this paper we presented the results of a risk analysis study for ropax ships in the strait of gibraltar hazard identification has been carried out using a combination of two standard techniques historical accident data analysis and expert judgement both performed as the first step in the study also a high level model risk for collisions was established through of an even tree to determinate the safety level of ropax ships in the area according to the results from the hazid it is concluded that failure to report ship movements by the port of gibraltar and fatigue among crew members are the hazards which are identified as high risk indices therefore it is recommended that specific studies be carried out to reduce their incidence in this respect potential risk control measures have been proposed such as a to establish formal links communications between the two port authorities gibraltar and algeciras to ensure a full exchange of traffic information b to review the operational procedures of the safety management system to establish clear instructions that promote safety of navigation and the environment in addition the present study illustrates current levels of safety in terms of accidental frequency for the total fleet in general and ropax fleet in particular the frequencies have been calculated per ship year and per ship movements year in this respect the authors consider more appropriate to perform a calculation of frequencies by using ship movements for a specific area whenever possible from the results we find that collisions are the most common accident scenarios affecting ropax ships also the frequencies of collisions occurrence in the sog are greater than frequencies calculated in previous studies especially if for the same period the world ropax fleet is compared with the ropax fleet in the sog finally risk model for collisions estimates that the individual risk levels for both crew and passenger are inside to the alarp region on this basis a future research in continuation of this study would be performed to promote risk control options and to reduce the safety levels of ropax ships in the area acknowledgements the authors are indebted to the anonymous reviewers and editor whose comments improved the presentation and the content of an earlier version of the manuscript 
23601,the strait of gibraltar sog is one of the principal navigation areas in the world the maritime traffic registered in the area is approximately 110 000 ship movements per year where thirty three per cent of total traffic involves roll on roll off passenger ropax ships which run scheduled voyages between ports in the area there are presently many accidents involving this type of ship being reported although these incidents have serious consequences both based on a financial scale and regarding human safety there is no formal maritime risk analysis study for this area carried out to date the aim of this paper is to present the results of a risk analysis for ropax ships operating in the sog based on accidents statistics covering the period 2000 2011 the work has been performed using the two first steps of the imo formal safety assessment methodology hazard identification and risk analysis to identify the hazards and their associated scenarios and quantify their frequencies and consequences the historical accident data analysis and expert judgement techniques were used a risk matrix has been drawn up to calculate the risk indices of the identified hazards a comparative study of the accident frequencies obtained from similar previous studies is also presented in the paper a high level model risk for collisions was established through the elaboration and quantification of an event tree calculating the individual and social risks the conclusions of this study could serve as recommendations to be used in a subsequent decision making process keywords risk analysis ropax ships maritime safety 1 introduction ships designed to carry passengers and roll on roll off cargo ropax ships are among functional types of ships the flexibility ability to integrate with other transport systems and operating speed has become extremely popular in many routes however as past accident statistics demonstrate there are numerous examples of accidents involving ropax ships consequences of these accidents include large numbers of lost lives serious damage to the environment and economic costs the capsize of the herald of free enterprise in 1987 the fire of onboard the scandinavian star in 1990 and the sinking of the estonia in 1994 are notable cases of well known and investigated maritime casualties unfortunately major maritime disasters involving ropax ships still occurring in the last decade as the sinking of the ferry al salam boccaccio 98 in february 2006 and more recently the sinking of sewol in april 2014 studies relating to shipping risk assessment have received growing interest in the last years risk assessment has been very helpful for the review and development of new rules and regulations in order to reduce accidents and improve maritime safety many methods and applications for maritime transportation risk analysis have been presented in the literature goerlandt and montewka 2015 present a complete review of scientific approaches to risk analysis focusing on applications addressing accidental risk of shipping in a sea area the review covers the period from 1970 to 2014 up to a total of 58 applications a number of risk assessments studies applied to ropax ships can be found in the literature det norske veritas dnv 1996 carried out a study on ropax ships sailing in the north west of europe focusing on the investigation of operational dangers and causes of such vessels and quantify when possible their frequency and consequences through the creation of a risk model based on event trees et van dorp et al 2001 and merrick et al 2003 carried out both risk assessments on ferries at specific geographical areas washington state and san francisco bay respectively otto et al 2002 submitted a risk analysis for these ships to study the damages produced from collision and grounding at the same time the international maritime organization imo adopted in 2002 the formal safety assessment imo 2002 a structured and systematic methodology aimed to increase maritime safety based on risk analysis accordingly we can find studies as the hazard identification related to casualties of ropax vessels antao and soares 2006 which used the fsa methodology another risk analysis study for the world wide ropax fleet konovessis and vassalos 2007 konovessis et al 2008 guarin et al 2009 was carried out as part of the activities of the safedor integrated project safedor 2005 2009 and submitted to imo imo 2008a in addition gemelos and ventikos 2008 present a risk assessment on greek passenger ships in order to estimate the safety level of greek coastal shipping more recently other studies montewka et al 2014 goerland et al 2014 are focusing in the development of a novel framework for estimating the risk and consequences resulting of open sea collisions involving a ropax using tools such as bayesian belief networks bbn to calculate the fatalities risk on this background in this paper we present results of a risk analysis study for ropax ships in a specific area the strait of gibraltar sog there is no earlier study available in the literature to the maritime risk analysis in this specific area with exception of piniella and walliser 2013 which however focused in drawing the taxonomy and distribution of maritime emergencies in the area the strait of gibraltar is one of the main shipping areas in the world it is well known for its high volume of maritime traffic which is dependent on a maritime organization system mainly controlled by a traffic separation scheme tss the aim of this tss is to organize traffic and to avoid the occurrence of maritime accidents however this area is prone to problematic conditions which lead to the sog being a hazardous area for safe maritime navigation the weather is one such troublesome element due to the continuous winds from either the east or west there is also frequent fog in the area in addition there is a limited geographical area intended for navigation aggravated by frequent geopolitical disputes between the different states who lay claim to the waters and disagree over who has control of the total traffic registered in the area approximately 110 000 vessel movements per year 33 involves ropax ships which run regular voyages between the ports in the area in 2012 35 925 movements of ropax were recorded representing an average of 98 daily movements 1 1 traffic data have been provided by spanish maritime safety agency http www salvamentomaritimo es and although these ships are extremely common in this area they are involved in a high number of accidents as a consequence of the hazards associated to this type of transport and the area in which they sail ropax ships sail in the area according to three route types connecting the following ports a route between algeciras port and ceuta port b route between algeciras port and tanger mediterraneo port c route between tarifa port and tanger city port fig 1 shows the ports located in the area as well as the ropax ship routes it is noticed that these routes cross the tts located in the area which means the ropax ships are constantly cut crossing the main sog maritime traffic increasing the risk of collision in addition the modification of the tts in 2007 on the occasion of the opening of the service tanger mediterraneo port in 2009 has contributed on an increase of cross points of conflict in the area particularly in the eastern part ropax fleet compromises of ferries and high speed craft hsc above 1000 gt during the period covered by the present study there were on average 16 ropax ships covering these services one example of a common ropax ship in the area can be seen in fig 2 the aim of this study is to carry out a risk analysis for ropax ships navigating in the sog based on accidents statistics collected for the period 2000 2011 the work has been performed in accordance with the imo fsa guidelines the first objective consists of identifying the hazards and their associated scenarios and quantifying to the extent possible their frequencies and consequences in order to do this the historical accident data analysis and expert judgement techniques were used a risk matrix has been drawn up which is supported by the opinion of experts in order to calculate the rates and levels of risk also a high level model risk for collisions will be established through the elaboration and quantification of an et it is used to determinate the safety level of ropax calculating the individual and social risks a study of the accident frequencies obtained from similar previous studies has also been carried out finally the conclusions of this study could serve as recommendations for a subsequent decision making process 2 methodology 2 1 approach adopted the fsa adopted by the international maritime organization imo in 2002 is a structured and systematic methodology aimed at enhancing maritime safety this includes the protection of life health property and the marine environment by using risk analysis and cost benefit assessment the last update of the fsa guidelines was in 2012 imo 2012 fsa consists of five steps as follow 1 identification of hazards 2 risk analysis 3 risk control options 4 cost benefit assessment and 5 recommendations for decision making this paper is focused in the step1 hazard identification and step 2 risk analysis 2 2 hazard identification the aim of this step is to identify a list of hazards and their associated scenarios prioritized by risk levels to determine these hazards standardized techniques are used these hazard identification tools give answers to questions such as what can go wrong and why the guidelines for carrying out an fsa imo 2012 recommend using a combination of creative and analytical techniques for developing the hazard identification hazid the creative element aims to ensure the process uses foresight rather than just being limited to those hazards that have materialized in the past on the other hand the analytical element guarantees that experience is used for full advantage by using the information available this study has been carried out using historical accident data analysis and expert judgement techniques it follows that the former one is analytical and makes use of the information available on accidents that have happened in the past while the latter one is creative and assures that this research will not be limited to only those hazards which have triggered accidents 2 2 1 historical accident data analysis historical analysis has been carried out using the ihs fairplay sea web database earlier known as lloyds register fairplay from 2000 to 2011 the information registered in this database has been supported by the official reports of the accidents which were produced by the different government organizations 2 2 2 expert judgment methodology expert judgement involves using the information and data given by qualified individuals in order to solve problems or make decisions in a range of areas skjong and wentworth 2001 meyer and booker 1991 an expert is defined as a person with experience in the area of study whose expertise is recognised by their peers in this field in this way expert judgement generates opinions based on the individual training and experience of those who participate the experts give information evidence opinions and assessment of the subject in our study the opinions of the experts have been extracted using the delphi method a delphi study involves choosing a group of experts who are asked their opinion about a particular matter this information is then gathered together so that subsequent questionnaires can be designed with the aim of finally reaching a consensus all the time the complete autonomy of the participants is respected astigarra 2003 it could be considered as a feedback process where a unique consensus is reached through the completion of diverse questionnaires one of the advantages of this method is that it does not require experts to come together in group sessions in this manner any view expressed will not be conditioned upon personal conflicts pressures from within the group the appearance of a leader and so forth thus the process was divided into three stages choosing the experts elicitation and aggregation phases respectively the elicitation phase refers to the process of suggesting opinions or expert judgements through special means this is usually supplied through interviews and or questionnaires the aggregation phase is the converging of the different expert opinions proposed in the study meyer and booker 1991 stage 1 choosing the experts the first step was to choose the experts who would take part in the study in this case the number of participants was 15 where the experts were 7 captains and 5 officers with experience in maritime traffic of the area 2 members of the maritime pilot s association of the bay of algeciras and 1 flag state surveyor stage 2 elicitation once the participants were chosen the process for obtaining the information was outlined the process used was the delphi method through the use of questionnaires in order to draw up the questionnaires earlier study carried out by åland sea fsa nyman et al 2010 was used as a reference based on the process outlined above a first open end questionnaire was designed with the objective of identifying a list of hazards and their associated scenarios which could result in a risk for maritime traffic in the sog the experts had to define those initial events which in their opinion could lead to undesirable events which would have negative consequences for people property and or the environment as this study focuses on risks during navigation all hazards associated with the loading and unloading of people and vehicles port manoeuvres and mooring were excluded from the study stage 3 aggregation with the results from the first questionnaire a list was drawn up which included all the identified hazards in total there were 63 hazards related to maritime traffic this list of hazards was re introduced to the participants as a closed end questionnaire so that they could rank the hazards posing the greatest threat to ropax ships during this aggregation stage the experts had to highlight the ten most significant hazards among the list of 63 hazards contained in the questionnaire 2 2 3 risk indices classification the identified hazards have been classified according to their index of risk the experts gave each hazard a rating depending on the frequency with which it occurs and the severity of the consequences for this purpose this study used the logarithmic frequency scale and logarithmic severity scale recommended by the imo fsa guidelines imo 2012 see tables 1 and 2 the combination of these scales result in a risk index represented in the form a risk matrix 2 3 risk analysis risk analysis should lead to a thorough investigation of the causes and consequences of the associated scenarios to the major hazards identified in the hazid for that probabilistic and deterministic techniques are used which solve issues as how often and what are the effects frequency is the number of occurrences of an undesirable event expressed as events per unit of time in this paper accident frequency was defined as the number of incidents divided by the number of ship years casualty statistics analysis has been carried out on the basis of historical data using the sea web database for the period 2000 2011 the fleet statistics for the same period was obtained by the spanish maritime safety agency also a high level model risk for collisions in ropax vessels under way at the sog was established through the elaboration and quantification of an et as collisions are the most frequent accident involving ropax ships according to the accident statistics analysed for the sog area et is a standard technique recommended by the imo fsa guidelines imo 2012 the assignment of branch probabilities on the event tree was done on the basis of the accident statistics of the present study for the period 2000 2011 earlier research studies dnv 1996 imo 2008a b konovessis et al 2008 and expert judgment 3 hazid results 3 1 historical accident analysis for ropax vessels in the sog according to the sea web database the accidents are classified according to six categories sinking fire or explosion collision contact grounding and damage caused to the hull or machinery accidents involving ropax ships during the period 2000 to 2011 in the sog include the following 7 accidents caused by collision 2 accidents caused by contact 2 cases of damage to the hull or machinery and 1 accident involving fire or explosion this total 12 incidents involving ropax ships given that the majority of the incidents come under the collision category this study will focus on identifying the hazards and causes associated with this type of accident with reference to table 3 it is noted that the collision between al mansour and ciudad de málaga of 18 04 2007 ocurred in the port whilst al mansour was moored hence in this respect 6 out of the 7 collisions of table 4 86 are collisions when the ship was under way and 1 collision 14 when the ship was moored 3 2 analysis of the causes of collisions in order to determine the causes of collisions which occurred in the sog during the period from 2000 to 2011 it was necessary to refer to the official accident reports drawn up by the appropriate authorities cma 2007 dgmm 2000 dgmm 2001 dgmm 2006 for the purpose of analysis of causes we will also use the accident report dgmm 2012 of a collision incident involving the ferry milleniun ii which occurred in january 2012 from the accident reports it is observed that the main causes for collision between ropax ships in the area are due to the following failure to comply with the international regulations for preventing collisions at sea colreg this mainly relates to rule 7 risk of collision and rule 8 action to avoid collision inadequate surveillance on the bridge this deals with both visual and radar surveillance this cause would also be classified as failure to comply with colreg under rule 5 look out poor visibility due to fog evidence suggests that the frequent fogs in the sog can hinder safe navigation in the area the occurrence of collisions under such circumstances would appear to be associated with a failure to comply with colreg rule 19 conduct of vessels in restricted visibility unsafe speed related to a those collisions where ships were navigating in reduced visibility and did not reduce their speed as is required by colreg rule 6 safe speed b as a consequence of the high speeds reached by high speed crafts hscs under pressure to meet schedule times poor or non existent ship to ship and ship to shore communication there are several accidents in which the ships involved have not established communication between themselves before collision this also applies to vts on shore and ships characterized by the poor insufficient or lack of information being transmitted 3 3 hazid using expert judgement with the aim of incorporating the creative analytical element in identifying the hazards which pose a threat to safe traffic of ropax ships in the sog a hazid study has been carried out using expert judgement the hazards identified by the experts are shown in table 5 sorted by number of votes obtained the table only shows those hazards which were voted by at least a third of the experts this provides a total of 13 hazards 3 4 calculation of the rates and indices of risk for the hazid so that the hazards can be classified according to their index of risk the experts gave each hazard a rating depending on the frequency with which it occurs and the severity of the consequences in this way the hazards identified in table 4 were re introduced to the experts so that they could assign a value to each hazard depending on the index of frequency and severity outlined above the results from the sum of each scale are shown in table 5 where the hazards appear in order from the highest to the lowest index of risk it also shows the risk matrix developed with reference to the risk index table 6 the different levels of the matrix represent the index of risk for each hazard both quantitatively and qualitatively from low to high risk therefore green indicates those hazards which have a low risk risk rating 2 5 yellow indicates those hazards which have a moderate risk risk rating 6 7 and red is attributed to those hazards which have a high risk index risk rating 8 9 based on the results it can be concluded that according to expert opinion the failure to report ship movements by the port of gibraltar and fatigue among crew members are the hazards which are identified as being of highest risk index regarding the failure to report ship movements by the port of gibraltar the high index of risk is due to its rate of frequency which scores between frequent and reasonably probable fatigue also borders on reasonably probable in both cases the consequences are considered significant the rest of the hazards shown in table 6 fall under the considerable risk category and ways to reduce them should also be considered in order to show uncertainty of main hazard outcomes in both severe consequences and expected frequency probability tukey box plots are performed mcgill et al 1978 fig 3 shows the distribution of the probability and consequence variables to both events 57 failure to report ship movements by the port of gibraltar and 1 fatigue among crew members in addition according to the proposal of goerlandt and reniers 2016 the qualitative strength of evidence assessment has been performed using a 3 level categorization of each applicable evidence category namely expert judgments and assumptions a traffic light symbolism is applied to rate each evidence category whereas green yellow red means strong medium red the evidence for probability and consequence is for the main risk events are performed in fig 3 for each event each evidence type is assessed using a set of evidential qualities described in table 7 see goerlandt and reniers 2016 for further details 4 risk analysis 4 1 accidents statistics and frequency analysis casualty statistics analysis has been carried out on the basis of historical data for the period 2000 2011 the fleet statistics for the same period was obtained by the spanish maritime safety agency table 8 shows the records of incidents and their frequencies two types of frequency have been calculated for this study a frequency per ship year where the number of the fleet at risk corresponds to the total number of ropax that operated in the area during the specified period b frequency per movement year where the number of the fleet at risk corresponds to the total number of records of ship movements underway in the area during the specified period previous studies for ropax ships dnv 1996 referring to north west european experience for the period 1978 1994 and imo 2008a b referring to world wide fleet for the period 1994 2004 have only calculated the frequencies per ship year however it seems appropriate to perform a calculation of frequencies by ship movements since the risk of an accident will be greater the more the ship moves as the sog is an area controlled by a vts it is possible to get the movement ship records for the established period table 8 shows the frequency of collisions in ropax ships is the highest value compared to other categories of incidents the frequency of collisions is estimated to be 3 65e 02 per ship year if the collision involving the millenium ii of 2012 is introduced the collision frequency becomes 3 84e 02 per ship year table 8 also shows that the frequency of collisions expressed in movement ships year at sog for the period 2000 2011 is estimated to be 1 83e 05 for ropax and 8 89e 06 for rest of the fleet this indicates that the frequency of collisions in ropax is higher in relation to the rest of the fleet 4 2 calculation of consequences 4 2 1 introduction to risk criteria risk acceptance criteria used in this study have been determined from the relevant literature skjong et al 2007 imo 2008a imo 2012 individual risk is the risk of death injury and illness experienced by a single individual crew member or passenger in a given time period who in our case is exposed to hazards relating to ropax operations the individual risk is usually expressed as the frequency of an individual fatality per year imo 2012 proposes a series of criteria for individual risk in shipping operations at the same level as those used by the uk health and safety executive these criteria are shown in table 9 see table 10 social risk is the average risk regarding the number of deaths experienced by a whole group of people crew member or passengers traveling on a ship social risk is usually taken as the risk of death and is typically expressed as potential loss of life pll or fn diagrams imo 2012 in this study we use pll defined as the expected number of deaths for a year 4 2 2 mortal incidents once the risk criteria have been established it is necessary to estimate the individual and social risks derived from this study there is only an accident registered in ropax ship during the period 2000 2011 where there were mortal victims the collision in the ferry ciudad de ceuta with five fatalities table 9 presents the calculation for the potential loss of life during the period 2000 2011 at the sog the pll is estimated to be 2 60e 02 per ship year 4 2 3 risk model in order to calculate a consequences level which is not only based on historical data a risk model to estimate the level of consequences has been developed a high level model risk for collisions in ropax vessels underway at the sog was established through the elaboration and quantification of an et the selection of this initiating event collision is in agreement with the outcome of the hazid work through the consideration which all identified hazards may cause one collision in addition as it can be seen from the frequency analysis this initiating event represents 58 of all incidents involving ropax ships in the sog fig 4 presents the generic collision et based on ropax fleet at the sog during the period 2000 2011 the frequency for collision incidents estimated is 3 65e 02 per ship year data used and assumptions made are the follows level 1 casualty data for ropax at the sog indicate that during the period 2000 2011 of the 7 recorded collision casualties 6 86 are collisions under way and only one collision is striking whilst at berth these percentages are used as the branch probabilities on the event tree level 2 of the total collisions under way 5 83 are recorded as minor incidents and 1 17 represent a serious casualty 2 2 a collision is considered serious if as result of it has been one or multiple fatalities or damage to the vessel that has halted service or total loss those collisions where only ship damages occurred and only reparations were needed are considered minor incidents level 3 the probability the ship being the struck or the striking ship is assumed to be 50 50 konovessis et al 2008 level 4 none of the collisions occurred during the period 2000 2011 in the sog involving ropax ships resulted in flooding or fire onboard the ship due to the limited number of collisions in the area other references based on relevant previous studies imo 2003 imo 2008a vanen and skjong 2004 were taken into account to estimate the branch probability of flooding or fire occurring on the struck ship in this respect it is considered that the probability of flooding into a struck ship after a collision is 50 it is assumed that the probability of fire is zero in the same way it is assumed that the probability of flooding on a ship striking is 5 it is understood that damage to the striking ship is limited to her bow area where the collision bulkhead is hence avoiding water ingress beyond the collision bulkhead level 5 and 6 branch probabilities in these levels are adopted from relevant results from imo 2003 imo 2008a it is noticed that we distinguish two cases for impacts one involving fatalities due the impact and the most common case of impacts not resulting in fatalities this is due to the fact that the only collision case involving fatalities in our study can be classified as an impact involving fatalities this is reflected in the et we estimate the corresponding branch probabilities in the event tree on the following assumption just in one of 5 collisions there have been produced mortal victims that means that 80 have produced no mortal impacts and 20 mortal impacts 4 2 3 1 risk calculations and reference data fort the calculation of individual risk the following is assumed average number of crew on ropax ships operating in the area is 30 average maximum carrying capacity is 780 passengers there are different fluctuations of number of passengers depending to the seasonal period according to this and to maximum capacity of 780 passengers is assumed a 30 of trips carrying full passengers load 780 passengers b 40 of trips carrying half of maximum passengers load 390 passengers c 30 of trips carrying 35 of maximum passenger load 273 passengers these percentages are based on expert opinions captain and officers currently employed by shipping companies average fatality rates used for the different potential scenarios are the same used in imo 2008a b dnv 1996 namely a 12 fatality for flooding incidents leading to slow sinking and 66 for incidents leading to rapid capsize there are not probabilities of fatalities for these cases which the ship had only impact without flood the cruise ships fsa study imo 2008c includes fatality rates for the impact scenario only due to differences between the types of ships of each study and the number of passengers it does not seem appropriate to rely on their results according to our data the following assumptions could be made a on board of ciudad de ceuta ferry only collision with fatalities had 331 people of which 5 died in the collision the averages fatality rate is 0 015 b if the average number of persons on board a ropax ship at half load in the sog is 420 people and there were seven collisions this represents 2940 persons exposed to the collision as for those people only there were 5 deaths the mortality rate will be 0 17 for all ships representing 0 02 of each ship as shown the results are so low 0 015 0 02 we could disregard this risk furthermore the assumptions are based on our registered accidents representing a very limited number of cases table 11 present the risk calculations for the outcomes of collision on the basis of the event tree of fig 4 4 2 3 2 individual risk calculation on basis collision risk model table 12 summarizes the calculations presented the individual risk calculated by the collision risk model is 1 17e 0 4 per year assuming the ship being at sea and a person being onboard for the full year to estimate of the individual risk experienced by crew members and passengers the following considerations can be made for crew members assuming that the ship operates 24 h a day of which half 12 h is in navigation and there is a crew rotating of 50 50 each day the model predicts an overall individual risk for crew of 2 92e 05 per year but usually in these vessels object of this study it is that there are several crews for one ship ensuring periods of rest holiday thus we can further assume a if there is a period of rest generated 1 1 i e one working day corresponds to a day of rest equivalent to 4 crews for one ship the overall individual risk becomes 1 46e 05 per year b if the period of rest is generated 2 1 i e one working day corresponds to a half of rest equivalent to 3 crews for one ship the overall individual risk becomes 1 95e 05 per year for passenger assuming that the average duration of a trip is 90 min so a return trip is 3 h we have a for a passenger traveling once a year round trip the average individual risk is 4 01e 08 per year b for a passenger who usually makes a trip round trip a week for a year the individual risk is 2 08e 06 per year this probability could be applied to truck drivers who regularly use these lines considering the fig 5 it can be concluded that individual risk levels during a collision are within the alarp as low as reasonably practicable region for both passenger and crew members it would be desirable to reduce that risk as long as feasible potential loss of life is 5 85e 02 ship years equivalent to 1 death per year 5 comparison with previous studies a comparison with frequencies calculated in dnv 1996 referring to north west european experience for the period 1978 1994 and imo 2008a b referring to world wide experience for the period 1994 2004 is attempted in this section the following are the points that can be made collision the frequency of collisions world wide during the period 1994 2004 was 1 25e 02 per ship year imo 2008a b from table 8 the frequency of collisions at sog for the period 2000 2011 is estimated to be 3 65e 02 per ship year this indicates a frequency 2 92 times higher in the sog if the frequency of collisions under way is compared it can be shown that the frequency of collisions under way at north west europe during the period 1978 1994 was 1 32e 02 per ship year dnv 1996 and 7 88e 03 per ship year for word wide during the period 1994 2004 imo 2008a b from table 8 and considering that collisions under way represent 86 of the total frequency as explained in section 4 1 the frequency of collisions under way at sog for the period 2000 2011 is estimated to be 3 12e 02 per ship year this indicates a frequency 2 36 times higher in the sog with respect to nw europe and 3 96 times higher with respect to world wide exposure contact the frequency of contact at north west europe during the period 1978 1994 was 4 90e 02 per ship year dnv 1996 and for world wide during the period 1994 2004 was 1 25e 02 per ship year imo 2008a b from table 8 the frequency of contact at sog for the period 2000 2011 is estimated to be 1 04e 02 per ship year this indicates a frequency 4 71 times higher in nw europe and 1 2 times higher in world wide exposure with respect to sog fire the frequency of fires at north west europe during the period 1978 1994 was 1 00e 02 per ship year dnv 1996 and for word wide during the period 1994 2004 was 8 28e 03 per ship year imo 2008a b from table 8 the frequency of fires at sog for the period 2000 2011 is estimated to be 5 21e 03 per ship year this indicates a frequency 1 92 times higher in nw europe and 1 59 times higher in world wide exposure with respect to sog overall frequency the overall frequency for all critical scenarios collisions under way groundings impacts fires and flooding at north west europe during the period 1978 1994 was 9 44e 02 per ship year dnv 1996 and for word wide during the period 1994 2004 was 4 05e 02 per ship year imo 2008a b from table 8 the overall frequency at strait of gibraltar for the period 2000 2011 is estimated to be 4 69e 02 per ship year this indicates a frequency 2 times higher in nw europe and 1 16 times lower in world wide exposure with respect to sog it is noticed that for grounding and flooding the accident frequency is estimated to be zero at the sog for the period examined it is evident from the above analysis and comparison that collision frequency in the sog area is very much higher compared with experience from north west europe dnv 199nw and world wide imo 2008a b it is significant that studying a particular geographical area such as the sog the concentration of collisions involving ropax fleet is greater in relation to another area nw europe experience however due to differences in reporting to both database and periods the differences between frequencies calculated above should be used as reference only in this respect an early study papanikolaou et al 2015 gives an overview of all basic merchant ship types in terms of accidents occurrence frequencies and consequences for the period 1990 2012 this research uses the same resource as us ihs sea web database and therefore it analyzes the same accidents type the outcomes of the research showed above indicate that the overall frequency of serious accident s occurrence in ropax ships during the period 2000 2012 was 4 39e 02 per ship year the overall frequency at strait of gibraltar for the period 2000 2011 is estimated to be 6 25e 02 per ship year this indicates a frequency 1 42 times higher in the sog in addition the frequency of collisions in ropax world fleet during the period 2000 2012 was 5 69e 03 per ship year the frequency of collisions at sog for the same period is estimated to be 3 84e 02 per ship year this indicates a frequency 6 7 times higher in the sog 6 analysis and discussion of the results based on historical accident data analysis and expert judgement the main hazards associated to the navigation of ropax ships in the sog are a communication problems ship to shore and ship to ship the lack of a formal procedure for the exchange of information between the vts in algeciras and the port of gibraltar can mean that vessels coming into or going out of the bay of algeciras do not know the exact movements of other ships that are sailing or operating in the same area in fact the failure of the gibraltar port authority to report ship movements has been identified by the experts as the hazard with the highest risk factor given its frequency rate a limited level of navigational assistance for ships has also been detected on the part of the on shore stations and ports dgmm 2012 especially in conflict situations all this is exacerbated by the lack of communication between ships that are in situations where collision is imminent b fatigue it appears to be an inconsistency that despite 100 of the participants in the expert judgement process voting fatigue as one of the main hazards affecting navigation in this area it was not contemplated as a cause in any of the official reports on the collisions that have occurred in the past 3 3 the official report on the collision between the ciudad de ceuta and the ciudad de tanger mentions fatigue as an underlying or remote cause which lessens its significance it is important to emphasise that according to the experts this hazard is classified with a high risk factor therefore action should be taken to reduce this situation c non compliance with colreg regarding this matter both studies appear to agree that failure to comply with some of the colreg rules are direct causes of collisions there was a failure to observe rules 7 and 8 in all cases if the accident occurred then it seems logical that it was because one of these rules was not followed d inadequate vigilance and distraction of oow although inadequate surveillance comes under non compliance with colreg it is worth explaining this factor in greater detail on passenger ships the usual procedure is to have surveillance on the bridge by an on duty officer and a lookout there can even be two officers in some cases however the official collision reports studied show this type of vigilance to be far from effective the lack of vigilance both visual and through navigational aids was identified as a cause of collision the routine of the journey in regular lines detracting attention due to situations of conflict the oow being distracted by other tasks during the watch stress tiredness and even the automation of the bridge equipment could be underlying causes which lead to a poor or inadequate surveillance e overconfidence on the part of the oow it is often the case that the oow has too much faith in the vessel s capabilities e g manoeuvrability or speed this excess confidence is what can often produce for example drastic manoeuvres in order to avoid collision between hscs and ships in transit which is an abuse not only of the regulations but also of good seamanship f fog poor visibility as a direct consequence of the emergence of fog is considered as a determining factor detrimental to safe navigation in this area g light pollution the difficulty of port calls in the bay of algeciras at night with poor visibility is well known this is a result of the high level of light pollution in the area due to the location of the ports of algeciras and gibraltar the refinery at san roque etc which at times can make both the visual navigational aids and the lights from other boats imperceptible this can lead to confusion on the part of the oow on some occasions this pollution to a lesser extent also affects the area to the south of the strait in the vicinity of ceuta h high presence of fishing boats in the area the existence of moroccan fishing vessels in the vicinity of the tss at the point of cape spartel often compromises the safety of the vessels in transit this is aggravated by the deployment of driftnets by these vessels for fishing these nets clog the area and compromise the safety of the vessel propulsion systems fishing vessels carrying the spanish flag do not normally cause conflict situations with the tss as these boats fish inside the traffic separation zone however the movements of these vessels to the place of fishing can pose a threat to navigation i density of traffic in the area the geographical location of the sog makes it a privileged enclave as the gateway to the mediterranean from the atlantic ocean thus it is one of the main shipping areas in the world approximately 110 000 vessel movements are recorder every year resulting in a very congested area complicating safe navigation through its waters j occupation of areas at the entrance to the bay of algeciras as commented earlier the strait is one of the main navigational areas so for this reason the ports located in the area have enhanced their provisioning and bunkering services as is the case with the ports of algeciras and gibraltar therefore the ships which arrive in the bay for provisioning often use the outer limits of the area to carry out these operations these boats are regarded as off limits the gathering of these boats which sit at the entrance to the bay impede the safe transit in this area to a large extent k competition between ferries although there is no accident report which refers to this matter the experts believe that this does occur in the area and is detrimental to safe navigation commercial pressure to meet service timetable schedules can sometimes mean that ferries from different companies compete during their voyages to see who can arrive first at port for mooring request pilotage etc in the light of the foregoing some risk control measures could be advised as follow regarding to main identified hazards related with human factors such as fatigue overconfidence and lack lookout of oow the ropax ships owners operators are recommended to review the operational procedures of the safety management system to establish clear instructions that promote safety of navigation and the environment and through appropriate supervision ensure that the operational procedures are complied with in particular the following should be addressed 1 maintaining a proper lookout at all times 2 the provision of effective bridge resource management and bridge team 3 revision of the rest hours of the crew members regarding to hazard related with technician factor such as the communication problems ship to shore and ship to ship this research suggests establishing formal links communications between the two port authorities gibraltar and algeciras to ensure a full exchange of traffic information in addition on the bases of the risk analysis results it is observe that collision accidents are the most common accident involving ropax ships also the frequencies of occurrence s collisions in the sog are greater than frequencies of previous studies two types of incident frequencies were calculated frequency per ship year and frequency per movements year however the authors consider more appropriate to perform a calculation of frequencies by ship movements whenever possible since the risk of an accident will be greater the more the ship moves finally risk model for collisions estimates that the individual risk levels for both crew and passenger are inside to the alarp region it would be desirable to reduce that risk as long as feasible 7 conclusions in this paper we presented the results of a risk analysis study for ropax ships in the strait of gibraltar hazard identification has been carried out using a combination of two standard techniques historical accident data analysis and expert judgement both performed as the first step in the study also a high level model risk for collisions was established through of an even tree to determinate the safety level of ropax ships in the area according to the results from the hazid it is concluded that failure to report ship movements by the port of gibraltar and fatigue among crew members are the hazards which are identified as high risk indices therefore it is recommended that specific studies be carried out to reduce their incidence in this respect potential risk control measures have been proposed such as a to establish formal links communications between the two port authorities gibraltar and algeciras to ensure a full exchange of traffic information b to review the operational procedures of the safety management system to establish clear instructions that promote safety of navigation and the environment in addition the present study illustrates current levels of safety in terms of accidental frequency for the total fleet in general and ropax fleet in particular the frequencies have been calculated per ship year and per ship movements year in this respect the authors consider more appropriate to perform a calculation of frequencies by using ship movements for a specific area whenever possible from the results we find that collisions are the most common accident scenarios affecting ropax ships also the frequencies of collisions occurrence in the sog are greater than frequencies calculated in previous studies especially if for the same period the world ropax fleet is compared with the ropax fleet in the sog finally risk model for collisions estimates that the individual risk levels for both crew and passenger are inside to the alarp region on this basis a future research in continuation of this study would be performed to promote risk control options and to reduce the safety levels of ropax ships in the area acknowledgements the authors are indebted to the anonymous reviewers and editor whose comments improved the presentation and the content of an earlier version of the manuscript 
23602,the very nature of a complex system does not allow that a single person be able to master all required competences for its development or operation therefore specialization team work and coordination are required to achieve the desired goals in order to assure positive synergy between every participant system engineering concepts must be taken into account like development methods product decomposition functional classification design patterns interface and compatibility assessment configuration management technological plans technical standards integration policies system commissioning and validation these systems engineering concepts are shown in the presented work through the development of a fully functional auv system and control architecture the control system and its components are properly described and compared against other state of the art architectures it is also shown that it was possible to sustain the project development tasks among many successive group or generation of students demonstrating the benefits of the proposed engineering methods the proposed system was tested in field tests of the auv during an oceanographic mission keywords auv can control architecture systems engineering underwater vehicles 1 introduction research institutions working with complex systems typically have developed informal rules to allow parallel work of many researchers and assure positive synergy between co workers those rules are an essential part of the intellectual capital of the institution yet they are typically neglected in the academic world which focus on the physical phenomena themselves therefore new departments or new institutions may face research continuity problems regardless of the academic excellence of the participants because they do not know how to organize the institutional work to achieve wider and permanent goals similar problems are faced by other institutions developing complex systems like nasa which published its systems engineering handbook nasa 2007 this publication provides general definitions best practices guidelines and alternative approaches in product development especially for long lasting life cycles with many intermediary milestones in parallel the software engineering institute cmmi 2010 proposes a framework of tools to improve product development and predictability of results for questions related to safety the generic iec 61508 standard also discuss some important concepts that should be considered a number of works have presented control architectures for mobile robots in general that influenced also the auv embedded system design in many cases the architecture main characteristic is rooted on some artificial intelligence paradigm such as the deliberative approach nilsson 1969 bowen et al 1990 reactive or behaviour based control brooks 1986 kumar and stover 2000 bellinghamet al 1994 and other approaches biologically inspired arkin 1990 more recently hybrid approaches combining deliberative and reactive control applications are becoming usual in the robotics community gat 1998 brutzman et al 1998 valavanis et al 1997 palomeras et al 2012 sheikh et al 2014 ranganathan et al 2001 goldberg 2011 müller 1996 on the other hand with the advance of computer science hardware resources software tools and frameworks the focus on organization of software with possibilities to combine the different paradigms hewitt and inman 1991 kim and yuh 2004 amianti and de barros 2008 eickstedt et al chitre 2008 has becoming dominant specially in the case of service robots inspired by state of art methods in systems engineering nasa 2007 cmmi 2010 this paper presents an auv control architecture which took into account unforeseen requirements beyond the main scientific and technical aspects such as the need to master project costs and delays the understanding and workarounds when dealing with local market constraints safety requirements and also the workforce availability and continuity systems engineering approach and software development standards are applied to develop a field robot in the university environment this auv has been applied to oceanographic missions as seen in cmmi cmmi 2010 there are three critical dimensions that organizations typically focus on people tools and equipment and procedures and methods tools are products and equipment used by the people to execute their work they can be physical spaces like rooms and workbenches equipment like workstations servers and instruments or software tools like integrated development environment ide for computer programming of course tools require qualified personnel for their proper use procedures and methods result from the academic work embracing theories and other scientific data used to predict results like newton s mechanical laws of course in the scope of a project methods need to be applied by people mastering them these three dimensions are connected altogether through a process fig 1 process is the set of rules people follow during the project most typically a process describes who does what and when cmmi cmmi 2010 states process definition as a measure of maturity for an institution this work advocates that researchers and engineers should share in their articles not only their methods and tools but also the process because of its importance going ahead it may be stated that process and methods are more relevant and potentially more permanent than the tools as the tools evolve very quickly and may be particular for some place in the world on the other hand a valid process may probably be applied everywhere and anytime in the world as the human nature is almost the same except for some particular law or culture in the last 20 years auvs have gained an important economical place they offer advantages such as the ability to act as platforms that can use survey sensors close enough to the seabed thereby obtaining high quality results free from surface and ship noises in addition they are unconstrained since they do not need to be connected to a ship borne power supply not even requiring the control of an operator for some applications they offer great increases in cost effectiveness and true force multiplication for the military and financial leverage for all sectors given the multidisciplinary nature of designing auvs and its increasing complexity this theme should be considered as a systems engineering activity even for small vehicles developed by universities there are at least problems from mechanical engineering pressure resistant vessels seals naval engineering arrangement hydrodynamics and propulsion electrical engineering actuators energy supply energy storage embedded electronics electromagnetic compatibility and interference computer engineering processor architecture memory buses telecommunication engineering telemetry and on board communications control engineering control algorithms for the autopilot and guidance system and software engineering from low level drivers to high level tasks and routines of a real time operating system this large number of expertises and competences results in a complexity too big to be handled by a single person in other words a proper auv project has many men hours accumulated and much more than the work of a single person considering the short time of an academic work on the other hand a single auv may allow research in many fields at same time attracting many contributors in an academic environment this article is presented as follows in section ii the main concepts applicable to auv development and the proposed architecture are described in section iii it is presented a comparative analysis between the proposed architecture and other similar approaches concerning main characteristics of the embedded system and its development an application to oceanographic missions using the actual auv and the proposed control architecture is presented in section iv and the experimental results are described in section v in section vi final conclusions and suggestions for the future work are presented 2 development the original version of the auv control architecture has its main characteristics already introduced by the authors in former works de barros et al 2010 de barroset al 2012 the vehicle was originally proposed as a test bed for investigations on dynamics of underwater vehicles fig 2 a and fig 2b recently it has been adapted to oceanographic missions fig 2c main guidelines of the architecture development and its characteristics are described as follows 2 1 requirements and constraints important general requirements for an auv control architecture developed in an academic environment include a simple flexible and modular hardware and software system such scheme allows many students to work in parallel and independently and can be easily adapted to other applications in small time frame by applying design patterns and code reuse the system should also be easily expansible in order to include new functionalities as needed robust enough to avoid damage or equipment loss with low cost small size and low energy demand 2 2 risk management it is proposed a topology where all system tasks could be handled by small electronic processing nodes each one dedicated to a particular function e g a node to read a sensor a node to control propulsion and so on in such scheme the implementation of each node is assigned to a student or group of students thus responsibilities are clear and the problems become smaller simpler and adequate to each student level as one may implement a complicated kalman filter and other a sensor transducer besides teamwork is also stimulated with cooperation between the students avoiding conflicts and frustration besides modularity the overall risk control were achieved also through the use of open standards design patterns and team consultation participation and follow up by teachers and specialists as discussed below 2 3 design flexibility the control architecture is a critical system whose requirements may be not accurately defined in the beginning of its design in order to reduce time effort and risks for its development reuse is a great solution but it is important to provide flexibility in the design of components so that new functions are easily added during the architecture development that flexibility has the drawback of making the solutions suboptimal in terms of cost and energy consumption but a good compromise may be found 2 4 product decomposition it is proposed a system which comprises nodes for each group of sensors and actuators drivers and low level control as long as a central node dedicated to more sophisticated tasks navigation guidance decision making all nodes are connected to a can bus a concept advocated by ortizet al 1999 as can be seen in fig 3 the overall control architecture characteristic is a combination of hierarchical and behaviour based systems for example normal functions provided by the auv like seabed mapping are going to be performed in a hierarchical way on the other hand emergency actions like going to surface when a flood is detected are provided by a behaviour based approach beyond the risk management aspects of responsibility clarification this approach also allows simplification of interfaces functional atomicity and grouping of competences demanding less of each student to achieve results 2 5 functional classification the nodes related to the safety of the auv and the can bus are developed with quality constraints to avoid the loss of the auv at sea the more sophisticated functions like navigation do not require quality control because they are constantly changing during research do not affect directly the auv safety and involve software with a big amount of lines which is not easy to develop according to quality constraints consequently these functions do not use direct communication with the can bus but use a secure rs 232 connection to a node which does fig 4 depicts this situation as can be seem in the node which implements the extended kalman filter ekf algorithm responsible for the navigation function however for nodes connected directly to the can bus general quality constraints included in a first approach adoption of misra c guidelines for software development revision of the software code by a supervisor use of the safety certified real time operating system μc osii which is free of charge for academic purposes use of protection boxes to avoid salt corrosion on the nodes as shown in fig 5 2 6 electrical interfaces for electrical interfaces each node has different connector categories ranging from connectors dedicated to power supply can bus other communication interfaces and connectors for analogue and digital i o to sensors and actuators these different categories have unique male and female plugs and receptacles polarized as long as known color codes this standardization prevents improper or reverse connections to be made that could possibly damage the electrical interfaces or even destroy the entire node electronics 2 7 verification integration and validation the first step is the development of the node functions by a student thereafter a supervisor verifies the stand alone node functionality against the desired specifications also checking the student s code conformity to software quality standards after this verification integration to other nodes can be performed this is accomplished through wire connections to other system boards on a laboratory test bed with the aid of measurement devices oscilloscopes and other debugging instruments all nodes can then communicate and operate with each other along other system electronics such as the real vehicle batteries power converters and electronics from scientific payloads the auv will be hosting during its missions upon integration and burn in tests the auv can be validated for further field testing and afterwards real missions all these integration and validation activities should be carried out with all students interacting and cooperating with each other to ease the process 2 8 emi emc assessment in complex systems electromagnetic interference emi and electromagnetic compatibility emc problems may be responsible for malfunctions from some non critical system defect erratic behaviours the complete failure of a system node or even an internal communication breakdown during the laboratory test bed integration besides locating interface problems and implementation issues it is important to detect and analyse any emi and emc issues found between the many electrical parts of the system since electronics will be installed into a confined space inside the vehicle the cable harness and wire assembly are susceptible to interferences and other effects moreover as long as the main energy backbone of the auv supplies power to each board of every subsystem changes in energy consumption electrical transients surges and faults in one board may impact the whole system operation most of these problems can be addressed with proper shielding filtering and decoupling circuits however even after careful design of the embedded electronics using standards guidelines and best practices found in the literature several interferences may arise when the whole system is put to test such as power supply voltage fluctuations resonances conducted noises irradiated radio frequency disturbances electromagnetic coupling effects grounding loops esd effects etc these phenomena are difficult to be detected and isolated and may happen only during specific operating conditions they require the expertise and participation of the whole engineering team to be solved 2 9 configuration management each time a functional version of the auv is tested in the field the software modules are saved in a folder along with the description of the hardware used and the tests performed these folders are kept under a supervisor responsibility to aid the traceability and the memory of the system evolution 2 10 adoption of standards simple guidelines are adopted for the development because students need an easy reference at first for the software development the misra c was adopted and for the hardware development the reference pcb 1999 was judged adequate also addressing emi emc issues 2 11 technological plan the first aspect was the choice of a base programming language the ansi c language was chosen due to its maturity popularity and effectiveness with embedded systems with many examples available in the internet high adopted in many universities curriculum with free and open tools and libraries available for the integrated development environment ide it was chosen the eclipse cdt framework as long as the gnuarm traditional gcc toolchain for the preprocessor compiler and linker both tools are open source mature and freely available the arm micro controller family was chosen for implementing the processing unities because it has low power consumption low cost reasonable processing power wide use in automotive industry and it is available in the local market it is a compact and flexible technology which may be applied to simple functions such as sensors and drivers as long as more complex functions like control and guidance as well for more computation intensive functions if any an additional node with a superior arm architecture micro controller may be used at first the node boards were implemented into the standard of half credit card size with a 64 pin header bus allowing the stacking of many boards it was decided to keep a small portfolio of boards which may be stacked accordingly to the current need in a concept similar to commercial programmable logic computers as seen in fig 6 for instance it was developed one board with power supply network and processor one board with sd card memory and two rs 232 ports one board with five analogue inputs and one analogue output and one board with four pwm outputs after some refinements the latest version abandoned the stacking of half credit card sized boards towards two generic models of boards providing all the interfaces needed for every node this two board models can be seem in fig 7 one of the boards has connections for rs 232 can analogue inputs and gpio pins that can also be used for pwm the other model is slightly larger and includes also spi and i2c connectors one analogue output a buzzer for audible signalling of errors and sockets for a micro sd card and a battery used to maintain a real time clock each board use an arm micro controller oscillators a jtag connector leds to indicate system status and errors as long as dc dc converters required for the power supply although the flexibility of the system has been diminished it was possible to reduce the volume occupied by the auv electronics to around half of its initial space even with two more nodes included for interfacing new equipment for the underlying communication network it was adopted a can bus due to the availability of transceivers and controllers in the local market mcp2515 and mcp2551 can bus is also cheap compact has low power consumption and it is a well established and still in development technology voss 2005 can bus is less powerful in terms of bandwidth than an ethernet network however when aspects like compactness predictability energy consumption simplicity reliability latency and cabling are taken into account can bus becomes more interesting for the auv ortizet al 1999 sibenacet al 2002 wanget al 2005 zhanget al 2008 however it is not excluded for future versions the possibility of an ethernet network for information exchange between payload computers requiring greater bandwidth like side scan sonar realizing simultaneous navigation and mapping these functions may be developed without software quality control and they shall communicate with the safety related functions of the control architecture by a standardized data link 2 12 software architecture the nodes related to the safety of the auv and the can bus are developed according to constraints that include the adoption of misra c standards for software development code revision and the real time μc osii operational system the embedded hardware and software system is composed by nodes directly related to each group of sensors and actuators drivers and low level control and a central node for more sophisticated tasks related to mission management all connected to a can bus fig 8 the embedded software can be described by grouping the control units according to abstraction levels and functionality fig 9 shows the latest version of the software model at the lowest abstraction level there are peripheral controller routines they are responsible for the hardware abstraction layer which controls the microprocessor peripherals through configuration registers it is where generic routines of resource usage and configuration are located such as pwm generation digital to analogue conversion serial port configuration etc the second layer includes the embedded real time operating system rtos i e the μc osii where task scheduling and resource management are provided the third layer is responsible for driving the auv sensors and actuators it is where sensors are automatically initialized and configured as soon as the system is turned on in this layer data acquisition digital signal processing and measurements are performed for every external signal and reference values are synthesized or transferred to electric motor drivers and actuators the fourth layer establishes communication among nodes and allows the access of each running process to the other system information that is implemented through a shared memory where the nodes access the necessary information for their tasks this layer takes advantage of message oriented characteristics of the can bus protocol in order to keep a common database for all nodes with lower bandwidth demands if compared to a peer to peer protocol in this way it is possible to distribute evenly the high level tasks of the fifth layer between the nodes and to easily add new functionalities as needed in the fifth layer there are the high level functions like guidance control mission management user communication etc an example of high level software is the autonomous object shown at fig 10 in a petri net which is the routine that is responsible to make pre programmed manoeuvres like turning or zigzag while keeping a constant depth the software structure has practically remained the same along the years few components have been included and few have their position changed in the software structure fig 8 a number of sensors have been added and tested in the vehicle and the software has been developed following the proposed guidelines of safety and modularity the only significant modification made recently in the software was the inclusion of a system initialization task which is responsible for concentrating the initialization of all peripheral and modules of each node 3 comparison with other architectures this section presents a brief comparison of the architecture of this work with others based in similar concepts such as the back seat eickstedt et al and the dsaav architectures chitre 2008 the first one is inspired by systems and software engineering concepts together with robustness standards the back seat and dssav take advantage of standardization of interfaces and flexible software topology to reduce development time and effort when these proposed engineering methods were first applied to auv design other important development frameworks were undergoing research in the academic area the massachusetts institute of technology mission orientated operating suite moos and the stanford university robotic operating system ros today both systems are important milestones for robotics by providing a stable platform independent communication aware programming environment for vehicles and robots this section also analyses the use of those frameworks and other middleware based solutions although the current overall technical solution may not be considered superior to the aforementioned studies this work focused on the process which allowed the development of a complex engineering solution by a team of students and supervisors and not individuals assuring long term continuity and support 3 1 people qualification universities are an ambient where most of the man power is based on undergraduate students or engineers in the m sc ph d course most of this personnel does not have software development skills beyond the engineering basics back on 2008 the former approach used to develop the pirajuba auv control architecture was directed to establish robust real time software amianti and de barros 2008 at this time most coding efforts were applied in dedicated and commercially available hardware platforms however this approach was much more demanding for the programmer the previous software development had been oriented according to the following guidelines development process certification norms operating system tools and mathematical tools the development process adopted the harmony approach douglass 2007 which combines systems engineering with software engineering the norm adopted was the do 178b rtc which was originally created for avionics but has been applied to the certification process in other industries highrely the qnx neutrino which has the do 178b certification was adopted as the real time operating system and qnx momentics as the development environment the c which is certified by the do 178b norm was employed as the programming language the auml rt for the specification method and the sysml as a graphical modelling language for analysing designing and verifying the software control architecture the application tools also included the telelogic raphsody which creates executable systems from uml models this later tool was responsible for implementing sysml auml rt do 178b and integrate them into qnx neutrino and momentics using common off the shelf microcomputers in that case the programmer in order to master those tools and start to work in the auv architecture needed at least one year of studies and practices in those software engineering concepts and programming tools at that time the hardware components based on the pc104 platform and ethernet communication were treated as black boxes every hardware support or adaptation were difficult and required other external expertises and costs in order to reduce the overall software development efforts tools must not require expensive or time consuming training the decision to apply popular programming languages like ansi c reduced the project dependence on software engineering professional for the code upgrade and maintenance however every new development should maintain the structured programming policies and object oriented concepts once used the use of a small size operating system with c language source code available not only is easier for the students to understand but also to realize the real role of a rtos in an complex embedded environment providing a good pedagogical tool on top of that it is necessary to train some students for safety and safety related components in the development of software in compliance with coding standards and with documentation procedures to keep a high quality work for hardware development it is proposed to students to master some pcb layout design practices which can be based on some supplier s application notes such as ortizet al 1999 and other good practices pcb 1999 the reduced number of components also reduces the time to study datasheets in order to create new modules today with the engineering methods mentioned in this article a programmer who comes with a background on c language needs only an introduction to can protocol a brief knowledge about the custom electronics and its microcontroller the study of misra c and the practice of its rules during the software development in a couple of months the student is ready to work on a specific code for the auv with easy integration with other hardware and software modules support activities also became fast and accessible since every software or hardware modules are simple and very well known by the team the engineering methods used in this project to manage the man power allowed the participation of 5 consecutive groups of students these students carried on the auv development activities for 7 years during which dozens of real tests and missions could be performed both in pool conditions or in real field experiences in open sea 3 2 development frameworks at time the present architecture was being developed moos and ros frameworks were being executed only inside 86 intel compatible architectures with linux or windows nt as their base operating system any practical solution would require a lot of memory processing power floating point arithmetic with expensive hardware and accessories like commercial pc104 motherboards communications interfaces data acquisition and synthesis boards the final solution may be not easily fitted inside a small vehicle autonomy would also be impaired since the computers had high energy consumptions and losses from our early experiences with such hardware platforms amianti and de barros 2008 it was decided to move to small distributed and energy efficient microcontroller boards however porting ros or moos to these environments would be difficult or impossible since the first microcontrollers used had around 4 kb of ram and 32 kb of rom flash thus this project resulted in the creation of its own solution a full custom robust simple and minimal development framework created entirely from scratch but inspired in these other architectures despite not being a high level programming environment that framework has proven itself easy to maintain and to receive contributions fast delivering practical applications and missions however with the advent of new microcontroller families with 32 bits lots of memory processing power floating point arithmetic and communication interfaces the project team aims to be compatible with other development frameworks the project team studies to support moos functionalities as long as support to the mentioned automatic coding generation frameworks and also to higher level languages like python finally it is important to point out that other development architectures and platforms using high level languages and tools with or without automatic code generation such as labview isagraph etas rta matlab simulink solidthinking embed or other middlewares and frameworks can produce software applications with assured safety and reliability however these solutions mostly rely on proprietary or commercial hardware somewhat expensive complex and designed for general applications with bigger sizes higher energy consumption and losses whenever possible when these solutions are ported to custom small cheap electronic boards the coding of all hardware abstraction layers are difficult and requires a deep knowledge of both the hardware and software layers when done in small teams with lower speciality and skills this porting is very time consuming and is prone to errors and bugs that directly impacts the safety of the application thus compared to these other architectures the choices made in this project lead to a better beneffits cost ratio 3 3 safety and availability despite being a powerful and popular language for embedded system c language is also dangerous since it was not developed for safety applications however since its pitfalls are very well delimited through the enforcement of coding standards misra c or jpl standards holzmann 2006 and good practices separation of normal and safety functions safe interfaces code reuse and design patterns it is possible to achieve a reasonable level of safety and reliability from a software point of view in hardware aspects the ruggedness and simplicity of components provide a good level of reliability besides the low thermal emission low operating temperature and low energy consumption along with waterproof casings minimize the wear and corrosion on pins and tracks thus reducing the rate of failure 3 4 development time once hobbyists frequently make their source code available in the internet many software resources and libraries could be adapted from other projects found worldwide like a fat file system or a can driver this speeds up the development if compared to tools which are not well known as the philosophy is to develop for reuse apart the first version which must be developed carefully all the other versions will require a small effort some tools are not very efficient for instance there is no automatic code generation so far that can generate optimized source code from high level functional block diagrams however they are easy to learn and may become a good starting point given the high changeover of people it is believed that for this particular case the development time is the smallest possible 3 5 costs the hardware costs are quite low compared to ethernet based architectures due to the use of common off the shelf components usually applied in the automotive industry arm and can chips using robust reliable and cheap components which are all available in local market and a modular architecture it is expected that damages due to human error or accidents will be restricted and will not imply in bigger delays another factor which should reduce costs in long term is the technology assessment nasa 2007 if only widely used technologies are employed the probability of product discontinuation is quite small this means less rework due to obsolescence problems or logistic expenses the costs for software development should be quite low since only free tools like eclipse cdt gnuarm gcc tool chain μc osii rtos are being used although it is not possible to say these are high productivity tools they are easily understood by undergraduate students and average engineers requiring little time to train people besides adopting a single technology and common tools to all system engineering will certainly reduce training efforts 3 6 energy consumption arm microcontrollers are very well known by their low power consumption characteristics the arm implementation used arm7 family demands around 0 4 w around two orders of magnitude less power than the one used by an intel compatible 86 architecture processor for a small battery powered auv this reduction in power means a direct improvement in autonomy and a significant reduction in the internal temperature since most power consumed by a processor is converted into heat 3 7 can versus ethernet usually the ethernet has been the most popular choice although can and rs 232 can also be found in other auv implementations however a number of authors report drawbacks when using ethernet that are related to message collision and the space necessary for hub switch and cabling especially when redundancy are considered in the system smithet al 1994 wanget al 2005 if one takes into account aspects such as cost availability volume cabling energy consumption predictability and bandwidth can frequently becomes a better choice than ethernet sibenacet al 2002 while ethernet uses higher clock frequencies 20 mhz at least can uses a frequency of only 1 mhz this allows for more robust and simple cabling 8 wires for ethernet against 2 for can besides ethernet requires a star topology where a hub which needs energy is at centre requiring more cables than a can based bus topology although ethernet enabled devices are becoming cheaper over time they are still more expensive than can devices finally the can protocol is powerful enough for the basic functions control navigation communication and basic sensing of a low to medium cost auv control architecture this analysis should be reconsidered in the near future due to the advent of new can fd bus power over ethernet poe and ethercat schemes 3 8 expandability the number of possible can nodes is virtually unlimited for a small auv supposing each node receives and sends a small number of message identifiers ids accordingly to can specification each id can have 11 bits resulting in 2048 different possibilities it is easy to upgrade to can 2 0 standard which can allow more than 4 billion different messages ids because of small need of cabling to add redundancies is not difficult there is a limit in the communication bandwidth 1 mbits s which restrain the type of sensors that can be connected in the safety network however most sensors do not need more than 8 bytes at 10 hz requiring less than 800 bits s of bandwidth usage this means that a non redundant can network could employ around one thousand sensors or actuators keeping good time response for high priority messages what cannot be easily accomplished by an ethernet bus the major drawback is the lower bandwidth availability in order to incorporate image transmission in the safety network however due to normal and safety functions separation and once image transmission and processing are normal functions it would not be incorporated in the safety data bus but in a dedicated auxiliary interface 3 9 flexibility the hardware flexibility achieved a high level than those from dsaav chitre 2008 by the use of standard modules and interfaces it is easy to assemble a new node for a new function and incorporate it in the system the software also has a high level of flexibility once medium level and below functions are not affected by the inclusion of new high level functions besides the node where a given function is performed is transparent to the others being limited only by the hardware peripherals this allows distributing the processing power among the nodes one drawback if compared to dsaav chitre 2008 is the lack of reprogramming resources with no physical access to the nodes although there is no need of opening the waterproof casing there is a need of opening the auv resistant hull however from a safety point of view the impossibility of changing a safety firmware in the field can be seen as a positive aspect because of human errors avoidance for the normal functions which may have an ethernet wireless connection the online reprogramming resources may be added in the future 4 application a field application is presented in order to show that one of the results from the developed architecture was an operational auv capable of performing pre programmed manoeuvres estimating its position through dead reckoning algorithms and acquiring relevant oceanographic data thin layers are oceanographic features comprised by aggregations of suspended particulate matter and plankton retained in dense concentrations within a small vertical thickness usually less than 3 m such structures are characterized as hot spots of biogeochemical activity in stratified oceans but there are no studies on their occurrence and persistence in coastal ecosystems of the southern hemisphere thin layers are difficult to detect using traditional sampling techniques an overview of thin layers characteristics and detection methods can be found in durham and stocker 2012 recently auvs have been applied to thin layer studies for instance it is worth mention the layered organization in coastal ocean loco experiments and the investigations carried out by the monterey bay aquarium research institute mbari in the loco experiment the remus auv was adapted to measurements of turbulence and chlorophyll a fields for 8h duration manoeuvres across isobath legs of 2 5 km in extent wang and goodman 2009 2010 the mbari experiments have used the dorado auv which includes physical chemical sensors and water samplers ryan et al 2008 zhang et al 2012 those samplers are triggered by embedded computer algorithms that can classify water types the pirajuba auv is low cost compared to the mentioned vehicles despite the fact it includes last generation sensors for evaluating physical chemical and bio optical properties relevant for thin layer detection the pirajuba auv is a free flooding type vehicle having an external hull made of fiberglass and three aluminium pressure vessels inside named as main manoeuvring and thruster vessel the main vessel carries the computer units motion sensors rate gyro compass and inclinometers depth and liquid level sensors lithium polymer batteries communication and power electronics the manoeuvring vessel includes the servo systems for moving the control surfaces and the thruster electronic driver the thruster vessel carries a 150 w dc motor which moves a polyurethane propeller made by a rapid prototyping manufacturing system de barros et al 2010 the propeller parameters were selected from the wageningen series and its geometry defined in a cad based numerical tool all the vessels structure was calculated for operating at a depth of 100 m originally the common type of shape related to the myring geometry which is present in vehicles such as remus and maya made pirajuba a useful test bed for studies on auv hydrodynamics and control fig 11 in this sense similarly to the case of joint investigations on ship manoeuvring this shape can be adopted as a reference for tests in different institutions de barroset al 2012 the original configuration of the auv has changed in order to include oceanographic sensors new modules of the hull were constructed and attached to the middle body the nose has been modified to carry a ctd conductivity temperature and depth sensors system optical sensors more dedicated to the thin layer detection have been included as well the whole set of sensors is integrated into the main module which includes the ctd set up and optical sensors module fig 12 the sensor package includes a aml metrex x ctd and wetlabs eco fl fluorometers measuring chlorophyll a cdom chromophoric dissolved organic matter and phycoerythrin in addition to an optical backscattering sensor eco bb the backscattering sensor measures scattering at 117 the angle determined as a minimum convergence point for variations in the volume scattering function vsf induced by suspended materials and water itself as a result the data provided by this sensor are less determined by the type and size of the materials in the water and is more directly correlated to the concentration of the materials in the blue ocean environments the chlorophyll a detector is a major component in the biological activity of phytoplankton one of the major classes of organisms present in the blue ocean thin layers and coastal waters the cdom sensor indicates the presence of organic matter more related to a permanent activity instead of a casual detection the fourth optical sensor included in the module is related to the presence of zooplankton the phycoerythrin fluorimeter detects the presence of the cyanobacteria additional appendixes to the hull come from an acoustic pinger a doppler velocity log dvl and radio link and gps antennas all of them constitute the final oceanographic version of the auv fig 2c based on the cruising speed during the recent sea trials the new configuration produced an increase of approximately 25 on the drag of the whole vehicle the ctd module has embedded data storage the energy however is supplied by the auv batteries that are kept in the electronics vessel from this vessel start and finish recording commands are also generated to the ctd in the oceanographic mission the vehicle should perform pre programmed manoeuvres fig 13 in ubatuba sao paulo south east of brazil some transversal paths to the coast were executed by the auv oscillating in the dive plane between the surface and a distance close to the sea bed the auv speed is set so that the sensors can capture the water oceanographic properties at the required resolution for the thin layer study in the next phase of the project a special structure attached to the auv will carry a holographic camera for the detection of microorganisms 5 results preliminary results validate the control architecture effectiveness for carrying out the previously planned manoeuvres and acquiring the oceanographic data physical and biological data from the sea water was acquired by the pirajuba auv in the inner coast of ubatuba fig 13 during manoeuvres that last approximately 2 5 h each each transect was executed in the horizontal plane by a heading autopilot according to a pre programmed time of 30 40 min in each leg fig 14 a in the diving plane the yo yo path was produced by a pitch control manoeuvre pitch angle set point oscillated between 17 and 17 as the auv reached depths of 2 m and 12 m respectively fig 14b the pitch angle references were defined based on a maximum speed of 1 75 m s which corresponds to the vertical speed of 0 5 m s since the time response of the sensors is 0 25 s detection of layers of minimum thickness around 0 5 m is assured the heading pitch and speed controls were accomplished by simple pid controllers in the horizontal plane the vehicle navigated a total path of 10456 m and 2617 m in the vertical plane during the same manoeuvre data from the attitude and heading reference system ahrs was saved together with the doppler velocity log dvl data in order to estimate the auv trajectory a dead reckoning localization system based on the fusion of dvl and ahrs data was tested the quality of position estimates could be evaluated by comparison to gps measurements at the turning points when the auv was detected visually by the research ship fig 15 the maximum discrepancy between gps measurement and the estimated location was 6 of a travelled length for the lawnmower trajectory in the horizontal plane figs 16 and 17 present samples of the oceanographic data the auv profiles indicated the occurrence of both vertical stratification and horizontal gradients in thermohaline properties fig 16 lower temperatures were depicted close to the bottom and especially on the northern end of the transects probably as a result of slightly cold water protruding from the boqueirao channel which is deeper 35 m than surrounding waters salinity followed a similar trend chlorophyll a distribution fig 17 clearly responded to the environmental setting as higher concentrations were found where temperature and salinity gradients were highest cdom spatial variability was less defined but also showed a similar trend as chlorophyll a profiles 6 conclusions given the multidisciplinary character of auvs the contributions of many people with different skills to a single project must be added up to improve efficiency this works emphasizes on process as a solution to tie up different domains and people in order to achieve results as a team given local aspects and ever changing technologies researchers could obtain better results if they pay more attention to process than to technological aspects in order to keep a good synergy between the participants some management aspects must be taken into account these are the availability of components the availability of skills training time versus personnel change problem configuration and interface management there are also reflexes on the employed architecture the formal separation between service functions constraints and solution becomes a must to avoid legacy problems modularity becomes very important to allow the contributors to work in their field of expertise care must be taken to allow reuse and to promote coding standards and documentation finally the functional separation of safety routines improves the reliability reduces costs and eases reuse it is expected to achieve a stable and continuous development without dependence on particular individuals and with the lowest requirements on training as possible this is not usually a goal for many auvs once enterprises can dispose of people for a long time so tools with more training time but with higher productivity are preferred on the other hand many auvs developed in universities have a short life cycle being used only as a tool to explore an academic field but even for universities there is a gain if some management techniques are applied to assure a positive synergy between the many actors the feedback from field tests results shall proceed in order to improve the control architecture functionality robustness and its performance in oceanographic applications acknowledgement this work has been supported by fapesp under the grant 2013 16669 7 and capes under the grant aux 2001 2014 cimar appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j oceaneng 2018 01 016 
23602,the very nature of a complex system does not allow that a single person be able to master all required competences for its development or operation therefore specialization team work and coordination are required to achieve the desired goals in order to assure positive synergy between every participant system engineering concepts must be taken into account like development methods product decomposition functional classification design patterns interface and compatibility assessment configuration management technological plans technical standards integration policies system commissioning and validation these systems engineering concepts are shown in the presented work through the development of a fully functional auv system and control architecture the control system and its components are properly described and compared against other state of the art architectures it is also shown that it was possible to sustain the project development tasks among many successive group or generation of students demonstrating the benefits of the proposed engineering methods the proposed system was tested in field tests of the auv during an oceanographic mission keywords auv can control architecture systems engineering underwater vehicles 1 introduction research institutions working with complex systems typically have developed informal rules to allow parallel work of many researchers and assure positive synergy between co workers those rules are an essential part of the intellectual capital of the institution yet they are typically neglected in the academic world which focus on the physical phenomena themselves therefore new departments or new institutions may face research continuity problems regardless of the academic excellence of the participants because they do not know how to organize the institutional work to achieve wider and permanent goals similar problems are faced by other institutions developing complex systems like nasa which published its systems engineering handbook nasa 2007 this publication provides general definitions best practices guidelines and alternative approaches in product development especially for long lasting life cycles with many intermediary milestones in parallel the software engineering institute cmmi 2010 proposes a framework of tools to improve product development and predictability of results for questions related to safety the generic iec 61508 standard also discuss some important concepts that should be considered a number of works have presented control architectures for mobile robots in general that influenced also the auv embedded system design in many cases the architecture main characteristic is rooted on some artificial intelligence paradigm such as the deliberative approach nilsson 1969 bowen et al 1990 reactive or behaviour based control brooks 1986 kumar and stover 2000 bellinghamet al 1994 and other approaches biologically inspired arkin 1990 more recently hybrid approaches combining deliberative and reactive control applications are becoming usual in the robotics community gat 1998 brutzman et al 1998 valavanis et al 1997 palomeras et al 2012 sheikh et al 2014 ranganathan et al 2001 goldberg 2011 müller 1996 on the other hand with the advance of computer science hardware resources software tools and frameworks the focus on organization of software with possibilities to combine the different paradigms hewitt and inman 1991 kim and yuh 2004 amianti and de barros 2008 eickstedt et al chitre 2008 has becoming dominant specially in the case of service robots inspired by state of art methods in systems engineering nasa 2007 cmmi 2010 this paper presents an auv control architecture which took into account unforeseen requirements beyond the main scientific and technical aspects such as the need to master project costs and delays the understanding and workarounds when dealing with local market constraints safety requirements and also the workforce availability and continuity systems engineering approach and software development standards are applied to develop a field robot in the university environment this auv has been applied to oceanographic missions as seen in cmmi cmmi 2010 there are three critical dimensions that organizations typically focus on people tools and equipment and procedures and methods tools are products and equipment used by the people to execute their work they can be physical spaces like rooms and workbenches equipment like workstations servers and instruments or software tools like integrated development environment ide for computer programming of course tools require qualified personnel for their proper use procedures and methods result from the academic work embracing theories and other scientific data used to predict results like newton s mechanical laws of course in the scope of a project methods need to be applied by people mastering them these three dimensions are connected altogether through a process fig 1 process is the set of rules people follow during the project most typically a process describes who does what and when cmmi cmmi 2010 states process definition as a measure of maturity for an institution this work advocates that researchers and engineers should share in their articles not only their methods and tools but also the process because of its importance going ahead it may be stated that process and methods are more relevant and potentially more permanent than the tools as the tools evolve very quickly and may be particular for some place in the world on the other hand a valid process may probably be applied everywhere and anytime in the world as the human nature is almost the same except for some particular law or culture in the last 20 years auvs have gained an important economical place they offer advantages such as the ability to act as platforms that can use survey sensors close enough to the seabed thereby obtaining high quality results free from surface and ship noises in addition they are unconstrained since they do not need to be connected to a ship borne power supply not even requiring the control of an operator for some applications they offer great increases in cost effectiveness and true force multiplication for the military and financial leverage for all sectors given the multidisciplinary nature of designing auvs and its increasing complexity this theme should be considered as a systems engineering activity even for small vehicles developed by universities there are at least problems from mechanical engineering pressure resistant vessels seals naval engineering arrangement hydrodynamics and propulsion electrical engineering actuators energy supply energy storage embedded electronics electromagnetic compatibility and interference computer engineering processor architecture memory buses telecommunication engineering telemetry and on board communications control engineering control algorithms for the autopilot and guidance system and software engineering from low level drivers to high level tasks and routines of a real time operating system this large number of expertises and competences results in a complexity too big to be handled by a single person in other words a proper auv project has many men hours accumulated and much more than the work of a single person considering the short time of an academic work on the other hand a single auv may allow research in many fields at same time attracting many contributors in an academic environment this article is presented as follows in section ii the main concepts applicable to auv development and the proposed architecture are described in section iii it is presented a comparative analysis between the proposed architecture and other similar approaches concerning main characteristics of the embedded system and its development an application to oceanographic missions using the actual auv and the proposed control architecture is presented in section iv and the experimental results are described in section v in section vi final conclusions and suggestions for the future work are presented 2 development the original version of the auv control architecture has its main characteristics already introduced by the authors in former works de barros et al 2010 de barroset al 2012 the vehicle was originally proposed as a test bed for investigations on dynamics of underwater vehicles fig 2 a and fig 2b recently it has been adapted to oceanographic missions fig 2c main guidelines of the architecture development and its characteristics are described as follows 2 1 requirements and constraints important general requirements for an auv control architecture developed in an academic environment include a simple flexible and modular hardware and software system such scheme allows many students to work in parallel and independently and can be easily adapted to other applications in small time frame by applying design patterns and code reuse the system should also be easily expansible in order to include new functionalities as needed robust enough to avoid damage or equipment loss with low cost small size and low energy demand 2 2 risk management it is proposed a topology where all system tasks could be handled by small electronic processing nodes each one dedicated to a particular function e g a node to read a sensor a node to control propulsion and so on in such scheme the implementation of each node is assigned to a student or group of students thus responsibilities are clear and the problems become smaller simpler and adequate to each student level as one may implement a complicated kalman filter and other a sensor transducer besides teamwork is also stimulated with cooperation between the students avoiding conflicts and frustration besides modularity the overall risk control were achieved also through the use of open standards design patterns and team consultation participation and follow up by teachers and specialists as discussed below 2 3 design flexibility the control architecture is a critical system whose requirements may be not accurately defined in the beginning of its design in order to reduce time effort and risks for its development reuse is a great solution but it is important to provide flexibility in the design of components so that new functions are easily added during the architecture development that flexibility has the drawback of making the solutions suboptimal in terms of cost and energy consumption but a good compromise may be found 2 4 product decomposition it is proposed a system which comprises nodes for each group of sensors and actuators drivers and low level control as long as a central node dedicated to more sophisticated tasks navigation guidance decision making all nodes are connected to a can bus a concept advocated by ortizet al 1999 as can be seen in fig 3 the overall control architecture characteristic is a combination of hierarchical and behaviour based systems for example normal functions provided by the auv like seabed mapping are going to be performed in a hierarchical way on the other hand emergency actions like going to surface when a flood is detected are provided by a behaviour based approach beyond the risk management aspects of responsibility clarification this approach also allows simplification of interfaces functional atomicity and grouping of competences demanding less of each student to achieve results 2 5 functional classification the nodes related to the safety of the auv and the can bus are developed with quality constraints to avoid the loss of the auv at sea the more sophisticated functions like navigation do not require quality control because they are constantly changing during research do not affect directly the auv safety and involve software with a big amount of lines which is not easy to develop according to quality constraints consequently these functions do not use direct communication with the can bus but use a secure rs 232 connection to a node which does fig 4 depicts this situation as can be seem in the node which implements the extended kalman filter ekf algorithm responsible for the navigation function however for nodes connected directly to the can bus general quality constraints included in a first approach adoption of misra c guidelines for software development revision of the software code by a supervisor use of the safety certified real time operating system μc osii which is free of charge for academic purposes use of protection boxes to avoid salt corrosion on the nodes as shown in fig 5 2 6 electrical interfaces for electrical interfaces each node has different connector categories ranging from connectors dedicated to power supply can bus other communication interfaces and connectors for analogue and digital i o to sensors and actuators these different categories have unique male and female plugs and receptacles polarized as long as known color codes this standardization prevents improper or reverse connections to be made that could possibly damage the electrical interfaces or even destroy the entire node electronics 2 7 verification integration and validation the first step is the development of the node functions by a student thereafter a supervisor verifies the stand alone node functionality against the desired specifications also checking the student s code conformity to software quality standards after this verification integration to other nodes can be performed this is accomplished through wire connections to other system boards on a laboratory test bed with the aid of measurement devices oscilloscopes and other debugging instruments all nodes can then communicate and operate with each other along other system electronics such as the real vehicle batteries power converters and electronics from scientific payloads the auv will be hosting during its missions upon integration and burn in tests the auv can be validated for further field testing and afterwards real missions all these integration and validation activities should be carried out with all students interacting and cooperating with each other to ease the process 2 8 emi emc assessment in complex systems electromagnetic interference emi and electromagnetic compatibility emc problems may be responsible for malfunctions from some non critical system defect erratic behaviours the complete failure of a system node or even an internal communication breakdown during the laboratory test bed integration besides locating interface problems and implementation issues it is important to detect and analyse any emi and emc issues found between the many electrical parts of the system since electronics will be installed into a confined space inside the vehicle the cable harness and wire assembly are susceptible to interferences and other effects moreover as long as the main energy backbone of the auv supplies power to each board of every subsystem changes in energy consumption electrical transients surges and faults in one board may impact the whole system operation most of these problems can be addressed with proper shielding filtering and decoupling circuits however even after careful design of the embedded electronics using standards guidelines and best practices found in the literature several interferences may arise when the whole system is put to test such as power supply voltage fluctuations resonances conducted noises irradiated radio frequency disturbances electromagnetic coupling effects grounding loops esd effects etc these phenomena are difficult to be detected and isolated and may happen only during specific operating conditions they require the expertise and participation of the whole engineering team to be solved 2 9 configuration management each time a functional version of the auv is tested in the field the software modules are saved in a folder along with the description of the hardware used and the tests performed these folders are kept under a supervisor responsibility to aid the traceability and the memory of the system evolution 2 10 adoption of standards simple guidelines are adopted for the development because students need an easy reference at first for the software development the misra c was adopted and for the hardware development the reference pcb 1999 was judged adequate also addressing emi emc issues 2 11 technological plan the first aspect was the choice of a base programming language the ansi c language was chosen due to its maturity popularity and effectiveness with embedded systems with many examples available in the internet high adopted in many universities curriculum with free and open tools and libraries available for the integrated development environment ide it was chosen the eclipse cdt framework as long as the gnuarm traditional gcc toolchain for the preprocessor compiler and linker both tools are open source mature and freely available the arm micro controller family was chosen for implementing the processing unities because it has low power consumption low cost reasonable processing power wide use in automotive industry and it is available in the local market it is a compact and flexible technology which may be applied to simple functions such as sensors and drivers as long as more complex functions like control and guidance as well for more computation intensive functions if any an additional node with a superior arm architecture micro controller may be used at first the node boards were implemented into the standard of half credit card size with a 64 pin header bus allowing the stacking of many boards it was decided to keep a small portfolio of boards which may be stacked accordingly to the current need in a concept similar to commercial programmable logic computers as seen in fig 6 for instance it was developed one board with power supply network and processor one board with sd card memory and two rs 232 ports one board with five analogue inputs and one analogue output and one board with four pwm outputs after some refinements the latest version abandoned the stacking of half credit card sized boards towards two generic models of boards providing all the interfaces needed for every node this two board models can be seem in fig 7 one of the boards has connections for rs 232 can analogue inputs and gpio pins that can also be used for pwm the other model is slightly larger and includes also spi and i2c connectors one analogue output a buzzer for audible signalling of errors and sockets for a micro sd card and a battery used to maintain a real time clock each board use an arm micro controller oscillators a jtag connector leds to indicate system status and errors as long as dc dc converters required for the power supply although the flexibility of the system has been diminished it was possible to reduce the volume occupied by the auv electronics to around half of its initial space even with two more nodes included for interfacing new equipment for the underlying communication network it was adopted a can bus due to the availability of transceivers and controllers in the local market mcp2515 and mcp2551 can bus is also cheap compact has low power consumption and it is a well established and still in development technology voss 2005 can bus is less powerful in terms of bandwidth than an ethernet network however when aspects like compactness predictability energy consumption simplicity reliability latency and cabling are taken into account can bus becomes more interesting for the auv ortizet al 1999 sibenacet al 2002 wanget al 2005 zhanget al 2008 however it is not excluded for future versions the possibility of an ethernet network for information exchange between payload computers requiring greater bandwidth like side scan sonar realizing simultaneous navigation and mapping these functions may be developed without software quality control and they shall communicate with the safety related functions of the control architecture by a standardized data link 2 12 software architecture the nodes related to the safety of the auv and the can bus are developed according to constraints that include the adoption of misra c standards for software development code revision and the real time μc osii operational system the embedded hardware and software system is composed by nodes directly related to each group of sensors and actuators drivers and low level control and a central node for more sophisticated tasks related to mission management all connected to a can bus fig 8 the embedded software can be described by grouping the control units according to abstraction levels and functionality fig 9 shows the latest version of the software model at the lowest abstraction level there are peripheral controller routines they are responsible for the hardware abstraction layer which controls the microprocessor peripherals through configuration registers it is where generic routines of resource usage and configuration are located such as pwm generation digital to analogue conversion serial port configuration etc the second layer includes the embedded real time operating system rtos i e the μc osii where task scheduling and resource management are provided the third layer is responsible for driving the auv sensors and actuators it is where sensors are automatically initialized and configured as soon as the system is turned on in this layer data acquisition digital signal processing and measurements are performed for every external signal and reference values are synthesized or transferred to electric motor drivers and actuators the fourth layer establishes communication among nodes and allows the access of each running process to the other system information that is implemented through a shared memory where the nodes access the necessary information for their tasks this layer takes advantage of message oriented characteristics of the can bus protocol in order to keep a common database for all nodes with lower bandwidth demands if compared to a peer to peer protocol in this way it is possible to distribute evenly the high level tasks of the fifth layer between the nodes and to easily add new functionalities as needed in the fifth layer there are the high level functions like guidance control mission management user communication etc an example of high level software is the autonomous object shown at fig 10 in a petri net which is the routine that is responsible to make pre programmed manoeuvres like turning or zigzag while keeping a constant depth the software structure has practically remained the same along the years few components have been included and few have their position changed in the software structure fig 8 a number of sensors have been added and tested in the vehicle and the software has been developed following the proposed guidelines of safety and modularity the only significant modification made recently in the software was the inclusion of a system initialization task which is responsible for concentrating the initialization of all peripheral and modules of each node 3 comparison with other architectures this section presents a brief comparison of the architecture of this work with others based in similar concepts such as the back seat eickstedt et al and the dsaav architectures chitre 2008 the first one is inspired by systems and software engineering concepts together with robustness standards the back seat and dssav take advantage of standardization of interfaces and flexible software topology to reduce development time and effort when these proposed engineering methods were first applied to auv design other important development frameworks were undergoing research in the academic area the massachusetts institute of technology mission orientated operating suite moos and the stanford university robotic operating system ros today both systems are important milestones for robotics by providing a stable platform independent communication aware programming environment for vehicles and robots this section also analyses the use of those frameworks and other middleware based solutions although the current overall technical solution may not be considered superior to the aforementioned studies this work focused on the process which allowed the development of a complex engineering solution by a team of students and supervisors and not individuals assuring long term continuity and support 3 1 people qualification universities are an ambient where most of the man power is based on undergraduate students or engineers in the m sc ph d course most of this personnel does not have software development skills beyond the engineering basics back on 2008 the former approach used to develop the pirajuba auv control architecture was directed to establish robust real time software amianti and de barros 2008 at this time most coding efforts were applied in dedicated and commercially available hardware platforms however this approach was much more demanding for the programmer the previous software development had been oriented according to the following guidelines development process certification norms operating system tools and mathematical tools the development process adopted the harmony approach douglass 2007 which combines systems engineering with software engineering the norm adopted was the do 178b rtc which was originally created for avionics but has been applied to the certification process in other industries highrely the qnx neutrino which has the do 178b certification was adopted as the real time operating system and qnx momentics as the development environment the c which is certified by the do 178b norm was employed as the programming language the auml rt for the specification method and the sysml as a graphical modelling language for analysing designing and verifying the software control architecture the application tools also included the telelogic raphsody which creates executable systems from uml models this later tool was responsible for implementing sysml auml rt do 178b and integrate them into qnx neutrino and momentics using common off the shelf microcomputers in that case the programmer in order to master those tools and start to work in the auv architecture needed at least one year of studies and practices in those software engineering concepts and programming tools at that time the hardware components based on the pc104 platform and ethernet communication were treated as black boxes every hardware support or adaptation were difficult and required other external expertises and costs in order to reduce the overall software development efforts tools must not require expensive or time consuming training the decision to apply popular programming languages like ansi c reduced the project dependence on software engineering professional for the code upgrade and maintenance however every new development should maintain the structured programming policies and object oriented concepts once used the use of a small size operating system with c language source code available not only is easier for the students to understand but also to realize the real role of a rtos in an complex embedded environment providing a good pedagogical tool on top of that it is necessary to train some students for safety and safety related components in the development of software in compliance with coding standards and with documentation procedures to keep a high quality work for hardware development it is proposed to students to master some pcb layout design practices which can be based on some supplier s application notes such as ortizet al 1999 and other good practices pcb 1999 the reduced number of components also reduces the time to study datasheets in order to create new modules today with the engineering methods mentioned in this article a programmer who comes with a background on c language needs only an introduction to can protocol a brief knowledge about the custom electronics and its microcontroller the study of misra c and the practice of its rules during the software development in a couple of months the student is ready to work on a specific code for the auv with easy integration with other hardware and software modules support activities also became fast and accessible since every software or hardware modules are simple and very well known by the team the engineering methods used in this project to manage the man power allowed the participation of 5 consecutive groups of students these students carried on the auv development activities for 7 years during which dozens of real tests and missions could be performed both in pool conditions or in real field experiences in open sea 3 2 development frameworks at time the present architecture was being developed moos and ros frameworks were being executed only inside 86 intel compatible architectures with linux or windows nt as their base operating system any practical solution would require a lot of memory processing power floating point arithmetic with expensive hardware and accessories like commercial pc104 motherboards communications interfaces data acquisition and synthesis boards the final solution may be not easily fitted inside a small vehicle autonomy would also be impaired since the computers had high energy consumptions and losses from our early experiences with such hardware platforms amianti and de barros 2008 it was decided to move to small distributed and energy efficient microcontroller boards however porting ros or moos to these environments would be difficult or impossible since the first microcontrollers used had around 4 kb of ram and 32 kb of rom flash thus this project resulted in the creation of its own solution a full custom robust simple and minimal development framework created entirely from scratch but inspired in these other architectures despite not being a high level programming environment that framework has proven itself easy to maintain and to receive contributions fast delivering practical applications and missions however with the advent of new microcontroller families with 32 bits lots of memory processing power floating point arithmetic and communication interfaces the project team aims to be compatible with other development frameworks the project team studies to support moos functionalities as long as support to the mentioned automatic coding generation frameworks and also to higher level languages like python finally it is important to point out that other development architectures and platforms using high level languages and tools with or without automatic code generation such as labview isagraph etas rta matlab simulink solidthinking embed or other middlewares and frameworks can produce software applications with assured safety and reliability however these solutions mostly rely on proprietary or commercial hardware somewhat expensive complex and designed for general applications with bigger sizes higher energy consumption and losses whenever possible when these solutions are ported to custom small cheap electronic boards the coding of all hardware abstraction layers are difficult and requires a deep knowledge of both the hardware and software layers when done in small teams with lower speciality and skills this porting is very time consuming and is prone to errors and bugs that directly impacts the safety of the application thus compared to these other architectures the choices made in this project lead to a better beneffits cost ratio 3 3 safety and availability despite being a powerful and popular language for embedded system c language is also dangerous since it was not developed for safety applications however since its pitfalls are very well delimited through the enforcement of coding standards misra c or jpl standards holzmann 2006 and good practices separation of normal and safety functions safe interfaces code reuse and design patterns it is possible to achieve a reasonable level of safety and reliability from a software point of view in hardware aspects the ruggedness and simplicity of components provide a good level of reliability besides the low thermal emission low operating temperature and low energy consumption along with waterproof casings minimize the wear and corrosion on pins and tracks thus reducing the rate of failure 3 4 development time once hobbyists frequently make their source code available in the internet many software resources and libraries could be adapted from other projects found worldwide like a fat file system or a can driver this speeds up the development if compared to tools which are not well known as the philosophy is to develop for reuse apart the first version which must be developed carefully all the other versions will require a small effort some tools are not very efficient for instance there is no automatic code generation so far that can generate optimized source code from high level functional block diagrams however they are easy to learn and may become a good starting point given the high changeover of people it is believed that for this particular case the development time is the smallest possible 3 5 costs the hardware costs are quite low compared to ethernet based architectures due to the use of common off the shelf components usually applied in the automotive industry arm and can chips using robust reliable and cheap components which are all available in local market and a modular architecture it is expected that damages due to human error or accidents will be restricted and will not imply in bigger delays another factor which should reduce costs in long term is the technology assessment nasa 2007 if only widely used technologies are employed the probability of product discontinuation is quite small this means less rework due to obsolescence problems or logistic expenses the costs for software development should be quite low since only free tools like eclipse cdt gnuarm gcc tool chain μc osii rtos are being used although it is not possible to say these are high productivity tools they are easily understood by undergraduate students and average engineers requiring little time to train people besides adopting a single technology and common tools to all system engineering will certainly reduce training efforts 3 6 energy consumption arm microcontrollers are very well known by their low power consumption characteristics the arm implementation used arm7 family demands around 0 4 w around two orders of magnitude less power than the one used by an intel compatible 86 architecture processor for a small battery powered auv this reduction in power means a direct improvement in autonomy and a significant reduction in the internal temperature since most power consumed by a processor is converted into heat 3 7 can versus ethernet usually the ethernet has been the most popular choice although can and rs 232 can also be found in other auv implementations however a number of authors report drawbacks when using ethernet that are related to message collision and the space necessary for hub switch and cabling especially when redundancy are considered in the system smithet al 1994 wanget al 2005 if one takes into account aspects such as cost availability volume cabling energy consumption predictability and bandwidth can frequently becomes a better choice than ethernet sibenacet al 2002 while ethernet uses higher clock frequencies 20 mhz at least can uses a frequency of only 1 mhz this allows for more robust and simple cabling 8 wires for ethernet against 2 for can besides ethernet requires a star topology where a hub which needs energy is at centre requiring more cables than a can based bus topology although ethernet enabled devices are becoming cheaper over time they are still more expensive than can devices finally the can protocol is powerful enough for the basic functions control navigation communication and basic sensing of a low to medium cost auv control architecture this analysis should be reconsidered in the near future due to the advent of new can fd bus power over ethernet poe and ethercat schemes 3 8 expandability the number of possible can nodes is virtually unlimited for a small auv supposing each node receives and sends a small number of message identifiers ids accordingly to can specification each id can have 11 bits resulting in 2048 different possibilities it is easy to upgrade to can 2 0 standard which can allow more than 4 billion different messages ids because of small need of cabling to add redundancies is not difficult there is a limit in the communication bandwidth 1 mbits s which restrain the type of sensors that can be connected in the safety network however most sensors do not need more than 8 bytes at 10 hz requiring less than 800 bits s of bandwidth usage this means that a non redundant can network could employ around one thousand sensors or actuators keeping good time response for high priority messages what cannot be easily accomplished by an ethernet bus the major drawback is the lower bandwidth availability in order to incorporate image transmission in the safety network however due to normal and safety functions separation and once image transmission and processing are normal functions it would not be incorporated in the safety data bus but in a dedicated auxiliary interface 3 9 flexibility the hardware flexibility achieved a high level than those from dsaav chitre 2008 by the use of standard modules and interfaces it is easy to assemble a new node for a new function and incorporate it in the system the software also has a high level of flexibility once medium level and below functions are not affected by the inclusion of new high level functions besides the node where a given function is performed is transparent to the others being limited only by the hardware peripherals this allows distributing the processing power among the nodes one drawback if compared to dsaav chitre 2008 is the lack of reprogramming resources with no physical access to the nodes although there is no need of opening the waterproof casing there is a need of opening the auv resistant hull however from a safety point of view the impossibility of changing a safety firmware in the field can be seen as a positive aspect because of human errors avoidance for the normal functions which may have an ethernet wireless connection the online reprogramming resources may be added in the future 4 application a field application is presented in order to show that one of the results from the developed architecture was an operational auv capable of performing pre programmed manoeuvres estimating its position through dead reckoning algorithms and acquiring relevant oceanographic data thin layers are oceanographic features comprised by aggregations of suspended particulate matter and plankton retained in dense concentrations within a small vertical thickness usually less than 3 m such structures are characterized as hot spots of biogeochemical activity in stratified oceans but there are no studies on their occurrence and persistence in coastal ecosystems of the southern hemisphere thin layers are difficult to detect using traditional sampling techniques an overview of thin layers characteristics and detection methods can be found in durham and stocker 2012 recently auvs have been applied to thin layer studies for instance it is worth mention the layered organization in coastal ocean loco experiments and the investigations carried out by the monterey bay aquarium research institute mbari in the loco experiment the remus auv was adapted to measurements of turbulence and chlorophyll a fields for 8h duration manoeuvres across isobath legs of 2 5 km in extent wang and goodman 2009 2010 the mbari experiments have used the dorado auv which includes physical chemical sensors and water samplers ryan et al 2008 zhang et al 2012 those samplers are triggered by embedded computer algorithms that can classify water types the pirajuba auv is low cost compared to the mentioned vehicles despite the fact it includes last generation sensors for evaluating physical chemical and bio optical properties relevant for thin layer detection the pirajuba auv is a free flooding type vehicle having an external hull made of fiberglass and three aluminium pressure vessels inside named as main manoeuvring and thruster vessel the main vessel carries the computer units motion sensors rate gyro compass and inclinometers depth and liquid level sensors lithium polymer batteries communication and power electronics the manoeuvring vessel includes the servo systems for moving the control surfaces and the thruster electronic driver the thruster vessel carries a 150 w dc motor which moves a polyurethane propeller made by a rapid prototyping manufacturing system de barros et al 2010 the propeller parameters were selected from the wageningen series and its geometry defined in a cad based numerical tool all the vessels structure was calculated for operating at a depth of 100 m originally the common type of shape related to the myring geometry which is present in vehicles such as remus and maya made pirajuba a useful test bed for studies on auv hydrodynamics and control fig 11 in this sense similarly to the case of joint investigations on ship manoeuvring this shape can be adopted as a reference for tests in different institutions de barroset al 2012 the original configuration of the auv has changed in order to include oceanographic sensors new modules of the hull were constructed and attached to the middle body the nose has been modified to carry a ctd conductivity temperature and depth sensors system optical sensors more dedicated to the thin layer detection have been included as well the whole set of sensors is integrated into the main module which includes the ctd set up and optical sensors module fig 12 the sensor package includes a aml metrex x ctd and wetlabs eco fl fluorometers measuring chlorophyll a cdom chromophoric dissolved organic matter and phycoerythrin in addition to an optical backscattering sensor eco bb the backscattering sensor measures scattering at 117 the angle determined as a minimum convergence point for variations in the volume scattering function vsf induced by suspended materials and water itself as a result the data provided by this sensor are less determined by the type and size of the materials in the water and is more directly correlated to the concentration of the materials in the blue ocean environments the chlorophyll a detector is a major component in the biological activity of phytoplankton one of the major classes of organisms present in the blue ocean thin layers and coastal waters the cdom sensor indicates the presence of organic matter more related to a permanent activity instead of a casual detection the fourth optical sensor included in the module is related to the presence of zooplankton the phycoerythrin fluorimeter detects the presence of the cyanobacteria additional appendixes to the hull come from an acoustic pinger a doppler velocity log dvl and radio link and gps antennas all of them constitute the final oceanographic version of the auv fig 2c based on the cruising speed during the recent sea trials the new configuration produced an increase of approximately 25 on the drag of the whole vehicle the ctd module has embedded data storage the energy however is supplied by the auv batteries that are kept in the electronics vessel from this vessel start and finish recording commands are also generated to the ctd in the oceanographic mission the vehicle should perform pre programmed manoeuvres fig 13 in ubatuba sao paulo south east of brazil some transversal paths to the coast were executed by the auv oscillating in the dive plane between the surface and a distance close to the sea bed the auv speed is set so that the sensors can capture the water oceanographic properties at the required resolution for the thin layer study in the next phase of the project a special structure attached to the auv will carry a holographic camera for the detection of microorganisms 5 results preliminary results validate the control architecture effectiveness for carrying out the previously planned manoeuvres and acquiring the oceanographic data physical and biological data from the sea water was acquired by the pirajuba auv in the inner coast of ubatuba fig 13 during manoeuvres that last approximately 2 5 h each each transect was executed in the horizontal plane by a heading autopilot according to a pre programmed time of 30 40 min in each leg fig 14 a in the diving plane the yo yo path was produced by a pitch control manoeuvre pitch angle set point oscillated between 17 and 17 as the auv reached depths of 2 m and 12 m respectively fig 14b the pitch angle references were defined based on a maximum speed of 1 75 m s which corresponds to the vertical speed of 0 5 m s since the time response of the sensors is 0 25 s detection of layers of minimum thickness around 0 5 m is assured the heading pitch and speed controls were accomplished by simple pid controllers in the horizontal plane the vehicle navigated a total path of 10456 m and 2617 m in the vertical plane during the same manoeuvre data from the attitude and heading reference system ahrs was saved together with the doppler velocity log dvl data in order to estimate the auv trajectory a dead reckoning localization system based on the fusion of dvl and ahrs data was tested the quality of position estimates could be evaluated by comparison to gps measurements at the turning points when the auv was detected visually by the research ship fig 15 the maximum discrepancy between gps measurement and the estimated location was 6 of a travelled length for the lawnmower trajectory in the horizontal plane figs 16 and 17 present samples of the oceanographic data the auv profiles indicated the occurrence of both vertical stratification and horizontal gradients in thermohaline properties fig 16 lower temperatures were depicted close to the bottom and especially on the northern end of the transects probably as a result of slightly cold water protruding from the boqueirao channel which is deeper 35 m than surrounding waters salinity followed a similar trend chlorophyll a distribution fig 17 clearly responded to the environmental setting as higher concentrations were found where temperature and salinity gradients were highest cdom spatial variability was less defined but also showed a similar trend as chlorophyll a profiles 6 conclusions given the multidisciplinary character of auvs the contributions of many people with different skills to a single project must be added up to improve efficiency this works emphasizes on process as a solution to tie up different domains and people in order to achieve results as a team given local aspects and ever changing technologies researchers could obtain better results if they pay more attention to process than to technological aspects in order to keep a good synergy between the participants some management aspects must be taken into account these are the availability of components the availability of skills training time versus personnel change problem configuration and interface management there are also reflexes on the employed architecture the formal separation between service functions constraints and solution becomes a must to avoid legacy problems modularity becomes very important to allow the contributors to work in their field of expertise care must be taken to allow reuse and to promote coding standards and documentation finally the functional separation of safety routines improves the reliability reduces costs and eases reuse it is expected to achieve a stable and continuous development without dependence on particular individuals and with the lowest requirements on training as possible this is not usually a goal for many auvs once enterprises can dispose of people for a long time so tools with more training time but with higher productivity are preferred on the other hand many auvs developed in universities have a short life cycle being used only as a tool to explore an academic field but even for universities there is a gain if some management techniques are applied to assure a positive synergy between the many actors the feedback from field tests results shall proceed in order to improve the control architecture functionality robustness and its performance in oceanographic applications acknowledgement this work has been supported by fapesp under the grant 2013 16669 7 and capes under the grant aux 2001 2014 cimar appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j oceaneng 2018 01 016 
23603,this work aims at using oil transport and weathering models available in literature and compare them numerically by developing algorithms of lagrangian particles for oil spilled on the ocean surface this is the first step in the development of a new model for oil spills in the brazilian atlantic ocean comparing it with two reference models developed by the national oceanic and atmospheric administration noaa and distributed by the environmental protection agency epa the results in this work were subject to statistical tests which showed good compatibility between the new and reference models the code was tested on the southeast coast of brazil under different meteorological and oceanic conditions the modeling results of the physical and chemical transformations related to the oil weathering presented very similar results even for completely different formulations similar results were also obtained for the transport and trajectory of the oil slick whereas slick size varied greatly according to different turbulence parameters assumed for the water oil air interface layer all the models analyzed presented very similar results for all analyzed effects being considered reliable to describe the oil slick trajectory and weathering keywords fate model weathering offshore oil spill 1 introduction the oil and gas consumption recorded over the last decades resulted in the increase in extraction refining and transport of fossil fuels lardner and zodiatis 2017 alves et al 2014 2015 2016 and consequently there is a higher risk of environmental accidents in maritime areas milani et al 2000 efficient performance for the spilled oil removal depends on the knowledge of the slick size formed to dimension the required containment and on the ability to predict the spilled oil behavior in the short and long term huang 1983 for this the transport and oil degradation models can be used in both the preventive and corrective aspects as they allow simulating hypothetical situations and from these hypotheses enable action plans to face possible future accidents preventive action or simulate real time conditions of an accident in progress and therefore trigger the most effective countermeasures to contain the slick corrective action ferreira et al 2003 state that modeling has become an important tood in defining an area likely to be target of an accidental offshore oil spill the spilled offshore oil transportation and destination are mostly managed in a short period by processes of transport and physical chemical transformation and in a long period by processes of biological degradation according to the local oceanic and atmospheric conditions the physicochemical changes in the oil start when the lighter fractions of the spill are evaporated small oil droplets disperse in the water column entrainment of the oil in the water below the slick emulsions are formed entrainment of marine water in the oil being stabilized in the presence of asphaltenes and inorganic components are dissolved shen et al 1987 the models of oil spill transport and degradation by lagrangean particle propose to numerically represent the physical chemical effects acting on a continuous oil slick using a large number of discrete particles alves et al 2015 2016 in contrast our study aims to test and evaluate recent models available in literature such as those by wang wang et al 2005 zadeh zadeh and hejazi 2012 and stringari stringari et al 2013 wang and zadeh based their work on similar premises in weathering but describe the slick movement differently while stringari represents the weathering differently from the other two the algorithm results were compared and validated with gnome noaa 2012 and adios2 noaa 1994 in their most current and revised versions lehr et al 2002 thus the objective of this work is to develop algorithms based on three models already consolidated and available for use in literature and compare the results enabling in this way to detect the main points and as well as opportunities to upgrade our new oil slicks drift and weathering model this paper presents the first step for a new brazilian oil spilled model called stfm spill transport and fate model focusing the six main processes of oil spill development and deterioration on the ocean surface advection mechanical gravitational spreading turbulent diffusion evaporation emulsification and dispersion table 1 1 1 advection advection the transport of the oil slick by wind waves and sea currents is essential to describe the oil slick spread on the ocean surface determining all the areas affected by the spill from its origin to its final destination since this process is dependent on wind current and fluctuation capacity given the oil density it can occur on both the sea surface and the sea layers beneath the surface huang 1983 1 2 mechanical spreading spreading basically the first environmental effect of the oil slick and its formation is one of the most important process correctly describing the development of oil spills it consists of a balance of forces between the gravity which causes the slick to spread makes it thinner and increases the area and the viscosity which acts to keep it as cohesive as possible the adequate knowledge of the oil slick spread is essential to control the extension of the contaminated area as well as due to its influence on the rate of weathering processes such as evaporation dissolution photo oxidation and biodegradation huang 1983 1 3 turbulent diffusion turbulent diffusion is the mass transport within the oil slick through random and chaotic movements arising from the shear internal movements to the fluid itself when moving in turbulent flow in the models the spatial and temporal resolution processes smaller than the scales used are parameterized as turbulent diffusion and resolved from equations of random principle wang et al 2005 a coefficient of diffusivity zadeh and hejazi 2012 is the most common approach to describe both the turbulent diffusion and also to represent the turbulence in these models 1 4 evaporation evaporation the major process in the oil weathering affects mainly the lighter components being also the main responsible for the natural oil spilled removal on the ocean surface the oil evaporation rate is assumed as a kinetic function of first order obtained from the spill area wind speed vapor pressure and temperature huang 1983 the analytical methods for oil evaporation have been the most used in the oil slick spill and formation models although considering several simplifications reed et al 1999 1 5 emulsification the emulsification described in this study is the gradual inflow of seawater droplets into the spilled oil forming a new stable system the emulsion is a mixture between two immiscible liquids in which one of them in this case seawater penetrates in the form of small droplets inside the other in this case the oil spilled forming a stable mixture fingas and fieldhouse 2004 as the chemical composition varies greatly not all oils emulsify and still several crude oils only form emulsions after a certain period of evaporation in all cases once the emulsification is started the process follows practically uninterrupted until the saturation limit of the slick being the content of waxes resins and asphaltenes determinants for the slick emulsification lehr et al 2002 1 6 sinking oil sinking represents not only an important part of the oil spill fate but also a process which provokes significant pollution on coastlines this process starts with increased specific gravity of the oil after weathering followed by moving downward and adherence of the smallest drops to particulate matter with densities higher than the water and finally sinks huang 1983 in a first stage heavy compounds whose densities are higher than those of the sea water sink to the inside water column which usually happens due to the adhesion of particles or organic matter from the sea water to the oil slick the oil particulate aggregate increases the velocity of oil heavy compounds downwards until they are settled as part of the ocean bottom li et al 2016 twenty years after the exxon valdez spill the population of sea otters in the region still presents sequels and chronic effects due to the existence of residual oil fractions on the seabed monson et al 2011 important studies on the sinking oil effects enabled for the first time ever bathymetric oceanographic geological and anthropogenic data to produce hazard maps for the eastern mediterranean sea lardner and zodiatis 2017 alves et al 2014 2015 2016 the results of these studies will be tested on the oil spill that occurred in salamina island athens coast in september 2017 as the region will be suffering the effects of this spill for the years to come 2 material and methods we applied the proposed and already established models stringari et al 2013 wang et al 2005 zadeh and hejazi 2012 writing algorithms to test the equations comparing the results with those obtained by gnome noaa 2012 and adios2 noaa 1994 in their most updated versions lehr et al 2002 in order to understand the level of agreement between the results statistical tests were carried out according to the methodology of performance evaluation of models chang and hanna 2004 the limits of acceptance of the results presented here are more restrictive than those proposed by chang and hanna 2004 as they are consolidated models from the same assumptions 2 1 computational model of reference for the study of trajectory gnome general noaa operational modeling environment is a modeling tool developed by the office of response and restoration s or r emergency response division of noaa and used to predict possible trajectories of oil slicks from spills noaa 2012 gnome is a particle model that allows some customizations for the simulation of spilled oil with a proprietary database and because of its known and widely tested formulation was taken as reference for this study 2 2 computational model of reference for the study of weathering adios automated data inquiry for oil spills is an oil degradation model developed by noaa as a response tool for assessing the behavior of different oil types exposed to weathering on the sea surface including physicochemical transformations lehr et al 2002 adios consists basically of a database with more than one thousand different types of crude oil and refined products being also noaa s standard tool as it holds one of the most complete oil databases available for oil weathering study 3 results and discussion 3 1 model testing hypothetical spill of 500 bbl on august 1 2016 the stfm trajectory modeling analysis was performed by comparing the results between gnome noaa 2012 and the algorithm used in the methodology proposed by wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 input data shown on table 2 the coastline the hydrodynamic fields of hycom and gfs atmospheric data were obtained from goods zelenke et al 2012 gnome noaa 2012 uses the nearest neighbor interpolation in space having a linear relationship in function of the time the other simulated models used the same interpolation for reasons of compatibility and comparison an interesting strategy to evaluate models of this type is to start the study by analyzing the dispersion of the oil slick as particles in the area in order to determine their dispersion numerical behavior through the simulation the models based on fay type spreading fay 1971 have an elliptical treatment for oil slicks wang et al 2005 zadeh and hejazi 2012 whereas the models that use spreading coefficients produce an isotropic treatment of the slick growth stringari et al 2013 which must be weighted by the wind to obtain the preferred direction of the spreading in this aspect the slick growth as proposed by stringari stringari et al 2013 is less parameterized and more consistent with slick evolution reality while the reference model gnome noaa 2012 applies similar description to represent this effect fig 1 the spreading distributes the particles according to a preferred axis accompanying the wind direction and a secondary axis perpendicular to the first defining an ellipse the diffusion distributes these particles isotropically according to local turbulence applying a random variable in the particles distribution along the area determined by the spreading equations and then again applying a random repositioning within the limits determined by the diffusion equations resulting the particles final position within the oil slick in the area fig 1 the combined effect of these two equations spreading and diffusion with their specific areas elliptic and isotropic and their respective random levels both in the particle distance and direction results in a single probability distribution for each proposed algorithm fig 2 the spreading is the predominant effect before diffusion in the four models tested in the initial stage of the slick growth fig 1 it results in a more uniform distribution of the particles along the preferred axis of wind direction and a more concentrated distribution of the particles along the perpendicular axis fig 2 the diffusion within the numerical scheme repositions these particles and distributes them more evenly throughout the whole oil slick the random functions implemented in the numerical schemes within the spreading and diffusion equations of the tested models do not distribute the particles either properly or evenly throughout the oil slick being the gaussian distributions visible in all models tested fig 2 this undesirable result of the random effect ends up interfering in the calculation of the oil slick thickness due to the existence of particle accumulation in certain areas creating thickness gradients within the slick through numerical products and not because of the effects physically observed in the oil slick the analysis of the slick trajectories due to the advection fig 3 was made from the comparison between the simulations performed with gnome model noaa 2012 and algorithms developed from wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 the four models showed very similar trajectories and impacted areas fig 3 all in more linear hydrodynamic field conditions spill started on 44 w and in more turbulent hydrodynamic field locations spill started on 42 w and 46 w even for different formulations table 1 the algorithms for reading and interpolating hydrodynamic data developed for these models have different modes of execution nearest neighbor linear interpolation bilinear interpolation inverse square however gnome noaa 2012 can only be executed with the nearest neighbor interpolation some tests were performed by using models with different data interpolation methods being the results significantly different which made comparisons of results impossible the nearest neighbor method used by gnome noaa 2012 is undoubtedly the least suitable among the available methods as it boosts up the small differences among the formulations of the models and the algorithms developed especially in situations of marked convergence in the hydrodynamic field the cumulative effect of the abrupt discontinuity of the hydrodynamic field due to the nearest neighbor ends up affecting the trajectory the determination of the thickness and the area of the oil slick statistical tests on the temporal evolution of the oil slick area table 3 by spreading and diffusion were inconclusive since they presented results within the limits of acceptance for some statistical tests and outside the limits for others table 3 the results presented on table 3 show that elliptic spreading models such as those developed by fay 1971 and lehr lehr et al 1984 were not fully consistent with spreading models based on diffusive coefficient johansen 1984 and the shear stress elliot et al 1986 the review about the offshore oil dispersion models reed et al 1999 showed the poor prognosis by fay 1971 and lehr lehr et al 1984 because the estimate growth of the slick parameterized by time tends to impose a growth rate which represent unsatisfactory the direct action of environmental forces although gnome algorithm provides a spreading comparable to the other models in the distribution of the points along the wind direction its calculation of the spreading in the direction perpendicular to the wind is much inferior resulting in ellipses very elongated but with small effective area hindering the analysis of the results table 3 the high values presented by the correlation coefficient table 3 added to the observations of the time series behavior of the slick area fig 4 show that the dimensions of the slick had in a way a behavior or a temporal evolution similar to all tested models despite different dimensions this effect stems from the passage of the slick by areas of divergence in the hydrodynamic fields by which the particles are distributed more quickly than the spreading and dispersion equations this ends up being enhanced by using the nearest neighbor absence of velocity interpolation and consequent intensification of gradients although these different parameterizations might indicate the inviability to compare the applied models they meet the criteria of the statistical tests adopted for the oil slick simulation which makes our proposed model very promising the results of the statistical tests on table 4 show that the three proposed algorithms wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 presented trajectories tracked by center of mass consistent with gnome reference model noaa 2012 in all simulations statistical tests show that for all purposes the trajectories of the four simulated models can be considered as identical table 4 3 2 model testing weathering the analysis of the oil weathering effects was performed by comparing the results between adios2 lehr et al 2002 with the algorithms based from wang et al 2005 stringari et al 2013 and zadeh and hejazi 2012 the adios2 lehr et al 2002 is not a transport model simulating only weathering due to meteorological conditions it needs also to define the slick size in the area to calculate the rates of evaporation and emulsification in this aspect the adios2 lehr et al 2002 uses a parameter based on fay 1971 to determine the area of oil slick thus presenting results that are more consistent with the tested models than gnome noaa 2012 the weathering test was performed with constant hydrodynamic fields in order to adapt the conditions of the tested models and to input data table 5 the adios2 lehr et al 2002 being a short period model until five days simulations does not focus on diffusion effects but solely on the spilled oil weathering the time series of evaporation simulated by adios2 lehr et al 2002 shows a faster loss of mass in the first few hours unlike other models with lower evaporation velocity fig 5 the difference among the results shows that the equations used by wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 work better for heavier crude oils whereas the method to estimate evaporation by pseudocomponents jones 1997 used by adios2 lehr et al 2002 presents better results for both lighter and heavier oils fig 5 although heavy crude oils are characterized by few light volatile fractions it is difficult for these compounds to escape and evaporate therefore root functions of time are better parameterizations for these oils whereas logarithmic functions of time describe the evaporation of lighter crude oils the evaporation estimated by the pseudocomponents method jones 1997 despite being difficult to implement presents better results while the other parameterized methods easier to implement show inaccurate results the emulsification follow basically the formulations proposed by mackay mackay et al 1980 in all models used in this study except for a few changes in the coefficients of equations adjustment the formulation adopted by adios2 lehr et al 2002 has emulsification rates which also parameterize wave energy and interfacial area these differences in emulsification coefficients result in the most intense velocity observed in adios2 results lehr et al 2002 compared to the other algorithms tested wang et al 2005 stringari et al 2013 zadeh and hejazi 2012 this emulsification was better represented with the proposed algorithms with no need for applications of resource limit value functions fig 5a the property density viscosity surface tension etc formulation varies similarly in all models and the adjustment coefficients have few differences so the property evolution fig 5 results directly from the variations in evaporation and emulsification 4 conclusions the use of the entry environmental data fields by the nearest neighbor enabled gnome to be compatible with the other models assisting the analysis and verification of results but it is far from ideal because it produces more inaccurate results influencing the position and size of the oil slick the statistical tests were based on acceptance criteria much more restricted than the reference publication chang and hanna 2004 and all the models tested presented trajectory results within acceptance ranges even with the occurrence of discontinuities resulting from the nearest neighbor method the statistical tests results show that the spreading descriptions presented a certain level of disagreement among the models tested the reference model noaa 2012 does not have an explicit spreading formulation but only a diffusion term based in wind covariance while the tested formulations by wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 presented more distinct results the different weathering parameterizations consistent and robust can be easily used without much adjustment to the simple cases while the analytical equations wang et al 2005 zadeh and hejazi 2012 produce a good mass balance in complex cases the stringari stringari et al 2013 approaches applying mackay mackay et al 1980 equations in differential form are much more useful than the other analyzed propositions as they enable mass balance simulation per particle making it possible to simulate spills in places with intense temperature and density gradients the spreading description in the diffusive coefficient form used by stringari stringari et al 2013 responds much better to the environmental fields variations producing more interesting results in turbulent near field simulations than the calm water model based on fay 1971 and lehr lehr et al 1984 as a general evaluation all the models presented very similar results for all effects and are able to describe the oil slick trajectory and weathering in a reliable way acknowledgments the authors thank the financial support capes proex post graduation program in meteorology 
23603,this work aims at using oil transport and weathering models available in literature and compare them numerically by developing algorithms of lagrangian particles for oil spilled on the ocean surface this is the first step in the development of a new model for oil spills in the brazilian atlantic ocean comparing it with two reference models developed by the national oceanic and atmospheric administration noaa and distributed by the environmental protection agency epa the results in this work were subject to statistical tests which showed good compatibility between the new and reference models the code was tested on the southeast coast of brazil under different meteorological and oceanic conditions the modeling results of the physical and chemical transformations related to the oil weathering presented very similar results even for completely different formulations similar results were also obtained for the transport and trajectory of the oil slick whereas slick size varied greatly according to different turbulence parameters assumed for the water oil air interface layer all the models analyzed presented very similar results for all analyzed effects being considered reliable to describe the oil slick trajectory and weathering keywords fate model weathering offshore oil spill 1 introduction the oil and gas consumption recorded over the last decades resulted in the increase in extraction refining and transport of fossil fuels lardner and zodiatis 2017 alves et al 2014 2015 2016 and consequently there is a higher risk of environmental accidents in maritime areas milani et al 2000 efficient performance for the spilled oil removal depends on the knowledge of the slick size formed to dimension the required containment and on the ability to predict the spilled oil behavior in the short and long term huang 1983 for this the transport and oil degradation models can be used in both the preventive and corrective aspects as they allow simulating hypothetical situations and from these hypotheses enable action plans to face possible future accidents preventive action or simulate real time conditions of an accident in progress and therefore trigger the most effective countermeasures to contain the slick corrective action ferreira et al 2003 state that modeling has become an important tood in defining an area likely to be target of an accidental offshore oil spill the spilled offshore oil transportation and destination are mostly managed in a short period by processes of transport and physical chemical transformation and in a long period by processes of biological degradation according to the local oceanic and atmospheric conditions the physicochemical changes in the oil start when the lighter fractions of the spill are evaporated small oil droplets disperse in the water column entrainment of the oil in the water below the slick emulsions are formed entrainment of marine water in the oil being stabilized in the presence of asphaltenes and inorganic components are dissolved shen et al 1987 the models of oil spill transport and degradation by lagrangean particle propose to numerically represent the physical chemical effects acting on a continuous oil slick using a large number of discrete particles alves et al 2015 2016 in contrast our study aims to test and evaluate recent models available in literature such as those by wang wang et al 2005 zadeh zadeh and hejazi 2012 and stringari stringari et al 2013 wang and zadeh based their work on similar premises in weathering but describe the slick movement differently while stringari represents the weathering differently from the other two the algorithm results were compared and validated with gnome noaa 2012 and adios2 noaa 1994 in their most current and revised versions lehr et al 2002 thus the objective of this work is to develop algorithms based on three models already consolidated and available for use in literature and compare the results enabling in this way to detect the main points and as well as opportunities to upgrade our new oil slicks drift and weathering model this paper presents the first step for a new brazilian oil spilled model called stfm spill transport and fate model focusing the six main processes of oil spill development and deterioration on the ocean surface advection mechanical gravitational spreading turbulent diffusion evaporation emulsification and dispersion table 1 1 1 advection advection the transport of the oil slick by wind waves and sea currents is essential to describe the oil slick spread on the ocean surface determining all the areas affected by the spill from its origin to its final destination since this process is dependent on wind current and fluctuation capacity given the oil density it can occur on both the sea surface and the sea layers beneath the surface huang 1983 1 2 mechanical spreading spreading basically the first environmental effect of the oil slick and its formation is one of the most important process correctly describing the development of oil spills it consists of a balance of forces between the gravity which causes the slick to spread makes it thinner and increases the area and the viscosity which acts to keep it as cohesive as possible the adequate knowledge of the oil slick spread is essential to control the extension of the contaminated area as well as due to its influence on the rate of weathering processes such as evaporation dissolution photo oxidation and biodegradation huang 1983 1 3 turbulent diffusion turbulent diffusion is the mass transport within the oil slick through random and chaotic movements arising from the shear internal movements to the fluid itself when moving in turbulent flow in the models the spatial and temporal resolution processes smaller than the scales used are parameterized as turbulent diffusion and resolved from equations of random principle wang et al 2005 a coefficient of diffusivity zadeh and hejazi 2012 is the most common approach to describe both the turbulent diffusion and also to represent the turbulence in these models 1 4 evaporation evaporation the major process in the oil weathering affects mainly the lighter components being also the main responsible for the natural oil spilled removal on the ocean surface the oil evaporation rate is assumed as a kinetic function of first order obtained from the spill area wind speed vapor pressure and temperature huang 1983 the analytical methods for oil evaporation have been the most used in the oil slick spill and formation models although considering several simplifications reed et al 1999 1 5 emulsification the emulsification described in this study is the gradual inflow of seawater droplets into the spilled oil forming a new stable system the emulsion is a mixture between two immiscible liquids in which one of them in this case seawater penetrates in the form of small droplets inside the other in this case the oil spilled forming a stable mixture fingas and fieldhouse 2004 as the chemical composition varies greatly not all oils emulsify and still several crude oils only form emulsions after a certain period of evaporation in all cases once the emulsification is started the process follows practically uninterrupted until the saturation limit of the slick being the content of waxes resins and asphaltenes determinants for the slick emulsification lehr et al 2002 1 6 sinking oil sinking represents not only an important part of the oil spill fate but also a process which provokes significant pollution on coastlines this process starts with increased specific gravity of the oil after weathering followed by moving downward and adherence of the smallest drops to particulate matter with densities higher than the water and finally sinks huang 1983 in a first stage heavy compounds whose densities are higher than those of the sea water sink to the inside water column which usually happens due to the adhesion of particles or organic matter from the sea water to the oil slick the oil particulate aggregate increases the velocity of oil heavy compounds downwards until they are settled as part of the ocean bottom li et al 2016 twenty years after the exxon valdez spill the population of sea otters in the region still presents sequels and chronic effects due to the existence of residual oil fractions on the seabed monson et al 2011 important studies on the sinking oil effects enabled for the first time ever bathymetric oceanographic geological and anthropogenic data to produce hazard maps for the eastern mediterranean sea lardner and zodiatis 2017 alves et al 2014 2015 2016 the results of these studies will be tested on the oil spill that occurred in salamina island athens coast in september 2017 as the region will be suffering the effects of this spill for the years to come 2 material and methods we applied the proposed and already established models stringari et al 2013 wang et al 2005 zadeh and hejazi 2012 writing algorithms to test the equations comparing the results with those obtained by gnome noaa 2012 and adios2 noaa 1994 in their most updated versions lehr et al 2002 in order to understand the level of agreement between the results statistical tests were carried out according to the methodology of performance evaluation of models chang and hanna 2004 the limits of acceptance of the results presented here are more restrictive than those proposed by chang and hanna 2004 as they are consolidated models from the same assumptions 2 1 computational model of reference for the study of trajectory gnome general noaa operational modeling environment is a modeling tool developed by the office of response and restoration s or r emergency response division of noaa and used to predict possible trajectories of oil slicks from spills noaa 2012 gnome is a particle model that allows some customizations for the simulation of spilled oil with a proprietary database and because of its known and widely tested formulation was taken as reference for this study 2 2 computational model of reference for the study of weathering adios automated data inquiry for oil spills is an oil degradation model developed by noaa as a response tool for assessing the behavior of different oil types exposed to weathering on the sea surface including physicochemical transformations lehr et al 2002 adios consists basically of a database with more than one thousand different types of crude oil and refined products being also noaa s standard tool as it holds one of the most complete oil databases available for oil weathering study 3 results and discussion 3 1 model testing hypothetical spill of 500 bbl on august 1 2016 the stfm trajectory modeling analysis was performed by comparing the results between gnome noaa 2012 and the algorithm used in the methodology proposed by wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 input data shown on table 2 the coastline the hydrodynamic fields of hycom and gfs atmospheric data were obtained from goods zelenke et al 2012 gnome noaa 2012 uses the nearest neighbor interpolation in space having a linear relationship in function of the time the other simulated models used the same interpolation for reasons of compatibility and comparison an interesting strategy to evaluate models of this type is to start the study by analyzing the dispersion of the oil slick as particles in the area in order to determine their dispersion numerical behavior through the simulation the models based on fay type spreading fay 1971 have an elliptical treatment for oil slicks wang et al 2005 zadeh and hejazi 2012 whereas the models that use spreading coefficients produce an isotropic treatment of the slick growth stringari et al 2013 which must be weighted by the wind to obtain the preferred direction of the spreading in this aspect the slick growth as proposed by stringari stringari et al 2013 is less parameterized and more consistent with slick evolution reality while the reference model gnome noaa 2012 applies similar description to represent this effect fig 1 the spreading distributes the particles according to a preferred axis accompanying the wind direction and a secondary axis perpendicular to the first defining an ellipse the diffusion distributes these particles isotropically according to local turbulence applying a random variable in the particles distribution along the area determined by the spreading equations and then again applying a random repositioning within the limits determined by the diffusion equations resulting the particles final position within the oil slick in the area fig 1 the combined effect of these two equations spreading and diffusion with their specific areas elliptic and isotropic and their respective random levels both in the particle distance and direction results in a single probability distribution for each proposed algorithm fig 2 the spreading is the predominant effect before diffusion in the four models tested in the initial stage of the slick growth fig 1 it results in a more uniform distribution of the particles along the preferred axis of wind direction and a more concentrated distribution of the particles along the perpendicular axis fig 2 the diffusion within the numerical scheme repositions these particles and distributes them more evenly throughout the whole oil slick the random functions implemented in the numerical schemes within the spreading and diffusion equations of the tested models do not distribute the particles either properly or evenly throughout the oil slick being the gaussian distributions visible in all models tested fig 2 this undesirable result of the random effect ends up interfering in the calculation of the oil slick thickness due to the existence of particle accumulation in certain areas creating thickness gradients within the slick through numerical products and not because of the effects physically observed in the oil slick the analysis of the slick trajectories due to the advection fig 3 was made from the comparison between the simulations performed with gnome model noaa 2012 and algorithms developed from wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 the four models showed very similar trajectories and impacted areas fig 3 all in more linear hydrodynamic field conditions spill started on 44 w and in more turbulent hydrodynamic field locations spill started on 42 w and 46 w even for different formulations table 1 the algorithms for reading and interpolating hydrodynamic data developed for these models have different modes of execution nearest neighbor linear interpolation bilinear interpolation inverse square however gnome noaa 2012 can only be executed with the nearest neighbor interpolation some tests were performed by using models with different data interpolation methods being the results significantly different which made comparisons of results impossible the nearest neighbor method used by gnome noaa 2012 is undoubtedly the least suitable among the available methods as it boosts up the small differences among the formulations of the models and the algorithms developed especially in situations of marked convergence in the hydrodynamic field the cumulative effect of the abrupt discontinuity of the hydrodynamic field due to the nearest neighbor ends up affecting the trajectory the determination of the thickness and the area of the oil slick statistical tests on the temporal evolution of the oil slick area table 3 by spreading and diffusion were inconclusive since they presented results within the limits of acceptance for some statistical tests and outside the limits for others table 3 the results presented on table 3 show that elliptic spreading models such as those developed by fay 1971 and lehr lehr et al 1984 were not fully consistent with spreading models based on diffusive coefficient johansen 1984 and the shear stress elliot et al 1986 the review about the offshore oil dispersion models reed et al 1999 showed the poor prognosis by fay 1971 and lehr lehr et al 1984 because the estimate growth of the slick parameterized by time tends to impose a growth rate which represent unsatisfactory the direct action of environmental forces although gnome algorithm provides a spreading comparable to the other models in the distribution of the points along the wind direction its calculation of the spreading in the direction perpendicular to the wind is much inferior resulting in ellipses very elongated but with small effective area hindering the analysis of the results table 3 the high values presented by the correlation coefficient table 3 added to the observations of the time series behavior of the slick area fig 4 show that the dimensions of the slick had in a way a behavior or a temporal evolution similar to all tested models despite different dimensions this effect stems from the passage of the slick by areas of divergence in the hydrodynamic fields by which the particles are distributed more quickly than the spreading and dispersion equations this ends up being enhanced by using the nearest neighbor absence of velocity interpolation and consequent intensification of gradients although these different parameterizations might indicate the inviability to compare the applied models they meet the criteria of the statistical tests adopted for the oil slick simulation which makes our proposed model very promising the results of the statistical tests on table 4 show that the three proposed algorithms wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 presented trajectories tracked by center of mass consistent with gnome reference model noaa 2012 in all simulations statistical tests show that for all purposes the trajectories of the four simulated models can be considered as identical table 4 3 2 model testing weathering the analysis of the oil weathering effects was performed by comparing the results between adios2 lehr et al 2002 with the algorithms based from wang et al 2005 stringari et al 2013 and zadeh and hejazi 2012 the adios2 lehr et al 2002 is not a transport model simulating only weathering due to meteorological conditions it needs also to define the slick size in the area to calculate the rates of evaporation and emulsification in this aspect the adios2 lehr et al 2002 uses a parameter based on fay 1971 to determine the area of oil slick thus presenting results that are more consistent with the tested models than gnome noaa 2012 the weathering test was performed with constant hydrodynamic fields in order to adapt the conditions of the tested models and to input data table 5 the adios2 lehr et al 2002 being a short period model until five days simulations does not focus on diffusion effects but solely on the spilled oil weathering the time series of evaporation simulated by adios2 lehr et al 2002 shows a faster loss of mass in the first few hours unlike other models with lower evaporation velocity fig 5 the difference among the results shows that the equations used by wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 work better for heavier crude oils whereas the method to estimate evaporation by pseudocomponents jones 1997 used by adios2 lehr et al 2002 presents better results for both lighter and heavier oils fig 5 although heavy crude oils are characterized by few light volatile fractions it is difficult for these compounds to escape and evaporate therefore root functions of time are better parameterizations for these oils whereas logarithmic functions of time describe the evaporation of lighter crude oils the evaporation estimated by the pseudocomponents method jones 1997 despite being difficult to implement presents better results while the other parameterized methods easier to implement show inaccurate results the emulsification follow basically the formulations proposed by mackay mackay et al 1980 in all models used in this study except for a few changes in the coefficients of equations adjustment the formulation adopted by adios2 lehr et al 2002 has emulsification rates which also parameterize wave energy and interfacial area these differences in emulsification coefficients result in the most intense velocity observed in adios2 results lehr et al 2002 compared to the other algorithms tested wang et al 2005 stringari et al 2013 zadeh and hejazi 2012 this emulsification was better represented with the proposed algorithms with no need for applications of resource limit value functions fig 5a the property density viscosity surface tension etc formulation varies similarly in all models and the adjustment coefficients have few differences so the property evolution fig 5 results directly from the variations in evaporation and emulsification 4 conclusions the use of the entry environmental data fields by the nearest neighbor enabled gnome to be compatible with the other models assisting the analysis and verification of results but it is far from ideal because it produces more inaccurate results influencing the position and size of the oil slick the statistical tests were based on acceptance criteria much more restricted than the reference publication chang and hanna 2004 and all the models tested presented trajectory results within acceptance ranges even with the occurrence of discontinuities resulting from the nearest neighbor method the statistical tests results show that the spreading descriptions presented a certain level of disagreement among the models tested the reference model noaa 2012 does not have an explicit spreading formulation but only a diffusion term based in wind covariance while the tested formulations by wang wang et al 2005 stringari stringari et al 2013 and zadeh zadeh and hejazi 2012 presented more distinct results the different weathering parameterizations consistent and robust can be easily used without much adjustment to the simple cases while the analytical equations wang et al 2005 zadeh and hejazi 2012 produce a good mass balance in complex cases the stringari stringari et al 2013 approaches applying mackay mackay et al 1980 equations in differential form are much more useful than the other analyzed propositions as they enable mass balance simulation per particle making it possible to simulate spills in places with intense temperature and density gradients the spreading description in the diffusive coefficient form used by stringari stringari et al 2013 responds much better to the environmental fields variations producing more interesting results in turbulent near field simulations than the calm water model based on fay 1971 and lehr lehr et al 1984 as a general evaluation all the models presented very similar results for all effects and are able to describe the oil slick trajectory and weathering in a reliable way acknowledgments the authors thank the financial support capes proex post graduation program in meteorology 
23604,to account for the discharging flow effect in recoil analysis of a drilling riser during an emergency disconnection scenario researchers have developed two notable fluid column models i e sfm slug force model and wfcm whole fluid column model which can be solved in the pre processing of an existing global riser analysis code it is found that the sfm is developed based on the dynamical equilibrium of the internal drilling mud column whereas the wfcm is established by analyzing the whole drilling mud and refilled seawater column this study devotes efforts on the discharging flow effect on the recoil behaviour of a deep water drilling riser employing sfm and wfcm large deviations are observed when comparing the results adopting these two fluid models which can be contributed to the different resultant restoring forces exerted on the inner fluid columns nevertheless same conclusions are drawn on the discharging flow effect and the drag loading mass loss and frontal force effects have been identified an important finding is that when considering the inner fluid follows the oscillation of the riser the recoil response of the riser can be affected remarkably highlighting the great significance of this coupling effect keywords recoil analysis discharging flow effect fluid column model emergency disconnection scenario 1 introduction a deep water drilling riser is required to perform emergency disconnect of the lmrp lower marine riser package and bop blow out preventer in the event of loss of the vessel s station keeping capability either in extreme weather or through a failure of the dynamic positioning system as the stretch of a drilling riser under normal operation condition gives a significant amount of stored energy the riser is accelerated to travel upward at the moment of disconnection which has been termed riser recoil the drilling riser system is required to be capable of controlling the recoil behaviour to ensure that the lmrp is lifted sufficiently far away from the bop the recoil is slowed down to avoid crashing up in the drill deck and no local buckling occurs along the riser emergency disconnection of the tensioned riser is a much more critical event than a planned disconnection as there is no time for circulating out the drilling mud and lowering the tension researchers have devoted great efforts on the recoil analysis of a drilling riser during an emergency disconnection scenario miller and young 1985 young et al 1992 stahl et al 2004 lang et al 2009 li et al 2012 ma et al 2013 grϕnevik 2013 pestana et al 2016 as the continuous global demand for natural resources has driven offshore exploration into deeper and harsher waters the riser becomes much slender and ma et al 2013 have demonstrated the great sensitivity of water depth on the recoil performance of the riser this is the motivation to continuously devote efforts on recoil analysis in case of an emergency disconnect for optimizing the riser stack up and anti recoil control system design the drilling mud should be released following an emergency disconnect to avoid undesired axial loading from resonance in the mud column miller and young 1985 the flow continues until the mud column reaches a u tube balance or until the mud is completely evacuated from the riser when the seawater fill up valves are equipped as it is difficult to conduct experiments owing to the transient fluid structure interaction nature some outstanding models have been developed to simulate this unsteady flow which can be incorporated into the existing global riser analysis software packages for recoil analysis young et al 1992 stahl et al 2004 lang et al 2009 grytϕyr et al 2011 li et al 2012 ma et al 2013 grϕnevik 2013 lang et al 2009 have implemented a 1 d one dimensional fv finite volume discharging flow model and the calculated fluid pressure and velocity fields are used to determine drag loading and the effective tension distribution along the riser when the riser response is obtained in flexcom the results are passed back to the fluid model grytϕyr et al 2011 found that a constant velocity was giving the best possible fit to the expected velocity variation and then tried to simulate the falling mud column by a slug force model they reproduced the mass loss effect by a modified sfm slug force mode in which the slug is defined to have a length that matches the length of the riser and the velocity of the mud column is calculated by a rigid body analysis based on a differential equation although lang et al 2009 and grytϕyr et al 2011 have given a detailed literal description of the adopted simulation methodologies the fluid model formulas are not provided fortunately the sfm model is employed in grϕnevik 2013 and the fluid formula is found to be developed based on the dynamical equilibrium of the mud column based on the idea that the discharging flow induced friction force can be modeled as a user defined loads in the pre processing in riflex marintek 2009 grytϕyr et al 2011 li et al 2016 focused on modelling the internal fluid column and proposed a wfcm whole fluid column model by applying newton s second law to the whole drilling mud and refilled seawater column it is also noted that grϕnevik 2013 and li et al 2016 adopts different computation methods for the frictional forces resulting from the internal flow it is emphasized that a small negative lmrp bop clearance e g 0 10 ft can lead to the failure of recoil control of a drilling riser ma et al 2013 which means the transient response of the riser after disconnect is critical therefore some fundamental questions are essential to be figured out 1 what are the differences of the simulation results employing sfm and wfcm when utilizing the same approach for the internal flow induced frictional forces 2 how is the recoil response when accounting for the coupling effect of the internal flow and the riser structure 3 the inner flow imposed drag loading can significantly affect the recoil behaviour grϕnevik 2013 and ma et al 2013 have demonstrated the sensitivity of mud density on recoil response however the damping force and the mass loss effects remain to be distinguished these are the main motivations of this study in this paper section 2 describes the simulation models with analysis methodologies section 3 devotes efforts on the simulations of the fluid column and the recoil responses of a deep water drilling riser employing sfm and wfcm conclusions are drawn in section 4 and the suggestions in future study are also given 2 simulation model 2 1 riser model with adopted coordinates fig 1 a shows a typical drilling riser stack up in the event of an emergency disconnection the riser system consists a pipe of length l density ρ r modulus of elasticity e and mass per unit length m p the external and internal diameters of the pipe are d e and d i respectively which are relatively small compared to the pipe length the corresponding external and internal cross sectional areas are a e x and a i n and the cross area of the pipe is a a e x a i n when the drilling mud is released and exposed to the low surrounding pressure it can lead to the collapse of the riser when the seawater cannot be filled fast enough several refill valves should be placed along the length of the riser to refill the seawater fast enough which is very critical for deep water risers this study assumes that the seawater can fill the vacancy in time and the internal flow has a downward velocity of u relative to the riser structure for the internal flow the seawater has a mass of m w per unit length with density ρ w and the drilling mud has a mass of m m per unit length with density ρ m the sketch of the suspended riser with adopted coordinate systems is illustrated in fig 1 b as this study focuses on the discharging flow effect the drilling vessel is assumed to be fixed eulerian coordinate x y is adopted with the origin set at the bottom end of the vessel o 1 and x is in the direction of gravity lagrangian coordinate x is employed along the centerline in the undeformed state with the origin set at the top end of the riser b 1 x 0 denotes the clearance of the lmrp at b 2 and the bop at o 2 in the opposite direction of gravity in the following sections a prime stands for x and an overdot represents t where t is time 2 2 internal fluid column model the sfm in grϕnevik 2013 is illustrated in fig 2 a where l w and l m are the lengths of the seawater and drilling mud columns respectively m f w and f m are the total frictional forces exerted on the seawater and the mud columns n g e is the effective weight of the mud column n p a m and p b m are the pressure at the top and end of mud column respectively n m2 the frontal force f e n d is created when the falling column hits seawater at riser bottom n according to grϕnevik 2013 the dynamical equilibrium can be established between mud weight hydrostatic pressure difference frictional forces from both the drilling mud column and refilled seawater column and acceleration force as 1 m w l w m m l m a f p a m p b m a i n g e f w f m f end where a f is the acceleration of the total column system the drilling mud and refilled seawater m s2 the effective gravity force g e ρ m ρ w g a i n l m the hydrostatic pressure difference of the mud column is p a m p b m ρ m ρ w g l m f e n d is neglected the frictional forces resulting from the drilling mud and seawater columns can be obtained by 2 f w δ p w a i n f m δ p m a i n where δ p w and δ p m are the pressure drops of seawater and mud columns due to the frictional forces n m2 δ p w and δ p m can be calculated by 3 δ p w ρ w u 2 f w c l w 2 d i δ p m ρ m u 2 f m c l m 2 d i based on the darcy weisbach formula where f w c and f m c are the non dimensional frictional coefficients of the seawater and drilling mud columns respectively f w c and f m c can be computed by use of the haaland formula as 4 1 f w c 1 8 log 6 9 r ew ε 3 7 d i 1 11 5 1 f m c 1 8 log 6 9 r ew ε 3 7 d i 1 11 where ε is the roughness parameter of the riser m r ew and r em are the reynolds numbers of the seawater and drilling mud in the riser respectively the wfcm in li et al 2016 is illustrated in fig 2 b where l w l m f w and f m have the same definitions with fig 2 a p a and p b are the pressures at the top of the seawater column and the end of mud column respectively n m2 g w and g m are respectively the weights of the seawater and mud columns n different from the sfm the dynamic formulation is developed by analysing the loads and movement of the whole fluid column drilling mud and refilled seawater 6 m w l w m m l m a f p a p b a i n g w g m f w f m f e n d where the pressure difference between the top end and bottom end of the riser is p a p b ρ w g l n m2 the gravity force of the seawater column is g w ρ w g a i n l w n the gravity force of the mud column is g m ρ m g a i n l m n the frontal force is f end 1 2 ρ m a i n u 2 the friction forces of the two fluid columns f w and f m can be calculated by use of the herschel bulkley rheology model based on flow regimes neglecting the roughness of the pipe wall li et al 2016 and they are computed by use of eq 2 in the present study in order to perform a direct comparison of sfm and wfcm referring to rinaldi and païdoussis 2010 the frontal force f end depends on the configurations of the lmrp as shown in fig 3 when the mud goes straight through the lmrp as shown in fig 3 a f end 1 2 ρ m a i n u 2 based on bernoulli s equation which agrees with li et al 2016 and it can be named as water hammer effect when the straight through path of the lmrp is blocked as shown in fig 3 b the mud is discharged radially from the holes perpendicular to the pipe and f end ρ m a i n u 2 is yield based on momentum change theory which can be called nozzle effect as in rinaldi and païdoussis 2010 and li et al 2012 2 3 riser structural model standard drilling riser analyses are established through the industry recommended code api rp 16q for a drilling riser the amount of the over pull tension should be within a range to avoid any local buckling and the minimum required over pull is 100 kips at the connector however the available industry codes requires to be improved when the offshore exploration steps into deeper and harsher waters for example ma et al 2013 have concluded that industry codes such as api rp 16q are recommended to adopt water depth dependent factors for the calculation of top tension requirements since successful emergency disconnect only occurs in a narrow range of top tension for each riser stack up referring to kuiper et al 2008 it is common for a tlp tension leg platform in practice to use a pretension t 0 n that is 1 3 times higher than the submerged weight of the riser at normal operation condition as 7 t 0 1 3 w s l where submerged weight of the riser per unit length is w s m p m m g w t n with the buoyancy force w t ρ w a e x g n the stiffness of the heave compensator k s n m is tuned to compensate for the submerged weight of the top tensioned riser in case the drilling unit heaves with a given critical amplitude a c 10 m as 8 k s l w s a c the present study aims to investigate the discharging flow effect on the recoil response of a deep water drilling riser after an emergency disconnection scenario and it tries to make a comparison of the two available fluid column models i e sfm and wfcm for simplicity this study employs eq 7 to determine the tension imposed at the top end of the riser moreover the ant recoil system is not considered and the tensioner model is still modeled as a spring element by eq 8 rather than a detailed description of the tensioner the first stage of recoil analysis consists of a static study to find the riser tension distribution and the stretch along the riser by use of the finite element method the static equilibrium is obtained as 9 k 0 x f 0 where the displacements of the riser x in eulerian coordinate is the resultants of the displacement of the tensioner spring and the elastic deformation of the riser k 0 and f 0 are the stiffness matrix and external force vector respectively which are not given here for brevity in the next stage of recoil analysis the sudden release of the lmrp connector cause the riser to recoil upwards immediately owing to the low axial stiffness of the tensioner the elastic deformation of the riser is much smaller than the displacement of the tensioner spring in the recoil process and thus it is rational to assume the stretch of the riser disappears at the time of disconnect to cope with the solid fluid column the riser is formulated by a two element mass damper spring model by use of the lumped mass method as shown in fig 4 and 10 m 1 x 1 k p x 2 x 1 c 2 x 2 x 1 k s x 1 c 1 x 1 f 1 11 m 2 x 2 k p x 2 x 1 c 2 x 2 x 1 f 2 where the mass elements of the riser system are 12 m 1 m 2 m p l m w l w m m l m 2 the axial stiffness is k p e a l n m c 1 is the tensioner damping coefficient n s m c 2 is the structural damping coefficient n s m f 1 and f 2 are the resultant external force exerted on the lumped masses n which can be computed by 13 f 1 f w f m g 1 f d 1 14 f 2 f end g 2 f d 2 where the gravitational forces g 1 and g 2 are 15 g 1 g 2 m p l m w l w m m l m ρ w a e x g 2 which means the entire fluid column weight seawater and drilling mud is assumed to be evenly distributed along the riser the damping forces resulting from the surrounding seawater per unit length of the riser f d 1 and f d 2 n can be calculated by 16 f d i 1 2 ρ e d e π c t x i x i i 1 2 with the dimensionless tangential drag coefficient c t 0 015 referring to gobat and grosenbaugh 2006 by use of the standard assemblage procedure 17 m x c x k x f is obtained where m c k and f are the global mass damping stiffness and force matrices respectively 2 4 recoil analysis methodology according to grytϕyr et al 2011 when the fluid model is solved in the pre processing the velocity of the column is controlled to reproduce the mass loss effect and the damping force which can be added as uer specified time varying force in riflex as the vibration of the riser is not accounted for when solving the fluid model the acceleration of the inner flow can be calculated by a f u in sfm i e eq 1 and wfcm i e eq 6 and this simulation algorithm can be named as no feedback recoil analysis actually the velocity of the riser is v r r s t where r s t is the position vector to a point measured from the origin in eulerian coordinate and the velocity of the internal flow can be expressed as v f v p u τ where τ is the unit vector tangential to the pipe centreline referring to païdoussis 2014 the acceleration of the internal fluid column a f is 18 a f t u x 2 r s t when only the axial vibration of the riser is considered 19 a f x u x 2 u x u 2 x i is obtained which gives rise to the centrifugal force and coriolis force meng et al 2017b as the stretch of the riser disappears in the recoil process x 1 and 20 a f x u i where i is the unit vector in the x direction when a f in calculated in eq 1 and eq 6 u can be obtained by use of eq 20 once the solution of the riser model is achieved x is passed back to the fluid column model this algorithm is named as feedback recoil analysis 3 riser recoil simulations the deep water drilling riser in grϕnevik 2013 is employed here for simulations and the system parameters are listed in table 1 where ν w and ν m are the viscosities of seawater and drilling mud respectively the tensioner and structural damping forces are neglected with c 1 c 2 0 and the gravitational acceleration is assumed to be g 9 8 m s 2 simulations are conducted without considering the discharging flow effect at first time traces of the lmrp bop clearance x 0 when the riser is filled with seawater or drilling mud are shown in fig 5 the riser experiences periodic oscillations since no damping force or internal flow effect is accounted for the vibration amplitude is smaller when the riser is filled with drilling mud ascribed to the larger mass and gravity force of the riser system 3 1 no feedback recoil analysis 3 1 1 fluid column dynamics the predicted time traces of the discharging flow velocity u employing sfm and wfcm in the mud discharging and seawater refilling process are illustrated in fig 6 the simulation is stopped when the riser is fully refilled by the seawater i e l m 0 and l w l adopting the sfm with f end 0 the mud column is accelerated to the maximum velocity u max 18 3 m s at t 7 s once the friction forces are greater than the resulting force from mud weight and hydrostatic pressure the fluid column velocity is decreasing until mud discharge is finished at t 35 s it is noted that the u is nonzero when the mud discharge is finished these observations compare very well with the results in grϕnevik 2013 which has been plotted by red circles in fig 6 the total pressure drop δ p δ p w δ p m owing to the frictional forces and the time decreasing mud length l m are also compared with the results in grϕnevik 2013 and good agreements are observed which are not presented here for brevity adopting the wfcm with f end 0 the mud column is accelerated to the maximum velocity u max 12 5 m s at t 10 s then the fluid column starts to be slowed down until mud discharge is finished at t 52 s simulations are also carried out at f end 1 2 ρ m a i n u 2 and f end ρ m a i n u 2 in both two fluid models as shown in fig 6 it is found the frontal force f end can decrease the discharging velocity u and increase the discharge time when f end ρ m a i n u 2 the discharge time is increased to t 39 s in sfm and it is t 56 s in wfcm it can be concluded that sfm and wfcm can predict the same dynamic features of the fluid column that the flow velocity reaches a maximum value at first and then decrease until mud discharge is finished however large deviations have been witnessed in the time traces of u as shown in fig 6 adopting the two fluid models and inevitably remarkable differences occurs in the time histories of the pressure loss mud length et al when looking into eq 1 and eq 6 the total restoring force imposed on the inner fluid column is 2 ρ m ρ w g a i n l m in sfm and it is ρ m ρ w g a i n l m in wfcm which can explain the deviations in simulations employing the two fluid models for the frontal force same conclusions are drawn that f end has a negligible effect on the fluid column dynamics but it can increase the discharge time obviously it is stated that it is not rational to justify the two fluid models here before experiments the present simulation study can help understand the physical behaviour of the fluid column which is beneficial for experiment design as it is rather difficult in scaled experiments to accurately capture the transient fluid structure interaction dynamics in the mud discharge and seawater refilling process experiment tests will be conducted in the circulation channel of shanghai jiao tong university and a comparison will be conducted with the simulation results employing sfm and wfcm 3 1 2 riser recoil behaviour fig 7 a shows the time histories of the lmrp bop clearance x 0 when only the mass loss effect is considered i e f w f m f end 0 it is clear that the lmrp is lifted well clear of the bop in both models the predicted minimum clearance x m i n is 0 85 m at t 8 s in sfm and x m i n 0 5 m at t 8 s in wfcm compared to fig 5 the mass loss effect can accelerate the lift of the lmpr away from the bop in the mud discharge and seawater refilling process fig 7 b shows the predicted time traces of the lmrp bop clearance x 0 only considering the internal flow induced drag load the riser is assumed to be filled with seawater and therefore the mass of the internal fluid column is m i m w l the riser has experienced negative lmrp bop clearances in both sfm and wfcm which means lmrp clashing with the bop the minimum clearance x m i n 6 1 m at t 7 s in sfm and x m i n 3 m at t 7 s in wfcm it is concluded that the drag damping has an essential effect on the transient recoil behaviour of the riser fig 7 c shows the time traces of the clearance x 0 when both the internal flow induced damping drag force and mass loss effect are accounted for at f end 0 the minimum clearance x m i n 5 7 m at t 7 s in sfm while x m i n 2 7 m at t 8 s in wfcm compared to fig 7 b the x m i n is increased in both sfm and wfcm which further demonstrates that the mass loss effect can assist the lmpr to leave away from the bop however the riser still experiences negative lmrp bop clearances as the mass loss effect cannot counteract the damping force effect simulations are also carried out at f end 1 2 ρ m a i n u 2 and f end ρ m a i n u 2 and it is found that the frontal force can decrease the clearance x 0 slightly fig 7 d shows the time histories of the clearance x 0 at f end 0 and f end ρ m a i n u 2 in sfm when considering both damping drag force and mass loss effects the minimum lmrp bop clearance x m i n 5 9 m at f end ρ m a i n u 2 which is slightly smaller than the minimum x 0 at f end 0 it can be concluded that f end has an insignificant effect on the recoil response of the riser 3 2 feedback recoil analysis 3 2 1 mass loss effect when only the mass loss effect is considered i e f w f m f end 0 the predicted time traces of the discharging flow velocity u and the lmrp bop clearance x 0 employing the two fluid models in feedback recoil analysis are shown in fig 8 compared to fig 6 the time trace of u has an obvious fluctuating component the maximum velocity u max 20 m s at t 9 s in sfm while u max 14 5 m s at t 9 s in wfcm which are quite distinct from the results in no feedback recoil analysis on the other hand the predicted minimum clearance x 0 is x m i n 0 85 m at t 8 s and the discharge time is t 35 s in sfm the predicted minimum clearance x 0 is x m i n 0 5 m at t 8 s and the discharge time is t 52 s in wfcm these observations are the same as the predictions in the no feedback recoil analysis it is concluded that the coupling effect of the inner flow and the riser structure cannot affect the mass loss effect on riser recoil analysis 3 2 2 damping force effect fig 9 illustrates the simulation results when only considering the internal flow induced drag load and the riser is assumed to be filled with seawater with m i m w l the maximum velocity u max 21 m s at t 8 s in sfm while u max 15 m s at t 9 s in wfcm it is noted that the time traces of clearance x 0 differs considerably with figs 7 and 8 b the minimum lmrp bop clearance x m i n 3 2 m at t 7 s in sfm while it is x m i n 0 3 m at t 7 s in wfcm compared to fig 7 b the minimum negative clearance x 0 is increased remarkably and the transient response in the first few seconds becomes much more significant which demonstrates that the coupling effect of the inner flow and the riser structure has a crucial effect on the transient recoil response of the riser 3 2 3 combined effects of damping force and mass loss the time traces of flow velocity u and the clearance x 0 when accounting for both the damping drag force and mass loss effect when f end 0 are shown in fig 10 different from the results in the no feedback recoil analysis in fig 7 c the maximum discharging flow velocity is u max 20 m s at t 8 s in sfm while u max 14 m s at t 9 s in wfcm the minimum lmrp bop clearance x m i n 4 5 m at t 7 s in sfm while x m i n 1 5 m at t 7 s in wfcm moreover compared to fig 9 it is found that the mass loss effect can decrease the minimum x 0 in both fluid models dramatically which is opposite to the observations in no feedback recoil analysis 3 2 4 frontal force effect simulations are also performed at f end 1 2 ρ m a i n u 2 and f end ρ m a i n u 2 same as the conclusions in the no feedback recoil analysis that although the frontal force has a negligible effect on the discharging flow velocity it can increase the discharge time obviously and decrease the minimum clearance x 0 slightly which are not presented here for conciseness 4 conclusions and future study this study has demonstrated that sfm and wfcm can predict the same dynamic features of the internal discharging fluid in an emergency disconnection scenario however significant deviations have been witnessed ascribed to the different restoring force in the fluid column formulas the discharging flow induced damping drag has a critical influence on the transient response of the riser which leads to the occurrence of negative lmrp bop clearance and the mass loss effect can decrease the minimum clearance remarkably based on feedback recoil analysis although the frontal force has a negligible effect on mud discharge velocity and the recoil performance of the riser it can increase the discharge time obviously the most importantly finding is that the recoil response of the riser in feedback recoil analysis differs considerably with the result in the no feedback recoil analysis highlighting the great significance to account for the coupling effect with the riser in modelling the inner fluid column to accurately capture the transient response of the riser after an emergency disconnection a more detailed structural model of the riser system should be implemented considering the stored elastic energy and a detailed description of the tensioner lang et al 2009 moreover the hydrodynamic loads in a combination of vessel motions and the effect of disconnection time should be applied in a full 3 d analysis where the internal flow induced centrifugal force and coriolis force should be accounted for meng et al 2017a acknowledgement the authors gratefully acknowledge the supports of the national natural science foundation of china grant no 51509153 and the general financial grant from the china postdoctoral science foundation grant no 2015m570367 
23604,to account for the discharging flow effect in recoil analysis of a drilling riser during an emergency disconnection scenario researchers have developed two notable fluid column models i e sfm slug force model and wfcm whole fluid column model which can be solved in the pre processing of an existing global riser analysis code it is found that the sfm is developed based on the dynamical equilibrium of the internal drilling mud column whereas the wfcm is established by analyzing the whole drilling mud and refilled seawater column this study devotes efforts on the discharging flow effect on the recoil behaviour of a deep water drilling riser employing sfm and wfcm large deviations are observed when comparing the results adopting these two fluid models which can be contributed to the different resultant restoring forces exerted on the inner fluid columns nevertheless same conclusions are drawn on the discharging flow effect and the drag loading mass loss and frontal force effects have been identified an important finding is that when considering the inner fluid follows the oscillation of the riser the recoil response of the riser can be affected remarkably highlighting the great significance of this coupling effect keywords recoil analysis discharging flow effect fluid column model emergency disconnection scenario 1 introduction a deep water drilling riser is required to perform emergency disconnect of the lmrp lower marine riser package and bop blow out preventer in the event of loss of the vessel s station keeping capability either in extreme weather or through a failure of the dynamic positioning system as the stretch of a drilling riser under normal operation condition gives a significant amount of stored energy the riser is accelerated to travel upward at the moment of disconnection which has been termed riser recoil the drilling riser system is required to be capable of controlling the recoil behaviour to ensure that the lmrp is lifted sufficiently far away from the bop the recoil is slowed down to avoid crashing up in the drill deck and no local buckling occurs along the riser emergency disconnection of the tensioned riser is a much more critical event than a planned disconnection as there is no time for circulating out the drilling mud and lowering the tension researchers have devoted great efforts on the recoil analysis of a drilling riser during an emergency disconnection scenario miller and young 1985 young et al 1992 stahl et al 2004 lang et al 2009 li et al 2012 ma et al 2013 grϕnevik 2013 pestana et al 2016 as the continuous global demand for natural resources has driven offshore exploration into deeper and harsher waters the riser becomes much slender and ma et al 2013 have demonstrated the great sensitivity of water depth on the recoil performance of the riser this is the motivation to continuously devote efforts on recoil analysis in case of an emergency disconnect for optimizing the riser stack up and anti recoil control system design the drilling mud should be released following an emergency disconnect to avoid undesired axial loading from resonance in the mud column miller and young 1985 the flow continues until the mud column reaches a u tube balance or until the mud is completely evacuated from the riser when the seawater fill up valves are equipped as it is difficult to conduct experiments owing to the transient fluid structure interaction nature some outstanding models have been developed to simulate this unsteady flow which can be incorporated into the existing global riser analysis software packages for recoil analysis young et al 1992 stahl et al 2004 lang et al 2009 grytϕyr et al 2011 li et al 2012 ma et al 2013 grϕnevik 2013 lang et al 2009 have implemented a 1 d one dimensional fv finite volume discharging flow model and the calculated fluid pressure and velocity fields are used to determine drag loading and the effective tension distribution along the riser when the riser response is obtained in flexcom the results are passed back to the fluid model grytϕyr et al 2011 found that a constant velocity was giving the best possible fit to the expected velocity variation and then tried to simulate the falling mud column by a slug force model they reproduced the mass loss effect by a modified sfm slug force mode in which the slug is defined to have a length that matches the length of the riser and the velocity of the mud column is calculated by a rigid body analysis based on a differential equation although lang et al 2009 and grytϕyr et al 2011 have given a detailed literal description of the adopted simulation methodologies the fluid model formulas are not provided fortunately the sfm model is employed in grϕnevik 2013 and the fluid formula is found to be developed based on the dynamical equilibrium of the mud column based on the idea that the discharging flow induced friction force can be modeled as a user defined loads in the pre processing in riflex marintek 2009 grytϕyr et al 2011 li et al 2016 focused on modelling the internal fluid column and proposed a wfcm whole fluid column model by applying newton s second law to the whole drilling mud and refilled seawater column it is also noted that grϕnevik 2013 and li et al 2016 adopts different computation methods for the frictional forces resulting from the internal flow it is emphasized that a small negative lmrp bop clearance e g 0 10 ft can lead to the failure of recoil control of a drilling riser ma et al 2013 which means the transient response of the riser after disconnect is critical therefore some fundamental questions are essential to be figured out 1 what are the differences of the simulation results employing sfm and wfcm when utilizing the same approach for the internal flow induced frictional forces 2 how is the recoil response when accounting for the coupling effect of the internal flow and the riser structure 3 the inner flow imposed drag loading can significantly affect the recoil behaviour grϕnevik 2013 and ma et al 2013 have demonstrated the sensitivity of mud density on recoil response however the damping force and the mass loss effects remain to be distinguished these are the main motivations of this study in this paper section 2 describes the simulation models with analysis methodologies section 3 devotes efforts on the simulations of the fluid column and the recoil responses of a deep water drilling riser employing sfm and wfcm conclusions are drawn in section 4 and the suggestions in future study are also given 2 simulation model 2 1 riser model with adopted coordinates fig 1 a shows a typical drilling riser stack up in the event of an emergency disconnection the riser system consists a pipe of length l density ρ r modulus of elasticity e and mass per unit length m p the external and internal diameters of the pipe are d e and d i respectively which are relatively small compared to the pipe length the corresponding external and internal cross sectional areas are a e x and a i n and the cross area of the pipe is a a e x a i n when the drilling mud is released and exposed to the low surrounding pressure it can lead to the collapse of the riser when the seawater cannot be filled fast enough several refill valves should be placed along the length of the riser to refill the seawater fast enough which is very critical for deep water risers this study assumes that the seawater can fill the vacancy in time and the internal flow has a downward velocity of u relative to the riser structure for the internal flow the seawater has a mass of m w per unit length with density ρ w and the drilling mud has a mass of m m per unit length with density ρ m the sketch of the suspended riser with adopted coordinate systems is illustrated in fig 1 b as this study focuses on the discharging flow effect the drilling vessel is assumed to be fixed eulerian coordinate x y is adopted with the origin set at the bottom end of the vessel o 1 and x is in the direction of gravity lagrangian coordinate x is employed along the centerline in the undeformed state with the origin set at the top end of the riser b 1 x 0 denotes the clearance of the lmrp at b 2 and the bop at o 2 in the opposite direction of gravity in the following sections a prime stands for x and an overdot represents t where t is time 2 2 internal fluid column model the sfm in grϕnevik 2013 is illustrated in fig 2 a where l w and l m are the lengths of the seawater and drilling mud columns respectively m f w and f m are the total frictional forces exerted on the seawater and the mud columns n g e is the effective weight of the mud column n p a m and p b m are the pressure at the top and end of mud column respectively n m2 the frontal force f e n d is created when the falling column hits seawater at riser bottom n according to grϕnevik 2013 the dynamical equilibrium can be established between mud weight hydrostatic pressure difference frictional forces from both the drilling mud column and refilled seawater column and acceleration force as 1 m w l w m m l m a f p a m p b m a i n g e f w f m f end where a f is the acceleration of the total column system the drilling mud and refilled seawater m s2 the effective gravity force g e ρ m ρ w g a i n l m the hydrostatic pressure difference of the mud column is p a m p b m ρ m ρ w g l m f e n d is neglected the frictional forces resulting from the drilling mud and seawater columns can be obtained by 2 f w δ p w a i n f m δ p m a i n where δ p w and δ p m are the pressure drops of seawater and mud columns due to the frictional forces n m2 δ p w and δ p m can be calculated by 3 δ p w ρ w u 2 f w c l w 2 d i δ p m ρ m u 2 f m c l m 2 d i based on the darcy weisbach formula where f w c and f m c are the non dimensional frictional coefficients of the seawater and drilling mud columns respectively f w c and f m c can be computed by use of the haaland formula as 4 1 f w c 1 8 log 6 9 r ew ε 3 7 d i 1 11 5 1 f m c 1 8 log 6 9 r ew ε 3 7 d i 1 11 where ε is the roughness parameter of the riser m r ew and r em are the reynolds numbers of the seawater and drilling mud in the riser respectively the wfcm in li et al 2016 is illustrated in fig 2 b where l w l m f w and f m have the same definitions with fig 2 a p a and p b are the pressures at the top of the seawater column and the end of mud column respectively n m2 g w and g m are respectively the weights of the seawater and mud columns n different from the sfm the dynamic formulation is developed by analysing the loads and movement of the whole fluid column drilling mud and refilled seawater 6 m w l w m m l m a f p a p b a i n g w g m f w f m f e n d where the pressure difference between the top end and bottom end of the riser is p a p b ρ w g l n m2 the gravity force of the seawater column is g w ρ w g a i n l w n the gravity force of the mud column is g m ρ m g a i n l m n the frontal force is f end 1 2 ρ m a i n u 2 the friction forces of the two fluid columns f w and f m can be calculated by use of the herschel bulkley rheology model based on flow regimes neglecting the roughness of the pipe wall li et al 2016 and they are computed by use of eq 2 in the present study in order to perform a direct comparison of sfm and wfcm referring to rinaldi and païdoussis 2010 the frontal force f end depends on the configurations of the lmrp as shown in fig 3 when the mud goes straight through the lmrp as shown in fig 3 a f end 1 2 ρ m a i n u 2 based on bernoulli s equation which agrees with li et al 2016 and it can be named as water hammer effect when the straight through path of the lmrp is blocked as shown in fig 3 b the mud is discharged radially from the holes perpendicular to the pipe and f end ρ m a i n u 2 is yield based on momentum change theory which can be called nozzle effect as in rinaldi and païdoussis 2010 and li et al 2012 2 3 riser structural model standard drilling riser analyses are established through the industry recommended code api rp 16q for a drilling riser the amount of the over pull tension should be within a range to avoid any local buckling and the minimum required over pull is 100 kips at the connector however the available industry codes requires to be improved when the offshore exploration steps into deeper and harsher waters for example ma et al 2013 have concluded that industry codes such as api rp 16q are recommended to adopt water depth dependent factors for the calculation of top tension requirements since successful emergency disconnect only occurs in a narrow range of top tension for each riser stack up referring to kuiper et al 2008 it is common for a tlp tension leg platform in practice to use a pretension t 0 n that is 1 3 times higher than the submerged weight of the riser at normal operation condition as 7 t 0 1 3 w s l where submerged weight of the riser per unit length is w s m p m m g w t n with the buoyancy force w t ρ w a e x g n the stiffness of the heave compensator k s n m is tuned to compensate for the submerged weight of the top tensioned riser in case the drilling unit heaves with a given critical amplitude a c 10 m as 8 k s l w s a c the present study aims to investigate the discharging flow effect on the recoil response of a deep water drilling riser after an emergency disconnection scenario and it tries to make a comparison of the two available fluid column models i e sfm and wfcm for simplicity this study employs eq 7 to determine the tension imposed at the top end of the riser moreover the ant recoil system is not considered and the tensioner model is still modeled as a spring element by eq 8 rather than a detailed description of the tensioner the first stage of recoil analysis consists of a static study to find the riser tension distribution and the stretch along the riser by use of the finite element method the static equilibrium is obtained as 9 k 0 x f 0 where the displacements of the riser x in eulerian coordinate is the resultants of the displacement of the tensioner spring and the elastic deformation of the riser k 0 and f 0 are the stiffness matrix and external force vector respectively which are not given here for brevity in the next stage of recoil analysis the sudden release of the lmrp connector cause the riser to recoil upwards immediately owing to the low axial stiffness of the tensioner the elastic deformation of the riser is much smaller than the displacement of the tensioner spring in the recoil process and thus it is rational to assume the stretch of the riser disappears at the time of disconnect to cope with the solid fluid column the riser is formulated by a two element mass damper spring model by use of the lumped mass method as shown in fig 4 and 10 m 1 x 1 k p x 2 x 1 c 2 x 2 x 1 k s x 1 c 1 x 1 f 1 11 m 2 x 2 k p x 2 x 1 c 2 x 2 x 1 f 2 where the mass elements of the riser system are 12 m 1 m 2 m p l m w l w m m l m 2 the axial stiffness is k p e a l n m c 1 is the tensioner damping coefficient n s m c 2 is the structural damping coefficient n s m f 1 and f 2 are the resultant external force exerted on the lumped masses n which can be computed by 13 f 1 f w f m g 1 f d 1 14 f 2 f end g 2 f d 2 where the gravitational forces g 1 and g 2 are 15 g 1 g 2 m p l m w l w m m l m ρ w a e x g 2 which means the entire fluid column weight seawater and drilling mud is assumed to be evenly distributed along the riser the damping forces resulting from the surrounding seawater per unit length of the riser f d 1 and f d 2 n can be calculated by 16 f d i 1 2 ρ e d e π c t x i x i i 1 2 with the dimensionless tangential drag coefficient c t 0 015 referring to gobat and grosenbaugh 2006 by use of the standard assemblage procedure 17 m x c x k x f is obtained where m c k and f are the global mass damping stiffness and force matrices respectively 2 4 recoil analysis methodology according to grytϕyr et al 2011 when the fluid model is solved in the pre processing the velocity of the column is controlled to reproduce the mass loss effect and the damping force which can be added as uer specified time varying force in riflex as the vibration of the riser is not accounted for when solving the fluid model the acceleration of the inner flow can be calculated by a f u in sfm i e eq 1 and wfcm i e eq 6 and this simulation algorithm can be named as no feedback recoil analysis actually the velocity of the riser is v r r s t where r s t is the position vector to a point measured from the origin in eulerian coordinate and the velocity of the internal flow can be expressed as v f v p u τ where τ is the unit vector tangential to the pipe centreline referring to païdoussis 2014 the acceleration of the internal fluid column a f is 18 a f t u x 2 r s t when only the axial vibration of the riser is considered 19 a f x u x 2 u x u 2 x i is obtained which gives rise to the centrifugal force and coriolis force meng et al 2017b as the stretch of the riser disappears in the recoil process x 1 and 20 a f x u i where i is the unit vector in the x direction when a f in calculated in eq 1 and eq 6 u can be obtained by use of eq 20 once the solution of the riser model is achieved x is passed back to the fluid column model this algorithm is named as feedback recoil analysis 3 riser recoil simulations the deep water drilling riser in grϕnevik 2013 is employed here for simulations and the system parameters are listed in table 1 where ν w and ν m are the viscosities of seawater and drilling mud respectively the tensioner and structural damping forces are neglected with c 1 c 2 0 and the gravitational acceleration is assumed to be g 9 8 m s 2 simulations are conducted without considering the discharging flow effect at first time traces of the lmrp bop clearance x 0 when the riser is filled with seawater or drilling mud are shown in fig 5 the riser experiences periodic oscillations since no damping force or internal flow effect is accounted for the vibration amplitude is smaller when the riser is filled with drilling mud ascribed to the larger mass and gravity force of the riser system 3 1 no feedback recoil analysis 3 1 1 fluid column dynamics the predicted time traces of the discharging flow velocity u employing sfm and wfcm in the mud discharging and seawater refilling process are illustrated in fig 6 the simulation is stopped when the riser is fully refilled by the seawater i e l m 0 and l w l adopting the sfm with f end 0 the mud column is accelerated to the maximum velocity u max 18 3 m s at t 7 s once the friction forces are greater than the resulting force from mud weight and hydrostatic pressure the fluid column velocity is decreasing until mud discharge is finished at t 35 s it is noted that the u is nonzero when the mud discharge is finished these observations compare very well with the results in grϕnevik 2013 which has been plotted by red circles in fig 6 the total pressure drop δ p δ p w δ p m owing to the frictional forces and the time decreasing mud length l m are also compared with the results in grϕnevik 2013 and good agreements are observed which are not presented here for brevity adopting the wfcm with f end 0 the mud column is accelerated to the maximum velocity u max 12 5 m s at t 10 s then the fluid column starts to be slowed down until mud discharge is finished at t 52 s simulations are also carried out at f end 1 2 ρ m a i n u 2 and f end ρ m a i n u 2 in both two fluid models as shown in fig 6 it is found the frontal force f end can decrease the discharging velocity u and increase the discharge time when f end ρ m a i n u 2 the discharge time is increased to t 39 s in sfm and it is t 56 s in wfcm it can be concluded that sfm and wfcm can predict the same dynamic features of the fluid column that the flow velocity reaches a maximum value at first and then decrease until mud discharge is finished however large deviations have been witnessed in the time traces of u as shown in fig 6 adopting the two fluid models and inevitably remarkable differences occurs in the time histories of the pressure loss mud length et al when looking into eq 1 and eq 6 the total restoring force imposed on the inner fluid column is 2 ρ m ρ w g a i n l m in sfm and it is ρ m ρ w g a i n l m in wfcm which can explain the deviations in simulations employing the two fluid models for the frontal force same conclusions are drawn that f end has a negligible effect on the fluid column dynamics but it can increase the discharge time obviously it is stated that it is not rational to justify the two fluid models here before experiments the present simulation study can help understand the physical behaviour of the fluid column which is beneficial for experiment design as it is rather difficult in scaled experiments to accurately capture the transient fluid structure interaction dynamics in the mud discharge and seawater refilling process experiment tests will be conducted in the circulation channel of shanghai jiao tong university and a comparison will be conducted with the simulation results employing sfm and wfcm 3 1 2 riser recoil behaviour fig 7 a shows the time histories of the lmrp bop clearance x 0 when only the mass loss effect is considered i e f w f m f end 0 it is clear that the lmrp is lifted well clear of the bop in both models the predicted minimum clearance x m i n is 0 85 m at t 8 s in sfm and x m i n 0 5 m at t 8 s in wfcm compared to fig 5 the mass loss effect can accelerate the lift of the lmpr away from the bop in the mud discharge and seawater refilling process fig 7 b shows the predicted time traces of the lmrp bop clearance x 0 only considering the internal flow induced drag load the riser is assumed to be filled with seawater and therefore the mass of the internal fluid column is m i m w l the riser has experienced negative lmrp bop clearances in both sfm and wfcm which means lmrp clashing with the bop the minimum clearance x m i n 6 1 m at t 7 s in sfm and x m i n 3 m at t 7 s in wfcm it is concluded that the drag damping has an essential effect on the transient recoil behaviour of the riser fig 7 c shows the time traces of the clearance x 0 when both the internal flow induced damping drag force and mass loss effect are accounted for at f end 0 the minimum clearance x m i n 5 7 m at t 7 s in sfm while x m i n 2 7 m at t 8 s in wfcm compared to fig 7 b the x m i n is increased in both sfm and wfcm which further demonstrates that the mass loss effect can assist the lmpr to leave away from the bop however the riser still experiences negative lmrp bop clearances as the mass loss effect cannot counteract the damping force effect simulations are also carried out at f end 1 2 ρ m a i n u 2 and f end ρ m a i n u 2 and it is found that the frontal force can decrease the clearance x 0 slightly fig 7 d shows the time histories of the clearance x 0 at f end 0 and f end ρ m a i n u 2 in sfm when considering both damping drag force and mass loss effects the minimum lmrp bop clearance x m i n 5 9 m at f end ρ m a i n u 2 which is slightly smaller than the minimum x 0 at f end 0 it can be concluded that f end has an insignificant effect on the recoil response of the riser 3 2 feedback recoil analysis 3 2 1 mass loss effect when only the mass loss effect is considered i e f w f m f end 0 the predicted time traces of the discharging flow velocity u and the lmrp bop clearance x 0 employing the two fluid models in feedback recoil analysis are shown in fig 8 compared to fig 6 the time trace of u has an obvious fluctuating component the maximum velocity u max 20 m s at t 9 s in sfm while u max 14 5 m s at t 9 s in wfcm which are quite distinct from the results in no feedback recoil analysis on the other hand the predicted minimum clearance x 0 is x m i n 0 85 m at t 8 s and the discharge time is t 35 s in sfm the predicted minimum clearance x 0 is x m i n 0 5 m at t 8 s and the discharge time is t 52 s in wfcm these observations are the same as the predictions in the no feedback recoil analysis it is concluded that the coupling effect of the inner flow and the riser structure cannot affect the mass loss effect on riser recoil analysis 3 2 2 damping force effect fig 9 illustrates the simulation results when only considering the internal flow induced drag load and the riser is assumed to be filled with seawater with m i m w l the maximum velocity u max 21 m s at t 8 s in sfm while u max 15 m s at t 9 s in wfcm it is noted that the time traces of clearance x 0 differs considerably with figs 7 and 8 b the minimum lmrp bop clearance x m i n 3 2 m at t 7 s in sfm while it is x m i n 0 3 m at t 7 s in wfcm compared to fig 7 b the minimum negative clearance x 0 is increased remarkably and the transient response in the first few seconds becomes much more significant which demonstrates that the coupling effect of the inner flow and the riser structure has a crucial effect on the transient recoil response of the riser 3 2 3 combined effects of damping force and mass loss the time traces of flow velocity u and the clearance x 0 when accounting for both the damping drag force and mass loss effect when f end 0 are shown in fig 10 different from the results in the no feedback recoil analysis in fig 7 c the maximum discharging flow velocity is u max 20 m s at t 8 s in sfm while u max 14 m s at t 9 s in wfcm the minimum lmrp bop clearance x m i n 4 5 m at t 7 s in sfm while x m i n 1 5 m at t 7 s in wfcm moreover compared to fig 9 it is found that the mass loss effect can decrease the minimum x 0 in both fluid models dramatically which is opposite to the observations in no feedback recoil analysis 3 2 4 frontal force effect simulations are also performed at f end 1 2 ρ m a i n u 2 and f end ρ m a i n u 2 same as the conclusions in the no feedback recoil analysis that although the frontal force has a negligible effect on the discharging flow velocity it can increase the discharge time obviously and decrease the minimum clearance x 0 slightly which are not presented here for conciseness 4 conclusions and future study this study has demonstrated that sfm and wfcm can predict the same dynamic features of the internal discharging fluid in an emergency disconnection scenario however significant deviations have been witnessed ascribed to the different restoring force in the fluid column formulas the discharging flow induced damping drag has a critical influence on the transient response of the riser which leads to the occurrence of negative lmrp bop clearance and the mass loss effect can decrease the minimum clearance remarkably based on feedback recoil analysis although the frontal force has a negligible effect on mud discharge velocity and the recoil performance of the riser it can increase the discharge time obviously the most importantly finding is that the recoil response of the riser in feedback recoil analysis differs considerably with the result in the no feedback recoil analysis highlighting the great significance to account for the coupling effect with the riser in modelling the inner fluid column to accurately capture the transient response of the riser after an emergency disconnection a more detailed structural model of the riser system should be implemented considering the stored elastic energy and a detailed description of the tensioner lang et al 2009 moreover the hydrodynamic loads in a combination of vessel motions and the effect of disconnection time should be applied in a full 3 d analysis where the internal flow induced centrifugal force and coriolis force should be accounted for meng et al 2017a acknowledgement the authors gratefully acknowledge the supports of the national natural science foundation of china grant no 51509153 and the general financial grant from the china postdoctoral science foundation grant no 2015m570367 
