index,text
25480,there has so far been no shared understanding of validity in agent based simulation we here conceptualise validation as systematically substantiating the premises on which conclusions from simulation analysis for a particular modelling context are built given such a systematic perspective validity of agent based models cannot be ensured if validation is merely understood as an isolated step in the modelling process rather valid conclusions from simulation analysis require context adequate method choices at all steps of the simulation analysis including model construction model and parameter inference uncertainty analysis and simulation we present a twelve step protocol to highlight the often hidden premises for methodological choices and their link to the modelling context it is designed to aid modelers in understanding their context and in choosing and documenting context adequate and mutually consistent methods throughout the modelling process its purpose is to assist reviewers and the community as a whole in assessing and discussing context adequacy graphical abstract image 1 keywords model validation model inference calibration generalisation regime shift data availability no data was used for the research described in the article 1 introduction the increasing application of agent based simulation models abm for policy analysis in environmental and land system sciences among other fields has been accompanied by persistent calls to improve and formalise methods for their validation heppenstall et al 2021 elsawah et al 2020 an et al 2020 niamir et al 2020b brown et al 2017 filatova 2015 filatova et al 2013 heckbert et al 2010 marshall and galea 2015 rand and rust 2011 siebers et al 2010 midgley et al 2007 these calls are motivated by the concern that an abm must prove its ability to provide useful and reliable insight for solving real world problems if it is intended to be more than a theoretically appealing academic thought instrument if we look at discussions of validation in simulation modelling in general then traditionally empirical validation i e comparing model predictions to observations of the behaviour of a real world system was regarded as the ideal method for showing relevance and reliability of a model oreskes et al 1994 it entails reproducible protocols and quantitative replicable and transparently communicable results however along with any type of model inference from observed system behaviour behaviour based inference 1 1 behaviour based inference comprises modelling steps typcially called estimation calibration data driven model selection inverse modelling or empirical validation see section 2 1 it relies on statistical assumptions about the data and modelled system and can be severely misleading if these assumptions are not fulfilled in a specific research context e g oreskes et al 1994 polhill and salt 2017 structural validation for contrast aims to ensure correspondence of the structure processes and mechanisms within the model with their real world counterparts it is often limited by incomplete structural system knowledge and typically less formalised when it is conducted as empirical validation of model component behaviour structural behaviour validation microvalidation it is subject to similar statistical prerequisites as empirical behaviour validation at the macro level recognizing that neither empirical nor structural validation can ultimately prove absolute correspondence of a model to reality and that models are by definition abstractions from reality oreskes et al 1994 quine 1951 has led the scientific community to replace the condition for model validity from corresponds to the real system to is adequate for its intended purpose e g forrester and senge 1980 gass 1983 mccarl and apland 1986 oreskes et al 1994 barlas 1996 kydland and prescott 1996 rykiel 1996 beck et al 1997 jakeman et al 2006 deichsel and pyka 2009 augusiak et al 2014 edmonds et al 2019 this means that the conditions for a valid i e adequate model and simulation analysis are context dependent they do not only depend on the characteristics of the system to be modelled but also on the availability of data describing the system and its behaviour as well as the research question to be answered discussing the validation of abm against this background one first notices that abm are used for a large variety of purposes and contexts edmonds et al 2019 lippe et al 2019 schulze et al 2017 ligmann zielinska et al 2020 as inherently structure rich models they are often but not always used in contexts where data driven modelling approaches are not applicable and as a consequence many prerequisites for empirical validation are not fulfilled berger and troost 2014 the importance of structural validation uncertainty and sensitivity analysis for abm used in these contexts has been widely recognised moss and edmonds 2005 brenner and werker 2007 augusiak et al 2014 troost and berger 2015a marshall and galea 2015 polhill and salt 2017 and has even led some to dismiss empirical inference and validation of abm altogether verhoog et al 2016 nevertheless methods for behaviour based inference incl empirical validation of abm have been developed for specific disciplinary contexts for example indirect inference of abm in financial economics chen et al 2012 pattern oriented modelling as de facto standard in ecological modelling grimm et al 2005 thiele et al 2014 approximate bayesian computation for inference of individual based models van der vaart et al 2015 micro validation in energy economics niamir et al 2020a automatised calibration for innovation diffusion models jensen and chappin 2016 and real estate market interactions filatova 2015 magliocca et al 2016 de koning and filatova 2020 or robust inference of parameter distributions in agricultural economics arnold et al 2015 troost and berger 2015a berger et al 2017 hence agent based modelling appears as a very diverse field in which a multitude of methods for model construction model inference validation evaluation and sensitivity analysis is being used and advocated unfortunately the contexts in which specific methods are applicable are typically not explicitly discussed in general terms the abm community has successfully addressed communication challenges caused by the diversity of modelling structures through adopting the odd protocol grimm et al 2010 2020 for formal model documentation the trace format schmolcke et al 2010 grimm et al 2014 was suggested for documenting also hidden steps of the modelling process however a consensus or a formal protocol regarding which modeling methods to choose for a specific abm application context that transcends disciplines has not yet been established not even within the more confined field of abm in environmental and land system sciences an et al 2020 polhill and salt 2017 filatova 2015 this article 2 2 this article resulted from community discussions initiated at workshop w9 of the 2020 iemss conference aims to fill this gap by formalising a framework for validation i e a concept and guideline for ensuring and documenting the adequacy of an abm and the simulation analysis for which it is used in the following section we conceptualise validation as challenging and substantiating the premises on which the conclusions from simulation analysis are built we revisit premises of inference and validation typically used in simulation analysis in general and discuss to what extent they are tested and to what extent they are actually presupposed by empirical and structural validation behaviour based model inference uncertainty analysis and result interpretation it becomes clear that given the diversity of contexts in which abm are applied it is not useful to prescribe one statistical or structural validation procedure to all abm what is more under a paradigm of adequacy and given the constraints on empirical validation validity cannot be tested solely by examining the behaviour or structure of the model once it has been constructed validation cannot consist solely in one confined isolated step of the modelling process typically located after calibration and before predictive simulations as which it is commonly still understood instead validation if understood as systematically examining the adequacy of a model for its purpose requires careful justification of context adequate and mutually consistent choices at all stages of the simulation analysis including the choice of model components and choice of methods for model inference inverse modelling calibration estimation empirical validation and a consistent tracing documentation and interpretation of uncertainties through the modelling process to finally ensure the validity of the conclusions drawn from the analysis on this basis in the third section we develop a step by step protocol of guiding questions to help agent based modellers keep it adequate kia by i defining the modelling context ii adequately selecting models and methods for model inference and uncertainty documentation and iii adequately deriving and interpreting simulation results and their uncertainty the fourth section discusses and concludes how the kia protocol can help the abm community it is intended to a guide modellers during the research process b provide a template structure for transparently documenting the rationale for modelling choices c serve as a checklist for reviewers and stakeholders addressees of simulation results when assessing the validity of a documented study and its conclusions d foster efficient communication between authors and reviewers and e help in structuring the scientific discussion on the merits of choices regarding model selection inference and evaluation made during the modelling process 2 arguments for model validity and their premises if there is one cross disciplinary consensus in the scientific literature on model validation it is that model validity cannot be established in general but only with respect to a specific purpose for which the model is intended to be used model validity is the adequacy of a model for its intended purpose e g forrester and senge 1980 gass 1983 mccarl and apland 1986 oreskes et al 1994 barlas 1996 kydland and prescott 1996 rykiel 1996 beck et al 1997 jakeman et al 2006 deichsel and pyka 2009 augusiak et al 2014 edmonds et al 2019 the purpose of any scientific simulation analysis is to answer a research question scientific answers result as conclusions from scientific argumentation and are accepted if the conclusions can be validly derived from accepted premises mccloskey 1983 hands 2001 scientific objectiveness is ensured by transparently subjecting all premises and deductions to critical scrutiny and peer review klappholz and agassi 1959 caldwell 1991 longino 1992 in its most generic form scientific arguments that employ simulation modelling conform to the following logical proposition troost and berger 2020 major premise a if a simulation s fulfils conditions u and results in y for inputs x we can conclude z s u s r s x y z minor premise b our simulation t results in y for inputs x and fulfils conditions u r t x y u t conclusion we conclude z z by a b and modus ponens premise b is a conjunction of two premises the first premise r t x y our model results in y for inputs x is supported by result analysis showing that the second premise u t our simulation analysis fulfils conditions u holds is what is typically understood as validation a typical example we conclude z climate change will increase poverty among farming households if r t x y simulated farm agent income is lower in climate change scenarios than in the baseline the necessary condition u s is very often formulated as the model employed in our simulation analysis provides sufficiently reliable predictions of y x in the real world system empirical output validation and structural validation test whether a simulation t fulfils this or a very similar formulation of u s but they in turn rely on further necessary premises these premises will be discussed in the following two subsections the third subsection emphasises the role of uncertainty analysis for sound and robust conclusions showing sufficient reliability in the fourth subsection we highlight that simulation analysis may also rely on differently formulated conditions u s that allow for more useful conclusions in some contexts 2 1 premises of behaviour based inference including empirical validation the key underlying premise of any form of inference from the comparison of model and observed system behaviour is predictive performance of a model in observed situations can be generalised to the target situations i e the system situations relevant for the research question this premise is trivially fulfilled if the target situation is part of the observed situations in sample setting however very often the simulation purpose is to anticipate 3 3 this purpose can be called prediction projection scenario analysis counterfactual simulation forecast or just simulation depending on context a more detailed discussion follows in section 3 1 system behaviour for target situations life after climate changed in our example that have not been fully observed or in other cases to find a generalisable model that explains mechanisms governing system behaviour in many target situations explanation edmonds et al 2019 direct generalisation of behaviour i e observed x y relationships between system input and output including the strength of this relationship predictive performance from observed to unobserved situations relies on the two premises that the observed sample is redundant enough to control for sampling error and the target situations are part of a statistical population for which the observed sample is representative representative sample setting these basic statistical preconditions of representativity and control for sampling error apply to any form of model inference from observed behaviour behaviour based inference inverse modelling whether parameter values estimation calibration or model structures data driven model selection are selected or predictive accuracy is estimated and compared to some implicit benchmark goodness of fit evaluation or between training and test samples cross validation 4 4 the latter two are most often associated with the term empirical validation both are behaviour based inference methods because they are used to select accept a model by comparison to other models sample averages in the simplest case see section 3 2 and 3 3 1 if not satisfied the search typically continues until a better model is found in terms such as calibration validation the second word typically refers to the second stage in a simple two sample cross validation within that cross validation process the calibration and validation stage each have their separate roles but together they constitute a method for model selection this narrow meaning of validation is not to be confused with the comprehensive idea of validation as evaluating model adequacy for purpose advocated in this article which involves the adequacy of a model selection inference method in all cases ignoring sampling error and non representativity bias leads to the generalisation of spurious unsystematic confounded or unstable relationships overfitting that causes inaccurate and misleading out of sample predictions and makes the inference invalid browne 2000 forster 2000 hansen and heckman 1996 sampling error is the unavoidable unsystematic error caused by using a sample and not the full population it can potentially be reduced by increased sampling rates williams et al 2022 non representativity occurs due to a biased sample which can be caused by different sometimes subtle reasons including attrition self selection survivorship or failure bias observer bias and unobserved heterogeneity vandecasteele and debels 2007 gangl 2010 gormley and matsa 2014 jager et al 2020 smith 2020 while some minor biases may be corrected by statistical means structural breaks non stationarity or regime shifts such as climate change substantially alter statistical x y relationships causing extreme sample bias observed and target situations are so fundamentally different that they must be considered different statistical sub populations non representative sample setting and direct generalisation is not possible perron 2006 andersen et al 2009 leamer 2010 filatova et al 2016 verstegen et al 2016 in non representative sample settings anticipation of system behaviour for unobserved situations has to rely on structural knowledge about internal system processes see next section nevertheless a sample can still be useful for indirect generalisation structural knowledge often admits alternative model formulations or parameter values candidates even if a sample is not representative of the target situations it can help discriminate between candidates if it is representative and sufficiently redundant for selected situations in which the candidates imply clearly distinguishable behaviour generalisation to a target situation then relies exclusively on structural knowledge embodied in the chosen candidate whereas observed behavioural data only contributes indirectly by selecting this candidate 5 5 similarly indirect generalisation occurs if the output variable of interest has not been observed itself and a model is indirectly tested using another related output variable generalisation of the variable of interest then relies on the premise that the structural knowledge embodied in the model correctly relates the two variables importantly the predictive accuracy measured in the sample cannot be straightforwardly generalised to the target situation in these cases as even systematic differences in prediction errors between sample and target situations cannot be ruled out preconditions for reliably discriminating between candidates are structural and practical identifiability bellman and åström 1970 cobelli and distefano 1980 stigter et al 2017 guillaume et al 2019 structural identifiability means that different candidates are not observationally equivalent i e do not imply the same system behaviour in the observed situations even a fully representative and redundant sample is not able to distinguish between models that predict the same output for the same input 6 6 structural identifiability in our understanding subsumes also the problems of endogeneity often encountered in econometrics practical identifiability means that the variation in the observational data in connection with statistical assumptions e g on representativity and the form of model errors is sufficient to unambiguously attribute effects to the individual parameters of a given model structure sampling error confounded input variation correlated variables multicollinearity unobserved heterogeneity and omitted variable bias are key obstacles for unambiguous model selection and parameter estimation more complex models require more data or more restrictive prior assumptions on parameters to be practically identifiable browne 2000 burnham and anderson 2004 polhill and salt 2017 two candidates that cannot be discriminated by given data are termed equifinal beven and freer 2001 2 2 premises of structure based model choice and structural validation structure based simulation is essential to anticipate behaviour for target situations for which direct generalisation from observed data is not possible and to derive structural explanations of system behaviour structure based simulation deduces system reaction from existing knowledge about system components and their interactions it is sometimes argued that such a deductive process does not create new information however as frisch 1933 argued the key contribution of quantitative modelling is to analyse the interplay of processes and compare the magnitudes and directions of their individual effects in relation to each other in order to deduce the behaviour of the whole system this anticipated or emergent behaviour is new information that was not obvious from looking at existing knowledge on individual processes in isolation the key premise of structure based modelling and structural validation is a model that contains a sufficiently complete and accurate representation of the internal structure and processes of a system is expected to predict system behaviour well structurally assessing the premise of sufficient completeness is often complicated by incomplete knowledge of the system and its potential reconfigurations in addition modellers are typically forced to strike a balance between completeness and efficiency striving to include all relevant processes while omitting unimportant ones that complicate the model construction forrester and senge 1980 assessing the premise of sufficient accuracy in the representation of individual processes is the subject of micro validation moss and edmonds 2005 windrum et al 2007 midgley et al 2007 arnold et al 2015 ghaffarian et al 2021 some structural processes and their parameters may be directly observable and measurable others however may have been generalised from observed subsystem behaviour by behaviour based inference in which case the preconditions discussed in section 2 1 sample representativity identifiability and control of sampling error apply the inclusion of estimated model components into a composite model requires ensuring that the observations from which they have been generalised are representative for all contexts for which they are applied in the composite system 2 3 uncertainty analysis the premises for robust conclusions given the statistical nature of model inference and the typically incomplete nature of structural knowledge discussed in the previous subsections simulation analysis is practically always subject to uncertainty just showing that one particular model results in a specific output for a particular input is hence not convincing it invites the immediate criticism that plausible alternative models might show different results rather it must be shown that the final conclusions towards the research question are robust and not affected by uncertainty and bias van asselt 2000 walker et al 2003 saltelli et al 2013 fischhoff and davis 2014 berger and troost 2014 troost and berger 2015a marchau et al 2019 this implies firstly that the type and degree of uncertainty and bias that are compatible with conclusion z must be carefully specified in the major premise secondly it is a necessary subpremise of u s that implications of uncertainty in structural knowledge and uncertainty in model inference from data and in predictive analysis uncertainty in the anticipated input for target situations and their effects on results have been carefully assessed 2 4 alternative basic premises not every scientific argument using simulation analysis is based on the premise that the model provides reliable predictions of y x in the real world system edmonds et al 2019 have noted that some types of analysis e g theoretical exposition do not require any immediate claims about the relation of the model to reality at all or put more emphasis in representing stakeholder s views of the system a subtler relation is discussed by troost and berger 2020 p 6f who use the following hypothetical abm application economic policy analysis often works in a normative context policy makers need to justify actions with respect to established societal values norms or ideologies for example they might work in a political setting in which the state is supposed to safeguard minimum living incomes but only to interfere in economic processes if market participants are not at all able to help themselves assume that in this context analysts build their abm to simulate the adaptation of farmers to climatic change and model each farm agent decision as a rational optimisation problem with perfect anticipation of projected climatic impacts on production and market conditions in addition farm agents are embedded into a social network of mutual solidarity in which agents less affected by climatic extreme events indiscriminately help the severely affected ones analysing their simulations the analysts find that their optimising farm agents become food insecure under projected impacts they conclude that if perfectly foresighted optimising agents in a perfectly functioning social solidarity network do not fare well real world farmers are even more unlikely to do so and should receive government help as troost and berger 2020 observe the model would likely not pass conventional structural and empirical validation key modelled processes do not correspond to our best knowledge of their real world counterparts in reality farmers do not behave as fully rational optimisers with perfect foresight and networks typically discriminate by family ties ethnicity etc the model will almost surely overestimate observed farm incomes in the past nevertheless the conclusions would withstand such criticism because accurately predicted farmer or network behaviour is not a relevant premise of the argument here in this case the premise that would need to be challenged in validation is that the model calculates the best possible reaction in economic terms empirically this could be done for example by searching for observed cases for which the model predicts worse than observed outcomes one might also identify other unexpected deviations e g larger farm holdings having higher per area incomes than smaller ones which might be observed in the data but not in the model or vice versa and that are not expected to be caused by imperfect optimisation of real world farmers alone nevertheless even if the intention is not to show accurate prediction premises on representativity sampling error and identifiability also apply here structural validation could for example assess whether assumed constraints are overly pessimistic or alternative production safety or income options that might become available with climate change have been omitted troost and berger 2020 further observe that if for contrast the analysts find that their computational agents fare well it would be a logical fallacy to conclude that real world agents will fare well based on the same premises such an argument would require different premises that are much more difficult to support using a model with a clear upward bias both cases use the same model in the same empirical context towards the same motivating research question this illustrates that to judge a model s adequacy we require a very precise definition of its empirical context and the exact argumentative premise it is supposed to support 3 a protocol for ensuring validity in agent based simulation summarising section 2 validation means ensuring the adequacy of simulation analysis for answering a specific well defined research question and such adequacy requires i laying out a logically valid argumentative structure on which potential conclusions from simulation towards the research question can be built ii choosing model components and methods of inference and evaluation that i fit the requirements implied by this argumentative structure and ii rely only on preconditions regarding observation data system properties and structural knowledge that are fulfilled in the given context iii carefully assessing whether the simulation results and specifically their uncertainty and bias are consistent with the requirements of the argument points i iii imply that adequacy is relative to a modelling context which consists of the purpose research question and the available knowledge and data about the modelled system validity cannot be ensured by examining model structure and behaviour ex post only ensuring it requires assessing the adequacy and mutual consistency of choices at all stages of the modelling process given the diversity of contexts in which abm are applied it will be impossible to identify one fits all model structures or statistical methods for all abm and assessments of adequacy need to be able to cover a broad set of possible contexts taking this into account in the following sections we propose a protocol fig 1 of 12 steps covering and linking all stages of simulation analysis the protocol helps characterise the modelling context part i section 3 1 guides the choice of context adequate methods based on this characterisation part ii section 3 2 and emphasises the documentation and consistent propagation of uncertainty through the modelling process so that finally the robustness of conclusions can be comprehensively assessed part iii section 3 3 the protocol itself is provided in tables 1 4 and 6 while the sections in the main text explain the rationale for each step where available we list formal methods of analysis with useful references and highlight the premises for their applicability the concept of the protocol involves eleven dimensions marked by letters a k in fig 1 that characterise modelling contexts and determine adequate choices of models and methods six of these represent requirements of the research question fig 1 a f that can be determined already at the beginning of the modelling process while the other five fig 1 g k require a more in depth analysis of the relationship between research question and system knowledge and data during the modelling process for a better overview the numbering in fig 1 links the main stages of the modelling process blue boxes and context dimensions grey boxes to the associated steps of the protocol the classification and propagation of uncertainty is indicated in red 3 1 part i defining the modelling context the first step is to characterise the modelling context the precise research question and the knowledge and data that are available about the system being modelled table 1 3 1 1 step 1 precisely define the research question a research question typically arises from a larger debate discourse or decision problem for example a public political or scientific debate a participatory planning problem or an economic decision problem a research question to be addressed by simulation analysis is supposed to contribute to this debate even if answering it may not necessarily resolve the whole debate useful contributions can comprise very different questions edmonds et al 2019 epstein 2008 e g detailed precise forecasts of future states of the world statistical testing of explanatory models but also exploring and stress testing possible consequences of decision options berger and troost 2014 lempert 2019 or purely theoretical questions concerning hypothetical models themselves theoretical exposition in the sense of edmonds et al 2019 it is paramount to be clear about what precise question the simulation analysis is supposed to answer and what precise argument it could contribute to the debate 3 1 2 step 2 characterise requirements implied by research question while typologies of model purposes exist e g edmonds et al 2019 epstein 2008 the understanding of commonly employed terms such as prediction forecast projection exploration differs between scientific disciplines often they are used inconsistently bray and von storch 2009 and all lack the necessary precision on some aspects relevant for methodological choices instead table 1 a defines six dimensions to precisely describe the requirements imposed by a research question the most basic consideration is the focus of interest does this lie in anticipating system behaviour in specific situations 7 7 such as in prediction scenario analysis counterfactual simulation projection or forecasts output focus or in describing or understanding system structure 8 8 such as in explanation causal identification or description structure focus carefully defining the target situations is a necessary precondition for judging the degree of generalisation in the next step required resolution required transparency as well as computational resource constraints impose limits on a priori model selection judging the robustness of conclusions requires understanding the required precision and accuracy tolerable uncertainty in simulation outcomes at this point it is often not yet possible to formulate this quantitatively e g 2 deviation is acceptable and should be done in terms of consequences on conclusions e g uncertainty should not affect ranking of policy alternatives by evaluation criteria together these dimensions define requirements that the simulation analysis aims to fulfil whether this is actually possible can only be judged at the end of the modelling process see section 3 3 and table 7 3 1 3 step 3 identify knowledge and data about structure and behaviour of the modelled system in addition to the research question the modelling context is defined by the available information about the simulated system in the form of structural and process knowledge available observations of system behaviour input output trace data as well as in the case of an output focus the anticipated system input data for target situations the next step is to identify which data information and knowledge are available can be obtained with reasonable effort or will remain unattainable for the analysis e g input output observations of far future system states table 1b 3 2 part ii context adequate model and parameter selection and uncertainty documentation appropriate simulation models can be selected in two steps in a first structural step a set of candidate models and candidate parameter sets is constructed or identified whose theoretical characteristics comply with structural system knowledge and the requirements implied by the modelling context steps 4 6 tables 2 and 3 a set of multiple candidates fulfilling the requirements represents the prior model uncertainty 9 9 while we use terminology prior posterior uncertainty borrowed from bayesian statistics here this does not mean that this uncertainty can necessarily be cast into a formal prior probability distribution more often than not it cannot and it may well only be qualitative descriptions of uncertainty cf also beck at al 1997 for this general use steps 7 8 tab 4 in a potential second step behaviour based inference can possibly be used to ascribe empirical likelihood to the candidates rank them and narrow down the candidate set reducing prior to posterior model uncertainty beck et al 1997 step 9 table 4 ideally the two steps complement each other the first step is key to ensure that only adequate candidates are considered in behaviour based inference omitting this theory based preselection can only be adequate if the simulation analysis is output focused and the modelling context allows for the direct generalisation of statistical relationships namely the expected predictive accuracy to the target situations representative and sufficiently redundant data step 6i table 3 only in this specific case expected out of sample predictive accuracy and practical identifiability can be derived solely from the data and are sufficient criteria for model selection polhill and salt 2017 nevertheless even for these direct generalisation cases incorporating structural knowledge in chosen candidate models becomes more essential the scarcer the data a defensible structure based error model specification and pre selection of candidate models increases practical identifiability see e g troost et al 2022 for the second step it is key to ensure the adequacy of the inference process itself steps 7 9 table 4 do the necessary preconditions discussed in section 2 1 hold in the given modelling context is the specific method chosen appropriate for the context is uncertainty properly considered and documented if not behaviour based model inference is clearly not adequate 3 2 1 step 4 representativity of data and degree of generalisation the first step in model selection is to contrast the observed or observable data with the target situation of the research question to determine the degree of generalisation and extrapolation implied following the considerations on representativity and constancy regime shifts structural breaks stationarity discussed in section 2 1 this analysis requires a basic system conceptualisation not yet a full conceptual model that allows judging the system s degree of openness internal stability complexity and stochasticity step 4 table 2 3 2 2 step 5 abm as composite models structuring component context while our protocol addresses modellers that are inclined to use an abm one question to ask in structural model choice is of course whether an abm indeed suits the given modelling context or a different modelling approach is more promising abm are typically composite models model systems which are composed of lower hierarchy models components that mirror relevant subsystems and processes for example they typically contain a model of individual agent behaviour based on the internal state of and external influence on the agent this submodel for agent behaviour in turn may itself be a composite of lower hierarchy components e g for learning demographics and economic decisions schlüter et al 2017 abm also typically contain models of agent interactions e g communication markets auction collective action or network models schreinemachers and berger 2011 in addition many abm in natural resource management link to biophysical components that model responses of natural systems e g a crop field or watershed to agent intervention arnold et al 2015 10 10 whether the overall composite model is labeled as abm or the abm is itself considered part of the integrated composite is irrelevant the discussed considerations apply in both cases system behaviour in an abm emerges not only from the interactions between agents but conceptually also from the interactions of individual model components in general such structure rich composite models are typically used for structure focused analysis or for output focused analysis when direct generalisation from observed data is not possible nolan et al 2009 voinov and shugart 2013 in direct generalisation contexts prediction is often achieved more efficiently with statistical or machine learning models polhill and salt 2017 11 11 this does not imply that abm cannot be used for direct generalisation contexts there may often just be more efficient approaches the adequacy of a composite model relies on i an assembly of components that together fulfil the relevant premises for the overall research question to be answered ii a careful assessment of the adequacy of each lower hierarchy component for its intended role in the composite and iii a consistent consideration of the uncertainty in each component at the composite level arnold et al 2015 it is important to realise that each component has its specific own question to answer and has its own specific modelling context which may differ considerably from the modelling context of the composite as a whole or that of other components even if the abm is used in an overall modelling context that is not apt for direct statistical inference this does not rule out that within model contexts of lower hierarchy components exist in which representative samples allow for direct generalisation and e g the use of machine learning methods for these components for example we may not yet have observed how a specific group of farmers behaves and fares in a warmer climate so we cannot empirically measure the predictive performance of a composite model that simulates potential future farmer behaviour and welfare we may however be able to include a plant growth component into this composite model that can be tested based on observations and experiments in a range of warmer and colder regions if we consider this range representative for potential future growth conditions troost et al 2020 an important step hence is to structure the overall modelling task into subcomponents and then recursively revisit the steps of the protocol also for each component individually step 5 table 2 this step may often not directly result in the final structure but may involve various iterations through steps 4 10 until an adequate composite structure for the overall modelling context has been established which may or may not involve an abm 3 2 3 step 6 choosing structurally adequate candidate models and prior parameter ranges for each component the guiding questions in table 3 step 6 items i ix help to check potential model component candidates for context adequacy from a structural point of view the table also lists selected literature sources that provide formal tests or more in depth discussions of each question for adequate structure based model selection it is useful to first sketch a comprehensive conceptual system model argent et al 2016 even if not all system processes can or finally have to be included in the simulation model this conceptual sketch can serve as a benchmark to check a candidate s match of the domain of applicability and sufficient completeness of processes for the target situations parker et al 2008 it must be ensured that model structure and parameters fixed in the candidate are also expected to be constant no change over time and invariant unaffected by policy treatment change to target situation in the real world system lucas 1976 engle and hendry 1993 hendry 1996 relevant changes between situations must be captured as exogenous input or result from internal feedback in the model it is not always possible to explicitly simulate all potential real world feedback in the model itself but it should then at least be possible to capture potential feedback as changing boundary conditions that may then later be assessed in uncertainty analysis troost and berger 2015b troost et al 2022 table 3 ii iv expected deviations i e the part of the system behaviour that is not explained or predicted by the model from a theoretical point of view should be consistent with the precision and accuracy required by the research question table 3 v research questions requiring accuracy with respect to an absolute reference necessitate not only a high degree of model completeness with respect to all systematic processes but also with respect to probability distributions for unsystematic effects as well as reliable system input data for target situations research questions requiring accuracy only with respect to the relationships between simulated target situations demand model completeness only with respect to systematic differences simplifying assumptions such as optimising agents in our example introduced in section 2 4 may lead to systematic over or underestimation bias this is not problematic as long as major conclusions drawn from the simulation analysis will not depend on such simplification robustness to the relaxation of simplifying assumptions no model artefacts 12 12 the lucas critique lucas 1976 is a famous example in economics for a challenge to modelling practice based on these grounds 13 13 conclusions that are based on comparing model results to asymmetrical one sided thresholds even get stronger if the methodological approach is biased against them conversely they are weakened by biases in their favour especially if these cannot be precisely quantified and corrected this principle mirrors the conservative rationale in statistical hypothesis testing type ii errors false negatives are preferred over type i errors false positives logical consistency correct technical implementation and fit to the required resolution transparency and resource constraints are obvious preconditions that must be assessed even if the component context allows for direct generalisation table 3 vi ix 3 2 4 steps 7 and 8 documenting prior and input data uncertainty and assessing structural identifiability structure based model selection typically results in a number of plausible model structures and parameter values this prior uncertainty should be documented even if not all plausible alternatives can be implemented and tested step 7 table 4 the first step in determining whether behaviour based inference can reduce this prior uncertainty then is to assess the structural identifiability of candidates in the observed range of data i e check whether the behaviour of candidate models differs in the domain for which the data is representative step 8 table 4 a variety of analytical and numerical approaches to assess structural identifiability are available guillaume et al 2019 chis et al 2011 including numerical parameter screening methods from sensitivity analysis campolongo et al 2007 troost and berger 2015a not only uncertain parameters and structure in the model itself but also uncertain auxiliary parameters or assumptions e g error distributions for expected deviations and measurement error in input data imputation to deal with incompleteness in the data 14 14 a frequently encountered example in agricultural abm would be a parameter used in imputing cash reserves of farm agents which are typically unobserved or undisclosed at simulation start from observed characteristics such as farm size location land use or livestock ownership alternative choices in data curation preparation or aggregation must be documented and considered when assessing identifiability structural identifiability may differ between parameters of the same model some parameters can be structurally identifiable in the available data see appendix a 1 while others are not and their uncertainty cannot be reduced by behaviour based inference e g troost and berger 2015a structural non identifiability cannot be resolved by more of the same data but requires either widening the range of situations observed or considering more dimensions of the data 3 2 5 step 9 choosing adequate methods for behaviour based inference and measurement of predictive accuracy if structural identifiability is given or direct generalisation is possible one can choose an adequate method for behaviour based inference step 9 table 4 if not it is sometimes still useful to measure sample predictive accuracy of candidates and compare it against a null model to ensure the models do not completely go astray behaviour based inference requires choosing a loss function a metric to weight deviations between observed and simulated behaviour and an algorithm to characterise the distribution of the loss function over candidates exploration estimation of posterior parameter distribution or find the candidate with the optimal loss function value optimisation calibration 3 2 5 1 adequate choice of loss function or likelihood loss functions step 9i table 4 are used to weight deviations between simulations and observations by severity from a decision theoretic point of view loss functions should more strongly penalise those errors that would lead to stronger changes in conclusions in direct generalisation cases and when sampling error has been controlled for e g by cross validation see below the measured loss can be directly generalised to target situations hence in this case one can choose a loss function that is limited to output variables of interest and whose weighting directly reflects the precision accuracy relativity and symmetry required by the research question see step 2 penalising misclassifications based on their practical implications e g prefer models with stronger deviations overall but high reliability in critical areas manderscheid 1965 berger 1980 mccloskey 1985 farahmand et al 2017 manski 2019 15 15 in the direct generalisation case if we are interested in predicting deforestation for example then we can focus on the ability of the model to predict changes from forest to some other land use without caring whether it also correctly predicts the new land use or changes among non forest land use classes we thank judith verstegen for this example in indirect generalisation cases and structure focused analysis loss functions must reflect the impact of model errors on our confidence that the candidate reflects underlying system processes in this case loss functions should reflect the expected deviations of the model including sampling error model bias and error correlation schoups and vrugt 2010 regarding all observed output variables linked to the modelled mechanisms 16 16 in the indirect generalisation case even if we are only interested in predicting deforestation but the mechanisms that we have to trust to anticipate developments in unseen situations are supposed to also determine changes in other land uses accurately then deviations in predictions of these other variables also undermine our trust in predicting deforestation since we cannot assume that predictive accuracy on deforestation observed in the sample is the same in the future this holds even if prediction of deforestation in the sample is accurate theoretically anticipated deviations of candidate models are considered less severe than deviations unlikely to occur if the model predicts according to its theoretically expected precision hansen and heckman 1996 blavatskyy and pogrebna 2010 for example if a model is designed to predict an upper bound underestimation of observations should be penalised overestimation not 17 17 bayes estimators allow combining a loss function for relevant errors in model application with a likelihood for the posterior probability of the model bassett and deride 2019 if the model is expected to be well specified and implies a well defined tractable stochastic error distribution a parametric likelihood function can be formulated using parametric likelihoods in cases where their underlying assumptions are not fulfilled or in doubt leads to biased model selection and overconfident conclusions beven et al 2008 stedinger et al 2008 robust loss functions allow for occasional outliers potentially generated by processes not captured in the model willmott and matsuura 2005 hyndman and koehler 2006 if the model is expected to capture the essential systematic relationship but the exact error distribution is unknown or intractable summary statistics that capture relevant systematic relationships can be estimated on both observations and model output a loss function can then be applied to the difference in the summary statistics rather than the individual observations classical and bayesian indirect inference chen et al 2012 beaumont 2010 drovandi et al 2015 pattern oriented modelling generalises this principle to incorporate more qualitatively described strong and weak statistical patterns grimm and railsback 2012 in other cases qualitative criteria are used to define binary valued acceptance functions spear and hornberger 1980 troost and berger 2015a often absolute goodness of fit measures e g model efficiencies are used instead of pure loss functions or likelihoods step 9ii table 4 while the latter provide a relative ranking between candidate models but their absolute values are specific to the sample used absolute goodness of fit measures don t change the relative ranking but take the sample variance into account in order to allow comparison between models estimated from different samples bennett et al 2013 hauduc et al 2015 implicitly efficiency criteria compare the evaluated model with a benchmark or null model that employs only basic information of the data r2 and model efficiency for example contain the sample average as a null model this null benchmark should be carefully chosen the sample average is only one possible choice trend extrapolation random allocation or seasonal or group specific averages are often more adequate benchmarks schaeffli and gupta 2007 pontius and millones 2011 as an alternative grimm and railsback 2012 suggest to always explicitly include a benchmark null model among the candidates 3 2 5 2 adequate assessment of practical identifiability and posterior uncertainty it is paramount to document uncertainty in measured predictive accuracy and model rankings and to assess how reliable the data could discriminate between candidates practical identifiability step 9 iii iv table 4 methods for behaviour based inference considerably differ in the extent to which uncertainty in the selection process is characterised and to which prior uncertainty is considered and it is important to select a combination of method s whose premises fit the application case table 5 for example classical minimum loss or maximum likelihood based parameter estimation presuppose that both the likelihood and the model structure are certain and correctly specified and all considered candidate parameterisations are a priori equally likely stigler 2007 they identify one best fitting model and limit quantification of posterior uncertainty to confidence intervals for parameters while large confidence intervals point to low practical identifiability they cannot conceptually be interpreted as posterior probabilities for parameters bayesian frameworks hobbs and hilborn 2006 can overcome the latter limitations if prior probabilities are specifiable k fold cross validation 18 18 the traditional separation of data into one training and one validation dataset is the most basic form of cross validation but is subject to sampling error itself k fold cross validation is the more robust extension is the essential non parametric method to quantify sampling error in estimated expected loss or predictive accuracy for unseen situations from a sample browne 2000 arlot and celisse 2010 bennett et al 2013 vehtari et al 2017 it should be combined with any of the basic inference methods and also avoids the complexity bias when model structures are uncertain selecting model structures purely based on predictive accuracy measured in one sample is biased towards models with a higher number of freely adaptable parameters which increases the danger of overfitting adequate model inference requires correcting this bias e g by k fold cross validation only when parametric likelihoods are applicable see above information criteria aic bic dic waic or formal bayesian frameworks with appropriately specified prior likelihoods burnham and anderson 2004 ward 2008 vehtari et al 2017 provide an alternative statistical diagnostics for influential observations e g cook s distance and multicollinearity in the data e g variance inflation factors common in econometric analysis should complement the analysis of posterior uncertainty 3 3 part iii adequate derivation and interpretation of simulation results and uncertainty fig 1 illustrated how an adequate modelling process structures quantifies and potentially reduces uncertainty the definition of a research question divides uncertainty regarding the research question from uncertainty about wider implications in the debate theory based model selection structures the uncertainty about the research question into prior model uncertainty represented by different candidate model structures and parameter ranges input uncertainty uncertainty in boundary and initial conditions expected deviation error terms bias aleatory uncertainty and unmodelled uncertainty alternative models not included in the analysis 19 19 brenner and werker 2007 emphasise an inclusion of all logically possible parameter values and model structures consistent with structural and empirical knowledge we recognise that this is often not feasible in practice however this needs to be acknowledged as unmodelled uncertainty and appropriately discussed when deriving conclusions processes that have been ignored potential exogenous events not considered non formalised error terms unforeseeable events critical assumptions for which no alternatives are tested etc if applicable and successful behaviour based inference potentially reduces prior model uncertainty to posterior model uncertainty if discrimination of candidate models by data is not possible the posterior uncertainty remains the same as the prior uncertainty in structure focused analysis description explanation the resulting posterior model uncertainty is already the final uncertainty to be interpreted for conclusions in output focused analysis prediction scenario analysis exploration posterior uncertainty and input uncertainty still need to be translated into predictive uncertainty for target situations e g future or policy scenarios by simulation experiments that include uncertainty analysis in an adequate modelling process in which uncertainty is properly analysed and propagated the final posterior predictive uncertainty and the unmodelled uncertainty describe the actual state of knowledge regarding the research question that can be defensibly extracted from the available data and structural system knowledge this final model uncertainty can then be compared with the precision required by the research question for interpretation and derivation of conclusions 3 3 1 step 10 interpretation of predictive accuracy and posterior uncertainty if sampling error has been properly controlled for e g by cross validation expected predictive accuracy indicates how well the model predicts or explains the variation in the population of situations for which the sample is representative subject to the importance weighting embodied in likelihood or loss function this is valuable information in its own right however whenever using this information to draw further conclusions step 10i table 6 e g about the model being sufficiently good or the correct or best explanation care has to be taken oreskes et al 1994 even though absolute goodness of fit measures such as model efficiencies project predictive error onto an absolute scale between null model and perfect fit defining any threshold to indicate sufficient fit on this scale remains subjective or based on convention similar to significance levels in statistical analysis unless this threshold can be convincingly derived from the research question pontius and millones 2011 the same holds for thresholds defined on posterior densities or relative differences in information criteria stephens et al 2005 the well known problems of induction under determination and theory ladenness imply that proving by comparison to observation that a model is the true model is ultimately impossible oreskes et al 1994 quine 1951 expected predictive accuracy provides a relative ranking and allows identification of the best among the candidate models for the given sample the more comprehensive the list of candidate models and parameterisations that has been tested and the more representative the sample the higher can be the confidence in having identified a generalisable best model or parameterisation as all other statistical relationships measured expected predictive accuracy cannot be generalised to target situations across structural breaks uncertainty in inference can be quantified as a posterior probability for the candidates if a formal bayesian framework with proper prior probabilities and appropriate likelihood has been used in inverse modelling however also in those cases where posterior probabilities or credible intervals cannot be derived it is important to consider posterior uncertainty step 10ii table 6 and recognise that the best model does not necessarily have or even approach a posterior probability of one troost and berger 2015a the potential explanatory and predictive power of alternatives should not be neglected in interpretation if the analysis is structure focused and interested in which model provides the better explanation it remains inconclusive whenever two alternative models cannot be robustly discriminated by data or needs to employ additional theoretical considerations e g parsimony as an epistemological principle 20 20 parsimony as a epistemological principle simpler models are always to be preferred differs from a pragmatic argument for parsimony in estimating models for prediction simpler models are less prone to overfitting or correspondence to established theory to justify a decision for one or the other model in output focused analysis subsequent predictive simulation should use the full posterior distribution consider confidence or credible intervals or at least a representative ensemble of all candidates that show nonnegligible explanatory power ensemble modelling model averaging 3 3 2 step 11 analysis and interpretation of predictive uncertainty only in rare cases it will be permissible to directly generalise expected predictive uncertainty from behaviour based inference to the target situation preconditions representative sample negligible input uncertainty one clearly best model generally predictive uncertainty for a target situation is a function of the uncertainty about the systematic effect of system input on behaviour that is captured in the set of models and parameterisations posterior model uncertainty the model error bias and unsystematic aleatory uncertainty and the uncertainty in system inputs e g scenarios boundary conditions for target situations building on the considerations by marchau et al 2019 and walker et al 2003 table 7 lists which forms of predictive simulation outputs are adequate depending on the level of uncertainty in each of these dimensions unconditional predictions require low uncertainty in all locations of uncertainty for all higher levels of uncertainty comprehensive uncertainty analysis is necessary step 11 table 6 depending on model complexity and available computational resources one can choose from a considerable number of approaches for efficient uncertainty and sensitivity analysis 21 21 following the definition of helton et al 2006 uncertainty analysis is concerned with quantifying the uncertainty variance in simulation outputs while sensitivity analysis is concerned with linking this uncertainty to uncertainty in model inputs i e determine which uncertain input factors are responsible to which degree for the uncertainty in outputs helton et al 2006 saltelli et al 2008 gramacy and lee 2009 troost et al 2022 clear conditions for appropriate choices have been formulated uncertainty analysis must be global i e cover the full range of potential input values including interactions and correlation between input factors saltelli and annoni 2010 probabilistic predictions require probability information in all locations it is key that exploration of predictive uncertainty focuses on the output quantity precision and resolution relevant to answering the targeted research question when we compare two target situations we can distinguish the apparent or observable difference i e the difference between two predictions that includes unsystematic stochastic effects and the systematic difference i e the difference between two predictions controlled for unsystematic effects in many decision support situations the future may not be precisely predictable but for a good decision it is enough if the systematic differences caused by decision options can be pointed out using pairwise comparison at each tested combination of input factor values berger and troost 2014 for stochastic models this requires common random numbers schemes stout and goldie 2008 troost and berger 2016 the alternative is running sufficient repetitions and applying statistical comparison tests e g verstegen et al 2019 22 22 common random number schemes are more efficient in terms of required model runs but sometimes quite difficult to implement see example in troost and berger 2016 especially when uncertainty is high in all locations rather than trying to merely describe all possible outcomes strategies to detect decision options that are robust under many different scenarios and assumptions should be emphasised assumptions based planning stress testing red teaming lempert 2019 marchau et al 2019 3 3 3 step 12 interpretation and conclusions the interpretation of results should compare the final uncertainty step 10 or 11 to the required precision and accuracy of the research question step 2 if the required certainty is reached conclusions that are consistent with the simulated output can be considered valid and sound if uncertainty is too high we have to conclude that the knowledge employed in the process is insufficient for the desired type of conclusions e g carauta et al 2021 it should not be necessary to emphasise that this is an equally valuable and relevant result leamer 2010 the structure of the argument and the premises that are critical to support the conclusions must be clearly laid out step 12 table 6 this involves the premises that are supported by simulation results but also the auxiliary and hidden premises prior model evidence representativity of data identifiability posterior uncertainty both unstructured uncertainty about wider implications step 1 and unmodelled uncertainty step 7 remain qualitative and unquantified in the modelling process nevertheless they must be an important part of the interpretation conclusions must be qualified with respect to the information omitted from the modelling process hypotheses on how omitted processes or alternative system conceptualisations could affect conclusions must be discussed forrester and senge 1980 banerjee et al 2016 argue for an explicit and structured section for speculation about external validity generalisability of results obtained from case studies especially when using models to inform decision makers in the face of deep uncertainty transparent documentation of critical and potentially value laden fundamental assumptions see protocols in kloprogge et al 2011 saltelli et al 2013 fischhoff and davis 2014 van der sluijs 2017 and additional effort to assess the robustness of decision option outcomes to these assumptions is essential lempert 2019 marchau et al 2019 4 discussion and conclusions the purpose of validation is to ensure the adequacy of simulation analysis for answering a specific well defined research question this requires a careful analysis of the logical argumentative structure and assessment of the critical premises that conclusions from simulation analysis build upon such premises rest on simulation outcomes but are also implicit in the choice of models and methods of inference from data especially the latter is not always obvious to modellers reviewers and addressees of simulation results for example empirical validation and model inference presuppose representativity of data identifiability and control of sampling error moreover specific methods such as maximum likelihood estimation rely on even more restrictive not always obvious premises see tables 5 and 7 validation needs to ensure that models and methods chosen fit the modelling context which comprises the research question and available system knowledge and data on system behaviour and it needs to assess whether the final uncertainty in simulation results fits the requirements on precision and accuracy implied by the research question in most cases this is more complex and subtler than a single step matching of context to a method rather it is a hierarchical process i e outcomes of earlier steps affect choices in later steps e g behaviour based inference should not be pursued without first ensuring representative data and structural identifiability it is recursive i e in composite models such as abm the context of each component must be assessed and iterative i e outcomes of subsequent steps may encourage receding a number of steps and reconsidering choices for example if the evaluation of structural identifiability practical identifiability or predictive uncertainty leads to unsatisfactory results it may be useful to go back to structure based model selection or even to a redefinition of the research question it may be possible to answer a more restricted question that is already useful where the context does not allow to reliably answer the original question the kia protocol that we have proposed in this article is intended to guide modellers in making adequate choices during the process of simulation analysis and justify them with adequate argumentation it provides a guideline to reviewers who can use it by starting from the final conclusions and their premises and working backward to evaluate whether the steps taken during the modelling process adequately support the premises in the given context moreover it is intended to structure documentation i as a checklist to ensure modelling context and justification for all relevant modelling decisions have been discussed in the main body of an article and ii as a template for well structured tabular documentation in an appendix the protocol mirrors and is compatible with established recommendations for a structured modelling process e g jakeman et al 2006 but it emphasises the linkages and propagation of uncertainty between modelling stages and highlights general criteria for the choice of adequate methods at each stage it operationalises the principle as empirical as possible as general as necessary coined for abm by brenner and werker 2007 it incorporates the different levels of uncertainty of walker et al 2003 and marchau et al 2019 but also explains how this uncertainty comes about in the modelling process similar to polhill and salt 2017 it highlights the importance of structural model choice compared with purely data driven model inference while we have not extensively discussed stakeholder participation the protocol is meant to be open to valuable stakeholder input and feedback at any step of the process e g in shaping the encompassing debate defining the targeted research questions providing information in model selection and inference and shared interpretation voinov et al 2016 barreteau et al 2010 the exhaustive discussion of many of the guiding questions listed in the tables of the protocol would warrant their own articles our intention here has been to comprehensively list them and highlight their interlinkages we have linked many of the guiding questions to literature with more detailed explanation or formal assessment methods this list of methods does not claim to be complete and it will certainly have to be extended over time as new approaches for model testing selection or estimation are developed to deal with the formulated questions we actually hope that this protocol sparks interest in developing new methods and then assists in clearly communicating the conditions for which they are suitable in defining eleven dimensions for the characterization of modelling contexts we have moved beyond discrete typologies of model purpose e g edmonds et al 2019 epstein 2008 typologies such as edmonds et al 2019 and especially terms such as prediction forecast projection or exploration whose understanding and usage differ between and sometimes even within disciplines bray and von storch 2009 can be mapped onto these dimensions to allow for more precise communication see appendix a 2 the dimensions are intended to improve communication on methodology by helping to identify which abm applications share a similar modelling context and might learn from each other and which not for example troost and berger 2015a and carrella et al 2020 both deal with unknown or intractable likelihoods for model inference however the former face both low structural and practical identifiability while the latter assume few parameters and a large number of identifying summary statistics i e high practical identifiability as both are explicit about the assumed modelling context this can be read from their articles but may still be easily overlooked our protocol is intended to highlight these differences and in this way avoid common pitfalls in discussions between modellers and reviewers about adequate and valid model use and inference e g avoid discussions about an appropriate loss function when structural identifiability is the more important issue avoid overemphasis on separation of training and validation data when validation data is not representative for target situations avoid discussions about unreliability of unconditional predictions when these are neither possible nor necessary avoid suggesting model simplification to increase practical identifiability when model complexity is required for structural reasons and direct generalisation is not adequate etc given the breadth of application contexts for abm and their potential components we strived to be general in redacting the protocol we believe that the principles discussed here are applicable to any modelling endeavour and most disciplinary standards that have been established form special cases that are in principle covered by the protocol in this sense we expect that it can be useful for many different types of simulation not only for abm at this point the kia protocol itself is a theory based hypothesis that requires practical testing we propose it to the community of agent based modellers for adoption in model construction documentation and review its use in practice will tell if it proves useful as guidance for model development and a communication device in documentation and review based on practical experience it should then be reviewed and improved declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank all participants of workshop w9 best practice in agent based model parameterisation and validation at the 10th international congress on environmental modelling software 2020 for their valuable and constructive input and comments ct and tb acknowledge funding by the federal ministry of education and research of germany bmbf for the project simlearn 01is19073c ln acknowledges funding from the energy demand changes induced by technological and social innovations edits project provided by ministry of economy trade and industry meti japan gp acknowledges funding by the scottish government strategic research programme 2022 27 project id jhi c5 1 qbl acknowledges support by the cgiar research program on grain legumes and dry cereals crp gldc and initiative on sustainable intensification of mixed farming systems tf acknowledges the support of the european research council erc under the european union s horizon 2020 research and innovation program grant agreement number 758014 author contributions ct conceptualized and wrote the manuscript rh ab hvd tf qbl ml ln jgp zs and tb contributed ideas comments and corrections at all stages of manuscript writing we thank judith verstegen and a further anonymous referee for highly constructive and valuable feedback during the review process appendix a a 1 notes on differences in structural identifiability of parameters structural identifiability in the data can considerably differ between different groups of parameters or model components for example parameters that relate short term agent behaviour to static characteristics can be estimated from sufficiently heterogeneous cross sectional data for contrast parameters that affect dynamic behaviour or accumulative development over several periods require panel data troost and berger 2015a 2020 parameters that affect the probability of low probability events can only be identified if enough low probability events have been observed filatova et al 2016 structural non identifiability cannot be resolved by more of the same data but requires either widening the range of situations observed or more dimensions of the data under certain conditions unidentifiable parameters may be temporarily fixed to allow identification of other components however fixing has to be reversed for latter predictive simulation in order not to obscure model uncertainty noninfluence in the observed domain does not necessarily mean noninfluence in the target situation see example in troost and berger 2015a a 2 mapping purposes to modelling contexts we believe that terms like prediction forecast or projection which are often ambiguous or defined differently between disciplines as well as typologies of edmonds et al 2019 can be communicated more precisely using the suggested dimensions of the modelling context for example the seven modelling purposes of edmonds et al 2019 could be coarsely mapped onto our characterisations of modelling context as follows in theoretical exposition and illustration the system under study is the model itself with the former being output focused moving from an insufficient sample situation to an in sample situation by exhaustive simulation and the latter putting emphasis on transparency and interpretability analogy does relate to a real system and is structure focused with a low demand on precision and comprehensiveness but high demands on transparency and interpretability in this three cases conclusions about the relationship of the model to the real world are left aside for a moment or discussed as unmodelled uncertainty social learning and education can happen in all contexts can be about the model opinions of participants or the real system output or structure but require transparency and interpretability description corresponds to structure focused in sample analysis output focused in sample analysis not mentioned by edmonds et al could be termed compression storing and reproducing observations in a more resource efficient way than explicitly listing them explanation is structure focused out of sample generalization prediction is any output focused analysis in out of sample or non representative sample settings this wide scope of prediction still opens up a lot of room for misunderstanding and clearer definitions of modelling context using the dimensions of required precision and accuracy transparency etc can help in this context to link to appropriate forms of simulation analysis e g marchau et al 2019 
25480,there has so far been no shared understanding of validity in agent based simulation we here conceptualise validation as systematically substantiating the premises on which conclusions from simulation analysis for a particular modelling context are built given such a systematic perspective validity of agent based models cannot be ensured if validation is merely understood as an isolated step in the modelling process rather valid conclusions from simulation analysis require context adequate method choices at all steps of the simulation analysis including model construction model and parameter inference uncertainty analysis and simulation we present a twelve step protocol to highlight the often hidden premises for methodological choices and their link to the modelling context it is designed to aid modelers in understanding their context and in choosing and documenting context adequate and mutually consistent methods throughout the modelling process its purpose is to assist reviewers and the community as a whole in assessing and discussing context adequacy graphical abstract image 1 keywords model validation model inference calibration generalisation regime shift data availability no data was used for the research described in the article 1 introduction the increasing application of agent based simulation models abm for policy analysis in environmental and land system sciences among other fields has been accompanied by persistent calls to improve and formalise methods for their validation heppenstall et al 2021 elsawah et al 2020 an et al 2020 niamir et al 2020b brown et al 2017 filatova 2015 filatova et al 2013 heckbert et al 2010 marshall and galea 2015 rand and rust 2011 siebers et al 2010 midgley et al 2007 these calls are motivated by the concern that an abm must prove its ability to provide useful and reliable insight for solving real world problems if it is intended to be more than a theoretically appealing academic thought instrument if we look at discussions of validation in simulation modelling in general then traditionally empirical validation i e comparing model predictions to observations of the behaviour of a real world system was regarded as the ideal method for showing relevance and reliability of a model oreskes et al 1994 it entails reproducible protocols and quantitative replicable and transparently communicable results however along with any type of model inference from observed system behaviour behaviour based inference 1 1 behaviour based inference comprises modelling steps typcially called estimation calibration data driven model selection inverse modelling or empirical validation see section 2 1 it relies on statistical assumptions about the data and modelled system and can be severely misleading if these assumptions are not fulfilled in a specific research context e g oreskes et al 1994 polhill and salt 2017 structural validation for contrast aims to ensure correspondence of the structure processes and mechanisms within the model with their real world counterparts it is often limited by incomplete structural system knowledge and typically less formalised when it is conducted as empirical validation of model component behaviour structural behaviour validation microvalidation it is subject to similar statistical prerequisites as empirical behaviour validation at the macro level recognizing that neither empirical nor structural validation can ultimately prove absolute correspondence of a model to reality and that models are by definition abstractions from reality oreskes et al 1994 quine 1951 has led the scientific community to replace the condition for model validity from corresponds to the real system to is adequate for its intended purpose e g forrester and senge 1980 gass 1983 mccarl and apland 1986 oreskes et al 1994 barlas 1996 kydland and prescott 1996 rykiel 1996 beck et al 1997 jakeman et al 2006 deichsel and pyka 2009 augusiak et al 2014 edmonds et al 2019 this means that the conditions for a valid i e adequate model and simulation analysis are context dependent they do not only depend on the characteristics of the system to be modelled but also on the availability of data describing the system and its behaviour as well as the research question to be answered discussing the validation of abm against this background one first notices that abm are used for a large variety of purposes and contexts edmonds et al 2019 lippe et al 2019 schulze et al 2017 ligmann zielinska et al 2020 as inherently structure rich models they are often but not always used in contexts where data driven modelling approaches are not applicable and as a consequence many prerequisites for empirical validation are not fulfilled berger and troost 2014 the importance of structural validation uncertainty and sensitivity analysis for abm used in these contexts has been widely recognised moss and edmonds 2005 brenner and werker 2007 augusiak et al 2014 troost and berger 2015a marshall and galea 2015 polhill and salt 2017 and has even led some to dismiss empirical inference and validation of abm altogether verhoog et al 2016 nevertheless methods for behaviour based inference incl empirical validation of abm have been developed for specific disciplinary contexts for example indirect inference of abm in financial economics chen et al 2012 pattern oriented modelling as de facto standard in ecological modelling grimm et al 2005 thiele et al 2014 approximate bayesian computation for inference of individual based models van der vaart et al 2015 micro validation in energy economics niamir et al 2020a automatised calibration for innovation diffusion models jensen and chappin 2016 and real estate market interactions filatova 2015 magliocca et al 2016 de koning and filatova 2020 or robust inference of parameter distributions in agricultural economics arnold et al 2015 troost and berger 2015a berger et al 2017 hence agent based modelling appears as a very diverse field in which a multitude of methods for model construction model inference validation evaluation and sensitivity analysis is being used and advocated unfortunately the contexts in which specific methods are applicable are typically not explicitly discussed in general terms the abm community has successfully addressed communication challenges caused by the diversity of modelling structures through adopting the odd protocol grimm et al 2010 2020 for formal model documentation the trace format schmolcke et al 2010 grimm et al 2014 was suggested for documenting also hidden steps of the modelling process however a consensus or a formal protocol regarding which modeling methods to choose for a specific abm application context that transcends disciplines has not yet been established not even within the more confined field of abm in environmental and land system sciences an et al 2020 polhill and salt 2017 filatova 2015 this article 2 2 this article resulted from community discussions initiated at workshop w9 of the 2020 iemss conference aims to fill this gap by formalising a framework for validation i e a concept and guideline for ensuring and documenting the adequacy of an abm and the simulation analysis for which it is used in the following section we conceptualise validation as challenging and substantiating the premises on which the conclusions from simulation analysis are built we revisit premises of inference and validation typically used in simulation analysis in general and discuss to what extent they are tested and to what extent they are actually presupposed by empirical and structural validation behaviour based model inference uncertainty analysis and result interpretation it becomes clear that given the diversity of contexts in which abm are applied it is not useful to prescribe one statistical or structural validation procedure to all abm what is more under a paradigm of adequacy and given the constraints on empirical validation validity cannot be tested solely by examining the behaviour or structure of the model once it has been constructed validation cannot consist solely in one confined isolated step of the modelling process typically located after calibration and before predictive simulations as which it is commonly still understood instead validation if understood as systematically examining the adequacy of a model for its purpose requires careful justification of context adequate and mutually consistent choices at all stages of the simulation analysis including the choice of model components and choice of methods for model inference inverse modelling calibration estimation empirical validation and a consistent tracing documentation and interpretation of uncertainties through the modelling process to finally ensure the validity of the conclusions drawn from the analysis on this basis in the third section we develop a step by step protocol of guiding questions to help agent based modellers keep it adequate kia by i defining the modelling context ii adequately selecting models and methods for model inference and uncertainty documentation and iii adequately deriving and interpreting simulation results and their uncertainty the fourth section discusses and concludes how the kia protocol can help the abm community it is intended to a guide modellers during the research process b provide a template structure for transparently documenting the rationale for modelling choices c serve as a checklist for reviewers and stakeholders addressees of simulation results when assessing the validity of a documented study and its conclusions d foster efficient communication between authors and reviewers and e help in structuring the scientific discussion on the merits of choices regarding model selection inference and evaluation made during the modelling process 2 arguments for model validity and their premises if there is one cross disciplinary consensus in the scientific literature on model validation it is that model validity cannot be established in general but only with respect to a specific purpose for which the model is intended to be used model validity is the adequacy of a model for its intended purpose e g forrester and senge 1980 gass 1983 mccarl and apland 1986 oreskes et al 1994 barlas 1996 kydland and prescott 1996 rykiel 1996 beck et al 1997 jakeman et al 2006 deichsel and pyka 2009 augusiak et al 2014 edmonds et al 2019 the purpose of any scientific simulation analysis is to answer a research question scientific answers result as conclusions from scientific argumentation and are accepted if the conclusions can be validly derived from accepted premises mccloskey 1983 hands 2001 scientific objectiveness is ensured by transparently subjecting all premises and deductions to critical scrutiny and peer review klappholz and agassi 1959 caldwell 1991 longino 1992 in its most generic form scientific arguments that employ simulation modelling conform to the following logical proposition troost and berger 2020 major premise a if a simulation s fulfils conditions u and results in y for inputs x we can conclude z s u s r s x y z minor premise b our simulation t results in y for inputs x and fulfils conditions u r t x y u t conclusion we conclude z z by a b and modus ponens premise b is a conjunction of two premises the first premise r t x y our model results in y for inputs x is supported by result analysis showing that the second premise u t our simulation analysis fulfils conditions u holds is what is typically understood as validation a typical example we conclude z climate change will increase poverty among farming households if r t x y simulated farm agent income is lower in climate change scenarios than in the baseline the necessary condition u s is very often formulated as the model employed in our simulation analysis provides sufficiently reliable predictions of y x in the real world system empirical output validation and structural validation test whether a simulation t fulfils this or a very similar formulation of u s but they in turn rely on further necessary premises these premises will be discussed in the following two subsections the third subsection emphasises the role of uncertainty analysis for sound and robust conclusions showing sufficient reliability in the fourth subsection we highlight that simulation analysis may also rely on differently formulated conditions u s that allow for more useful conclusions in some contexts 2 1 premises of behaviour based inference including empirical validation the key underlying premise of any form of inference from the comparison of model and observed system behaviour is predictive performance of a model in observed situations can be generalised to the target situations i e the system situations relevant for the research question this premise is trivially fulfilled if the target situation is part of the observed situations in sample setting however very often the simulation purpose is to anticipate 3 3 this purpose can be called prediction projection scenario analysis counterfactual simulation forecast or just simulation depending on context a more detailed discussion follows in section 3 1 system behaviour for target situations life after climate changed in our example that have not been fully observed or in other cases to find a generalisable model that explains mechanisms governing system behaviour in many target situations explanation edmonds et al 2019 direct generalisation of behaviour i e observed x y relationships between system input and output including the strength of this relationship predictive performance from observed to unobserved situations relies on the two premises that the observed sample is redundant enough to control for sampling error and the target situations are part of a statistical population for which the observed sample is representative representative sample setting these basic statistical preconditions of representativity and control for sampling error apply to any form of model inference from observed behaviour behaviour based inference inverse modelling whether parameter values estimation calibration or model structures data driven model selection are selected or predictive accuracy is estimated and compared to some implicit benchmark goodness of fit evaluation or between training and test samples cross validation 4 4 the latter two are most often associated with the term empirical validation both are behaviour based inference methods because they are used to select accept a model by comparison to other models sample averages in the simplest case see section 3 2 and 3 3 1 if not satisfied the search typically continues until a better model is found in terms such as calibration validation the second word typically refers to the second stage in a simple two sample cross validation within that cross validation process the calibration and validation stage each have their separate roles but together they constitute a method for model selection this narrow meaning of validation is not to be confused with the comprehensive idea of validation as evaluating model adequacy for purpose advocated in this article which involves the adequacy of a model selection inference method in all cases ignoring sampling error and non representativity bias leads to the generalisation of spurious unsystematic confounded or unstable relationships overfitting that causes inaccurate and misleading out of sample predictions and makes the inference invalid browne 2000 forster 2000 hansen and heckman 1996 sampling error is the unavoidable unsystematic error caused by using a sample and not the full population it can potentially be reduced by increased sampling rates williams et al 2022 non representativity occurs due to a biased sample which can be caused by different sometimes subtle reasons including attrition self selection survivorship or failure bias observer bias and unobserved heterogeneity vandecasteele and debels 2007 gangl 2010 gormley and matsa 2014 jager et al 2020 smith 2020 while some minor biases may be corrected by statistical means structural breaks non stationarity or regime shifts such as climate change substantially alter statistical x y relationships causing extreme sample bias observed and target situations are so fundamentally different that they must be considered different statistical sub populations non representative sample setting and direct generalisation is not possible perron 2006 andersen et al 2009 leamer 2010 filatova et al 2016 verstegen et al 2016 in non representative sample settings anticipation of system behaviour for unobserved situations has to rely on structural knowledge about internal system processes see next section nevertheless a sample can still be useful for indirect generalisation structural knowledge often admits alternative model formulations or parameter values candidates even if a sample is not representative of the target situations it can help discriminate between candidates if it is representative and sufficiently redundant for selected situations in which the candidates imply clearly distinguishable behaviour generalisation to a target situation then relies exclusively on structural knowledge embodied in the chosen candidate whereas observed behavioural data only contributes indirectly by selecting this candidate 5 5 similarly indirect generalisation occurs if the output variable of interest has not been observed itself and a model is indirectly tested using another related output variable generalisation of the variable of interest then relies on the premise that the structural knowledge embodied in the model correctly relates the two variables importantly the predictive accuracy measured in the sample cannot be straightforwardly generalised to the target situation in these cases as even systematic differences in prediction errors between sample and target situations cannot be ruled out preconditions for reliably discriminating between candidates are structural and practical identifiability bellman and åström 1970 cobelli and distefano 1980 stigter et al 2017 guillaume et al 2019 structural identifiability means that different candidates are not observationally equivalent i e do not imply the same system behaviour in the observed situations even a fully representative and redundant sample is not able to distinguish between models that predict the same output for the same input 6 6 structural identifiability in our understanding subsumes also the problems of endogeneity often encountered in econometrics practical identifiability means that the variation in the observational data in connection with statistical assumptions e g on representativity and the form of model errors is sufficient to unambiguously attribute effects to the individual parameters of a given model structure sampling error confounded input variation correlated variables multicollinearity unobserved heterogeneity and omitted variable bias are key obstacles for unambiguous model selection and parameter estimation more complex models require more data or more restrictive prior assumptions on parameters to be practically identifiable browne 2000 burnham and anderson 2004 polhill and salt 2017 two candidates that cannot be discriminated by given data are termed equifinal beven and freer 2001 2 2 premises of structure based model choice and structural validation structure based simulation is essential to anticipate behaviour for target situations for which direct generalisation from observed data is not possible and to derive structural explanations of system behaviour structure based simulation deduces system reaction from existing knowledge about system components and their interactions it is sometimes argued that such a deductive process does not create new information however as frisch 1933 argued the key contribution of quantitative modelling is to analyse the interplay of processes and compare the magnitudes and directions of their individual effects in relation to each other in order to deduce the behaviour of the whole system this anticipated or emergent behaviour is new information that was not obvious from looking at existing knowledge on individual processes in isolation the key premise of structure based modelling and structural validation is a model that contains a sufficiently complete and accurate representation of the internal structure and processes of a system is expected to predict system behaviour well structurally assessing the premise of sufficient completeness is often complicated by incomplete knowledge of the system and its potential reconfigurations in addition modellers are typically forced to strike a balance between completeness and efficiency striving to include all relevant processes while omitting unimportant ones that complicate the model construction forrester and senge 1980 assessing the premise of sufficient accuracy in the representation of individual processes is the subject of micro validation moss and edmonds 2005 windrum et al 2007 midgley et al 2007 arnold et al 2015 ghaffarian et al 2021 some structural processes and their parameters may be directly observable and measurable others however may have been generalised from observed subsystem behaviour by behaviour based inference in which case the preconditions discussed in section 2 1 sample representativity identifiability and control of sampling error apply the inclusion of estimated model components into a composite model requires ensuring that the observations from which they have been generalised are representative for all contexts for which they are applied in the composite system 2 3 uncertainty analysis the premises for robust conclusions given the statistical nature of model inference and the typically incomplete nature of structural knowledge discussed in the previous subsections simulation analysis is practically always subject to uncertainty just showing that one particular model results in a specific output for a particular input is hence not convincing it invites the immediate criticism that plausible alternative models might show different results rather it must be shown that the final conclusions towards the research question are robust and not affected by uncertainty and bias van asselt 2000 walker et al 2003 saltelli et al 2013 fischhoff and davis 2014 berger and troost 2014 troost and berger 2015a marchau et al 2019 this implies firstly that the type and degree of uncertainty and bias that are compatible with conclusion z must be carefully specified in the major premise secondly it is a necessary subpremise of u s that implications of uncertainty in structural knowledge and uncertainty in model inference from data and in predictive analysis uncertainty in the anticipated input for target situations and their effects on results have been carefully assessed 2 4 alternative basic premises not every scientific argument using simulation analysis is based on the premise that the model provides reliable predictions of y x in the real world system edmonds et al 2019 have noted that some types of analysis e g theoretical exposition do not require any immediate claims about the relation of the model to reality at all or put more emphasis in representing stakeholder s views of the system a subtler relation is discussed by troost and berger 2020 p 6f who use the following hypothetical abm application economic policy analysis often works in a normative context policy makers need to justify actions with respect to established societal values norms or ideologies for example they might work in a political setting in which the state is supposed to safeguard minimum living incomes but only to interfere in economic processes if market participants are not at all able to help themselves assume that in this context analysts build their abm to simulate the adaptation of farmers to climatic change and model each farm agent decision as a rational optimisation problem with perfect anticipation of projected climatic impacts on production and market conditions in addition farm agents are embedded into a social network of mutual solidarity in which agents less affected by climatic extreme events indiscriminately help the severely affected ones analysing their simulations the analysts find that their optimising farm agents become food insecure under projected impacts they conclude that if perfectly foresighted optimising agents in a perfectly functioning social solidarity network do not fare well real world farmers are even more unlikely to do so and should receive government help as troost and berger 2020 observe the model would likely not pass conventional structural and empirical validation key modelled processes do not correspond to our best knowledge of their real world counterparts in reality farmers do not behave as fully rational optimisers with perfect foresight and networks typically discriminate by family ties ethnicity etc the model will almost surely overestimate observed farm incomes in the past nevertheless the conclusions would withstand such criticism because accurately predicted farmer or network behaviour is not a relevant premise of the argument here in this case the premise that would need to be challenged in validation is that the model calculates the best possible reaction in economic terms empirically this could be done for example by searching for observed cases for which the model predicts worse than observed outcomes one might also identify other unexpected deviations e g larger farm holdings having higher per area incomes than smaller ones which might be observed in the data but not in the model or vice versa and that are not expected to be caused by imperfect optimisation of real world farmers alone nevertheless even if the intention is not to show accurate prediction premises on representativity sampling error and identifiability also apply here structural validation could for example assess whether assumed constraints are overly pessimistic or alternative production safety or income options that might become available with climate change have been omitted troost and berger 2020 further observe that if for contrast the analysts find that their computational agents fare well it would be a logical fallacy to conclude that real world agents will fare well based on the same premises such an argument would require different premises that are much more difficult to support using a model with a clear upward bias both cases use the same model in the same empirical context towards the same motivating research question this illustrates that to judge a model s adequacy we require a very precise definition of its empirical context and the exact argumentative premise it is supposed to support 3 a protocol for ensuring validity in agent based simulation summarising section 2 validation means ensuring the adequacy of simulation analysis for answering a specific well defined research question and such adequacy requires i laying out a logically valid argumentative structure on which potential conclusions from simulation towards the research question can be built ii choosing model components and methods of inference and evaluation that i fit the requirements implied by this argumentative structure and ii rely only on preconditions regarding observation data system properties and structural knowledge that are fulfilled in the given context iii carefully assessing whether the simulation results and specifically their uncertainty and bias are consistent with the requirements of the argument points i iii imply that adequacy is relative to a modelling context which consists of the purpose research question and the available knowledge and data about the modelled system validity cannot be ensured by examining model structure and behaviour ex post only ensuring it requires assessing the adequacy and mutual consistency of choices at all stages of the modelling process given the diversity of contexts in which abm are applied it will be impossible to identify one fits all model structures or statistical methods for all abm and assessments of adequacy need to be able to cover a broad set of possible contexts taking this into account in the following sections we propose a protocol fig 1 of 12 steps covering and linking all stages of simulation analysis the protocol helps characterise the modelling context part i section 3 1 guides the choice of context adequate methods based on this characterisation part ii section 3 2 and emphasises the documentation and consistent propagation of uncertainty through the modelling process so that finally the robustness of conclusions can be comprehensively assessed part iii section 3 3 the protocol itself is provided in tables 1 4 and 6 while the sections in the main text explain the rationale for each step where available we list formal methods of analysis with useful references and highlight the premises for their applicability the concept of the protocol involves eleven dimensions marked by letters a k in fig 1 that characterise modelling contexts and determine adequate choices of models and methods six of these represent requirements of the research question fig 1 a f that can be determined already at the beginning of the modelling process while the other five fig 1 g k require a more in depth analysis of the relationship between research question and system knowledge and data during the modelling process for a better overview the numbering in fig 1 links the main stages of the modelling process blue boxes and context dimensions grey boxes to the associated steps of the protocol the classification and propagation of uncertainty is indicated in red 3 1 part i defining the modelling context the first step is to characterise the modelling context the precise research question and the knowledge and data that are available about the system being modelled table 1 3 1 1 step 1 precisely define the research question a research question typically arises from a larger debate discourse or decision problem for example a public political or scientific debate a participatory planning problem or an economic decision problem a research question to be addressed by simulation analysis is supposed to contribute to this debate even if answering it may not necessarily resolve the whole debate useful contributions can comprise very different questions edmonds et al 2019 epstein 2008 e g detailed precise forecasts of future states of the world statistical testing of explanatory models but also exploring and stress testing possible consequences of decision options berger and troost 2014 lempert 2019 or purely theoretical questions concerning hypothetical models themselves theoretical exposition in the sense of edmonds et al 2019 it is paramount to be clear about what precise question the simulation analysis is supposed to answer and what precise argument it could contribute to the debate 3 1 2 step 2 characterise requirements implied by research question while typologies of model purposes exist e g edmonds et al 2019 epstein 2008 the understanding of commonly employed terms such as prediction forecast projection exploration differs between scientific disciplines often they are used inconsistently bray and von storch 2009 and all lack the necessary precision on some aspects relevant for methodological choices instead table 1 a defines six dimensions to precisely describe the requirements imposed by a research question the most basic consideration is the focus of interest does this lie in anticipating system behaviour in specific situations 7 7 such as in prediction scenario analysis counterfactual simulation projection or forecasts output focus or in describing or understanding system structure 8 8 such as in explanation causal identification or description structure focus carefully defining the target situations is a necessary precondition for judging the degree of generalisation in the next step required resolution required transparency as well as computational resource constraints impose limits on a priori model selection judging the robustness of conclusions requires understanding the required precision and accuracy tolerable uncertainty in simulation outcomes at this point it is often not yet possible to formulate this quantitatively e g 2 deviation is acceptable and should be done in terms of consequences on conclusions e g uncertainty should not affect ranking of policy alternatives by evaluation criteria together these dimensions define requirements that the simulation analysis aims to fulfil whether this is actually possible can only be judged at the end of the modelling process see section 3 3 and table 7 3 1 3 step 3 identify knowledge and data about structure and behaviour of the modelled system in addition to the research question the modelling context is defined by the available information about the simulated system in the form of structural and process knowledge available observations of system behaviour input output trace data as well as in the case of an output focus the anticipated system input data for target situations the next step is to identify which data information and knowledge are available can be obtained with reasonable effort or will remain unattainable for the analysis e g input output observations of far future system states table 1b 3 2 part ii context adequate model and parameter selection and uncertainty documentation appropriate simulation models can be selected in two steps in a first structural step a set of candidate models and candidate parameter sets is constructed or identified whose theoretical characteristics comply with structural system knowledge and the requirements implied by the modelling context steps 4 6 tables 2 and 3 a set of multiple candidates fulfilling the requirements represents the prior model uncertainty 9 9 while we use terminology prior posterior uncertainty borrowed from bayesian statistics here this does not mean that this uncertainty can necessarily be cast into a formal prior probability distribution more often than not it cannot and it may well only be qualitative descriptions of uncertainty cf also beck at al 1997 for this general use steps 7 8 tab 4 in a potential second step behaviour based inference can possibly be used to ascribe empirical likelihood to the candidates rank them and narrow down the candidate set reducing prior to posterior model uncertainty beck et al 1997 step 9 table 4 ideally the two steps complement each other the first step is key to ensure that only adequate candidates are considered in behaviour based inference omitting this theory based preselection can only be adequate if the simulation analysis is output focused and the modelling context allows for the direct generalisation of statistical relationships namely the expected predictive accuracy to the target situations representative and sufficiently redundant data step 6i table 3 only in this specific case expected out of sample predictive accuracy and practical identifiability can be derived solely from the data and are sufficient criteria for model selection polhill and salt 2017 nevertheless even for these direct generalisation cases incorporating structural knowledge in chosen candidate models becomes more essential the scarcer the data a defensible structure based error model specification and pre selection of candidate models increases practical identifiability see e g troost et al 2022 for the second step it is key to ensure the adequacy of the inference process itself steps 7 9 table 4 do the necessary preconditions discussed in section 2 1 hold in the given modelling context is the specific method chosen appropriate for the context is uncertainty properly considered and documented if not behaviour based model inference is clearly not adequate 3 2 1 step 4 representativity of data and degree of generalisation the first step in model selection is to contrast the observed or observable data with the target situation of the research question to determine the degree of generalisation and extrapolation implied following the considerations on representativity and constancy regime shifts structural breaks stationarity discussed in section 2 1 this analysis requires a basic system conceptualisation not yet a full conceptual model that allows judging the system s degree of openness internal stability complexity and stochasticity step 4 table 2 3 2 2 step 5 abm as composite models structuring component context while our protocol addresses modellers that are inclined to use an abm one question to ask in structural model choice is of course whether an abm indeed suits the given modelling context or a different modelling approach is more promising abm are typically composite models model systems which are composed of lower hierarchy models components that mirror relevant subsystems and processes for example they typically contain a model of individual agent behaviour based on the internal state of and external influence on the agent this submodel for agent behaviour in turn may itself be a composite of lower hierarchy components e g for learning demographics and economic decisions schlüter et al 2017 abm also typically contain models of agent interactions e g communication markets auction collective action or network models schreinemachers and berger 2011 in addition many abm in natural resource management link to biophysical components that model responses of natural systems e g a crop field or watershed to agent intervention arnold et al 2015 10 10 whether the overall composite model is labeled as abm or the abm is itself considered part of the integrated composite is irrelevant the discussed considerations apply in both cases system behaviour in an abm emerges not only from the interactions between agents but conceptually also from the interactions of individual model components in general such structure rich composite models are typically used for structure focused analysis or for output focused analysis when direct generalisation from observed data is not possible nolan et al 2009 voinov and shugart 2013 in direct generalisation contexts prediction is often achieved more efficiently with statistical or machine learning models polhill and salt 2017 11 11 this does not imply that abm cannot be used for direct generalisation contexts there may often just be more efficient approaches the adequacy of a composite model relies on i an assembly of components that together fulfil the relevant premises for the overall research question to be answered ii a careful assessment of the adequacy of each lower hierarchy component for its intended role in the composite and iii a consistent consideration of the uncertainty in each component at the composite level arnold et al 2015 it is important to realise that each component has its specific own question to answer and has its own specific modelling context which may differ considerably from the modelling context of the composite as a whole or that of other components even if the abm is used in an overall modelling context that is not apt for direct statistical inference this does not rule out that within model contexts of lower hierarchy components exist in which representative samples allow for direct generalisation and e g the use of machine learning methods for these components for example we may not yet have observed how a specific group of farmers behaves and fares in a warmer climate so we cannot empirically measure the predictive performance of a composite model that simulates potential future farmer behaviour and welfare we may however be able to include a plant growth component into this composite model that can be tested based on observations and experiments in a range of warmer and colder regions if we consider this range representative for potential future growth conditions troost et al 2020 an important step hence is to structure the overall modelling task into subcomponents and then recursively revisit the steps of the protocol also for each component individually step 5 table 2 this step may often not directly result in the final structure but may involve various iterations through steps 4 10 until an adequate composite structure for the overall modelling context has been established which may or may not involve an abm 3 2 3 step 6 choosing structurally adequate candidate models and prior parameter ranges for each component the guiding questions in table 3 step 6 items i ix help to check potential model component candidates for context adequacy from a structural point of view the table also lists selected literature sources that provide formal tests or more in depth discussions of each question for adequate structure based model selection it is useful to first sketch a comprehensive conceptual system model argent et al 2016 even if not all system processes can or finally have to be included in the simulation model this conceptual sketch can serve as a benchmark to check a candidate s match of the domain of applicability and sufficient completeness of processes for the target situations parker et al 2008 it must be ensured that model structure and parameters fixed in the candidate are also expected to be constant no change over time and invariant unaffected by policy treatment change to target situation in the real world system lucas 1976 engle and hendry 1993 hendry 1996 relevant changes between situations must be captured as exogenous input or result from internal feedback in the model it is not always possible to explicitly simulate all potential real world feedback in the model itself but it should then at least be possible to capture potential feedback as changing boundary conditions that may then later be assessed in uncertainty analysis troost and berger 2015b troost et al 2022 table 3 ii iv expected deviations i e the part of the system behaviour that is not explained or predicted by the model from a theoretical point of view should be consistent with the precision and accuracy required by the research question table 3 v research questions requiring accuracy with respect to an absolute reference necessitate not only a high degree of model completeness with respect to all systematic processes but also with respect to probability distributions for unsystematic effects as well as reliable system input data for target situations research questions requiring accuracy only with respect to the relationships between simulated target situations demand model completeness only with respect to systematic differences simplifying assumptions such as optimising agents in our example introduced in section 2 4 may lead to systematic over or underestimation bias this is not problematic as long as major conclusions drawn from the simulation analysis will not depend on such simplification robustness to the relaxation of simplifying assumptions no model artefacts 12 12 the lucas critique lucas 1976 is a famous example in economics for a challenge to modelling practice based on these grounds 13 13 conclusions that are based on comparing model results to asymmetrical one sided thresholds even get stronger if the methodological approach is biased against them conversely they are weakened by biases in their favour especially if these cannot be precisely quantified and corrected this principle mirrors the conservative rationale in statistical hypothesis testing type ii errors false negatives are preferred over type i errors false positives logical consistency correct technical implementation and fit to the required resolution transparency and resource constraints are obvious preconditions that must be assessed even if the component context allows for direct generalisation table 3 vi ix 3 2 4 steps 7 and 8 documenting prior and input data uncertainty and assessing structural identifiability structure based model selection typically results in a number of plausible model structures and parameter values this prior uncertainty should be documented even if not all plausible alternatives can be implemented and tested step 7 table 4 the first step in determining whether behaviour based inference can reduce this prior uncertainty then is to assess the structural identifiability of candidates in the observed range of data i e check whether the behaviour of candidate models differs in the domain for which the data is representative step 8 table 4 a variety of analytical and numerical approaches to assess structural identifiability are available guillaume et al 2019 chis et al 2011 including numerical parameter screening methods from sensitivity analysis campolongo et al 2007 troost and berger 2015a not only uncertain parameters and structure in the model itself but also uncertain auxiliary parameters or assumptions e g error distributions for expected deviations and measurement error in input data imputation to deal with incompleteness in the data 14 14 a frequently encountered example in agricultural abm would be a parameter used in imputing cash reserves of farm agents which are typically unobserved or undisclosed at simulation start from observed characteristics such as farm size location land use or livestock ownership alternative choices in data curation preparation or aggregation must be documented and considered when assessing identifiability structural identifiability may differ between parameters of the same model some parameters can be structurally identifiable in the available data see appendix a 1 while others are not and their uncertainty cannot be reduced by behaviour based inference e g troost and berger 2015a structural non identifiability cannot be resolved by more of the same data but requires either widening the range of situations observed or considering more dimensions of the data 3 2 5 step 9 choosing adequate methods for behaviour based inference and measurement of predictive accuracy if structural identifiability is given or direct generalisation is possible one can choose an adequate method for behaviour based inference step 9 table 4 if not it is sometimes still useful to measure sample predictive accuracy of candidates and compare it against a null model to ensure the models do not completely go astray behaviour based inference requires choosing a loss function a metric to weight deviations between observed and simulated behaviour and an algorithm to characterise the distribution of the loss function over candidates exploration estimation of posterior parameter distribution or find the candidate with the optimal loss function value optimisation calibration 3 2 5 1 adequate choice of loss function or likelihood loss functions step 9i table 4 are used to weight deviations between simulations and observations by severity from a decision theoretic point of view loss functions should more strongly penalise those errors that would lead to stronger changes in conclusions in direct generalisation cases and when sampling error has been controlled for e g by cross validation see below the measured loss can be directly generalised to target situations hence in this case one can choose a loss function that is limited to output variables of interest and whose weighting directly reflects the precision accuracy relativity and symmetry required by the research question see step 2 penalising misclassifications based on their practical implications e g prefer models with stronger deviations overall but high reliability in critical areas manderscheid 1965 berger 1980 mccloskey 1985 farahmand et al 2017 manski 2019 15 15 in the direct generalisation case if we are interested in predicting deforestation for example then we can focus on the ability of the model to predict changes from forest to some other land use without caring whether it also correctly predicts the new land use or changes among non forest land use classes we thank judith verstegen for this example in indirect generalisation cases and structure focused analysis loss functions must reflect the impact of model errors on our confidence that the candidate reflects underlying system processes in this case loss functions should reflect the expected deviations of the model including sampling error model bias and error correlation schoups and vrugt 2010 regarding all observed output variables linked to the modelled mechanisms 16 16 in the indirect generalisation case even if we are only interested in predicting deforestation but the mechanisms that we have to trust to anticipate developments in unseen situations are supposed to also determine changes in other land uses accurately then deviations in predictions of these other variables also undermine our trust in predicting deforestation since we cannot assume that predictive accuracy on deforestation observed in the sample is the same in the future this holds even if prediction of deforestation in the sample is accurate theoretically anticipated deviations of candidate models are considered less severe than deviations unlikely to occur if the model predicts according to its theoretically expected precision hansen and heckman 1996 blavatskyy and pogrebna 2010 for example if a model is designed to predict an upper bound underestimation of observations should be penalised overestimation not 17 17 bayes estimators allow combining a loss function for relevant errors in model application with a likelihood for the posterior probability of the model bassett and deride 2019 if the model is expected to be well specified and implies a well defined tractable stochastic error distribution a parametric likelihood function can be formulated using parametric likelihoods in cases where their underlying assumptions are not fulfilled or in doubt leads to biased model selection and overconfident conclusions beven et al 2008 stedinger et al 2008 robust loss functions allow for occasional outliers potentially generated by processes not captured in the model willmott and matsuura 2005 hyndman and koehler 2006 if the model is expected to capture the essential systematic relationship but the exact error distribution is unknown or intractable summary statistics that capture relevant systematic relationships can be estimated on both observations and model output a loss function can then be applied to the difference in the summary statistics rather than the individual observations classical and bayesian indirect inference chen et al 2012 beaumont 2010 drovandi et al 2015 pattern oriented modelling generalises this principle to incorporate more qualitatively described strong and weak statistical patterns grimm and railsback 2012 in other cases qualitative criteria are used to define binary valued acceptance functions spear and hornberger 1980 troost and berger 2015a often absolute goodness of fit measures e g model efficiencies are used instead of pure loss functions or likelihoods step 9ii table 4 while the latter provide a relative ranking between candidate models but their absolute values are specific to the sample used absolute goodness of fit measures don t change the relative ranking but take the sample variance into account in order to allow comparison between models estimated from different samples bennett et al 2013 hauduc et al 2015 implicitly efficiency criteria compare the evaluated model with a benchmark or null model that employs only basic information of the data r2 and model efficiency for example contain the sample average as a null model this null benchmark should be carefully chosen the sample average is only one possible choice trend extrapolation random allocation or seasonal or group specific averages are often more adequate benchmarks schaeffli and gupta 2007 pontius and millones 2011 as an alternative grimm and railsback 2012 suggest to always explicitly include a benchmark null model among the candidates 3 2 5 2 adequate assessment of practical identifiability and posterior uncertainty it is paramount to document uncertainty in measured predictive accuracy and model rankings and to assess how reliable the data could discriminate between candidates practical identifiability step 9 iii iv table 4 methods for behaviour based inference considerably differ in the extent to which uncertainty in the selection process is characterised and to which prior uncertainty is considered and it is important to select a combination of method s whose premises fit the application case table 5 for example classical minimum loss or maximum likelihood based parameter estimation presuppose that both the likelihood and the model structure are certain and correctly specified and all considered candidate parameterisations are a priori equally likely stigler 2007 they identify one best fitting model and limit quantification of posterior uncertainty to confidence intervals for parameters while large confidence intervals point to low practical identifiability they cannot conceptually be interpreted as posterior probabilities for parameters bayesian frameworks hobbs and hilborn 2006 can overcome the latter limitations if prior probabilities are specifiable k fold cross validation 18 18 the traditional separation of data into one training and one validation dataset is the most basic form of cross validation but is subject to sampling error itself k fold cross validation is the more robust extension is the essential non parametric method to quantify sampling error in estimated expected loss or predictive accuracy for unseen situations from a sample browne 2000 arlot and celisse 2010 bennett et al 2013 vehtari et al 2017 it should be combined with any of the basic inference methods and also avoids the complexity bias when model structures are uncertain selecting model structures purely based on predictive accuracy measured in one sample is biased towards models with a higher number of freely adaptable parameters which increases the danger of overfitting adequate model inference requires correcting this bias e g by k fold cross validation only when parametric likelihoods are applicable see above information criteria aic bic dic waic or formal bayesian frameworks with appropriately specified prior likelihoods burnham and anderson 2004 ward 2008 vehtari et al 2017 provide an alternative statistical diagnostics for influential observations e g cook s distance and multicollinearity in the data e g variance inflation factors common in econometric analysis should complement the analysis of posterior uncertainty 3 3 part iii adequate derivation and interpretation of simulation results and uncertainty fig 1 illustrated how an adequate modelling process structures quantifies and potentially reduces uncertainty the definition of a research question divides uncertainty regarding the research question from uncertainty about wider implications in the debate theory based model selection structures the uncertainty about the research question into prior model uncertainty represented by different candidate model structures and parameter ranges input uncertainty uncertainty in boundary and initial conditions expected deviation error terms bias aleatory uncertainty and unmodelled uncertainty alternative models not included in the analysis 19 19 brenner and werker 2007 emphasise an inclusion of all logically possible parameter values and model structures consistent with structural and empirical knowledge we recognise that this is often not feasible in practice however this needs to be acknowledged as unmodelled uncertainty and appropriately discussed when deriving conclusions processes that have been ignored potential exogenous events not considered non formalised error terms unforeseeable events critical assumptions for which no alternatives are tested etc if applicable and successful behaviour based inference potentially reduces prior model uncertainty to posterior model uncertainty if discrimination of candidate models by data is not possible the posterior uncertainty remains the same as the prior uncertainty in structure focused analysis description explanation the resulting posterior model uncertainty is already the final uncertainty to be interpreted for conclusions in output focused analysis prediction scenario analysis exploration posterior uncertainty and input uncertainty still need to be translated into predictive uncertainty for target situations e g future or policy scenarios by simulation experiments that include uncertainty analysis in an adequate modelling process in which uncertainty is properly analysed and propagated the final posterior predictive uncertainty and the unmodelled uncertainty describe the actual state of knowledge regarding the research question that can be defensibly extracted from the available data and structural system knowledge this final model uncertainty can then be compared with the precision required by the research question for interpretation and derivation of conclusions 3 3 1 step 10 interpretation of predictive accuracy and posterior uncertainty if sampling error has been properly controlled for e g by cross validation expected predictive accuracy indicates how well the model predicts or explains the variation in the population of situations for which the sample is representative subject to the importance weighting embodied in likelihood or loss function this is valuable information in its own right however whenever using this information to draw further conclusions step 10i table 6 e g about the model being sufficiently good or the correct or best explanation care has to be taken oreskes et al 1994 even though absolute goodness of fit measures such as model efficiencies project predictive error onto an absolute scale between null model and perfect fit defining any threshold to indicate sufficient fit on this scale remains subjective or based on convention similar to significance levels in statistical analysis unless this threshold can be convincingly derived from the research question pontius and millones 2011 the same holds for thresholds defined on posterior densities or relative differences in information criteria stephens et al 2005 the well known problems of induction under determination and theory ladenness imply that proving by comparison to observation that a model is the true model is ultimately impossible oreskes et al 1994 quine 1951 expected predictive accuracy provides a relative ranking and allows identification of the best among the candidate models for the given sample the more comprehensive the list of candidate models and parameterisations that has been tested and the more representative the sample the higher can be the confidence in having identified a generalisable best model or parameterisation as all other statistical relationships measured expected predictive accuracy cannot be generalised to target situations across structural breaks uncertainty in inference can be quantified as a posterior probability for the candidates if a formal bayesian framework with proper prior probabilities and appropriate likelihood has been used in inverse modelling however also in those cases where posterior probabilities or credible intervals cannot be derived it is important to consider posterior uncertainty step 10ii table 6 and recognise that the best model does not necessarily have or even approach a posterior probability of one troost and berger 2015a the potential explanatory and predictive power of alternatives should not be neglected in interpretation if the analysis is structure focused and interested in which model provides the better explanation it remains inconclusive whenever two alternative models cannot be robustly discriminated by data or needs to employ additional theoretical considerations e g parsimony as an epistemological principle 20 20 parsimony as a epistemological principle simpler models are always to be preferred differs from a pragmatic argument for parsimony in estimating models for prediction simpler models are less prone to overfitting or correspondence to established theory to justify a decision for one or the other model in output focused analysis subsequent predictive simulation should use the full posterior distribution consider confidence or credible intervals or at least a representative ensemble of all candidates that show nonnegligible explanatory power ensemble modelling model averaging 3 3 2 step 11 analysis and interpretation of predictive uncertainty only in rare cases it will be permissible to directly generalise expected predictive uncertainty from behaviour based inference to the target situation preconditions representative sample negligible input uncertainty one clearly best model generally predictive uncertainty for a target situation is a function of the uncertainty about the systematic effect of system input on behaviour that is captured in the set of models and parameterisations posterior model uncertainty the model error bias and unsystematic aleatory uncertainty and the uncertainty in system inputs e g scenarios boundary conditions for target situations building on the considerations by marchau et al 2019 and walker et al 2003 table 7 lists which forms of predictive simulation outputs are adequate depending on the level of uncertainty in each of these dimensions unconditional predictions require low uncertainty in all locations of uncertainty for all higher levels of uncertainty comprehensive uncertainty analysis is necessary step 11 table 6 depending on model complexity and available computational resources one can choose from a considerable number of approaches for efficient uncertainty and sensitivity analysis 21 21 following the definition of helton et al 2006 uncertainty analysis is concerned with quantifying the uncertainty variance in simulation outputs while sensitivity analysis is concerned with linking this uncertainty to uncertainty in model inputs i e determine which uncertain input factors are responsible to which degree for the uncertainty in outputs helton et al 2006 saltelli et al 2008 gramacy and lee 2009 troost et al 2022 clear conditions for appropriate choices have been formulated uncertainty analysis must be global i e cover the full range of potential input values including interactions and correlation between input factors saltelli and annoni 2010 probabilistic predictions require probability information in all locations it is key that exploration of predictive uncertainty focuses on the output quantity precision and resolution relevant to answering the targeted research question when we compare two target situations we can distinguish the apparent or observable difference i e the difference between two predictions that includes unsystematic stochastic effects and the systematic difference i e the difference between two predictions controlled for unsystematic effects in many decision support situations the future may not be precisely predictable but for a good decision it is enough if the systematic differences caused by decision options can be pointed out using pairwise comparison at each tested combination of input factor values berger and troost 2014 for stochastic models this requires common random numbers schemes stout and goldie 2008 troost and berger 2016 the alternative is running sufficient repetitions and applying statistical comparison tests e g verstegen et al 2019 22 22 common random number schemes are more efficient in terms of required model runs but sometimes quite difficult to implement see example in troost and berger 2016 especially when uncertainty is high in all locations rather than trying to merely describe all possible outcomes strategies to detect decision options that are robust under many different scenarios and assumptions should be emphasised assumptions based planning stress testing red teaming lempert 2019 marchau et al 2019 3 3 3 step 12 interpretation and conclusions the interpretation of results should compare the final uncertainty step 10 or 11 to the required precision and accuracy of the research question step 2 if the required certainty is reached conclusions that are consistent with the simulated output can be considered valid and sound if uncertainty is too high we have to conclude that the knowledge employed in the process is insufficient for the desired type of conclusions e g carauta et al 2021 it should not be necessary to emphasise that this is an equally valuable and relevant result leamer 2010 the structure of the argument and the premises that are critical to support the conclusions must be clearly laid out step 12 table 6 this involves the premises that are supported by simulation results but also the auxiliary and hidden premises prior model evidence representativity of data identifiability posterior uncertainty both unstructured uncertainty about wider implications step 1 and unmodelled uncertainty step 7 remain qualitative and unquantified in the modelling process nevertheless they must be an important part of the interpretation conclusions must be qualified with respect to the information omitted from the modelling process hypotheses on how omitted processes or alternative system conceptualisations could affect conclusions must be discussed forrester and senge 1980 banerjee et al 2016 argue for an explicit and structured section for speculation about external validity generalisability of results obtained from case studies especially when using models to inform decision makers in the face of deep uncertainty transparent documentation of critical and potentially value laden fundamental assumptions see protocols in kloprogge et al 2011 saltelli et al 2013 fischhoff and davis 2014 van der sluijs 2017 and additional effort to assess the robustness of decision option outcomes to these assumptions is essential lempert 2019 marchau et al 2019 4 discussion and conclusions the purpose of validation is to ensure the adequacy of simulation analysis for answering a specific well defined research question this requires a careful analysis of the logical argumentative structure and assessment of the critical premises that conclusions from simulation analysis build upon such premises rest on simulation outcomes but are also implicit in the choice of models and methods of inference from data especially the latter is not always obvious to modellers reviewers and addressees of simulation results for example empirical validation and model inference presuppose representativity of data identifiability and control of sampling error moreover specific methods such as maximum likelihood estimation rely on even more restrictive not always obvious premises see tables 5 and 7 validation needs to ensure that models and methods chosen fit the modelling context which comprises the research question and available system knowledge and data on system behaviour and it needs to assess whether the final uncertainty in simulation results fits the requirements on precision and accuracy implied by the research question in most cases this is more complex and subtler than a single step matching of context to a method rather it is a hierarchical process i e outcomes of earlier steps affect choices in later steps e g behaviour based inference should not be pursued without first ensuring representative data and structural identifiability it is recursive i e in composite models such as abm the context of each component must be assessed and iterative i e outcomes of subsequent steps may encourage receding a number of steps and reconsidering choices for example if the evaluation of structural identifiability practical identifiability or predictive uncertainty leads to unsatisfactory results it may be useful to go back to structure based model selection or even to a redefinition of the research question it may be possible to answer a more restricted question that is already useful where the context does not allow to reliably answer the original question the kia protocol that we have proposed in this article is intended to guide modellers in making adequate choices during the process of simulation analysis and justify them with adequate argumentation it provides a guideline to reviewers who can use it by starting from the final conclusions and their premises and working backward to evaluate whether the steps taken during the modelling process adequately support the premises in the given context moreover it is intended to structure documentation i as a checklist to ensure modelling context and justification for all relevant modelling decisions have been discussed in the main body of an article and ii as a template for well structured tabular documentation in an appendix the protocol mirrors and is compatible with established recommendations for a structured modelling process e g jakeman et al 2006 but it emphasises the linkages and propagation of uncertainty between modelling stages and highlights general criteria for the choice of adequate methods at each stage it operationalises the principle as empirical as possible as general as necessary coined for abm by brenner and werker 2007 it incorporates the different levels of uncertainty of walker et al 2003 and marchau et al 2019 but also explains how this uncertainty comes about in the modelling process similar to polhill and salt 2017 it highlights the importance of structural model choice compared with purely data driven model inference while we have not extensively discussed stakeholder participation the protocol is meant to be open to valuable stakeholder input and feedback at any step of the process e g in shaping the encompassing debate defining the targeted research questions providing information in model selection and inference and shared interpretation voinov et al 2016 barreteau et al 2010 the exhaustive discussion of many of the guiding questions listed in the tables of the protocol would warrant their own articles our intention here has been to comprehensively list them and highlight their interlinkages we have linked many of the guiding questions to literature with more detailed explanation or formal assessment methods this list of methods does not claim to be complete and it will certainly have to be extended over time as new approaches for model testing selection or estimation are developed to deal with the formulated questions we actually hope that this protocol sparks interest in developing new methods and then assists in clearly communicating the conditions for which they are suitable in defining eleven dimensions for the characterization of modelling contexts we have moved beyond discrete typologies of model purpose e g edmonds et al 2019 epstein 2008 typologies such as edmonds et al 2019 and especially terms such as prediction forecast projection or exploration whose understanding and usage differ between and sometimes even within disciplines bray and von storch 2009 can be mapped onto these dimensions to allow for more precise communication see appendix a 2 the dimensions are intended to improve communication on methodology by helping to identify which abm applications share a similar modelling context and might learn from each other and which not for example troost and berger 2015a and carrella et al 2020 both deal with unknown or intractable likelihoods for model inference however the former face both low structural and practical identifiability while the latter assume few parameters and a large number of identifying summary statistics i e high practical identifiability as both are explicit about the assumed modelling context this can be read from their articles but may still be easily overlooked our protocol is intended to highlight these differences and in this way avoid common pitfalls in discussions between modellers and reviewers about adequate and valid model use and inference e g avoid discussions about an appropriate loss function when structural identifiability is the more important issue avoid overemphasis on separation of training and validation data when validation data is not representative for target situations avoid discussions about unreliability of unconditional predictions when these are neither possible nor necessary avoid suggesting model simplification to increase practical identifiability when model complexity is required for structural reasons and direct generalisation is not adequate etc given the breadth of application contexts for abm and their potential components we strived to be general in redacting the protocol we believe that the principles discussed here are applicable to any modelling endeavour and most disciplinary standards that have been established form special cases that are in principle covered by the protocol in this sense we expect that it can be useful for many different types of simulation not only for abm at this point the kia protocol itself is a theory based hypothesis that requires practical testing we propose it to the community of agent based modellers for adoption in model construction documentation and review its use in practice will tell if it proves useful as guidance for model development and a communication device in documentation and review based on practical experience it should then be reviewed and improved declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank all participants of workshop w9 best practice in agent based model parameterisation and validation at the 10th international congress on environmental modelling software 2020 for their valuable and constructive input and comments ct and tb acknowledge funding by the federal ministry of education and research of germany bmbf for the project simlearn 01is19073c ln acknowledges funding from the energy demand changes induced by technological and social innovations edits project provided by ministry of economy trade and industry meti japan gp acknowledges funding by the scottish government strategic research programme 2022 27 project id jhi c5 1 qbl acknowledges support by the cgiar research program on grain legumes and dry cereals crp gldc and initiative on sustainable intensification of mixed farming systems tf acknowledges the support of the european research council erc under the european union s horizon 2020 research and innovation program grant agreement number 758014 author contributions ct conceptualized and wrote the manuscript rh ab hvd tf qbl ml ln jgp zs and tb contributed ideas comments and corrections at all stages of manuscript writing we thank judith verstegen and a further anonymous referee for highly constructive and valuable feedback during the review process appendix a a 1 notes on differences in structural identifiability of parameters structural identifiability in the data can considerably differ between different groups of parameters or model components for example parameters that relate short term agent behaviour to static characteristics can be estimated from sufficiently heterogeneous cross sectional data for contrast parameters that affect dynamic behaviour or accumulative development over several periods require panel data troost and berger 2015a 2020 parameters that affect the probability of low probability events can only be identified if enough low probability events have been observed filatova et al 2016 structural non identifiability cannot be resolved by more of the same data but requires either widening the range of situations observed or more dimensions of the data under certain conditions unidentifiable parameters may be temporarily fixed to allow identification of other components however fixing has to be reversed for latter predictive simulation in order not to obscure model uncertainty noninfluence in the observed domain does not necessarily mean noninfluence in the target situation see example in troost and berger 2015a a 2 mapping purposes to modelling contexts we believe that terms like prediction forecast or projection which are often ambiguous or defined differently between disciplines as well as typologies of edmonds et al 2019 can be communicated more precisely using the suggested dimensions of the modelling context for example the seven modelling purposes of edmonds et al 2019 could be coarsely mapped onto our characterisations of modelling context as follows in theoretical exposition and illustration the system under study is the model itself with the former being output focused moving from an insufficient sample situation to an in sample situation by exhaustive simulation and the latter putting emphasis on transparency and interpretability analogy does relate to a real system and is structure focused with a low demand on precision and comprehensiveness but high demands on transparency and interpretability in this three cases conclusions about the relationship of the model to the real world are left aside for a moment or discussed as unmodelled uncertainty social learning and education can happen in all contexts can be about the model opinions of participants or the real system output or structure but require transparency and interpretability description corresponds to structure focused in sample analysis output focused in sample analysis not mentioned by edmonds et al could be termed compression storing and reproducing observations in a more resource efficient way than explicitly listing them explanation is structure focused out of sample generalization prediction is any output focused analysis in out of sample or non representative sample settings this wide scope of prediction still opens up a lot of room for misunderstanding and clearer definitions of modelling context using the dimensions of required precision and accuracy transparency etc can help in this context to link to appropriate forms of simulation analysis e g marchau et al 2019 
25481,the increasing complexity of the dynamics captured in land use and land cover lulc change modelling has made model behaviour less transparent and calibration more extensive for cellular automata models in particular this is compounded by the fact that validation is typically performed indirectly using final simulated change maps rather than directly considering the probabilistic predictions of transition potential this study demonstrates that evaluating transition potential predictions provides detail into model behaviour and performance that cannot be obtained from simulated map comparison alone this is illustrated by modelling lulc transitions in switzerland using both logistic regression and random forests the results emphasize the need for lulc modellers to explicitly consider the performance of individual transition models independently to ensure robust predictions additionally this study highlights the potential for predictor variable selection as a means to improve transition model generalizability and parsimony which is beneficial for simulating future lulc change keywords land use change modelling cellular automata random forests land transition potential predictor variable selection land use change model calibration software and data availability all data preparation modelling and analysis was conducted in r 4 0 5 r core team 2021 and the processed data and scripts to replicate the results of this research have been made available alongside this publication https doi org 10 5281 zenodo 6912914 1 introduction over the last three decades a wide variety of modelling approaches have been developed to simulate land use and land cover change lulcc such as agent based models econometric models and cellular automata lambin 1997 schaldach and priess 2008 van schrojenstein lantman et al 2011 ren et al 2019 what all of these models have in common is that they all attempt to capture the dynamics by which land changes from one use or state to another in many of these lulcc models the methods by which this is achieved have become increasingly complex over time brown et al 2013 expanding their capacity to represent non linear and non stationary aspects of the lulcc system santé et al 2010 verstegen et al 2016 however this complexity can come at a cost as it can deter or even hinder users from understanding the nature of the relationships being modelled sohl and claggett 2013 which in turn can make the process of model calibration to produce accurate results inefficient and in transparent van vliet et al 2016 at the same time numerous approaches by which these complex models can be better explored have been expounded although their adoption is still limited tong and feng 2020 the development of cellular automata models of lulcc lulcc cas are no exception to the trend of increasing model complexity leading to in transparency and inefficiency in calibration mas et al 2018 the basic premise of lulcc cas is that the study area is abstracted to a finite grid of cells of different lulc states the likelihood of cells to change state is calculated often as a probability based on 1 their previous state 2 the influence of surrounding cells states neighbourhood effect and 3 transition rules or transition potential tp models that encode the relationship between state transitions and external driving variables tobler 1979 white and engelen 1997 white et al 2012 lulcc is then simulated over discrete time steps with cellular transitions typically allocated on the basis of rates of change derived through markov chain analysis mas et al 2014 since early examples of lulcc cas for the simulation of urban development in the 1990s batty and xie 1994 white and engelen 1993 many aspects of the approach have been expanded notably the statistical modelling techniques used for tp models have become substantially more complex over time moving from logistic regression kolb et al 2013 towards approaches such as neural networks li and yeh 2002 support vector machines yang et al 2008 bayesian weights of evidence rodrigues and soares filho 2018 and random forests rf kamusoko and gamba 2015 du et al 2018 one thing that many of these approaches have in common is that they are supervised learning techniques i e models trained directly on observational data which means that it is possible for their performance to be evaluated directly after model fitting in the context of lulcc ca modelling such evaluation is referred to by different terms with paegelow et al 2018 dubbing it soft prediction validation and tong and feng 2020 grouping it under the term procedure assessment neither of these terms represent a succinct description and as such we will henceforth refer to the direct validation of supervised tp models simply as stage 1 validation although stage 1 validation can thus be considered the most direct way of validating tp models the majority of lulcc ca studies do not include stage 1 validation as part of model calibration van vliet et al 2016 tong and feng 2020 instead of performing a stage 1 validation many studies typically focus on the evaluation of tp models outputs only after they have been used to produce simulated lulc maps at the end of the ca process which are validated against observed historical maps this procedure is referred to as hard prediction validation paegelow et al 2014 2018 or result assessment tong and feng 2020 although we will refer to it as stage 2 validation the need for such stage 2 validation is unquestionable as it represents confirmation of the final outputs of the model however relying only on this validation approach is sub optimal in two respects firstly it makes for an inefficient calibration process mas et al 2018 because it requires simultaneously considering all ca parameters the number of these parameters vary with the particular model but examples include factors related to the perturbation of transition probabilities to incorporate uncertainty or policy regulations affecting land use as well as parameters pertaining to the allocation process used to assign transitions mas et al 2014 when these parameters are considered in conjunction with the aspects of the tp models that must be calibrated such as the choice of statistical model e g logistic regression vs random forests and attendant hyperparameters e g number of layers in neural networks or trees in random forests model scale and the optimization of the selection of predictor variables mas et al 2018 then the number of possible parameter combinations and model specifications to test becomes substantial newland et al 2018a this is typically dealt with using a brute force calibration approach where the model is systematically re run by altering one parameter value whilst the others are held constant torrens 2011 or by only addressing some of the aspects related to the tp models and ignoring others the second reason why only employing stage 2 validation for calibration is sub optimal is because validation is being performed on discrete classifications i e allocated as either a transition or non transition of probabilistic predictions that have been binarized and as such this does not provide information as to the certainty of these decisions for example it allows no insight into the distribution of transition probability values amongst the cells that were assigned to transition at a given time point having access to this distribution may show that in order to meet lulcc demand some transitioning cells in fact exhibit a low modelled transition probability this knowledge could prompt further decisions such as re assessing the demand component of the model given that stage 2 validation cannot provide such insights this is a clear example of in transparency in the modelling process both of these limitations of stage 2 validation can be improved by better utilising the opportunity for stage 1 validation firstly using stage 1 validation to test a greater range of specifications for the tp models is more efficient than doing so with only a stage 2 validation because the stage 1 calibration is occurring in isolation from the other lulcc ca parameters and the transition allocation procedure secondly including stage 1 validation allows for the performance of each tp model to be immediately assessed separately in contrast as the predictions of the tp models have already been combined into a single output map for stage 2 validation the performance of individual tp models is difficult to assess assessing tp model performance separately is useful because it allows for exploration of the range in performance across different transitions and more easily identify causes of poor performance such as low sample sizes or high imbalance in the numbers of instances of transitions vs persistence in the data in addition to this stage 1 validation creates the opportunity to utilise a wider range of model performance metrics to provide insights into tp model behaviour than stage 2 validation paegelow et al 2014 this is due to the fact that it is possible to validate the cellular transition values as either probabilities or as discretized binary values by applying a classification threshold when validation of the transition probabilities is performed it is typically with non threshold dependent measures such as the receiver operating characteristic roc approach paegelow et al 2018 although there is potential to utilise other complementary metrics such as the boyce index boyce et al 2002 hirzel et al 2006 which as a presence only measure focuses only on the instances of observed lulc transitions and not persistence this index is particularly applicable because lulcc datasets are typically strongly skewed towards the latter given that the majority of the landscape does not change a further benefit of stage 1 validation is the fact that it allows for independent validation i e for model accuracy to be assessed using instances data points that have not been used to train the models this is often not possible for stage 2 validation because the ca allocation process of binarizing probabilistic predictions is not robust to being performed on a subset of the instances for example allocation algorithms in dinamica ego utilise the spatial aggregation of instances rodrigues and soares filho 2018 independent validation is useful as it can provide insights as to the generalizability of models bishop 2006 in the fields of machine learning and data mining generalizable models are those that exhibit less overfitting on training data and as such perform better in the prediction of new unseen data abu mostafa et al 2012 thus generalizability is desirable trait for tp models given that they are supplied with temporally dynamic data to simulate future lulc mas 2004 soares filho et al 2013 one means of improving model generalizability that can be leveraged through stage 1 validation is by incorporating processes of predictor variable selection to remove redundant variables that constitute noise in the data and can lead to models becoming overfit i e non generalizable guyon et al 2006 furthermore an additional incentive to use predictor variable selection is to produce parsimonious tp models i e models optimized to have compact and non redundant predictor sets whilst still having an acceptable level of accuracy parsimonious tp models with less predictors minimize the need to acquire or extrapolate temporally dynamic predictor values or adopt stationarity assumptions when performing future lulcc simulations despite these potential benefits the incorporation of stage 1 validation as part of the calibration of lulcc cas still remains under utilized tong and feng 2020 as such the aim of this study is to highlight the utility of stage 1 validation as a means of providing insights into tp model behaviour and performance that can be used to improve the efficiency of the calibration of lulcc cas to this end we present an applied example of the stage 1 validation of tp models for lulcc in switzerland between 2009 and 2018 we use multiple model validation metrics to highlight differences in the performance of tp models under different specifications and apply a two step approach to predictor variable selection and how this can be used to improve tp model generalizability and parsimony 2 methods the methodological process of this study is presented in fig 1 first lulc transitions in the form of changes from a specific initial lulc class to another specific final class i e lulc class x to lulc class y were identified and combined with predictors to form transition datasets see sections 2 1 2 2 these datasets where then used to prepare models of transition potential under different specifications see section 2 3 which were subjected to stage 1 validation see section 2 4 it is important to note that this study does not address any of the subsequent steps below the stage 1 validation presented in fig 1 however for the purpose of discussing the efficiency benefits of incorporating stage 1 validation these latter steps have been deliberately included in the figure 2 1 lulc transition dataset creation 2 1 1 lulc data preparation analysing lulcc requires two historical lulc maps to identify change between for this we utilized the swiss area statistics swiss federal office for statistics and geoinformation 2021 for the periods of 2004 2009 and 2013 2018 these are classified lulc datasets derived from aerial photography using a 100 m point grid covering the entirety of switzerland 41 285 km2 whilst a date range is given for each period this is the extent of the data collection flying period and in fact each product represents a single lulc map as such for simplicity the datasets will henceforth be referred to as 2009 and 2018 respectively meaning that the time period for analysis is between these two dates the original datasets include 72 lulc classes but following the example of previous lulcc studies in switzerland price et al 2015 gago silva et al 2017 gerecke et al 2019 these were aggregated to 10 classes as presented in table 1 finally in order to be intersected with the predictor data the lulc transition point datasets were converted to rasters with a resolution of 100 m 2 1 2 transition dataset identification to identify the specific lulc transitions to be modelled we created a cross tabulation matrix of the areal changes in the 10 aggregated lulc classes that occurred between the 2009 and 2018 datasets this matrix was first filtered to remove illogical transitions for example sealed surfaces such as motorways are unlikely to be converted to semi natural surfaces as highly imbalanced data i e having many more transitions than non transitions or vice versa can decrease model robustness further filtering was applied to exclude any lulc transitions that resulted in an areal change of less than 0 5 of the total area for the initial class this left a final list of 30 viable lulc transitions which was used to identify transition datasets by evaluating all pixels from the two lulc data layers on the basis of two criteria for each lulc transition i pixels that displayed the initial lulc class x and final lulc class y corresponding to the lulc transition were assigned a value of 1 to represent change pixels ii all other pixels displaying initial lulc class x to any other final class than y were assigned a value of 0 to represent non change pixels pixels identified by these criteria formed the units of analysis or instances for a given lulc transition dataset this resulted in 30 switzerland wide transition datasets as transition probability can be strongly dependent on region specific conditions these transition datasets were sub divided by the extent of the six biogeographical regions of switzerland jura plateau northern prealps southern prealps western central alps and eastern central alps gonseth et al 2001 this left a total of 174 regionalized transition datasets some transitions involving glaciers were not present in particular regions 2 2 predictor selection 2 2 1 conceptual prediction model predictor variables within the tp models of lulcc cas are commonly grouped into three categories suitability accessibility and neighbourhood variables escobar 2018 suitability predictors are typically biophysical and socio economic predictors that are perceived to be related to the suitability for a given land use or land use change e g elevation precipitation human population density etc accessibility predictors as the name suggests capture the spatial proximity of individual instances to infrastructure or landscape features e g distance to roads or urban centres neighbourhood predictors represent the influence of surrounding lulc on the likelihood of a given instance cell to undergo a lulc transition which can be quantified with a range of approaches verburg et al 2004 santé et al 2010 roodposhti et al 2020 given this diversity of approaches neighbourhood influence is typically one of the most extensively calibrated aspects of lulcc cas van vliet et al 2013 newland et al 2018b many lulc cas utilise some combination of the predictor categories described above but may differ in terms of the weighting ascribed to each based on perceived importance given that this study focuses on statistical models of tp we operate on a simple conceptual model of lulcc that considers all categories of predictors equally illustrated mathematically as follows eqn 1 p t j i f s t j i a t j i n t j i where p is the probability for lulc transition j to occur at the location of instance i at time t given the values of suitability s accessibility a and neighbourhood n predictor variables 2 2 2 suitability and accessibility predictors suitability and accessibility predictors for this study were chosen based on those employed by previous lulcc studies in switzerland price et al 2015 gago silva et al 2017 gerecke et al 2019 table 2 below details the names of the predictors utilized and their data sources all predictor data was resampled to rasters with 100 100 m cell size and then combined with the lulc transition datasets 2 2 3 neighbourhood predictors to incorporate the effect of surrounding lulc neighbourhood influence on lulc transitions first 5 active lulc classes were identified based on their perceived influence on transitions these were settlement urban amenities intensive agriculture alpine pastures grassland meadows and permanent crops following this we adopted the approach of roodposhti et al 2020 by creating a set of 25 pythagorean matrices of varying size 9 121 cells with randomized central values and decay rates these matrices were then applied as moving focal windows across the rasters of active lulc classes whereby the values in the matrix accumulate according to the locations of the active lulc class pixels further details of this process including exemplar matrices have been included in appendix a this resulted in 125 neighbourhood predictor layers 25 per active lulc class that capture different realizations of the influence of active lulc classes on their surroundings these were natively produced as 100 m rasters and were also combined with the suitability and accessibility predictors 2 3 transition potential modelling 2 3 1 predictor variable selection predictor variable selection techniques can be categorized as filter wrapper and embedded approaches stańczyk 2015 in this study we utilized a two step approach to predictor variable selection that combined filter and embedded approaches a adde university of lausanne 2022 personal communication the first filter based step involved using univariate regression models to rank all predictors and then use pairwise pearson s correlation testing to iteratively remove predictors whose correlation exceeded a threshold of 0 7 dormann et al 2013 in the second step the filtered sets of predictors were then subject to further variable selection using a model embedded approach in the form of the guided regularized random forests grrf algorithm grrf is an adaptation to the random forests algorithm developed for the purpose of selecting compact i e non redundant subsets of predictors deng and runger 2013 thereby giving rise to parsimonious models for brevity the full details of both predictor variable selection steps are presented in appendix b 2 3 2 random forests random forests rf is an ensemble decision tree algorithm capable of being utilized for binary or multi class classification or regression problems in either a supervised or unsupervised context given its wide spread usage we will not detail the rf algorithm here instead readers should refer to the seminal work of breiman 2001 for this study we created rf models for two variations of each regionalized lulc transition dataset see 2 1 2 one model for each dataset without predictor variable selection being applied rf full and one model with predictor variable selection applied rf reduced rf classification models were fitted using the randomforest r package cutler et al 2022 the specifications for the models under both conditions were largely the same the minimum size of terminal nodes nodesize and number of variables randomly sampled as candidates at each split mtry were both set to the default values for classification whereas the optimum number of trees was determined through testing appendix c to be 500 trees a systematic down sampling approach was used to address class imbalance utilising the option for tree level proportional sampling through the sampsize argument based on the degree of imbalance in the dataset appendix c overall this resulted in the creation of 174 rf models under each condition rf reduced vs rf full that were used in model comparison 2 3 2 logistic regression logistic regression lr despite acknowledged limitations mustafa et al 2018 is still widely considered to be the most popular and benchmark technique for tp modelling in lulcc cas feng et al 2018 we created two variations of lr models for comparison to the rf models lr models for each regionalized lulc transition dataset subject to predictor variable selection lr reduced and without predictor variable selection lr full all lr models were fitted using the base r glm function with the family argument set as binomial r core team 2021 2 4 model validation for all rf and lr models fitting was performed for five replicates using a split sample approach 70 30 training and test set splits using proportional random sampling without replacement to allow for hold out validation i e model performance is validated using the test set the relative operating characteristic roc method is a commonly applied technique for the validation of probabilistic tp model outputs paegelow et al 2018 the roc approach involves plotting a curve of the rate of true positives i e correct predictions of change versus the rate of false positives from the comparison of observed lulc transitions with predicted boolean transition values generated by applying multiple classification thresholds to the probabilistic predictions from the tp model pontius and parmentier 2014 the benefit of the roc approach is that the area under the curve auc can be calculated as a single valued metric representing the degree of association between high predicted probabilities of lulc transition and actual observed transitions a complementary metric to the auc roc that is not commonly applied but is useful for tp model validation is the boyce index boyce et al 2002 hirzel et al 2006 calculating the boyce index involves separating the instances of observed lulc transitions into classes according to the probabilistic prediction values assigned to them then calculating class wise ratios of the frequency of instances predicted to fall into the class vs the expected frequency of instances in the class under a random distribution predicted to expected p e ratio hirzel et al 2006 if models are well fitted then the p e ratio values of the classes should exhibit a monotonically increasing curve as the value of prediction probability increases as such the value of the boyce index is the spearman rank correlation coefficient between the p e ratios and the probability classes we calculated auc roc and boyce index values using the rocr and ecospat packages respectively with the moving window approach for the ecospat boyce function using 1000 windows sing et al 2020 broennimann et al 2022 for all variations of the lr and rf tp models values for each metric were calculated individually for each of the test and training sets under the five replicates before being aggregated into average values across the replicates for each model finally in order to present a single validation metric we re scaled the auc value to the same range as the boyce index 1 to 1 and took an average of the two which we will refer to as the model score in evaluating models using the roc approach an auc value of 0 7 is considered to be a general threshold value for acceptable performance hosmer and lemeshow 2000 whereas no such value has been proposed for the boyce index as such to evaluate models of the different model specifications rf reduced rf full lr reduced and lr full we re scaled this auc threshold value according to the range of the boyce index and model score to give a threshold value of 0 4 for these metrics in terms of statistical comparisons between model specifications this represented an un replicated complete block design given that the auc boyce and model score results for the models all violated the normality assumption for parametric repeated measures anova shapiro wilks test of between group residuals we utilized the non parametric friedman test pereira et al 2015 to test for significant differences in performance between models both with and without the removal of outliers following the friedman test post hoc testing was completed using pairwise conover s all pairs comparisons test with bonferroni correction of the p values conover 1999 we utilized the results of this analysis to select tp models that exhibited poor performance to demonstrate how the boyce index and roc auc can be used to provide more detailed information on model behaviour in this regard we produced boyce index curves of the ratio of the predicted vs expected frequency of instances across prediction probability classes as well roc curves finally to demonstrate the benefits of predictor variable selection in terms of model generalizability we calculated the differences in average model score between the test and training datasets for the lr and rf models under both the reduced and full specifications the magnitude of the difference in performance between the test and training datasets represents an approximation of the generalization error roelofs 2019 and as such is a simplistic gauge of generalizability whereby the smaller the difference in performance the more generalizable less likely to be overfit the model is whereas to highlight benefits in terms of model parsimony we calculated the differences in average model score between tp models under the rf reduced vs rf full and lr reduced vs lr full specifications 3 results 3 1 transition datasets a table detailing the lulc classes and the number of instances in each of the switzerland wide and bioregional lulcc transition datasets is presented in appendix d 3 2 model performance fig 2 shows that the performance of the tp models in terms of the average model score auc and boyce index values varied under the different specifications lr full lr reduced and rf full rf reduced simple visual comparison between the distributions of values suggest that both of the rf specifications outperformed the lr specifications and indeed this was confirmed with statistical testing appendix e more importantly fig 2 shows that under all specifications there were a number of models that exhibited values of the performance metrics below the respective threshold values indicating poor performance specifically for the auc metric for which the threshold value of 0 7 is well recognised fig 2 shows that even under the best performing rf model specifications there were 36 transition models below the threshold however the benefit of utilising non threshold metrics such as the boyce index and the roc approach is that the behaviour of these models can be explored in greater detail through the graphical representation of model performance across the prediction probability gradient in this regard fig 3 presents boyce and roc curves along with the corresponding values of the metrics for a single replicate randomly selected for each of the rf and lr reduced models for the lulc transition of intensive agriculture to grassland in the southern pre alps region which was one of the transitions that performed below the thresholds fig 2 the boyce index values in fig 3a suggest that both the lr and rf models for this transition exhibited relatively similar performance ρ 0 28 and 0 47 respectively however the curves highlight some discrepancies between them whereas well fitted models should exhibit a monotonically increasing curve of p e ratio to predicted probability the curve for the lr model shows a plateau of low p e ratio at prediction probability of 0 22 0 24 and a subsequent drop in p e ratio at probabilities 0 3 a similar pattern is exhibited by the curve for the rf model albeit at comparatively greater values of predicted probability in both cases the large drop in p e ratio at the upper bounds of the predicted probability values is notable because this means that at these high probabilities where we should expect the greatest frequencies of transitions to be predicted the model is in fact predicting fewer transitions than should be expected under a random distribution p e ratio values 1 in addition to this the fact that the highest predicted probability values from either model in fig 3a do not exceed 0 5 would mean that if a threshold of 50 predicted probability was applied to select cells to transition in the ca then none of the instances of transitions in this dataset would be selected which is of course erroneous this indicates that these tp models are not capable of strongly discriminating between instances of transitions and persistence this is further supported by the roc curves in fig 3b for which the curve of the lr model shows that there is range of prediction probability thresholds for which the true positive rate is approximately equal to the false positive rate i e where the curve crosses the dashed no discrimination line in the upper right quadrant of the plot this indicates that at these values of prediction probability the model is no better at discriminating between instances of transitions and persistence than a random model by comparison the prediction probability at which the model demonstrates the best discrimination i e greatest difference between true and false positive rates is a value of 0 06 or a 6 likelihood of transition labelled this is problematic as ideally the model should exhibit better discrimination at high probability values as cells with these values are more likely to be selected to transition in the ca allocation process 3 3 impacts of predictor variable selection 3 3 1 model generalizability fig 4 shows a comparison of the difference in average model score between the training and test datasets of the models without vs with predictor variable selection lr full vs lr reduced rf full vs rf reduced fig 4 shows that under lr the reduced models displayed significantly lower differences in average model score as compared to the full models median 0 02 vs 0 15 respectively this indicates that when using lr the use of predictor selection resulted in more generalizable models the same general trend is true for the rf models with the reduced models exhibiting a median difference in average model score value of 9 25e 03 as compared to a median value of 0 04 for the full models however for the rf models the difference was non significant under the wilcoxon signed rank test 3 3 2 model parsimony fig 5 shows the differences in average model score between the models with and without predictor variable selection i e reduced model full model for both the lr and rf models from fig 5 it is clear that predictor variable selection had different effects on performance depending on the type of model for lr there was a substantially greater number of models that showed positive values of differences in average model score as compared to negative values 124 50 respectively indicating that predictor variable selection tended to improve performance furthermore the mean value of the difference in average model score was also comparatively greater for models exhibiting positive values 0 129 than negative values 0 067 which highlights that even when predictor variable selection reduced model performance the effect was not as pronounced as the positive effect as for rf the number of models exhibiting positive vs negative values of difference in average model score were fairly even 84 90 with the means of each group also being of a similar magnitude this indicates that predictor variable selection had less of impact on the performance of rf models which is supported by the statistical pairwise comparisons presented in appendix e 4 discussion in this study we have demonstrated how stage 1 validation allows for more comprehensive insights into tp model performance and behaviour by comparing individual tp models performance under multiple metrics we highlighted that many tp models were not of a threshold quality sufficient for accurate prediction of data from the time period they were fitted upon this is especially problematic considering that these models are required to be used in future simulations on unseen data regardless the capability to provide this insight is a clear advantage of stage 1 validation whereas with stage 2 validation it would have been necessary to perform disaggregation of the validation results to achieve this level of detail following this we showed how stage 1 validation can be used to understand how tp models may be performing poorly through inspection of the boyce and roc curves which is not possible with stage 2 validation given that the model predictions have already been discretized into binary values these curves offer insights into performance across the prediction probability gradient and whilst the roc approach is frequently used in lulcc ca studies that do incorporate stage 1 validation our application of the boyce curve to this domain is novel we demonstrated how the boyce curve can be used to identify prediction probabilities at which tp models are predicting less transitions than should be expected to occur by chance whilst this insight does not directly indicate how tp models can be improved it is useful in informing further investigation such as mapping the instances at prediction probabilities with low p e ratio values to identify possible causative factors ultimately poor performance of tp models must be addressed if they are to be utilized within a lulcc ca in this regard there are two general options to try and improve the models by keeping the algorithm the same but altering parameters du et al 2018 or to use a different algorithmic approach or a combination of multiple methods e g ensemble modelling shafizadeh moghadam 2019 we have demonstrated elements of both solutions through hyper parameter tuning and attempting to address class imbalance appendix c but primarily by exploring the benefits of predictor variable selection which has not been covered extensively within previous lulcc ca studies using rf kamusoko and gamba 2015 du et al 2018 gounaridis et al 2019 roodposhti et al 2019 zhang et al 2019 chen et al 2019 li and chen 2020 rienow et al 2021 we demonstrated that predictor variable selection not only improved tp model generalizability but also improved performance in a substantial proportion of cases whilst sometimes removing over 50 of predictors see figure b1 these findings have important consequences for tp models being used within lulcc cas to make predictions of future lulc because the models remain stationary but are fitted with new data and hence less generalizable models will produce less accurate simulations soares filho et al 2013 however it is important to highlight that these benefits of predictor variable selection differed between the model types utilized with the process having less impact on rf than lr this is likely because the regular rf algorithm is inherently robust to redundant variables to an extent due to the fact that variables are chosen during node splitting based on a measure of importance kubus 2018 also the fact that the majority of predictors that were removed were neighbourhood predictors which given that they represented different realizations of the same phenomena appendix a could have been disproportionately contributing to the overfitting of the models at the same time it is important to note that the conception of generalizability and the means of quantifying it employed in this research are not definitive indeed an alternative approach to quantifying generalizability would be to compare the performance of models trained on data from a given time period when used to predict transition potential for data from a subsequent period sometimes referred to as external validation ho et al 2020 the decision to instead utilise independent validation to quantify generalizability in this research was intended to demonstrate that this is possible with stage 1 validation but not stage 2 validation this is a useful capability of stage 1 validation given that future lulcc simulation modelling typically only utilizes tp models for the most recent time period available and hence we have an interest in quantifying whether these specific models are generalizable and of course this cannot be done using external validation because subsequent time period data does not exist however further research should be performed to compare the estimates of generalizability produced by independent versus external validation approaches regardless of how the improvement of tp model performance is pursued a final benefit of stage 1 validation is that it makes the process more efficient this is because it allows for the comparison of different specifications without the need to instantiate other parameters in the ca and run the allocation process to produce simulated lulc maps fig 1 in the context of this study only using stage 2 validation for hyper parameter testing four different rf ensemble sizes for both reduced and full datasets appendix c and the two additional lr specifications of tp models would have required the allocation process to be run a minimum of 10 times the time required for allocation varies dependent on the specific lulcc ca being utilized but generally it can be expected to scale with the size of the study area and the number of lulc transitions being modelled given that we are modelling lulcc at the scale of the whole of switzerland with 100 m resolution and 174 lulc transitions the fact that we did not have to prepare the ca model or run the allocation process to calibrate the tp models represents a substantial improvement in efficiency despite the benefits of utilising stage 1 validation as shown by this study and acknowledged by other authors kolb et al 2013 it still remains under utilized in lulcc ca studies tong and feng 2020 the reason for this is difficult to attribute but one suggestion is because extensive validation of tp models is often not central to the aims of many lulcc ca studies ibid for example where the goal of research is to devise and simulate a range of future lulcc scenarios it is understandable that model calibration is given limited attention another possible explanation could stem from the fact that many popular lulcc cas are proprietary software e g dinamica ego land change modeller and conduct the fitting of the tp models internally thereby removing some of the onus on the user to specify and interpret the models furthermore whilst many proprietary lulcc cas do include the option to calculate performance metrics for stage 1 validation these are limited in comparison to the means for stage 2 validation paegelow et al 2018 and by no means do they force users to explicitly consider the performance of the tp models in isolation on the other hand the fact that lulcc cas such as dinamica ego offer a flexible modular and graphical modelling environment makes them accessible to a greater range of users in this regard it is clear that promoting increased scrutiny of tp models through stage 1 validation should not come at the expense of the usability of lulcc ca software a possible solution to this in order to increase the adoption of stage 1 validation within lulcc cas studies could be the establishment of a universal protocol for the calibration of tp models that is generalizable across the popular lulcc ca models in this regard the framework proposed by moulds et al 2015 represents some progress however it is largely a software focused approach instead such a protocol should specifically elucidate the steps involved in preparing and evaluating tp models and the relevant aspects that should be considered such as the use of different model scales algorithmic techniques predictor variable selection addressing imbalanced datasets model uncertainty and the merits of different performance metrics whilst we do not present such a protocol we hope that by sharing the data and scripts that allow our research to be replicated we are contributing in a small way towards its development 5 conclusion our research has shown that directly evaluating probabilistic predictions of lulc transition models which we dub stage 1 validation has the potential to improve the efficiency and transparency of the calibration process for lulcc cas as highlighted the potential to utilise stage 1 validation is not novel rather it has been overlooked as part of the status quo approach for lulcc ca calibration we hope that this introspective approach to highlighting existing opportunities to improve practice could serve to stimulate similar efforts other fields of land use change modelling declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we gratefully acknowledge financial support from the swiss federal office for the environment foen under the project valpar ch values of the ecological infrastructure in swiss parks of the action plan of the swiss biodiversity strategy we would also like to thank philipp brun swiss federal institute for forest snow and landscape research for providing r scripts and functions for species distribution modelling that were adapted for this research appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105574 
25481,the increasing complexity of the dynamics captured in land use and land cover lulc change modelling has made model behaviour less transparent and calibration more extensive for cellular automata models in particular this is compounded by the fact that validation is typically performed indirectly using final simulated change maps rather than directly considering the probabilistic predictions of transition potential this study demonstrates that evaluating transition potential predictions provides detail into model behaviour and performance that cannot be obtained from simulated map comparison alone this is illustrated by modelling lulc transitions in switzerland using both logistic regression and random forests the results emphasize the need for lulc modellers to explicitly consider the performance of individual transition models independently to ensure robust predictions additionally this study highlights the potential for predictor variable selection as a means to improve transition model generalizability and parsimony which is beneficial for simulating future lulc change keywords land use change modelling cellular automata random forests land transition potential predictor variable selection land use change model calibration software and data availability all data preparation modelling and analysis was conducted in r 4 0 5 r core team 2021 and the processed data and scripts to replicate the results of this research have been made available alongside this publication https doi org 10 5281 zenodo 6912914 1 introduction over the last three decades a wide variety of modelling approaches have been developed to simulate land use and land cover change lulcc such as agent based models econometric models and cellular automata lambin 1997 schaldach and priess 2008 van schrojenstein lantman et al 2011 ren et al 2019 what all of these models have in common is that they all attempt to capture the dynamics by which land changes from one use or state to another in many of these lulcc models the methods by which this is achieved have become increasingly complex over time brown et al 2013 expanding their capacity to represent non linear and non stationary aspects of the lulcc system santé et al 2010 verstegen et al 2016 however this complexity can come at a cost as it can deter or even hinder users from understanding the nature of the relationships being modelled sohl and claggett 2013 which in turn can make the process of model calibration to produce accurate results inefficient and in transparent van vliet et al 2016 at the same time numerous approaches by which these complex models can be better explored have been expounded although their adoption is still limited tong and feng 2020 the development of cellular automata models of lulcc lulcc cas are no exception to the trend of increasing model complexity leading to in transparency and inefficiency in calibration mas et al 2018 the basic premise of lulcc cas is that the study area is abstracted to a finite grid of cells of different lulc states the likelihood of cells to change state is calculated often as a probability based on 1 their previous state 2 the influence of surrounding cells states neighbourhood effect and 3 transition rules or transition potential tp models that encode the relationship between state transitions and external driving variables tobler 1979 white and engelen 1997 white et al 2012 lulcc is then simulated over discrete time steps with cellular transitions typically allocated on the basis of rates of change derived through markov chain analysis mas et al 2014 since early examples of lulcc cas for the simulation of urban development in the 1990s batty and xie 1994 white and engelen 1993 many aspects of the approach have been expanded notably the statistical modelling techniques used for tp models have become substantially more complex over time moving from logistic regression kolb et al 2013 towards approaches such as neural networks li and yeh 2002 support vector machines yang et al 2008 bayesian weights of evidence rodrigues and soares filho 2018 and random forests rf kamusoko and gamba 2015 du et al 2018 one thing that many of these approaches have in common is that they are supervised learning techniques i e models trained directly on observational data which means that it is possible for their performance to be evaluated directly after model fitting in the context of lulcc ca modelling such evaluation is referred to by different terms with paegelow et al 2018 dubbing it soft prediction validation and tong and feng 2020 grouping it under the term procedure assessment neither of these terms represent a succinct description and as such we will henceforth refer to the direct validation of supervised tp models simply as stage 1 validation although stage 1 validation can thus be considered the most direct way of validating tp models the majority of lulcc ca studies do not include stage 1 validation as part of model calibration van vliet et al 2016 tong and feng 2020 instead of performing a stage 1 validation many studies typically focus on the evaluation of tp models outputs only after they have been used to produce simulated lulc maps at the end of the ca process which are validated against observed historical maps this procedure is referred to as hard prediction validation paegelow et al 2014 2018 or result assessment tong and feng 2020 although we will refer to it as stage 2 validation the need for such stage 2 validation is unquestionable as it represents confirmation of the final outputs of the model however relying only on this validation approach is sub optimal in two respects firstly it makes for an inefficient calibration process mas et al 2018 because it requires simultaneously considering all ca parameters the number of these parameters vary with the particular model but examples include factors related to the perturbation of transition probabilities to incorporate uncertainty or policy regulations affecting land use as well as parameters pertaining to the allocation process used to assign transitions mas et al 2014 when these parameters are considered in conjunction with the aspects of the tp models that must be calibrated such as the choice of statistical model e g logistic regression vs random forests and attendant hyperparameters e g number of layers in neural networks or trees in random forests model scale and the optimization of the selection of predictor variables mas et al 2018 then the number of possible parameter combinations and model specifications to test becomes substantial newland et al 2018a this is typically dealt with using a brute force calibration approach where the model is systematically re run by altering one parameter value whilst the others are held constant torrens 2011 or by only addressing some of the aspects related to the tp models and ignoring others the second reason why only employing stage 2 validation for calibration is sub optimal is because validation is being performed on discrete classifications i e allocated as either a transition or non transition of probabilistic predictions that have been binarized and as such this does not provide information as to the certainty of these decisions for example it allows no insight into the distribution of transition probability values amongst the cells that were assigned to transition at a given time point having access to this distribution may show that in order to meet lulcc demand some transitioning cells in fact exhibit a low modelled transition probability this knowledge could prompt further decisions such as re assessing the demand component of the model given that stage 2 validation cannot provide such insights this is a clear example of in transparency in the modelling process both of these limitations of stage 2 validation can be improved by better utilising the opportunity for stage 1 validation firstly using stage 1 validation to test a greater range of specifications for the tp models is more efficient than doing so with only a stage 2 validation because the stage 1 calibration is occurring in isolation from the other lulcc ca parameters and the transition allocation procedure secondly including stage 1 validation allows for the performance of each tp model to be immediately assessed separately in contrast as the predictions of the tp models have already been combined into a single output map for stage 2 validation the performance of individual tp models is difficult to assess assessing tp model performance separately is useful because it allows for exploration of the range in performance across different transitions and more easily identify causes of poor performance such as low sample sizes or high imbalance in the numbers of instances of transitions vs persistence in the data in addition to this stage 1 validation creates the opportunity to utilise a wider range of model performance metrics to provide insights into tp model behaviour than stage 2 validation paegelow et al 2014 this is due to the fact that it is possible to validate the cellular transition values as either probabilities or as discretized binary values by applying a classification threshold when validation of the transition probabilities is performed it is typically with non threshold dependent measures such as the receiver operating characteristic roc approach paegelow et al 2018 although there is potential to utilise other complementary metrics such as the boyce index boyce et al 2002 hirzel et al 2006 which as a presence only measure focuses only on the instances of observed lulc transitions and not persistence this index is particularly applicable because lulcc datasets are typically strongly skewed towards the latter given that the majority of the landscape does not change a further benefit of stage 1 validation is the fact that it allows for independent validation i e for model accuracy to be assessed using instances data points that have not been used to train the models this is often not possible for stage 2 validation because the ca allocation process of binarizing probabilistic predictions is not robust to being performed on a subset of the instances for example allocation algorithms in dinamica ego utilise the spatial aggregation of instances rodrigues and soares filho 2018 independent validation is useful as it can provide insights as to the generalizability of models bishop 2006 in the fields of machine learning and data mining generalizable models are those that exhibit less overfitting on training data and as such perform better in the prediction of new unseen data abu mostafa et al 2012 thus generalizability is desirable trait for tp models given that they are supplied with temporally dynamic data to simulate future lulc mas 2004 soares filho et al 2013 one means of improving model generalizability that can be leveraged through stage 1 validation is by incorporating processes of predictor variable selection to remove redundant variables that constitute noise in the data and can lead to models becoming overfit i e non generalizable guyon et al 2006 furthermore an additional incentive to use predictor variable selection is to produce parsimonious tp models i e models optimized to have compact and non redundant predictor sets whilst still having an acceptable level of accuracy parsimonious tp models with less predictors minimize the need to acquire or extrapolate temporally dynamic predictor values or adopt stationarity assumptions when performing future lulcc simulations despite these potential benefits the incorporation of stage 1 validation as part of the calibration of lulcc cas still remains under utilized tong and feng 2020 as such the aim of this study is to highlight the utility of stage 1 validation as a means of providing insights into tp model behaviour and performance that can be used to improve the efficiency of the calibration of lulcc cas to this end we present an applied example of the stage 1 validation of tp models for lulcc in switzerland between 2009 and 2018 we use multiple model validation metrics to highlight differences in the performance of tp models under different specifications and apply a two step approach to predictor variable selection and how this can be used to improve tp model generalizability and parsimony 2 methods the methodological process of this study is presented in fig 1 first lulc transitions in the form of changes from a specific initial lulc class to another specific final class i e lulc class x to lulc class y were identified and combined with predictors to form transition datasets see sections 2 1 2 2 these datasets where then used to prepare models of transition potential under different specifications see section 2 3 which were subjected to stage 1 validation see section 2 4 it is important to note that this study does not address any of the subsequent steps below the stage 1 validation presented in fig 1 however for the purpose of discussing the efficiency benefits of incorporating stage 1 validation these latter steps have been deliberately included in the figure 2 1 lulc transition dataset creation 2 1 1 lulc data preparation analysing lulcc requires two historical lulc maps to identify change between for this we utilized the swiss area statistics swiss federal office for statistics and geoinformation 2021 for the periods of 2004 2009 and 2013 2018 these are classified lulc datasets derived from aerial photography using a 100 m point grid covering the entirety of switzerland 41 285 km2 whilst a date range is given for each period this is the extent of the data collection flying period and in fact each product represents a single lulc map as such for simplicity the datasets will henceforth be referred to as 2009 and 2018 respectively meaning that the time period for analysis is between these two dates the original datasets include 72 lulc classes but following the example of previous lulcc studies in switzerland price et al 2015 gago silva et al 2017 gerecke et al 2019 these were aggregated to 10 classes as presented in table 1 finally in order to be intersected with the predictor data the lulc transition point datasets were converted to rasters with a resolution of 100 m 2 1 2 transition dataset identification to identify the specific lulc transitions to be modelled we created a cross tabulation matrix of the areal changes in the 10 aggregated lulc classes that occurred between the 2009 and 2018 datasets this matrix was first filtered to remove illogical transitions for example sealed surfaces such as motorways are unlikely to be converted to semi natural surfaces as highly imbalanced data i e having many more transitions than non transitions or vice versa can decrease model robustness further filtering was applied to exclude any lulc transitions that resulted in an areal change of less than 0 5 of the total area for the initial class this left a final list of 30 viable lulc transitions which was used to identify transition datasets by evaluating all pixels from the two lulc data layers on the basis of two criteria for each lulc transition i pixels that displayed the initial lulc class x and final lulc class y corresponding to the lulc transition were assigned a value of 1 to represent change pixels ii all other pixels displaying initial lulc class x to any other final class than y were assigned a value of 0 to represent non change pixels pixels identified by these criteria formed the units of analysis or instances for a given lulc transition dataset this resulted in 30 switzerland wide transition datasets as transition probability can be strongly dependent on region specific conditions these transition datasets were sub divided by the extent of the six biogeographical regions of switzerland jura plateau northern prealps southern prealps western central alps and eastern central alps gonseth et al 2001 this left a total of 174 regionalized transition datasets some transitions involving glaciers were not present in particular regions 2 2 predictor selection 2 2 1 conceptual prediction model predictor variables within the tp models of lulcc cas are commonly grouped into three categories suitability accessibility and neighbourhood variables escobar 2018 suitability predictors are typically biophysical and socio economic predictors that are perceived to be related to the suitability for a given land use or land use change e g elevation precipitation human population density etc accessibility predictors as the name suggests capture the spatial proximity of individual instances to infrastructure or landscape features e g distance to roads or urban centres neighbourhood predictors represent the influence of surrounding lulc on the likelihood of a given instance cell to undergo a lulc transition which can be quantified with a range of approaches verburg et al 2004 santé et al 2010 roodposhti et al 2020 given this diversity of approaches neighbourhood influence is typically one of the most extensively calibrated aspects of lulcc cas van vliet et al 2013 newland et al 2018b many lulc cas utilise some combination of the predictor categories described above but may differ in terms of the weighting ascribed to each based on perceived importance given that this study focuses on statistical models of tp we operate on a simple conceptual model of lulcc that considers all categories of predictors equally illustrated mathematically as follows eqn 1 p t j i f s t j i a t j i n t j i where p is the probability for lulc transition j to occur at the location of instance i at time t given the values of suitability s accessibility a and neighbourhood n predictor variables 2 2 2 suitability and accessibility predictors suitability and accessibility predictors for this study were chosen based on those employed by previous lulcc studies in switzerland price et al 2015 gago silva et al 2017 gerecke et al 2019 table 2 below details the names of the predictors utilized and their data sources all predictor data was resampled to rasters with 100 100 m cell size and then combined with the lulc transition datasets 2 2 3 neighbourhood predictors to incorporate the effect of surrounding lulc neighbourhood influence on lulc transitions first 5 active lulc classes were identified based on their perceived influence on transitions these were settlement urban amenities intensive agriculture alpine pastures grassland meadows and permanent crops following this we adopted the approach of roodposhti et al 2020 by creating a set of 25 pythagorean matrices of varying size 9 121 cells with randomized central values and decay rates these matrices were then applied as moving focal windows across the rasters of active lulc classes whereby the values in the matrix accumulate according to the locations of the active lulc class pixels further details of this process including exemplar matrices have been included in appendix a this resulted in 125 neighbourhood predictor layers 25 per active lulc class that capture different realizations of the influence of active lulc classes on their surroundings these were natively produced as 100 m rasters and were also combined with the suitability and accessibility predictors 2 3 transition potential modelling 2 3 1 predictor variable selection predictor variable selection techniques can be categorized as filter wrapper and embedded approaches stańczyk 2015 in this study we utilized a two step approach to predictor variable selection that combined filter and embedded approaches a adde university of lausanne 2022 personal communication the first filter based step involved using univariate regression models to rank all predictors and then use pairwise pearson s correlation testing to iteratively remove predictors whose correlation exceeded a threshold of 0 7 dormann et al 2013 in the second step the filtered sets of predictors were then subject to further variable selection using a model embedded approach in the form of the guided regularized random forests grrf algorithm grrf is an adaptation to the random forests algorithm developed for the purpose of selecting compact i e non redundant subsets of predictors deng and runger 2013 thereby giving rise to parsimonious models for brevity the full details of both predictor variable selection steps are presented in appendix b 2 3 2 random forests random forests rf is an ensemble decision tree algorithm capable of being utilized for binary or multi class classification or regression problems in either a supervised or unsupervised context given its wide spread usage we will not detail the rf algorithm here instead readers should refer to the seminal work of breiman 2001 for this study we created rf models for two variations of each regionalized lulc transition dataset see 2 1 2 one model for each dataset without predictor variable selection being applied rf full and one model with predictor variable selection applied rf reduced rf classification models were fitted using the randomforest r package cutler et al 2022 the specifications for the models under both conditions were largely the same the minimum size of terminal nodes nodesize and number of variables randomly sampled as candidates at each split mtry were both set to the default values for classification whereas the optimum number of trees was determined through testing appendix c to be 500 trees a systematic down sampling approach was used to address class imbalance utilising the option for tree level proportional sampling through the sampsize argument based on the degree of imbalance in the dataset appendix c overall this resulted in the creation of 174 rf models under each condition rf reduced vs rf full that were used in model comparison 2 3 2 logistic regression logistic regression lr despite acknowledged limitations mustafa et al 2018 is still widely considered to be the most popular and benchmark technique for tp modelling in lulcc cas feng et al 2018 we created two variations of lr models for comparison to the rf models lr models for each regionalized lulc transition dataset subject to predictor variable selection lr reduced and without predictor variable selection lr full all lr models were fitted using the base r glm function with the family argument set as binomial r core team 2021 2 4 model validation for all rf and lr models fitting was performed for five replicates using a split sample approach 70 30 training and test set splits using proportional random sampling without replacement to allow for hold out validation i e model performance is validated using the test set the relative operating characteristic roc method is a commonly applied technique for the validation of probabilistic tp model outputs paegelow et al 2018 the roc approach involves plotting a curve of the rate of true positives i e correct predictions of change versus the rate of false positives from the comparison of observed lulc transitions with predicted boolean transition values generated by applying multiple classification thresholds to the probabilistic predictions from the tp model pontius and parmentier 2014 the benefit of the roc approach is that the area under the curve auc can be calculated as a single valued metric representing the degree of association between high predicted probabilities of lulc transition and actual observed transitions a complementary metric to the auc roc that is not commonly applied but is useful for tp model validation is the boyce index boyce et al 2002 hirzel et al 2006 calculating the boyce index involves separating the instances of observed lulc transitions into classes according to the probabilistic prediction values assigned to them then calculating class wise ratios of the frequency of instances predicted to fall into the class vs the expected frequency of instances in the class under a random distribution predicted to expected p e ratio hirzel et al 2006 if models are well fitted then the p e ratio values of the classes should exhibit a monotonically increasing curve as the value of prediction probability increases as such the value of the boyce index is the spearman rank correlation coefficient between the p e ratios and the probability classes we calculated auc roc and boyce index values using the rocr and ecospat packages respectively with the moving window approach for the ecospat boyce function using 1000 windows sing et al 2020 broennimann et al 2022 for all variations of the lr and rf tp models values for each metric were calculated individually for each of the test and training sets under the five replicates before being aggregated into average values across the replicates for each model finally in order to present a single validation metric we re scaled the auc value to the same range as the boyce index 1 to 1 and took an average of the two which we will refer to as the model score in evaluating models using the roc approach an auc value of 0 7 is considered to be a general threshold value for acceptable performance hosmer and lemeshow 2000 whereas no such value has been proposed for the boyce index as such to evaluate models of the different model specifications rf reduced rf full lr reduced and lr full we re scaled this auc threshold value according to the range of the boyce index and model score to give a threshold value of 0 4 for these metrics in terms of statistical comparisons between model specifications this represented an un replicated complete block design given that the auc boyce and model score results for the models all violated the normality assumption for parametric repeated measures anova shapiro wilks test of between group residuals we utilized the non parametric friedman test pereira et al 2015 to test for significant differences in performance between models both with and without the removal of outliers following the friedman test post hoc testing was completed using pairwise conover s all pairs comparisons test with bonferroni correction of the p values conover 1999 we utilized the results of this analysis to select tp models that exhibited poor performance to demonstrate how the boyce index and roc auc can be used to provide more detailed information on model behaviour in this regard we produced boyce index curves of the ratio of the predicted vs expected frequency of instances across prediction probability classes as well roc curves finally to demonstrate the benefits of predictor variable selection in terms of model generalizability we calculated the differences in average model score between the test and training datasets for the lr and rf models under both the reduced and full specifications the magnitude of the difference in performance between the test and training datasets represents an approximation of the generalization error roelofs 2019 and as such is a simplistic gauge of generalizability whereby the smaller the difference in performance the more generalizable less likely to be overfit the model is whereas to highlight benefits in terms of model parsimony we calculated the differences in average model score between tp models under the rf reduced vs rf full and lr reduced vs lr full specifications 3 results 3 1 transition datasets a table detailing the lulc classes and the number of instances in each of the switzerland wide and bioregional lulcc transition datasets is presented in appendix d 3 2 model performance fig 2 shows that the performance of the tp models in terms of the average model score auc and boyce index values varied under the different specifications lr full lr reduced and rf full rf reduced simple visual comparison between the distributions of values suggest that both of the rf specifications outperformed the lr specifications and indeed this was confirmed with statistical testing appendix e more importantly fig 2 shows that under all specifications there were a number of models that exhibited values of the performance metrics below the respective threshold values indicating poor performance specifically for the auc metric for which the threshold value of 0 7 is well recognised fig 2 shows that even under the best performing rf model specifications there were 36 transition models below the threshold however the benefit of utilising non threshold metrics such as the boyce index and the roc approach is that the behaviour of these models can be explored in greater detail through the graphical representation of model performance across the prediction probability gradient in this regard fig 3 presents boyce and roc curves along with the corresponding values of the metrics for a single replicate randomly selected for each of the rf and lr reduced models for the lulc transition of intensive agriculture to grassland in the southern pre alps region which was one of the transitions that performed below the thresholds fig 2 the boyce index values in fig 3a suggest that both the lr and rf models for this transition exhibited relatively similar performance ρ 0 28 and 0 47 respectively however the curves highlight some discrepancies between them whereas well fitted models should exhibit a monotonically increasing curve of p e ratio to predicted probability the curve for the lr model shows a plateau of low p e ratio at prediction probability of 0 22 0 24 and a subsequent drop in p e ratio at probabilities 0 3 a similar pattern is exhibited by the curve for the rf model albeit at comparatively greater values of predicted probability in both cases the large drop in p e ratio at the upper bounds of the predicted probability values is notable because this means that at these high probabilities where we should expect the greatest frequencies of transitions to be predicted the model is in fact predicting fewer transitions than should be expected under a random distribution p e ratio values 1 in addition to this the fact that the highest predicted probability values from either model in fig 3a do not exceed 0 5 would mean that if a threshold of 50 predicted probability was applied to select cells to transition in the ca then none of the instances of transitions in this dataset would be selected which is of course erroneous this indicates that these tp models are not capable of strongly discriminating between instances of transitions and persistence this is further supported by the roc curves in fig 3b for which the curve of the lr model shows that there is range of prediction probability thresholds for which the true positive rate is approximately equal to the false positive rate i e where the curve crosses the dashed no discrimination line in the upper right quadrant of the plot this indicates that at these values of prediction probability the model is no better at discriminating between instances of transitions and persistence than a random model by comparison the prediction probability at which the model demonstrates the best discrimination i e greatest difference between true and false positive rates is a value of 0 06 or a 6 likelihood of transition labelled this is problematic as ideally the model should exhibit better discrimination at high probability values as cells with these values are more likely to be selected to transition in the ca allocation process 3 3 impacts of predictor variable selection 3 3 1 model generalizability fig 4 shows a comparison of the difference in average model score between the training and test datasets of the models without vs with predictor variable selection lr full vs lr reduced rf full vs rf reduced fig 4 shows that under lr the reduced models displayed significantly lower differences in average model score as compared to the full models median 0 02 vs 0 15 respectively this indicates that when using lr the use of predictor selection resulted in more generalizable models the same general trend is true for the rf models with the reduced models exhibiting a median difference in average model score value of 9 25e 03 as compared to a median value of 0 04 for the full models however for the rf models the difference was non significant under the wilcoxon signed rank test 3 3 2 model parsimony fig 5 shows the differences in average model score between the models with and without predictor variable selection i e reduced model full model for both the lr and rf models from fig 5 it is clear that predictor variable selection had different effects on performance depending on the type of model for lr there was a substantially greater number of models that showed positive values of differences in average model score as compared to negative values 124 50 respectively indicating that predictor variable selection tended to improve performance furthermore the mean value of the difference in average model score was also comparatively greater for models exhibiting positive values 0 129 than negative values 0 067 which highlights that even when predictor variable selection reduced model performance the effect was not as pronounced as the positive effect as for rf the number of models exhibiting positive vs negative values of difference in average model score were fairly even 84 90 with the means of each group also being of a similar magnitude this indicates that predictor variable selection had less of impact on the performance of rf models which is supported by the statistical pairwise comparisons presented in appendix e 4 discussion in this study we have demonstrated how stage 1 validation allows for more comprehensive insights into tp model performance and behaviour by comparing individual tp models performance under multiple metrics we highlighted that many tp models were not of a threshold quality sufficient for accurate prediction of data from the time period they were fitted upon this is especially problematic considering that these models are required to be used in future simulations on unseen data regardless the capability to provide this insight is a clear advantage of stage 1 validation whereas with stage 2 validation it would have been necessary to perform disaggregation of the validation results to achieve this level of detail following this we showed how stage 1 validation can be used to understand how tp models may be performing poorly through inspection of the boyce and roc curves which is not possible with stage 2 validation given that the model predictions have already been discretized into binary values these curves offer insights into performance across the prediction probability gradient and whilst the roc approach is frequently used in lulcc ca studies that do incorporate stage 1 validation our application of the boyce curve to this domain is novel we demonstrated how the boyce curve can be used to identify prediction probabilities at which tp models are predicting less transitions than should be expected to occur by chance whilst this insight does not directly indicate how tp models can be improved it is useful in informing further investigation such as mapping the instances at prediction probabilities with low p e ratio values to identify possible causative factors ultimately poor performance of tp models must be addressed if they are to be utilized within a lulcc ca in this regard there are two general options to try and improve the models by keeping the algorithm the same but altering parameters du et al 2018 or to use a different algorithmic approach or a combination of multiple methods e g ensemble modelling shafizadeh moghadam 2019 we have demonstrated elements of both solutions through hyper parameter tuning and attempting to address class imbalance appendix c but primarily by exploring the benefits of predictor variable selection which has not been covered extensively within previous lulcc ca studies using rf kamusoko and gamba 2015 du et al 2018 gounaridis et al 2019 roodposhti et al 2019 zhang et al 2019 chen et al 2019 li and chen 2020 rienow et al 2021 we demonstrated that predictor variable selection not only improved tp model generalizability but also improved performance in a substantial proportion of cases whilst sometimes removing over 50 of predictors see figure b1 these findings have important consequences for tp models being used within lulcc cas to make predictions of future lulc because the models remain stationary but are fitted with new data and hence less generalizable models will produce less accurate simulations soares filho et al 2013 however it is important to highlight that these benefits of predictor variable selection differed between the model types utilized with the process having less impact on rf than lr this is likely because the regular rf algorithm is inherently robust to redundant variables to an extent due to the fact that variables are chosen during node splitting based on a measure of importance kubus 2018 also the fact that the majority of predictors that were removed were neighbourhood predictors which given that they represented different realizations of the same phenomena appendix a could have been disproportionately contributing to the overfitting of the models at the same time it is important to note that the conception of generalizability and the means of quantifying it employed in this research are not definitive indeed an alternative approach to quantifying generalizability would be to compare the performance of models trained on data from a given time period when used to predict transition potential for data from a subsequent period sometimes referred to as external validation ho et al 2020 the decision to instead utilise independent validation to quantify generalizability in this research was intended to demonstrate that this is possible with stage 1 validation but not stage 2 validation this is a useful capability of stage 1 validation given that future lulcc simulation modelling typically only utilizes tp models for the most recent time period available and hence we have an interest in quantifying whether these specific models are generalizable and of course this cannot be done using external validation because subsequent time period data does not exist however further research should be performed to compare the estimates of generalizability produced by independent versus external validation approaches regardless of how the improvement of tp model performance is pursued a final benefit of stage 1 validation is that it makes the process more efficient this is because it allows for the comparison of different specifications without the need to instantiate other parameters in the ca and run the allocation process to produce simulated lulc maps fig 1 in the context of this study only using stage 2 validation for hyper parameter testing four different rf ensemble sizes for both reduced and full datasets appendix c and the two additional lr specifications of tp models would have required the allocation process to be run a minimum of 10 times the time required for allocation varies dependent on the specific lulcc ca being utilized but generally it can be expected to scale with the size of the study area and the number of lulc transitions being modelled given that we are modelling lulcc at the scale of the whole of switzerland with 100 m resolution and 174 lulc transitions the fact that we did not have to prepare the ca model or run the allocation process to calibrate the tp models represents a substantial improvement in efficiency despite the benefits of utilising stage 1 validation as shown by this study and acknowledged by other authors kolb et al 2013 it still remains under utilized in lulcc ca studies tong and feng 2020 the reason for this is difficult to attribute but one suggestion is because extensive validation of tp models is often not central to the aims of many lulcc ca studies ibid for example where the goal of research is to devise and simulate a range of future lulcc scenarios it is understandable that model calibration is given limited attention another possible explanation could stem from the fact that many popular lulcc cas are proprietary software e g dinamica ego land change modeller and conduct the fitting of the tp models internally thereby removing some of the onus on the user to specify and interpret the models furthermore whilst many proprietary lulcc cas do include the option to calculate performance metrics for stage 1 validation these are limited in comparison to the means for stage 2 validation paegelow et al 2018 and by no means do they force users to explicitly consider the performance of the tp models in isolation on the other hand the fact that lulcc cas such as dinamica ego offer a flexible modular and graphical modelling environment makes them accessible to a greater range of users in this regard it is clear that promoting increased scrutiny of tp models through stage 1 validation should not come at the expense of the usability of lulcc ca software a possible solution to this in order to increase the adoption of stage 1 validation within lulcc cas studies could be the establishment of a universal protocol for the calibration of tp models that is generalizable across the popular lulcc ca models in this regard the framework proposed by moulds et al 2015 represents some progress however it is largely a software focused approach instead such a protocol should specifically elucidate the steps involved in preparing and evaluating tp models and the relevant aspects that should be considered such as the use of different model scales algorithmic techniques predictor variable selection addressing imbalanced datasets model uncertainty and the merits of different performance metrics whilst we do not present such a protocol we hope that by sharing the data and scripts that allow our research to be replicated we are contributing in a small way towards its development 5 conclusion our research has shown that directly evaluating probabilistic predictions of lulc transition models which we dub stage 1 validation has the potential to improve the efficiency and transparency of the calibration process for lulcc cas as highlighted the potential to utilise stage 1 validation is not novel rather it has been overlooked as part of the status quo approach for lulcc ca calibration we hope that this introspective approach to highlighting existing opportunities to improve practice could serve to stimulate similar efforts other fields of land use change modelling declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we gratefully acknowledge financial support from the swiss federal office for the environment foen under the project valpar ch values of the ecological infrastructure in swiss parks of the action plan of the swiss biodiversity strategy we would also like to thank philipp brun swiss federal institute for forest snow and landscape research for providing r scripts and functions for species distribution modelling that were adapted for this research appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105574 
25482,modflow is a modular hydrologic model developed by the united states geological service since 1984 the last version modflow 6 allows the use of finite volume fv general unstructured grid approach to solve the groundwater flow equations the fv method entails constraints that are implicitly satisfied by a voronoi discretization in this paper new versions of the freeware voro2mesh and tough2viewer tools are presented for voronoi grid generation for modflow originally developed for the tough family of codes both software has been improved to also work with modflow voro2mesh and tough2viewer have been applied to two case studies and the results have been compared with analytical solutions outcomes have demonstrated that voro2mesh grids give simulation results very close to locally refined grids created using the quadtree approach furthermore the simulation activities are substantially enhanced using tough2viewer keywords modflow tough unstructured grids groundwater modelling 3d voronoi data availability i have shared the link to my data at the attach file step data for unstructured voronoi grids for modflow original data mendeley data 1 introduction groundwater numerical modelling is nowadays one of the most used techniques for groundwater resource characterization numerical modelling implies the numerical resolution of the groundwater flow equations several finite difference fd software are available to the scientific community today among these it is worth mentioning the modular finite difference flow model modflow developed by the us geological survey harbaugh 2005 modflow is a command line software program written in fortran unstructured grid capabilities have been introduced in 2013 in the modflow usg version panday et al 2013 the use of unstructured grid allows for flexibility in grid design can be used for example to focus resolution along rivers and around wells or to subdiscretize individual layers to better represent hydrostratigraphic units panday et al 2013 in 2017 the general unstructured grid based on concepts developed for modflow usg has been implemented in modflow 6 hughes et al 2017 as for spatial discretization modflow 6 can use a generalized control volume finite difference cvfd formulation the cvfd method is a type of the finite volume fv method see for example bear and cheng 2010 sometimes referred to as an integrated finite difference method ifdm the cvfd formulation is therefore equivalent to the ifdm implemented in tough2 pruess et al 1999 the fv diagram requires a geometric constraint on the grid blocks in which each interface area between two blocks must be orthogonal to the segment connecting the two block nodes modflow 6 can use the ghost node correction method panday and langevin 2012 in order to correct the approximation introduced by the use of grids with non orthogonal geometry this allows local mass conservation modflow 6 can also use a local grid refinement lgr approach mehl and hill 2013 as implemented for example in modelmuse winston 2009 that automatically generates the ghost node file to allow corrections for the modflow 6 simulation results additionally modflow 6 allows application of the xt3d formulation approach for advanced capabilities to simulate three dimensional anisotropy and dispersion and correct grid errors for cell connections that violate generalized cvfd assumptions provost et al 2017 modflow simulation input files are in ascii text format while output files are in both ascii and binary format in the case of unstructured grids modflow 6 output files are limited to the binary format file format details can be found in the modflow 6 user manual hughes et al 2017 the modflow numerical model contains the information about grid blocks such as geometrical characteristics area of the interfaces among blocks volumes etc flow and hydrodynamic parameters porosity permeability etc initial and boundary conditions this information is stored in a set of input files using a specified file format simple numerical models can be edited manually but the need for increasing complexity and the use of very detailed numerical models necessitate the use of software tools to handle modflow input files modflow utility tools have also been developed to support geological information system gis software to generate modflow models directly from gis data bittner et al 2020 gardner et al 2018 guzman et al 2015 park et al 2019 rossetto et al 2019 tian et al 2018 its popularity lead to a widespread utilization of modflow in several fields and applications e g as a soil and water assessment tool swat aliyari et al 2019 bailey et al 2020 chen et al 2018 maleki tirabadi et al 2021 wei et al 2019 and water pollution elsayed and oumeraci 2018 esfahani et al 2021 in which modlow has been coupled with mt3d bedekar et al 2016 in order to improve the numerical model creation task and visualization of the results several commercial and free graphical user interface gui tools for modflow tools have been developed by the scientific community a review of modflow gui can be found in hariharan and uma shankar 2017 among the free tools for modflow 6 it is worth mentioning modelmuse winston 2009 freewat rossetto et al 2019 and flopy bakker et al 2016 modelmuse is a free and open source gui for modflow and related software developed by the u s geological survey usgs freewat is an open source and public domain gis integrated modelling environment for the simulation of water quantity and quality in surface water and groundwater with an integrated water management and planning module flopy is a python package for creating running and post processing modflow based models among the commercial tools it is worth mentioning visual modflow groundwater modelling system gms and groundwater vistas visual modflow is a commercial gui for modflow it was introduced by the waterloo hydrogeologic company in august 1994 hariharan and uma shankar 2017 gms is distributed from aquaveo groundwater vistas rumbaugh and rumbaugh 2011 is distributed by environmental simulations inc esi usa table 1 reports a summary of the main characteristics of the above mentioned tools for modflow as it can be noted from table 1 currently there are no free tools for voronoi grid generation and management for modflow available for the scientific community voronoi grids implicitly satisfy the geometrical constraint of the fv computation avoiding ghost node corrections and allowing local grid refinement without loss of accuracy in literature a growing number of algorithms and codes have been developed for voronoi tessellation among them we can find qhull barber et al 1996 paravt gonzález 2016 voro rycroft 2009 tetgen si 2015 cgal the cgal project 2018 these are general purpose tools and they need to be specifically adapted for a specific application the present paper illustrates application of a free toolset for modflow 6 to generate unstructured voronoi grids and to visualize simulation results obtained on unstructured voronoi grids the toolset is composed of two software programs voro2mesh bonduà et al 2017 and tough2viewer bondua et al 2012 bonduà et al 2017 bonduà and bortolotti 2020 originally developed for the tough family of codes this set of tests has unambiguously proven the effectiveness of the voro2mesh and tough2viewer to improve modflow numerical modelling 2 materials and methods the verification and validation of the two tools were performed by means of two case studies case study a compares the results of a 2d darcy flow problem numerically modelled with a lgr mehl and hill 2013 and a voronoi grid created using voro2mesh and edited in tough2viewer case study b compares the numerical simulations of the classic 2d quarter five spot problem with a well known analytical formulation several lgr grids have been created with both modelmuse and voro2mesh tools the workflow of the proposed tool set for the numerical simulation can be resumed in i voro2mesh grid generation ii grid visualization and editing by using tough2viewer iii modflow numerical computation iv tough2viewer results visualization the flowchart of the workflow is shown in fig 1 2 1 voro2mesh voro2mesh bonduà et al 2017 is a command line utility software that allows the creation of fully 3d voronoi tessellation of a convex domain voro2mesh is written in c and is based on the voro library rycroft 2009 basically voro2mesh can operate by using a set of seed points as inputs or it can generate the necessary seeds points by using a set of input surfaces representing the geological horizons originally developed for tough grids voro2mesh has been improved with modflow 6 grid generation capabilities modflow 6 does not allow the use of a fully 3d voronoi discretization but it allows 2 5d voronoi grid generation however unstructured layers can be managed thanks to the ghost nodes approach used by modflow 6 see hughes et al 2017 the grid generated by voro2mesh can be visualized and edited with tough2viewer the current version of voro2mesh for modflow grid generation is limited to 2d models in this new voro2mesh version the modflow 6 unstructured grid generation is activated by setting the specific keyword generate modflow2d disu 1 of the voro2mesh par parameter file the keyword iprn doubles will define the grid format as specified in table 2 voro2mesh generates a turnkey set of input files for modflow flow disu is the classic modflow ascii disu unstructured discretization file it contains the reference to the following files each containing specific data values of the grid blocks flow disu area dat the horizontal projected block area flow disu bottom dat the elevation of the bottom block face flow disu top dat the elevation of the top block face flow disu cl12 dat the array containing connection lengths between the centre of cell n and the face shared with each adjacent m cell flow disu hwva dat a symmetric array of size nja for horizontal connections entries in hwva are the horizontal width perpendicular to flow for vertical connections entries in hwva are the vertical area for flow flow disu ihc dat an index array indicating the direction between node n and all of its m connections flow disu ja dat a list of cell number n followed by its connecting cell numbers m for each of the m cells connected to cell n where nja from the modflow manual langevin et al 2017 is the sum of the number of connections and nodes including n to m and m to n and the total number of cells hwva from the modflow manual langevin et al 2017 is a symmetric array of size nja for horizontal connections entries in hwva are the horizontal widths perpendicular to flow for vertical connections entries in hwva are the vertical areas for flow m and n two connected cells an example of the output file is reported in appendix a referring to the grid in fig 4b in this work the seed points are generated outside voro2mesh in the case of structured cartesian grids in the case of unstructured grids the seed points are computed by voro2mesh using a centroidal weighted voronoi tessellation cwvt approach 2 2 tough2viewer tough2viewer is originally developed for simulation result visualization of tough codes which use the integral finite difference method ifdm a fv method to solve governing heat and mass balance equations tough2viewer is not only a viewer but it allows modification of grid block proprieties boundaries and initial conditions through intuitive gui in the scope of presented work tough2viewer capabilities have been extended allowing the visualization of modflow 6 simulation results and modification of grid block proprieties boundaries and initial conditions tough2viewer is written in java and uses the java3d library for 3d rendering of finite volume grid models regarding tough2viewer modflow 6 visualization capabilities tough2viewer can read disu output files binary format and disv grid files the input files needed for a modflow 6 disu or disv grid visualization are tough2viewer dat file the file generated by voro2mesh during disu grid generation it contains the geometric information of the blocks such as node coordinates number of vertices vertex coordinates face vertices index etc a detailed explanation of the tough2viewer dat file can be found in bonduà et al 2017 optionally tough2viewer can also read disv binary files for result visualization hds or bhd binary file the hydraulic head file results generated by modflow during a simulation run grb binary file contains the flow between connected blocks cbc binary file contains the budget information ic an ascii file containing the initial conditions of the numerical model chd ascii file the constant head specification npf ascii file node property flow npf package these files are read in tough2viewer by a dedicated gui as shown in fig 2 modflow 6 simulation results can be directly visualized in tough2viewer or can be exported from tough2viewer in several formats such as csv tables txt files or in paraview 1 1 http www paraview org last access 2022 09 29 file format vtu ahrens et al 2005 allowing an enhanced 3d model visualization see fig 3 2 3 case studies 2 3 1 case a the 2d darcy flow this case study consists of two dimensional confined groundwater steady state flow in an homogenous aquifer as described in panday et al 2013 the domain size is 700 700 100 m3 the case study is analysed by using three grids the first one 01 darcy quadtree is a quadtree grid and it belongs to the example data set included with the modflow distribution it is composed of two nested grids a coarse and a fine grid in the coarse grid the element size is 100 100 100 m3 while in the fine grid element size is 33 33 33 33 100 m3 see fig 4a the second grid 02 darcy quadtree no gnc has the same geometry as the first grid above but it does not use the ghost node correction gnc the third grid 03 darcy voronoi is a voronoi grid generated by voro2mesh the seed points used for the tessellation have the same coordinates as the cell centres of the nested grid see fig 4b boundary conditions have been set using the prescribed head boundary package in modflow by setting the elements at coordinate of x 50 m a constant head of 1 0 m while elements at coordinate of x 650 m a constant head of 0 0 m the permeability is 1 0 m s the numerical solutions are compared with the available analytical solution 2 3 2 case b the 2d quarter five spot problem in this case study we used the classic five spot scheme in the case of a totally penetrating well in an confined aquifer an analytical solution for the computation of the hydraulic head changes in space and time exists theis 1935 h 0 h q 4 π h u e a a a w i t h u r 2 s 4 t t where q is the constant pumping rate h the hydraulic head h 0 the initial hydraulic head t the time since pumping has begun r the radial distance from the pumping well s the storativity coefficient the five spot problem is defined as an infinite set of injection and production wells in a staggered pattern see fig 5 the injection and production wells were arranged on a regular staggered grid of infinite extent in this configuration the superposition effects of the theis solution can be applied for symmetry reasons it is sufficient to consider a quarter of the total five spot domain a dirichlet boundary condition with zero flow was used for lateral surface of the square domain initial condition of the entire model has been set to hydraulic head equal to 0 0 m the analytical pressure state has therefore been computed for a quarter domain applying the superposition effect of a theoretically infinite but in practice limited to 500 500 elements staggered set of injection production wells in the 500 0 500 0 m2 square domain with a volumetric flow rate of 4 0 m3 s the parameters of the model are given in table 3 in order to verify and validate the toolset code the five spot problem has been simulated using different grid types structured lgr and unstructured voronoi grids table 4 summarizes the generated grids the 01 structured fig 6 a 02 voronoi fig 6b 04 voronoi wells refined fig 6d and 06 voronoi wells line refined fig 6f have been generated using voro2mesh the 03 quadtree wells refined fig 6c and 05 quadtree wells line refined fig 6e have been generated by using modelmuse version 4 0 0 0 the 01 structured grid fig 6a is a 10x10 structured grid composed of 100 blocks of the same size volume the 02 voronoi fig 6 b grid is composed of 100 grid blocks in which the blocks near the two wells have the same position as the 01 structured grid and the other blocks satisfy the centroidal voronoi tessellation cvt definition the seed nodes are the centres of mass of the generated voronoi blocks du et al 1999 the 03 quadtree wells refined grid fig 6c is a quadtree grid obtained with a level 6 quadtree refinement the 04 voronoi wells refined grid fig 6d has the same number of blocks as the 03 quadtree wells refined fig 6c but the remaining elements follow the weighted cvt approach wcvt inaba et al 1994 the 05 quadtree wells line refined fig 6e grid is similar to the 03 quadtree wells refined grid but there is a non symmetrical grid distortion along a line the 06 voronoi wells line refined grid has the same number of grid blocks as 05 quadtree wells line refined and follows the wcvt approach the computed dynamic evolution of the hydraulic head pressure has been computed considering an injection well with a volumetric rate of 1 0 m3 s located at 0 01 0 01 and a production well with a volumetric rate of 1 0 m3 s located at the point 499 9 499 9 3 results and discussion 3 1 case a the 2d darcy flow problem the analytical solution of the presented problem is represented by the formula 2 2 https modflow6 examples readthedocs io en master examples ex gwf u1gwfgwf html last accessed 2022 09 15 h 1 1 600 x 50 where h is the hydraulic head m and x is the abscissa of the point with x 50 0 650 0 fig 7 and table 5 report the deviations of the simulation results computed on the three numerical models with respect to the analytical solutions the deviations of the analytical and the numerical solutions of hydraulic head value using different grids are always in good agreement the use of the gnc correction 01 darcy quadtree grid allows reduction of the deviation by four orders of magnitude see table 5 with respect to the no gnc grid 02 darcy quadtree no gnc the voronoi grid shows a deviation very similar to the quadtree grid with a slight improvement in accuracy see table 5 and fig 7 3 2 case b the 2d five spot problem the plots of the deviation between analytical and numerical head computations with modflow 6 6 0 4 version as a function of the distance between the simulated block node and the well are shown in fig 8 the heads computed on the first two grids fig 8a show a very similar behaviour and the deviations from the analytical solution are almost identical note that in the voronoi grid the 3 blocks near the well sites have the same coordinates as the structured grid blocks for the blocks located near the wells it is possible to observe a marked difference of the voronoi grid node position with respect to the quadtree grid blocks distance from 70 up to 120 m the difference is reduced at greater distances it is worth mentioning that the physical system cannot be exactly replicated by the numerical model so for example because of the finite dimension of the grid blocks the wells cannot be located at coordinate 0 0 0 0 and 500 0 500 00 but in this simple case they are located inevitably at 50 0 50 0 and 450 0 450 0 respectively for the locally refined grids near the wells 03 quadtree and 04 voronoi grids the grid blocks representing the wells and the surrounding three blocks have the same coordinates from the points plotted in fig 8b and fig 8c it is possible to observe a similar behaviour as that observed in the previous comparison the deviations of the voronoi grid are higher near the wells ca in the first 5 m they are lower from 5 to 120 m and they are slightly higher for the longer distances for the last two grids the 05 quadtree and the 06 voronoi we observe a very similar behaviour as in the previous two cases the spatial distributions of the deviations deviation maps with respect to the analytical solutions at the end of the simulation are shown in fig 9 the deviation maps shown in fig 9 note that the colours refer to the absolute value of the deviation and they are in logarithmic scale denotes a higher deviation related in general to the proximity to the well nodes as already highlighted by the graphs in fig 8 the quadtree grids fig 9c and fig 9b reveals higher and lower deviations near the blocks where the block size changes due to the quadtree refinement due to the ghost node correction operated by modflow to reduce the error introduced when using non conformal orthogonal grids conversely the voronoi grids do not suffer from the grid change resolution the deviation range lower and upper values average and variance are shown in table 6 in general results show that the deviation observed in the six grids are very close to the analytical solution used as a reference and therefore that all grids can be used to correctly reproduce the five spot problem the voronoi grid exhibits higher isolated deviation errors near the wells while the deviation average in the whole domain is lower as well as the standard deviation is lower for the voronoi grids the use of the voronoi grids allows a local grid refinement where model features require a finer resolution without introducing ghost node corrections and reduce the deviation between the analytical and numerical model results near lgr zones the computational time for the presented problem is shown in table 7 the simulation has been performed using a pc equipped with a intel r core tm i7 6700hq cpu 2 60 ghz 16 mb of ram the computation time of the unstructured voronoi grids is lower than the lgr grids the gap increases by increasing the size of the grid 4 conclusions voro2mesh and tough2viewer provides a flexible tool for the calculation of voronoi grids grid editing and results visualization for modflow numerical models to the best knowledge of the authors there are not dedicated free and open source software for voronoi grid generation for modflow published in the scientific literature among the others capabilities it is worth to mention the capability of tough2viewer of exporting the numerical results in a file format for paraview one of the most powerful visualization tools available in the literature for scientific applications the validity of the two tools has been demonstrated on two case studies the comparison of numerical simulation results with analytical solutions strongly highlighted the usefulness of these tools in simulations in the local grid refinements with voronoi grids which implicitly satisfy the orthogonality requirement there is a small lower loss of accuracy with respect to the analytical solution than when the refinements are made on square grids that adopt the ghost node corrections due to its utility and simplicity of use it is expected that voro2mesh and tough2viewer will assist hydrogeologists and environmental scientists in using modflow simulator with voronoi grids improving the performance of the simulation tasks including results analysis and visualization future work will include the possibility of voro2mesh to produce 2 5d grids for modflow voro2mesh and tough2viewer can be freely used and downloaded from https site unibo it softwaredicam en software voro2mesh and https site unibo it softwaredicam en software tough2viewer software availability name of the software voro2mesh and tough2viewer developer s bonduà contact address stefano bondua unibo it designed by s bonduà v bortolotti k strpić first year available 2020 hardware requirement pc software requirement windows os program language c java program size voro2mesh 475 kb binary file tough2viewer 20 mb binary file availability voro2mesh freely available as a binary file at https site unibo it softwaredicam en software voro2mesh tough2viewer freely available as a binary file at https site unibo it softwaredicam en software tough2viewer source code is available at https github com stebond tough2viewer declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105563 
25482,modflow is a modular hydrologic model developed by the united states geological service since 1984 the last version modflow 6 allows the use of finite volume fv general unstructured grid approach to solve the groundwater flow equations the fv method entails constraints that are implicitly satisfied by a voronoi discretization in this paper new versions of the freeware voro2mesh and tough2viewer tools are presented for voronoi grid generation for modflow originally developed for the tough family of codes both software has been improved to also work with modflow voro2mesh and tough2viewer have been applied to two case studies and the results have been compared with analytical solutions outcomes have demonstrated that voro2mesh grids give simulation results very close to locally refined grids created using the quadtree approach furthermore the simulation activities are substantially enhanced using tough2viewer keywords modflow tough unstructured grids groundwater modelling 3d voronoi data availability i have shared the link to my data at the attach file step data for unstructured voronoi grids for modflow original data mendeley data 1 introduction groundwater numerical modelling is nowadays one of the most used techniques for groundwater resource characterization numerical modelling implies the numerical resolution of the groundwater flow equations several finite difference fd software are available to the scientific community today among these it is worth mentioning the modular finite difference flow model modflow developed by the us geological survey harbaugh 2005 modflow is a command line software program written in fortran unstructured grid capabilities have been introduced in 2013 in the modflow usg version panday et al 2013 the use of unstructured grid allows for flexibility in grid design can be used for example to focus resolution along rivers and around wells or to subdiscretize individual layers to better represent hydrostratigraphic units panday et al 2013 in 2017 the general unstructured grid based on concepts developed for modflow usg has been implemented in modflow 6 hughes et al 2017 as for spatial discretization modflow 6 can use a generalized control volume finite difference cvfd formulation the cvfd method is a type of the finite volume fv method see for example bear and cheng 2010 sometimes referred to as an integrated finite difference method ifdm the cvfd formulation is therefore equivalent to the ifdm implemented in tough2 pruess et al 1999 the fv diagram requires a geometric constraint on the grid blocks in which each interface area between two blocks must be orthogonal to the segment connecting the two block nodes modflow 6 can use the ghost node correction method panday and langevin 2012 in order to correct the approximation introduced by the use of grids with non orthogonal geometry this allows local mass conservation modflow 6 can also use a local grid refinement lgr approach mehl and hill 2013 as implemented for example in modelmuse winston 2009 that automatically generates the ghost node file to allow corrections for the modflow 6 simulation results additionally modflow 6 allows application of the xt3d formulation approach for advanced capabilities to simulate three dimensional anisotropy and dispersion and correct grid errors for cell connections that violate generalized cvfd assumptions provost et al 2017 modflow simulation input files are in ascii text format while output files are in both ascii and binary format in the case of unstructured grids modflow 6 output files are limited to the binary format file format details can be found in the modflow 6 user manual hughes et al 2017 the modflow numerical model contains the information about grid blocks such as geometrical characteristics area of the interfaces among blocks volumes etc flow and hydrodynamic parameters porosity permeability etc initial and boundary conditions this information is stored in a set of input files using a specified file format simple numerical models can be edited manually but the need for increasing complexity and the use of very detailed numerical models necessitate the use of software tools to handle modflow input files modflow utility tools have also been developed to support geological information system gis software to generate modflow models directly from gis data bittner et al 2020 gardner et al 2018 guzman et al 2015 park et al 2019 rossetto et al 2019 tian et al 2018 its popularity lead to a widespread utilization of modflow in several fields and applications e g as a soil and water assessment tool swat aliyari et al 2019 bailey et al 2020 chen et al 2018 maleki tirabadi et al 2021 wei et al 2019 and water pollution elsayed and oumeraci 2018 esfahani et al 2021 in which modlow has been coupled with mt3d bedekar et al 2016 in order to improve the numerical model creation task and visualization of the results several commercial and free graphical user interface gui tools for modflow tools have been developed by the scientific community a review of modflow gui can be found in hariharan and uma shankar 2017 among the free tools for modflow 6 it is worth mentioning modelmuse winston 2009 freewat rossetto et al 2019 and flopy bakker et al 2016 modelmuse is a free and open source gui for modflow and related software developed by the u s geological survey usgs freewat is an open source and public domain gis integrated modelling environment for the simulation of water quantity and quality in surface water and groundwater with an integrated water management and planning module flopy is a python package for creating running and post processing modflow based models among the commercial tools it is worth mentioning visual modflow groundwater modelling system gms and groundwater vistas visual modflow is a commercial gui for modflow it was introduced by the waterloo hydrogeologic company in august 1994 hariharan and uma shankar 2017 gms is distributed from aquaveo groundwater vistas rumbaugh and rumbaugh 2011 is distributed by environmental simulations inc esi usa table 1 reports a summary of the main characteristics of the above mentioned tools for modflow as it can be noted from table 1 currently there are no free tools for voronoi grid generation and management for modflow available for the scientific community voronoi grids implicitly satisfy the geometrical constraint of the fv computation avoiding ghost node corrections and allowing local grid refinement without loss of accuracy in literature a growing number of algorithms and codes have been developed for voronoi tessellation among them we can find qhull barber et al 1996 paravt gonzález 2016 voro rycroft 2009 tetgen si 2015 cgal the cgal project 2018 these are general purpose tools and they need to be specifically adapted for a specific application the present paper illustrates application of a free toolset for modflow 6 to generate unstructured voronoi grids and to visualize simulation results obtained on unstructured voronoi grids the toolset is composed of two software programs voro2mesh bonduà et al 2017 and tough2viewer bondua et al 2012 bonduà et al 2017 bonduà and bortolotti 2020 originally developed for the tough family of codes this set of tests has unambiguously proven the effectiveness of the voro2mesh and tough2viewer to improve modflow numerical modelling 2 materials and methods the verification and validation of the two tools were performed by means of two case studies case study a compares the results of a 2d darcy flow problem numerically modelled with a lgr mehl and hill 2013 and a voronoi grid created using voro2mesh and edited in tough2viewer case study b compares the numerical simulations of the classic 2d quarter five spot problem with a well known analytical formulation several lgr grids have been created with both modelmuse and voro2mesh tools the workflow of the proposed tool set for the numerical simulation can be resumed in i voro2mesh grid generation ii grid visualization and editing by using tough2viewer iii modflow numerical computation iv tough2viewer results visualization the flowchart of the workflow is shown in fig 1 2 1 voro2mesh voro2mesh bonduà et al 2017 is a command line utility software that allows the creation of fully 3d voronoi tessellation of a convex domain voro2mesh is written in c and is based on the voro library rycroft 2009 basically voro2mesh can operate by using a set of seed points as inputs or it can generate the necessary seeds points by using a set of input surfaces representing the geological horizons originally developed for tough grids voro2mesh has been improved with modflow 6 grid generation capabilities modflow 6 does not allow the use of a fully 3d voronoi discretization but it allows 2 5d voronoi grid generation however unstructured layers can be managed thanks to the ghost nodes approach used by modflow 6 see hughes et al 2017 the grid generated by voro2mesh can be visualized and edited with tough2viewer the current version of voro2mesh for modflow grid generation is limited to 2d models in this new voro2mesh version the modflow 6 unstructured grid generation is activated by setting the specific keyword generate modflow2d disu 1 of the voro2mesh par parameter file the keyword iprn doubles will define the grid format as specified in table 2 voro2mesh generates a turnkey set of input files for modflow flow disu is the classic modflow ascii disu unstructured discretization file it contains the reference to the following files each containing specific data values of the grid blocks flow disu area dat the horizontal projected block area flow disu bottom dat the elevation of the bottom block face flow disu top dat the elevation of the top block face flow disu cl12 dat the array containing connection lengths between the centre of cell n and the face shared with each adjacent m cell flow disu hwva dat a symmetric array of size nja for horizontal connections entries in hwva are the horizontal width perpendicular to flow for vertical connections entries in hwva are the vertical area for flow flow disu ihc dat an index array indicating the direction between node n and all of its m connections flow disu ja dat a list of cell number n followed by its connecting cell numbers m for each of the m cells connected to cell n where nja from the modflow manual langevin et al 2017 is the sum of the number of connections and nodes including n to m and m to n and the total number of cells hwva from the modflow manual langevin et al 2017 is a symmetric array of size nja for horizontal connections entries in hwva are the horizontal widths perpendicular to flow for vertical connections entries in hwva are the vertical areas for flow m and n two connected cells an example of the output file is reported in appendix a referring to the grid in fig 4b in this work the seed points are generated outside voro2mesh in the case of structured cartesian grids in the case of unstructured grids the seed points are computed by voro2mesh using a centroidal weighted voronoi tessellation cwvt approach 2 2 tough2viewer tough2viewer is originally developed for simulation result visualization of tough codes which use the integral finite difference method ifdm a fv method to solve governing heat and mass balance equations tough2viewer is not only a viewer but it allows modification of grid block proprieties boundaries and initial conditions through intuitive gui in the scope of presented work tough2viewer capabilities have been extended allowing the visualization of modflow 6 simulation results and modification of grid block proprieties boundaries and initial conditions tough2viewer is written in java and uses the java3d library for 3d rendering of finite volume grid models regarding tough2viewer modflow 6 visualization capabilities tough2viewer can read disu output files binary format and disv grid files the input files needed for a modflow 6 disu or disv grid visualization are tough2viewer dat file the file generated by voro2mesh during disu grid generation it contains the geometric information of the blocks such as node coordinates number of vertices vertex coordinates face vertices index etc a detailed explanation of the tough2viewer dat file can be found in bonduà et al 2017 optionally tough2viewer can also read disv binary files for result visualization hds or bhd binary file the hydraulic head file results generated by modflow during a simulation run grb binary file contains the flow between connected blocks cbc binary file contains the budget information ic an ascii file containing the initial conditions of the numerical model chd ascii file the constant head specification npf ascii file node property flow npf package these files are read in tough2viewer by a dedicated gui as shown in fig 2 modflow 6 simulation results can be directly visualized in tough2viewer or can be exported from tough2viewer in several formats such as csv tables txt files or in paraview 1 1 http www paraview org last access 2022 09 29 file format vtu ahrens et al 2005 allowing an enhanced 3d model visualization see fig 3 2 3 case studies 2 3 1 case a the 2d darcy flow this case study consists of two dimensional confined groundwater steady state flow in an homogenous aquifer as described in panday et al 2013 the domain size is 700 700 100 m3 the case study is analysed by using three grids the first one 01 darcy quadtree is a quadtree grid and it belongs to the example data set included with the modflow distribution it is composed of two nested grids a coarse and a fine grid in the coarse grid the element size is 100 100 100 m3 while in the fine grid element size is 33 33 33 33 100 m3 see fig 4a the second grid 02 darcy quadtree no gnc has the same geometry as the first grid above but it does not use the ghost node correction gnc the third grid 03 darcy voronoi is a voronoi grid generated by voro2mesh the seed points used for the tessellation have the same coordinates as the cell centres of the nested grid see fig 4b boundary conditions have been set using the prescribed head boundary package in modflow by setting the elements at coordinate of x 50 m a constant head of 1 0 m while elements at coordinate of x 650 m a constant head of 0 0 m the permeability is 1 0 m s the numerical solutions are compared with the available analytical solution 2 3 2 case b the 2d quarter five spot problem in this case study we used the classic five spot scheme in the case of a totally penetrating well in an confined aquifer an analytical solution for the computation of the hydraulic head changes in space and time exists theis 1935 h 0 h q 4 π h u e a a a w i t h u r 2 s 4 t t where q is the constant pumping rate h the hydraulic head h 0 the initial hydraulic head t the time since pumping has begun r the radial distance from the pumping well s the storativity coefficient the five spot problem is defined as an infinite set of injection and production wells in a staggered pattern see fig 5 the injection and production wells were arranged on a regular staggered grid of infinite extent in this configuration the superposition effects of the theis solution can be applied for symmetry reasons it is sufficient to consider a quarter of the total five spot domain a dirichlet boundary condition with zero flow was used for lateral surface of the square domain initial condition of the entire model has been set to hydraulic head equal to 0 0 m the analytical pressure state has therefore been computed for a quarter domain applying the superposition effect of a theoretically infinite but in practice limited to 500 500 elements staggered set of injection production wells in the 500 0 500 0 m2 square domain with a volumetric flow rate of 4 0 m3 s the parameters of the model are given in table 3 in order to verify and validate the toolset code the five spot problem has been simulated using different grid types structured lgr and unstructured voronoi grids table 4 summarizes the generated grids the 01 structured fig 6 a 02 voronoi fig 6b 04 voronoi wells refined fig 6d and 06 voronoi wells line refined fig 6f have been generated using voro2mesh the 03 quadtree wells refined fig 6c and 05 quadtree wells line refined fig 6e have been generated by using modelmuse version 4 0 0 0 the 01 structured grid fig 6a is a 10x10 structured grid composed of 100 blocks of the same size volume the 02 voronoi fig 6 b grid is composed of 100 grid blocks in which the blocks near the two wells have the same position as the 01 structured grid and the other blocks satisfy the centroidal voronoi tessellation cvt definition the seed nodes are the centres of mass of the generated voronoi blocks du et al 1999 the 03 quadtree wells refined grid fig 6c is a quadtree grid obtained with a level 6 quadtree refinement the 04 voronoi wells refined grid fig 6d has the same number of blocks as the 03 quadtree wells refined fig 6c but the remaining elements follow the weighted cvt approach wcvt inaba et al 1994 the 05 quadtree wells line refined fig 6e grid is similar to the 03 quadtree wells refined grid but there is a non symmetrical grid distortion along a line the 06 voronoi wells line refined grid has the same number of grid blocks as 05 quadtree wells line refined and follows the wcvt approach the computed dynamic evolution of the hydraulic head pressure has been computed considering an injection well with a volumetric rate of 1 0 m3 s located at 0 01 0 01 and a production well with a volumetric rate of 1 0 m3 s located at the point 499 9 499 9 3 results and discussion 3 1 case a the 2d darcy flow problem the analytical solution of the presented problem is represented by the formula 2 2 https modflow6 examples readthedocs io en master examples ex gwf u1gwfgwf html last accessed 2022 09 15 h 1 1 600 x 50 where h is the hydraulic head m and x is the abscissa of the point with x 50 0 650 0 fig 7 and table 5 report the deviations of the simulation results computed on the three numerical models with respect to the analytical solutions the deviations of the analytical and the numerical solutions of hydraulic head value using different grids are always in good agreement the use of the gnc correction 01 darcy quadtree grid allows reduction of the deviation by four orders of magnitude see table 5 with respect to the no gnc grid 02 darcy quadtree no gnc the voronoi grid shows a deviation very similar to the quadtree grid with a slight improvement in accuracy see table 5 and fig 7 3 2 case b the 2d five spot problem the plots of the deviation between analytical and numerical head computations with modflow 6 6 0 4 version as a function of the distance between the simulated block node and the well are shown in fig 8 the heads computed on the first two grids fig 8a show a very similar behaviour and the deviations from the analytical solution are almost identical note that in the voronoi grid the 3 blocks near the well sites have the same coordinates as the structured grid blocks for the blocks located near the wells it is possible to observe a marked difference of the voronoi grid node position with respect to the quadtree grid blocks distance from 70 up to 120 m the difference is reduced at greater distances it is worth mentioning that the physical system cannot be exactly replicated by the numerical model so for example because of the finite dimension of the grid blocks the wells cannot be located at coordinate 0 0 0 0 and 500 0 500 00 but in this simple case they are located inevitably at 50 0 50 0 and 450 0 450 0 respectively for the locally refined grids near the wells 03 quadtree and 04 voronoi grids the grid blocks representing the wells and the surrounding three blocks have the same coordinates from the points plotted in fig 8b and fig 8c it is possible to observe a similar behaviour as that observed in the previous comparison the deviations of the voronoi grid are higher near the wells ca in the first 5 m they are lower from 5 to 120 m and they are slightly higher for the longer distances for the last two grids the 05 quadtree and the 06 voronoi we observe a very similar behaviour as in the previous two cases the spatial distributions of the deviations deviation maps with respect to the analytical solutions at the end of the simulation are shown in fig 9 the deviation maps shown in fig 9 note that the colours refer to the absolute value of the deviation and they are in logarithmic scale denotes a higher deviation related in general to the proximity to the well nodes as already highlighted by the graphs in fig 8 the quadtree grids fig 9c and fig 9b reveals higher and lower deviations near the blocks where the block size changes due to the quadtree refinement due to the ghost node correction operated by modflow to reduce the error introduced when using non conformal orthogonal grids conversely the voronoi grids do not suffer from the grid change resolution the deviation range lower and upper values average and variance are shown in table 6 in general results show that the deviation observed in the six grids are very close to the analytical solution used as a reference and therefore that all grids can be used to correctly reproduce the five spot problem the voronoi grid exhibits higher isolated deviation errors near the wells while the deviation average in the whole domain is lower as well as the standard deviation is lower for the voronoi grids the use of the voronoi grids allows a local grid refinement where model features require a finer resolution without introducing ghost node corrections and reduce the deviation between the analytical and numerical model results near lgr zones the computational time for the presented problem is shown in table 7 the simulation has been performed using a pc equipped with a intel r core tm i7 6700hq cpu 2 60 ghz 16 mb of ram the computation time of the unstructured voronoi grids is lower than the lgr grids the gap increases by increasing the size of the grid 4 conclusions voro2mesh and tough2viewer provides a flexible tool for the calculation of voronoi grids grid editing and results visualization for modflow numerical models to the best knowledge of the authors there are not dedicated free and open source software for voronoi grid generation for modflow published in the scientific literature among the others capabilities it is worth to mention the capability of tough2viewer of exporting the numerical results in a file format for paraview one of the most powerful visualization tools available in the literature for scientific applications the validity of the two tools has been demonstrated on two case studies the comparison of numerical simulation results with analytical solutions strongly highlighted the usefulness of these tools in simulations in the local grid refinements with voronoi grids which implicitly satisfy the orthogonality requirement there is a small lower loss of accuracy with respect to the analytical solution than when the refinements are made on square grids that adopt the ghost node corrections due to its utility and simplicity of use it is expected that voro2mesh and tough2viewer will assist hydrogeologists and environmental scientists in using modflow simulator with voronoi grids improving the performance of the simulation tasks including results analysis and visualization future work will include the possibility of voro2mesh to produce 2 5d grids for modflow voro2mesh and tough2viewer can be freely used and downloaded from https site unibo it softwaredicam en software voro2mesh and https site unibo it softwaredicam en software tough2viewer software availability name of the software voro2mesh and tough2viewer developer s bonduà contact address stefano bondua unibo it designed by s bonduà v bortolotti k strpić first year available 2020 hardware requirement pc software requirement windows os program language c java program size voro2mesh 475 kb binary file tough2viewer 20 mb binary file availability voro2mesh freely available as a binary file at https site unibo it softwaredicam en software voro2mesh tough2viewer freely available as a binary file at https site unibo it softwaredicam en software tough2viewer source code is available at https github com stebond tough2viewer declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105563 
25483,salinity can have major detrimental impacts on soil health crop yield and environmental ecosystem services in this paper we introduce swat modflow salt a version of swat modflow that simulates the fate and transport of major salt ions so4 2 cl co3 2 hco3 ca2 na mg2 k in watershed systems and focus on its development and application to irrigated stream aquifer systems the modeling code accounts for water and salt mass storage in soils aquifer and streams and the movement of water and salt mass in surface runoff soil lateral flow percolation and recharge groundwater flow and groundwater surface water exchange the model is applied to a highly managed irrigated 700 km2 region of the saline lower arkansas river valley in colorado usa the model is tested against groundwater salt concentrations and in stream salt mass loadings and then used to quantify the impact of soil salinity on crop yield keywords swat salinity modflow groundwater irrigation data availability in summary and conclusions we have included a github link to the code software availability name of software swat modflow salt fortran 421 files 7 4 mb developer and contact information ryan bailey rtbailey colostate edu year first available 2022 hardware required standard desktop pc software required none availability and cost source files are freely available for download at https github com ryantbailey swat modflow salt 1 introduction salinity is a common water quality threat in many watersheds worldwide specific threats include soil salinization and associated decrease in crop yield groundwater salinity in drinking water ecosystem health impacts and the health and diversity of fish and waterfowl populations the threat of soil salinization is especially pronounced for arid and semi arid regions that use irrigation water to sustain crop growth with inadequate drainage leading to evapo concentration of salts in the root zone of the soil profile approximately one third of irrigated land is adversely affected by salinity shrivastava and kumar 2015 with a loss of 0 25 0 5 million ha each year globally nlwra 2001 the movement of salt in these systems must be tracked and managed to ameliorate conditions for cultivation and general ecosystem services numerical hydrologic models are often used as tools in the general framework of salt management these models estimate salt storage in watershed systems soil profile aquifer streams and simulate salt movement in various hydrologic pathways and when tested against salt measurement data from real systems can be used in scenario analysis to determine the effect of climate and management on salt pollution available published models vary in terms of the type of system simulated and type of salinity transport algorithm some models focus on the saturated zone burkhalter and gates 2005 on the soil profile and shallow aquifer šimůnek and suarez 1994 on large scale variably saturated groundwater systems schoups et al 2005 tavakoli kivi et al 2019 on land surface and streams bailey et al 2019 tirabadi et al 2021 and on surface subsurface systems bailey et al 2022 table 1 lists these studies and the corresponding model names along with attributes of each model including land scape transport stream transport groundwater transport physically based spatially distributed pbsd groundwater simulation the dimension of the groundwater model and equilibrium ion chemistry while each of these approaches and models listed in table 1 has unique strengths none can be applied to simulate the movement and fate of salt ion species in all major hydrologic reservoirs and pathways of a watershed system subject to irrigation hydrus 2d 3d and rt3d salt focus on subsurface systems whereas swat salt and swat s focus on land surface soil and stream systems with significant limitations in groundwater salt storage and transport i e not simulating pbsd groundwater flow and interactions with surrounding hydrologic features apex modflow salt does simulate pbsd groundwater flow but does not account for salinity in surface water irrigation water or groundwater i e pumped irrigation only being applied to a mountainous natural system bailey et al 2022 in addition the code does not include water and salt ion mass exchange between the soil profile and the aquifer when shallow groundwater occurs there is a need for a model that combines the strengths of the available models listed in table 1 to provide comprehensive assessment of salt movement in managed watershed systems thereby providing an asset for salt management in future decades this paper has two objectives the first objective is to present a new version of swat modflow bailey et al 2106 that accounts for salt ion so4 2 cl co3 2 hco3 ca2 na mg2 k transport in the soil profile the groundwater system and the stream network of a given watershed system along with soil groundwater and groundwater stream salt ion mass exchange swat modflow is a coupled surface subsurface hydro chemical model that has been used extensively in recent years e g aliyari et al 2019 chunn et al 2019 gao et al 2019 molina navarro et al 2019 liu et al 2020 guevara ochoa et al 2020 wei and bailey 2021 for simulating water and nutrient movement in watershed systems the second objective is to demonstrate the use of the model for a saline irrigated watershed system for the first objective the salt module used in swat salt bailey et al 2019 see table 1 is integrated into the swat modflow modeling code also for the first time the swat modflow modeling code includes provisions for transferring water and solute mass from the aquifer to the root zone when the water table is within the soil profile for the second objective the watershed system is located in southeastern colorado along the alluvial aquifer system of the arkansas river which has experienced elevated salinity in soils groundwater and streams during recent decades due to long term irrigation and lack of adequate drainage in a semi arid climate 2 swat modflow salt development this section provides an overview of the swat modflow salt modeling code which is a modification of the most recent swat modflow version wei and bailey 2021 to include the salinity module developed for swat bailey et al 2019 the evolution of swat modflow from initial development for hydrologic applications to more recent modifications for groundwater nutrient transport is presented followed by the modifications performed for this current study to include salinity fate and transport for both surface and subsurface processes 2 1 swat modflow 2 1 1 overview of original swat modflow code the swat modflow modeling code bailey et al 2106 combines the swat watershed modeling modeling code arnold et al 1998 and the modflow nwt groundwater flow modeling code niswonger et al 2011 into a single fortran code that when compiled provides a single executable for model simulation within the linked model swat simulates land surface soil and stream hydrologic processes whereas modflow simulates groundwater hydrologic processes including groundwater surface water exchange fig 1 shows the respective processes simulated by swat and modflow black and blue text respectively within the swat modflow model swat is a physically based hydrologic simulator for watershed systems operating on a daily time the model calculates the mass balance of water sediment and nutrients nitrogen and phosphorus for the soil profile and aquifer of individual hydrologic response units hrus with each hru a unique combination of soil type topographic slope and land use within a subbasin of a delineated watershed these mass balance equations result in the estimation of system response variables e g soil water content nitrate concentration in soil water and groundwater phosphorus concentration in soil water and fluxes surface runoff soil lateral flow groundwater discharge corresponding nitrogen and phosphorus loads for each day of the simulation period the portion of each flux reaching the subbasin stream is calculated with values summed from all hrus to provide an aggregated mass loaded to the stream the water sediment and nutrient loaded to each stream is then routed through the watershed stream network with all mass eventually exiting the watershed through the watershed outlet modflow is a numerical model developed and maintained by the united state geological survey that simulates the storage and movement of groundwater in an aquifer system using physically based principles of groundwater storage and groundwater flow equations of storage and flow are based on aquifer properties hydraulic conductivity k m day specific yield s y specific storage s s 1 m and aquifer hydrologic inflows and outflows e g recharge pumping stream seepage groundwater discharge to streams that can vary in space throughout the aquifer system the modeling code reads in spatially distributed data of aquifer properties and aquifer inflows outflows and then solves for groundwater head h m above a datum at predefined locations within the aquifer system using the following equation for unconfined aquifers with one equation for each computational point 1 x f s k x x h x y f s k y y h y z f f k z z h z w φ f s t f s s s h t where x y z are the three dimensions in the aquifer φ is porosity l3 l3 and is assumed equal to specific yield s y l3 l3 f s is the fraction of the cell thickness that is saturated and f f is a function of f s set to 1 for niswonger et al 2011 the computational points are spaced according to a grid divided into cells for which equation 1 is applied once head h is solved for at each grid cell h in adjacent grid cells can be used along with k of the grid cells to estimate flow rate and flow direction using darcy s law the modflow code also allows for groundwater modeling in confined aquifers equation not shown for stand alone modflow applications recharge is specified as user input groundwater stream exchange rates are calculated for designated river cells i e cells in geographic connection with streams and rivers using the river package which uses darcy s law and user provided values of stream width streambed thickness and streambed k the linked swat modflow model uses the following sequential procedure for each day of the simulation period to pass hydrologic fluxes between the computational units of swat hrus subbasin streams and modflow grid cells 1 swat solves mass balance equations for each hru in the watershed generating surface runoff soil lateral flow and soil deep percolation 2 swat passes soil deep percolation water from hrus to modflow grid cells using a specified groundwater delay time i e the model does not simulate water movement in the vadose zone 3 swat passes stream depth for all watershed streams to modflow river cells 4 modflow solves mass balance equations for each grid cell in the watershed calculating groundwater storage and groundwater head for each cell exchange rates between groundwater and streams are calculated for each river cell 5 the groundwater stream exchange rates are summed for each subbasin stream using all river cells within a given subbasin these rates are provided to swat s stream routing array 6 swat routes water through the watershed stream network for step 2 spatial connections and intersections between hrus and grid cells must be specified because each hru is a unique combination of soil type topographic slope and land use within a given subbasin it does not have a single geographic location but is typically composed of several disconnected polygons therefore each hru is separated geographically into a set of disaggregated hrus dhrus and then intersected with the modflow cells for step 5 spatial connections between subbasin streams and river cells must be specified the maps in fig 2 summarize these spatial connections for an example subbasin fig 2a shows the subbasin boundary the subbasin stream and the modflow grid cells with the river cells i e cells intersecting the stream highlighted in blue fig 2b shows the hrus with a single hru highlighted in blue disaggregated into 233 dhrus within this subbasin there are 224 hrus and 10 931 dhrus fig 2c shows a highlighted dhru intersected by portions of 6 grid cells and a highlighted cell intersected by portions of 7 dhrus within the swat modflow code each grid cell receives a weighted value of recharge from all intersecting dhrus spatial connections between subbasins streams hrus dhrus grid cells and river calls are contained in a set of input files read in at the beginning of the swat modflow simulation swat modflow has been used by many researchers in the past several years including for irrigation management wei and bailey 2019 aliyari et al 2019 assessing groundwater surface water interactions semiromi and koch 2019 chunn et al 2019 quantifying surface water resources gao et al 2019 and determining effects of pumping on streamflow molina navarro et al 2019 2 1 2 automated linking between swat and modflow models although a tutorial is available https swat tamu edu software swat modflow for using arcmap routines to link swat hrus subbasins and modflow grid cells computational units two software tools have been published to automate model linkage and model simulations thereby limiting human error and decreasing preparation time swatmod prep bailey et al 2017 uses a stand alone python script to perform the necessary geographic linkages and prepare necessary linkage input files qswatmod park et al 2019 is a qgis plug in tool that reads in swat shape files reads in or creates modflow shape files performs spatial linking operations writes linage input files runs the swat modflow simulation and provides visualization tools for stream hydrographs groundwater levels groundwater budget fluxes recharge groundwater surface water exchange and watershed wide temporal water balances 2 1 3 hydrologic modifications to swat modflow for this study a shortcoming of the published swat modflow code is the lack of water exchange between groundwater and the soil profile when the water table is near the ground surface all studies that have used swat modflow assume that shallow groundwater is not transferred to the soil profile this assumption is valid for watersheds in which the water table does not rise within the soil profile however not so for conditions of shallow groundwater and waterlogging the code presented in this paper rectifies this issue and provides a general simulation of groundwater soil water transfer for the condition of the water table rising into the soil profile at each daily time step the code determines for each hru if groundwater in connected grid cells is in the soil profile if groundwater head is above the base of the soil profile then the volume of groundwater above the base of the soil profile cell area height above s y is calculated from cells intersecting the hru and this volume is transferred from the grid cells to the hru soil profile a water balance check is performed within the code to verify that all groundwater removed from the grid cells is accounted for in the soil water of the hrus 2 1 4 swat modflow for nutrient transport modeling recent studies have expanded swat modflow to include nutrient fate and transport modeling swat s base code simulates the fate and transport of nitrogen n and phosphorus p species in soils aquifer and streams but the original swat modflow code bailey et al 2106 does not simulate these species transport in groundwater and hence the code could be used only for hydrologic applications the work by wei et al 2019 modified the swat modflow code to include nutrient fate and transport modeling in the groundwater system by incorporating the groundwater reactive transport code rt3d clement 1997 rt3d written in fortran solves the following mass balance equation for the concentration c of m reactive solutes in a saturated groundwater system in space and time due to advection dispersion sorption and kinetic reactions 2 c k t r k x i v i c k x i d i j c k x j q s φ c s k r k k 1 2 m where m is the total number of aqueous phase species c k is the concentration of the kth species m f l f 3 where f denotes the fluid phase d ij is the hydrodynamic dispersion coefficient l 2 t 1 v is the average seepage velocity l b t 1 where b denotes the bulk phase φ is the soil porosity l f 3 l b 3 q s is the volumetric flux of water representing sources and sinks of the species l f 3 t 1 l b 3 c s k is the concentration of the source or sink m f l f 3 r represents the rate of all reactions that occur in the aqueous phase for the kth species m f l f 3 t 1 ρ b is the bulk density of the porous media m b l b 3 and c k is the concentration of the kth species sorbed on solids m f m b 1 rt3d receives v and q s from results of a modflow simulation solving for c for specified time steps at the same grid cells used by modflow within the swat modflow mode rt3d is called as a subroutine by modflow once the groundwater calculations have been performed wei et al 2019 wrote the linking routines to simulate nitrate no3 transport in the aquifer system no3 in recharge water was obtained from leaching concentrations from hru soil profiles and no3 in stream water used for groundwater surface water no3 mass exchange was obtained from each subbasin stream for each daily time step rt3d calculates the groundwater concentration of no3 at each grid cell and for river cells any no3 mass exchanged between groundwater and streams the mass balance equation for no3 used in wei et al 2019 is 3 c n o 3 t r n o 3 x i v i c n o 3 x i d i j c n o 3 x j q s φ c s n o 3 r n o 3 where r represents the rate law for denitrification written as 4 r no 3 μ n o 3 c n o 3 c n o 3 k n o 3 c n o 3 where μ n o 3 is the first order rate constant d 1 for denitrification and k n o 3 is the monod half saturation constant m f l f 3 for no3 the model was tested against c n o 3 in groundwater and no3 loads in the river system the work of wei et al 2019 was expanded by wei and bailey 2021 to include the reactive transport of phosphorus p in groundwater using the following equation within rt3d 5 c p o 4 t r p o 4 x i v i c p o 4 x i d i j c p o 4 x j q s φ c s p o 4 r p o 4 r p o 4 is 1 due to the strong sorbing capacity of p conversely soluble p has no appreciable reduction rate in groundwater system and therefore r p o 4 is set to 0 wei and bailey 2021 used the updated version of swat modflow to assess no3 and p in the irrigated stream aquifer system of the lower arkansas river valley colorado and quantify the impact of best management practices on nutrient concentrations and in stream loads 2 2 swat modflow salt for this study the swat modflow with rt3d code is modified to include the fate and transport of 8 major salt ions so4 2 cl co3 2 hco3 ca2 na mg2 k in the soil system groundwater system and stream system bailey et al 2019 included this functionality into swat using the equilibrium chemistry salt module of tavakoli kivi et al 2019 and in this study we combine the codes of swat modflow with rt3d functionality and swat salt to create swat modflow salt for comprehensive salinity transport modeling in all major hydrologic pathways surface runoff soil lateral flow percolation pumping groundwater flow groundwater surface water exchange using physically based spatially distributed groundwater modeling fig 1 shows the salinity processes included in swat modflow salt in both swat processes salt module for land surface soil and streams and rt3d processes reactive transport in groundwater equation 6 shows the mass balance equation for a generic salt ion in the soil profile of each hru 6 d m s o i l d t m i r r i g s w m i r r i g g w m d i s s m p r e c i p m r o m l a t m p e r c m g w s o i l where m represents salt ion mass irrig sw represents surface water irrigation irrig gw represents groundwater irrigation diss represents dissolution of salt minerals precip represents precipitation of salt minerals ro represents surface runoff only for layer 1 lat represents soil lateral flow perc represents percolation from the bottom of the soil layer and gw soil represents water transfer from the aquifer to the soil profile for conditions of groundwater rising into the soil profile equation 6 is solved for each salt ion so4 cl co3 hco3 ca na mg k for each hru for each day of the simulation period this is the same process as described in the swat salt presentation of bailey et al 2019 except for the inclusion of gw soil in this study dissolution and precipitation are simulated using equilibrium chemistry methods as presented in bailey et al 2019 and similar to other soil chemistry models e g parkhust and appelo 2013 precipitation dissolution reactions are included for 5 salt minerals caso4 caco3 mgco3 nacl mgso4 the concentration of each dissolved species is calculated using a stoichiometric algorithm approach to solve mass balance and mass action equations shapiro and shapley 1965 lindsay 1979 initial conditions of salt mineral fractions and salt ion dissolved concentrations are specified for each hru groundwater concentration of each salt ion is calculated using a combination of the mass balance equations of rt3d and equilibrium chemistry equations e g precipitation dissolution the mass balance equations of rt3d are solved first to obtain intermediate concentrations of each salt ion at each grid cell followed by the equilibrium chemistry equations to modify these concentrations given precipitation and dissolution processes as an example the rt3d mass balance equation for so4 is 7 φ c s o 4 t x i φ v i c s o 4 x i φ d i j c s o 4 x j q r e c h c s o 4 s w a t q g w s w c s o 4 q s w g w c s o 4 s w a t where so 4 swat represents concentration from swat units for recharge q rech these values are obtained from the concentration of so4 in the soil water percolating from the base of the soil profile of hrus for stream seepage q sw gw these values are obtained from the concentration of so4 in the stream water of subbasins equation 7 is repeated for the other 7 salt ions initial concentrations of each of the 8 salt ions are specified for each grid cell of the rt3d grid in this study we modified the swat crop growth subroutines to include soil salinity impacts on crop yield the relative yield equations of maas 1993 along with reported values for the threshold electrical conductivity ec ds m and slope of salinity impact on relative yield decrease are used as a surrogate for a daily stress value the following steps are used to compute a salinity stress factor for each hru for each day during the growing season 1 salt concentration total dissolved solids tds mg l in the soil profile is computed by summing the concentrations of the 8 ions in the soil water 2 tds mg l is converted to ec sat ec at saturation by multiplying by water content and then dividing by a tds ec conversion factor with the latter obtained from an analysis of field data in the modeled region 3 the reduction in plant growth due to salinity i e the salinity stress factor str salt is calculated using the relative yield ry equation of maas 1993 using the threshold a and slope b parameters for the crop type of the hru 8 r y s t r s a l t 100 b e c s a t a 100 as reported by maas 1993 for gypsiferous soils i e soils with high gypsum caso4 content a should be increased by 2 0 indicating a smaller impact on plant growth in regions than for other soils e g high nacl 4 the salinity stress factor str salt is used in conjunction with the original stress factors used by swat water temperature nitrogen and phosphorus to provide an overall reduction in biomass for that day fig 3 shows a schematic of data flow within the swat modflow salt code inputs for the salinity module include solubility products for each salt mineral initial soil water salt ion concentration for each hru initial salt mineral fraction for each hru initial groundwater salt ion concentration for each rt3d grid cells initial aquifer salt mineral fraction for each rt3d grid cell and parameters a and b for each of the 121 plant types in the swat database besides the basic hydrologic and nutrients outputs of swat modflow surface runoff soil lateral flow groundwater storage and head groundwater surface water exchange rates each with accompanying nutrient loads model outputs for swat modflow salt include soil salt ion concentrations for each hru groundwater salt ion concentrations for each grid cell crop yield with salinity influence and salt ion fluxes for surface runoff soil lateral flow recharge groundwater discharge to streams stream seepage to groundwater groundwater soil transfer surface water irrigation groundwater irrigation precipitation and dissolution 3 model application lower arkansas river valley colorado the swat modflow salt model is applied to a study region in the saline affected lower arkansas river valley larv in southeastern colorado usa the model is tested against measured system response variables groundwater salt ion concentration in stream salt ion loading crop yield to demonstrate its suitability for use in salinity impact scenarios which can be performed in future studies 3 1 study region the study region to which the swat modflow salt modeling code is applied is a 732 km2 section of the larv the region has been irrigated for over one hundred years due to semi arid climate conditions precipitation 375 mm yr irrigation water is derived either from the arkansas river through a network of 6 irrigation canals or from a network of groundwater pumping wells fig 4 a main crops grown include melons onions peppers wheat sorghum corn alfalfa and legumes the local aquifer is made up of alluvium sand and gravel deposited over millenia by the arkansas river and its tributaries and on average is 15 m in thickness from the ground surface to the shale bedrock salinity in soils groundwater and streams has steadily increased over previous decades due to salt mineral dissolution principally caco3 and caso4 and lack of adequate drainage leading to shallow groundwater and consequent evapo concentration of salts in the soil water of the rooting zone approximately 70 of the region has experienced a decrease in crop yield due to elevated soil salinity gates et al 2002 morway and gates 2012 from 443 groundwater samples collected from the network of 75 monitoring wells fig 4a average total dissolved solids tds concentration between 2006 and 2010 was 3300 mg l during this same time period average tds in the arkansas river and its tributaries was 1145 mg l strategies for curbing salinity increase are being sought for this region 3 2 model set up and testing the data inputs for the swat modflow salt model used in this study is based on the swat modflow with rt3d model of wei and bailey 2021 to assess n and p management scenarios and the swat salt model of bailey et al 2019 to assess salt ion fate and transport basically the model inputs from wei and bailey 2021 were modified to include the salt inputs of bailey et al 2019 with the addition of initial concentrations of salt ions for each rt3d grid cell these two modeling studies will now be summarized to provide context for this current study the swat modflow application to the study region wei and bailey 2019 used the swat model from wei et al 2018 tested for streamflow at three gages in the arkansas river and two gages in tributaries linked to the modflow model morway et al 2013 tested against groundwater levels groundwater discharge to streams and evapotranspiration the swat model used 72 subbasins and 5270 hrus each field designated as a unique hru with climate data provided from weather stations in the colorado agricultural meteorological network the modflow model used 250 m grid cells and included groundwater pumping from a network of pumping wells canal seepage from the 6 earthen irrigation canals and groundwater river exchange along the arkansas river and its tributaries the swat modflow model was run for the 1999 2016 time period which include both extremely wet and dry periods the swat salt application to the study region bailey et al 2019 used the same hru and subbasin set up as the original swat hydrologic model of wei et al 2018 salinity module inputs included solubility products for the five salt minerals caso4 caco3 mgco3 nacl mgso4 initial salt ion concentrations in soil water and groundwater and initial salt mineral fractions in the soil profile and the aquifer with these latter four specified for each of the 5270 hrus mineral fractions for caso4 and caco3 were obtained from ssurgo soil map data initial salt ion concentrations in the soil water and groundwater were set to spatially uniform values with the value for each salt ion based on an average of field data from 2006 to 2010 see details of these data in gates et al 2009 the model was tested against in stream salt ion concentrations 2006 2010 time period in the arkansas river and two tributaries timpas creek crooked arroyo soil salinity and groundwater salt ion concentrations the application of swat modflow salt in this study uses the hru and subbasin set up from wei et al 2018 the modflow grid pumping data and aquifer properties from morway et al 2013 and wei and bailey 2019 and the salinity soil and groundwater initial concentrations and salt mineral fractions from bailey et al 2019 datasets for model construction and model parameterization are presented in table 2 fig 4 shows the delineation of swat subbasins fig 4a the modflow grid fig 4e the spatial distribution of soil caco3 fig 4b and caso4 fig 4c and hydrologic features for model inputs and testing stream network and gages monitoring wells pumping wells and irrigation canals fig 4a wei et al 2018 compared two irrigation management schemes 1 irrigation diversions and applied depths based on canal diversion and groundwater pumping data from the colorado division of water resources cdwr and 2 auto irrigation routines within swat and found that results water balance stream discharge in tributaries and the arkansas river were similar in magnitude and timing when linked with swat modflow wei and bailey 2019 auto irrigation routines were used for canal derived irrigation whereas measured monthly rates from the cdwr were specified in modflow s well package and then linked to daily irrigation depths for swat hrus this same set up for irrigation is used in this study the swat modflow salt model is run for the 1999 2009 period which included an extremely wet year 1999 and a major drought period 2002 2003 the model is corroborated against the following system response variables stream discharge soil salinity concentration groundwater salt ion concentration in stream salt ion loading and crop yield groundwater levels were previously tested in the swat modflow application of wei and bailey 2019 as the intent of this paper is to present the new swat modflow salt code and not as yet use it in our study region for scenario analysis no calibration is performed in this study calibrated hydrologic parameters from the swat wei et al 2018 and swat modflow wei and bailey 2019 models are used for the salinity module the only model factors that can be used in model calibration and testing are initial conditions of salt mineral fractions and salt ion concentrations in soil water and groundwater we have elected to keep these values unchanged from the fractions and concentrations used in the swat salt application of bailey et al 2019 3 3 results and discussion results are shown for water balance groundwater head patterns streamflow salt balance groundwater salt ion concentrations in stream salt loading and crop yield we have attempted to provide an assortment of model output and comparison with measured system response variables to verify that the model satisfies the water and salt balance in the region and is a reasonable and accurate simulator of general salt ion concentrations and loadings in the soil aquifer and stream systems 3 3 1 hydrologic results hydrologic results are similar to the previous swat modflow application in the region wei and bailey 2019 with the exception that we are now simulating the transfer of groundwater to soil water when the water table rises into the soil prolife of swat hrus we provide hydrologic results here for completeness and to provide context for the pathways of salt ion transport and loading the average annual water balance mm yr blue text of the model domain is shown in fig 5 water enters the system via precipitation 298 mm and surface water irrigation 64 with this irrigation water diverted from the arkansas river at locations upstream of the model domain water exits the system via et 250 mm surface runoff 23 mm soil lateral flow 0 5 mm and groundwater discharge 88 mm surface runoff soil lateral flow and groundwater discharge constitute the landscape water yield of the region for a total of 111 5 mm 37 of precipitation with 79 from groundwater discharge the groundwater driven nature of the arkansas river is an important feature of the river basin for both water movement and solute transport morway et al 2013 bailey et al 2014 recharge an internal flux is estimated to be 177 mm yr with 50 mm of water transferred from the aquifer to the soil profile resulting in a net recharge of 127 mm yr 42 of annual precipitation groundwater pumping also an internal flux as water is transferred from the aquifer to the soil profile via irrigation events is 24 mm yr or 37 5 of the amount of water applied as irrigation water monthly time series of water balance components are shown in fig 6 with inflow outflow fluxes mm shown for the entire watershed system fig 6a and the aquifer system fig 6b the time series of system wide fluxes fig 6a shows seasonal patterns of high irrigation and et during the irrigation season april october and groundwater discharge steady but increasing during the latter months of the season august november due to increased groundwater gradients from irrigation recharge fig 7 a shows groundwater head patterns one value for each grid cell for 2008 showing the equipotential lines that induce groundwater flow towards the arkansas river and tributaries appreciable surface runoff occurs only during the months of irrigation fig 6c shows the daily fluctuation in total soil water mm in the model domain resulting from a balance of inflows precipitation surface water irrigation groundwater irrigation groundwater transfer to soil water and outflows et surface runoff soil lateral flow recharge see arrows into and out of the soil profile box in fig 5 the time series of aquifer fluxes fig 6b shows the strong influence of recharge highest during the irrigation seasons and groundwater discharge fig 6c shows the daily fluctuation in total groundwater mm in the model domain resulting from a balance of inflows recharge and outflows groundwater transfer to soil water groundwater discharge groundwater pumping see fig 5 in response to seasonal patterns of irrigation maps of average groundwater head average water table depth ground surface groundwater head average daily recharge m3 day and average daily groundwater transferred to soil m3 day are shown in fig 7 for 2008 for each grid cell in the modflow model domain shallow groundwater occurs throughout the area fig 7c with many areas experiencing a water table within 2 3 m of the ground surface the occurrence of shallow groundwater results in groundwater being transferred from modflow grid cells to swat hru soil profiles fig 7d particularly in areas of high recharge fig 7b and groundwater head within 2 m of the ground surface fig 7c in general groundwater patterns correspond to previous groundwater modeling efforts morway et al 2013 and hydrologic fluxes correspond to the previous swat modflow model of wei and bailey 2019 simulated monthly stream discharge is plotted with measured stream discharge in fig 8 for three gage sites in the arkansas river rocky ford la junta las animas see fig 4a for locations and two tributaries crooked arroyo timpas creek similar to previous modeling efforts in this region wei et al 2018 the model performs well in for sites in the arkansas river nash sutcliffe coefficient efficiency nsce 0 75 0 72 0 55 respectively but underestimates flow in the tributaries nsce 0 07 0 35 although temporal patterns are mostly replicated 3 3 2 assessment of salt ion transport and loading 3 3 2 1 salt balance and loading the salt balance and annual loadings kg x 106 for the model domain are shown alongside the hydrologic flux depths in fig 5 the salt loading is the sum of the loading of the eight salt ions in addition to salt movement along hydrologic pathways salt enters soil water and groundwater via dissolution of salt minerals see fig 5 also in the current model application we assume that snow and rainfall total precipitation does not contain salt mass the main hydrologic drivers of salt movement are recharge 286 106 kg and groundwater loading to streams 168 followed by groundwater irrigation 127 cycling salt from the aquifer to the soil groundwater transfer to soil 121 dissolution of salt minerals in the aquifer 116 and salt loading to the soil via surface water irrigation 54 of the salt that enters the river from the landscape surface runoff 20 soil lateral flow 3 groundwater loading 168 88 is from groundwater this percentage is higher than the groundwater portion of water yield 79 due to the higher salt ion concentration in groundwater than in surface runoff and soil lateral flow also of note is the higher salt loading in groundwater irrigation 127 as compared to surface water irrigation 54 although there is more actual surface irrigation water applied 64 than groundwater 24 again this is due to the higher salt concentration of groundwater as compared to stream water monthly salt fluxes kg x 106 for the watershed system and the aquifer system are shown in fig 9 salt loading to the soil system via surface water irrigation and groundwater irrigation sw irrigation pumping occur during the growing season months whereas groundwater salt loading to streams occurs during each month due to a constant groundwater gradient from upland to stream channels the salt loading to the aquifer via recharge fig 9b is mirrored by salt loading to the soil via groundwater that rises into the soil profile gw soil although a net recharge loading occurs 165 286 121 see fig 5 of course the temporal patterns of recharge salt loading are due to the patterns of actual recharge see fig 6b 3 3 2 2 salt in soil water and groundwater the spatial distribution of salt is quantified for soil water salt ion concentration fig 10 and groundwater salt ion concentration fig 11 for each hru and each grid cell respectively maps are shown for each of the 8 salt ions and tds for average annual concentrations mg l the soil water salt ion concentration maps fig 10 showing daily average values during 2006 2009 for each hru provide insights into the spatial variation of salt recharge loading by combining these concentration values with the recharge volumes shown in fig 7b concentrations are due to salt loading from mineral dissolution groundwater irrigation surface water irrigation and groundwater transfer with dilution occurring due to precipitation and strengthening occurring due to evapotranspiration soil water salt concentrations can be extremely high 10 000 mg l due to areas of low water content in the soil profile groundwater salt ion concentration maps fig 11 show hotspots of each salt ion due to recharge salt loading and aquifer salt mineral dissolution figs 5 and 9b groundwater salt loading to the soil profile via irrigation is calculated within the model by multiplying time dependent groundwater concentrations by pumping rates at cells where pumping wells are located see fig 4a for locations and groundwater salt loading to the streams arkansas river tributaries is calculated within the model by multiplying concentrations by groundwater discharge rates at river cells for both soil water and groundwater so4 and ca are the dominant ions due to the extensive presence of gypsum caso4 in the soils and aquifer of the region average so4 and ca concentrations in the aquifer are 1470 mg l and 730 mg l respectively hco3 320 mg l mg 260 mg l na 215 mg l and cl 70 mg l are moderately high followed by k 6 mg l and co3 0 1 mg l simulated groundwater salt ion concentrations compare well to measured concentrations according to frequency distributions fig 12 for these frequency distributions simulated values come from each grid cell in layer 2 of the model saturated portion of the aquifer overall the model results compare well with the measured values particularly for so4 cl mg na and k indicating that the model is able to capture the region wide spatio temporal magnitudes of groundwater salt ion concentrations in the model domain groundwater concentrations of ca are over estimated values of hco3 are under estimated and values of co3 are underestimate although measured values of co3 are extremely low average 1 mg l so residuals between simulated and measured values also are very low 3 3 2 3 salt in stream water however although ca groundwater salt ion concentrations are over estimated in stream loadings compare well with measured loadings as shown in fig 13 upper right chart in general simulated in stream salt ion concentrations and loadings compare well with measured values fig 13 in the arkansas river rocky ford r2 0 71 to 0 83 but similar to the stream discharge results shown in fig 8 loadings are very low compared to measured values in the tributary timpas creek as the simulated concentrations for timpas creek are of the same magnitude as the measured concentrations see fig 13 timpas creek concentration charts for so4 and ca the underestimation of loading is due to the underestimation of stream discharge this also occurs for the las animas site with concentrations matching well r2 0 31 to 0 49 but loads being underestimated due to the underestimation in stream discharge see fig 8 the spatial distribution of in stream salt ion loading is shown in fig 14 these maps show for each of the 72 subbasins in the model domain the average daily simulated in stream loading kg day for each of the 8 salt ions and tds of note are the high magnitudes of loading along the arkansas river also of note are the higher magnitudes in the upstream subbasins as compared to the downstream subbasins with the same pattern occurring for each ion this is due to a significant portion of the water and salt in the arkansas river being diverted to three irrigation canals with diversion points between the towns of rocky ford and la junta see fig 4a this highlights the intense human influence on salt movement in this region salt removed from the river by irrigation canals salt transferred from the aquifer to the land surface by groundwater pumping for irrigation additional salt leaching due to irrigation events groundwater salt loading to the arkansas river and its tributaries that occurs due to high groundwater levels which is due to season long irrigation 3 3 2 4 salinity effect on crop yield the spatial by hru and temporal annual sum for 1999 2009 variations in crop yield are presented in fig 15 for both conditions of salinity stress and no salinity stress on daily crop growth for the condition of no salinity stress the simulation was run without including the reduction in crop growth simulated by equation 8 fig 15a shows the average annual yield ton ha for each of the cultivated fields in the region each corresponding to a unique hru and fig 15b shows the increase in crop yield ton ha when salinity impacts are not simulated many fields have a difference of 2 ton ha due to the presence of salinity average yield ton ha for the condition of salinity stress is 4 67 kg ha compared to 4 76 kg ha for the condition of no salinity stress a difference of 1 9 this is the same difference when comparing total tons of crop yield for the two scenarios the crops most affected by salinity stress are alfalfa corn watermelon onions green beans and soybeans fig 15c and d shows the annual yield ton of alfalfa and corn respectively for each year of the 1999 2009 period for both conditions for alfalfa the total decrease in crop yield due to salinity impacts is 2 7 whereas the decrease for corn is 2 1 for watermelon onions green beans and soybeans the decrease is 13 4 4 7 3 and 2 although watermelon onions and green beans compose only a small fraction 0 6 of the crop yield in the model domain whereas alfalfa and corn compose 69 of crop yield other major plant types rangeland wheat and sorghum are impacted only slightly 0 2 by soil salinity overall 2153 hrus out of the 5270 hrus in the model domain has crop yields decrease due to the presence of salt in the root zone these results agree with the findings of morway and gates 2012 who concluded that 70 of the region experiences crop yield decrease due to soil salinity also the swat modflow application of the region by wei and bailey 2019 overestimated crop yield for alfalfa and corn on a county wide assessment this study provides a model that by virtue of including salt ion fate and transport and its effect on crop growth brings crop yield values closer in line with estimated region wide values 3 4 limitations of the modeling study this study introduces a new version of swat modflow that includes a salinity module for salt ion fate and transport although we have attempted to simulate each major hydrologic and salt ion chemical process in an irrigated stream aquifer system we note the following limitations although we simulate the transfer of groundwater and associated salt ion mass to the soil profile when the water table is above the base of the soil profile we do not simulate the movement of water and salt ion mass in the vadose zone i e the zone between the base of the soil profile and the water table for conditions of a water table that is lower in elevation than the soil profile the time of travel in the vadose zone is represented and simulated by a groundwater delay term days which loads the soil water and salt ion mass to the water table by a transit function a unique value can be assigned to each hru this limitation however is likely to be of more consequence in a region with deeper water tables we do not consider salt movement in erosion runoff due to large storm events and surface runoff fluxes the inclusion of this feature in the model however likely is more important for high desert watersheds with steep terrains the simulation of bicarbonate hco3 fate and transport is more difficult than other ions due to its dependence on ph which is not explicitly simulated in the model previous attempts at simulating the chemical transport of this ion in this region tavakoli kivi et al 2019 bailey et al 2019 always results in mis matches between groundwater concentrations and in stream loadings this can be addressed by including ph in the salinity module but is not included in the current version of swat modflow 4 summary and conclusions this paper presents a new version of the coupled swat modflow watershed model that simulates the fate and transport of eight major salt ions so4 2 cl co3 2 hco3 ca2 na mg2 k in soils aquifer and streams salt ion loads are simulated for all major hydrologic pathways surface runoff soil lateral flow percolation and recharge groundwater soil transfer in areas of shallow groundwater groundwater discharge stream and canal seepage and streamflow under the influence of salt mineral precipitation dissolution reactions the model uses rt3d as the groundwater reactive solute simulator coupled with a salinity module for both advective dispersive transport and equilibrium chemistry model applicability accuracy and usefulness is demonstrated for a saline affected irrigated stream aquifer system in the lower arkansas river valley colorado usa with model results tested against groundwater salt ion concentrations and in stream salt ion loadings the model is also run with and without plant salinity stress on crop yield to demonstrate and quantify the effect of salinity on crop yield in the region this paper is intended as a proof of concept with the new version of the swat modflow code now available for distribution at https github com ryantbailey swat modflow salt the intent is to provide a watershed modeling code that can be applied to saline affected regions to quantify impacts of climate change land management and water management on groundwater salinity surface water salinity soil salinity and crop yield although the model presentation and application is focused on an irrigated region the swat modflow salt modeling code can be applied to any saline affected area however if the model is to be applied to salt movement in high desert watersheds that experience high intensity short duration thunderstorms the salinity module should be modified to include equations for salt mobilization and loading in rainfall induced erosion author contributions r b and p h designed the research r b performed model coding r b and p h performed the research r b and p h analyzed the data r b and p h wrote the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the funding received for this project from both the colorado agricultural experiment station project colo0794 and by nsf career award no 1845605 
25483,salinity can have major detrimental impacts on soil health crop yield and environmental ecosystem services in this paper we introduce swat modflow salt a version of swat modflow that simulates the fate and transport of major salt ions so4 2 cl co3 2 hco3 ca2 na mg2 k in watershed systems and focus on its development and application to irrigated stream aquifer systems the modeling code accounts for water and salt mass storage in soils aquifer and streams and the movement of water and salt mass in surface runoff soil lateral flow percolation and recharge groundwater flow and groundwater surface water exchange the model is applied to a highly managed irrigated 700 km2 region of the saline lower arkansas river valley in colorado usa the model is tested against groundwater salt concentrations and in stream salt mass loadings and then used to quantify the impact of soil salinity on crop yield keywords swat salinity modflow groundwater irrigation data availability in summary and conclusions we have included a github link to the code software availability name of software swat modflow salt fortran 421 files 7 4 mb developer and contact information ryan bailey rtbailey colostate edu year first available 2022 hardware required standard desktop pc software required none availability and cost source files are freely available for download at https github com ryantbailey swat modflow salt 1 introduction salinity is a common water quality threat in many watersheds worldwide specific threats include soil salinization and associated decrease in crop yield groundwater salinity in drinking water ecosystem health impacts and the health and diversity of fish and waterfowl populations the threat of soil salinization is especially pronounced for arid and semi arid regions that use irrigation water to sustain crop growth with inadequate drainage leading to evapo concentration of salts in the root zone of the soil profile approximately one third of irrigated land is adversely affected by salinity shrivastava and kumar 2015 with a loss of 0 25 0 5 million ha each year globally nlwra 2001 the movement of salt in these systems must be tracked and managed to ameliorate conditions for cultivation and general ecosystem services numerical hydrologic models are often used as tools in the general framework of salt management these models estimate salt storage in watershed systems soil profile aquifer streams and simulate salt movement in various hydrologic pathways and when tested against salt measurement data from real systems can be used in scenario analysis to determine the effect of climate and management on salt pollution available published models vary in terms of the type of system simulated and type of salinity transport algorithm some models focus on the saturated zone burkhalter and gates 2005 on the soil profile and shallow aquifer šimůnek and suarez 1994 on large scale variably saturated groundwater systems schoups et al 2005 tavakoli kivi et al 2019 on land surface and streams bailey et al 2019 tirabadi et al 2021 and on surface subsurface systems bailey et al 2022 table 1 lists these studies and the corresponding model names along with attributes of each model including land scape transport stream transport groundwater transport physically based spatially distributed pbsd groundwater simulation the dimension of the groundwater model and equilibrium ion chemistry while each of these approaches and models listed in table 1 has unique strengths none can be applied to simulate the movement and fate of salt ion species in all major hydrologic reservoirs and pathways of a watershed system subject to irrigation hydrus 2d 3d and rt3d salt focus on subsurface systems whereas swat salt and swat s focus on land surface soil and stream systems with significant limitations in groundwater salt storage and transport i e not simulating pbsd groundwater flow and interactions with surrounding hydrologic features apex modflow salt does simulate pbsd groundwater flow but does not account for salinity in surface water irrigation water or groundwater i e pumped irrigation only being applied to a mountainous natural system bailey et al 2022 in addition the code does not include water and salt ion mass exchange between the soil profile and the aquifer when shallow groundwater occurs there is a need for a model that combines the strengths of the available models listed in table 1 to provide comprehensive assessment of salt movement in managed watershed systems thereby providing an asset for salt management in future decades this paper has two objectives the first objective is to present a new version of swat modflow bailey et al 2106 that accounts for salt ion so4 2 cl co3 2 hco3 ca2 na mg2 k transport in the soil profile the groundwater system and the stream network of a given watershed system along with soil groundwater and groundwater stream salt ion mass exchange swat modflow is a coupled surface subsurface hydro chemical model that has been used extensively in recent years e g aliyari et al 2019 chunn et al 2019 gao et al 2019 molina navarro et al 2019 liu et al 2020 guevara ochoa et al 2020 wei and bailey 2021 for simulating water and nutrient movement in watershed systems the second objective is to demonstrate the use of the model for a saline irrigated watershed system for the first objective the salt module used in swat salt bailey et al 2019 see table 1 is integrated into the swat modflow modeling code also for the first time the swat modflow modeling code includes provisions for transferring water and solute mass from the aquifer to the root zone when the water table is within the soil profile for the second objective the watershed system is located in southeastern colorado along the alluvial aquifer system of the arkansas river which has experienced elevated salinity in soils groundwater and streams during recent decades due to long term irrigation and lack of adequate drainage in a semi arid climate 2 swat modflow salt development this section provides an overview of the swat modflow salt modeling code which is a modification of the most recent swat modflow version wei and bailey 2021 to include the salinity module developed for swat bailey et al 2019 the evolution of swat modflow from initial development for hydrologic applications to more recent modifications for groundwater nutrient transport is presented followed by the modifications performed for this current study to include salinity fate and transport for both surface and subsurface processes 2 1 swat modflow 2 1 1 overview of original swat modflow code the swat modflow modeling code bailey et al 2106 combines the swat watershed modeling modeling code arnold et al 1998 and the modflow nwt groundwater flow modeling code niswonger et al 2011 into a single fortran code that when compiled provides a single executable for model simulation within the linked model swat simulates land surface soil and stream hydrologic processes whereas modflow simulates groundwater hydrologic processes including groundwater surface water exchange fig 1 shows the respective processes simulated by swat and modflow black and blue text respectively within the swat modflow model swat is a physically based hydrologic simulator for watershed systems operating on a daily time the model calculates the mass balance of water sediment and nutrients nitrogen and phosphorus for the soil profile and aquifer of individual hydrologic response units hrus with each hru a unique combination of soil type topographic slope and land use within a subbasin of a delineated watershed these mass balance equations result in the estimation of system response variables e g soil water content nitrate concentration in soil water and groundwater phosphorus concentration in soil water and fluxes surface runoff soil lateral flow groundwater discharge corresponding nitrogen and phosphorus loads for each day of the simulation period the portion of each flux reaching the subbasin stream is calculated with values summed from all hrus to provide an aggregated mass loaded to the stream the water sediment and nutrient loaded to each stream is then routed through the watershed stream network with all mass eventually exiting the watershed through the watershed outlet modflow is a numerical model developed and maintained by the united state geological survey that simulates the storage and movement of groundwater in an aquifer system using physically based principles of groundwater storage and groundwater flow equations of storage and flow are based on aquifer properties hydraulic conductivity k m day specific yield s y specific storage s s 1 m and aquifer hydrologic inflows and outflows e g recharge pumping stream seepage groundwater discharge to streams that can vary in space throughout the aquifer system the modeling code reads in spatially distributed data of aquifer properties and aquifer inflows outflows and then solves for groundwater head h m above a datum at predefined locations within the aquifer system using the following equation for unconfined aquifers with one equation for each computational point 1 x f s k x x h x y f s k y y h y z f f k z z h z w φ f s t f s s s h t where x y z are the three dimensions in the aquifer φ is porosity l3 l3 and is assumed equal to specific yield s y l3 l3 f s is the fraction of the cell thickness that is saturated and f f is a function of f s set to 1 for niswonger et al 2011 the computational points are spaced according to a grid divided into cells for which equation 1 is applied once head h is solved for at each grid cell h in adjacent grid cells can be used along with k of the grid cells to estimate flow rate and flow direction using darcy s law the modflow code also allows for groundwater modeling in confined aquifers equation not shown for stand alone modflow applications recharge is specified as user input groundwater stream exchange rates are calculated for designated river cells i e cells in geographic connection with streams and rivers using the river package which uses darcy s law and user provided values of stream width streambed thickness and streambed k the linked swat modflow model uses the following sequential procedure for each day of the simulation period to pass hydrologic fluxes between the computational units of swat hrus subbasin streams and modflow grid cells 1 swat solves mass balance equations for each hru in the watershed generating surface runoff soil lateral flow and soil deep percolation 2 swat passes soil deep percolation water from hrus to modflow grid cells using a specified groundwater delay time i e the model does not simulate water movement in the vadose zone 3 swat passes stream depth for all watershed streams to modflow river cells 4 modflow solves mass balance equations for each grid cell in the watershed calculating groundwater storage and groundwater head for each cell exchange rates between groundwater and streams are calculated for each river cell 5 the groundwater stream exchange rates are summed for each subbasin stream using all river cells within a given subbasin these rates are provided to swat s stream routing array 6 swat routes water through the watershed stream network for step 2 spatial connections and intersections between hrus and grid cells must be specified because each hru is a unique combination of soil type topographic slope and land use within a given subbasin it does not have a single geographic location but is typically composed of several disconnected polygons therefore each hru is separated geographically into a set of disaggregated hrus dhrus and then intersected with the modflow cells for step 5 spatial connections between subbasin streams and river cells must be specified the maps in fig 2 summarize these spatial connections for an example subbasin fig 2a shows the subbasin boundary the subbasin stream and the modflow grid cells with the river cells i e cells intersecting the stream highlighted in blue fig 2b shows the hrus with a single hru highlighted in blue disaggregated into 233 dhrus within this subbasin there are 224 hrus and 10 931 dhrus fig 2c shows a highlighted dhru intersected by portions of 6 grid cells and a highlighted cell intersected by portions of 7 dhrus within the swat modflow code each grid cell receives a weighted value of recharge from all intersecting dhrus spatial connections between subbasins streams hrus dhrus grid cells and river calls are contained in a set of input files read in at the beginning of the swat modflow simulation swat modflow has been used by many researchers in the past several years including for irrigation management wei and bailey 2019 aliyari et al 2019 assessing groundwater surface water interactions semiromi and koch 2019 chunn et al 2019 quantifying surface water resources gao et al 2019 and determining effects of pumping on streamflow molina navarro et al 2019 2 1 2 automated linking between swat and modflow models although a tutorial is available https swat tamu edu software swat modflow for using arcmap routines to link swat hrus subbasins and modflow grid cells computational units two software tools have been published to automate model linkage and model simulations thereby limiting human error and decreasing preparation time swatmod prep bailey et al 2017 uses a stand alone python script to perform the necessary geographic linkages and prepare necessary linkage input files qswatmod park et al 2019 is a qgis plug in tool that reads in swat shape files reads in or creates modflow shape files performs spatial linking operations writes linage input files runs the swat modflow simulation and provides visualization tools for stream hydrographs groundwater levels groundwater budget fluxes recharge groundwater surface water exchange and watershed wide temporal water balances 2 1 3 hydrologic modifications to swat modflow for this study a shortcoming of the published swat modflow code is the lack of water exchange between groundwater and the soil profile when the water table is near the ground surface all studies that have used swat modflow assume that shallow groundwater is not transferred to the soil profile this assumption is valid for watersheds in which the water table does not rise within the soil profile however not so for conditions of shallow groundwater and waterlogging the code presented in this paper rectifies this issue and provides a general simulation of groundwater soil water transfer for the condition of the water table rising into the soil profile at each daily time step the code determines for each hru if groundwater in connected grid cells is in the soil profile if groundwater head is above the base of the soil profile then the volume of groundwater above the base of the soil profile cell area height above s y is calculated from cells intersecting the hru and this volume is transferred from the grid cells to the hru soil profile a water balance check is performed within the code to verify that all groundwater removed from the grid cells is accounted for in the soil water of the hrus 2 1 4 swat modflow for nutrient transport modeling recent studies have expanded swat modflow to include nutrient fate and transport modeling swat s base code simulates the fate and transport of nitrogen n and phosphorus p species in soils aquifer and streams but the original swat modflow code bailey et al 2106 does not simulate these species transport in groundwater and hence the code could be used only for hydrologic applications the work by wei et al 2019 modified the swat modflow code to include nutrient fate and transport modeling in the groundwater system by incorporating the groundwater reactive transport code rt3d clement 1997 rt3d written in fortran solves the following mass balance equation for the concentration c of m reactive solutes in a saturated groundwater system in space and time due to advection dispersion sorption and kinetic reactions 2 c k t r k x i v i c k x i d i j c k x j q s φ c s k r k k 1 2 m where m is the total number of aqueous phase species c k is the concentration of the kth species m f l f 3 where f denotes the fluid phase d ij is the hydrodynamic dispersion coefficient l 2 t 1 v is the average seepage velocity l b t 1 where b denotes the bulk phase φ is the soil porosity l f 3 l b 3 q s is the volumetric flux of water representing sources and sinks of the species l f 3 t 1 l b 3 c s k is the concentration of the source or sink m f l f 3 r represents the rate of all reactions that occur in the aqueous phase for the kth species m f l f 3 t 1 ρ b is the bulk density of the porous media m b l b 3 and c k is the concentration of the kth species sorbed on solids m f m b 1 rt3d receives v and q s from results of a modflow simulation solving for c for specified time steps at the same grid cells used by modflow within the swat modflow mode rt3d is called as a subroutine by modflow once the groundwater calculations have been performed wei et al 2019 wrote the linking routines to simulate nitrate no3 transport in the aquifer system no3 in recharge water was obtained from leaching concentrations from hru soil profiles and no3 in stream water used for groundwater surface water no3 mass exchange was obtained from each subbasin stream for each daily time step rt3d calculates the groundwater concentration of no3 at each grid cell and for river cells any no3 mass exchanged between groundwater and streams the mass balance equation for no3 used in wei et al 2019 is 3 c n o 3 t r n o 3 x i v i c n o 3 x i d i j c n o 3 x j q s φ c s n o 3 r n o 3 where r represents the rate law for denitrification written as 4 r no 3 μ n o 3 c n o 3 c n o 3 k n o 3 c n o 3 where μ n o 3 is the first order rate constant d 1 for denitrification and k n o 3 is the monod half saturation constant m f l f 3 for no3 the model was tested against c n o 3 in groundwater and no3 loads in the river system the work of wei et al 2019 was expanded by wei and bailey 2021 to include the reactive transport of phosphorus p in groundwater using the following equation within rt3d 5 c p o 4 t r p o 4 x i v i c p o 4 x i d i j c p o 4 x j q s φ c s p o 4 r p o 4 r p o 4 is 1 due to the strong sorbing capacity of p conversely soluble p has no appreciable reduction rate in groundwater system and therefore r p o 4 is set to 0 wei and bailey 2021 used the updated version of swat modflow to assess no3 and p in the irrigated stream aquifer system of the lower arkansas river valley colorado and quantify the impact of best management practices on nutrient concentrations and in stream loads 2 2 swat modflow salt for this study the swat modflow with rt3d code is modified to include the fate and transport of 8 major salt ions so4 2 cl co3 2 hco3 ca2 na mg2 k in the soil system groundwater system and stream system bailey et al 2019 included this functionality into swat using the equilibrium chemistry salt module of tavakoli kivi et al 2019 and in this study we combine the codes of swat modflow with rt3d functionality and swat salt to create swat modflow salt for comprehensive salinity transport modeling in all major hydrologic pathways surface runoff soil lateral flow percolation pumping groundwater flow groundwater surface water exchange using physically based spatially distributed groundwater modeling fig 1 shows the salinity processes included in swat modflow salt in both swat processes salt module for land surface soil and streams and rt3d processes reactive transport in groundwater equation 6 shows the mass balance equation for a generic salt ion in the soil profile of each hru 6 d m s o i l d t m i r r i g s w m i r r i g g w m d i s s m p r e c i p m r o m l a t m p e r c m g w s o i l where m represents salt ion mass irrig sw represents surface water irrigation irrig gw represents groundwater irrigation diss represents dissolution of salt minerals precip represents precipitation of salt minerals ro represents surface runoff only for layer 1 lat represents soil lateral flow perc represents percolation from the bottom of the soil layer and gw soil represents water transfer from the aquifer to the soil profile for conditions of groundwater rising into the soil profile equation 6 is solved for each salt ion so4 cl co3 hco3 ca na mg k for each hru for each day of the simulation period this is the same process as described in the swat salt presentation of bailey et al 2019 except for the inclusion of gw soil in this study dissolution and precipitation are simulated using equilibrium chemistry methods as presented in bailey et al 2019 and similar to other soil chemistry models e g parkhust and appelo 2013 precipitation dissolution reactions are included for 5 salt minerals caso4 caco3 mgco3 nacl mgso4 the concentration of each dissolved species is calculated using a stoichiometric algorithm approach to solve mass balance and mass action equations shapiro and shapley 1965 lindsay 1979 initial conditions of salt mineral fractions and salt ion dissolved concentrations are specified for each hru groundwater concentration of each salt ion is calculated using a combination of the mass balance equations of rt3d and equilibrium chemistry equations e g precipitation dissolution the mass balance equations of rt3d are solved first to obtain intermediate concentrations of each salt ion at each grid cell followed by the equilibrium chemistry equations to modify these concentrations given precipitation and dissolution processes as an example the rt3d mass balance equation for so4 is 7 φ c s o 4 t x i φ v i c s o 4 x i φ d i j c s o 4 x j q r e c h c s o 4 s w a t q g w s w c s o 4 q s w g w c s o 4 s w a t where so 4 swat represents concentration from swat units for recharge q rech these values are obtained from the concentration of so4 in the soil water percolating from the base of the soil profile of hrus for stream seepage q sw gw these values are obtained from the concentration of so4 in the stream water of subbasins equation 7 is repeated for the other 7 salt ions initial concentrations of each of the 8 salt ions are specified for each grid cell of the rt3d grid in this study we modified the swat crop growth subroutines to include soil salinity impacts on crop yield the relative yield equations of maas 1993 along with reported values for the threshold electrical conductivity ec ds m and slope of salinity impact on relative yield decrease are used as a surrogate for a daily stress value the following steps are used to compute a salinity stress factor for each hru for each day during the growing season 1 salt concentration total dissolved solids tds mg l in the soil profile is computed by summing the concentrations of the 8 ions in the soil water 2 tds mg l is converted to ec sat ec at saturation by multiplying by water content and then dividing by a tds ec conversion factor with the latter obtained from an analysis of field data in the modeled region 3 the reduction in plant growth due to salinity i e the salinity stress factor str salt is calculated using the relative yield ry equation of maas 1993 using the threshold a and slope b parameters for the crop type of the hru 8 r y s t r s a l t 100 b e c s a t a 100 as reported by maas 1993 for gypsiferous soils i e soils with high gypsum caso4 content a should be increased by 2 0 indicating a smaller impact on plant growth in regions than for other soils e g high nacl 4 the salinity stress factor str salt is used in conjunction with the original stress factors used by swat water temperature nitrogen and phosphorus to provide an overall reduction in biomass for that day fig 3 shows a schematic of data flow within the swat modflow salt code inputs for the salinity module include solubility products for each salt mineral initial soil water salt ion concentration for each hru initial salt mineral fraction for each hru initial groundwater salt ion concentration for each rt3d grid cells initial aquifer salt mineral fraction for each rt3d grid cell and parameters a and b for each of the 121 plant types in the swat database besides the basic hydrologic and nutrients outputs of swat modflow surface runoff soil lateral flow groundwater storage and head groundwater surface water exchange rates each with accompanying nutrient loads model outputs for swat modflow salt include soil salt ion concentrations for each hru groundwater salt ion concentrations for each grid cell crop yield with salinity influence and salt ion fluxes for surface runoff soil lateral flow recharge groundwater discharge to streams stream seepage to groundwater groundwater soil transfer surface water irrigation groundwater irrigation precipitation and dissolution 3 model application lower arkansas river valley colorado the swat modflow salt model is applied to a study region in the saline affected lower arkansas river valley larv in southeastern colorado usa the model is tested against measured system response variables groundwater salt ion concentration in stream salt ion loading crop yield to demonstrate its suitability for use in salinity impact scenarios which can be performed in future studies 3 1 study region the study region to which the swat modflow salt modeling code is applied is a 732 km2 section of the larv the region has been irrigated for over one hundred years due to semi arid climate conditions precipitation 375 mm yr irrigation water is derived either from the arkansas river through a network of 6 irrigation canals or from a network of groundwater pumping wells fig 4 a main crops grown include melons onions peppers wheat sorghum corn alfalfa and legumes the local aquifer is made up of alluvium sand and gravel deposited over millenia by the arkansas river and its tributaries and on average is 15 m in thickness from the ground surface to the shale bedrock salinity in soils groundwater and streams has steadily increased over previous decades due to salt mineral dissolution principally caco3 and caso4 and lack of adequate drainage leading to shallow groundwater and consequent evapo concentration of salts in the soil water of the rooting zone approximately 70 of the region has experienced a decrease in crop yield due to elevated soil salinity gates et al 2002 morway and gates 2012 from 443 groundwater samples collected from the network of 75 monitoring wells fig 4a average total dissolved solids tds concentration between 2006 and 2010 was 3300 mg l during this same time period average tds in the arkansas river and its tributaries was 1145 mg l strategies for curbing salinity increase are being sought for this region 3 2 model set up and testing the data inputs for the swat modflow salt model used in this study is based on the swat modflow with rt3d model of wei and bailey 2021 to assess n and p management scenarios and the swat salt model of bailey et al 2019 to assess salt ion fate and transport basically the model inputs from wei and bailey 2021 were modified to include the salt inputs of bailey et al 2019 with the addition of initial concentrations of salt ions for each rt3d grid cell these two modeling studies will now be summarized to provide context for this current study the swat modflow application to the study region wei and bailey 2019 used the swat model from wei et al 2018 tested for streamflow at three gages in the arkansas river and two gages in tributaries linked to the modflow model morway et al 2013 tested against groundwater levels groundwater discharge to streams and evapotranspiration the swat model used 72 subbasins and 5270 hrus each field designated as a unique hru with climate data provided from weather stations in the colorado agricultural meteorological network the modflow model used 250 m grid cells and included groundwater pumping from a network of pumping wells canal seepage from the 6 earthen irrigation canals and groundwater river exchange along the arkansas river and its tributaries the swat modflow model was run for the 1999 2016 time period which include both extremely wet and dry periods the swat salt application to the study region bailey et al 2019 used the same hru and subbasin set up as the original swat hydrologic model of wei et al 2018 salinity module inputs included solubility products for the five salt minerals caso4 caco3 mgco3 nacl mgso4 initial salt ion concentrations in soil water and groundwater and initial salt mineral fractions in the soil profile and the aquifer with these latter four specified for each of the 5270 hrus mineral fractions for caso4 and caco3 were obtained from ssurgo soil map data initial salt ion concentrations in the soil water and groundwater were set to spatially uniform values with the value for each salt ion based on an average of field data from 2006 to 2010 see details of these data in gates et al 2009 the model was tested against in stream salt ion concentrations 2006 2010 time period in the arkansas river and two tributaries timpas creek crooked arroyo soil salinity and groundwater salt ion concentrations the application of swat modflow salt in this study uses the hru and subbasin set up from wei et al 2018 the modflow grid pumping data and aquifer properties from morway et al 2013 and wei and bailey 2019 and the salinity soil and groundwater initial concentrations and salt mineral fractions from bailey et al 2019 datasets for model construction and model parameterization are presented in table 2 fig 4 shows the delineation of swat subbasins fig 4a the modflow grid fig 4e the spatial distribution of soil caco3 fig 4b and caso4 fig 4c and hydrologic features for model inputs and testing stream network and gages monitoring wells pumping wells and irrigation canals fig 4a wei et al 2018 compared two irrigation management schemes 1 irrigation diversions and applied depths based on canal diversion and groundwater pumping data from the colorado division of water resources cdwr and 2 auto irrigation routines within swat and found that results water balance stream discharge in tributaries and the arkansas river were similar in magnitude and timing when linked with swat modflow wei and bailey 2019 auto irrigation routines were used for canal derived irrigation whereas measured monthly rates from the cdwr were specified in modflow s well package and then linked to daily irrigation depths for swat hrus this same set up for irrigation is used in this study the swat modflow salt model is run for the 1999 2009 period which included an extremely wet year 1999 and a major drought period 2002 2003 the model is corroborated against the following system response variables stream discharge soil salinity concentration groundwater salt ion concentration in stream salt ion loading and crop yield groundwater levels were previously tested in the swat modflow application of wei and bailey 2019 as the intent of this paper is to present the new swat modflow salt code and not as yet use it in our study region for scenario analysis no calibration is performed in this study calibrated hydrologic parameters from the swat wei et al 2018 and swat modflow wei and bailey 2019 models are used for the salinity module the only model factors that can be used in model calibration and testing are initial conditions of salt mineral fractions and salt ion concentrations in soil water and groundwater we have elected to keep these values unchanged from the fractions and concentrations used in the swat salt application of bailey et al 2019 3 3 results and discussion results are shown for water balance groundwater head patterns streamflow salt balance groundwater salt ion concentrations in stream salt loading and crop yield we have attempted to provide an assortment of model output and comparison with measured system response variables to verify that the model satisfies the water and salt balance in the region and is a reasonable and accurate simulator of general salt ion concentrations and loadings in the soil aquifer and stream systems 3 3 1 hydrologic results hydrologic results are similar to the previous swat modflow application in the region wei and bailey 2019 with the exception that we are now simulating the transfer of groundwater to soil water when the water table rises into the soil prolife of swat hrus we provide hydrologic results here for completeness and to provide context for the pathways of salt ion transport and loading the average annual water balance mm yr blue text of the model domain is shown in fig 5 water enters the system via precipitation 298 mm and surface water irrigation 64 with this irrigation water diverted from the arkansas river at locations upstream of the model domain water exits the system via et 250 mm surface runoff 23 mm soil lateral flow 0 5 mm and groundwater discharge 88 mm surface runoff soil lateral flow and groundwater discharge constitute the landscape water yield of the region for a total of 111 5 mm 37 of precipitation with 79 from groundwater discharge the groundwater driven nature of the arkansas river is an important feature of the river basin for both water movement and solute transport morway et al 2013 bailey et al 2014 recharge an internal flux is estimated to be 177 mm yr with 50 mm of water transferred from the aquifer to the soil profile resulting in a net recharge of 127 mm yr 42 of annual precipitation groundwater pumping also an internal flux as water is transferred from the aquifer to the soil profile via irrigation events is 24 mm yr or 37 5 of the amount of water applied as irrigation water monthly time series of water balance components are shown in fig 6 with inflow outflow fluxes mm shown for the entire watershed system fig 6a and the aquifer system fig 6b the time series of system wide fluxes fig 6a shows seasonal patterns of high irrigation and et during the irrigation season april october and groundwater discharge steady but increasing during the latter months of the season august november due to increased groundwater gradients from irrigation recharge fig 7 a shows groundwater head patterns one value for each grid cell for 2008 showing the equipotential lines that induce groundwater flow towards the arkansas river and tributaries appreciable surface runoff occurs only during the months of irrigation fig 6c shows the daily fluctuation in total soil water mm in the model domain resulting from a balance of inflows precipitation surface water irrigation groundwater irrigation groundwater transfer to soil water and outflows et surface runoff soil lateral flow recharge see arrows into and out of the soil profile box in fig 5 the time series of aquifer fluxes fig 6b shows the strong influence of recharge highest during the irrigation seasons and groundwater discharge fig 6c shows the daily fluctuation in total groundwater mm in the model domain resulting from a balance of inflows recharge and outflows groundwater transfer to soil water groundwater discharge groundwater pumping see fig 5 in response to seasonal patterns of irrigation maps of average groundwater head average water table depth ground surface groundwater head average daily recharge m3 day and average daily groundwater transferred to soil m3 day are shown in fig 7 for 2008 for each grid cell in the modflow model domain shallow groundwater occurs throughout the area fig 7c with many areas experiencing a water table within 2 3 m of the ground surface the occurrence of shallow groundwater results in groundwater being transferred from modflow grid cells to swat hru soil profiles fig 7d particularly in areas of high recharge fig 7b and groundwater head within 2 m of the ground surface fig 7c in general groundwater patterns correspond to previous groundwater modeling efforts morway et al 2013 and hydrologic fluxes correspond to the previous swat modflow model of wei and bailey 2019 simulated monthly stream discharge is plotted with measured stream discharge in fig 8 for three gage sites in the arkansas river rocky ford la junta las animas see fig 4a for locations and two tributaries crooked arroyo timpas creek similar to previous modeling efforts in this region wei et al 2018 the model performs well in for sites in the arkansas river nash sutcliffe coefficient efficiency nsce 0 75 0 72 0 55 respectively but underestimates flow in the tributaries nsce 0 07 0 35 although temporal patterns are mostly replicated 3 3 2 assessment of salt ion transport and loading 3 3 2 1 salt balance and loading the salt balance and annual loadings kg x 106 for the model domain are shown alongside the hydrologic flux depths in fig 5 the salt loading is the sum of the loading of the eight salt ions in addition to salt movement along hydrologic pathways salt enters soil water and groundwater via dissolution of salt minerals see fig 5 also in the current model application we assume that snow and rainfall total precipitation does not contain salt mass the main hydrologic drivers of salt movement are recharge 286 106 kg and groundwater loading to streams 168 followed by groundwater irrigation 127 cycling salt from the aquifer to the soil groundwater transfer to soil 121 dissolution of salt minerals in the aquifer 116 and salt loading to the soil via surface water irrigation 54 of the salt that enters the river from the landscape surface runoff 20 soil lateral flow 3 groundwater loading 168 88 is from groundwater this percentage is higher than the groundwater portion of water yield 79 due to the higher salt ion concentration in groundwater than in surface runoff and soil lateral flow also of note is the higher salt loading in groundwater irrigation 127 as compared to surface water irrigation 54 although there is more actual surface irrigation water applied 64 than groundwater 24 again this is due to the higher salt concentration of groundwater as compared to stream water monthly salt fluxes kg x 106 for the watershed system and the aquifer system are shown in fig 9 salt loading to the soil system via surface water irrigation and groundwater irrigation sw irrigation pumping occur during the growing season months whereas groundwater salt loading to streams occurs during each month due to a constant groundwater gradient from upland to stream channels the salt loading to the aquifer via recharge fig 9b is mirrored by salt loading to the soil via groundwater that rises into the soil profile gw soil although a net recharge loading occurs 165 286 121 see fig 5 of course the temporal patterns of recharge salt loading are due to the patterns of actual recharge see fig 6b 3 3 2 2 salt in soil water and groundwater the spatial distribution of salt is quantified for soil water salt ion concentration fig 10 and groundwater salt ion concentration fig 11 for each hru and each grid cell respectively maps are shown for each of the 8 salt ions and tds for average annual concentrations mg l the soil water salt ion concentration maps fig 10 showing daily average values during 2006 2009 for each hru provide insights into the spatial variation of salt recharge loading by combining these concentration values with the recharge volumes shown in fig 7b concentrations are due to salt loading from mineral dissolution groundwater irrigation surface water irrigation and groundwater transfer with dilution occurring due to precipitation and strengthening occurring due to evapotranspiration soil water salt concentrations can be extremely high 10 000 mg l due to areas of low water content in the soil profile groundwater salt ion concentration maps fig 11 show hotspots of each salt ion due to recharge salt loading and aquifer salt mineral dissolution figs 5 and 9b groundwater salt loading to the soil profile via irrigation is calculated within the model by multiplying time dependent groundwater concentrations by pumping rates at cells where pumping wells are located see fig 4a for locations and groundwater salt loading to the streams arkansas river tributaries is calculated within the model by multiplying concentrations by groundwater discharge rates at river cells for both soil water and groundwater so4 and ca are the dominant ions due to the extensive presence of gypsum caso4 in the soils and aquifer of the region average so4 and ca concentrations in the aquifer are 1470 mg l and 730 mg l respectively hco3 320 mg l mg 260 mg l na 215 mg l and cl 70 mg l are moderately high followed by k 6 mg l and co3 0 1 mg l simulated groundwater salt ion concentrations compare well to measured concentrations according to frequency distributions fig 12 for these frequency distributions simulated values come from each grid cell in layer 2 of the model saturated portion of the aquifer overall the model results compare well with the measured values particularly for so4 cl mg na and k indicating that the model is able to capture the region wide spatio temporal magnitudes of groundwater salt ion concentrations in the model domain groundwater concentrations of ca are over estimated values of hco3 are under estimated and values of co3 are underestimate although measured values of co3 are extremely low average 1 mg l so residuals between simulated and measured values also are very low 3 3 2 3 salt in stream water however although ca groundwater salt ion concentrations are over estimated in stream loadings compare well with measured loadings as shown in fig 13 upper right chart in general simulated in stream salt ion concentrations and loadings compare well with measured values fig 13 in the arkansas river rocky ford r2 0 71 to 0 83 but similar to the stream discharge results shown in fig 8 loadings are very low compared to measured values in the tributary timpas creek as the simulated concentrations for timpas creek are of the same magnitude as the measured concentrations see fig 13 timpas creek concentration charts for so4 and ca the underestimation of loading is due to the underestimation of stream discharge this also occurs for the las animas site with concentrations matching well r2 0 31 to 0 49 but loads being underestimated due to the underestimation in stream discharge see fig 8 the spatial distribution of in stream salt ion loading is shown in fig 14 these maps show for each of the 72 subbasins in the model domain the average daily simulated in stream loading kg day for each of the 8 salt ions and tds of note are the high magnitudes of loading along the arkansas river also of note are the higher magnitudes in the upstream subbasins as compared to the downstream subbasins with the same pattern occurring for each ion this is due to a significant portion of the water and salt in the arkansas river being diverted to three irrigation canals with diversion points between the towns of rocky ford and la junta see fig 4a this highlights the intense human influence on salt movement in this region salt removed from the river by irrigation canals salt transferred from the aquifer to the land surface by groundwater pumping for irrigation additional salt leaching due to irrigation events groundwater salt loading to the arkansas river and its tributaries that occurs due to high groundwater levels which is due to season long irrigation 3 3 2 4 salinity effect on crop yield the spatial by hru and temporal annual sum for 1999 2009 variations in crop yield are presented in fig 15 for both conditions of salinity stress and no salinity stress on daily crop growth for the condition of no salinity stress the simulation was run without including the reduction in crop growth simulated by equation 8 fig 15a shows the average annual yield ton ha for each of the cultivated fields in the region each corresponding to a unique hru and fig 15b shows the increase in crop yield ton ha when salinity impacts are not simulated many fields have a difference of 2 ton ha due to the presence of salinity average yield ton ha for the condition of salinity stress is 4 67 kg ha compared to 4 76 kg ha for the condition of no salinity stress a difference of 1 9 this is the same difference when comparing total tons of crop yield for the two scenarios the crops most affected by salinity stress are alfalfa corn watermelon onions green beans and soybeans fig 15c and d shows the annual yield ton of alfalfa and corn respectively for each year of the 1999 2009 period for both conditions for alfalfa the total decrease in crop yield due to salinity impacts is 2 7 whereas the decrease for corn is 2 1 for watermelon onions green beans and soybeans the decrease is 13 4 4 7 3 and 2 although watermelon onions and green beans compose only a small fraction 0 6 of the crop yield in the model domain whereas alfalfa and corn compose 69 of crop yield other major plant types rangeland wheat and sorghum are impacted only slightly 0 2 by soil salinity overall 2153 hrus out of the 5270 hrus in the model domain has crop yields decrease due to the presence of salt in the root zone these results agree with the findings of morway and gates 2012 who concluded that 70 of the region experiences crop yield decrease due to soil salinity also the swat modflow application of the region by wei and bailey 2019 overestimated crop yield for alfalfa and corn on a county wide assessment this study provides a model that by virtue of including salt ion fate and transport and its effect on crop growth brings crop yield values closer in line with estimated region wide values 3 4 limitations of the modeling study this study introduces a new version of swat modflow that includes a salinity module for salt ion fate and transport although we have attempted to simulate each major hydrologic and salt ion chemical process in an irrigated stream aquifer system we note the following limitations although we simulate the transfer of groundwater and associated salt ion mass to the soil profile when the water table is above the base of the soil profile we do not simulate the movement of water and salt ion mass in the vadose zone i e the zone between the base of the soil profile and the water table for conditions of a water table that is lower in elevation than the soil profile the time of travel in the vadose zone is represented and simulated by a groundwater delay term days which loads the soil water and salt ion mass to the water table by a transit function a unique value can be assigned to each hru this limitation however is likely to be of more consequence in a region with deeper water tables we do not consider salt movement in erosion runoff due to large storm events and surface runoff fluxes the inclusion of this feature in the model however likely is more important for high desert watersheds with steep terrains the simulation of bicarbonate hco3 fate and transport is more difficult than other ions due to its dependence on ph which is not explicitly simulated in the model previous attempts at simulating the chemical transport of this ion in this region tavakoli kivi et al 2019 bailey et al 2019 always results in mis matches between groundwater concentrations and in stream loadings this can be addressed by including ph in the salinity module but is not included in the current version of swat modflow 4 summary and conclusions this paper presents a new version of the coupled swat modflow watershed model that simulates the fate and transport of eight major salt ions so4 2 cl co3 2 hco3 ca2 na mg2 k in soils aquifer and streams salt ion loads are simulated for all major hydrologic pathways surface runoff soil lateral flow percolation and recharge groundwater soil transfer in areas of shallow groundwater groundwater discharge stream and canal seepage and streamflow under the influence of salt mineral precipitation dissolution reactions the model uses rt3d as the groundwater reactive solute simulator coupled with a salinity module for both advective dispersive transport and equilibrium chemistry model applicability accuracy and usefulness is demonstrated for a saline affected irrigated stream aquifer system in the lower arkansas river valley colorado usa with model results tested against groundwater salt ion concentrations and in stream salt ion loadings the model is also run with and without plant salinity stress on crop yield to demonstrate and quantify the effect of salinity on crop yield in the region this paper is intended as a proof of concept with the new version of the swat modflow code now available for distribution at https github com ryantbailey swat modflow salt the intent is to provide a watershed modeling code that can be applied to saline affected regions to quantify impacts of climate change land management and water management on groundwater salinity surface water salinity soil salinity and crop yield although the model presentation and application is focused on an irrigated region the swat modflow salt modeling code can be applied to any saline affected area however if the model is to be applied to salt movement in high desert watersheds that experience high intensity short duration thunderstorms the salinity module should be modified to include equations for salt mobilization and loading in rainfall induced erosion author contributions r b and p h designed the research r b performed model coding r b and p h performed the research r b and p h analyzed the data r b and p h wrote the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the funding received for this project from both the colorado agricultural experiment station project colo0794 and by nsf career award no 1845605 
25484,watershed and hydro geomorphic delineation is a critical first step in most environmental and natural resource assessments analyses and research while existing geospatial tools have provided exceptional advances less attention has been put toward developing fully automated tools for subdividing landscapes into their constituent hydro geomorphic units or discretizing river corridor features here we present a new open source arcgis toolbox called the utah state university applied usual watershed tools the usual tools are a set of streamlined easy to use arcgis toolboxes that automate the delineation of watersheds sub catchments river adjacent interfluves and discretized river networks with the topological structure and feature attributes necessary for one dimensional source to sink transport modeling through large watersheds this novel geospatial toolset replaces the need for extensive delineation workflows providing the earth science environmental science and natural resources communities with the ability to rapidly and easily automate necessary but often time intensive tasks in a format familiar to arcgis users keywords river watershed delineation 1 d modeling arcgis pro data availability availability https github com watershedswildfireresearchcollaborative usual 1 introduction watershed and hydro geomorphic feature delineation is a fundamental step for geospatial research and analysis of natural resources across a variety of disciplines delineation of hydro geomorphic landscapes are often performed using geospatial software to compartmentalize landscapes into functional units which allows environmental parameters to be binned and modeled across discrete process domains breaking landscapes into functional hydro geomorphic units can provide critical insights into natural resource conditions and processes with respect to biology and ecology hydro geomorphic delineation can help inform the characterization of physical habitat e g belletti et al 2017 models evaluating the effects of physical habitat disturbance e g murphy et al 2020 and the surveys and management of species e g wang et al 2012 additionally hydro geomorphic delineation can assist in natural resource planning and impact assessments e g timber harvest fisher et al 2021 especially when the dispersal and distribution of environmental effects are linked to surface hydrologic pathways and source to sink transport of matter including but not limited to sediment nutrients and pollutants e g ahammad et al 2021 launay et al 2015 saleh et al 2013 finally the hydro geomorphic delineation of landscapes allows for more accurate modeling of the variable erosional processes that occur across the different geomorphic units within watersheds e g cavalli et al 2013 gannon et al 2019 gartner et al 2014 murphy et al 2019 staley et al 2017 wall et al 2022 rivers and waterways control the surface transport of matter through landscapes from a point source to a downstream sink whether that sink is local e g reservoir or lake or global oceans tracking matter through a landscape often employs one dimensional 1 d routing models of water e g david et al 2011 saleh et al 2013 sediment e g ahammad et al 2021 czuba et al 2017 murphy et al 2019 viparelli et al 2013 nutrients or pollutants e g launay et al 2015 or any other material that can be entrained in water and moved through a river network however these methods rely on computing spatial metrics over the two dimensional area of a watershed this typically requires extensive and often manual geospatial analysis or the application of multiple tools developed specifically for morphometric characterization of rivers and hydrologic modeling clubb et al 2014 passalacqua et al 2010 2012 schwanghart and scherler 2014 schwenk et al 2020 tarboton 1997 to extract the parameters from a watershed necessary to initialize the routing models e g czuba et al 2017 for instance the methods required for murphy et al 2019 to delineate and attribute a river network and hundreds of sub catchments for analysis of post wildfire erosion and sediment dynamics involved more than 50 arcgis tools interspersed with many manual steps as well as passing data back and forth between external development environments to execute scripts and perform functions not available in arcgis hence there is a need for a streamlined set of gis tools designed specifically for hydro geomorphic watershed delineation and attribution that serves the needs of natural resource management environmental science and source to sink modeling several geospatial tools currently exist to delineate watersheds river networks sub watersheds and to compute spatial morphometrics for instance topotoolbox delineates river networks watersheds and sub watersheds schwanghart and scherler 2014 additionally tak is a toolkit built on top of topotoolbox which streamlines functions and removes the need for a matlab license forte and whipple 2019 however topotoolbox and tak were designed to compute topographic metrics for advanced longitudinal river profile analysis channel steepness chi plots etc another commonly used tool lsdtopotools clubb et al 2014 is a linux based toolkit which primarily focuses on channel extraction and attribution lsdtopotools also contains functionality to delineate fluvial features such as floodplains and terraces clubb et al 2017 lsdtopotools also incorporates components of geonet sangireddy et al 2016 which uses advanced filtering techniques and attributes the network for high resolution topographic data i e lidar another tool rivgraph schwenk et al 2020 schwenk and hariharan 2021 automates extraction of river networks and attributes e g flow direction reach length and width branching angle etc from binary images of river networks without the need for an underlying digital elevation model dem collectively these tools have advanced our ability to conduct higher order fluvial and landscape analysis however all of these tools require knowledge of computational languages in potentially unfamiliar computing environments moreover these tools are largely focused on delineating and extracting morphometrics specific to the river network alone in contrast taudem tarboton 1997 and swat https swat tamu edu software qsswat and arcswat are examples of tools that were built with graphical user interfaces gui in common gis platforms arcgis and qgis to provide methods for delineating rivers watersheds and sub watersheds both taudem and swat were designed and developed to delineate watersheds for setting up common hydrologic routing schemes e g muskingum method accordingly it is important to recognize that they subdivide watersheds into hydrologic units which both conceptually and functionally differ from hydro geomorphic process domains specifically in their sub watershed delineation neither taudem nor swat distinguish between interfluvial process domains where hillslope runoff and erosion processes dominate and tributary sub catchments where streamflow and fluvial processes dominate instead taudem and swat subdivide watersheds into sub watersheds which define all areas that directly contribute surface flow to each discretized reach of the delineated river network fig 1 a as a result this approach aggregates all tributary catchments and hillslope process domains along each river network reach into a single polygonal area however these different process domains contribute flow sediment and other materials to each reach by distinctly different processes and magnitudes aggregating process domains may work for hydrologic models but these two tools are unsuitable for discretizing landscapes for many process based hydro geomorphic models for example the u s geological survey model for predicting debris flows within burned watersheds staley et al 2017 is applied at the scale of first and second order tributary sub catchments the debris flow modeling domain does not include network adjacent interfluves or reaches of the major river network and the inclusion of these areas through the use of the sub watershed extents produced by either taudem or swat would mischaracterize extracted model predictors and skew model predictions to date existing delineation tools have focused primarily on hydrologic modeling river channel extraction and topographic morphometrics however less attention has been given to delineation of watersheds at the sub catchment and interfluve scale fig 1b which is critical for accurately computing localized geospatial metrics and modeling runoff and erosion within hydro geomorphic process domains specifically for source to sink models therefore we introduce the utah state university applied usual watershed tools or for brevity usual the usual watershed tools are an open source python based toolkit for arcgis that have been designed to delineate watersheds sub catchments interfluves and river networks discretize river networks and extract attributes necessary for fluvial network routing and provide new gis functions for feature extraction and attribution usual has an arcgis graphical user interface gui to make the toolkit easy to use and familiar for users with experience in arcgis software however we have also made the underlying python scripts available for advanced users who wish to adapt and modify the tools novel attributes of the usual watershed tools include 1 delineating both interfluves and sub catchments fig 1b 2 providing the ability to exclude user defined regions of the topography that would traditionally yield erroneous delineations e g delineating catchments and interfluves across waterbodies 3 automating the identification and generation of pour points required to delineate sub catchments and interfluves 4 automating the discretization and attribution of a river network for 1 d routing 5 automating the computation of reach averaged widths from fluvial features e g river floodplain valley bottom hence the usual watershed tools automate many frequently required and labor intensive steps in watershed delineation and generates outputs that seamlessly integrate with state of the art 1 d routing models e g czuba 2018 pfeiffer et al 2020 tangi et al 2019 herein we introduce the components of the usual watershed tools describe how each works detail all required and optional inputs and describe the final gis products output from each tool section 2 we then explore and demonstrate how usual handles 1 variable dem resolution 2 shallow vs steep gradient landscapes 3 watersheds of varying channel complexity and 4 the computational efficiency of feature delineation section 3 finally we provide an example application of the usual watershed tools in the logan river utah to demonstrate its utility for evaluating common models of soil erosion and delivery as well as its capability to inform sediment routing models section 4 2 toolkit description the usual watershed tools are a set of python based scripts built on the esri arcgis platform developed using arcgis pro version 2 9 and available for download on github see software and data availability for link the toolkit requires a basic arcgis pro license as well as spatial analyst and 3 d analyst licenses usual provides users with the familiar easy to use arcgis toolbox guis and is constructed as a suite of toolboxes that can be executed independently or in sequence fig 2 however the underlying python code can also be opened and executed in arcgis or externally using any python integrated development environment ide e g spyder jupyter notebook usual is comprised of four main tools used for extracting delineating and attributing spatial metrics across different morphologic features in watersheds fig 2 additionally usual contains two optional sub toolkits each containing two tools to extract additional attributes fig 2 broadly the toolkit will delineate a watershed a river network all sub catchments draining to the network the interfluves between the sub catchments and adjacent to the river network fig 3 and compute and attribute spatial metrics for the delineated features running the primary workflow for the usual watershed tools requires a minimum of just two data inputs a dem raster and a pour point shapefile identifying the downstream most point for the watershed however in the following subsections we will outline and describe all the required and optional inputs and parameters functionality and outputs from each tool 2 1 watershed and river delineation the first tool in the usual workflow the watershed and river delineation tool delineates the watershed extent and river network upstream of a user specified point in a landscape pour point based on topography and surface hydrologic pathways this tool is essentially a wrapper script that streamlines the standard order of operations within the esri hydrologic toolbox for basic watershed delineation and river network delineation but with some critical additional functionality to handle more complex but commonly encountered watershed scenarios the watershed and river delineation tool requires just two inputs table s1 a watershed pour point specified as a point feature and a topographic raster representing the dem additionally the user must specify a river network drainage area threshold value in hydro geomorphology this typically defines the minimum drainage area marking the transition point between hillslopes and river channels in the landscape montgomery and foufoula georgiou 1993 tarboton et al 1991 however as the usual watershed tools can be used to generate river networks for 1 d routing this threshold can also be viewed more simply as defining the extent of the network intended for modeling fluvial material transport although the lower order tributaries may be important for some models users may not want to include the full network in a large scale watershed routing analysis in the usual framework this threshold also inherently defines the maximum drainage area of tributary channels that are delineated as sub catchments see section 2 2 1 the tool also accepts three additional optional inputs table s1 an area of interest polygon feature a pour point snapping tolerance numeric input and an option to conduct a nested watershed delineation the area of interest aoi polygon clips the input dem to the extent of the aoi before running the esri hydrology tools which helps improve computational efficiency if the input dem is not pre clipped and significantly larger than the watershed being delineated the pour point snapping tolerance defines a radius in map units that feeds into the esri snap pour point tool and moves the pour point location to the highest flow accumulation cell within the search radius to ensure the point falls within the river channel the nested analysis option allows users to define and input multiple pour points for a watershed fig 4 a and allows the tool to delineate multiple nested watersheds inside the primary watershed fig 4b first the tool delineates the entire watershed extent based on the prescribed pour point using esri hydrology toolset it fills small hydrologic depressions sinks in the dem computes flow direction d8 algorithm and flow accumulation across the dem and finally delineates the extent of the entire watershed as a polygon feature next the tool delineates the river channel network by reclassifying the flow accumulation raster based on the prescribed river drainage area threshold and applies the esri stream to feature tool to convert the reclassified binary raster into a stream network polyline in many watersheds there may be reservoirs upstream and within the larger watershed extent the presence of these nested reservoirs can influence network routing modeling and analysis i e they represent significant sinks along flow paths as well as the assessment and modeling of dispersal for aquatic populations the watershed and river network delineation tool could be run individually for each of these nested watersheds however rectifying the entire watershed delineation would require subsequent manual merging of the multiple watersheds and networks while this provides a relatively simple solution to produce a visually satisfactory gis product the simple merging of features would not result in the creation of a single and functional routing network with consistent and linked attributes that would allow for modeling material transport or migratory behavior through the entire watershed to address the common issue of nested watersheds we incorporated the option to run a nested analysis this function does not automatically identify the presence of nested watersheds or generate pour points for any potential nested reservoirs rather the user must input multiple pour points to the tool the tool first delineates the larger scale watershed that encompasses all the reservoirs upstream from the downstream most pour point and then delineates the nested watersheds based on the natural drainage divides and clips them by the delineated watersheds of upstream reservoirs fig 4 although delineating the nested watersheds as separate features the tool still generates a single continuous river network for the entire watershed extent that maintains accurate drainage area attributes for each reach i e the drainage area calculated immediately below a nested reservoir is not inaccurately identified as a drainage boundary with no contributing upstream area the tool also outputs sub networks for each nested watershed in case the user has a need for these features outputs from the watershed and river delineation tool include table s1 1 a watershed polygon delineating the extent of the watershed 2 a river network polyline mapping out the extent of the river network 3 rasters for the filled dem flow accumulation flow direction and river channel clipped to the watershed extent if a nested analysis is performed the tool will output all the above features for the full watershed as well as for each of the nested watersheds 2 2 sub catchment and interfluve delineation the sub catchment and interfluve delineation tool comes second in the primary workflow and subdivides and delineates the watershed into two feature groups 1 sub catchments that define the drainage areas for all tributaries that flow directly to the delineated river network and 2 the interfluves between sub catchments that are directly adjacent to and along the river network figs 3b 5 although interfluves are smaller in area relative to the sub catchments within the watershed and often not included in other delineation tools the interfluves are directly stream adjacent and comprise a significant percentage of the river network length accordingly interfluves can be a significant source of direct material inputs to the river network in some watersheds fisher et al 2021 kelly and belmont 2018 vaughan et al 2017 furthermore compartmentalizing the landscape into discrete components creates a geospatial framework where every geomorphic process domain across the landscape that may serve as a point source input can be evaluated or modeled as a potential source and associated with spatially explicit locations of delivery to the river network the sub catchment and interfluve delineation tool requires five input datasets all of which are generated by and output from the watershed and river delineation tool table s2 1 filled dem 2 flow direction raster 3 flow accumulation raster 4 river network shapefile and 5 watershed polygon additionally the user must prescribe a sub catchment drainage area threshold which defines the minimum drainage area for delineated sub catchments the upper limit for sub catchment drainage area is set by the river network drainage area threshold value for example if the river network drainage area threshold value was set to 5 km2 and the sub catchment drainage area threshold value was set to 1 km2 then the drainage area of every sub catchment delineated in the watershed would be between 1 and 5 km2 it is worth noting that while this tool can conveniently be run using the outputs from the previous watershed and river delineation tool the outputs from other tools e g a geonet or rivgraph derived network could also be used as input data the tool also contains a novel function for handling exclusion areas within the watershed these are regions of topography that would be included in standard esri delineations and other delineation tools but which result in the creation of erroneous sub catchment and interfluve extents that are not a reflection of reality and could result in the mischaracterization of extracted properties from those domains e g zonal statistics for example a standard delineation of sub catchments draining directly into a waterbody e g lake or reservoir extend out across the surface of the waterbody fig 5a thus including large areas that are not representative of the actual sub catchment topography fig 5b typically correcting this issue requires selecting or adjusting pour point locations for each of these sub catchments by hand our exclusion area function eliminates this tedious and time intensive step by automatically identifying and generating sub catchment pour points at the perimeter of the exclusion area before feature delineation additionally the tool will correctly delineate the interfluves with downslope boundaries that terminate at the perimeter of the exclusion area fig 5a and b to utilize this function the sub catchment and interfluve delineation tool includes an optional input of a polygon shapefile that encompasses the extent of exclusion area e g reservoir urban area valley bottom the input feature can contain a single polygon or multiple polygons additionally the tool contains an optional input allowing the user to define a number of cells to buffer the input polygons which helps correct flow paths between interfluves and sub catchments converging at the edge of the exclusion area however buffering an irregularly shaped polygon can create holes in the buffered exclusion area polygon fig s1 to overcome this the toolkit also contains an option to fill any holes in an exclusion area polygon fig s1 the sub catchment and interfluve delineation tool contains up to three options for outputs table s2 1 sub catchments 2 interfluves at fine resolution and 3 interfluves at a coarse resolution by default the toolbox outputs all three however the arcgis toolbox contains tick boxes booleans in the code which allow users to select or suppress outputs for any of the three features in the following two subsections we outline how each is delineated and their respective outputs 2 2 1 sub catchment delineation sub catchment pour points are internally identified by first creating a binary raster where flow accumulation cells equal to or above the sub catchment drainage area threshold value are given a value of one and cells below the threshold are given a value of zero second if an area to exclude polygon is provided the portions of the input binary network raster one for river cells zero otherwise inside this area are converted to a value of one raster calculator is then used to subtract the binary network raster with exclusion area from the binary flow accumulation raster above the sub catchment drainage area threshold value the result is a binary raster all negative values are set to zero indicating which cells flow directly into but are not in the river network those raster cells are then converted to pour points directly adjacent to the river channel and exclusion area and input to the esri watershed tool to delineate the contributing area of each sub catchment fig 5a and b the tool outputs a polygon shapefile with all sub catchment polygons and a shapefile of the pour points used to extract each sub catchment each sub catchment and its associated pour point are assigned a unique identifier which is written to their attribute tables so they can be linked for source to sink modeling 2 2 2 interfluve delineation interfluve delineation can be performed in two ways at a coarse fig 5b and fine fig 5c resolution interfluve pour points are identified by locating all flow accumulation cells adjacent to the river network as well as around the perimeter of any optional exclusion areas that have contributing areas greater than zero but less than the defined sub catchment drainage area threshold value these pour points are then used in the esri watershed tool to delineate the contributing area of every interfluve flow path fig 5c this fine scale resolution of interfluves may be beneficial for users modeling and tracking non channelized flow paths of potential point source inputs to the river network e g soil erosion or nutrient runoff from agricultural fields however if a user were interested in coarser scale delineation of interfluves for morphometric calculations the tool can also aggregate and merge all of the fine scale interfluves between the sub catchments to create coarse resolution interfluves fig 5b for this latter case the tool does not output pour points because the coarse resolution interfluves are intended for broader scale environmental parameter characterization rather than source to sink modeling additionally the tool provides an optional input to delete all coarse interfluves containing less than a user defined number of cells thus filtering out all small coarse resolution interfluves 2 3 network discretization and attribution the network discretization and attribution tool discretizes the river network polyline into discrete reaches based on a user defined maximum length and consideration of geometric constraints from tributary junctions in the river network fig 6 additionally it computes and attributes key fluvial routing parameters to each associated reach including upstream drainage area reach length and reach average slope fig 6 discretizing river networks is a necessary step for calculating reach averaged morphometrics e g channel width valley bottom width river slope creating topologic networks through which numerical models can route flow and other materials and characterizing the conditions of discrete patches of aquatic habitat inputs for this tool include table s3 1 a polyline feature of the river network 2 the flow accumulation raster and 3 a filled dem additionally the user defines a maximum discretization length of the network reaches in map units the tool initially splits the sections of river network between each of the network confluences into segments at the user defined maximum length starting from the upstream extent and moving downstream however in this first pass discretized reaches that are considerably shorter than the user defined length are typically generated as it is rare that a length of river between confluences is perfectly divisible by the defined discretization length these short reaches sometimes just a few meters in length are often not long enough to reliably characterize reach averaged morphometrics or habitat conditions they can also be problematic as inputs to network routing models e g if modeled transport lengths exceed reach lengths over the scale of one time step it can lead to numerical instabilities to address this issue usual identifies all reaches less than one half of the maximum length and merges each with the next upstream reach the tool then splits the newly merged reach at the midpoint to avoid creating reaches longer than the user defined maximum for attribution of reach parameters fig 6 one required input by the network discretization and attribution tool is a minimum river reach slope the tool extracts the upstream and downstream elevations m of each reach as well as the reach length m and then computes the average slope m m 1 along each reach as the difference between the maximum and minimum elevations divided by the length derived from a filled dem it is possible the river network may include reaches with zero slope or the network may pass through extremely flat regions such as lakes or reservoirs these extremely low or zero slope reaches can cause numerical instabilities in 1 d network routing models therefore the defined minimum threshold value is used to replace any reach slope values less than this value if a user does not wish to replace derived slopes they can set the minimum value to less than or equal to 0 the tool outputs table s3 a new updated river network shapefile that is discretized and includes an attribute table containing a unique reach id reach length upstream and downstream elevations reach average slope and a field identifying the id of the next reach downstream fig 6 which is critical information for network routing 2 4 network routing preparation the last tool in the main workflow of the usual watershed tools fig 1 is the network routing preparation tool the tool ensures all pour points and the river network contain the necessary attributes to be applied for 1 d network routing the tool has two functions 1 snapping all pour points sub catchments and interfluves to the river network and 2 flagging any network reaches that intersect polygonal areas of interest e g water bodies urban areas vegetation types or land cover snapping the pour points for all of the interfluves and sub catchments to the river network both functionally and spatially links the potential flow and material input locations throughout the watershed to points along the river network the pour points produced from the sub catchment and interfluve delineation tool are all generated at flow accumulation cells immediately upstream or upslope but not directly on or intersecting the river network or perimeter of exclusion areas this is required for the proper delineation of these features however to use these locations as spatially explicit point source nodes of material delivery to the river network it is necessary the pour points intersect the river polyline and can be functionally linked to the appropriate reach or waterbody the tool uses the esri snap function to move the points onto the river network and then writes all of the attributes of the associated river reach to their respective snapped pour points the second function to flag river reaches provides the user the option to identify reaches of the river network that may be of interest for subsequent applications or analysis one example would be a river network delineated upstream of a dam that includes reaches delineated across the reservoir surface e g fig 4 for 1 d routing applications these reaches would not be appropriate locations to apply fluvial transport equations and thus valuable to identify as unique from other reaches other examples could include characterizing which reaches flow through urban areas or through the extent of a wildfire regardless of the intended application the tool uses a binary notation to identify which reaches in the river network intersect a user specified polygon of interest 1 and which do not 0 fig 6c the tool outputs table s4 new shapefiles of snapped pour points for both interfluves and sub catchments and leaves the original unsnapped pour point shapefiles unmodified additionally all network attributes including those added to the network in this tool i e flagging are appended to the attribute table of each newly snapped pour point shapefile 2 5 data regression tools the usual watershed tools contain an additional sub toolkit attribute regression tools which houses two tools for data regression within arcgis based on fields within a shapefile s attribute table the first tool scaling relationship generator tool computes a univariate regression between two variables and the second tool scaling relationship extrapolator tool applies the regression writing predicted values to a new field in the attribute table typically this type of data analysis would require exporting the attribute table data to conduct regressions in external software and then reading the data back into arcgis to continue any geospatial analysis an example application for this tool would be using a discretized river network that contains upstream drainage areas e g output from the river network discretization and attribution tool augmented with a field of river widths manually measured at select locations in the watershed to develop a drainage area river width scaling relationship using the scaling relationship generator tool the scaling relationship extrapolator tool applies the regression across the entire network attributing all links with river width based on an upstream drainage area the scaling relationship generator tool uses any attributed shapefile as an input i e this does not need to be a shapefile created by usual the user then specifies the attribute field table s5 containing the dependent dataset and the field containing the independent dataset and specifies one of three types of regressions to be applied linear exponential or power if the appropriate regression type to be applied is unknown the user should initially plot the data using the esri chart tools to visually determine the most appropriate trend to be applied before using the tool the scaling relationship generator tool regresses the data and appends the coefficient values to the shapefile attribute table with fields coefficient a and b defining the two regression coefficients if the tool is run multiple times on the same input dataset the tool will overwrite the coefficients previously written to the shapefile finally the tool includes an option to output a scatter plot to a user defined file directory that displays the input data regression line r2 value and the regression equation the second tool is the scaling relationship extrapolator tool this tool allows a user to apply one of the three regression types to generate a new attribute field of predicted values within a shapefile based on identified fields for a dependent variable and coefficients returning to the above drainage area river width relationship a regression could be applied to the upstream drainage area field of the discretized network to estimate river widths for every reach fig 6c and example application shown in section 4 1 the tool requires table s6 an input shapefile and requires the user define an output field name as well as identify the input attribute fieldnames for the dependent variable coefficients a b and the regression type associated with the coefficients exponential power law or linear the tool then applies the fit and writes the predicted values to the output field 2 6 fluvial reach average width tools the usual watershed tools contain a sub toolkit called the fluvial reach average width tools to measure reach average widths along an elongate polygon e g river channel or valley bottom this functionality addresses a common need in characterizing reach scale morphometrics as well as the generation of initial conditions often needed in 1 d network routing the first tool in the set the fluvial polygon transects generates a high density of nodes and transects that connect across the opposite sides of the elongate polygon feature the tool requires table s7 an input polygon a user defined transect spacing in map units and a polygon centerline fig 7 this centerline does not necessarily have be exactly centered through the polygon but must intersect the polygon at the tips and only at the tips while a river network could work generating a centerline manually or using the polygon to centerline tool requires the esri production mapping extension ensures the outlined criteria the elongate polygon e g river floodplain valley bottom needs to be generated externally from usual either by manual delineation or using existing tools e g clubb et al 2017 gilbert et al 2016 roux et al 2015 schwenk and hariharan 2021 alternatively centerlines can be generated using non esri tools and methods e g dilts 2015 lewandowicz and flisek 2020 roux et al 2015 schwenk and hariharan 2021 regardless of the approach one critical condition of any centerline used for these tools in usual is that it must extend to the polygon perimeter at all tips of the elongate polygon fig 6a c this is required because the tool relies on this intersection point of the line and polygon to identify and split the elongate polygon into left and right segments starting at the intersection point of the polygon and the centerline the tool generates points along the edges of the two halves of the polygon at the defined transect spacing fig 7b d a kd tree is applied to each point to identify the closest point on the opposite side of the polygon and to generate point pairs the paired points are then converted into lines producing a series of transects spanning the width of the elongate polygon fig 7b d there are two additional optional inputs 1 a search radius applied around the points where the centerlines intersect the polygon to exclude points from transect generation figs 7b and 2 a polygon feature to allow users to prescribe specific areas to exclude from transect generation fig 7c these two options are intended for the exclusion of perimeter points at the upstream and downstream most tips of the polygon fig 7 b c where generating transects across flat edges corners or tapered edges may result in unrepresentative polygon width calculations regardless of whether the optional exclusion functions are used the tool outputs table s7 a polyline shapefile containing transects that extend across the polygon with lengths calculated and attributed to each line feature fig 7b d the second tool is fluvial reach averaged width which computes and attributes average widths for a discretized line feature within the elongate polygon the intended application is to compute average widths of river channels or valley bottoms along each reach of the discretized river network but there could be other potential applications the tool densifies transect vertices converts them to points and assigns the transect length i e local polygon width to each these attributed points are used to generate a tin which is then converted to a raster of the user defined resolution with values representing the local polygon width fig 7e finally the esri zonal statistics tool is applied to calculate the average width value from the raster along each reach of the discretized polyline and write this value to the new output field in the attribute table table s8 2 7 model structure and data management overall the usual watershed tools require a minimum of just two input datasets a watershed pour point and a dem to delineate a watershed boundary generate a discretized and attributed river network delineate all sub catchment and interfluve extents and identify their associated point source input locations along the river network fig 2 moreover each tool in usual can be run independently using inputs created by other toolkits the additional tools provided in usual can also be applied to a variety of geospatial applications beyond watershed and network delineation key outputs from usual fig 2 are written to a user defined folder additionally the output folder will include a temp subfolder to house all intermediate outputs within which each tool creates its own subfolder of organized outputs the inclusion of this intermediate data is intended to help users troubleshoot problems and this folder structure makes it easy to delete all intermediate files if the final results are satisfactory if applying a nested analysis the output folder contains an additional folder with outputs for each nested watershed that is based on an optional user naming structure 3 evaluation testing and limitations in this section we demonstrate the capabilities of the usual watershed tools we test the toolkit in watersheds with varying dem resolutions topographic relief and river channel complexity and evaluate the computational efficiency of usual additionally we discuss potential limitations and considerations when running usual for different scenarios 3 1 dem resolution to demonstrate the applicability of the usual tools across sites with variable resolution of available dem datasets we explored three sites 1 the highest resolution site was the big walnut creek 125 km2 watershed near north salem indiana usa fig 8 a with a 1 5 m lidar derived dem https maps indiana edu 2 the intermediate resolution site was dells creek 41 km2 watershed upstream of little dell reservoir ut usa fig 8b with a 10 m usgs dem https apps nationalmap gov and 3 the coarsest resolution site was the fraser river between lytton bc canada and the southern end of edge hills provincial park bc fig 8b 5852 km2 watershed with a 30 m dem from trim https catalogue data gov bc ca dataset to delineate big walnut creek we used a river network threshold of 0 2 km2 and a sub catchment threshold of 0 05 km2 the dell creek delineation used a river network threshold of 5 km2 and a sub catchment threshold of 0 1 km2 for the fraser river fig 8c we used a river network threshold of 5 km2 and sub catchment threshold of 0 1 km2 additionally along the fraser river we input a pour point that was not at the mouth of the river and a dem that intentionally did not cover the entire extent of the watershed which covers 25 of british columbia rennie et al 2018 to demonstrate the applicability and limitations of usual when applied to large rivers with input dems that do not cover the entire watershed outputs from our delineations fig 8 reveal that the usual watershed tools can be applied to delineate features from a variety of data sources and across a wide range of dem resolutions furthermore our delineation of the fraser river dem that did not cover the entire watershed extent fig 8c reveals that the landscape delineations are still successful the interfluves and sub catchments are still correctly delineated because their pour points are based on drainage areas adjacent to the river channel however the upstream most reach of the fraser river was not delineated fig 8c due to significantly lower than accurate flow accumulation values throughout the fraser river reaches caused by the truncated extent of the dem hence if inputting a dem to usual that does not span the full watershed a user should ensure that the dem at least extends well beyond the upstream most river reach of interest additionally if applying the river network discretization and attribution tool users should beware that the drainage areas attributed to truncated watersheds and segments of larger rivers will be smaller than the real upstream drainage areas 3 2 topographic relief and channel complexity to demonstrate the capability of the usual watershed tools for delineating features in landscapes of varied topographic complexity we compared subsets of the fraser river and big walnut creek which reasonably represent end members of complexity the fraser river is a large river traversing a series of canyons rennie et al 2018 containing high topographic relief fig 9 a and a complex network of tributaries fig 8c big walnut creek is located in the central till plain of indiana gray 2000 which contains minimal topographic relief fig 9c in our aoi big walnut creek s downstream portion is a meandering river with well developed riparian vegetation whereas the headwaters are mostly small agricultural ditches representing a variety of anthropogenic influences e g channel straightening and constructed levees in the channel network fig 9c our results show that the usual watershed tools are capable of delineating features in both steep mountainous terrain fig 8c fig 9 a b and relatively flat landscapes figs 8a and 9 c d this is because the terrain processing tools operate on elevation differences between neighboring cells therefore the usual watershed tools work well on any dem with sufficiently high resolution that captures the topographic variability in detail additionally our analysis illustrates the utility of usual in landscapes with natural fig 9a and anthropogenic complexity fig 9c however one limitation of usual in low gradient landscapes is that advanced topographic filters passalacqua et al 2012 sangireddy et al 2016 are not included in the river network delineation which can help overcome problems introduced by bridges and other human engineered features hence in order to use usual in these settings a user must first pre condition the dem for delineation by manually lowering the elevations of these features to match the river network an additional limitation we note is that network delineation in usual was developed to handle single threaded channels and may not work as expected in braided or anabranching river systems 3 3 computational efficiency to evaluate computational efficiency we performed a series of repeated delineations on toroda creek washington usa with varying dem resolutions toroda creek is located in mountainous north central washington state with a watershed area of 175 km2 fig 10 a the input dem was 1 m resolution and derived from lidar https lidarportal dnr wa gov the dem was then resampled to resolutions ranging from 1 to 10 m at 1 m increments and from 10 to 100 m at 10 m increments giving a total of 19 different dem resolutions fig 9b the watershed and river delineation tool and sub catchment and interfluve delineation tool were run and timed for each dem resolution to account for any potential computational variability we ran each tool 10 times at each resolution and present the average computation time for our simulations we used an aoi that extended just beyond the drainage divides fig 10a a river network threshold of 5 km2 fig 10a and a sub catchment threshold value of 0 1 km2 simulations were performed on a consumer grade laptop containing i7 7700hq quad core 2 8 ghz 24 gb ddr3 ram and a nvidia gtx 1050ti graphics card each tool was called and parameterized using spyder ide and timed using the perf counter in the time module results from this watershed indicate that dem resolutions of 10 m here 1 8 106 cells or less ran both tools in under 1 min and dem resolutions less than 4 m here 11 3 106 cells ran in less than 5 min for the highest resolution 1 m lidar dem running both tools took 30 min here 175 5 106 cells fig 10b for a full break down see tables s9 and s10 supplementary information 4 example application in this section we provide an example application of the usual watershed tools to delineate watershed features discretize and attribute a river network and apply the outputs to a 1 d network routing model specifically we applied usual to the logan river watershed upstream 555 km2 watershed of the first dam reservoir in logan utah usa and use this site to demonstrate the utility of usual for an application of source to sink watershed modeling and analysis the usual delineated hydro geomorphic features were used as inputs to model spatially explicit soil erosion and delivery across the watershed initialize a 1 d sediment transport model and to evaluate the soil erosion model by comparing modeled and pre existing field based estimates of sedimentation rate at the downstream reservoir 4 1 usual application to the logan river we first applied the watershed and river delineation tool to the logan river fig 11 a for our delineation we used 10 m usgs dem specified an aoi that extended just beyond the drainage divides used a river network threshold of 5 km2 and specified a pour point at the outlet of first dam sub catchments and interfluves fig 11b were then generated with the sub catchment and interfluve delineation tool using a sub catchment threshold value of 0 1 km2 the river network discretization and attribution tool was applied using a maximum discretization length of 500 m and a minimum slope threshold of 0 01 m m to attribute the network with river widths we used the regression application tool and applied a power law relationship to scale drainage area to river width based on coefficients derived for the continental united states wilkerson et al 2014 1 b 1 41 a 0 462 where b is the bankfull width m and a is the drainage area km2 finally the network routing preparation tool was run to produce a river network ready for routing analysis to model spatially explicit sediment inputs to the logan river we used the revised universal soil loss equation rusle models of sediment delivery and the usual delineated features see supplementary information for more detailed methods applying rusle across the watershed we generated a 10 m raster output of annual erosion rate we then applied a sediment delivery ratio sdr equation based on flow path length to the river network to predict what fraction of soil eroded in each raster cell would be delivered to the river network gannon et al 2019 wagenbrenner and robichaud 2014 the sdr raster was then multiplied by the rusle raster to predict the volume of soil delivered to the river network each year from each raster cell the delineated interfluves and sub catchments from usual were then used to run the esri zonal statistics tool to sum the volume of sediment fig 11c eroded and delivered to the river network from each feature polygon using the unique ids linking polygons to pour points the modeled sediment input volumes were then written as attributes to the snapped pour points for each feature 4 2 application of 1 d sediment routing the delineated river network and snapped interfluve and sub catchment pour points from usual feed directly into 1 d network routing models such as the network sediment transporter nst pfeiffer et al 2020 the nst model uses a lagrangian framework to simulate the transport of sediment under space and time variable flow conditions on a river network czuba 2018 in the model framework each reach of a discretized river network serves as a link through which sediment is routed and the snapped and attributed pour points for sub catchments and interfluves generated by the usual watershed tools function as sediment input locations to the network the modeled sediment delivery volumes at each interfluve and sub catchment pour point inform the annual sediment loads supplied to the logan river network fig 11d sediment loads were divided into two grain size fractions sand and fines 2 mm based on soils data from statsgo reybold and teselle 1989 the fine fraction was treated as suspended load and was delivered to the reservoir based on a sediment rating curve the sand fraction was routed with nst using a mixed size sediment transport equation wilcock and crowe 2003 simulations were run for five water years using daily discharges fig 12 measured just upstream of first dam usgs gage 10109000 10 1 2016 9 30 2021 and scaled to each reach based on its upstream drainage area at the beginning of each month one twelfth of the predicted annual sediment delivery at each pour point was supplied to the river network and routed through the logan river 4 3 results and discussion our initial simulations indicated annual sedimentation in the reservoir was 2 5 orders of magnitude greater than the field measured volumes of 641 m3 yr 1 utah division of water resources udwr 2010 while sediment routing models have the potential to over or under estimate transport volumes the magnitude of disparity in these results likely reflects over predictions in soil erosion and delivery by rusle and or the sdr equations a possible source of error in the rusle calculations is the inherent limitations in the accuracy of geospatial data and uncertainty in factor parameterization kampf et al 2020 in particular the cover management factor has been shown to span up to 3 orders of magnitude larsen and macdonald 2007 further although rusle is commonly applied at landscape scales it has previously been shown to over predict erosion for sub catchments or watersheds larger than 0 01 km2 kampf et al 2020 additionally we evaluated a range of potential sdr equations for our analysis e g ebrahimzadeh et al 2018 and we chose the sdr model that resulted in the lowest magnitude of sediment delivery to the river network depite this effort we ultimately found that we had to impose a 95 watershed wide reduction to predicted sediment delivery volumes fig 11d in order to produce a model with annual reservoir sedimentation rates that approximated field estimates modeled sedimentation rate of 669 m3 yr 1 fig 12 this example underscores the utility of usual for broad applications in watershed analysis and more specifically an example of how usual can be used to evaluate and run watershed scale source to sink models additionally while it is more difficult to quantify and demonstrate here the use of usual for this application eliminated the time intensive steps typically required for delineating and extracting attributes for hydro geomorphic modeling the labor and computational time required to develop functional workflows to execute the 150 arcgis functions required for the hydro geomorphic delineation of just a single watershed see supplementary information for usual workflows and to code the interspersed non arcpy steps necessary to fully prepare a watershed for analysis and hydro geomorphic modeling can represent weeks of work for a single project e g murphy et al 2019 in contrast the usual watershed tools requires executing only 4 arcgis toolboxes does not necessitate exporting and re importing data to arcgis i e does not involve any steps that require software or scripts external to arcgis does not require any manual adjustments to ensure proper feature delineation and for a moderate sized watershed like the logan river was completed with less than 15 min of effort usual represents a simple and streamlined approach for delineating watersheds thus providing researchers more time to focus on analysis and evaluation of their results rather than time intensive geospatial pre processing 5 concluding remarks we have introduced the usual watershed tools a new python based esri toolkit for delineating hydro geomorphic features including watersheds rivers interfluves and sub catchments along river networks usual extracts relevant attributes such as upstream drainage area reach length average channel slope and the downstream connectivity of links additionally usual can develop and apply regressions between shapefile attributes within the arcgis environment and automate the measurement of a reach averaged river or valley bottom widths furthermore the usual watershed tools serve as a geospatial pre processor for network based models as the outputs can be directly fed into and used to run 1 d routing models e g pfeiffer et al 2020 usual delineates and generates commonly needed hydro geomorphic geospatial features that typically require time intensive arcgis methods reducing geospatial processing times by using usual will 1 provide users the potential to drastically scale up watershed based analyses and modeling to evaluate a significantly greater number of landscapes in less time and or 2 allow users to focus their efforts more on research e g model development sensitivity testing and analysis than landscape delineation furthermore usual allows users to easily extract evaluate and or model environmental metrics at appropriate hydro geomorphic scales and process domains by delineating landscapes into watersheds river networks sub catchments and interfluves finally usual is capable of delineating features across a diverse range of watersheds with variable drainage area topographic complexity and relief as well as with topographic data of varying spatial resolution software availability name of software utah state university applied usual watershed tools contact information scott david usu edu year first available 2022 program language python software requirements arcgis pro 2 9 and higher license gnu gpl 3 0 availability https github com watershedswildfireresearchcollaborative usual declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments development of this toolkit was supported by funding from nsf division of earth sciences nsf ear 1848667 to bpm and pb nsf ear 1848672 to jac joint fire science program award id 19 2 02 6 to bpm and pb the utah agricultural experiment station pb approved as paper 9620 the virginia agricultural experiment station jac and the usda hatch program 1017457 to jac appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105576 
25484,watershed and hydro geomorphic delineation is a critical first step in most environmental and natural resource assessments analyses and research while existing geospatial tools have provided exceptional advances less attention has been put toward developing fully automated tools for subdividing landscapes into their constituent hydro geomorphic units or discretizing river corridor features here we present a new open source arcgis toolbox called the utah state university applied usual watershed tools the usual tools are a set of streamlined easy to use arcgis toolboxes that automate the delineation of watersheds sub catchments river adjacent interfluves and discretized river networks with the topological structure and feature attributes necessary for one dimensional source to sink transport modeling through large watersheds this novel geospatial toolset replaces the need for extensive delineation workflows providing the earth science environmental science and natural resources communities with the ability to rapidly and easily automate necessary but often time intensive tasks in a format familiar to arcgis users keywords river watershed delineation 1 d modeling arcgis pro data availability availability https github com watershedswildfireresearchcollaborative usual 1 introduction watershed and hydro geomorphic feature delineation is a fundamental step for geospatial research and analysis of natural resources across a variety of disciplines delineation of hydro geomorphic landscapes are often performed using geospatial software to compartmentalize landscapes into functional units which allows environmental parameters to be binned and modeled across discrete process domains breaking landscapes into functional hydro geomorphic units can provide critical insights into natural resource conditions and processes with respect to biology and ecology hydro geomorphic delineation can help inform the characterization of physical habitat e g belletti et al 2017 models evaluating the effects of physical habitat disturbance e g murphy et al 2020 and the surveys and management of species e g wang et al 2012 additionally hydro geomorphic delineation can assist in natural resource planning and impact assessments e g timber harvest fisher et al 2021 especially when the dispersal and distribution of environmental effects are linked to surface hydrologic pathways and source to sink transport of matter including but not limited to sediment nutrients and pollutants e g ahammad et al 2021 launay et al 2015 saleh et al 2013 finally the hydro geomorphic delineation of landscapes allows for more accurate modeling of the variable erosional processes that occur across the different geomorphic units within watersheds e g cavalli et al 2013 gannon et al 2019 gartner et al 2014 murphy et al 2019 staley et al 2017 wall et al 2022 rivers and waterways control the surface transport of matter through landscapes from a point source to a downstream sink whether that sink is local e g reservoir or lake or global oceans tracking matter through a landscape often employs one dimensional 1 d routing models of water e g david et al 2011 saleh et al 2013 sediment e g ahammad et al 2021 czuba et al 2017 murphy et al 2019 viparelli et al 2013 nutrients or pollutants e g launay et al 2015 or any other material that can be entrained in water and moved through a river network however these methods rely on computing spatial metrics over the two dimensional area of a watershed this typically requires extensive and often manual geospatial analysis or the application of multiple tools developed specifically for morphometric characterization of rivers and hydrologic modeling clubb et al 2014 passalacqua et al 2010 2012 schwanghart and scherler 2014 schwenk et al 2020 tarboton 1997 to extract the parameters from a watershed necessary to initialize the routing models e g czuba et al 2017 for instance the methods required for murphy et al 2019 to delineate and attribute a river network and hundreds of sub catchments for analysis of post wildfire erosion and sediment dynamics involved more than 50 arcgis tools interspersed with many manual steps as well as passing data back and forth between external development environments to execute scripts and perform functions not available in arcgis hence there is a need for a streamlined set of gis tools designed specifically for hydro geomorphic watershed delineation and attribution that serves the needs of natural resource management environmental science and source to sink modeling several geospatial tools currently exist to delineate watersheds river networks sub watersheds and to compute spatial morphometrics for instance topotoolbox delineates river networks watersheds and sub watersheds schwanghart and scherler 2014 additionally tak is a toolkit built on top of topotoolbox which streamlines functions and removes the need for a matlab license forte and whipple 2019 however topotoolbox and tak were designed to compute topographic metrics for advanced longitudinal river profile analysis channel steepness chi plots etc another commonly used tool lsdtopotools clubb et al 2014 is a linux based toolkit which primarily focuses on channel extraction and attribution lsdtopotools also contains functionality to delineate fluvial features such as floodplains and terraces clubb et al 2017 lsdtopotools also incorporates components of geonet sangireddy et al 2016 which uses advanced filtering techniques and attributes the network for high resolution topographic data i e lidar another tool rivgraph schwenk et al 2020 schwenk and hariharan 2021 automates extraction of river networks and attributes e g flow direction reach length and width branching angle etc from binary images of river networks without the need for an underlying digital elevation model dem collectively these tools have advanced our ability to conduct higher order fluvial and landscape analysis however all of these tools require knowledge of computational languages in potentially unfamiliar computing environments moreover these tools are largely focused on delineating and extracting morphometrics specific to the river network alone in contrast taudem tarboton 1997 and swat https swat tamu edu software qsswat and arcswat are examples of tools that were built with graphical user interfaces gui in common gis platforms arcgis and qgis to provide methods for delineating rivers watersheds and sub watersheds both taudem and swat were designed and developed to delineate watersheds for setting up common hydrologic routing schemes e g muskingum method accordingly it is important to recognize that they subdivide watersheds into hydrologic units which both conceptually and functionally differ from hydro geomorphic process domains specifically in their sub watershed delineation neither taudem nor swat distinguish between interfluvial process domains where hillslope runoff and erosion processes dominate and tributary sub catchments where streamflow and fluvial processes dominate instead taudem and swat subdivide watersheds into sub watersheds which define all areas that directly contribute surface flow to each discretized reach of the delineated river network fig 1 a as a result this approach aggregates all tributary catchments and hillslope process domains along each river network reach into a single polygonal area however these different process domains contribute flow sediment and other materials to each reach by distinctly different processes and magnitudes aggregating process domains may work for hydrologic models but these two tools are unsuitable for discretizing landscapes for many process based hydro geomorphic models for example the u s geological survey model for predicting debris flows within burned watersheds staley et al 2017 is applied at the scale of first and second order tributary sub catchments the debris flow modeling domain does not include network adjacent interfluves or reaches of the major river network and the inclusion of these areas through the use of the sub watershed extents produced by either taudem or swat would mischaracterize extracted model predictors and skew model predictions to date existing delineation tools have focused primarily on hydrologic modeling river channel extraction and topographic morphometrics however less attention has been given to delineation of watersheds at the sub catchment and interfluve scale fig 1b which is critical for accurately computing localized geospatial metrics and modeling runoff and erosion within hydro geomorphic process domains specifically for source to sink models therefore we introduce the utah state university applied usual watershed tools or for brevity usual the usual watershed tools are an open source python based toolkit for arcgis that have been designed to delineate watersheds sub catchments interfluves and river networks discretize river networks and extract attributes necessary for fluvial network routing and provide new gis functions for feature extraction and attribution usual has an arcgis graphical user interface gui to make the toolkit easy to use and familiar for users with experience in arcgis software however we have also made the underlying python scripts available for advanced users who wish to adapt and modify the tools novel attributes of the usual watershed tools include 1 delineating both interfluves and sub catchments fig 1b 2 providing the ability to exclude user defined regions of the topography that would traditionally yield erroneous delineations e g delineating catchments and interfluves across waterbodies 3 automating the identification and generation of pour points required to delineate sub catchments and interfluves 4 automating the discretization and attribution of a river network for 1 d routing 5 automating the computation of reach averaged widths from fluvial features e g river floodplain valley bottom hence the usual watershed tools automate many frequently required and labor intensive steps in watershed delineation and generates outputs that seamlessly integrate with state of the art 1 d routing models e g czuba 2018 pfeiffer et al 2020 tangi et al 2019 herein we introduce the components of the usual watershed tools describe how each works detail all required and optional inputs and describe the final gis products output from each tool section 2 we then explore and demonstrate how usual handles 1 variable dem resolution 2 shallow vs steep gradient landscapes 3 watersheds of varying channel complexity and 4 the computational efficiency of feature delineation section 3 finally we provide an example application of the usual watershed tools in the logan river utah to demonstrate its utility for evaluating common models of soil erosion and delivery as well as its capability to inform sediment routing models section 4 2 toolkit description the usual watershed tools are a set of python based scripts built on the esri arcgis platform developed using arcgis pro version 2 9 and available for download on github see software and data availability for link the toolkit requires a basic arcgis pro license as well as spatial analyst and 3 d analyst licenses usual provides users with the familiar easy to use arcgis toolbox guis and is constructed as a suite of toolboxes that can be executed independently or in sequence fig 2 however the underlying python code can also be opened and executed in arcgis or externally using any python integrated development environment ide e g spyder jupyter notebook usual is comprised of four main tools used for extracting delineating and attributing spatial metrics across different morphologic features in watersheds fig 2 additionally usual contains two optional sub toolkits each containing two tools to extract additional attributes fig 2 broadly the toolkit will delineate a watershed a river network all sub catchments draining to the network the interfluves between the sub catchments and adjacent to the river network fig 3 and compute and attribute spatial metrics for the delineated features running the primary workflow for the usual watershed tools requires a minimum of just two data inputs a dem raster and a pour point shapefile identifying the downstream most point for the watershed however in the following subsections we will outline and describe all the required and optional inputs and parameters functionality and outputs from each tool 2 1 watershed and river delineation the first tool in the usual workflow the watershed and river delineation tool delineates the watershed extent and river network upstream of a user specified point in a landscape pour point based on topography and surface hydrologic pathways this tool is essentially a wrapper script that streamlines the standard order of operations within the esri hydrologic toolbox for basic watershed delineation and river network delineation but with some critical additional functionality to handle more complex but commonly encountered watershed scenarios the watershed and river delineation tool requires just two inputs table s1 a watershed pour point specified as a point feature and a topographic raster representing the dem additionally the user must specify a river network drainage area threshold value in hydro geomorphology this typically defines the minimum drainage area marking the transition point between hillslopes and river channels in the landscape montgomery and foufoula georgiou 1993 tarboton et al 1991 however as the usual watershed tools can be used to generate river networks for 1 d routing this threshold can also be viewed more simply as defining the extent of the network intended for modeling fluvial material transport although the lower order tributaries may be important for some models users may not want to include the full network in a large scale watershed routing analysis in the usual framework this threshold also inherently defines the maximum drainage area of tributary channels that are delineated as sub catchments see section 2 2 1 the tool also accepts three additional optional inputs table s1 an area of interest polygon feature a pour point snapping tolerance numeric input and an option to conduct a nested watershed delineation the area of interest aoi polygon clips the input dem to the extent of the aoi before running the esri hydrology tools which helps improve computational efficiency if the input dem is not pre clipped and significantly larger than the watershed being delineated the pour point snapping tolerance defines a radius in map units that feeds into the esri snap pour point tool and moves the pour point location to the highest flow accumulation cell within the search radius to ensure the point falls within the river channel the nested analysis option allows users to define and input multiple pour points for a watershed fig 4 a and allows the tool to delineate multiple nested watersheds inside the primary watershed fig 4b first the tool delineates the entire watershed extent based on the prescribed pour point using esri hydrology toolset it fills small hydrologic depressions sinks in the dem computes flow direction d8 algorithm and flow accumulation across the dem and finally delineates the extent of the entire watershed as a polygon feature next the tool delineates the river channel network by reclassifying the flow accumulation raster based on the prescribed river drainage area threshold and applies the esri stream to feature tool to convert the reclassified binary raster into a stream network polyline in many watersheds there may be reservoirs upstream and within the larger watershed extent the presence of these nested reservoirs can influence network routing modeling and analysis i e they represent significant sinks along flow paths as well as the assessment and modeling of dispersal for aquatic populations the watershed and river network delineation tool could be run individually for each of these nested watersheds however rectifying the entire watershed delineation would require subsequent manual merging of the multiple watersheds and networks while this provides a relatively simple solution to produce a visually satisfactory gis product the simple merging of features would not result in the creation of a single and functional routing network with consistent and linked attributes that would allow for modeling material transport or migratory behavior through the entire watershed to address the common issue of nested watersheds we incorporated the option to run a nested analysis this function does not automatically identify the presence of nested watersheds or generate pour points for any potential nested reservoirs rather the user must input multiple pour points to the tool the tool first delineates the larger scale watershed that encompasses all the reservoirs upstream from the downstream most pour point and then delineates the nested watersheds based on the natural drainage divides and clips them by the delineated watersheds of upstream reservoirs fig 4 although delineating the nested watersheds as separate features the tool still generates a single continuous river network for the entire watershed extent that maintains accurate drainage area attributes for each reach i e the drainage area calculated immediately below a nested reservoir is not inaccurately identified as a drainage boundary with no contributing upstream area the tool also outputs sub networks for each nested watershed in case the user has a need for these features outputs from the watershed and river delineation tool include table s1 1 a watershed polygon delineating the extent of the watershed 2 a river network polyline mapping out the extent of the river network 3 rasters for the filled dem flow accumulation flow direction and river channel clipped to the watershed extent if a nested analysis is performed the tool will output all the above features for the full watershed as well as for each of the nested watersheds 2 2 sub catchment and interfluve delineation the sub catchment and interfluve delineation tool comes second in the primary workflow and subdivides and delineates the watershed into two feature groups 1 sub catchments that define the drainage areas for all tributaries that flow directly to the delineated river network and 2 the interfluves between sub catchments that are directly adjacent to and along the river network figs 3b 5 although interfluves are smaller in area relative to the sub catchments within the watershed and often not included in other delineation tools the interfluves are directly stream adjacent and comprise a significant percentage of the river network length accordingly interfluves can be a significant source of direct material inputs to the river network in some watersheds fisher et al 2021 kelly and belmont 2018 vaughan et al 2017 furthermore compartmentalizing the landscape into discrete components creates a geospatial framework where every geomorphic process domain across the landscape that may serve as a point source input can be evaluated or modeled as a potential source and associated with spatially explicit locations of delivery to the river network the sub catchment and interfluve delineation tool requires five input datasets all of which are generated by and output from the watershed and river delineation tool table s2 1 filled dem 2 flow direction raster 3 flow accumulation raster 4 river network shapefile and 5 watershed polygon additionally the user must prescribe a sub catchment drainage area threshold which defines the minimum drainage area for delineated sub catchments the upper limit for sub catchment drainage area is set by the river network drainage area threshold value for example if the river network drainage area threshold value was set to 5 km2 and the sub catchment drainage area threshold value was set to 1 km2 then the drainage area of every sub catchment delineated in the watershed would be between 1 and 5 km2 it is worth noting that while this tool can conveniently be run using the outputs from the previous watershed and river delineation tool the outputs from other tools e g a geonet or rivgraph derived network could also be used as input data the tool also contains a novel function for handling exclusion areas within the watershed these are regions of topography that would be included in standard esri delineations and other delineation tools but which result in the creation of erroneous sub catchment and interfluve extents that are not a reflection of reality and could result in the mischaracterization of extracted properties from those domains e g zonal statistics for example a standard delineation of sub catchments draining directly into a waterbody e g lake or reservoir extend out across the surface of the waterbody fig 5a thus including large areas that are not representative of the actual sub catchment topography fig 5b typically correcting this issue requires selecting or adjusting pour point locations for each of these sub catchments by hand our exclusion area function eliminates this tedious and time intensive step by automatically identifying and generating sub catchment pour points at the perimeter of the exclusion area before feature delineation additionally the tool will correctly delineate the interfluves with downslope boundaries that terminate at the perimeter of the exclusion area fig 5a and b to utilize this function the sub catchment and interfluve delineation tool includes an optional input of a polygon shapefile that encompasses the extent of exclusion area e g reservoir urban area valley bottom the input feature can contain a single polygon or multiple polygons additionally the tool contains an optional input allowing the user to define a number of cells to buffer the input polygons which helps correct flow paths between interfluves and sub catchments converging at the edge of the exclusion area however buffering an irregularly shaped polygon can create holes in the buffered exclusion area polygon fig s1 to overcome this the toolkit also contains an option to fill any holes in an exclusion area polygon fig s1 the sub catchment and interfluve delineation tool contains up to three options for outputs table s2 1 sub catchments 2 interfluves at fine resolution and 3 interfluves at a coarse resolution by default the toolbox outputs all three however the arcgis toolbox contains tick boxes booleans in the code which allow users to select or suppress outputs for any of the three features in the following two subsections we outline how each is delineated and their respective outputs 2 2 1 sub catchment delineation sub catchment pour points are internally identified by first creating a binary raster where flow accumulation cells equal to or above the sub catchment drainage area threshold value are given a value of one and cells below the threshold are given a value of zero second if an area to exclude polygon is provided the portions of the input binary network raster one for river cells zero otherwise inside this area are converted to a value of one raster calculator is then used to subtract the binary network raster with exclusion area from the binary flow accumulation raster above the sub catchment drainage area threshold value the result is a binary raster all negative values are set to zero indicating which cells flow directly into but are not in the river network those raster cells are then converted to pour points directly adjacent to the river channel and exclusion area and input to the esri watershed tool to delineate the contributing area of each sub catchment fig 5a and b the tool outputs a polygon shapefile with all sub catchment polygons and a shapefile of the pour points used to extract each sub catchment each sub catchment and its associated pour point are assigned a unique identifier which is written to their attribute tables so they can be linked for source to sink modeling 2 2 2 interfluve delineation interfluve delineation can be performed in two ways at a coarse fig 5b and fine fig 5c resolution interfluve pour points are identified by locating all flow accumulation cells adjacent to the river network as well as around the perimeter of any optional exclusion areas that have contributing areas greater than zero but less than the defined sub catchment drainage area threshold value these pour points are then used in the esri watershed tool to delineate the contributing area of every interfluve flow path fig 5c this fine scale resolution of interfluves may be beneficial for users modeling and tracking non channelized flow paths of potential point source inputs to the river network e g soil erosion or nutrient runoff from agricultural fields however if a user were interested in coarser scale delineation of interfluves for morphometric calculations the tool can also aggregate and merge all of the fine scale interfluves between the sub catchments to create coarse resolution interfluves fig 5b for this latter case the tool does not output pour points because the coarse resolution interfluves are intended for broader scale environmental parameter characterization rather than source to sink modeling additionally the tool provides an optional input to delete all coarse interfluves containing less than a user defined number of cells thus filtering out all small coarse resolution interfluves 2 3 network discretization and attribution the network discretization and attribution tool discretizes the river network polyline into discrete reaches based on a user defined maximum length and consideration of geometric constraints from tributary junctions in the river network fig 6 additionally it computes and attributes key fluvial routing parameters to each associated reach including upstream drainage area reach length and reach average slope fig 6 discretizing river networks is a necessary step for calculating reach averaged morphometrics e g channel width valley bottom width river slope creating topologic networks through which numerical models can route flow and other materials and characterizing the conditions of discrete patches of aquatic habitat inputs for this tool include table s3 1 a polyline feature of the river network 2 the flow accumulation raster and 3 a filled dem additionally the user defines a maximum discretization length of the network reaches in map units the tool initially splits the sections of river network between each of the network confluences into segments at the user defined maximum length starting from the upstream extent and moving downstream however in this first pass discretized reaches that are considerably shorter than the user defined length are typically generated as it is rare that a length of river between confluences is perfectly divisible by the defined discretization length these short reaches sometimes just a few meters in length are often not long enough to reliably characterize reach averaged morphometrics or habitat conditions they can also be problematic as inputs to network routing models e g if modeled transport lengths exceed reach lengths over the scale of one time step it can lead to numerical instabilities to address this issue usual identifies all reaches less than one half of the maximum length and merges each with the next upstream reach the tool then splits the newly merged reach at the midpoint to avoid creating reaches longer than the user defined maximum for attribution of reach parameters fig 6 one required input by the network discretization and attribution tool is a minimum river reach slope the tool extracts the upstream and downstream elevations m of each reach as well as the reach length m and then computes the average slope m m 1 along each reach as the difference between the maximum and minimum elevations divided by the length derived from a filled dem it is possible the river network may include reaches with zero slope or the network may pass through extremely flat regions such as lakes or reservoirs these extremely low or zero slope reaches can cause numerical instabilities in 1 d network routing models therefore the defined minimum threshold value is used to replace any reach slope values less than this value if a user does not wish to replace derived slopes they can set the minimum value to less than or equal to 0 the tool outputs table s3 a new updated river network shapefile that is discretized and includes an attribute table containing a unique reach id reach length upstream and downstream elevations reach average slope and a field identifying the id of the next reach downstream fig 6 which is critical information for network routing 2 4 network routing preparation the last tool in the main workflow of the usual watershed tools fig 1 is the network routing preparation tool the tool ensures all pour points and the river network contain the necessary attributes to be applied for 1 d network routing the tool has two functions 1 snapping all pour points sub catchments and interfluves to the river network and 2 flagging any network reaches that intersect polygonal areas of interest e g water bodies urban areas vegetation types or land cover snapping the pour points for all of the interfluves and sub catchments to the river network both functionally and spatially links the potential flow and material input locations throughout the watershed to points along the river network the pour points produced from the sub catchment and interfluve delineation tool are all generated at flow accumulation cells immediately upstream or upslope but not directly on or intersecting the river network or perimeter of exclusion areas this is required for the proper delineation of these features however to use these locations as spatially explicit point source nodes of material delivery to the river network it is necessary the pour points intersect the river polyline and can be functionally linked to the appropriate reach or waterbody the tool uses the esri snap function to move the points onto the river network and then writes all of the attributes of the associated river reach to their respective snapped pour points the second function to flag river reaches provides the user the option to identify reaches of the river network that may be of interest for subsequent applications or analysis one example would be a river network delineated upstream of a dam that includes reaches delineated across the reservoir surface e g fig 4 for 1 d routing applications these reaches would not be appropriate locations to apply fluvial transport equations and thus valuable to identify as unique from other reaches other examples could include characterizing which reaches flow through urban areas or through the extent of a wildfire regardless of the intended application the tool uses a binary notation to identify which reaches in the river network intersect a user specified polygon of interest 1 and which do not 0 fig 6c the tool outputs table s4 new shapefiles of snapped pour points for both interfluves and sub catchments and leaves the original unsnapped pour point shapefiles unmodified additionally all network attributes including those added to the network in this tool i e flagging are appended to the attribute table of each newly snapped pour point shapefile 2 5 data regression tools the usual watershed tools contain an additional sub toolkit attribute regression tools which houses two tools for data regression within arcgis based on fields within a shapefile s attribute table the first tool scaling relationship generator tool computes a univariate regression between two variables and the second tool scaling relationship extrapolator tool applies the regression writing predicted values to a new field in the attribute table typically this type of data analysis would require exporting the attribute table data to conduct regressions in external software and then reading the data back into arcgis to continue any geospatial analysis an example application for this tool would be using a discretized river network that contains upstream drainage areas e g output from the river network discretization and attribution tool augmented with a field of river widths manually measured at select locations in the watershed to develop a drainage area river width scaling relationship using the scaling relationship generator tool the scaling relationship extrapolator tool applies the regression across the entire network attributing all links with river width based on an upstream drainage area the scaling relationship generator tool uses any attributed shapefile as an input i e this does not need to be a shapefile created by usual the user then specifies the attribute field table s5 containing the dependent dataset and the field containing the independent dataset and specifies one of three types of regressions to be applied linear exponential or power if the appropriate regression type to be applied is unknown the user should initially plot the data using the esri chart tools to visually determine the most appropriate trend to be applied before using the tool the scaling relationship generator tool regresses the data and appends the coefficient values to the shapefile attribute table with fields coefficient a and b defining the two regression coefficients if the tool is run multiple times on the same input dataset the tool will overwrite the coefficients previously written to the shapefile finally the tool includes an option to output a scatter plot to a user defined file directory that displays the input data regression line r2 value and the regression equation the second tool is the scaling relationship extrapolator tool this tool allows a user to apply one of the three regression types to generate a new attribute field of predicted values within a shapefile based on identified fields for a dependent variable and coefficients returning to the above drainage area river width relationship a regression could be applied to the upstream drainage area field of the discretized network to estimate river widths for every reach fig 6c and example application shown in section 4 1 the tool requires table s6 an input shapefile and requires the user define an output field name as well as identify the input attribute fieldnames for the dependent variable coefficients a b and the regression type associated with the coefficients exponential power law or linear the tool then applies the fit and writes the predicted values to the output field 2 6 fluvial reach average width tools the usual watershed tools contain a sub toolkit called the fluvial reach average width tools to measure reach average widths along an elongate polygon e g river channel or valley bottom this functionality addresses a common need in characterizing reach scale morphometrics as well as the generation of initial conditions often needed in 1 d network routing the first tool in the set the fluvial polygon transects generates a high density of nodes and transects that connect across the opposite sides of the elongate polygon feature the tool requires table s7 an input polygon a user defined transect spacing in map units and a polygon centerline fig 7 this centerline does not necessarily have be exactly centered through the polygon but must intersect the polygon at the tips and only at the tips while a river network could work generating a centerline manually or using the polygon to centerline tool requires the esri production mapping extension ensures the outlined criteria the elongate polygon e g river floodplain valley bottom needs to be generated externally from usual either by manual delineation or using existing tools e g clubb et al 2017 gilbert et al 2016 roux et al 2015 schwenk and hariharan 2021 alternatively centerlines can be generated using non esri tools and methods e g dilts 2015 lewandowicz and flisek 2020 roux et al 2015 schwenk and hariharan 2021 regardless of the approach one critical condition of any centerline used for these tools in usual is that it must extend to the polygon perimeter at all tips of the elongate polygon fig 6a c this is required because the tool relies on this intersection point of the line and polygon to identify and split the elongate polygon into left and right segments starting at the intersection point of the polygon and the centerline the tool generates points along the edges of the two halves of the polygon at the defined transect spacing fig 7b d a kd tree is applied to each point to identify the closest point on the opposite side of the polygon and to generate point pairs the paired points are then converted into lines producing a series of transects spanning the width of the elongate polygon fig 7b d there are two additional optional inputs 1 a search radius applied around the points where the centerlines intersect the polygon to exclude points from transect generation figs 7b and 2 a polygon feature to allow users to prescribe specific areas to exclude from transect generation fig 7c these two options are intended for the exclusion of perimeter points at the upstream and downstream most tips of the polygon fig 7 b c where generating transects across flat edges corners or tapered edges may result in unrepresentative polygon width calculations regardless of whether the optional exclusion functions are used the tool outputs table s7 a polyline shapefile containing transects that extend across the polygon with lengths calculated and attributed to each line feature fig 7b d the second tool is fluvial reach averaged width which computes and attributes average widths for a discretized line feature within the elongate polygon the intended application is to compute average widths of river channels or valley bottoms along each reach of the discretized river network but there could be other potential applications the tool densifies transect vertices converts them to points and assigns the transect length i e local polygon width to each these attributed points are used to generate a tin which is then converted to a raster of the user defined resolution with values representing the local polygon width fig 7e finally the esri zonal statistics tool is applied to calculate the average width value from the raster along each reach of the discretized polyline and write this value to the new output field in the attribute table table s8 2 7 model structure and data management overall the usual watershed tools require a minimum of just two input datasets a watershed pour point and a dem to delineate a watershed boundary generate a discretized and attributed river network delineate all sub catchment and interfluve extents and identify their associated point source input locations along the river network fig 2 moreover each tool in usual can be run independently using inputs created by other toolkits the additional tools provided in usual can also be applied to a variety of geospatial applications beyond watershed and network delineation key outputs from usual fig 2 are written to a user defined folder additionally the output folder will include a temp subfolder to house all intermediate outputs within which each tool creates its own subfolder of organized outputs the inclusion of this intermediate data is intended to help users troubleshoot problems and this folder structure makes it easy to delete all intermediate files if the final results are satisfactory if applying a nested analysis the output folder contains an additional folder with outputs for each nested watershed that is based on an optional user naming structure 3 evaluation testing and limitations in this section we demonstrate the capabilities of the usual watershed tools we test the toolkit in watersheds with varying dem resolutions topographic relief and river channel complexity and evaluate the computational efficiency of usual additionally we discuss potential limitations and considerations when running usual for different scenarios 3 1 dem resolution to demonstrate the applicability of the usual tools across sites with variable resolution of available dem datasets we explored three sites 1 the highest resolution site was the big walnut creek 125 km2 watershed near north salem indiana usa fig 8 a with a 1 5 m lidar derived dem https maps indiana edu 2 the intermediate resolution site was dells creek 41 km2 watershed upstream of little dell reservoir ut usa fig 8b with a 10 m usgs dem https apps nationalmap gov and 3 the coarsest resolution site was the fraser river between lytton bc canada and the southern end of edge hills provincial park bc fig 8b 5852 km2 watershed with a 30 m dem from trim https catalogue data gov bc ca dataset to delineate big walnut creek we used a river network threshold of 0 2 km2 and a sub catchment threshold of 0 05 km2 the dell creek delineation used a river network threshold of 5 km2 and a sub catchment threshold of 0 1 km2 for the fraser river fig 8c we used a river network threshold of 5 km2 and sub catchment threshold of 0 1 km2 additionally along the fraser river we input a pour point that was not at the mouth of the river and a dem that intentionally did not cover the entire extent of the watershed which covers 25 of british columbia rennie et al 2018 to demonstrate the applicability and limitations of usual when applied to large rivers with input dems that do not cover the entire watershed outputs from our delineations fig 8 reveal that the usual watershed tools can be applied to delineate features from a variety of data sources and across a wide range of dem resolutions furthermore our delineation of the fraser river dem that did not cover the entire watershed extent fig 8c reveals that the landscape delineations are still successful the interfluves and sub catchments are still correctly delineated because their pour points are based on drainage areas adjacent to the river channel however the upstream most reach of the fraser river was not delineated fig 8c due to significantly lower than accurate flow accumulation values throughout the fraser river reaches caused by the truncated extent of the dem hence if inputting a dem to usual that does not span the full watershed a user should ensure that the dem at least extends well beyond the upstream most river reach of interest additionally if applying the river network discretization and attribution tool users should beware that the drainage areas attributed to truncated watersheds and segments of larger rivers will be smaller than the real upstream drainage areas 3 2 topographic relief and channel complexity to demonstrate the capability of the usual watershed tools for delineating features in landscapes of varied topographic complexity we compared subsets of the fraser river and big walnut creek which reasonably represent end members of complexity the fraser river is a large river traversing a series of canyons rennie et al 2018 containing high topographic relief fig 9 a and a complex network of tributaries fig 8c big walnut creek is located in the central till plain of indiana gray 2000 which contains minimal topographic relief fig 9c in our aoi big walnut creek s downstream portion is a meandering river with well developed riparian vegetation whereas the headwaters are mostly small agricultural ditches representing a variety of anthropogenic influences e g channel straightening and constructed levees in the channel network fig 9c our results show that the usual watershed tools are capable of delineating features in both steep mountainous terrain fig 8c fig 9 a b and relatively flat landscapes figs 8a and 9 c d this is because the terrain processing tools operate on elevation differences between neighboring cells therefore the usual watershed tools work well on any dem with sufficiently high resolution that captures the topographic variability in detail additionally our analysis illustrates the utility of usual in landscapes with natural fig 9a and anthropogenic complexity fig 9c however one limitation of usual in low gradient landscapes is that advanced topographic filters passalacqua et al 2012 sangireddy et al 2016 are not included in the river network delineation which can help overcome problems introduced by bridges and other human engineered features hence in order to use usual in these settings a user must first pre condition the dem for delineation by manually lowering the elevations of these features to match the river network an additional limitation we note is that network delineation in usual was developed to handle single threaded channels and may not work as expected in braided or anabranching river systems 3 3 computational efficiency to evaluate computational efficiency we performed a series of repeated delineations on toroda creek washington usa with varying dem resolutions toroda creek is located in mountainous north central washington state with a watershed area of 175 km2 fig 10 a the input dem was 1 m resolution and derived from lidar https lidarportal dnr wa gov the dem was then resampled to resolutions ranging from 1 to 10 m at 1 m increments and from 10 to 100 m at 10 m increments giving a total of 19 different dem resolutions fig 9b the watershed and river delineation tool and sub catchment and interfluve delineation tool were run and timed for each dem resolution to account for any potential computational variability we ran each tool 10 times at each resolution and present the average computation time for our simulations we used an aoi that extended just beyond the drainage divides fig 10a a river network threshold of 5 km2 fig 10a and a sub catchment threshold value of 0 1 km2 simulations were performed on a consumer grade laptop containing i7 7700hq quad core 2 8 ghz 24 gb ddr3 ram and a nvidia gtx 1050ti graphics card each tool was called and parameterized using spyder ide and timed using the perf counter in the time module results from this watershed indicate that dem resolutions of 10 m here 1 8 106 cells or less ran both tools in under 1 min and dem resolutions less than 4 m here 11 3 106 cells ran in less than 5 min for the highest resolution 1 m lidar dem running both tools took 30 min here 175 5 106 cells fig 10b for a full break down see tables s9 and s10 supplementary information 4 example application in this section we provide an example application of the usual watershed tools to delineate watershed features discretize and attribute a river network and apply the outputs to a 1 d network routing model specifically we applied usual to the logan river watershed upstream 555 km2 watershed of the first dam reservoir in logan utah usa and use this site to demonstrate the utility of usual for an application of source to sink watershed modeling and analysis the usual delineated hydro geomorphic features were used as inputs to model spatially explicit soil erosion and delivery across the watershed initialize a 1 d sediment transport model and to evaluate the soil erosion model by comparing modeled and pre existing field based estimates of sedimentation rate at the downstream reservoir 4 1 usual application to the logan river we first applied the watershed and river delineation tool to the logan river fig 11 a for our delineation we used 10 m usgs dem specified an aoi that extended just beyond the drainage divides used a river network threshold of 5 km2 and specified a pour point at the outlet of first dam sub catchments and interfluves fig 11b were then generated with the sub catchment and interfluve delineation tool using a sub catchment threshold value of 0 1 km2 the river network discretization and attribution tool was applied using a maximum discretization length of 500 m and a minimum slope threshold of 0 01 m m to attribute the network with river widths we used the regression application tool and applied a power law relationship to scale drainage area to river width based on coefficients derived for the continental united states wilkerson et al 2014 1 b 1 41 a 0 462 where b is the bankfull width m and a is the drainage area km2 finally the network routing preparation tool was run to produce a river network ready for routing analysis to model spatially explicit sediment inputs to the logan river we used the revised universal soil loss equation rusle models of sediment delivery and the usual delineated features see supplementary information for more detailed methods applying rusle across the watershed we generated a 10 m raster output of annual erosion rate we then applied a sediment delivery ratio sdr equation based on flow path length to the river network to predict what fraction of soil eroded in each raster cell would be delivered to the river network gannon et al 2019 wagenbrenner and robichaud 2014 the sdr raster was then multiplied by the rusle raster to predict the volume of soil delivered to the river network each year from each raster cell the delineated interfluves and sub catchments from usual were then used to run the esri zonal statistics tool to sum the volume of sediment fig 11c eroded and delivered to the river network from each feature polygon using the unique ids linking polygons to pour points the modeled sediment input volumes were then written as attributes to the snapped pour points for each feature 4 2 application of 1 d sediment routing the delineated river network and snapped interfluve and sub catchment pour points from usual feed directly into 1 d network routing models such as the network sediment transporter nst pfeiffer et al 2020 the nst model uses a lagrangian framework to simulate the transport of sediment under space and time variable flow conditions on a river network czuba 2018 in the model framework each reach of a discretized river network serves as a link through which sediment is routed and the snapped and attributed pour points for sub catchments and interfluves generated by the usual watershed tools function as sediment input locations to the network the modeled sediment delivery volumes at each interfluve and sub catchment pour point inform the annual sediment loads supplied to the logan river network fig 11d sediment loads were divided into two grain size fractions sand and fines 2 mm based on soils data from statsgo reybold and teselle 1989 the fine fraction was treated as suspended load and was delivered to the reservoir based on a sediment rating curve the sand fraction was routed with nst using a mixed size sediment transport equation wilcock and crowe 2003 simulations were run for five water years using daily discharges fig 12 measured just upstream of first dam usgs gage 10109000 10 1 2016 9 30 2021 and scaled to each reach based on its upstream drainage area at the beginning of each month one twelfth of the predicted annual sediment delivery at each pour point was supplied to the river network and routed through the logan river 4 3 results and discussion our initial simulations indicated annual sedimentation in the reservoir was 2 5 orders of magnitude greater than the field measured volumes of 641 m3 yr 1 utah division of water resources udwr 2010 while sediment routing models have the potential to over or under estimate transport volumes the magnitude of disparity in these results likely reflects over predictions in soil erosion and delivery by rusle and or the sdr equations a possible source of error in the rusle calculations is the inherent limitations in the accuracy of geospatial data and uncertainty in factor parameterization kampf et al 2020 in particular the cover management factor has been shown to span up to 3 orders of magnitude larsen and macdonald 2007 further although rusle is commonly applied at landscape scales it has previously been shown to over predict erosion for sub catchments or watersheds larger than 0 01 km2 kampf et al 2020 additionally we evaluated a range of potential sdr equations for our analysis e g ebrahimzadeh et al 2018 and we chose the sdr model that resulted in the lowest magnitude of sediment delivery to the river network depite this effort we ultimately found that we had to impose a 95 watershed wide reduction to predicted sediment delivery volumes fig 11d in order to produce a model with annual reservoir sedimentation rates that approximated field estimates modeled sedimentation rate of 669 m3 yr 1 fig 12 this example underscores the utility of usual for broad applications in watershed analysis and more specifically an example of how usual can be used to evaluate and run watershed scale source to sink models additionally while it is more difficult to quantify and demonstrate here the use of usual for this application eliminated the time intensive steps typically required for delineating and extracting attributes for hydro geomorphic modeling the labor and computational time required to develop functional workflows to execute the 150 arcgis functions required for the hydro geomorphic delineation of just a single watershed see supplementary information for usual workflows and to code the interspersed non arcpy steps necessary to fully prepare a watershed for analysis and hydro geomorphic modeling can represent weeks of work for a single project e g murphy et al 2019 in contrast the usual watershed tools requires executing only 4 arcgis toolboxes does not necessitate exporting and re importing data to arcgis i e does not involve any steps that require software or scripts external to arcgis does not require any manual adjustments to ensure proper feature delineation and for a moderate sized watershed like the logan river was completed with less than 15 min of effort usual represents a simple and streamlined approach for delineating watersheds thus providing researchers more time to focus on analysis and evaluation of their results rather than time intensive geospatial pre processing 5 concluding remarks we have introduced the usual watershed tools a new python based esri toolkit for delineating hydro geomorphic features including watersheds rivers interfluves and sub catchments along river networks usual extracts relevant attributes such as upstream drainage area reach length average channel slope and the downstream connectivity of links additionally usual can develop and apply regressions between shapefile attributes within the arcgis environment and automate the measurement of a reach averaged river or valley bottom widths furthermore the usual watershed tools serve as a geospatial pre processor for network based models as the outputs can be directly fed into and used to run 1 d routing models e g pfeiffer et al 2020 usual delineates and generates commonly needed hydro geomorphic geospatial features that typically require time intensive arcgis methods reducing geospatial processing times by using usual will 1 provide users the potential to drastically scale up watershed based analyses and modeling to evaluate a significantly greater number of landscapes in less time and or 2 allow users to focus their efforts more on research e g model development sensitivity testing and analysis than landscape delineation furthermore usual allows users to easily extract evaluate and or model environmental metrics at appropriate hydro geomorphic scales and process domains by delineating landscapes into watersheds river networks sub catchments and interfluves finally usual is capable of delineating features across a diverse range of watersheds with variable drainage area topographic complexity and relief as well as with topographic data of varying spatial resolution software availability name of software utah state university applied usual watershed tools contact information scott david usu edu year first available 2022 program language python software requirements arcgis pro 2 9 and higher license gnu gpl 3 0 availability https github com watershedswildfireresearchcollaborative usual declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments development of this toolkit was supported by funding from nsf division of earth sciences nsf ear 1848667 to bpm and pb nsf ear 1848672 to jac joint fire science program award id 19 2 02 6 to bpm and pb the utah agricultural experiment station pb approved as paper 9620 the virginia agricultural experiment station jac and the usda hatch program 1017457 to jac appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105576 
