index,text
26175,a generic and scalable scheme is proposed for forecasting reservoir inflow to optimize reservoir operations for hydropower maximization short term weather forecasts and antecedent hydrological variables were inputs to a three layered hydrologically relevant artificial neural network ann to forecast inflow for 7 days of lead time application of the scheme was demonstrated over 23 dams in u s with varying hydrological characteristics and climate regimes probabilistic forecast was also explored by feeding ann with ensembles of weather forecast fields results suggest forecasting skill improves with decreasing coefficient of variation in inflow and increasing drainage area forecast informed operations were simulated using a rolling horizon scheme and assessed against benchmark control rules over two years of operations from pensacola dam oklahoma additional 47 253 mwh of energy could have been harvested without compromising flood risk with optimal operations this study reinforces the potential of a numerically efficient and skillful reservoir inflow forecasting scheme to address water energy security challenges keywords reservoir inflow forecasting artificial neural network numerical weather prediction baseflow separation hydropower maximization global scalability 1 introduction most of the world s artificial reservoirs are operated at daily or longer time scales based on rule curves that were designed using a climatology of historical flow observations and pre dam storage volumes lee et al 2009 ficchì et al 2015 yazicigil et al 1983 rule curves outline the reservoir storage targets that need to be met at specific times of the year operating strictly based on these climatology based rules can lead to mishandling of an unexpected peak reservoir inflow event miao et al 2016 for instance in a relatively dry flood season lowering the pool to rule curve level can result in significant loss in hydropower generation which could be avoided if weather forecasts were made ahead of time for efficient operations of a single multipurpose reservoir system the information about forecast inflow into the system is an important part of the real time decision making process short term daily to weekly scale forecasts are indispensable in maximizing societally important benefits of flood control and hydropower generation yazicigil et al 1983 anghileri et al 2016 flood mitigation requires sufficient flood storage before an impending flood while hydropower generation requires maximizing the economic value of water by releasing most of it through turbines and keeping reservoir at maximum pool reservoir inflow forecasts if reasonably skillful can help achieve a balance where hydropower can be maximized without compromising flood risk downstream jordan et al 2012 madsen et al 2009 qi et al 2017 often this is achieved by proactive storage or release from reservoir in anticipation of reservoir inflow recently a study on two dams in us showed application of numerical weather prediction model nwp based reservoir inflow forecasts for optimizing the reservoir operations ahmad et al 2018 significant hydropower benefits were demonstrated without compromising the flood control objective ongoing projects such as integrated forecast and reservoir management inform georgakakos et al 2007 and forecast informed reservoir operations firo overview 2017 focused over specific watersheds are also utilizing short term weather forecasts for operating the reservoirs the nwp models from various meteorological agencies produce weather scale forecast fields of temperatures wind precipitation soil moisture in three dimensions over the entire globe these publicly available forecasts represent an underutilized resource for the hydropower community due to concern over fossil fuel based environmental degradation dudhani et al 2006 li 2005 moreover efficient management of hydropower facilities is essential for emerging economies where demand for energy often exceeds supply or generation capacity asif and muneer 2007 as more hydropower dams are constructed especially in developing nations achieving the highest possible operational efficiency using available weather forecasts can address the growing resource needs of today the challenge today is to convert the global and public availability of weather forecasts to locally actionable data as reservoir inflow forecast and thus mainstream their use in the operational world however there are logistic hurdles to globalizing hydropower maximization techniques based on nwp based reservoir inflow forecasts even if the forecasts achieve certain level of accuracy adequate computational resources are required to capture sudden peak flow events considerably faster than real time this is because at a global scale the nwp models lack sufficient detail to force hydrological models over the reservoir catchments where precipitation can often exhibit higher spatial variability flint and flint 2012 do hoai et al 2011 dynamic downscaling techniques have subsequently emerged in recent past that are known to be locally constrained rendition of weather forecasts at hydrologically relevant scale wilby and wigley 1997 however dynamic downscaling is computationally intensive and not feasible for computationally constrained settings of most operational reservoir management agencies around the world for example a study on flood forecasting in houston tx found that it takes nearly 7 20 h for dynamically downscaling of nwp forecasts for 24 hr lead time depending on the configuration of cpu available in the current market sikder et al 2019 in summary there are two important hurdles for operationalization of reservoir inflow forecasts in real time reservoir optimization models around the world these are 1 uncertainty in inflow forecast zhao et al 2011 georgakakos et al 2008 faber and stedinger 2001 and 2 computational burden of deriving these reservoir inflow forecasts cheng et al 2014 for large scale applications of nwp forecast fields in optimizing reservoir operations for hydropower maximization an alternate and scalable numerical scheme is required such a scheme should have the following desired properties 1 it has to be numerically efficient and considerably faster than real time for practical operations 2 it has to allow rapid multi year historical assessment to understand risks of false positives and negatives in operations 3 it has to be effective in limited in situ hydrologic data settings for application in developing countries and 4 it has to be skillful and yield physically realistic forecasts for streamflow over the forecast horizon our selected scheme that we present here with such desired features is the data based technique of artificial neural networks ann tailored for reservoir inflow forecasting the ann technique over the last two decades has proven to be an efficient supplement to modeling qualitative and quantitative water resource variables and capturing the nonlinearity in flow maier et al 2010 jain et al 2009 maier and dandy 1996 hutton and kapelan 2015 wei 2016 govindaraju and rao 2000 wu et al 2014 as noted by asce task committee on application of anns in hydrology 2000 anns are robust tools for many of the complex hydrological modeling problems the review of relevant research on anns by maier et al 2010 suggest that a vast majority of studies have focused on flow prediction daily short term flow forecasting using ann has been explored amongst others by birikundavyi et al 2002 kişi 2005 wu et al 2009 coulibaly et al 2000 and zemzami et al 2016 as required in all modeling efforts following good practice that increases the credibility and impact of modeling results jakeman et al 2006 welsh 2008 is particularly important for ann models that are developed using available data and not explicitly based on underlying physical processes for the input nodes past conditions of streamflow and hydro meteorological variables are commonly used in predicting future flow studies also used other predictor combinations such as flow length and travel time akhtar et al 2009 previous day s rainfall and temperature lorrai and sechi 1995 flow rainfall and evapotranspiration anctil et al 2004 soil moisture deficits and runoff cheng and noguchi 1996 bartoletti et al 2018 eliminated the redundancy in antecedent rainfall using the data driven principal component analysis technique however to the best of our knowledge there is no work on the use of freely available global nwp weather forecasts as ann inputs several studies using only the antecedent conditions have indicated the inability of ann to capture the flow peaks e g campolo et al 1999 sudheer et al 2003 wu et al 2009 de vos and reintjes 2005 to provide an accurate estimate of peaks it is necessary to remove the local variations caused by extreme flows from the time series function being mapped streamflow transformation is used to simplify the data structure following a convenient statistical model sudheer et al 2003 as found by zemzami et al 2016 separation of streamflow into baseflow and runoff components improved the peak flow estimation and hence it was incorporated in the present research the runoff component was transformed by a moving average procedure to improve the peak flow prediction further studies on ann based reservoir inflow prediction have primarily focused over only a specific reservoir system in an effort to demonstrate global scalability application of any proposed inflow forecasting scheme extending over multiple dams with varying hydro climatic and geographic characteristics is necessary the innovative aspects of this study include development of ann model based on the coarser scale forecast fields of 1 7 days lead from nwp weather models to predict basin scale hydrology the technique was tested over an inventory of 23 dams with widely varying hydrological characteristics and local climate regimes a comprehensive validation framework was employed to assess the skillful and physical realism of the modeled forecasts the probabilistic nature of inflow forecasts was addressed by obtaining an ensemble of forecast flow derived from ensemble of weather forecast the forecasts were further incorporated in a reservoir optimization model for maximizing the daily hydropower generation while meeting other dam management constraints and regulations the specific research questions of this study are 1 can the use of ann with pertinent hydrologic knowledge be computationally efficient skillful and globally scalable in forecasting the reservoir inflow over short term forecast horizon 1 7 days 2 what role does the hydrological characteristics of the upstream basin and local climate zone play in driving the forecast skill of the designed ann model 3 can such fast inflow forecasts be used to optimize the reservoir operations to improve energy generation without compromising flood or dam safety the rest of the paper is organized as follows in the next section we discuss the selection of dam sites for application of the proposed technique followed by a description of datasets used this is then followed by a detailed methodology and different components involved in section 3 the case study results of ann based forecasts over the database of dams and its application for optimization of reservoir operations are presented in section 4 followed by discussion and concluding remarks in section 5 2 study sites and hydrometeorological data in order to address the first two research questions 23 dams located in various climate zones of the contiguous u s conus were selected all selected dams receive unregulated flow and are operated for a variety of purposes such as flood control hydropower generation water supply irrigation and recreation fifteen of these dams are powered the dam inventory also represents a wide range of upstream catchment area topography hydrological characteristics flow patterns reservoir storage and installed hydropower capacity the dams are shown in fig 1 by location and size of upstream drainage area apart from the study sites within us six dam sites in large river basins of ganges brahmaputra and mekong were identified table 1 presents the information about the selected study sites with descriptive statistics of the original flow data over 2007 2014 data for this study over the conus are as follows a deterministic and ensemble forecast hydro meteorological forcing fields b basin s antecedent conditions and c current reservoir state the deterministic forecast fields of precipitation temperature and windspeed were acquired from the global forecast system gfs global scale numerical weather prediction model at 0 5 resolution for 1 7 days lead time with a 3 hourly temporal resolution the archived data is available from oct 2006 due to which the period of analysis in this study was set to 2007 2017 and all the other datasets were gathered over this period for the ensemble forecast fields noaa s global ensemble forecasting system reforecast version 2 dataset gefs r hamill et al 2013 with 11 member ensemble of forecasts at 1 resolution was used the antecedent precipitation over the basin was obtained from the climate hazards group infrared precipitation with station data chirps gridded rainfall time series at 0 05 resolution funk et al 2015 antecedent temperature and windspeed from in situ global surface summary of the day gsod data global surface summary of the day 2018 and antecedent soil moisture from global land data assimilation systems gldas all the gridded datasets were processed to calculate the basin averaged values to be used as inputs to ann the antecedent observed reservoir inflow for us dams was obtained from the dam operating agencies of us army corps of engineers usace and us bureau of reclamation usbr state of current reservoir storage and headwater level was acquired from the respective agencies data portals the climate zones for each dam were extracted from köppen geiger climate classification peel et al 2007 for the real time forecasting and reservoir operation purpose a forecast horizon of 1 7 days ahead was chosen 3 methodology the experimental approach followed in the study is shown in fig 2 and described in the following sections 3 1 proposed ann architecture the ann architecture used here is the multilayer feedforward neural network this is one of the most popular architecture used for streamflow forecasting wu et al 2009 it involves input hidden and output layers where the hidden layer allows the network to perform complex nonlinear mapping between the input and output variables coulibaly et al 2000 funhashi et al 1989 a three layered ann with one hidden layer was implemented many experiments in the past have confirmed this to be adequate for most forecasting problems lippmann 1987 zhang et al 1998 coulibaly et al 2000 the network s ability to learn from training data and generalize depends on the number of nodes in input and hidden layers the sum of the weighted nodes of a layer form input to a transfer function that determines the output of that node nonlinear functions allow the network to learn nonlinear relationships between input and output vectors the s shaped log sigmoidal function was employed that acts as a squashing function bounding the output between zero and one and is the most commonly used for hidden layer nodes zealand et al 1999 the output y j from the j t h neuron of the layer is 1 y j 1 1 e σ w j i x i where w j i weight of the connection joining the j t h neuron in a layer with i t h neuron in the previous layer and x i value of the i t h neuron in the previous layer the linear function as recommended for the nonlinear regression problems was used for the output node activation 3 2 ann input predictors selection of input nodes for developing an ann model is a difficult task that needs attention and a good understanding of the underlying physical processes relying solely on the model to identify critical inputs from a large subset can lead to misconvergence poor accuracy and curse of dimensionality bowden et al 2005 sudheer et al 2002 proposed a better alternative to the popular trial and error approach utilizing the statistical properties cross auto and partial auto correlation of the observed data series for identifying appropriate input vector to the network the candidates for the input layer nodes for the proposed ann scheme were 1 nwp forecasts of precipitation temperature and windspeed obtained from the gfs model produced at 0 5 resolution 2 antecedent precipitation over the basin 3 antecedent streamflow into the reservoir and 4 antecedent baseflow the gridded hydrometeorological inputs were obtained as daily basin averaged values over the dam s upstream catchment area the flow series can be viewed as the sum of baseflow and runoff signals which when selected adequately as ann inputs can improve model performance zemzami et al 2016 the baseflow was separated from the daily observed reservoir inflow using the recursive digital filter rdf rdf has been shown to perform better than other filtering techniques corzo et al 2007 zemzami et al 2016 the general form of the filter is 3 b k 1 b f i m a x a b k 1 1 a b f i m a x y k 1 a b f i m a x 4 r k y k b k where y k is the total streamflow b k is the separated baseflow and r k is runoff component at time k it involves two filter parameters that need prior calibration a the recession constant and b f i m a x the maximum value of the baseflow index that can be modeled by the algorithm eckhardt et al 2005 the initial baseflow b 0 is also an unknown assumed zero in this study for simplicity the runoff signal exhibits high local variations due to varying skewness in the data series especially for the smaller drainage areas this can lead to underestimation of peak flow an appropriate data transformation is needed to reduce these variations and improve the performance sudheer et al 2003 the moving average method was used here that is also found to improve the time lag in modeled peaks flows de vos and reintjes 2005 the moving average smoothens flow time series by replacing each data point with the average of previous k data points k being the length of the memory window with same weight applied to each data point instead of the antecedent runoff time series the moving average over total streamflow was applied to obtain the input node once the candidate predictors were identified the statistical properties of cross and partial autocorrelation between the predictors and the streamflow were used for finding the optimal number of antecedent days for each variable the number of hidden layers in the architecture and the baseflow parameters were selected using a trial and error procedure with this configuration a sensitivity analysis was performed where ann was trained with different combinations of antecedent forecast predictors to select the best combination of input predictors for the study sites this procedure for input predictor selection is schematically shown in fig 3 3 3 ann training algorithm the training of an ann or determining the weights of the ann nodes was performed using a learning algorithm to minimize an error function by providing input output examples training data as found by several studies the levenberg marquardt lm training method is the most effective method for feed forward neural networks with respect to the training precision liu 2010 kişi 2007 sun et al 2016 anctil et al 2004 the algorithm blends the stability of steepest descent method and the speed advantage of gauss newton algorithm providing a robust way to find the optimal weights without having to compute the hessian matrix moré 1978 the matlab implementation of lm training in the neural network toolbox was used here the mathworks inc natick massachusetts the most critical issue in training a multi layer ann is its ability to generalize the modeled outputs while an overly complex ann structure can potentially fit the noise in the training data leading to the overfitting an insufficient level of complexity can result in lack of generalization ability that fails to detect regularities in the data set which is also known as underfitting coulibaly et al 2000 to avoid these issues the early stopped training approach sta is incorporated within the lm training the sta involves dividing the entire data into three subsets i a training set which is used to compute the gradient and update the weights and biases of the network ii validation set over which the errors are monitored during the training process and is used to decided when to stop training iii test set which is not used in the training process but is used to assess the expected performance in the future during the initial phase of training the training and validation set errors decrease however when the network begins to overfit the data the error on the validation set typically begins to rise when the validation error increases for a specified number of iterations the training is stopped and the weights and biases at the minimum of the validation error are returned this threshold number of iterations is chosen to be six in this study as recommended by maier and dandy 2000 this threshold avoids any utilization of validation test set data during the training process either to optimize the network inputs and parameters or to decide when to stop training another method to improve the generalization called regularization was used that modifies the performance function for training the procedure adds a term to the performance function consisting of the mean of the sum of squares of the network weights and biases this causes the network to have smaller weights and biases and forces the network response to be smoother and less likely to overfit the performance ratio of 0 5 which gives equal weight to the mean square errors and the mean square weights was chosen the daily inflow forecast over 7 day horizon was obtained using an iterative multi step forecasting method the ann modeled flows at the first lead time are used as the antecedent flow conditions for the next step s model input including all other past information with each subsequent lead time ann s own outputs are used iteratively to model the 7 day ahead forecasts the metrics used for assessing the ann performance include nash sutcliffe efficiency nse root mean square error rmse correlation r2 and mean absolute error mae 3 4 forecast performance assessment to ensure that the trained ann model does not contain known or detectable flaws and can be used with confidence over any unseen data a comprehensive assessment was performed using different facets of the modeled ann flow specifically three aspects of the model validity are considered here namely a replicative validity how well the ann model can capture the general underlying relationship in the calibration data b predictive validity the ability of the model to generalize or learn the specific patterns in the calibration data c structural validity plausibility of the model when compared with a priori knowledge humphrey et al 2017 for the replicative validity the model fit is evaluated and residuals are analyzed for violation of error model assumptions using graphical plots of predictions and residuals the predictive validity is ensured by demonstrating the model s performance on an independent test set in addition to the training and cross validation sets such that there is no overfitting and underfitting lastly for structural validity the relative contribution of each input predictor to the model output is assessed using a sensitivity analysis described in section 4 1 the more complex methods to quantify relative importance of inputs found in literature are skipped here as replicative and predictive validity are most important for forecasting problems for further details the reader is referred to humphrey et al 2017 who proposed such a validation framework for multilayer anns 3 5 ann based forecasts for reservoir operations optimization the inclusion of forecast variables as a part of ann input predictor set allows for using different forecast products to model the forecast flow the trained ann model when fed with the gfs and gefs based deterministic and ensemble forecast fields respectively over the test period results in deterministic and ensemble members of forecast for 1 7 days lead time the forecast flow was used to optimize the release decisions to maximize the hydropower generation without compromising the flood control and dam safety the focus in this study is on dams that overwhelmingly require daily or longer time horizon for decision making hence the optimization was set up with daily temporal scale over a horizon of 1 7 days the study formulates the following two operating schemes to demonstrate the application and value of forecast flows in reservoir operations 3 5 1 forecast guided operations using model predictive control scheme this scheme is devised to simulate the reservoir operations using short term deterministic forecasts over 1 7 days lead times to generate daily release decisions that are optimal according to a constrained objective function the release at first time step was implemented while the later ones were revised at the next step s model run using updated forecasts such a strategy termed as model predictive control mpc or rolling horizon was proposed by mayne et al 2000 and implemented by turner et al 2017 the objective in this study was set to minimize the hydropower deficit from maximum installed capacity or to maximize energy generated over the forecast horizon based on the optimized releases the end state of reservoir at the end of 7th day is controlled using a penalty function that considers the reservoir state beyond the forecast horizon this penalty function was proportional to the amount of deviation of the terminal reservoir level from the rule curve specified level during the flood seasons excessive spillway release was kept in check to minimize potential downstream damage if a flood event is forecasted causing the reservoir level to reach the flood storage pool the penalty function was modified to the sum of spillway release from the reservoir over the 7 day period thus the selected penalty function grants preference to those release decisions that either minimize large deviations from rule curve specified levels or minimize the downstream flood risk based on the season of reservoir operation the major objective of energy maximization and the penalty function were implemented in the form of a multi objective optimization problem mop seeking pareto optimal set of solutions madsen et al 2009 the suitability of pareto optimal solutions for mops in reservoir operations has been demonstrated amongst others by chen et al 2017 yang et al 2015 and giuliani et al 2016 the non dominated sorting genetic algorithm nsga ii deb et al 2002 was used to yield the pareto front of the optimal solutions where none of the objective functions can be improved further without violating the other several constraints were imposed for optimizing the reservoir releases in the interest of downstream stakeholders dam safety and environmental concerts logistical constraints included turbine and spillway capacity limiting the power and spillway release and storage volume continuity the minimum reservoir storage was set to 95 of the historical minimum while limited by the flood control pool the flood control constraint was implemented by limiting the total release to a safe threshold considering downstream flooding the environmental flow was used to set the minimum release the mathematical formulation and details of the constraints and objective functions are given in appendix b 3 5 2 benchmark scheme to assess the performance and value of optimizing the reservoir operations using the flow forecasts a benchmark operating scheme is needed in reality dam operations would take into account many other factors that cannot be easily accommodated in a scientific analysis including regulatory requirements power grid requests for hydropower dispatch and recreation demands from local agencies thus the hydropower benefits cannot be directly compared against the respective benefits from actually observed operations hence a customized operation scheme was designed targeted specifically at maximizing the hydropower objective function turner et al 2017 as proposed by turner et al 2017 the control rules were designed in the form of look up table where the optimal release is specified as a function of two state variables the reservoir storage level and season of year we followed here the most rigorous way of designing such rules by optimizing the operations using the observed time series data over 24 years from 1995 to 2018 the objective function was to maximize the total energy generated by the reservoir which is the same objective as used for the optimization based on the forecasts although the forecast guided operations were obtained using daily optimization a monthly time step was chosen to obtain the control release policy for benchmarking considering the huge number of data points to optimize over at daily scale 3 6 ann based ensemble forecasting the inherent uncertainty in the forecast variables of precipitation and temperature from the nwp models propagate in the modeled flow forecasts to account for this uncertainty in modeled flow the input predictors of forecast precipitation and minimum and maximum temperature from the ensemble forecast product of gefs were inputs to the selected ann configuration corresponding to each of the 11 gefs ensemble members 11 different realizations of the 1 7 days lead time forecast flow were obtained from the ann model the numerically fast ann technique allowed the simulation of all the ensemble members in minimal processing time next the optimization model set up to use deterministic forecasts was simulated with three scenarios of gefs based forecast flow the minimum maximum and average of the ensemble inflow members maximizing the objective function value across all ensemble members to optimize decisions will undermine the dam operator s ability to adjust release in response to new information turner et al 2017 the use of multi stage stochastic optimization for optimization based on ensemble forecasts can also be found in literature xu et al 2015 fan et al 2016 however due to its computationally demanding requirements and complex implementation we have limited the study here with a deterministic rolling horizon type optimization for probabilistic forecast using minimum maximum and average of the ensemble scenarios 4 results from case studies the ann based forecasting was tested for each of the 23 selected dam sites and the performance was assessed individually the ensemble forecasting was applied to 3 dams for demonstrating the concept and performance finally the reservoir operations optimization using the modeled forecasts is shown for a single dam the detailed results are described in the following sections 4 1 ann architecture and input selection for arriving at the optimal input combination first the number of antecedent days for the candidate predictor variables as identified in section 3 2 was determined using the statistical properties of the respective time series we chose to perform this analysis first for one of the dams pensacola dam located in oklahoma usa to obtain an estimate of probable predictors and then tested the configuration for other dams the cross correlation function ccf measuring the similarity of a time series with the lagged versions of another was obtained for the antecedent variables of precipitation baseflow temperature windspeed against the observed streamflow series at various lags partial autocorrelation of observed streamflow was plotted to assess the predictability in the antecedent flow as shown in fig 4 a the cross correlations decay with increased lags and are significantly high for precipitation and baseflow for use as predictors for ann the streamflow pacf indicates significant correlation up to lag 3 and falls sharply thereafter below the confidence limits for the moving averaged streamflow as input the best value of memory window length k was determined using the cross correlation between streamflow and antecedent moving averaged flow as plotted in fig 4 b for k values of 3 5 and 8 for the pensacola dam from the ccf plots it can be observed that the moving average flow correlates differently at different lags based on the window size for each lead time the window size giving the best cross correlation was identified to obtain the optimal k value based on this ccf based analysis following values were selected for antecedent moving averaged streamflow k 3 for lead times of 1 and 2 days k 5 for lead time of 3 days and k 8 for lead times of 4 7 days three days of antecedent streamflow were also used for forecasting beyond lead time of 3 days two and three days of antecedent precipitation and baseflow were considered respectively while the temperature soil moisture and windspeed were ignored due to lower ccf values in applying the filter for baseflow separation the values of 0 05 and 0 73 were found for a and b f i m a x respectively using a trial and error procedure for pensacola dam in addition to these antecedent variables the forecast forcings of precipitation and temperature minimum and maximum were also provided as input nodes to ann pertaining to the respective lead time of forecasting next to select the final set of input nodes and to assess the relative contribution of each predictor the ann was simulated with different combinations of predictors this demonstrates the structural validity of the selected model as explained in section 3 4 the configuration was kept same across all the dams to allow fair assessment across all 23 dams and keep the sensitivity analysis simple the sensitivity analysis is also advantageous as it can inform if the ann model trained using a particular set of inputs is robust enough to be scalable to locations with varying characteristics for assessing the performance the nse was calculated for each lead time of the modeled flow using the ann configuration that yielded the highest average nse across all the dams the best combination of input nodes was derived the period of jan 2007 to aug 2014 was used as the training set while the validation and testing sets extended from sep 2014 oct 2015 and nov 2015 dec 2017 respectively table 2 demonstrates the sensitivity analysis carried out with the different configurations tabulating the average nse values for lead times of 1 4 and 7 days over all the dams during the testing period the individual dam nse values are shown only for pensacola and green peter dams for the sake of brevity for the lead times of 1 and 7 days table 2 gives the value that each individual predictor adds to the ann forecast skill it can be seen that although the antecedent and forecast precipitation and temperature have little value by themselves they generally improve the performance when used in combination with the flow variables although exceptions exist to this trend when only nse is considered as a measure of performance as in table 2 the skill is improved in terms of minimized lag between the peaks in the modeled and observed flow this is more visible from the lagged correlations between the two streamflow time series plotted in fig 5 where higher correlation value close to lag 0 represents better performance with reduced peak time lagging using only the base flow and moving average flow as predictors fig 5 a and b the peak correlation occurs at a higher lag due to which the modeled peaks exhibit time lagging however as fig 5 b d show the lag is reduced significantly when forecast precipitation and temperature are included the correlation values at lag 0 increase in the latter combination based on this analysis the final predictor combination for the ann is selected as antecedent precipitation 2 days antecedent baseflow 3 days antecedent streamflow 3 days for lead times of 4 7 days antecedent moving average flow 3 5 and 8 day window based on lead time forecast precipitation 1 day and forecast min max temperature 1 day each the final ann architecture selected is shown in fig 6 4 2 ann based forecast assessment using the optimal combination of predictor nodes from section 4 1 the ann model was simulated for the selected 23 dams and the skill in forecast was assessed using various metrics of evaluation over the testing period to assess the predictive validity the evaluation of the modeled flow was performed over completely independent test set using metrics of nse correlation and mae table 3 a summarizes the metrics for each of the 23 dams the six stations on ganges brahmaputra and mekong river basins over which the ann model was setup are also included in the evaluation for the sake of comparison in table 3 b the plots of the observed and modeled flow time series are shown in fig 7 the replicative validity is assessed using i scatter plot of observed and modeled flow and ii time series of standardized residuals scatter plot of the observed and modeled flow data in fig 8 a shows robust performance where systematic divergence from the 1 1 line indicates unmodeled behavior the time series of the standardized residuals as plotted in fig 8 b identifies any serial correlation in the residuals that suggests unmodeled deterministic behavior the standardized residuals were calculated as raw residuals divided by their estimated standard deviation an ideal plot of residuals should lie randomly within a horizontal band with no visible patterns figs 7 and 8 suggest that the forecast skill decreases with increasing lead time as expected however the scatter plot of the observed and modeled flow fits well across the 1 1 line the standardized residuals are mostly randomly distributed around the zero line though the peak flow season has a few negative residual points for lead times of 4 and 7 days corresponding to the underestimations in modeled flow the performance for pensacola dam is specifically affected by an unusually extreme peak event that hit the reservoir during the testing period the likes of which did not occur over the training set of data this causes heavy underestimations in the flow forecast over the event a longer training period is required to improve the performance over such extreme peak flow events apart from this exceptional peak event serial correlation in the residuals is mostly minimal across dams after assessing model validity the next step in the ann performance assessment is to compare the performance across the 23 dams and 6 large scale basin stations as a function of dam s hydrologic and climate characteristics three factors were used to capture the varying characteristics upstream drainage area of dam local climate according to köppen geiger climate classes and coefficient of variation cov of observed streamflow defined as ratio of the standard deviation of the inflow time series over training period to its mean value cov is a measure of variability of the incoming flow into the reservoir fig 9 a below plots the distribution of nse values for lead time of 1 and 7 days measure of ann performance over the testing period as a function of the cov for each dam the respective climate class based distribution of the nse values for each dam is shown in fig 9 b cov is shown on the same plot in the bottom panel fig 9 a reveals strong negative signal between the cov and forecast skill the six stations in large river basins perform very well table 3 b with much larger catchment areas and smaller variation in flow due to monsoonal hydrology from fig 9 b it is evident that the performance improves as the drainage area increases especially for the cold humid and humid subtropical class dams the mediterranean class dams perform well even for smaller reservoirs due to the similar flow characteristics in all the four dams cov 1 and mean flow 1500cfs appendix a shows the map of climate class with ann performance for each dam the two exceptions from the expected trend encircled in red are explained below 1 the fort supply dam in humid subtropical climate receives very low flows with a mean value of 43 cfs over the training period and 18 cfs over validation testing leading to high cov this results in offsetting the effect of average drainage area and makes it harder for ann to learn the flow pattern 2 the fort peck with drainage area is 149 508 km2 not shown entirely in fig 9 b due to scaling issues is the only dam to have a few regulating flow structures upstream a part of annual inflow is contributed by the releases from other upstream dams in addition to the natural unregulated component which deteriorates the nse value at higher lead times and again counterbalances the effect of very high drainage area 4 3 ann based ensemble forecasts feeding the ann with ensemble of forecast predictors from gefs resulted in ensemble members of the forecast flow and estimates of flow uncertainty the spread and mean of the forecast flow were calculated the application is demonstrated over a single peak flow event for three dams these are i pensacola dam with peak event of may 2015 ii detroit dam with peak event of dec 15 iii jackson dam with peak event of may jun 2016 the results of ensemble forecasts compared with the deterministic using gfs forcing fields from section 4 2 and observed flows are shown in fig 10 the mean of the ensemble forecasts usually corresponds well with the deterministic forecasts except during the high flow events when the ensemble spread is higher the increase in forecast flow uncertainty with lead time is also visible from fig 10 for the selected dams 4 4 reservoir operations optimization using deterministic forecasts to demonstrate the optimization of reservoir operations based on the ann based forecasts the pensacola dam was selected a schematic of dam specifics with key operating constraints is shown in fig 11 a the observed data of inflow releases and hydropower generation were obtained from the usace monthly charts for grand lake 2018 4 4 1 forecasts based model predictive control scheme the optimization using data driven inflow forecast was performed over a period of two years sept 2014 aug 2016 for pensacola dam the period was selected to ensure it does not entail the training set of ann the optimization problem is solved at each time step applying nsga ii technique yielding the optimal release sequence for the horizon of 1 7 days with the mpc strategy the first of these is implemented in simulation and the remainder are subsequently updated next day when a new forecast is issued a balanced optimum solution was chosen on the pareto front for flood and non flood seasons to maximize hydropower and minimize the penalty function an example pareto optimal solution front between the hydropower deficit minimization and penalty function in terms of deviation from rule curve and a sample balanced solution over the front is shown in fig 11 b the optimized release decisions over the 2 year period are compared with the actual observed operations in fig 12 a while the resulting reservoir headwater levels based on optimized and observed releases are compared in fig 12 b 4 4 2 benchmark scheme the benchmark operating scheme in the form of control rules or look up table was designed to assess the performance of optimized operations based on forecasts the computations over the 24 year period 1995 2018 at monthly time step were performed using the r package reservoir turner and galelli 2016 with the sdp hydro function that implements stochastic dynamic programing under given constraints and constants release was discretized into 50 uniform values up to the maximum release threshold see appendix b while the storage was discretized into 200 values within the set bounds fig 13 shows the optimal release decisions storage behavior and power generated under control rules 4 4 3 comparison of forecast based optimization with the benchmark to assess the optimization performance two hydropower benefits were calculated i optimized hp hydropower generation resulting from the optimized releases based on the ann inflow forecasts while passing the observed inflow into the system ii benchmark hp hydropower generated from benchmark control rules based operations over the two years of assessment sep 2014 aug 2016 that includes flood and non flood seasons the optimized hp was 954 061 mwh in comparison to the benchmark hp of 906 807 mwh thus the optimized release decisions resulted in an additional and flood safe hydropower production of 47 253 mwh amounting to 4 611 892 using the average residential electricity rate in oklahoma city of 9 76 kwh electricity rates 2018 at an average electricity consumption of 900 kwh per month per us household this much of energy can fulfill the demands of around 45 530 households for one month for the penalty function addressing flood control during peak flow seasons two peak events in the two year period of assessment were analyzed for the improvement in peak release reduction the ann forecast flow over each of these events is shown in fig 14 the underestimation is much higher in the second peak flow event with increasing lead time as the event of such a magnitude is quite rare making it difficult for ann to learn due to this underestimation as the optimization over 7 day horizon proceeds the pre event releases are triggered only very close to the event and not much improvement is achieved in downstream flood control while on the contrary for the may 2015 with better forecast skill the optimal releases are bounded within the safe threshold of 30 000 cfs for most days and the peak release reduced by 18 80 000 cfs on may 30 see fig 12 a the operations improved in terms of the resulting reservoir levels being closer to rule curve as compared to the observed operations during drawdown and dry seasons as shown in fig 12 b 4 5 reservoir operations optimization using ensemble forecasts the ensemble of the forecasts was assimilated into the reservoir operations optimization the optimization model as setup in section 4 2 was simulated with three scenarios of gefs based forecast flow the minimum maximum and average of the ensemble inflow members the technique is illustrated using the peak flow event of may 2015 over pensacola dam the gefs ensemble based inflow forecasts as shown earlier in fig 10 a were used to simulate the optimization model the optimized releases and resulting reservoir elevations from each ensemble scenario and from previously obtained gfs based forecasts are plotted in fig 15 figs 15 a and fig 10 a suggest that the uncertainty in inflow forecasts which is minimal for lead time of 1 day and increases afterwards translates into the corresponding uncertainty in the release decisions however as the optimization proceeds on a rolling horizon basis the more certain and skillful forecasts are ingested to generate optimal release decisions that minimize the uncertainty in the policy and resulting reservoir storage behavior 5 discussion and conclusions a data driven weather forecast based reservoir inflow forecasting using ann technique was developed with a comprehensive validation and testing framework the scalability of the concept was proven by applying it over an inventory of 23 dams receiving unregulated natural inflow with varying hydrological and climate characteristics a consistent ann architecture and input predictor configuration was used to allow a fair comparison across dams in addition to the antecedent conditions short term forecast fields from the gfs model were used as inputs to train a three layered ann using a transformation of antecedent flow time series along with the forecast fields improved the peak flow estimation with minimum time lagging six stations with no dams on large river basins of ganges brahmaputra and mekong were also included in the assessment superior performance was observed for these international basins due to their large basin area and strong seasonal hydrology due to the predictable monsoon the forecast skill assessment suggested that the technique performs reasonably well with nse value greater than 0 5 for 48 of the sites including those on larger basins for forecasts at lead time of 7 days this suggests that the same ann configuration is able to capture the flow variations across a variety of dams the performance across the different dams also revealed trends pertaining to flow characteristics and area of the upstream catchment as reservoir receives inflow that is more variable over the annual scale with less visible seasonal patterns reflected by a high cov value the forecast performance decreases this can be explained as the ability of ann to learn from the limited training provided in this study is not sufficient when the variability in the flow is too high an improved performance can be achieved for such basins using a longer training period with varying flow regimes apart from cov another factor controlling the flow characteristics and the forecast skill is the drainage area of the basin decreasing drainage area makes the basin hydrologically more responsive with lower times of concentration and the basin generates flashier peak flows with lesser seasonal patterns this deteriorates the forecast skill especially with the increasing lead times furthermore although the performance of dams in mediterranean climate zone is generally better than dams in other climates there is no remarkable trend as a function of local climate this in fact verifies the robustness of the selected ann architecture s learning ability across diverse hydro climatologic conditions the results also prove the global scalability of the proposed technique as the forecast performance was tested here for a wide range of hydrologically different dam sites as well as undammed river stations in large river basins of asia and in different climates a comparison of the proposed ann approach against simpler modeling techniques such as a linear regression or conceptual rainfall runoff model will further underscore its value and will be a focus of future study as the deterministic flow forecasts from ann model were not able to capture the inherent uncertainty present in the streamflow estimates an ensemble of forecast fields was assimilated into the trained ann to propagate the uncertainty the results revealed that the highest spread in ensemble flow forecasts occurs during the peak flow seasons when the inflow into reservoir is most uncertain this is the regime when the dam operators can benefit most from these forecasts providing the operating agencies with the ensemble forecasts during flood seasons can help increase the odds of operating for the flows of high probability of occurrence and improve the energy generation without risking the flood control the forecasting technique was then coupled with a reservoir optimization model for maximizing the hydropower generation without compromising flood control and dam safety a rolling horizon scheme was employed to obtain optimal release policy over the horizon of 1 7 days a benchmark policy obtained as look up table by neglecting the forecasts but optimized for energy generation provided a fair benchmark to compare hydropower benefits from the forecast based optimal operations the long term assessment over two years yielded significant benefit over the benchmark policy with reduced flood risk downstream short term ensemble forecasts were also explored in this study the reservoir operations optimization model was also simulated using the different scenarios min max and average of the flows over the forecast horizon the uncertainty in the release decisions was propagated by the corresponding spread of the ensemble forecast flows although the effect was undermined due to the continual updating of forecasts every day during the optimization process it should be mentioned that uncertainty spread in the ensemble flow forecasts which is highest at times of extreme peak flow also leads to the uncertain operating policy mostly during the peak flow times this implies that the use of ensemble forecasts can be most efficient during the seasons of high flow and should be avoided over drier periods the proposed ann scheme that is numerically fast and efficient for forecasting of reservoir inflow allowed an assessment over multiple years and multiple sites using computationally modest resources the proposed ann technique is appropriate for global scale operationalization of short term reservoir inflow forecasts for any dam site provided the training data are selected carefully with acute hydro climatic understanding of the basin the inputs used herein are freely and globally available such a technique also allows long term risk assessment of historical operations by directly incorporating the forecast fields from nwp models the optimization using pareto optimality concept allows the operator to choose an appropriate optimal solution depending on the prevailing circumstances and analyzing the tradeoff between the conflicting objectives future work will constitute exploring this concept for network of dams that are operated as a system such as the colorado or the columbia river system another future extension of this work is to connect energy demand forecasting and energy market pricing with the ann based reservoir operations optimization a key component towards operationalization is dissemination of the outcome into a form useful for the operating agencies and practitioners so that the results and advisory become locally actionable our future research plans to address these issues and report them in a future study appendix a ann performance with köppen geiger climate classes the climate classes used in this study are shown in the map below with the distribution of selected dam sites the markers for each site are sized with the nse of ann forecast at lead time of 1 day fig a1 map showing the köppen geiger climate classes used in this study and the selected dam sites with markers sized with nse of ann forecasted flow at lead time of 1 day fig a1 appendix b optimization model setup for pensacola dam constraints mathematical formulation we define the following constraints for the optimization model with the specific values summarized in table b1 1 release from the turbines is constrained by the turbine capacity p t u r b b 1 r p t p t u r b t 2 the system follows storage volume continuity water balance equation which requires that in each period t b 2 s t 1 s t i t l t r p t r n p t δ t t however as the optimization is performed at daily time steps δ t 1 the losses due to evaporation and seepage l t were ignored 3 reservoir storage was limited to ensure dam safety and avoid infeasible scenarios such as the reservoir running empty b 3 s m i n s t s m a x t here s m i n is set up considering the conservation pool of the reservoir to keep it at a suitable level above the inactive pool while s m a x denotes the active storage capacity till the reservoir s full pool 4 to prevent the downstream flooding hazards the total release was constrained to a maximum limit r m a x b 4 r p t r n p t r m a x t 5 to avoid excessive and infeasible rates of non power release via the spillway the non power release rate was limited to the spillway capacity b 5 r n p t s p i l l m a x t 6 lastly the releases made from reservoir should comply with the environmental flow limit q e n v b 6 r n p t r p t q e n v t table b 1 constraints for optimization for pensacola dam ok table b 1 constraint value comments turbine capacity 12 000 cfs spillway capacity 525 000 cfs minimum storage 1 265 000 ac ft 95 of 10 yr minimum maximum storage 2 021 679 ac ft flood control pool maximum release 50 000 cfs if level exceeds 750 ft otherwise 30 000 cfs avoid flooding at downstream station of neosho river env flow limit 500 cfs objective functions mathematical formulation the following two functions objective and penalty were defined for setting up the optimization problem 1 energy maximization minimize the deficit in hydroelectric energy production mwh from the maximum generation capacity of the powerplant b 7 min f 1 h p m a x t ε h f t h t t r p t δ t t u r b 2 penalty function minimize the sum of the absolute value of storage deviations from the rule curve specified value over a fixed number of days of the optimization horizon it is represented as b 8 min f 2 t s t t t t 3 4 7 d a y s if a flood event is forecasted causing the reservoir level to reach the flood storage pool the function is modified to minimize the uncontrolled spillway release b 9 min f 2 t r n p t t 1 2 3 7 d a y s where h p m a x maximum hydropower generation capacity h f reservoir forebay water level obtained from area elevation curve h t reservoir tailrace water level ε turbine efficiency r n p spillway non power release r p power turbine release δ t t u r b turbine operating hours t target rule curve specified storage s reservoir storage t time step days over the period of optimization software availability name of the software ann reservoir inflow forecasting developer shahryar khalique ahmad contact email skahmad uw edu contact address dept of civil and environmental engineering univ of washington usa software required matlab 2018b needs deep learning toolbox formerly neural network toolbox available since february 2019 availability matlab code and necessary dataset available at https github com shahryaramd ann flowforecasting data availability the type and source of the data set considered in this study name of dataset data source developer data format data availability 1 pensacola dam inflow time series us army corps of engineers http www swt wc usace army mil penscharts html html webpage free 2 forecast precipitation temperature windspeed noaa s global forecast system gfs model https www ncdc noaa gov data access model data model datasets global forcast system gfs grib files free 3 ensemble forecast precipitation temperature noaa s global forecast ensemble system reforecast gefs r model ftp ftp cdc noaa gov projects reforecast2 grib files free 4 in situ precipitation temperature global surface summary of the day gsod https data noaa gov dataset dataset global surface summary of the day gsod text files free 5 gridded historical precipitation livneh daily conus data https www esrl noaa gov psd data gridded data livneh html netcdf files free appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 008 
26175,a generic and scalable scheme is proposed for forecasting reservoir inflow to optimize reservoir operations for hydropower maximization short term weather forecasts and antecedent hydrological variables were inputs to a three layered hydrologically relevant artificial neural network ann to forecast inflow for 7 days of lead time application of the scheme was demonstrated over 23 dams in u s with varying hydrological characteristics and climate regimes probabilistic forecast was also explored by feeding ann with ensembles of weather forecast fields results suggest forecasting skill improves with decreasing coefficient of variation in inflow and increasing drainage area forecast informed operations were simulated using a rolling horizon scheme and assessed against benchmark control rules over two years of operations from pensacola dam oklahoma additional 47 253 mwh of energy could have been harvested without compromising flood risk with optimal operations this study reinforces the potential of a numerically efficient and skillful reservoir inflow forecasting scheme to address water energy security challenges keywords reservoir inflow forecasting artificial neural network numerical weather prediction baseflow separation hydropower maximization global scalability 1 introduction most of the world s artificial reservoirs are operated at daily or longer time scales based on rule curves that were designed using a climatology of historical flow observations and pre dam storage volumes lee et al 2009 ficchì et al 2015 yazicigil et al 1983 rule curves outline the reservoir storage targets that need to be met at specific times of the year operating strictly based on these climatology based rules can lead to mishandling of an unexpected peak reservoir inflow event miao et al 2016 for instance in a relatively dry flood season lowering the pool to rule curve level can result in significant loss in hydropower generation which could be avoided if weather forecasts were made ahead of time for efficient operations of a single multipurpose reservoir system the information about forecast inflow into the system is an important part of the real time decision making process short term daily to weekly scale forecasts are indispensable in maximizing societally important benefits of flood control and hydropower generation yazicigil et al 1983 anghileri et al 2016 flood mitigation requires sufficient flood storage before an impending flood while hydropower generation requires maximizing the economic value of water by releasing most of it through turbines and keeping reservoir at maximum pool reservoir inflow forecasts if reasonably skillful can help achieve a balance where hydropower can be maximized without compromising flood risk downstream jordan et al 2012 madsen et al 2009 qi et al 2017 often this is achieved by proactive storage or release from reservoir in anticipation of reservoir inflow recently a study on two dams in us showed application of numerical weather prediction model nwp based reservoir inflow forecasts for optimizing the reservoir operations ahmad et al 2018 significant hydropower benefits were demonstrated without compromising the flood control objective ongoing projects such as integrated forecast and reservoir management inform georgakakos et al 2007 and forecast informed reservoir operations firo overview 2017 focused over specific watersheds are also utilizing short term weather forecasts for operating the reservoirs the nwp models from various meteorological agencies produce weather scale forecast fields of temperatures wind precipitation soil moisture in three dimensions over the entire globe these publicly available forecasts represent an underutilized resource for the hydropower community due to concern over fossil fuel based environmental degradation dudhani et al 2006 li 2005 moreover efficient management of hydropower facilities is essential for emerging economies where demand for energy often exceeds supply or generation capacity asif and muneer 2007 as more hydropower dams are constructed especially in developing nations achieving the highest possible operational efficiency using available weather forecasts can address the growing resource needs of today the challenge today is to convert the global and public availability of weather forecasts to locally actionable data as reservoir inflow forecast and thus mainstream their use in the operational world however there are logistic hurdles to globalizing hydropower maximization techniques based on nwp based reservoir inflow forecasts even if the forecasts achieve certain level of accuracy adequate computational resources are required to capture sudden peak flow events considerably faster than real time this is because at a global scale the nwp models lack sufficient detail to force hydrological models over the reservoir catchments where precipitation can often exhibit higher spatial variability flint and flint 2012 do hoai et al 2011 dynamic downscaling techniques have subsequently emerged in recent past that are known to be locally constrained rendition of weather forecasts at hydrologically relevant scale wilby and wigley 1997 however dynamic downscaling is computationally intensive and not feasible for computationally constrained settings of most operational reservoir management agencies around the world for example a study on flood forecasting in houston tx found that it takes nearly 7 20 h for dynamically downscaling of nwp forecasts for 24 hr lead time depending on the configuration of cpu available in the current market sikder et al 2019 in summary there are two important hurdles for operationalization of reservoir inflow forecasts in real time reservoir optimization models around the world these are 1 uncertainty in inflow forecast zhao et al 2011 georgakakos et al 2008 faber and stedinger 2001 and 2 computational burden of deriving these reservoir inflow forecasts cheng et al 2014 for large scale applications of nwp forecast fields in optimizing reservoir operations for hydropower maximization an alternate and scalable numerical scheme is required such a scheme should have the following desired properties 1 it has to be numerically efficient and considerably faster than real time for practical operations 2 it has to allow rapid multi year historical assessment to understand risks of false positives and negatives in operations 3 it has to be effective in limited in situ hydrologic data settings for application in developing countries and 4 it has to be skillful and yield physically realistic forecasts for streamflow over the forecast horizon our selected scheme that we present here with such desired features is the data based technique of artificial neural networks ann tailored for reservoir inflow forecasting the ann technique over the last two decades has proven to be an efficient supplement to modeling qualitative and quantitative water resource variables and capturing the nonlinearity in flow maier et al 2010 jain et al 2009 maier and dandy 1996 hutton and kapelan 2015 wei 2016 govindaraju and rao 2000 wu et al 2014 as noted by asce task committee on application of anns in hydrology 2000 anns are robust tools for many of the complex hydrological modeling problems the review of relevant research on anns by maier et al 2010 suggest that a vast majority of studies have focused on flow prediction daily short term flow forecasting using ann has been explored amongst others by birikundavyi et al 2002 kişi 2005 wu et al 2009 coulibaly et al 2000 and zemzami et al 2016 as required in all modeling efforts following good practice that increases the credibility and impact of modeling results jakeman et al 2006 welsh 2008 is particularly important for ann models that are developed using available data and not explicitly based on underlying physical processes for the input nodes past conditions of streamflow and hydro meteorological variables are commonly used in predicting future flow studies also used other predictor combinations such as flow length and travel time akhtar et al 2009 previous day s rainfall and temperature lorrai and sechi 1995 flow rainfall and evapotranspiration anctil et al 2004 soil moisture deficits and runoff cheng and noguchi 1996 bartoletti et al 2018 eliminated the redundancy in antecedent rainfall using the data driven principal component analysis technique however to the best of our knowledge there is no work on the use of freely available global nwp weather forecasts as ann inputs several studies using only the antecedent conditions have indicated the inability of ann to capture the flow peaks e g campolo et al 1999 sudheer et al 2003 wu et al 2009 de vos and reintjes 2005 to provide an accurate estimate of peaks it is necessary to remove the local variations caused by extreme flows from the time series function being mapped streamflow transformation is used to simplify the data structure following a convenient statistical model sudheer et al 2003 as found by zemzami et al 2016 separation of streamflow into baseflow and runoff components improved the peak flow estimation and hence it was incorporated in the present research the runoff component was transformed by a moving average procedure to improve the peak flow prediction further studies on ann based reservoir inflow prediction have primarily focused over only a specific reservoir system in an effort to demonstrate global scalability application of any proposed inflow forecasting scheme extending over multiple dams with varying hydro climatic and geographic characteristics is necessary the innovative aspects of this study include development of ann model based on the coarser scale forecast fields of 1 7 days lead from nwp weather models to predict basin scale hydrology the technique was tested over an inventory of 23 dams with widely varying hydrological characteristics and local climate regimes a comprehensive validation framework was employed to assess the skillful and physical realism of the modeled forecasts the probabilistic nature of inflow forecasts was addressed by obtaining an ensemble of forecast flow derived from ensemble of weather forecast the forecasts were further incorporated in a reservoir optimization model for maximizing the daily hydropower generation while meeting other dam management constraints and regulations the specific research questions of this study are 1 can the use of ann with pertinent hydrologic knowledge be computationally efficient skillful and globally scalable in forecasting the reservoir inflow over short term forecast horizon 1 7 days 2 what role does the hydrological characteristics of the upstream basin and local climate zone play in driving the forecast skill of the designed ann model 3 can such fast inflow forecasts be used to optimize the reservoir operations to improve energy generation without compromising flood or dam safety the rest of the paper is organized as follows in the next section we discuss the selection of dam sites for application of the proposed technique followed by a description of datasets used this is then followed by a detailed methodology and different components involved in section 3 the case study results of ann based forecasts over the database of dams and its application for optimization of reservoir operations are presented in section 4 followed by discussion and concluding remarks in section 5 2 study sites and hydrometeorological data in order to address the first two research questions 23 dams located in various climate zones of the contiguous u s conus were selected all selected dams receive unregulated flow and are operated for a variety of purposes such as flood control hydropower generation water supply irrigation and recreation fifteen of these dams are powered the dam inventory also represents a wide range of upstream catchment area topography hydrological characteristics flow patterns reservoir storage and installed hydropower capacity the dams are shown in fig 1 by location and size of upstream drainage area apart from the study sites within us six dam sites in large river basins of ganges brahmaputra and mekong were identified table 1 presents the information about the selected study sites with descriptive statistics of the original flow data over 2007 2014 data for this study over the conus are as follows a deterministic and ensemble forecast hydro meteorological forcing fields b basin s antecedent conditions and c current reservoir state the deterministic forecast fields of precipitation temperature and windspeed were acquired from the global forecast system gfs global scale numerical weather prediction model at 0 5 resolution for 1 7 days lead time with a 3 hourly temporal resolution the archived data is available from oct 2006 due to which the period of analysis in this study was set to 2007 2017 and all the other datasets were gathered over this period for the ensemble forecast fields noaa s global ensemble forecasting system reforecast version 2 dataset gefs r hamill et al 2013 with 11 member ensemble of forecasts at 1 resolution was used the antecedent precipitation over the basin was obtained from the climate hazards group infrared precipitation with station data chirps gridded rainfall time series at 0 05 resolution funk et al 2015 antecedent temperature and windspeed from in situ global surface summary of the day gsod data global surface summary of the day 2018 and antecedent soil moisture from global land data assimilation systems gldas all the gridded datasets were processed to calculate the basin averaged values to be used as inputs to ann the antecedent observed reservoir inflow for us dams was obtained from the dam operating agencies of us army corps of engineers usace and us bureau of reclamation usbr state of current reservoir storage and headwater level was acquired from the respective agencies data portals the climate zones for each dam were extracted from köppen geiger climate classification peel et al 2007 for the real time forecasting and reservoir operation purpose a forecast horizon of 1 7 days ahead was chosen 3 methodology the experimental approach followed in the study is shown in fig 2 and described in the following sections 3 1 proposed ann architecture the ann architecture used here is the multilayer feedforward neural network this is one of the most popular architecture used for streamflow forecasting wu et al 2009 it involves input hidden and output layers where the hidden layer allows the network to perform complex nonlinear mapping between the input and output variables coulibaly et al 2000 funhashi et al 1989 a three layered ann with one hidden layer was implemented many experiments in the past have confirmed this to be adequate for most forecasting problems lippmann 1987 zhang et al 1998 coulibaly et al 2000 the network s ability to learn from training data and generalize depends on the number of nodes in input and hidden layers the sum of the weighted nodes of a layer form input to a transfer function that determines the output of that node nonlinear functions allow the network to learn nonlinear relationships between input and output vectors the s shaped log sigmoidal function was employed that acts as a squashing function bounding the output between zero and one and is the most commonly used for hidden layer nodes zealand et al 1999 the output y j from the j t h neuron of the layer is 1 y j 1 1 e σ w j i x i where w j i weight of the connection joining the j t h neuron in a layer with i t h neuron in the previous layer and x i value of the i t h neuron in the previous layer the linear function as recommended for the nonlinear regression problems was used for the output node activation 3 2 ann input predictors selection of input nodes for developing an ann model is a difficult task that needs attention and a good understanding of the underlying physical processes relying solely on the model to identify critical inputs from a large subset can lead to misconvergence poor accuracy and curse of dimensionality bowden et al 2005 sudheer et al 2002 proposed a better alternative to the popular trial and error approach utilizing the statistical properties cross auto and partial auto correlation of the observed data series for identifying appropriate input vector to the network the candidates for the input layer nodes for the proposed ann scheme were 1 nwp forecasts of precipitation temperature and windspeed obtained from the gfs model produced at 0 5 resolution 2 antecedent precipitation over the basin 3 antecedent streamflow into the reservoir and 4 antecedent baseflow the gridded hydrometeorological inputs were obtained as daily basin averaged values over the dam s upstream catchment area the flow series can be viewed as the sum of baseflow and runoff signals which when selected adequately as ann inputs can improve model performance zemzami et al 2016 the baseflow was separated from the daily observed reservoir inflow using the recursive digital filter rdf rdf has been shown to perform better than other filtering techniques corzo et al 2007 zemzami et al 2016 the general form of the filter is 3 b k 1 b f i m a x a b k 1 1 a b f i m a x y k 1 a b f i m a x 4 r k y k b k where y k is the total streamflow b k is the separated baseflow and r k is runoff component at time k it involves two filter parameters that need prior calibration a the recession constant and b f i m a x the maximum value of the baseflow index that can be modeled by the algorithm eckhardt et al 2005 the initial baseflow b 0 is also an unknown assumed zero in this study for simplicity the runoff signal exhibits high local variations due to varying skewness in the data series especially for the smaller drainage areas this can lead to underestimation of peak flow an appropriate data transformation is needed to reduce these variations and improve the performance sudheer et al 2003 the moving average method was used here that is also found to improve the time lag in modeled peaks flows de vos and reintjes 2005 the moving average smoothens flow time series by replacing each data point with the average of previous k data points k being the length of the memory window with same weight applied to each data point instead of the antecedent runoff time series the moving average over total streamflow was applied to obtain the input node once the candidate predictors were identified the statistical properties of cross and partial autocorrelation between the predictors and the streamflow were used for finding the optimal number of antecedent days for each variable the number of hidden layers in the architecture and the baseflow parameters were selected using a trial and error procedure with this configuration a sensitivity analysis was performed where ann was trained with different combinations of antecedent forecast predictors to select the best combination of input predictors for the study sites this procedure for input predictor selection is schematically shown in fig 3 3 3 ann training algorithm the training of an ann or determining the weights of the ann nodes was performed using a learning algorithm to minimize an error function by providing input output examples training data as found by several studies the levenberg marquardt lm training method is the most effective method for feed forward neural networks with respect to the training precision liu 2010 kişi 2007 sun et al 2016 anctil et al 2004 the algorithm blends the stability of steepest descent method and the speed advantage of gauss newton algorithm providing a robust way to find the optimal weights without having to compute the hessian matrix moré 1978 the matlab implementation of lm training in the neural network toolbox was used here the mathworks inc natick massachusetts the most critical issue in training a multi layer ann is its ability to generalize the modeled outputs while an overly complex ann structure can potentially fit the noise in the training data leading to the overfitting an insufficient level of complexity can result in lack of generalization ability that fails to detect regularities in the data set which is also known as underfitting coulibaly et al 2000 to avoid these issues the early stopped training approach sta is incorporated within the lm training the sta involves dividing the entire data into three subsets i a training set which is used to compute the gradient and update the weights and biases of the network ii validation set over which the errors are monitored during the training process and is used to decided when to stop training iii test set which is not used in the training process but is used to assess the expected performance in the future during the initial phase of training the training and validation set errors decrease however when the network begins to overfit the data the error on the validation set typically begins to rise when the validation error increases for a specified number of iterations the training is stopped and the weights and biases at the minimum of the validation error are returned this threshold number of iterations is chosen to be six in this study as recommended by maier and dandy 2000 this threshold avoids any utilization of validation test set data during the training process either to optimize the network inputs and parameters or to decide when to stop training another method to improve the generalization called regularization was used that modifies the performance function for training the procedure adds a term to the performance function consisting of the mean of the sum of squares of the network weights and biases this causes the network to have smaller weights and biases and forces the network response to be smoother and less likely to overfit the performance ratio of 0 5 which gives equal weight to the mean square errors and the mean square weights was chosen the daily inflow forecast over 7 day horizon was obtained using an iterative multi step forecasting method the ann modeled flows at the first lead time are used as the antecedent flow conditions for the next step s model input including all other past information with each subsequent lead time ann s own outputs are used iteratively to model the 7 day ahead forecasts the metrics used for assessing the ann performance include nash sutcliffe efficiency nse root mean square error rmse correlation r2 and mean absolute error mae 3 4 forecast performance assessment to ensure that the trained ann model does not contain known or detectable flaws and can be used with confidence over any unseen data a comprehensive assessment was performed using different facets of the modeled ann flow specifically three aspects of the model validity are considered here namely a replicative validity how well the ann model can capture the general underlying relationship in the calibration data b predictive validity the ability of the model to generalize or learn the specific patterns in the calibration data c structural validity plausibility of the model when compared with a priori knowledge humphrey et al 2017 for the replicative validity the model fit is evaluated and residuals are analyzed for violation of error model assumptions using graphical plots of predictions and residuals the predictive validity is ensured by demonstrating the model s performance on an independent test set in addition to the training and cross validation sets such that there is no overfitting and underfitting lastly for structural validity the relative contribution of each input predictor to the model output is assessed using a sensitivity analysis described in section 4 1 the more complex methods to quantify relative importance of inputs found in literature are skipped here as replicative and predictive validity are most important for forecasting problems for further details the reader is referred to humphrey et al 2017 who proposed such a validation framework for multilayer anns 3 5 ann based forecasts for reservoir operations optimization the inclusion of forecast variables as a part of ann input predictor set allows for using different forecast products to model the forecast flow the trained ann model when fed with the gfs and gefs based deterministic and ensemble forecast fields respectively over the test period results in deterministic and ensemble members of forecast for 1 7 days lead time the forecast flow was used to optimize the release decisions to maximize the hydropower generation without compromising the flood control and dam safety the focus in this study is on dams that overwhelmingly require daily or longer time horizon for decision making hence the optimization was set up with daily temporal scale over a horizon of 1 7 days the study formulates the following two operating schemes to demonstrate the application and value of forecast flows in reservoir operations 3 5 1 forecast guided operations using model predictive control scheme this scheme is devised to simulate the reservoir operations using short term deterministic forecasts over 1 7 days lead times to generate daily release decisions that are optimal according to a constrained objective function the release at first time step was implemented while the later ones were revised at the next step s model run using updated forecasts such a strategy termed as model predictive control mpc or rolling horizon was proposed by mayne et al 2000 and implemented by turner et al 2017 the objective in this study was set to minimize the hydropower deficit from maximum installed capacity or to maximize energy generated over the forecast horizon based on the optimized releases the end state of reservoir at the end of 7th day is controlled using a penalty function that considers the reservoir state beyond the forecast horizon this penalty function was proportional to the amount of deviation of the terminal reservoir level from the rule curve specified level during the flood seasons excessive spillway release was kept in check to minimize potential downstream damage if a flood event is forecasted causing the reservoir level to reach the flood storage pool the penalty function was modified to the sum of spillway release from the reservoir over the 7 day period thus the selected penalty function grants preference to those release decisions that either minimize large deviations from rule curve specified levels or minimize the downstream flood risk based on the season of reservoir operation the major objective of energy maximization and the penalty function were implemented in the form of a multi objective optimization problem mop seeking pareto optimal set of solutions madsen et al 2009 the suitability of pareto optimal solutions for mops in reservoir operations has been demonstrated amongst others by chen et al 2017 yang et al 2015 and giuliani et al 2016 the non dominated sorting genetic algorithm nsga ii deb et al 2002 was used to yield the pareto front of the optimal solutions where none of the objective functions can be improved further without violating the other several constraints were imposed for optimizing the reservoir releases in the interest of downstream stakeholders dam safety and environmental concerts logistical constraints included turbine and spillway capacity limiting the power and spillway release and storage volume continuity the minimum reservoir storage was set to 95 of the historical minimum while limited by the flood control pool the flood control constraint was implemented by limiting the total release to a safe threshold considering downstream flooding the environmental flow was used to set the minimum release the mathematical formulation and details of the constraints and objective functions are given in appendix b 3 5 2 benchmark scheme to assess the performance and value of optimizing the reservoir operations using the flow forecasts a benchmark operating scheme is needed in reality dam operations would take into account many other factors that cannot be easily accommodated in a scientific analysis including regulatory requirements power grid requests for hydropower dispatch and recreation demands from local agencies thus the hydropower benefits cannot be directly compared against the respective benefits from actually observed operations hence a customized operation scheme was designed targeted specifically at maximizing the hydropower objective function turner et al 2017 as proposed by turner et al 2017 the control rules were designed in the form of look up table where the optimal release is specified as a function of two state variables the reservoir storage level and season of year we followed here the most rigorous way of designing such rules by optimizing the operations using the observed time series data over 24 years from 1995 to 2018 the objective function was to maximize the total energy generated by the reservoir which is the same objective as used for the optimization based on the forecasts although the forecast guided operations were obtained using daily optimization a monthly time step was chosen to obtain the control release policy for benchmarking considering the huge number of data points to optimize over at daily scale 3 6 ann based ensemble forecasting the inherent uncertainty in the forecast variables of precipitation and temperature from the nwp models propagate in the modeled flow forecasts to account for this uncertainty in modeled flow the input predictors of forecast precipitation and minimum and maximum temperature from the ensemble forecast product of gefs were inputs to the selected ann configuration corresponding to each of the 11 gefs ensemble members 11 different realizations of the 1 7 days lead time forecast flow were obtained from the ann model the numerically fast ann technique allowed the simulation of all the ensemble members in minimal processing time next the optimization model set up to use deterministic forecasts was simulated with three scenarios of gefs based forecast flow the minimum maximum and average of the ensemble inflow members maximizing the objective function value across all ensemble members to optimize decisions will undermine the dam operator s ability to adjust release in response to new information turner et al 2017 the use of multi stage stochastic optimization for optimization based on ensemble forecasts can also be found in literature xu et al 2015 fan et al 2016 however due to its computationally demanding requirements and complex implementation we have limited the study here with a deterministic rolling horizon type optimization for probabilistic forecast using minimum maximum and average of the ensemble scenarios 4 results from case studies the ann based forecasting was tested for each of the 23 selected dam sites and the performance was assessed individually the ensemble forecasting was applied to 3 dams for demonstrating the concept and performance finally the reservoir operations optimization using the modeled forecasts is shown for a single dam the detailed results are described in the following sections 4 1 ann architecture and input selection for arriving at the optimal input combination first the number of antecedent days for the candidate predictor variables as identified in section 3 2 was determined using the statistical properties of the respective time series we chose to perform this analysis first for one of the dams pensacola dam located in oklahoma usa to obtain an estimate of probable predictors and then tested the configuration for other dams the cross correlation function ccf measuring the similarity of a time series with the lagged versions of another was obtained for the antecedent variables of precipitation baseflow temperature windspeed against the observed streamflow series at various lags partial autocorrelation of observed streamflow was plotted to assess the predictability in the antecedent flow as shown in fig 4 a the cross correlations decay with increased lags and are significantly high for precipitation and baseflow for use as predictors for ann the streamflow pacf indicates significant correlation up to lag 3 and falls sharply thereafter below the confidence limits for the moving averaged streamflow as input the best value of memory window length k was determined using the cross correlation between streamflow and antecedent moving averaged flow as plotted in fig 4 b for k values of 3 5 and 8 for the pensacola dam from the ccf plots it can be observed that the moving average flow correlates differently at different lags based on the window size for each lead time the window size giving the best cross correlation was identified to obtain the optimal k value based on this ccf based analysis following values were selected for antecedent moving averaged streamflow k 3 for lead times of 1 and 2 days k 5 for lead time of 3 days and k 8 for lead times of 4 7 days three days of antecedent streamflow were also used for forecasting beyond lead time of 3 days two and three days of antecedent precipitation and baseflow were considered respectively while the temperature soil moisture and windspeed were ignored due to lower ccf values in applying the filter for baseflow separation the values of 0 05 and 0 73 were found for a and b f i m a x respectively using a trial and error procedure for pensacola dam in addition to these antecedent variables the forecast forcings of precipitation and temperature minimum and maximum were also provided as input nodes to ann pertaining to the respective lead time of forecasting next to select the final set of input nodes and to assess the relative contribution of each predictor the ann was simulated with different combinations of predictors this demonstrates the structural validity of the selected model as explained in section 3 4 the configuration was kept same across all the dams to allow fair assessment across all 23 dams and keep the sensitivity analysis simple the sensitivity analysis is also advantageous as it can inform if the ann model trained using a particular set of inputs is robust enough to be scalable to locations with varying characteristics for assessing the performance the nse was calculated for each lead time of the modeled flow using the ann configuration that yielded the highest average nse across all the dams the best combination of input nodes was derived the period of jan 2007 to aug 2014 was used as the training set while the validation and testing sets extended from sep 2014 oct 2015 and nov 2015 dec 2017 respectively table 2 demonstrates the sensitivity analysis carried out with the different configurations tabulating the average nse values for lead times of 1 4 and 7 days over all the dams during the testing period the individual dam nse values are shown only for pensacola and green peter dams for the sake of brevity for the lead times of 1 and 7 days table 2 gives the value that each individual predictor adds to the ann forecast skill it can be seen that although the antecedent and forecast precipitation and temperature have little value by themselves they generally improve the performance when used in combination with the flow variables although exceptions exist to this trend when only nse is considered as a measure of performance as in table 2 the skill is improved in terms of minimized lag between the peaks in the modeled and observed flow this is more visible from the lagged correlations between the two streamflow time series plotted in fig 5 where higher correlation value close to lag 0 represents better performance with reduced peak time lagging using only the base flow and moving average flow as predictors fig 5 a and b the peak correlation occurs at a higher lag due to which the modeled peaks exhibit time lagging however as fig 5 b d show the lag is reduced significantly when forecast precipitation and temperature are included the correlation values at lag 0 increase in the latter combination based on this analysis the final predictor combination for the ann is selected as antecedent precipitation 2 days antecedent baseflow 3 days antecedent streamflow 3 days for lead times of 4 7 days antecedent moving average flow 3 5 and 8 day window based on lead time forecast precipitation 1 day and forecast min max temperature 1 day each the final ann architecture selected is shown in fig 6 4 2 ann based forecast assessment using the optimal combination of predictor nodes from section 4 1 the ann model was simulated for the selected 23 dams and the skill in forecast was assessed using various metrics of evaluation over the testing period to assess the predictive validity the evaluation of the modeled flow was performed over completely independent test set using metrics of nse correlation and mae table 3 a summarizes the metrics for each of the 23 dams the six stations on ganges brahmaputra and mekong river basins over which the ann model was setup are also included in the evaluation for the sake of comparison in table 3 b the plots of the observed and modeled flow time series are shown in fig 7 the replicative validity is assessed using i scatter plot of observed and modeled flow and ii time series of standardized residuals scatter plot of the observed and modeled flow data in fig 8 a shows robust performance where systematic divergence from the 1 1 line indicates unmodeled behavior the time series of the standardized residuals as plotted in fig 8 b identifies any serial correlation in the residuals that suggests unmodeled deterministic behavior the standardized residuals were calculated as raw residuals divided by their estimated standard deviation an ideal plot of residuals should lie randomly within a horizontal band with no visible patterns figs 7 and 8 suggest that the forecast skill decreases with increasing lead time as expected however the scatter plot of the observed and modeled flow fits well across the 1 1 line the standardized residuals are mostly randomly distributed around the zero line though the peak flow season has a few negative residual points for lead times of 4 and 7 days corresponding to the underestimations in modeled flow the performance for pensacola dam is specifically affected by an unusually extreme peak event that hit the reservoir during the testing period the likes of which did not occur over the training set of data this causes heavy underestimations in the flow forecast over the event a longer training period is required to improve the performance over such extreme peak flow events apart from this exceptional peak event serial correlation in the residuals is mostly minimal across dams after assessing model validity the next step in the ann performance assessment is to compare the performance across the 23 dams and 6 large scale basin stations as a function of dam s hydrologic and climate characteristics three factors were used to capture the varying characteristics upstream drainage area of dam local climate according to köppen geiger climate classes and coefficient of variation cov of observed streamflow defined as ratio of the standard deviation of the inflow time series over training period to its mean value cov is a measure of variability of the incoming flow into the reservoir fig 9 a below plots the distribution of nse values for lead time of 1 and 7 days measure of ann performance over the testing period as a function of the cov for each dam the respective climate class based distribution of the nse values for each dam is shown in fig 9 b cov is shown on the same plot in the bottom panel fig 9 a reveals strong negative signal between the cov and forecast skill the six stations in large river basins perform very well table 3 b with much larger catchment areas and smaller variation in flow due to monsoonal hydrology from fig 9 b it is evident that the performance improves as the drainage area increases especially for the cold humid and humid subtropical class dams the mediterranean class dams perform well even for smaller reservoirs due to the similar flow characteristics in all the four dams cov 1 and mean flow 1500cfs appendix a shows the map of climate class with ann performance for each dam the two exceptions from the expected trend encircled in red are explained below 1 the fort supply dam in humid subtropical climate receives very low flows with a mean value of 43 cfs over the training period and 18 cfs over validation testing leading to high cov this results in offsetting the effect of average drainage area and makes it harder for ann to learn the flow pattern 2 the fort peck with drainage area is 149 508 km2 not shown entirely in fig 9 b due to scaling issues is the only dam to have a few regulating flow structures upstream a part of annual inflow is contributed by the releases from other upstream dams in addition to the natural unregulated component which deteriorates the nse value at higher lead times and again counterbalances the effect of very high drainage area 4 3 ann based ensemble forecasts feeding the ann with ensemble of forecast predictors from gefs resulted in ensemble members of the forecast flow and estimates of flow uncertainty the spread and mean of the forecast flow were calculated the application is demonstrated over a single peak flow event for three dams these are i pensacola dam with peak event of may 2015 ii detroit dam with peak event of dec 15 iii jackson dam with peak event of may jun 2016 the results of ensemble forecasts compared with the deterministic using gfs forcing fields from section 4 2 and observed flows are shown in fig 10 the mean of the ensemble forecasts usually corresponds well with the deterministic forecasts except during the high flow events when the ensemble spread is higher the increase in forecast flow uncertainty with lead time is also visible from fig 10 for the selected dams 4 4 reservoir operations optimization using deterministic forecasts to demonstrate the optimization of reservoir operations based on the ann based forecasts the pensacola dam was selected a schematic of dam specifics with key operating constraints is shown in fig 11 a the observed data of inflow releases and hydropower generation were obtained from the usace monthly charts for grand lake 2018 4 4 1 forecasts based model predictive control scheme the optimization using data driven inflow forecast was performed over a period of two years sept 2014 aug 2016 for pensacola dam the period was selected to ensure it does not entail the training set of ann the optimization problem is solved at each time step applying nsga ii technique yielding the optimal release sequence for the horizon of 1 7 days with the mpc strategy the first of these is implemented in simulation and the remainder are subsequently updated next day when a new forecast is issued a balanced optimum solution was chosen on the pareto front for flood and non flood seasons to maximize hydropower and minimize the penalty function an example pareto optimal solution front between the hydropower deficit minimization and penalty function in terms of deviation from rule curve and a sample balanced solution over the front is shown in fig 11 b the optimized release decisions over the 2 year period are compared with the actual observed operations in fig 12 a while the resulting reservoir headwater levels based on optimized and observed releases are compared in fig 12 b 4 4 2 benchmark scheme the benchmark operating scheme in the form of control rules or look up table was designed to assess the performance of optimized operations based on forecasts the computations over the 24 year period 1995 2018 at monthly time step were performed using the r package reservoir turner and galelli 2016 with the sdp hydro function that implements stochastic dynamic programing under given constraints and constants release was discretized into 50 uniform values up to the maximum release threshold see appendix b while the storage was discretized into 200 values within the set bounds fig 13 shows the optimal release decisions storage behavior and power generated under control rules 4 4 3 comparison of forecast based optimization with the benchmark to assess the optimization performance two hydropower benefits were calculated i optimized hp hydropower generation resulting from the optimized releases based on the ann inflow forecasts while passing the observed inflow into the system ii benchmark hp hydropower generated from benchmark control rules based operations over the two years of assessment sep 2014 aug 2016 that includes flood and non flood seasons the optimized hp was 954 061 mwh in comparison to the benchmark hp of 906 807 mwh thus the optimized release decisions resulted in an additional and flood safe hydropower production of 47 253 mwh amounting to 4 611 892 using the average residential electricity rate in oklahoma city of 9 76 kwh electricity rates 2018 at an average electricity consumption of 900 kwh per month per us household this much of energy can fulfill the demands of around 45 530 households for one month for the penalty function addressing flood control during peak flow seasons two peak events in the two year period of assessment were analyzed for the improvement in peak release reduction the ann forecast flow over each of these events is shown in fig 14 the underestimation is much higher in the second peak flow event with increasing lead time as the event of such a magnitude is quite rare making it difficult for ann to learn due to this underestimation as the optimization over 7 day horizon proceeds the pre event releases are triggered only very close to the event and not much improvement is achieved in downstream flood control while on the contrary for the may 2015 with better forecast skill the optimal releases are bounded within the safe threshold of 30 000 cfs for most days and the peak release reduced by 18 80 000 cfs on may 30 see fig 12 a the operations improved in terms of the resulting reservoir levels being closer to rule curve as compared to the observed operations during drawdown and dry seasons as shown in fig 12 b 4 5 reservoir operations optimization using ensemble forecasts the ensemble of the forecasts was assimilated into the reservoir operations optimization the optimization model as setup in section 4 2 was simulated with three scenarios of gefs based forecast flow the minimum maximum and average of the ensemble inflow members the technique is illustrated using the peak flow event of may 2015 over pensacola dam the gefs ensemble based inflow forecasts as shown earlier in fig 10 a were used to simulate the optimization model the optimized releases and resulting reservoir elevations from each ensemble scenario and from previously obtained gfs based forecasts are plotted in fig 15 figs 15 a and fig 10 a suggest that the uncertainty in inflow forecasts which is minimal for lead time of 1 day and increases afterwards translates into the corresponding uncertainty in the release decisions however as the optimization proceeds on a rolling horizon basis the more certain and skillful forecasts are ingested to generate optimal release decisions that minimize the uncertainty in the policy and resulting reservoir storage behavior 5 discussion and conclusions a data driven weather forecast based reservoir inflow forecasting using ann technique was developed with a comprehensive validation and testing framework the scalability of the concept was proven by applying it over an inventory of 23 dams receiving unregulated natural inflow with varying hydrological and climate characteristics a consistent ann architecture and input predictor configuration was used to allow a fair comparison across dams in addition to the antecedent conditions short term forecast fields from the gfs model were used as inputs to train a three layered ann using a transformation of antecedent flow time series along with the forecast fields improved the peak flow estimation with minimum time lagging six stations with no dams on large river basins of ganges brahmaputra and mekong were also included in the assessment superior performance was observed for these international basins due to their large basin area and strong seasonal hydrology due to the predictable monsoon the forecast skill assessment suggested that the technique performs reasonably well with nse value greater than 0 5 for 48 of the sites including those on larger basins for forecasts at lead time of 7 days this suggests that the same ann configuration is able to capture the flow variations across a variety of dams the performance across the different dams also revealed trends pertaining to flow characteristics and area of the upstream catchment as reservoir receives inflow that is more variable over the annual scale with less visible seasonal patterns reflected by a high cov value the forecast performance decreases this can be explained as the ability of ann to learn from the limited training provided in this study is not sufficient when the variability in the flow is too high an improved performance can be achieved for such basins using a longer training period with varying flow regimes apart from cov another factor controlling the flow characteristics and the forecast skill is the drainage area of the basin decreasing drainage area makes the basin hydrologically more responsive with lower times of concentration and the basin generates flashier peak flows with lesser seasonal patterns this deteriorates the forecast skill especially with the increasing lead times furthermore although the performance of dams in mediterranean climate zone is generally better than dams in other climates there is no remarkable trend as a function of local climate this in fact verifies the robustness of the selected ann architecture s learning ability across diverse hydro climatologic conditions the results also prove the global scalability of the proposed technique as the forecast performance was tested here for a wide range of hydrologically different dam sites as well as undammed river stations in large river basins of asia and in different climates a comparison of the proposed ann approach against simpler modeling techniques such as a linear regression or conceptual rainfall runoff model will further underscore its value and will be a focus of future study as the deterministic flow forecasts from ann model were not able to capture the inherent uncertainty present in the streamflow estimates an ensemble of forecast fields was assimilated into the trained ann to propagate the uncertainty the results revealed that the highest spread in ensemble flow forecasts occurs during the peak flow seasons when the inflow into reservoir is most uncertain this is the regime when the dam operators can benefit most from these forecasts providing the operating agencies with the ensemble forecasts during flood seasons can help increase the odds of operating for the flows of high probability of occurrence and improve the energy generation without risking the flood control the forecasting technique was then coupled with a reservoir optimization model for maximizing the hydropower generation without compromising flood control and dam safety a rolling horizon scheme was employed to obtain optimal release policy over the horizon of 1 7 days a benchmark policy obtained as look up table by neglecting the forecasts but optimized for energy generation provided a fair benchmark to compare hydropower benefits from the forecast based optimal operations the long term assessment over two years yielded significant benefit over the benchmark policy with reduced flood risk downstream short term ensemble forecasts were also explored in this study the reservoir operations optimization model was also simulated using the different scenarios min max and average of the flows over the forecast horizon the uncertainty in the release decisions was propagated by the corresponding spread of the ensemble forecast flows although the effect was undermined due to the continual updating of forecasts every day during the optimization process it should be mentioned that uncertainty spread in the ensemble flow forecasts which is highest at times of extreme peak flow also leads to the uncertain operating policy mostly during the peak flow times this implies that the use of ensemble forecasts can be most efficient during the seasons of high flow and should be avoided over drier periods the proposed ann scheme that is numerically fast and efficient for forecasting of reservoir inflow allowed an assessment over multiple years and multiple sites using computationally modest resources the proposed ann technique is appropriate for global scale operationalization of short term reservoir inflow forecasts for any dam site provided the training data are selected carefully with acute hydro climatic understanding of the basin the inputs used herein are freely and globally available such a technique also allows long term risk assessment of historical operations by directly incorporating the forecast fields from nwp models the optimization using pareto optimality concept allows the operator to choose an appropriate optimal solution depending on the prevailing circumstances and analyzing the tradeoff between the conflicting objectives future work will constitute exploring this concept for network of dams that are operated as a system such as the colorado or the columbia river system another future extension of this work is to connect energy demand forecasting and energy market pricing with the ann based reservoir operations optimization a key component towards operationalization is dissemination of the outcome into a form useful for the operating agencies and practitioners so that the results and advisory become locally actionable our future research plans to address these issues and report them in a future study appendix a ann performance with köppen geiger climate classes the climate classes used in this study are shown in the map below with the distribution of selected dam sites the markers for each site are sized with the nse of ann forecast at lead time of 1 day fig a1 map showing the köppen geiger climate classes used in this study and the selected dam sites with markers sized with nse of ann forecasted flow at lead time of 1 day fig a1 appendix b optimization model setup for pensacola dam constraints mathematical formulation we define the following constraints for the optimization model with the specific values summarized in table b1 1 release from the turbines is constrained by the turbine capacity p t u r b b 1 r p t p t u r b t 2 the system follows storage volume continuity water balance equation which requires that in each period t b 2 s t 1 s t i t l t r p t r n p t δ t t however as the optimization is performed at daily time steps δ t 1 the losses due to evaporation and seepage l t were ignored 3 reservoir storage was limited to ensure dam safety and avoid infeasible scenarios such as the reservoir running empty b 3 s m i n s t s m a x t here s m i n is set up considering the conservation pool of the reservoir to keep it at a suitable level above the inactive pool while s m a x denotes the active storage capacity till the reservoir s full pool 4 to prevent the downstream flooding hazards the total release was constrained to a maximum limit r m a x b 4 r p t r n p t r m a x t 5 to avoid excessive and infeasible rates of non power release via the spillway the non power release rate was limited to the spillway capacity b 5 r n p t s p i l l m a x t 6 lastly the releases made from reservoir should comply with the environmental flow limit q e n v b 6 r n p t r p t q e n v t table b 1 constraints for optimization for pensacola dam ok table b 1 constraint value comments turbine capacity 12 000 cfs spillway capacity 525 000 cfs minimum storage 1 265 000 ac ft 95 of 10 yr minimum maximum storage 2 021 679 ac ft flood control pool maximum release 50 000 cfs if level exceeds 750 ft otherwise 30 000 cfs avoid flooding at downstream station of neosho river env flow limit 500 cfs objective functions mathematical formulation the following two functions objective and penalty were defined for setting up the optimization problem 1 energy maximization minimize the deficit in hydroelectric energy production mwh from the maximum generation capacity of the powerplant b 7 min f 1 h p m a x t ε h f t h t t r p t δ t t u r b 2 penalty function minimize the sum of the absolute value of storage deviations from the rule curve specified value over a fixed number of days of the optimization horizon it is represented as b 8 min f 2 t s t t t t 3 4 7 d a y s if a flood event is forecasted causing the reservoir level to reach the flood storage pool the function is modified to minimize the uncontrolled spillway release b 9 min f 2 t r n p t t 1 2 3 7 d a y s where h p m a x maximum hydropower generation capacity h f reservoir forebay water level obtained from area elevation curve h t reservoir tailrace water level ε turbine efficiency r n p spillway non power release r p power turbine release δ t t u r b turbine operating hours t target rule curve specified storage s reservoir storage t time step days over the period of optimization software availability name of the software ann reservoir inflow forecasting developer shahryar khalique ahmad contact email skahmad uw edu contact address dept of civil and environmental engineering univ of washington usa software required matlab 2018b needs deep learning toolbox formerly neural network toolbox available since february 2019 availability matlab code and necessary dataset available at https github com shahryaramd ann flowforecasting data availability the type and source of the data set considered in this study name of dataset data source developer data format data availability 1 pensacola dam inflow time series us army corps of engineers http www swt wc usace army mil penscharts html html webpage free 2 forecast precipitation temperature windspeed noaa s global forecast system gfs model https www ncdc noaa gov data access model data model datasets global forcast system gfs grib files free 3 ensemble forecast precipitation temperature noaa s global forecast ensemble system reforecast gefs r model ftp ftp cdc noaa gov projects reforecast2 grib files free 4 in situ precipitation temperature global surface summary of the day gsod https data noaa gov dataset dataset global surface summary of the day gsod text files free 5 gridded historical precipitation livneh daily conus data https www esrl noaa gov psd data gridded data livneh html netcdf files free appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 008 
26176,traditional environmental model simulations often use archived data as inputs recent advancement of sensor web technologies in spatial data infrastructures sdis allows real time observations to be fed into models to generate live models a key challenge is how to efficiently process observation streams in models which is particularly important in time critical cases like disaster management this paper presents an observation stream computing model for live modelling which couples sensor web and models in stream computing environment to provide timely decision support information observation streams are proposed as information models to deal with observation stream processing the approach shows how mapreduce and apache spark stream processing can be leveraged to support coupling of observation streams and models the approach is applied in a disaster management case where in situ observation streams are processed to compute the waterlogging information in near real time the results illustrate applicability and effectiveness of the approach keywords stream computing sensor web environmental models spark spatial data infrastructure software availability program name emgeostreaming developer boyi shangguan contact address sgby whu edu cn year first required 2018 software required apache spark 2 1 0 or later apache kafka 2 11 programming language java 1 8 scala 2 11 program size 16 6 mb compressed source code availability https github com whu ypfamily emgeostreaming cost free of charge 1 introduction disaster risk management can benefit from geospatial technologies including geographic information systems gis and environmental modelling risk managers often gain decision support information from a modelling environment such as numerical modelling integrated with a gis goodchild et al 1992 however the provision of real time disaster decision support has been always a challenge availability of real time data and limited processing capability are among the main constraints zerger and smith 2003 batty 2016 as a result traditional environmental modelling and simulation often do not have a real time capability and require pre modelled data as inputs zerger and smith 2003 previous work has utilized high performance computing technologies to enhance the performance of environment modelling for example different kinds of high performance environmental models have been developed including hydrologic models subsurface models and landslides models jordi and wang 2012 freedman et al 2014 alvioli and baum 2016 these models could be coupled with top supercomputers such as mantle convection modelling on ibm bluegene rudi et al 2015 large scale nonlinear earthquake simulation modelling and atmospheric modelling on sunway taihulight ao et al 2017 fu et al 2017 recent advancement of sensor web and cloud computing technologies makes it possible to feed real time observations in spatial data infrastructures sdis conover et al 2010 yue et al 2015a it combines sensors computing and wireless communication with sensor networks and web technologies bröring et al 2011 by defining a suite of standard protocols and interfaces it provides access to sensors sensor networks and observations and integrate all of them into sdi ruiz et al 2003 zyl et al 2009 conover et al 2010 bröring et al 2011 thus it is possible to couple real time sensor observations with environmental models to generate live models for real time decision support both observation and modelling infrastructures could take advantage of web and cloud computing technologies laniak et al 2013 gao et al 2019 real time decision support information relies not only on the availability of data but also on the timely processing of these data observation streams from the sensor web can be considered as a kind of data streams produced by sensors babcock et al 2002 auger et al 2017 they need to be combined with timely processing to derive decision support information in models which is particularly important in time critical cases like the disaster management stream computing i e the processing and analysis of data streams in real or near real time has become a new processing paradigm in the big data era and it is widely used in many fields such as social networking financial analysis and internet of things ishii and suzumura 2011 yue et al 2016 ali et al 2017 the rapid development of stream computing technologies provides strong support for real time analytics of data streams thus it is helpful to combine stream computing with observation streams from sdi for timely decision support in urban disaster planning previous studies in the geospatial domain took advantage of the stream computing to process real time geospatial data kazemitabar et al 2011 galić et al 2017 ray et al 2017 for example researchers in the international business machines corporation ibm developed a scalable stream computing platform named ibm infosphere streams and applied it to process real time location based data from vehicle gps global positioning system to provide scalable real time and intelligent transportation services biem et al 2010 by extending the microsoft streaminsight stream query processing system researchers developed a system called geoinsight which enables interactive and extensive geo streaming data querying and demonstrated it with real time traffic data in los angeles county kazemitabar et al 2010 meanwhile microsoft researchers tried to combine the stream processing capability of microsoft streaminsight with the visualization capability of worldwide telescope to provide online visualization capability to manage process and analyze real time geospatial data ali et al 2011 the environmental systems research institute esri developed a stream service in the arcgis server product it is configured in a geoevent server allowing real time event based data streams to be disseminated and processed there are also some work on open solutions using open standards and open source stream computing platforms e g on how to improve the efficiency of stream processing for mobility data galić et al 2017 salmon and ray 2017 nevertheless how observation streams can work with environmental models within the cloud environment still needs to be investigated yue et al 2015b in this paper we present an observation stream computing model for coupling sensor observations with environmental models observation streams are transmitted into the stream computing supported environmental models to enable real time data computing capabilities or live models the open source software spark streaming zaharia et al 2012a is leveraged with sensor web open standards to provide discretized observation streams o streams which further facilitates the batch processing of observation streams and model simulations using the mapreduce paradigm the approach is applied in a disaster management case in which in situ observation streams are processed to compute the waterlogging information in near real time the results illustrate the applicability and effectiveness of the approach the rest of the paper is organized as follows section 2 introduces environmental modelling for urban waterlogging as a motivating example some background concepts are described in section 3 section 4 presents the observation stream computing model section 5 describes the walk through to derive waterlogging information implementation and result analysis are provided in section 6 finally section 7 presents the conclusion and suggestions for future work 2 the motivating example of waterlogging disaster environmental models play key roles in geographical process simulation whose goal is to dynamically show processes of geographical phenomena with gis technologies to provide decision support information the waterlogging disaster management is a typical case of geographical process simulation which is taken as the motivating example in this paper as one of the most destructive natural hazards the waterlogging disaster has been studied intensively and there is a number of simulation models available quan et al 2010 rozalis et al 2010 zhang and pan 2014 in general the simulation of waterlogging process could be split into three phases runoff generation flow concentration and water accumulation fig 1 2 1 runoff generation the runoff generation refers to the process of the water flow occurring on the ground surface after deducting all kinds of loss of rainfall beven 2004 the soil conservation service curve number scs cn is a widely used model for computation of runoff volume derived from precipitation mishra and singh 2013 it is an empirical model and has the advantages of simple construction low data requirement and a high degree of accuracy the core of scs cn is shown in formula 1 1 q p i a 2 p s i a p i a 0 p i a where q runoff volume mm p total precipitation volume mm i a initial abstraction of precipitation volume mm and s potential maximum retention mm it is often impossible to get the exact value of i a in real scenarios because this parameter could be influenced by various factors such as plant interception soil infiltration and depression detention therefore the model adopts an empirical formula formula 2 to calculate i a based on geological and climatic factors of the rainfall region 2 i a 0 2 s 3 s 25400 cn 254 in formula 3 c n is a dimensionless parameter which considers factors of the underlying surface that include but are not limited to soil types infiltration rate soil moisture and land use conditions intentionally in this study tests are run using fixed cn values see section 6 2 based on the assumption that these factors do not change in a certain region however it is worth acknowledging that these factors can change over time for example land use conditions as a consequence of anthropogenic impact on the natural environment the fixed cn value could be queried from existing records of engineering manuals rozalis et al 2010 by combining the formulas 1 2 and 3 we can get formula 4 to calculate the actual runoff volume 4 q p 0 2 25400 cn 254 2 p 0 8 25400 cn 254 p 0 2 25400 cn 254 0 p 0 2 25400 cn 254 2 2 flow concentration the flow concentration represents the process of water flow concentration on the ground surface in a certain area in order to calculate the total volume of final concentrated water flow the models in formula 5 and 6 can be used todini 1988 zhang and pan 2014 5 w q 1000 a d 6 d d t n where w the concentrated water flow volume m3 q the runoff volume calculated from formula 4 mm a the area of the rainfall region m2 d the total drainage volume m3 that is calculated from formula 6 d the average drainage volume per second m3 s t the cumulative rainfall time s and n the number of drainage outlets 2 3 water accumulation the water accumulation is the process of concentrated water flow retained in the low lying areas of the rainfall region using the volume of the concentrated water flow w and digital elevation model dem data of the rainfall region the surface elevation of standing water e w can be calculated through a gis identical bulk model according to formula 7 yin et al 2011 7 w i 1 n e w e g i δ σ 0 where n the dem cell numbers e g i the elevation of each c e l l i in the dem and δ σ the area of each cell then by comparing the water surface elevation with the terrain elevation information about waterlogging area and water depth could be derived in summary the input and output parameters of the waterlogging disaster model are shown in table 1 clearly if real time update of the precipitation volume in the rainfall event could be fed into the model in real time or near real time the model could produce timely decision support information for the waterlogging disaster management this capability would make this model a live model reflecting the most recent situation such a purpose can be achieved by using an efficient stream computing approach 3 background this section introduces two basic technologies that are related to our approach sensor web and stream computing 3 1 sensor web the concept of sensor web was born at u s national aeronautics and space administration nasa jet propulsion laboratory in 1997 where it was used to describe a novel wireless sensor network architecture in which individual pieces could act and coordinate as a whole delin and jackson 1999 since 2006 the open geospatial consortium ogc has been trying to develop a sensor web enablement swe framework by combining service oriented architecture soa with sensor networks and setting a series of specifications for sensor web service interfaces and information models botts et al 2007 nowadays with the development of sensor web original swe framework swe 1 0 has already been evolved to a new generation swe framework swe 2 0 bröring et al 2011 2012 which provides a significant step forward to provide technical supports for sharing and interoperation of sensor web resources table 2 shows some typical information models and service models of swe 2 0 bröring et al 2011 2012 information models are sets of systematic theories and methods for the encoding of sensor observations and processes for example observation measurement o m defines a domain independent and conceptual model and provides a series of xml schema definitions xsds for the representation of measurement features and sensor observations service models comprise a suit of standards specifying interfaces of sensor web services like sos that offers a web service interface allowing the access to sensor observations and sensor metadata in summary the sensor web technology provides the support for the interoperability of distributed observations and sensors the interoperability makes it possible to build plug in and play stream computing system on top of observation streams 3 2 stream computing stream computing has been identified as a significant type of data processing in the big data era ishii and suzumura 2011 data come as a continuous stream rather than in full for processing the huge volume of data arriving as streams and its sophisticated processing or mining often require distributed computation engine for big data there are already numbers of stream computing engines available such as apache flink storm and spark yet there is no clear winner at this point some have lower latencies some have higher latencies yet higher throughput chintapalli et al 2016 among them spark streaming apache 2018a could achieve a reasonable latency while keeping a high throughput e g it can process 100 k events in 1 s in a benchmark case chintapalli et al 2016 for this reason the spark streaming is selected to undertake the research presented in this paper the spark streaming is a typical stream computing system which is an extension to the spark the spark is an in memory cloud computing system which was originally developed by the uc berkeley amp lab in 2009 and became one of apache projects in 2010 layered on the mapreduce computing framework dean and ghemawat 2008 a novel core data abstraction named resilient distributed datasets rdds is proposed in the spark it is a read only collection of data objects partitioned across the large computer cluster and enables in memory computation in a fault tolerant manner zaharia et al 2012b for example the operation rdd map passes each dataset element with a function and returns a new rdd representing results in the memory and the operation rdd reduce aggregates all elements of the rdd using a function and returns the final result in recent years spark also provides new data types i e dataframe and dataset to allow more convenient data operations using sparksql these data could be transformed into rdd the key idea behind the spark streaming is to regard stream computing as a series of deterministic batch computations on small batch intervals with a programming model named the discretized streams d streams zaharia et al 2012a the d stream consists of a serial of rdds ordered by time and enables mapreduce operations to be run in each batch the spark streaming offers strong consistency efficient fault recovery and easy integration with the mapreduce framework a typical workflow of the spark streaming is shown in fig 2 at first the data stream as an input to the system is batched based on the duration set by users as a result the d stream that contains rdd of each batch is generated then the computation process of each rdd is created by parsing computation expressions of the d stream assigned by users lastly in each batch duration the computation of the rdd is transformed into spark tasks distributed in the cluster and results of each batch can be provided as outputs in near real time by taking advantage of the high performance of cluster computing 4 an observation stream computing model the observation stream from sensor web can be fed into the stream computing framework by using a set of extended rdd types which constitute a distributed in memory model for observation stream computing this section describes how observation stream can be modelled section 4 1 and how the model can provide in house support for stream computing section 4 2 a theoretical analysis of the performance is provided in section 4 3 to help understand how to achieve low latency in observation stream computing finally section 4 4 presents the framework for observation stream computing that couples sensor web and stream computing systems 4 1 information model fig 3 shows the organization of the observation stream together with the information model described by uml unified modelling language diagram by extending the d stream of the spark streaming the o stream is designed to represent the continuous observation data stream from the sensor web with the properties of the batch duration and the data stream source o stream divides the observation data stream into series of batches of observation data where each batch of observation is represented using the o rdd fig 4 the o rdd is a kind of the rdd for distributed management of observations from the observation data stream the attributes fromtime and untiltime are used to describe the part of the observation data stream that the observations come from as shown in fig 5 the o rdd offers capabilities to manage observations using a set of data partitions each data partition contains a collection of observation objects observation 1 observation 2 etc and can be processed in parallel the observation follows the observation concept defined in the ogc o m 2 0 standard which encompasses the following attributes bröring et al 2011 tomkins and lowe 2016 1 the result the measurement data of the observation that can be any kind of type 2 the phenomenon time a time object indicates when the observation was produced 3 the feature of interest a real world feature that is observed for example an area or a river 4 the observed property a property type object describing the property of the observed feature for example air temperature or precipitation 5 the procedure a sensor instrument or computation that produces the observation the operator is the abstraction of the processing of observation or o rdd it can be extended to the o rdd operator value operator spatial operator or any other user defined operators for instance the o rdd operator defines processing at the o rdd level for example grouping observations by keys a value operator defines the processing of values of observations a spatial operator defines a spatial processing of observing features for example spatial buffer processing or spatial overlay analysis the observation filter o filter is used to define the restrictions that observations need to satisfy before further processing the o filter has a property the filter operator which can be any kind of filter operators such as the spatial filter temporal filter value filter or composite filter the event model consists of a series of operators that constitute the computation model of the event by importing the event model thomas and johannes 2008 into the o stream all observations are processed and the results of the event processing are obtained for instance a runoff generation event model contains 1 an o rdd operator to group and average the precipitation observations having the same feature of interest 2 a value operator to process observations to compute the runoff of each region 4 2 computing model with the information model in section 4 1 we can accommodate two computing models to process the observation data stream filter computing and event computing the filter computing of the o stream is used to filter all observations in the data stream by a defined o filter to decide which observations can be further processed in the filtered o stream as shown in fig 6 it sends the o filter object to all o rdds that are generated upon the arrival of observations the filter computing will be performed in a distributed manner 1 the o filter object is copied and broadcasted to each data partition of the o rdd which manages a part of observations in a local computing executor 2 then data partitions filter observations in parallel thus the overall processing time is reduced 3 after the filter tasks of all the data partitions are complete the data are updated and a filtered o rdd is generated the event computing of the o stream is a more complex one that processes each observation with an event model to get the information of a specific event and transforms the o stream into an event o stream it sends the parameter an event model to all o rdds in the o stream and executes the event processing in each o rdd as shown in fig 7 1 when the o rdd receives an event model object it iterates through each operator of the event model in order and an event o rdd is generated finally 2 if the operator is the o rdd operator the o rdd is processed directly based on the operator 3 otherwise if the operator aims to process each observation object this is copied and broadcasted to each data partition in the o rdd and data partitions execute the operator respectively 4 then a new o rdd is generated and the starting o rdd object is updated for the next operation in the event model fig 8 introduces the whole computing workflow of the o stream to process the observation data stream from the sensor web and get real time environmental event results first an o filter is created and set to filter observations in the o stream then an event model which contains a set of processing methods on how to derive event information from observations is created and set to produce event results finally the event results generated in each batch duration are output for decision making all processing steps follow the mapreduce paradigm so that the whole computing procedure can achieve a high performance as well as good extensibility 4 3 performance evaluation model the primary performance consideration of the stream computing system is to achieve the low latency for timely processing of observation streams while this requirement could be satisfied by means of elastic computing feature of the cloud computing environment e g increasing the computing resources another consideration is to achieve cost effective resource usage while meeting the latency requirement in this section the first step is to define the latency of the stream computing system i e the average time delay of the stream data processing that can be calculated by formula 8 8 t i m e d e l a y t i m e b a t c h p r o c e s s t i m e b a t c h d u r a t i o n where the t i m e b a t c h d u r a t i o n denotes the time interval between two process batches and the t i m e b a t c h p r o c e s s denotes the average processing time of o rdd in each batch the following three constraints need to be considered for an efficient stream computing system 1 the t i m e b a t c h p r o c e s s is less than the t i m e b a t c h d u r a t i o n which means that the system should have enough time to process the current batch of data before the next batch of data is generated 2 the t i m e d e l a y is less than the maximum allowed value t i m e a l l o w e d d e l a y and the smaller the better which means that the system should have a latency as low as possible and meet the delay time requirements of users 3 the computing resource usage c o r e u s e d is less than the available resources c o r e a v a i l a b l e and the smaller the better which means the system should be computing resource conserving with these constraints we can define the performance evaluation model according to formula 9 to help find strategies to improve the performance of the observation stream computing the performance takes into consideration both latency and resource consumption and the strategies rely on the adjustment of the computing parameters including t i m e b a t c h i n t e r v a l and c o r e u s e d in the computing environment 9 p t i m e s i n g l e p r o c e s s t i m e b a t c h p r o c e s s c o r e u s e d t i m e b a t c h p r o c e s s t i m e b a t c h d u r a t i o n t i m e d e l a y t i m e a l l o w e d d e l a y c o r e u s e d c o r e a v a i l a b l e t i m e b a t c h p r o c e s s f n u m b a t c h d a t a c o r e u s e d n u m b a t c h d a t a r a t e d a t a i n p u t t i m e b a t c h d u r a t i o n p is the parallel efficiency defined as the speedup t i m e s i n g l e p r o c e s s t i m e b a t c h p r o c e s s divided by the number of units c o r e u s e d of execution the higher is p better it is the traditional concept of parallel efficiency is enriched with other constraints in formula 9 to meet the requirements in stream computing t i m e s i n g l e p r o c e s s is the average processing time when only a single core is used in each batch duration t i m e b a t c h p r o c e s s is determined by n u m b a t c h d a t a and c o r e u s e d in the distributed computing environment n u m b a t c h d a t a is the average amount of data in each batch which is derived by the r a t e d a t a i n p u t that represents the data input rate multiplied by t i m e b a t c h d u r a t i o n table 3 shows how the model works with regard to cases characterized by different values of the variables t i m e b a t c h d u r a t i o n and t i m e b a t c h p r o c e s s in the same stream computing environment t i m e s i n g l e p r o c e s s is tested in case 0 with t i m e b a t c h d u r a t i o n of 30s it can be found that case 1 does not meet the constraint that the processing time should be less than the batch duration so the performance of this case is worst in case 2 when t i m e b a t c h d u r a t i o n is set as 30s and c o r e u s e d is set as 8 the value of t i m e b a t c h p r o c e s s is 21s and p 2 is calculated as 0 893 based on formula 9 similarly in case 3 we can get p 3 as 0 833 by comparing values of p 2 and p 3 it is obvious that case 2 has better performance in the stream computing than case 3 in terms of the combination of latency and resource consumption 4 4 system framework the framework not only couples interoperable sensor web modules into the system but also allows running of environmental models into the cloud computing environment the standards in sensor web make the system interoperable reusable extensible and evolvable observation streams from different sensors can be shared and plugged in with less effort the environmental models are enacted in the cloud computing environment for stream computing which changes the traditional way to use computing resources and makes the modelling scaling on demand fig 9 shows the observation stream computing framework it includes four main modules sensor web environmental models data stream and o stream computing modules 4 4 1 sensor web module within the framework the sensor web module provides web service based way to access available sensors and interoperable observation data in the sensor web heterogeneous sensors are connected in sensor networks and accessible through web protocols the sensors and observations are managed and offered based on information and service models defined in standards 4 4 2 environmental model module the environmental model module is a set of algorithms or models for environmental simulation and analysis the algorithms refer to traditional geospatial analysis algorithms and models could be domain specific computational models here we use environmental models to refer to both algorithms and computational models they are treated as kinds of processing that can operate on observation data to identify event or derive decision support information 4 4 3 data stream module the role of data stream module is to transform sensor observations into the stream ready to be consumed by the stream computing system the sensor observations accessible through sensor web services are records encoded in extensible markup language xml and javascript object notation json the stream computing system needs streams of records therefore the module will create real time streaming data pipelines that reliably transfer data from sensor web to stream computing systems in the module observation data extractors access interfaces of sensor web services at each time step to obtain records on real time observation data observation data translators parse different encodings xml json etc of records and transform them into observations according to the information model of the o rdd all these observations are ordered by time and published by data stream producers to generate streams of observations 4 4 4 o stream computing module the o stream computing module works as a data stream consumer by subscribing observation data streams from the data stream module it generates an o stream in the stream computing system whenever observation data are published by data stream producers they are accessed and managed by the o stream the computing module loads models from the environmental model module processes the observation stream using its information and computing models proposed in section 4 1 and 4 2 and derives timely decision support information using the stream computing environment 5 walk through for waterlogging information derivation the use case of waterlogging disaster management described in section 2 is used to illustrate the approach the walk through example includes the transformation of environmental models as a set of operators in the event model section 5 1 and enactment of o stream computing section 5 2 5 1 computing of the waterlogging event model fig 10 shows the waterlogging event model computing in the o rdd the environmental models including runoff generation flow concentration and water accumulation are transformed into operators in the event model the event model is broadcasted to distributed nodes to compute waterlogging information for each data partition in parallel each data partition is a sub region of the whole spatial region affected by the waterlogging disaster the decomposition of the datasets into multiple computing units is called data parallelism which has been studied extensively in the literature and is applicable for the waterlogging case in this case data are divided into independent sub regions using a watershed analysis to facilitate the parallel processing gong and xie 2009 once the computation of each computing unit containing the data partition is complete the results are merged and the near real time waterlogging information in a batch duration can be obtained 5 2 enactment of o stream computing the use case can be developed by taking advantage of the information model in section 4 1 as an application programming interface api to facilitate observation stream processing fig 11 provides a workflow of o stream processing which includes three main steps 6 implementation in this section we introduce the implementation of a prototype system section 6 1 for waterlogging disaster management based on the model and framework proposed in section 4 experimental analysis and discussion are given in section 6 2 6 1 prototype system the prototype system is developed on a computer cluster with two inspure nf5270 server machines each server machine being equipped with ubuntu 14 04 5 lts operation system 24 cpu cores and 93 4 gb read only memory rom table 4 provides the details of hardware and software packages used in the prototype system the meteorological sensor web module in the prototype system is implemented with the opensensorhub opensensorhub 2018 an open standards based software platform able to support virtually any sensor following the ogc swe 2 0 standards opensensorhub provides off the shelf management of sensor web resources and offers standard service interfaces to access the resources as shown in fig 12 all meteorological sensors can be added into the sensorhub and their observations data are accessible in an interoperable manner the observation data stream is managed by apache kafka apache 2018b which is a distributed and scalable publish subscribe message system and can offer high data throughput to receive manage and send data streams distinguished by different kinds of topics kreps et al 2011 a sensorhub client application fig 13 using apis of apache kafka and sensor web services is developed to extract precipitation observation data from sensor web translate observation data into observation records and publish them to the waterlogging topic which is managed by apache kafka to produce precipitation observation data stream the stream computing of waterlogging information is deployed in a spark cluster fig 14 it continuously consumes precipitation data stream from the waterlogging topic of apache kafka and efficiently filters and computes observation data with environmental models to derive waterlogging information in near real time 6 2 experimental analysis and discussion to demonstrate the prototype system we select an experimental region covering around 1 46 square kilometers in hubei china this area has a low terrain averagely of 31 86 m ranging from 23 76 m to 37 95 m in these conditions waterlogging is a natural hazard process that can easily occur the experiment simulates a waterlogging disaster occurring in the region caused by a continuous rainstorm the 5 m resolution simulated dem data of the experimental region is divided into sub regions using a watershed analysis jenson and domingue 1988 which allows the decomposition of the data into independent computing units to facilitate the parallel processing gong and xie 2009 fig 15 shows that the rainfall region was divided into 70 sub regions during the experiment other required parameter values of models for each sub region have been prepared in advance including cn value area of the sub region and drainage volume the parameter values are stored and managed by a spatial database postgis during the experiment we adopted a simplified scenario which assumes that each sub region is deployed with a sensor providing simulated observations then a sensor web with 70 virtual meteorological sensors has been set up in the opensensorhub and simulated observations of precipitation are continually generated every minute when the system runs the sensorhub client continuously accesses precipitation observations from the meteorological sensor web in the opensensorhub and publishes these observations under the waterlogging topic of the apache kafka to create a precipitation observation stream from all sensors observations from each sensor in the stream is later grouped by sensor ids that are bound to specific sub regions by subscribing and receiving the data stream of this topic the stream computing application for waterlogging running on the spark cluster generates a precipitation o stream with the batch duration as 1 min using the o stream computing and waterlogging event model the waterlogging information of the region is computed and updated every minute as shown in fig 16 the change of the waterlogging information in the region over time can be derived and visualized to support timely decision making furthermore to analyze the performance of the prototype system the average processing time of each batch duration is tested in terms of different data partitions and cluster computing resources data partition is the basic processing task in the spark cluster in the test we firstly divide precipitation observations of the experimental region into 4 data partitions case 1 and acquire average processing time with different cluster computing resources then we make the same experiment with 70 data partitions case 2 which equal the number of sub regions the test results are compared in fig 17 it can be observed that 1 when the application runs on a single node with 1 cpu core the average processing time for each batch in both case 1 and 2 is higher than the time interval of each batch 60s this means that we need to increase the computing resources case 1 takes less time than case 2 this is due to the higher communication cost of the latter case 2 with the increase of computing resources the average processing time decreases quickly more data partitions can be higher is the degree of processing parallelization that can be achieved when the number of available cores in each node reaches 4 the average processing time in case 1 tends to be stable differently the average processing time in case 2 becomes stable with 16 cores in each node this situation can be explained because the computational resource requirement of the task is too small and computation over each partition could be completed very quickly in this situation the communication cost becomes the major factor in the time cost and so the time cost does not decrease as the cores increase 3 both case 1 and 2 could take much less time than the batch duration case 1 could reach 15 6s and case 2 could reach as low as 4 1s that means we could reach real time processing as soon as the data come in by increasing the computing power meanwhile the increase of data partitions could reduce effectively the average processing time and improve the performance of the system we also test these cases against the performance evaluation model described in section 4 3 which considers both latency and resource consumption with t i m e a l l o w e d d e l a y set to 90s c o r e a v a i l a b l e set to 48 the parallel efficiency p see formula 9 is calculated based on t i m e a l l o w e d d e l a y t i m e sin g l e pr o c e s s t i m e b a t c h p r o c e s s and c o r e u s e d the results are shown in fig 18 according to formula 8 and 9 t i m e d e l a y is higher than t i m e a l l o w e d d e l a y when c o r e u s e d is 1 and 2 so it is not qualified for formula 9 and we have not included this situation in fig 18 it can be found that 1 the performance of case 2 is better than the performance of case 1 2 when the processing time reduces slowly as the number of cores increases the performance degrades because the computing power of the existing cores is not fully used 3 the prototype system achieves the best performance when the c o r e u s e d is set to 4 in case 2 in this situation the computing capability of each core can be fully utilized compared with situations assigned with more cores the results show that the stream computing can provide a live modelling environment by accommodating real time observations in environmental modelling and computing over observations and models in near real time for time critical model simulation the approach matches the goal of sdi in that it utilizes the open geospatial standards i e sensor web standards for heterogeneous resource sharing in distributed information environments the stream computing component is a solid block in the cloud computing which has become a cornerstone technology for developing the sdi the scalable on demand cost effective and timely processing of incoming observation streams using elastic cloud computing provides a powerful computing infrastructure for sdi environmental modelling could benefit more from the stream computing environment it has been suggested that cloud computing and web service technologies will improve traditional modelling approaches by greatly reducing the cost and time at the end user side laniak et al 2013 resource intensive tasks could be moved to the server side in this paper environmental models are enacted in the cloud computing environment and implemented as mapreduce functions that are incorporated into operators in the information model this helps the creation of a model cloud environment however the web based access of models as services in the cloud is still not considered and discussed in this paper nativi et al 2013 and we intend to leave it as a future work 7 conclusions and future work this paper presents an approach for feeding real time observation data of the sensor web into environmental models with stream computing technologies to generate live models the approach makes it possible to derive timely decision support information in time critical environmental events a case on waterlogging disaster management is used to illustrate the approach in the paper an interoperable extensible and scalable framework is proposed to couple sensor web with environment model in a stream computing environment real time observation data stream is received and processed with environmental models to get near real time information based on a distributed in memory model for observation stream computing the o rdd information model working as an api can provide an observation stream computing interface that is connected to the sensor web standards the rdd concept is extended to support observations to provide off the shelf observation processing thus it helps hide the underlying mapreduce programming api bridges the sensor web and stream computing and provides a geospatial cloud middleware for observation stream processing the framework is implemented and exploited in a waterlogging disaster management case in which a waterlogging event computation model is build and real time precipitation observation stream produced by meteorological sensors are processed timely by the stream computing framework to get the waterlogging event information a prototype system is developed and demonstrated using a simulated waterlogging disaster event the results show the applicability effectiveness and extensibility of the system there are still some works that can be investigated in the future the waterlogging model in the paper is still a simplified model some more complex environment models could be explored to further demonstrate the applicability of the approach in addition the performance and efficiency of stream computing framework could be further explored against different observation streams we will also investigate how to access models as services in the cloud to improve the interoperability and accessibility of models on the web acknowledgements we appreciate the reviewers and editors for their constructive comments that helped improve the quality of the paper the work was supported by major state research development program of china no 2017yfb0503704 national natural science foundation of china no 41722109 61825103 91738302 hubei provincial natural science foundation of china no 2018cfa053 nature science foundation innovation group project of hubei province china no 2016cfa003 and wuhan yellow crane talents science program 2016 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 009 
26176,traditional environmental model simulations often use archived data as inputs recent advancement of sensor web technologies in spatial data infrastructures sdis allows real time observations to be fed into models to generate live models a key challenge is how to efficiently process observation streams in models which is particularly important in time critical cases like disaster management this paper presents an observation stream computing model for live modelling which couples sensor web and models in stream computing environment to provide timely decision support information observation streams are proposed as information models to deal with observation stream processing the approach shows how mapreduce and apache spark stream processing can be leveraged to support coupling of observation streams and models the approach is applied in a disaster management case where in situ observation streams are processed to compute the waterlogging information in near real time the results illustrate applicability and effectiveness of the approach keywords stream computing sensor web environmental models spark spatial data infrastructure software availability program name emgeostreaming developer boyi shangguan contact address sgby whu edu cn year first required 2018 software required apache spark 2 1 0 or later apache kafka 2 11 programming language java 1 8 scala 2 11 program size 16 6 mb compressed source code availability https github com whu ypfamily emgeostreaming cost free of charge 1 introduction disaster risk management can benefit from geospatial technologies including geographic information systems gis and environmental modelling risk managers often gain decision support information from a modelling environment such as numerical modelling integrated with a gis goodchild et al 1992 however the provision of real time disaster decision support has been always a challenge availability of real time data and limited processing capability are among the main constraints zerger and smith 2003 batty 2016 as a result traditional environmental modelling and simulation often do not have a real time capability and require pre modelled data as inputs zerger and smith 2003 previous work has utilized high performance computing technologies to enhance the performance of environment modelling for example different kinds of high performance environmental models have been developed including hydrologic models subsurface models and landslides models jordi and wang 2012 freedman et al 2014 alvioli and baum 2016 these models could be coupled with top supercomputers such as mantle convection modelling on ibm bluegene rudi et al 2015 large scale nonlinear earthquake simulation modelling and atmospheric modelling on sunway taihulight ao et al 2017 fu et al 2017 recent advancement of sensor web and cloud computing technologies makes it possible to feed real time observations in spatial data infrastructures sdis conover et al 2010 yue et al 2015a it combines sensors computing and wireless communication with sensor networks and web technologies bröring et al 2011 by defining a suite of standard protocols and interfaces it provides access to sensors sensor networks and observations and integrate all of them into sdi ruiz et al 2003 zyl et al 2009 conover et al 2010 bröring et al 2011 thus it is possible to couple real time sensor observations with environmental models to generate live models for real time decision support both observation and modelling infrastructures could take advantage of web and cloud computing technologies laniak et al 2013 gao et al 2019 real time decision support information relies not only on the availability of data but also on the timely processing of these data observation streams from the sensor web can be considered as a kind of data streams produced by sensors babcock et al 2002 auger et al 2017 they need to be combined with timely processing to derive decision support information in models which is particularly important in time critical cases like the disaster management stream computing i e the processing and analysis of data streams in real or near real time has become a new processing paradigm in the big data era and it is widely used in many fields such as social networking financial analysis and internet of things ishii and suzumura 2011 yue et al 2016 ali et al 2017 the rapid development of stream computing technologies provides strong support for real time analytics of data streams thus it is helpful to combine stream computing with observation streams from sdi for timely decision support in urban disaster planning previous studies in the geospatial domain took advantage of the stream computing to process real time geospatial data kazemitabar et al 2011 galić et al 2017 ray et al 2017 for example researchers in the international business machines corporation ibm developed a scalable stream computing platform named ibm infosphere streams and applied it to process real time location based data from vehicle gps global positioning system to provide scalable real time and intelligent transportation services biem et al 2010 by extending the microsoft streaminsight stream query processing system researchers developed a system called geoinsight which enables interactive and extensive geo streaming data querying and demonstrated it with real time traffic data in los angeles county kazemitabar et al 2010 meanwhile microsoft researchers tried to combine the stream processing capability of microsoft streaminsight with the visualization capability of worldwide telescope to provide online visualization capability to manage process and analyze real time geospatial data ali et al 2011 the environmental systems research institute esri developed a stream service in the arcgis server product it is configured in a geoevent server allowing real time event based data streams to be disseminated and processed there are also some work on open solutions using open standards and open source stream computing platforms e g on how to improve the efficiency of stream processing for mobility data galić et al 2017 salmon and ray 2017 nevertheless how observation streams can work with environmental models within the cloud environment still needs to be investigated yue et al 2015b in this paper we present an observation stream computing model for coupling sensor observations with environmental models observation streams are transmitted into the stream computing supported environmental models to enable real time data computing capabilities or live models the open source software spark streaming zaharia et al 2012a is leveraged with sensor web open standards to provide discretized observation streams o streams which further facilitates the batch processing of observation streams and model simulations using the mapreduce paradigm the approach is applied in a disaster management case in which in situ observation streams are processed to compute the waterlogging information in near real time the results illustrate the applicability and effectiveness of the approach the rest of the paper is organized as follows section 2 introduces environmental modelling for urban waterlogging as a motivating example some background concepts are described in section 3 section 4 presents the observation stream computing model section 5 describes the walk through to derive waterlogging information implementation and result analysis are provided in section 6 finally section 7 presents the conclusion and suggestions for future work 2 the motivating example of waterlogging disaster environmental models play key roles in geographical process simulation whose goal is to dynamically show processes of geographical phenomena with gis technologies to provide decision support information the waterlogging disaster management is a typical case of geographical process simulation which is taken as the motivating example in this paper as one of the most destructive natural hazards the waterlogging disaster has been studied intensively and there is a number of simulation models available quan et al 2010 rozalis et al 2010 zhang and pan 2014 in general the simulation of waterlogging process could be split into three phases runoff generation flow concentration and water accumulation fig 1 2 1 runoff generation the runoff generation refers to the process of the water flow occurring on the ground surface after deducting all kinds of loss of rainfall beven 2004 the soil conservation service curve number scs cn is a widely used model for computation of runoff volume derived from precipitation mishra and singh 2013 it is an empirical model and has the advantages of simple construction low data requirement and a high degree of accuracy the core of scs cn is shown in formula 1 1 q p i a 2 p s i a p i a 0 p i a where q runoff volume mm p total precipitation volume mm i a initial abstraction of precipitation volume mm and s potential maximum retention mm it is often impossible to get the exact value of i a in real scenarios because this parameter could be influenced by various factors such as plant interception soil infiltration and depression detention therefore the model adopts an empirical formula formula 2 to calculate i a based on geological and climatic factors of the rainfall region 2 i a 0 2 s 3 s 25400 cn 254 in formula 3 c n is a dimensionless parameter which considers factors of the underlying surface that include but are not limited to soil types infiltration rate soil moisture and land use conditions intentionally in this study tests are run using fixed cn values see section 6 2 based on the assumption that these factors do not change in a certain region however it is worth acknowledging that these factors can change over time for example land use conditions as a consequence of anthropogenic impact on the natural environment the fixed cn value could be queried from existing records of engineering manuals rozalis et al 2010 by combining the formulas 1 2 and 3 we can get formula 4 to calculate the actual runoff volume 4 q p 0 2 25400 cn 254 2 p 0 8 25400 cn 254 p 0 2 25400 cn 254 0 p 0 2 25400 cn 254 2 2 flow concentration the flow concentration represents the process of water flow concentration on the ground surface in a certain area in order to calculate the total volume of final concentrated water flow the models in formula 5 and 6 can be used todini 1988 zhang and pan 2014 5 w q 1000 a d 6 d d t n where w the concentrated water flow volume m3 q the runoff volume calculated from formula 4 mm a the area of the rainfall region m2 d the total drainage volume m3 that is calculated from formula 6 d the average drainage volume per second m3 s t the cumulative rainfall time s and n the number of drainage outlets 2 3 water accumulation the water accumulation is the process of concentrated water flow retained in the low lying areas of the rainfall region using the volume of the concentrated water flow w and digital elevation model dem data of the rainfall region the surface elevation of standing water e w can be calculated through a gis identical bulk model according to formula 7 yin et al 2011 7 w i 1 n e w e g i δ σ 0 where n the dem cell numbers e g i the elevation of each c e l l i in the dem and δ σ the area of each cell then by comparing the water surface elevation with the terrain elevation information about waterlogging area and water depth could be derived in summary the input and output parameters of the waterlogging disaster model are shown in table 1 clearly if real time update of the precipitation volume in the rainfall event could be fed into the model in real time or near real time the model could produce timely decision support information for the waterlogging disaster management this capability would make this model a live model reflecting the most recent situation such a purpose can be achieved by using an efficient stream computing approach 3 background this section introduces two basic technologies that are related to our approach sensor web and stream computing 3 1 sensor web the concept of sensor web was born at u s national aeronautics and space administration nasa jet propulsion laboratory in 1997 where it was used to describe a novel wireless sensor network architecture in which individual pieces could act and coordinate as a whole delin and jackson 1999 since 2006 the open geospatial consortium ogc has been trying to develop a sensor web enablement swe framework by combining service oriented architecture soa with sensor networks and setting a series of specifications for sensor web service interfaces and information models botts et al 2007 nowadays with the development of sensor web original swe framework swe 1 0 has already been evolved to a new generation swe framework swe 2 0 bröring et al 2011 2012 which provides a significant step forward to provide technical supports for sharing and interoperation of sensor web resources table 2 shows some typical information models and service models of swe 2 0 bröring et al 2011 2012 information models are sets of systematic theories and methods for the encoding of sensor observations and processes for example observation measurement o m defines a domain independent and conceptual model and provides a series of xml schema definitions xsds for the representation of measurement features and sensor observations service models comprise a suit of standards specifying interfaces of sensor web services like sos that offers a web service interface allowing the access to sensor observations and sensor metadata in summary the sensor web technology provides the support for the interoperability of distributed observations and sensors the interoperability makes it possible to build plug in and play stream computing system on top of observation streams 3 2 stream computing stream computing has been identified as a significant type of data processing in the big data era ishii and suzumura 2011 data come as a continuous stream rather than in full for processing the huge volume of data arriving as streams and its sophisticated processing or mining often require distributed computation engine for big data there are already numbers of stream computing engines available such as apache flink storm and spark yet there is no clear winner at this point some have lower latencies some have higher latencies yet higher throughput chintapalli et al 2016 among them spark streaming apache 2018a could achieve a reasonable latency while keeping a high throughput e g it can process 100 k events in 1 s in a benchmark case chintapalli et al 2016 for this reason the spark streaming is selected to undertake the research presented in this paper the spark streaming is a typical stream computing system which is an extension to the spark the spark is an in memory cloud computing system which was originally developed by the uc berkeley amp lab in 2009 and became one of apache projects in 2010 layered on the mapreduce computing framework dean and ghemawat 2008 a novel core data abstraction named resilient distributed datasets rdds is proposed in the spark it is a read only collection of data objects partitioned across the large computer cluster and enables in memory computation in a fault tolerant manner zaharia et al 2012b for example the operation rdd map passes each dataset element with a function and returns a new rdd representing results in the memory and the operation rdd reduce aggregates all elements of the rdd using a function and returns the final result in recent years spark also provides new data types i e dataframe and dataset to allow more convenient data operations using sparksql these data could be transformed into rdd the key idea behind the spark streaming is to regard stream computing as a series of deterministic batch computations on small batch intervals with a programming model named the discretized streams d streams zaharia et al 2012a the d stream consists of a serial of rdds ordered by time and enables mapreduce operations to be run in each batch the spark streaming offers strong consistency efficient fault recovery and easy integration with the mapreduce framework a typical workflow of the spark streaming is shown in fig 2 at first the data stream as an input to the system is batched based on the duration set by users as a result the d stream that contains rdd of each batch is generated then the computation process of each rdd is created by parsing computation expressions of the d stream assigned by users lastly in each batch duration the computation of the rdd is transformed into spark tasks distributed in the cluster and results of each batch can be provided as outputs in near real time by taking advantage of the high performance of cluster computing 4 an observation stream computing model the observation stream from sensor web can be fed into the stream computing framework by using a set of extended rdd types which constitute a distributed in memory model for observation stream computing this section describes how observation stream can be modelled section 4 1 and how the model can provide in house support for stream computing section 4 2 a theoretical analysis of the performance is provided in section 4 3 to help understand how to achieve low latency in observation stream computing finally section 4 4 presents the framework for observation stream computing that couples sensor web and stream computing systems 4 1 information model fig 3 shows the organization of the observation stream together with the information model described by uml unified modelling language diagram by extending the d stream of the spark streaming the o stream is designed to represent the continuous observation data stream from the sensor web with the properties of the batch duration and the data stream source o stream divides the observation data stream into series of batches of observation data where each batch of observation is represented using the o rdd fig 4 the o rdd is a kind of the rdd for distributed management of observations from the observation data stream the attributes fromtime and untiltime are used to describe the part of the observation data stream that the observations come from as shown in fig 5 the o rdd offers capabilities to manage observations using a set of data partitions each data partition contains a collection of observation objects observation 1 observation 2 etc and can be processed in parallel the observation follows the observation concept defined in the ogc o m 2 0 standard which encompasses the following attributes bröring et al 2011 tomkins and lowe 2016 1 the result the measurement data of the observation that can be any kind of type 2 the phenomenon time a time object indicates when the observation was produced 3 the feature of interest a real world feature that is observed for example an area or a river 4 the observed property a property type object describing the property of the observed feature for example air temperature or precipitation 5 the procedure a sensor instrument or computation that produces the observation the operator is the abstraction of the processing of observation or o rdd it can be extended to the o rdd operator value operator spatial operator or any other user defined operators for instance the o rdd operator defines processing at the o rdd level for example grouping observations by keys a value operator defines the processing of values of observations a spatial operator defines a spatial processing of observing features for example spatial buffer processing or spatial overlay analysis the observation filter o filter is used to define the restrictions that observations need to satisfy before further processing the o filter has a property the filter operator which can be any kind of filter operators such as the spatial filter temporal filter value filter or composite filter the event model consists of a series of operators that constitute the computation model of the event by importing the event model thomas and johannes 2008 into the o stream all observations are processed and the results of the event processing are obtained for instance a runoff generation event model contains 1 an o rdd operator to group and average the precipitation observations having the same feature of interest 2 a value operator to process observations to compute the runoff of each region 4 2 computing model with the information model in section 4 1 we can accommodate two computing models to process the observation data stream filter computing and event computing the filter computing of the o stream is used to filter all observations in the data stream by a defined o filter to decide which observations can be further processed in the filtered o stream as shown in fig 6 it sends the o filter object to all o rdds that are generated upon the arrival of observations the filter computing will be performed in a distributed manner 1 the o filter object is copied and broadcasted to each data partition of the o rdd which manages a part of observations in a local computing executor 2 then data partitions filter observations in parallel thus the overall processing time is reduced 3 after the filter tasks of all the data partitions are complete the data are updated and a filtered o rdd is generated the event computing of the o stream is a more complex one that processes each observation with an event model to get the information of a specific event and transforms the o stream into an event o stream it sends the parameter an event model to all o rdds in the o stream and executes the event processing in each o rdd as shown in fig 7 1 when the o rdd receives an event model object it iterates through each operator of the event model in order and an event o rdd is generated finally 2 if the operator is the o rdd operator the o rdd is processed directly based on the operator 3 otherwise if the operator aims to process each observation object this is copied and broadcasted to each data partition in the o rdd and data partitions execute the operator respectively 4 then a new o rdd is generated and the starting o rdd object is updated for the next operation in the event model fig 8 introduces the whole computing workflow of the o stream to process the observation data stream from the sensor web and get real time environmental event results first an o filter is created and set to filter observations in the o stream then an event model which contains a set of processing methods on how to derive event information from observations is created and set to produce event results finally the event results generated in each batch duration are output for decision making all processing steps follow the mapreduce paradigm so that the whole computing procedure can achieve a high performance as well as good extensibility 4 3 performance evaluation model the primary performance consideration of the stream computing system is to achieve the low latency for timely processing of observation streams while this requirement could be satisfied by means of elastic computing feature of the cloud computing environment e g increasing the computing resources another consideration is to achieve cost effective resource usage while meeting the latency requirement in this section the first step is to define the latency of the stream computing system i e the average time delay of the stream data processing that can be calculated by formula 8 8 t i m e d e l a y t i m e b a t c h p r o c e s s t i m e b a t c h d u r a t i o n where the t i m e b a t c h d u r a t i o n denotes the time interval between two process batches and the t i m e b a t c h p r o c e s s denotes the average processing time of o rdd in each batch the following three constraints need to be considered for an efficient stream computing system 1 the t i m e b a t c h p r o c e s s is less than the t i m e b a t c h d u r a t i o n which means that the system should have enough time to process the current batch of data before the next batch of data is generated 2 the t i m e d e l a y is less than the maximum allowed value t i m e a l l o w e d d e l a y and the smaller the better which means that the system should have a latency as low as possible and meet the delay time requirements of users 3 the computing resource usage c o r e u s e d is less than the available resources c o r e a v a i l a b l e and the smaller the better which means the system should be computing resource conserving with these constraints we can define the performance evaluation model according to formula 9 to help find strategies to improve the performance of the observation stream computing the performance takes into consideration both latency and resource consumption and the strategies rely on the adjustment of the computing parameters including t i m e b a t c h i n t e r v a l and c o r e u s e d in the computing environment 9 p t i m e s i n g l e p r o c e s s t i m e b a t c h p r o c e s s c o r e u s e d t i m e b a t c h p r o c e s s t i m e b a t c h d u r a t i o n t i m e d e l a y t i m e a l l o w e d d e l a y c o r e u s e d c o r e a v a i l a b l e t i m e b a t c h p r o c e s s f n u m b a t c h d a t a c o r e u s e d n u m b a t c h d a t a r a t e d a t a i n p u t t i m e b a t c h d u r a t i o n p is the parallel efficiency defined as the speedup t i m e s i n g l e p r o c e s s t i m e b a t c h p r o c e s s divided by the number of units c o r e u s e d of execution the higher is p better it is the traditional concept of parallel efficiency is enriched with other constraints in formula 9 to meet the requirements in stream computing t i m e s i n g l e p r o c e s s is the average processing time when only a single core is used in each batch duration t i m e b a t c h p r o c e s s is determined by n u m b a t c h d a t a and c o r e u s e d in the distributed computing environment n u m b a t c h d a t a is the average amount of data in each batch which is derived by the r a t e d a t a i n p u t that represents the data input rate multiplied by t i m e b a t c h d u r a t i o n table 3 shows how the model works with regard to cases characterized by different values of the variables t i m e b a t c h d u r a t i o n and t i m e b a t c h p r o c e s s in the same stream computing environment t i m e s i n g l e p r o c e s s is tested in case 0 with t i m e b a t c h d u r a t i o n of 30s it can be found that case 1 does not meet the constraint that the processing time should be less than the batch duration so the performance of this case is worst in case 2 when t i m e b a t c h d u r a t i o n is set as 30s and c o r e u s e d is set as 8 the value of t i m e b a t c h p r o c e s s is 21s and p 2 is calculated as 0 893 based on formula 9 similarly in case 3 we can get p 3 as 0 833 by comparing values of p 2 and p 3 it is obvious that case 2 has better performance in the stream computing than case 3 in terms of the combination of latency and resource consumption 4 4 system framework the framework not only couples interoperable sensor web modules into the system but also allows running of environmental models into the cloud computing environment the standards in sensor web make the system interoperable reusable extensible and evolvable observation streams from different sensors can be shared and plugged in with less effort the environmental models are enacted in the cloud computing environment for stream computing which changes the traditional way to use computing resources and makes the modelling scaling on demand fig 9 shows the observation stream computing framework it includes four main modules sensor web environmental models data stream and o stream computing modules 4 4 1 sensor web module within the framework the sensor web module provides web service based way to access available sensors and interoperable observation data in the sensor web heterogeneous sensors are connected in sensor networks and accessible through web protocols the sensors and observations are managed and offered based on information and service models defined in standards 4 4 2 environmental model module the environmental model module is a set of algorithms or models for environmental simulation and analysis the algorithms refer to traditional geospatial analysis algorithms and models could be domain specific computational models here we use environmental models to refer to both algorithms and computational models they are treated as kinds of processing that can operate on observation data to identify event or derive decision support information 4 4 3 data stream module the role of data stream module is to transform sensor observations into the stream ready to be consumed by the stream computing system the sensor observations accessible through sensor web services are records encoded in extensible markup language xml and javascript object notation json the stream computing system needs streams of records therefore the module will create real time streaming data pipelines that reliably transfer data from sensor web to stream computing systems in the module observation data extractors access interfaces of sensor web services at each time step to obtain records on real time observation data observation data translators parse different encodings xml json etc of records and transform them into observations according to the information model of the o rdd all these observations are ordered by time and published by data stream producers to generate streams of observations 4 4 4 o stream computing module the o stream computing module works as a data stream consumer by subscribing observation data streams from the data stream module it generates an o stream in the stream computing system whenever observation data are published by data stream producers they are accessed and managed by the o stream the computing module loads models from the environmental model module processes the observation stream using its information and computing models proposed in section 4 1 and 4 2 and derives timely decision support information using the stream computing environment 5 walk through for waterlogging information derivation the use case of waterlogging disaster management described in section 2 is used to illustrate the approach the walk through example includes the transformation of environmental models as a set of operators in the event model section 5 1 and enactment of o stream computing section 5 2 5 1 computing of the waterlogging event model fig 10 shows the waterlogging event model computing in the o rdd the environmental models including runoff generation flow concentration and water accumulation are transformed into operators in the event model the event model is broadcasted to distributed nodes to compute waterlogging information for each data partition in parallel each data partition is a sub region of the whole spatial region affected by the waterlogging disaster the decomposition of the datasets into multiple computing units is called data parallelism which has been studied extensively in the literature and is applicable for the waterlogging case in this case data are divided into independent sub regions using a watershed analysis to facilitate the parallel processing gong and xie 2009 once the computation of each computing unit containing the data partition is complete the results are merged and the near real time waterlogging information in a batch duration can be obtained 5 2 enactment of o stream computing the use case can be developed by taking advantage of the information model in section 4 1 as an application programming interface api to facilitate observation stream processing fig 11 provides a workflow of o stream processing which includes three main steps 6 implementation in this section we introduce the implementation of a prototype system section 6 1 for waterlogging disaster management based on the model and framework proposed in section 4 experimental analysis and discussion are given in section 6 2 6 1 prototype system the prototype system is developed on a computer cluster with two inspure nf5270 server machines each server machine being equipped with ubuntu 14 04 5 lts operation system 24 cpu cores and 93 4 gb read only memory rom table 4 provides the details of hardware and software packages used in the prototype system the meteorological sensor web module in the prototype system is implemented with the opensensorhub opensensorhub 2018 an open standards based software platform able to support virtually any sensor following the ogc swe 2 0 standards opensensorhub provides off the shelf management of sensor web resources and offers standard service interfaces to access the resources as shown in fig 12 all meteorological sensors can be added into the sensorhub and their observations data are accessible in an interoperable manner the observation data stream is managed by apache kafka apache 2018b which is a distributed and scalable publish subscribe message system and can offer high data throughput to receive manage and send data streams distinguished by different kinds of topics kreps et al 2011 a sensorhub client application fig 13 using apis of apache kafka and sensor web services is developed to extract precipitation observation data from sensor web translate observation data into observation records and publish them to the waterlogging topic which is managed by apache kafka to produce precipitation observation data stream the stream computing of waterlogging information is deployed in a spark cluster fig 14 it continuously consumes precipitation data stream from the waterlogging topic of apache kafka and efficiently filters and computes observation data with environmental models to derive waterlogging information in near real time 6 2 experimental analysis and discussion to demonstrate the prototype system we select an experimental region covering around 1 46 square kilometers in hubei china this area has a low terrain averagely of 31 86 m ranging from 23 76 m to 37 95 m in these conditions waterlogging is a natural hazard process that can easily occur the experiment simulates a waterlogging disaster occurring in the region caused by a continuous rainstorm the 5 m resolution simulated dem data of the experimental region is divided into sub regions using a watershed analysis jenson and domingue 1988 which allows the decomposition of the data into independent computing units to facilitate the parallel processing gong and xie 2009 fig 15 shows that the rainfall region was divided into 70 sub regions during the experiment other required parameter values of models for each sub region have been prepared in advance including cn value area of the sub region and drainage volume the parameter values are stored and managed by a spatial database postgis during the experiment we adopted a simplified scenario which assumes that each sub region is deployed with a sensor providing simulated observations then a sensor web with 70 virtual meteorological sensors has been set up in the opensensorhub and simulated observations of precipitation are continually generated every minute when the system runs the sensorhub client continuously accesses precipitation observations from the meteorological sensor web in the opensensorhub and publishes these observations under the waterlogging topic of the apache kafka to create a precipitation observation stream from all sensors observations from each sensor in the stream is later grouped by sensor ids that are bound to specific sub regions by subscribing and receiving the data stream of this topic the stream computing application for waterlogging running on the spark cluster generates a precipitation o stream with the batch duration as 1 min using the o stream computing and waterlogging event model the waterlogging information of the region is computed and updated every minute as shown in fig 16 the change of the waterlogging information in the region over time can be derived and visualized to support timely decision making furthermore to analyze the performance of the prototype system the average processing time of each batch duration is tested in terms of different data partitions and cluster computing resources data partition is the basic processing task in the spark cluster in the test we firstly divide precipitation observations of the experimental region into 4 data partitions case 1 and acquire average processing time with different cluster computing resources then we make the same experiment with 70 data partitions case 2 which equal the number of sub regions the test results are compared in fig 17 it can be observed that 1 when the application runs on a single node with 1 cpu core the average processing time for each batch in both case 1 and 2 is higher than the time interval of each batch 60s this means that we need to increase the computing resources case 1 takes less time than case 2 this is due to the higher communication cost of the latter case 2 with the increase of computing resources the average processing time decreases quickly more data partitions can be higher is the degree of processing parallelization that can be achieved when the number of available cores in each node reaches 4 the average processing time in case 1 tends to be stable differently the average processing time in case 2 becomes stable with 16 cores in each node this situation can be explained because the computational resource requirement of the task is too small and computation over each partition could be completed very quickly in this situation the communication cost becomes the major factor in the time cost and so the time cost does not decrease as the cores increase 3 both case 1 and 2 could take much less time than the batch duration case 1 could reach 15 6s and case 2 could reach as low as 4 1s that means we could reach real time processing as soon as the data come in by increasing the computing power meanwhile the increase of data partitions could reduce effectively the average processing time and improve the performance of the system we also test these cases against the performance evaluation model described in section 4 3 which considers both latency and resource consumption with t i m e a l l o w e d d e l a y set to 90s c o r e a v a i l a b l e set to 48 the parallel efficiency p see formula 9 is calculated based on t i m e a l l o w e d d e l a y t i m e sin g l e pr o c e s s t i m e b a t c h p r o c e s s and c o r e u s e d the results are shown in fig 18 according to formula 8 and 9 t i m e d e l a y is higher than t i m e a l l o w e d d e l a y when c o r e u s e d is 1 and 2 so it is not qualified for formula 9 and we have not included this situation in fig 18 it can be found that 1 the performance of case 2 is better than the performance of case 1 2 when the processing time reduces slowly as the number of cores increases the performance degrades because the computing power of the existing cores is not fully used 3 the prototype system achieves the best performance when the c o r e u s e d is set to 4 in case 2 in this situation the computing capability of each core can be fully utilized compared with situations assigned with more cores the results show that the stream computing can provide a live modelling environment by accommodating real time observations in environmental modelling and computing over observations and models in near real time for time critical model simulation the approach matches the goal of sdi in that it utilizes the open geospatial standards i e sensor web standards for heterogeneous resource sharing in distributed information environments the stream computing component is a solid block in the cloud computing which has become a cornerstone technology for developing the sdi the scalable on demand cost effective and timely processing of incoming observation streams using elastic cloud computing provides a powerful computing infrastructure for sdi environmental modelling could benefit more from the stream computing environment it has been suggested that cloud computing and web service technologies will improve traditional modelling approaches by greatly reducing the cost and time at the end user side laniak et al 2013 resource intensive tasks could be moved to the server side in this paper environmental models are enacted in the cloud computing environment and implemented as mapreduce functions that are incorporated into operators in the information model this helps the creation of a model cloud environment however the web based access of models as services in the cloud is still not considered and discussed in this paper nativi et al 2013 and we intend to leave it as a future work 7 conclusions and future work this paper presents an approach for feeding real time observation data of the sensor web into environmental models with stream computing technologies to generate live models the approach makes it possible to derive timely decision support information in time critical environmental events a case on waterlogging disaster management is used to illustrate the approach in the paper an interoperable extensible and scalable framework is proposed to couple sensor web with environment model in a stream computing environment real time observation data stream is received and processed with environmental models to get near real time information based on a distributed in memory model for observation stream computing the o rdd information model working as an api can provide an observation stream computing interface that is connected to the sensor web standards the rdd concept is extended to support observations to provide off the shelf observation processing thus it helps hide the underlying mapreduce programming api bridges the sensor web and stream computing and provides a geospatial cloud middleware for observation stream processing the framework is implemented and exploited in a waterlogging disaster management case in which a waterlogging event computation model is build and real time precipitation observation stream produced by meteorological sensors are processed timely by the stream computing framework to get the waterlogging event information a prototype system is developed and demonstrated using a simulated waterlogging disaster event the results show the applicability effectiveness and extensibility of the system there are still some works that can be investigated in the future the waterlogging model in the paper is still a simplified model some more complex environment models could be explored to further demonstrate the applicability of the approach in addition the performance and efficiency of stream computing framework could be further explored against different observation streams we will also investigate how to access models as services in the cloud to improve the interoperability and accessibility of models on the web acknowledgements we appreciate the reviewers and editors for their constructive comments that helped improve the quality of the paper the work was supported by major state research development program of china no 2017yfb0503704 national natural science foundation of china no 41722109 61825103 91738302 hubei provincial natural science foundation of china no 2018cfa053 nature science foundation innovation group project of hubei province china no 2016cfa003 and wuhan yellow crane talents science program 2016 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 009 
26177,accurate river bathymetry is required for applications including hydrodynamic flow modelling and understanding morphological processes bathymetric measurements are typically a set of depths at discrete points that must be reconstructed into a continuous surface a number of algorithms exist for this reconstruction including spline based techniques and kriging methods a novel and efficient method is introduced to produce a co ordinate system fitted to the river path suitable for bathymetric reconstructions the method is based on numerical conformal mapping and can handle topological features such as islands and branches in the river bathymetric surfaces generated using interpolation over a conformal map are compared to spline based and kriging methods on a section of the balonne river australia the results show that the conformal mapping algorithm produces reconstructions comparable in quality to existing methods preserves flow wise features and is relatively insensitive to the number of sample points enabling faster data collection in the field keywords river bathymetry spatial interpolation depth reconstruction conformal mapping 1 introduction knowledge of the depth and flow velocity of a river is crucial for applications including ecosystem management flood risk assessment and emergency operations fewtrell et al 2011 neal et al 2015 river flow patterns also drive sediment erosion and deposition oxygen exchange and pollutant mixing importantly sediment erosion can cause levee failure and bridge scour while sediment deposition is responsible for reservoir siltation moreover knowledge of flow dynamics oxygen availability and pollutant concentration are essential for aquatic and fish habitat ecological assessment marzadri et al 2014 benjankar et al 2015 vesipa et al 2017 while hydrodynamic models can be used to predict river flow dynamics the accuracy of these models heavily depends on the quality of the bathymetric input data horritt et al 2006 legleiter et al 2011a b several state of the art techniques exist for the measurement of river bathymetry for example point cloud river depth measurements can be obtained using multibeam sonar nittrouer et al 2008 airborne bathymetric light detection and ranging lidar systems mckean et al 2009 2014 or optical remote sensing imaging methods fonstad et al 2005 legleiter et al 2009 legleiter 2012 2013 2015 pan et al 2015 however each of these sampling techniques is constrained by technical limitations use of multibeam sonar is restricted to water depths typically deeper than 4 m and limited by navigation hazards glenn et al 2016 consequently these instruments are mainly used for coastal and marine applications zhi et al 2014 water turbidity bottom material composition and water surface roughness affect the retrieval of bathymetry with lidar systems and remote sensing imaging techniques meaning that the use these techniques is restricted to clear shallow a few meter deep waters with highly reflective substrates and no surface waves legleiter et al 2011a b 2016 legleiter 2012 abdallah et al 2013 kinzel et al 2013 mckean et al 2014 cheng et al 2015 consequently most reach scale studies rely on ground based surveys of river transects accomplished using sonar equipment mounted to small watercrafts altenau et al 2017 krüger et al 2018 these methods are time consuming and expensive but can be applied to shallow deep clear and turbid waters and provide both centimetre resolution and accuracy glenn et al 2016 all techniques for depth measurement result in a set of discrete points that require spatial interpolation algorithms to derive a representation of the river bathymetry a large number of interpolation algorithms have been proposed for river bathymetry and terrain topography reconstruction in the literature with extensive reviews and comparative evaluations provided by merwade et al 2006 chaplot et al 2006 and li et al 2011 li et al 2014 typical approaches for the interpolation of point cloud measurements include natural neighbour triangular irregular networks inverse distance weighting idw spline interpolation and various forms of kriging importantly when interpolating sparse transects special care must be taken to account for anisotropic trends in the river channel bathymetry tomczak 1998 merwade et al 2006 merwade 2009 and many previous studies used a channel fitted coordinate system s n for this purpose fukuoka et al 1973 goff et al 2004 merwade et al 2005 glenn et al 2016 lai et al 2018 in this approach the channel centre line or the channel thalweg that is the line connecting the deepest point in each cross section are used as reference where the s coordinate is the distance along this line and the n coordinate is the distance across the channel from the reference line additionally a number of modified point cloud interpolation techniques have been proposed in order to combine ease of implementation with the need to adequately reconstruct river anisotropy using sparse transect data examples include anisotropic idw tomczak 1998 zonal idw burroughes et al 2001 ellipsoidal idw merwade et al 2006 and rectilinear idw andes et al 2017 other approaches include linear interpolation bounded by user defined break lines schäppi et al 2010 customised spline interpolation algorithms flanagin et al 2007 caviedes voullième et al 2014 and radial basis functions curtarelli et al 2015 however these algorithms require calibration or user interaction furthermore it was shown that irregularly spaced data can generate excessive interpolation errors and overshooting of elevation values in steep slope areas li et al 2008 caviedes voullième et al 2014 reconstruction accuracy generally varies widely between particular case studies and input datasets hindering general conclusions on a recommended methodology and the expected performance of each method li et al 2011 gathered information from over 80 studies and showed the difficulty of providing absolute conclusions by demonstrating that the evaluation strategy impacted the results of the comparison between interpolation methods however in terms of some specific studies krüger et al 2018 compared idw radial basis functions and kriging and found that the latter had higher sub meter accuracy batista et al 2017 pointed out that depending on the river reach kriging algorithm resulted in root mean square error rmse values between 0 5 m and 3 m legleiter et al 2008 showed that the rmse values resulting from the application of kriging algorithm increased from 0 1 m to 0 5 m when cross section spacing increased from 7 m to 56 m this analysis highlighted the large sensibility of kriging accuracy to the input dataset even in a reconfigured channel with a relatively simple morphology rather than relying on simple geometric considerations some authors have also attempted to incorporate geomorphic and hydraulic considerations into the interpolation algorithm for instance legleiter et al 2008 and batista et al 2017 tested the use of kriging algorithms while zhang et al 2016 suggested incorporating a preliminary estimation of flow velocity into the topography interpolator in a related approach lai et al 2018 used the laplace equation to generate flow streamlines despite a higher complexity of these algorithms the accuracy of the reconstructed surface strictly depends on the characteristics of the measured dataset reconstruction for complicated morphologies with islands or confluences usually requires extensive manual intervention for processing separate features must be disaggregated by drawing break lines or two dimensional polygons of island boundaries surface interpolation is then performed separately for each area delimited by two break lines or polygons which is a customised and time consuming process brasington et al 2000 merwade et al 2008 schäppi et al 2010 costabile et al 2015 however braided rivers are found across a large range of climates such as glacial areas to arid regions and physiographic settings such as from steep mountain areas to low coastal plains e g from steep mountain areas to low coastal plains surian 2015 consistent research efforts are being made to improve the modelling and understanding of flow dynamics in these complex morphologies costelloe et al 2006 piégay et al 2009 mohammadi et al 2013 jarihani et al 2015 williams et al 2016 altenau et al 2017 a reconstruction algorithm which is loosely affected by the characteristics of the input dataset and can provide reliable reconstruction of river bathymetric surfaces even with sparse and incomplete measurement points could allow greater flexibility during data collection this paper presents such a method capable of reconstructing complicated morphologies the method is based on a two stage process a novel application of conformal mapping ahlfors 1979 to provide a co ordinate system fitted to the river geometry and a subsequent interpolation over this co ordinate system to provide the reconstruction the method could be automated as it requires only a mask of the land and water areas to generate the river fitted co ordinate system and a set of depth sampled points for the subsequent interpolation the proposed algorithm was applied for the reconstruction of a 3 km long reach of the balonne river queensland australia for which a bathymetric field dataset was available the process was compared to two other interpolation methods spline interpolation and kriging to assess the overall effectiveness ease of use and computational efficiency of the method compared to these commonly used approaches the resulting reconstructions were evaluated using qualitative plan view and longitudinal evaluations as well as a quantitative comparison of quantities such as storage volume and cross section flow area furthermore both this complete dataset and three artificially reduced datasets derived from the complete dataset were used in the reconstructions algorithms to understand the effect of data sparsity on the resulting reconstructed surfaces the analysis demonstrated that the overall process of conformal map generation with an associated interpolation provided reconstructions of comparable quality to existing methods but could be entirely automated furthermore interpolation over the river fitted co ordinate system was found to be less sensitive to the quality of the input dataset interpolation than the kriging and spline algorithms allowing potentially greater flexibility during data collection 2 existing methods 2 1 spline based algorithms spline interpolation algorithms attempt to preserve the information content of each measurement by producing an interpolation surface that passes exactly through the input points these algorithms require the river to be partitioned into user defined regions along break lines or transects fig 1 a to handle complex river shapes interpolation algorithms are then used to connect points measured along cross sections in both the stream wise and cross stream directions piecewise cubic splines in the stream wise direction and linear depth profiles in the cross stream direction can result in a smooth reconstructed surface if each cross section has an equivalent number of data points flanagin et al 2007 caviedes voullième et al 2014 a cubic spline based interpolation method was implemented in this study for the purpose of comparison the path of the river in between the transects in the stream wise direction s is described using a cubic bezier curve given by the equation 1 p s 1 s 3 p 0 3 1 s 2 t p 1 3 1 s s 2 p 2 s 3 p 3 where 0 s 1 this corresponds to a smooth curve which goes from the two dimensional points p 0 to p 3 towards p 1 and p 2 from each end fig 1b and c the procedure for mapping a given curvilinear coordinate s t to a world coordinate x y is to firstly determine the world coordinate point s p 0 and p 3 using the expressions p 0 1 t t n s t a r t t t n e n d 2 p 3 1 t t n 1 s t a r t t t n 1 e n d where t s t a r t and t e n d are the transect start and end points in world coordinates respectively at the current t n and next t n 1 transect the points p 1 and p 2 are calculated using p 1 p 0 1 2 p 03 n n 3 p 2 p 3 1 2 p 03 n n 1 where n are the normals of the current n n and next n n 1 transect while p 03 is the displacement between p 0 and p 3 the reverse process of mapping given world coordinates x y to curvilinear coordinates s t is difficult as there is no closed form expression for this inversion in this study a nonlinear optimisation process was implemented which sought to determine s t by minimising the distance error 4 e r r o r x y f s t the downhill simplex method nelder et al 1965 was used for this process if the coordinates exceeded the bounds s 0 s 1 t 0 and t 1 during the optimisation the positions calculated by the cubic bezier curve expressions were taken to be erroneous in this case the coordinates of the nearest constrained value s v a l i d t v a l i d and the following modified error expression were used such that 5 e r r o r x y f s v a l i d t v a l i d p 03 s t s v a l i d t v a l i d this modification penalises the s t coordinates for exceeding their bounds and allows the unconstrained optimisation method to work the overall approach was found to robustly determine all s t coordinates for the river transects considered in this study 2 2 kriging kriging is a geostatistical method that can be used for geospatial interpolation through the analysis of the spatial structure of observed data points the kriging method minimizes the variance of the prediction error at every interpolated point using a minimum mean squared error method the main strengths of kriging are the statistical quality of its predictions i e unbiasedness and its ability to predict the spatial distribution of uncertainty mitas 2005 one of the first applications of kriging for river bathymetry reconstruction was accomplished by carter et al 1997 while eriksson et al 2000 suggested the use of two dimensional semi variogram models in order to account for data anisotropy a number of variations of the kriging algorithm e g ordinary kriging simple kriging universal kriging co kriging regression kriging kriging with a trend stratified kriging with a trend kriging with external drift have since been applied for river reconstruction hilldale et al 2008 legleiter et al 2008 bailly et al 2010 cheng et al 2015 su et al 2015 batista et al 2017 with a number of comparative studies showing the merit of using kriging over simpler techniques such as idw radial basis functions and polynomial interpolation merwade et al 2006 maleika et al 2012 curtarelli et al 2015 ferreira et al 2017 su et al 2017 krüger et al 2018 some authors proposed variants of kriging to achieve an accurate representation of data anisotropy more specifically te stroet et al 2005 formulated a local anisotropy kriging lak for automatic detection of structures within the data which resulted in sub meter average absolute error with a decrease of 23 compared to ordinary kriging boisvert et al 2009 presented a locally varying anisotropy lva kriging algorithm based on non euclidean distances to reconstruct complex geological sites and magneron et al 2010 proposed an algorithm to integrate prior knowledge and locally varying parameters a general form of a kriging estimator can be written as 6 y 0 ˆ x α 1 n λ α x y x α μ x where y 0 ˆ x is the reconstructed surface at point x estimated using n known sample values y x α of the field at the locations x α while μ x is the mean function of the field and λ α x are weights determined in the kriging algorithm to determine these weights an experimental semi variogram γ e h must be computed 7 γ e h 1 2 n h α 1 n h y x α h y x α 2 where n h is the number of pairs of sampled locations separated by a lag distance h wackernagel 2003 this experimental variogram is evaluated for the pairs of sampled values by computing the squared difference between the sampled values and evaluating the variogram cloud against the lag distance h between the sampled locations usually it is observed that the value of an experimental variogram increases with separation distance and eventually reaches a saturation level referred to as the sill once an experimental variogram has been obtained it is modelled using a chosen continuous theoretical variogram γ t h the three most commonly used theoretical variogram models are the spherical model the exponential model and the gaussian model wackernagel 2003 a spherical model for the theoretical variogram was found to best represent the experimental variogram obtained from the data used in this study with the form 8 y t h r 1 5 h a 0 5 h a 3 i f h a r i f h a in order to replace the experimental variogram with the theoretical model a least square levenberg marquardt method was employed to fit the free parameter a in eq 8 once the theoretical variogram was fitted to the sampled data the semi variance for all the sampled locations could be computed using the theoretical variogram there is currently no common agreement on the optimal kriging formulation for river bathymetry and universal ordinary and simple kriging methods are often used maleika et al 2012 krüger et al 2018 the main difference between these three methods is the assumption of the mean function μ x in the kriging estimator simple kriging uses a given mean ordinary kriging has an unknown but constant mean and universal kriging has a spatially varying unknown mean the universal kriging method was used in this study as river bathymetry is typically not geostationary which is an assumption for ordinary and simple kriging cressie 1993 and wackernagel 2003 provide further details on these methods in universal kriging it is assumed that the random function is a linear combination of the mean drift μ x α and a nonstationary error e x α such that 9 y x α μ x α e x α for this application it can be considered that the river depth is a random function y x α sampled irregularly over the domain d with x α ε d the mean drift m x α can be regarded as a regular continuous variation of y x α whereas the nonstationary error e x α signifies the random small scale fluctuations it can also be said that the mean drift is the non stationary expectation of the random function y x α such that 10 e y x α μ x α and the nonstationary error has a zero expectation e e x α 0 for universal kriging the mean drift can be estimated as a k basis function of the spatial coordinates with either a linear quadratic or a higher order form in this study the mean drift was parametrized using the linear form 11 e y x α μ x α a 0 a 1 x 1 x α a 2 x 2 x α l 0 k a l f l x α where a 0 a 1 a 2 are the unknown coefficients of the linear equation and x 1 and x 2 are the longitude and latitude at the location x α respectively the coefficient terms in eq 11 were obtained using a least squares levenberg marquardt curve fitting method once an equation for the trend was selected and fitted to the sampled data the residual values were obtained as the deviation of observed values from the fitted values and are given as 12 r x α y x α μ x α these residuals were then used to obtain the experimental variogram using the semi variance defined as 13 γ ˆ r h 1 2 n h α 1 n h r x α h r x α 2 where the hat on γ represents a residual the experimental variogram eq 13 was used in eq 8 to find the free parameter a and the resulting fitted theoretical variogram was utilized in the kriging prediction the kriging prediction can be expressed in matrix notation as 14 y r x α λ r x α finally the weights defined by λ were obtained by solving 15 λ 1 λ 2 λ n a 0 a 1 a k c i j f l x α f l x α 0 1 c 10 c 20 c n 0 f 0 x 0 f 1 x 0 f k x 0 where c i j were obtained by evaluating the residual variogram for data to data and c i 0 from data to un sampled once the kriging prediction for the residuals was obtained it was added to the values obtained by evaluating the trend for the sampled locations to obtain the kriging estimate for bathymetric reconstructions the elevation data was found to be skewed away from a normal distribution towards lower depth values to transform the data back into a normal distribution required for the kriging algorithm a box cox log normal or normal score transformation can be used here we used the normal score transformation where the cumulative distribution of the elevation data was transformed to a normal distribution we chose to use the normal score transformation as both box cox and log normal transformation can result in deviations in the tail of the distribution whereas the normal score transformation gave an exact transformation to a normal distribution 3 reconstruction using conformal mapping 3 1 conformal mapping the new reconstruction algorithm presented in this study consists of three main steps these are 1 fitting a dimensionless conformal co ordinate system s t to the river 2 interpolating the bathymetry from known data points within this conformal co ordinate system and 3 mapping this interpolation back to real world projected coordinates x y the mapping is called conformal as it preserves the angle between orthogonal lines for example a cartesian projected co ordinate system with lines in x and y still has a 90 angle between the cartesian grid lines in s and t a schematic of the reconstruction algorithm is shown in fig 2 a given channel in projected x y co ordinates fig 2a is supplied or converted to a rasterised mask of cells containing either water or land fig 2b these are then processed into two sets of boundary conditions for s and t the values of s and t represent the stream wise and cross stream distance respectively the key component of the reconstruction algorithm is to generate the conformal map this is numerically calculated using a method based on complex functions a holomorphic complex function f z defines a conformal mapping 16 f z s x y i t x y where the two real functions s x y and t x y must obey the cauchy riemann equations s x t y 17 s y t x the functions s x y and t x y are also harmonic which means that they are the solutions to a pair of two dimensional laplace equations the laplace equation is widely used in physics more specifically in areas such as electromagnetism fluid flow classical gravitation and heat transfer the equation in two dimensions is given by 2 ϕ x y 0 where ϕ x y is a scalar physically the laplacian of a field 2 ϕ x y represents the curvature of the field so the laplace equation for ϕ x y provides the scalar field for which the curvature of ϕ x y is zero for eq 16 the pair of laplace equations is 2 s x y 0 18 2 t x y 0 the cauchy riemann equations can be used to determine the required boundary equations for the solution of eq 18 in order for the mapping to be conformal at the boundary the gradient of u is required to be zero at the banks of the river and the gradient of v to be zero at the upstream and downstream boundaries of the river these conditions are identical to zero flux neumann boundary conditions such that 19 s n 0 t t 0 where n is the normal direction vector at the banks of the river and t is the normal direction vector at the upstream and downstream boundaries substituting this requirement into eq 17 results in the straightforward condition on t to be an arbitrary constant at the banks of the river and s to be an arbitrary constant at the upstream and downstream boundaries of the river these constants were chosen as s 1 at the upstream boundary and s 1 at the downstream boundary fig 2c with t 1 at one side of the channel and t 1 at the other fig 2e the choice of values was purely for ease of use in interpolating algorithms and any other constants could be chosen 3 2 cross stream boundary conditions the boundary conditions for s are straightforward to implement but the boundary conditions for t are more complicated to impose as the mask contains no directional information for which side of the channel is which the cross stream boundary conditions have been implemented using two methods in this study the first is using a cross stream distance map which is relatively straightforward to implement and the second uses a more complex image analysis method which can account for branching and islands detailed in the next section a distance map is a rasterised map containing the scalar distance to the nearest interface here the nearest channel bank distance maps are very widely used in geographic information system gis analysis and can be rapidly constructed using a number of algorithms including the level set or fast marching method sethian 1999 in this study a level set method was used as this was found to be more stable and robust than the fast marching method cells in the map contain a scalar value representing the distance from the banks of the channel d or a null indicator indicating that the cell lies outside the river channel the initial distance value is set to d 1 if points lie outside the river area d 1 if the points are within the river area or d 0 for the edge case of the point exactly on the river bank the distance function can be constructed by solving the eikonal equation d 1 sethian 2001 with the given initial condition the level set method solves this for d by converting the eikonal equation to a time dependent equation and solving 20 d t d to steady state the second order method given by sethian 1999 was used to evaluate the gradient term on the right hand side of eq 20 to ensure stability at the boundaries of the domain or for a neighbouring null cell a neumann condition was applied by setting the gradient to zero the distance map then allowed the normal vector of the river channel banks n ˆ to be evaluated as 21 n ˆ d d the required t boundary conditions are calculated using the dot product of the skew gradient of s and the normal vector to the interface as 22 t s i g n s n ˆ where the normal vector is determined using eq 21 and the skew gradient is given by 23 s s y s x 3 3 conformal map generation once the boundary conditions have been set the values of s x y and t x y are found by separately solving the two laplace equations in eq 18 numerically solving the laplace equations for s and t with the boundary conditions provides the required mapping from x y s t the importance of the laplace equation has given rise to a wide range of numerical solution methods these range from stationary iterative methods such as gauss seidel and successive over relaxation sor to modern non stationary iterative methods such as the conjugate gradient and related methods in this study a direct ldlt method was implemented from the eigen computational library guennebaud et al 2010 the ldlt method decomposes a symmetric positive definite matrix a into a l d l t where l is a lower triangular matrix and d is a diagonal matrix this decomposition can be used to directly solve matrix equations of the form a x b through substitution of the decomposed components for the laplace equation the matrix a is derived from a suitable discretisation of the domain and the forcing vector b 0 this direct solution method was used as indirect conjugate gradient methods were found to have difficult converging for very long domains a two dimensional cartesian gridded domain was used for the reconstruction with equal cell spacing δ in the x and y directions the gridded domain contained cells classified as water which required reconstruction or land which were ignored the laplace equation was modified to only calculate cells containing water using the following finite difference discretisation of the laplace equation 2 s 2 s x 2 2 s y 2 s x s y s x 1 δ 2 0 s i 1 j s i 1 j s i j s i 1 j s i 1 j s i 1 j s i 1 j s i j s i 1 j s i 1 j s i 1 j 2 s i j s i 1 j s i 1 j s i 1 j 24 s y 1 δ 2 0 s i j 1 s i j 1 s i j s i j 1 s i j 1 s i j 1 s i j 1 s i j s i j 1 s i j 1 s i j 1 2 s i j s i j 1 s i j 1 s i j 1 where s i j is the streamwise co ordinate on the two dimensional grid with cell indexes i and j in the x and y directions respectively and is a null value for cells classified as land eq 24 applies a neumann boundary condition to any points with null values on either side enforcing eq 19 the final matrix expression is given by a s 0 where a is constructed from eq 24 an identical laplace equation is solved for the t co ordinate a reconstruction is shown for an example river mask in fig 3 the mask is shown on the left hand side with solutions for s and t shown in the centre at the top and bottom respectively the central images are shaded in grayscale from 1 white to 1 black with isolines of constant value superimposed the final conformal map for the mask is shown on the right hand side a more complex example applied to a real river channel is shown in fig 4 the area used was part of a reach from the edward wakool region in the murray basin australia the input data for the conformal map generation was only a rasterised map of the water and land areas at 1 m resolution the tortuosity of this reach and the length around 20 km makes any manual processing to define centre lines or transects a time consuming process however the conformal mapping algorithm efficiently generated a map in around 12 s using a single computational thread on an intel xeon e5 processor inspection of the results inset figures in fig 4 shows that the map fits the boundaries of the river and accurately handles the winding river path the solution of the laplace equation for the downstream values s with neumann boundary conditions on the banks of the channel is in fact identical the potential flow solution within the channel with impermeable boundary conditions batchelor et al 1967 the isolines of t are normal to the isolines of s are the potential flow streamlines the conformal mapping method can therefore be considered to have a physical basis for s and t with t equivalent to flow streamlines and s equivalent to a scaled downstream distance a related method for river reconstruction based on this physical solution from potential flow was presented by lai et al 2018 in which the solution to the laplace equation was used to generate streamlines that preserved the shape of the physical domain the water edge and the thalweg were used as boundary conditions in the method and the elevation of the vertices along the streamlines were linearly interpolated from the nearest cross sections although this method produced an identical s co ordinate to the conformal mapping method the algorithm presented in this paper is also capable of constructing the orthogonal t co ordinate and can be seen as a generalization of this method the resulting conformal map is also related to the curvilinear coordinate system used in a number of previous studies fukuoka et al 1973 goff et al 2004 merwade et al 2005 glenn et al 2016 lai et al 2018 the definition of a curvilinear coordinate system has traditionally relied on the identification of the channel centre line or of the channel thalweg the channel centre line can be identified from the analysis of optical images glenn et al 2016 while the channel thalweg could be estimated using either an iterative protocol merwade et al 2005 or measured bathymetric data lai et al 2018 these lines were then used as reference and the s coordinate was the distance along this line while the n coordinate was the distance across the channel from the reference line in the conformal mapping approach proposed here the t coordinate similarly follows the flow centre line in most cases however this result is achieved automatically by the conformal mapping algorithm and does not have to be imposed in the method as a separate step in the reconstruction process as such the n and t co ordinates may differ between the methods and we have denoted the cross stream co ordinate t to emphasise that this is a metric based on a solution to the laplace equation rather than a measure from a pre defined path 3 4 reconstruction from functional forms once constructed the conformal maps can be used with any basic swept profile depending on the s t stream wise and cross stream distances using a functional form for the bathymetry depth such as z x y k f s t where k is the maximum depth examples of various functions of s and t are shown in fig 5 for the example river mask shown in fig 3 the functions f s t are shown below each case in all cases k 50 δ 50 m and the domain size was 1 km by 1 km the example functions in fig 5 were chosen to illustrate the versatility of the method fig 5a uses a function resulting in a sharp v shaped bathymetry due to the 1 t term which deepens downstream due to the 2 s term fig 5b uses a function with a quadratic cross section resulting in a flattened base fig 5c uses a gaussian profile for the bathymetry it is also possible to combine two functional forms and blend them together along the length of the channel fig 5d uses the functions from fig 5a and b blended down the channel using a weighting parameter w 3 5 extension for branching and islands the conformal mapping can easily be extended to account for topological changes along the channel including branching and reconnection forming islands the change to the input conditions is very straightforward and only applies to the cross stream initial condition for t any branches or islands mid channel simply have a fixed boundary condition of 0 rather than the 1 or 1 of the banks however the actual implementation of this simple change of boundary conditions is not straightforward as the branches and islands must be identified from the input a contour detection method based on image analysis was used here for this purpose the input map was processed using a contrast border following algorithm implemented using the opencv image analysis library bradski 2000 returning a hierarchical set of closed contours within the image the contours were sorted in terms of size and for the examples within this study the largest two contours were assumed to be the river banks and any other contours were assumed to be mid channel islands any cells within two largest contours were set to 1 and 1 in order and cells within the remaining contours were set to zero an example of this process is shown in fig 6 the input as in the previous example is a rasterised mask of the land and water areas fig 6a this can be used directly for the s boundary conditions as in the previous section fig 6b for the t boundary conditions the input mask was first processed into closed contours fig 6c ordered by size contour 1 and 2 were here assumed to be the banks and contour 3 was assumed to be an island or mid channel branch this strategy determined the boundary conditions for t fig 6d from which the conformal map was generated fig 6e the algorithm must be further extended if multiple islands or branches occur across the channel in this case the boundary conditions can be set to a value ordered by their cross channel position in order to give a smooth conformal map the process for doing this is not currently automated and is the subject of future study however such an algorithm could be based on a combination of a cross channel distance calculation and the contour analysis method an example is shown in fig 7 for a braided river channel with multiple islands and branches the land and water mask is shown in fig 7a and the corresponding boundary conditions for the conformal mapping method are shown in fig 7b the boundary values are ordered depending on their proximity to the nearest river bank the resulting reconstruction is shown in fig 7c it can be seen that the s and t co ordinates smooth follow the channel and split around each of the islands re connecting on the other side 3 6 interpolation surfaces the second part of the reconstruction processing using the conformal map involves interpolation over the map to reconstruct the bathymetry if depth measurements are known at certain points the conformal map can be used to convert these points to s t space construct a continuous interpolation surface and then create the river bathymetry in x y space any interpolation method can be used on the conformal map for example inverse distance weighting kriging or polynomial interpolation here a general least squares surface fit was used to demonstrate an overall approach to a combined conformal mapping and interpolation method the least squares fit was implemented with both a polynomial and a radial basis approximation of the surface to show the flexibility of the approach it should be emphasised that any suitable interpolation method could be used and these particular methods were chosen to demonstrate the utility of the method if the river depth z s t at point s t can be expressed in a general form as 25 z s t i c i f a i s g b i t where f and g are smooth continuous functions c i are coefficients to be determined and a i and b i are integer permutation indexes the least squares minimisation scalar e is given by 26 e j i c i f a i s j g b i t j z j 2 where z j s j t j are a set of known depths the scalar is minimised with respect to each coefficient 27 e c i j 2 f a i s j g b i t j i c i f a i s j g b i t j z j 0 giving the matrix equation a c d where a i j k f a i s k g b i t k f a j s k g b j t k 28 d i j f a i s j g b i t j z j for polynomial interpolation functions such as 29 z s t i c i s a i t b i the a i and b i coefficients can be generated up to a given order using a straightforward permutation of exponents however the method is not restricted to polynomial interpolation and thus any combination of suitably smooth functions could be used in this study two interpolation methods were compared the first was polynomials constrained by a chosen functional form of 1 2 1 cos π v to ensure the reconstructed values on the river bank smoothly approached zero in the cross stream direction 30 f a i s s a i g b i t 1 2 1 cos π t t b i the second was radial basis functions constrained by a cross stream form of 1 t 6 31 f a i s exp 2 s δ a i 1 2 1 2 g b i t 1 2 1 cos π t exp 2 t δ b i 1 2 1 2 where δ is the radial basis spacing it was found that the radial basis reconstruction was prone to overshoots and the choice of 1 t 6 which served to impose a flat cross section reaching zero at the banks was less prone to overshoots for the radial basis functions than the cosine constraint used in the polynomial interpolation 4 reconstruction from bathymetric measurements 4 1 study area and data the reconstruction techniques were applied to a 3 km long reach of the balonne river in the condamine culgoa balonne catchment queensland australia fig 8 a the catchment drainage area is 136 014 km2 with a total main channel length of 1195 km the climate is semi arid with frequent droughts and floods the rivers in the region provide most of the water for agricultural and industrial demands the waters of the lower condamine river and of the balonne river have a high turbidity 100 500 ntu and 0 5 1 5 g l tss waters 2012 state of queensland department of natural resources mines and energy 2018 and field surveys rather than lidar scans are the only viable solution for bathymetric data collection bathymetric data of the balonne river reach close to the township of st george upstream of the jack taylor weir were collected in may 2016 using a sontek m9 hydrosurveyor acoustic doppler profiler mounted on a kayak the m9 has built in compensation for pitch and roll and an integrated differential global positioning system dgps positioning solution giving a horizontal accuracy of 1 m or better vertical profiles of salinity temperature and pressure were sampled using the sontek castaway and interpolated in space and time using the hydrosurveyor software in order to achieve a full sound speed compensation of depth data and thus a 0 02 m depth accuracy the sampling path aimed for the collection of cross sections the spacing and location of which were planned according to guidelines defined in previous studies cunge et al 1980 samuels 1990 castellarin et al 2009 conner et al 2014 efforts in the field were made to sample along the designated cross sections however weeds and protruding trunks often impeded the measurement of areas close to the riverbanks with the resulting cross sections being limited to the central area of the river whenever possible navigation between adjacent cross sections followed diagonal survey routes with the purpose to collect as much detail of the three dimensional channel morphologic variability as possible altenau et al 2017 the final sampling path is shown in fig 8b the database used for this study consisted of a total of 10 026 sonar samplings sonar samples provide the depth at the time of measurement and thus information of water surface elevation is required to convert these depth values into river bed elevations relative to a vertical datum the samples were adjusted relative to the australian height datum ahd using a planar surface fitted to water elevation records registered at st george 422201f qld department of natural resources mines and energy the field campaign was completed during a low flow period and so a horizontal water surface upstream of the jack taylor weir was assumed for the purpose of this study this surface had an elevation of 193 1 mahd this adjustment relative to a common elevation datum allowed a seamless digital elevation map of the river bed and surrounding land to be produced for applications such as hydrodynamic modelling of floodplain inundation fig 8b shows the measured depth values the derived elevation values of the soundings and the three dimensional representation of the floodplain provided by a 1 m lidar digital elevation model dem vertical accuracy of 0 15 m and horizontal accuracy of 0 45 m state of queensland department of natural resources mines and energy 2016 irregular patterns of river bed elevation values both along and across the flow direction revealed a complex three dimensional morphology which is the outcome of the interaction of low flow velocity upstream of the weir with physical and biological processes such as sedimentation and tree roots growth a total of 32 approximate cross sections were derived from the database of sampling data following an approach presented in previous studies marvanek et al 2017 the points along the measured path were reported along a straight line perpendicular to the main river stream using the nearest neighbour technique these linearized cross sections blue lines in fig 8c were connected to the floodplain to achieve an approximate representation of the river geometry and flow capacity the geometric properties of the approximate cross sections are summarised in fig 8c the river width w ranged between 110 8 and 229 2 m the maximum depth dmax varied between 2 6 m and 7 2 m and for each cross section it was found to be slightly higher than the mean channel depth computed as the ratio between flow area a and river width w thus revealing the non rectangular shape of the river the lidar dem used to map the land was also used to generate the water surface mask required as input to the conformal mapping reconstruction method in lidar and satellite altimeter derived terrain elevation datasets water bodies usually appear as nearly horizontal surfaces across the river and gently sloping along the river flow consequently a threshold was applied to the lidar dem to remove this planar surface and retrieve the water surface mask when a lidar dem is not available such water surface masks could be derived from optical or radar remote datasets mueller et al 2016 4 2 comparison of methods the capability of the spline based kriging and conformal mapping algorithms to reconstruct river bathymetry was evaluated using quantitative and qualitative approaches methods for strict quantitative evaluation include point scale bootstrapping error estimation osting 2004 merwade et al 2006 merwade 2009 altenau et al 2017 and transect cross validation approaches legleiter et al 2008 cheng et al 2015 lai et al 2018 however the use of these metrics has only been recommended for areas of high sampling density altenau et al 2017 with plan view qualitative comparisons of the reconstructed surfaces used to more effectively illustrate the spatial variability of the reconstructed morphological features in areas of low sampling density andes et al 2017 the difference between interpolated surfaces and measured points was computed to provide a quantitative evaluation of the accuracy of reconstructed surfaces at the local scale altenau et al 2017 analysis of the storage volume of the reconstructed river surfaces was also computed to investigate the potential impact of result inaccuracies on practical applications curtarelli et al 2015 andes et al 2017 ferreira et al 2017 following a common evaluation approach merwade 2009 kriging and the conformal mapping approach were tested for their capability to reconstruct measurement derived cross section shape and flow area in fact the assessment of cross section flow area is pivotal for the accuracy of hydraulic flood forecasting models at the basin and continental scale where knowledge of the exact shape of each cross section is not required but information on flow capacity is essential trigg et al 2009 caviedes voullième et al 2014 finally evaluation of the reconstructed depth values along the river centre line and along a hypothetical thalweg fig 8d were used to evaluate the potential effect of a reconstructed surface as input to a numerical hydrodynamic model as non realistic abrupt stream wise slopes or oscillations in the river bed can lead to numerical instabilities in such models andes et al 2017 the river centre line was defined using the river banks extracted from the lidar data a thalweg line was defined by connecting the deepest point of each linearized cross section a consequence of the non completeness of the measured cross sections due to sampled data not always reaching the river banks as discussed in the previous section is that this thalweg line should be considered only as a hypothetical thalweg line based on the available data the sensitivity of reconstructed surfaces to the input datasets has relevant practical implications santillan et al 2016 krüger et al 2018 for this reason data scarce scenarios were artificially created in order to investigate the effect of limited input data on the reconstruction algorithms in total four different datasets were used as input to the reconstruction algorithms these four datasets were the full measurement dataset which will be referred to as data rich dr scenario and three smaller samples of measured points which will be referred to as data scarce scenarios 1 2 and 3 ds1 ds2 and ds3 fig 8d the sampling points of the ds scenarios were extracted from the dr dataset following hypothetical quicker sampling paths a reduced field data collection workload was the only criterion used for the creation of the data scarce scenarios more specifically ds1 included 9 of the 32 cross sections of the dr scenario a straight navigation path close to the river centre was used for nearly continuous sampling between cross sections ds2 was then derived from ds1 more specifically it retained 5 cross sections and sampling along the river centre was highly discontinuous finally ds3 had the same navigation path as ds2 but lower sampling frequency it is here clarified that when sampling the navigation speed has to be limited to guarantee measurement accuracy the maximum navigation speed while sampling depends on the instrument specifications for instance for the sontek m9 hydrosurveyor navigation speed has to be lower than twice the river flow velocity based on the authors experience the collection of the data scarce scenarios could require 50 ds1 25 ds2 and 20 ds3 of the measurement time required by the dr scenario quantitative evaluation of the accuracy of the reconstruction algorithms for all the input dataset was achieved by computing two performance metrics specifically the rmse and the correlation coefficient r 32 r m s e 1 n i 1 n r i m i 2 33 r 1 n 1 i 1 n r i μ r σ r m i μ m σ m where r i represents the result of the reconstruction algorithm m i represents the measurements n is the total number of measurements used for the analysis μ r μ m are the mean of r i and m i and σ r σ m are the standard deviation of r i and m i a relative comparison between the performance of the reconstruction algorithms in the dr and ds scenarios was achieved by computing the percent variance pv of the selected performance metric m as 34 p v 100 m d s m d r m d r where m d r is the value of the selected performance metric in the dr scenario and m d s is the value of the same metric in ds scenarios 1 2 or 3 it should be noted that the results of the spline based algorithm and kriging and conformal mapping algorithms are not entirely comparable as points were used as input to the kriging algorithms and conformal mapping algorithms whereas the spline based algorithm used only linearized cross sections which could not include small channels around islands in fact the impact of measurement density on the accuracy of spline based reconstruction methods has been previously investigated schäppi et al 2010 caviedes voullième et al 2014 and the performance of methods that allow more flexibility during data collection without strictly requiring cross sections is of greater interest in this study for these reasons the analysis presented here focused on kriging and conformal mapping algorithms similarly to the approach in sanders et al 2004 the results of the well established kriging algorithm were used as the benchmark for the evaluation of the performance of the novel conformal mapping algorithm simple and ordinary kriging methods were also trialled in addition to the universal kriging method and their performances briefly discussed below 4 2 1 surface and volume comparison the spline derived reconstruction algorithm shown in fig 9 naturally produced smooth surfaces in the streamflow direction however abrupt changes of river depth values can be seen close to the river banks this effect appears to be due to the lack of completeness in the measured data used for the assessment of river cross sections fig 10 shows the reconstructed surfaces for the data rich and the three data scarce scenarios the dr scenario fig 10a c includes all the points of the irregular three dimensional input dataset universal kriging resulted in reconstructed surfaces with several localised bumps simple and ordinary kriging resulted in very similar patterns however as suggested in section 2 2 these methods attribute more weight to the average value of the input dataset leading to non realistic results for morphologically complex areas for this reason only the surfaces derived using universal kriging are discussed in detail here conformal mapping algorithms resulted in smooth surfaces with features stretched in the stream wise direction however when using radial basis functions the conformal mapping algorithm returned some values lower than zero close to the river banks due to overshoots in the fitted surface the conformal mapping approach using polynomial functions was found to produce the smoothest surface across the methodologies implemented here in the data scarce scenarios ds1 ds2 and ds3 fig 10d l the algorithms were constrained using a decreasing number of measurement points the universal kriging method resulted in reconstructed surfaces of increasing smoothness nevertheless localised bumps persisted in each ds scenario the conformal mapping methods generally produced smooth features in the stream wise direction nevertheless the conformal mapping using radial basis functions still resulted in outliers in ds1 and in unrealistic deepening of the upstream river reach in ds2 and ds3 in fact the inaccuracies observed in the dr scenario were exacerbated when using a smaller input dataset the results of the conformal mapping method using polynomial functions were qualitatively consistent for all the scenarios the storage volume of the reconstructed river reach is shown in fig 11 top in the spline based algorithms cubic interpolations between cross sections could not reconstruct some deep pools thus leading to smaller values of reservoir volume moreover this method could not include the small river on the other side of the island the large storage volume predicted by the kriging algorithms is the outcome of the abrupt change in depth values close to the river banks in contrast as detailed in section 3 6 the conformal mapping approach used here contained a term that automatically forced the depth values to zero at the river banks it should be noted that only a relative comparison between the algorithms could be carried out as the actual storage volume for the reach is unknown fig 11 bottom shows the rmse between the reconstructed surfaces and the measured points the spline based algorithm resulted in the largest discrepancy from the measured data several meters the use of only the linearized cross sections and the lack of information provided by points sampled between cross sections significantly contributed to these large rmse values this result demonstrated that interpolation of data between adjacent cross sections can lead to large inaccuracies in reconstructed surfaces in the data rich dr scenario the input dataset and the evaluation dataset were identical for both the kriging and conformal mapping methods this choice does not allow a rigorous comparison between methods as reconstructed surfaces based on kriging are constrained to pass through all the input points whereas the conformal mapping algorithm produces parameterized surfaces which may not pass through these points due to this surfaces reconstructed using the conformal mapping method may have larger deviations from the input points however the comparison can serve to highlight different features of the kriging and conformal mapping algorithms consequently the universal kriging algorithm returned the lowest rmse value 0 08 m while conformal mapping algorithms resulted in larger rmse values with radial functions having higher point accuracy than polynomial functions rmse values were 0 27 m and 0 56 m respectively when evaluating the data scarce scenarios ds1 ds2 and ds3 the rmse was computed using the points excluded from the input dataset the conformal mapping method was less affected by the size of the input dataset than the kriging method more specifically for each data scarce scenario the conformal mapping with polynomial functions had the lowest loss of accuracy with a maximum pv of 38 in ds3 the conformal mapping with radial functions had intermediate performances with pv values of 340 380 and 430 for ds1 ds2 and ds3 respectively the universal kriging method resulted in the largest loss of accuracy with pv values of 640 750 and 830 for ds1 ds2 and ds3 respectively as a consequence the conformal mapping with polynomial functions resulted in rmse values seven times larger than rmse values of universal kriging for dr but similar 0 78 m for ds3 these results confirmed the high sensitivity of the kriging algorithm to the density of the input dataset detailed in previous studies legleiter et al 2008 and suggested that conformal mapping approaches allow for a certain degree of flexibility for sampling strategy despite rmse values allow only point scale assessment these results showed that exploring the geometry of the river from bank to bank was essential for bathymetric accuracy while navigating along the centre line or high along path sampling frequency added little information this latter conclusion was somewhat expected however the low sensitivity of the results of the conformal mapping algorithm to the sample dataset suggested that a zig zag navigation trajectory could be a cost effective strategy for field data collection if this method were to be used 4 2 2 cross sections comparison fig 12 shows measured and estimated depth profiles for three representative cross sections the locations of these cross sections named 4 12 and 25 are shown in fig 8c measurement points along the selected transects were available only for the dr scenario reconstruction was attempted using kriging with the addition of zero depth points at the river banks however the addition of these points strongly affected the results of the kriging algorithm especially for the data scarce scenarios this appeared to be caused by the kriging algorithm attempting to reproduce the zero depth points along the boundary and consequently returning flat non realistic surfaces which underestimated cross section flow capacity subsequently kriging algorithms were not forced to zero depth at the river banks and in order to allow a strict comparison between reconstruction algorithms these points were not considered in the following analysis in the data rich scenario universal kriging provided the most accurate reconstruction of the cross section shape with an average r value over the 32 cross sections above 0 96 the conformal mapping method with radial functions resulted in an adequate reconstruction of cross sections shape with average r value of 0 68 the conformal mapping method with polynomial functions could not precisely reconstruct many morphological singularities and resulted in an averaged r value of only 0 21 in the data scarce scenarios the kriging algorithm resulted in nearly flat cross sections as opposed to the conformal mapping methods which resulted in a large morphological variability especially when using the radial function flow area values were computed for cross sections included in the dr scenario but not used in the ds scenarios after excluding the points on the river banks fig 13 shows the rmse values between the values of flow area of reconstructed and measurement derived cross sections for each method and input data scenario universal kriging had the highest accuracy in the dr scenario yet computation of pv highlighted that this method had the highest sensitivity to the density of the input data more specifically in ds3 pv values were 1350 950 and 60 for universal kriging conformal mapping with radial functions and conformal mapping with polynomial functions respectively in fact the conformal mapping with polynomial functions was the least sensitive to the input dataset and resulted in the lowest rmse value for both ds2 and ds3 finally it is noted that the results of this analysis suggested that a reduced along path sampling frequency can be a viable solution when reducing along river sampling frequency 4 2 3 centre and thalweg line comparison fig 14 a shows the comparison between the reconstructed centre lines for dr ds1 ds2 ds3 measured points included and excluded by each scenario are also shown the spline based algorithm resulted in a large depth variation at the downstream boundary of the river reach since the analysis of the measured data did not support the existence of such a morphological feature this large oscillation was considered an artefact of the spline algorithm for the dr scenario the universal kriging algorithm attempted to match each measurement point rmse 0 26 m with the centre line showing small but frequent oscillations the conformal mapping approach with radial functions achieved a good point accuracy rmse 0 32 m with some irregularities while the conformal mapping approach with polynomial functions method returned a gently sloping centre line which reproduced the general trend of the measured data at the cost of a lower point accuracy rmse 0 45 m the oscillations and irregularities observed in the dr scenario for kriging and conformal mapping with radial functions were exacerbated in the ds scenarios in contrast the conformal mapping method with polynomial functions returned a smooth line in ds3 the pv of rmse metric for the universal kriging algorithm the conformal mapping method with radial functions and the conformal mapping method with polynomial functions were 148 159 and 43 respectively fig 14b shows the reconstructed thalweg lines and measured points for dr ds1 ds2 ds3 the point scale evaluation confirmed the results of the centre line analysis with the spline based algorithm resulting in large several meters oscillations which were not produced by other algorithms as mentioned above the spline based algorithms connect corresponding points in adjacent cross sections in this case study the measurement of complete cross sections was often impeded and points belonging to morphological features close to and far from the banks were sometimes artificially connected by the interpolation algorithm this led to non realistic artefacts in the reconstructed surface in the ds scenarios the conformal mapping method with radial functions resulted in large oscillations thus showing the highest sensitivity to input data 4 2 4 computational efficiency it is difficult to strictly compare the computational cost of the interpolation algorithms as they were implemented using different computation and processing methods the spline method was implemented in c using a single threaded approach the conformal methods were implemented in c with a graphical processing unit gpu accelerated method for the computation of the matrix coefficients in eq 28 and a c library eigen for the inversion of the dense matrix the kriging method was fully implemented in python and used the numpy library which uses the dgemm and dgesv subroutines from lapack dongarra et al 1991 to perform the matrix multiplication and dense matrix inversion operations in eq 20 overall the spline interpolation method and conformal mapping methods were found to take approximately the same time to complete approximately depending on the case however it should be noted that the spline interpolation method required cross sections as input data which were pre processed from the sampled points whereas the conformal and kriging methods could directly use these points without any pre processing the kriging method took much longer in some cases taking several hours to complete it is likely that this was mostly due to the overhead of paging large quantities of data between python and the matrix libraries however examination of the raw operational complexity shows that kriging requires an o n 3 inversion of a dense matrix where n is the number of measured data points plus the application of this inverted matrix to a vector for the n required data points o n n 3 plus a further n n dot product operations the creation of a conformal map requires the solution of two poisson equations this was implemented here using a direct o n 3 matrix inversion for accuracy plus a single least squares matrix inversion of o m 3 where m is the number of polynomial or radial basis function coefficients plus a further m n dot products hence although the initial solutions for conformal map generation and kriging require the same o n 3 operations to leading order the subsequent reconstruction takes o n n 3 operations for kriging and only o n m operations for the conformal mapping approach as a result the conformal mapping approach using interpolation will generally take a far shorter computational time than a kriging method 5 discussion the reconstruction of bathymetry in river systems is essential to the implementation of numerical models for ecosystem management in many catchments worldwide different levels of accuracy and detail of the reconstructed surface are needed for different applications including catchment scale flood forecasting biological studies and hydrodynamic flow models trigg et al 2009 legleiter et al 2011a b costabile et al 2015 glenn et al 2016 grimaldi et al 2018 this study was motivated in part by the need for a river reconstruction algorithm which could overcome or by pass the limitations of existing methodologies and support the investigation of a large number of real world scenarios more specifically the algorithm had to be automatic and computationally efficient yet capable of effectively handling islands complex morphologies and incomplete input datasets the resulting reconstructed surface had to meet the requirements for direct implementation of numerical models thus avoiding the need for any further time consuming tailored editing although many commercial geospatial software tools allow bathymetric reconstructions to be created these typically still require some manual input and can involve a complex processing workflow for example a workflow for bathymetric reconstruction of was developed in arcgis for the river section of the murray basin australia shown in fig 4 this consisted of reading transect data and resampling these measured depth profiles at a fixed number of equidistant points for cross channel transects this used basic arcgis feature conversion tools such as line splitting and subdivision then these profiles were linearly interpolated between corresponding points in the flow wise direction the interpolation maintained the proportional distance across the channel to ensure the interpolated depth was relative to the position across the channel regardless of bends in the river or variation in channel width this process populated the channel with a dense array of datapoints that was amenable to conventional spatial interpolation the method was found to work well but required a significant amount of manual effort at each stage and large numbers of additional data points to be generated the conformal mapping approach presented here may be able to overcome some of these constraints as river fitted co ordinates systems can be generated from only raster maps demarking water and land as demonstrated for the river sections in figs 4 and 15 applying an interpolation method over the conformal map then allows bathymetry to be reconstructed from either a sparse set of measured data points without needing to resample or generate further points along transects in particular it is difficult to handle features such as branching and islands in spline based sweeping methods especially in an automated manner such topology can be automatically processing using the conformal mapping approach for example fig 15 shows the resulting conformal map of the balonne river reach used in the study highlighting the ability of the method to deal with islands in the flow path the quality of a reconstructed surface depends on the location and spacing of depth measurements a sampling density capable of adequately capturing channel features is crucial for the reconstruction of channel morphology te stroet et al 2005 legleiter et al 2008 merwade et al 2008 glenn et al 2016 it is known that overshooting and abrupt vertical changes in reconstructed surfaces are often triggered or exacerbated by input data inaccuracy and sparsity merwade et al 2008 caviedes voullième et al 2014 and this can lead to significant inaccuracy in hydrodynamic flow modelling horritt et al 2006 nevertheless bathymetric measurements are affected by errors and data sparsity is an extremely frequent condition in real world contexts consequently reconstruction algorithms for hydrodynamic modelling must produce surfaces without overshoots and the quality of which are only loosely affected by the characteristics of the input dataset the proposed novel methodology was therefore tested on a real case scenario a 3 km long reach of the balonne river qld australia for which bathymetric survey data was available yet affected by a number of limitations and uncertainties which are common to many real world scenarios detailed in section 4 1 the subsequent analysis presented in this study demonstrated the negative impact on reconstructed surfaces from inadequate input data even when considerable effort was dedicated to the field data collection more specifically in the case study presented here data collection of straight bank to bank cross sections was impeded by navigation obstacles and weeds with a strong impact on the results of spline and kriging algorithms conversely the results of the conformal mapping algorithm were less affected by the sampling strategy in fact a sampling strategy based on cross sections perpendicular to the main stream has been strictly recommended to enhance the accuracy of kriging algorithms heritage et al 2007 santillan et al 2016 krüger et al 2018 however depending on the local flow conditions the pursuit of sampling trajectories perpendicular to the main stream might require time consuming manoeuvring and repetitions while obtaining data following zig zag sampling trajectories is usually easier and quicker the analysis of data scarce scenarios based on quicker cruise sampling trajectories requiring a fraction of the workload of the data rich scenario showed that the conformal mapping approach had lower sensitivity to the size and characteristics of the input dataset than kriging although further testing on other study areas are required the results presented in this paper suggest that the conformal mapping algorithm could be a cost effective solution for the reconstruction of river bathymetry in large basins a limited sensitivity of the results to the sampling path can enable optimal allocation of monetary resources for the sampling of longer river traits or for sampling repetition over time to monitor alterations of river path and morphology after flood events or changes in sediment transport conditions buffington 2012 soar et al 2017 the analysis suggests that the conformal mapping approach could be used to both lower the workload in the field and reduce computational time for the reconstruction of river bathymetry in large basins although the approach has been demonstrated here for the interpolation of ground based measurements the flexibility in the characteristics of the input dataset has the potential to allow reliable results for the interpolation of highly sparse measurements of bathymetry from remote sensors this type of data could be provided by the new generation of satellite borne lidar sensors that enable rapid detection of river depth in large river systems with relatively clear waters but with a sampling spacing as large as 10 m for example the advanced topographic laser altimeter system atlas a lidar system carried by the national aeronautics and space administration nasa satellite mission ice cloud and land elevation satellite 2 icesat 2 launched in september 2018 atlas has the potential for bathymetry retrieval at 19 m resolution forfinski sarkozi et al 2016 markus et al 2017 in middle depth water up to 8 m data from such sensors in conjunction with a mask of water areas could be potentially converted to bathymetric maps automatically using the conformal mapping approach 6 conclusions this paper presented a new and computationally efficient approach for the reconstruction of large river systems based on the construction of a river fitted co ordinate system using numerical conformal mapping and interpolation over this conformal map the accuracy and the optimal characteristics of the input dataset were assessed using a 3 km long river reach of the balonne river sampling datasets of decreasing size were sequentially used as input to the conformal mapping approach as well as benchmark spline and kriging algorithms the results showed that despite the morphological complexity of the surveyed area the conformal mapping approach with polynomial and radial basis interpolation produced a smooth surface that could be readily used as input to hydrodynamic models and which was loosely affected by sampling characteristics the results of this study suggest that the conformal mapping approach could enable efficient reconstruction of large river systems with islands and confluences in data scarce scenarios although further testing of the algorithm in other case studies and scenarios is required the analysis suggests that the accuracy of the reconstructed surfaces is only slightly affected by the decreasing size of the measurement sample this may relax the strict requirement for cross sectional depths sections to be measured perpendicularly from the river bank allowing for quicker and easier field campaigns and reducing the monetary and time costs for data collection finally although conceived for the interpolation of ground based measurements the proposed approach has adequate flexibility to be potentially applied for the interpolation of remote sensing lidar data future work includes extension of the current algorithm to the modelling of rivers with several islands the application of the conformal mapping algorithm to other study areas and the assessment of the impacts of the reconstructed surfaces on the outputs of hydrodynamic models acknowledgements the field data collection was funded through the monash faculty of engineering seed grants optimization of a hydraulic model using a doppler profiler and strategic high resolution monitoring of streams to improve operational flood forecasts stefania grimaldi is funded through the bushfire and natural hazards collaborative research centre grant improving flood forecast skill using remote sensing data 
26177,accurate river bathymetry is required for applications including hydrodynamic flow modelling and understanding morphological processes bathymetric measurements are typically a set of depths at discrete points that must be reconstructed into a continuous surface a number of algorithms exist for this reconstruction including spline based techniques and kriging methods a novel and efficient method is introduced to produce a co ordinate system fitted to the river path suitable for bathymetric reconstructions the method is based on numerical conformal mapping and can handle topological features such as islands and branches in the river bathymetric surfaces generated using interpolation over a conformal map are compared to spline based and kriging methods on a section of the balonne river australia the results show that the conformal mapping algorithm produces reconstructions comparable in quality to existing methods preserves flow wise features and is relatively insensitive to the number of sample points enabling faster data collection in the field keywords river bathymetry spatial interpolation depth reconstruction conformal mapping 1 introduction knowledge of the depth and flow velocity of a river is crucial for applications including ecosystem management flood risk assessment and emergency operations fewtrell et al 2011 neal et al 2015 river flow patterns also drive sediment erosion and deposition oxygen exchange and pollutant mixing importantly sediment erosion can cause levee failure and bridge scour while sediment deposition is responsible for reservoir siltation moreover knowledge of flow dynamics oxygen availability and pollutant concentration are essential for aquatic and fish habitat ecological assessment marzadri et al 2014 benjankar et al 2015 vesipa et al 2017 while hydrodynamic models can be used to predict river flow dynamics the accuracy of these models heavily depends on the quality of the bathymetric input data horritt et al 2006 legleiter et al 2011a b several state of the art techniques exist for the measurement of river bathymetry for example point cloud river depth measurements can be obtained using multibeam sonar nittrouer et al 2008 airborne bathymetric light detection and ranging lidar systems mckean et al 2009 2014 or optical remote sensing imaging methods fonstad et al 2005 legleiter et al 2009 legleiter 2012 2013 2015 pan et al 2015 however each of these sampling techniques is constrained by technical limitations use of multibeam sonar is restricted to water depths typically deeper than 4 m and limited by navigation hazards glenn et al 2016 consequently these instruments are mainly used for coastal and marine applications zhi et al 2014 water turbidity bottom material composition and water surface roughness affect the retrieval of bathymetry with lidar systems and remote sensing imaging techniques meaning that the use these techniques is restricted to clear shallow a few meter deep waters with highly reflective substrates and no surface waves legleiter et al 2011a b 2016 legleiter 2012 abdallah et al 2013 kinzel et al 2013 mckean et al 2014 cheng et al 2015 consequently most reach scale studies rely on ground based surveys of river transects accomplished using sonar equipment mounted to small watercrafts altenau et al 2017 krüger et al 2018 these methods are time consuming and expensive but can be applied to shallow deep clear and turbid waters and provide both centimetre resolution and accuracy glenn et al 2016 all techniques for depth measurement result in a set of discrete points that require spatial interpolation algorithms to derive a representation of the river bathymetry a large number of interpolation algorithms have been proposed for river bathymetry and terrain topography reconstruction in the literature with extensive reviews and comparative evaluations provided by merwade et al 2006 chaplot et al 2006 and li et al 2011 li et al 2014 typical approaches for the interpolation of point cloud measurements include natural neighbour triangular irregular networks inverse distance weighting idw spline interpolation and various forms of kriging importantly when interpolating sparse transects special care must be taken to account for anisotropic trends in the river channel bathymetry tomczak 1998 merwade et al 2006 merwade 2009 and many previous studies used a channel fitted coordinate system s n for this purpose fukuoka et al 1973 goff et al 2004 merwade et al 2005 glenn et al 2016 lai et al 2018 in this approach the channel centre line or the channel thalweg that is the line connecting the deepest point in each cross section are used as reference where the s coordinate is the distance along this line and the n coordinate is the distance across the channel from the reference line additionally a number of modified point cloud interpolation techniques have been proposed in order to combine ease of implementation with the need to adequately reconstruct river anisotropy using sparse transect data examples include anisotropic idw tomczak 1998 zonal idw burroughes et al 2001 ellipsoidal idw merwade et al 2006 and rectilinear idw andes et al 2017 other approaches include linear interpolation bounded by user defined break lines schäppi et al 2010 customised spline interpolation algorithms flanagin et al 2007 caviedes voullième et al 2014 and radial basis functions curtarelli et al 2015 however these algorithms require calibration or user interaction furthermore it was shown that irregularly spaced data can generate excessive interpolation errors and overshooting of elevation values in steep slope areas li et al 2008 caviedes voullième et al 2014 reconstruction accuracy generally varies widely between particular case studies and input datasets hindering general conclusions on a recommended methodology and the expected performance of each method li et al 2011 gathered information from over 80 studies and showed the difficulty of providing absolute conclusions by demonstrating that the evaluation strategy impacted the results of the comparison between interpolation methods however in terms of some specific studies krüger et al 2018 compared idw radial basis functions and kriging and found that the latter had higher sub meter accuracy batista et al 2017 pointed out that depending on the river reach kriging algorithm resulted in root mean square error rmse values between 0 5 m and 3 m legleiter et al 2008 showed that the rmse values resulting from the application of kriging algorithm increased from 0 1 m to 0 5 m when cross section spacing increased from 7 m to 56 m this analysis highlighted the large sensibility of kriging accuracy to the input dataset even in a reconfigured channel with a relatively simple morphology rather than relying on simple geometric considerations some authors have also attempted to incorporate geomorphic and hydraulic considerations into the interpolation algorithm for instance legleiter et al 2008 and batista et al 2017 tested the use of kriging algorithms while zhang et al 2016 suggested incorporating a preliminary estimation of flow velocity into the topography interpolator in a related approach lai et al 2018 used the laplace equation to generate flow streamlines despite a higher complexity of these algorithms the accuracy of the reconstructed surface strictly depends on the characteristics of the measured dataset reconstruction for complicated morphologies with islands or confluences usually requires extensive manual intervention for processing separate features must be disaggregated by drawing break lines or two dimensional polygons of island boundaries surface interpolation is then performed separately for each area delimited by two break lines or polygons which is a customised and time consuming process brasington et al 2000 merwade et al 2008 schäppi et al 2010 costabile et al 2015 however braided rivers are found across a large range of climates such as glacial areas to arid regions and physiographic settings such as from steep mountain areas to low coastal plains e g from steep mountain areas to low coastal plains surian 2015 consistent research efforts are being made to improve the modelling and understanding of flow dynamics in these complex morphologies costelloe et al 2006 piégay et al 2009 mohammadi et al 2013 jarihani et al 2015 williams et al 2016 altenau et al 2017 a reconstruction algorithm which is loosely affected by the characteristics of the input dataset and can provide reliable reconstruction of river bathymetric surfaces even with sparse and incomplete measurement points could allow greater flexibility during data collection this paper presents such a method capable of reconstructing complicated morphologies the method is based on a two stage process a novel application of conformal mapping ahlfors 1979 to provide a co ordinate system fitted to the river geometry and a subsequent interpolation over this co ordinate system to provide the reconstruction the method could be automated as it requires only a mask of the land and water areas to generate the river fitted co ordinate system and a set of depth sampled points for the subsequent interpolation the proposed algorithm was applied for the reconstruction of a 3 km long reach of the balonne river queensland australia for which a bathymetric field dataset was available the process was compared to two other interpolation methods spline interpolation and kriging to assess the overall effectiveness ease of use and computational efficiency of the method compared to these commonly used approaches the resulting reconstructions were evaluated using qualitative plan view and longitudinal evaluations as well as a quantitative comparison of quantities such as storage volume and cross section flow area furthermore both this complete dataset and three artificially reduced datasets derived from the complete dataset were used in the reconstructions algorithms to understand the effect of data sparsity on the resulting reconstructed surfaces the analysis demonstrated that the overall process of conformal map generation with an associated interpolation provided reconstructions of comparable quality to existing methods but could be entirely automated furthermore interpolation over the river fitted co ordinate system was found to be less sensitive to the quality of the input dataset interpolation than the kriging and spline algorithms allowing potentially greater flexibility during data collection 2 existing methods 2 1 spline based algorithms spline interpolation algorithms attempt to preserve the information content of each measurement by producing an interpolation surface that passes exactly through the input points these algorithms require the river to be partitioned into user defined regions along break lines or transects fig 1 a to handle complex river shapes interpolation algorithms are then used to connect points measured along cross sections in both the stream wise and cross stream directions piecewise cubic splines in the stream wise direction and linear depth profiles in the cross stream direction can result in a smooth reconstructed surface if each cross section has an equivalent number of data points flanagin et al 2007 caviedes voullième et al 2014 a cubic spline based interpolation method was implemented in this study for the purpose of comparison the path of the river in between the transects in the stream wise direction s is described using a cubic bezier curve given by the equation 1 p s 1 s 3 p 0 3 1 s 2 t p 1 3 1 s s 2 p 2 s 3 p 3 where 0 s 1 this corresponds to a smooth curve which goes from the two dimensional points p 0 to p 3 towards p 1 and p 2 from each end fig 1b and c the procedure for mapping a given curvilinear coordinate s t to a world coordinate x y is to firstly determine the world coordinate point s p 0 and p 3 using the expressions p 0 1 t t n s t a r t t t n e n d 2 p 3 1 t t n 1 s t a r t t t n 1 e n d where t s t a r t and t e n d are the transect start and end points in world coordinates respectively at the current t n and next t n 1 transect the points p 1 and p 2 are calculated using p 1 p 0 1 2 p 03 n n 3 p 2 p 3 1 2 p 03 n n 1 where n are the normals of the current n n and next n n 1 transect while p 03 is the displacement between p 0 and p 3 the reverse process of mapping given world coordinates x y to curvilinear coordinates s t is difficult as there is no closed form expression for this inversion in this study a nonlinear optimisation process was implemented which sought to determine s t by minimising the distance error 4 e r r o r x y f s t the downhill simplex method nelder et al 1965 was used for this process if the coordinates exceeded the bounds s 0 s 1 t 0 and t 1 during the optimisation the positions calculated by the cubic bezier curve expressions were taken to be erroneous in this case the coordinates of the nearest constrained value s v a l i d t v a l i d and the following modified error expression were used such that 5 e r r o r x y f s v a l i d t v a l i d p 03 s t s v a l i d t v a l i d this modification penalises the s t coordinates for exceeding their bounds and allows the unconstrained optimisation method to work the overall approach was found to robustly determine all s t coordinates for the river transects considered in this study 2 2 kriging kriging is a geostatistical method that can be used for geospatial interpolation through the analysis of the spatial structure of observed data points the kriging method minimizes the variance of the prediction error at every interpolated point using a minimum mean squared error method the main strengths of kriging are the statistical quality of its predictions i e unbiasedness and its ability to predict the spatial distribution of uncertainty mitas 2005 one of the first applications of kriging for river bathymetry reconstruction was accomplished by carter et al 1997 while eriksson et al 2000 suggested the use of two dimensional semi variogram models in order to account for data anisotropy a number of variations of the kriging algorithm e g ordinary kriging simple kriging universal kriging co kriging regression kriging kriging with a trend stratified kriging with a trend kriging with external drift have since been applied for river reconstruction hilldale et al 2008 legleiter et al 2008 bailly et al 2010 cheng et al 2015 su et al 2015 batista et al 2017 with a number of comparative studies showing the merit of using kriging over simpler techniques such as idw radial basis functions and polynomial interpolation merwade et al 2006 maleika et al 2012 curtarelli et al 2015 ferreira et al 2017 su et al 2017 krüger et al 2018 some authors proposed variants of kriging to achieve an accurate representation of data anisotropy more specifically te stroet et al 2005 formulated a local anisotropy kriging lak for automatic detection of structures within the data which resulted in sub meter average absolute error with a decrease of 23 compared to ordinary kriging boisvert et al 2009 presented a locally varying anisotropy lva kriging algorithm based on non euclidean distances to reconstruct complex geological sites and magneron et al 2010 proposed an algorithm to integrate prior knowledge and locally varying parameters a general form of a kriging estimator can be written as 6 y 0 ˆ x α 1 n λ α x y x α μ x where y 0 ˆ x is the reconstructed surface at point x estimated using n known sample values y x α of the field at the locations x α while μ x is the mean function of the field and λ α x are weights determined in the kriging algorithm to determine these weights an experimental semi variogram γ e h must be computed 7 γ e h 1 2 n h α 1 n h y x α h y x α 2 where n h is the number of pairs of sampled locations separated by a lag distance h wackernagel 2003 this experimental variogram is evaluated for the pairs of sampled values by computing the squared difference between the sampled values and evaluating the variogram cloud against the lag distance h between the sampled locations usually it is observed that the value of an experimental variogram increases with separation distance and eventually reaches a saturation level referred to as the sill once an experimental variogram has been obtained it is modelled using a chosen continuous theoretical variogram γ t h the three most commonly used theoretical variogram models are the spherical model the exponential model and the gaussian model wackernagel 2003 a spherical model for the theoretical variogram was found to best represent the experimental variogram obtained from the data used in this study with the form 8 y t h r 1 5 h a 0 5 h a 3 i f h a r i f h a in order to replace the experimental variogram with the theoretical model a least square levenberg marquardt method was employed to fit the free parameter a in eq 8 once the theoretical variogram was fitted to the sampled data the semi variance for all the sampled locations could be computed using the theoretical variogram there is currently no common agreement on the optimal kriging formulation for river bathymetry and universal ordinary and simple kriging methods are often used maleika et al 2012 krüger et al 2018 the main difference between these three methods is the assumption of the mean function μ x in the kriging estimator simple kriging uses a given mean ordinary kriging has an unknown but constant mean and universal kriging has a spatially varying unknown mean the universal kriging method was used in this study as river bathymetry is typically not geostationary which is an assumption for ordinary and simple kriging cressie 1993 and wackernagel 2003 provide further details on these methods in universal kriging it is assumed that the random function is a linear combination of the mean drift μ x α and a nonstationary error e x α such that 9 y x α μ x α e x α for this application it can be considered that the river depth is a random function y x α sampled irregularly over the domain d with x α ε d the mean drift m x α can be regarded as a regular continuous variation of y x α whereas the nonstationary error e x α signifies the random small scale fluctuations it can also be said that the mean drift is the non stationary expectation of the random function y x α such that 10 e y x α μ x α and the nonstationary error has a zero expectation e e x α 0 for universal kriging the mean drift can be estimated as a k basis function of the spatial coordinates with either a linear quadratic or a higher order form in this study the mean drift was parametrized using the linear form 11 e y x α μ x α a 0 a 1 x 1 x α a 2 x 2 x α l 0 k a l f l x α where a 0 a 1 a 2 are the unknown coefficients of the linear equation and x 1 and x 2 are the longitude and latitude at the location x α respectively the coefficient terms in eq 11 were obtained using a least squares levenberg marquardt curve fitting method once an equation for the trend was selected and fitted to the sampled data the residual values were obtained as the deviation of observed values from the fitted values and are given as 12 r x α y x α μ x α these residuals were then used to obtain the experimental variogram using the semi variance defined as 13 γ ˆ r h 1 2 n h α 1 n h r x α h r x α 2 where the hat on γ represents a residual the experimental variogram eq 13 was used in eq 8 to find the free parameter a and the resulting fitted theoretical variogram was utilized in the kriging prediction the kriging prediction can be expressed in matrix notation as 14 y r x α λ r x α finally the weights defined by λ were obtained by solving 15 λ 1 λ 2 λ n a 0 a 1 a k c i j f l x α f l x α 0 1 c 10 c 20 c n 0 f 0 x 0 f 1 x 0 f k x 0 where c i j were obtained by evaluating the residual variogram for data to data and c i 0 from data to un sampled once the kriging prediction for the residuals was obtained it was added to the values obtained by evaluating the trend for the sampled locations to obtain the kriging estimate for bathymetric reconstructions the elevation data was found to be skewed away from a normal distribution towards lower depth values to transform the data back into a normal distribution required for the kriging algorithm a box cox log normal or normal score transformation can be used here we used the normal score transformation where the cumulative distribution of the elevation data was transformed to a normal distribution we chose to use the normal score transformation as both box cox and log normal transformation can result in deviations in the tail of the distribution whereas the normal score transformation gave an exact transformation to a normal distribution 3 reconstruction using conformal mapping 3 1 conformal mapping the new reconstruction algorithm presented in this study consists of three main steps these are 1 fitting a dimensionless conformal co ordinate system s t to the river 2 interpolating the bathymetry from known data points within this conformal co ordinate system and 3 mapping this interpolation back to real world projected coordinates x y the mapping is called conformal as it preserves the angle between orthogonal lines for example a cartesian projected co ordinate system with lines in x and y still has a 90 angle between the cartesian grid lines in s and t a schematic of the reconstruction algorithm is shown in fig 2 a given channel in projected x y co ordinates fig 2a is supplied or converted to a rasterised mask of cells containing either water or land fig 2b these are then processed into two sets of boundary conditions for s and t the values of s and t represent the stream wise and cross stream distance respectively the key component of the reconstruction algorithm is to generate the conformal map this is numerically calculated using a method based on complex functions a holomorphic complex function f z defines a conformal mapping 16 f z s x y i t x y where the two real functions s x y and t x y must obey the cauchy riemann equations s x t y 17 s y t x the functions s x y and t x y are also harmonic which means that they are the solutions to a pair of two dimensional laplace equations the laplace equation is widely used in physics more specifically in areas such as electromagnetism fluid flow classical gravitation and heat transfer the equation in two dimensions is given by 2 ϕ x y 0 where ϕ x y is a scalar physically the laplacian of a field 2 ϕ x y represents the curvature of the field so the laplace equation for ϕ x y provides the scalar field for which the curvature of ϕ x y is zero for eq 16 the pair of laplace equations is 2 s x y 0 18 2 t x y 0 the cauchy riemann equations can be used to determine the required boundary equations for the solution of eq 18 in order for the mapping to be conformal at the boundary the gradient of u is required to be zero at the banks of the river and the gradient of v to be zero at the upstream and downstream boundaries of the river these conditions are identical to zero flux neumann boundary conditions such that 19 s n 0 t t 0 where n is the normal direction vector at the banks of the river and t is the normal direction vector at the upstream and downstream boundaries substituting this requirement into eq 17 results in the straightforward condition on t to be an arbitrary constant at the banks of the river and s to be an arbitrary constant at the upstream and downstream boundaries of the river these constants were chosen as s 1 at the upstream boundary and s 1 at the downstream boundary fig 2c with t 1 at one side of the channel and t 1 at the other fig 2e the choice of values was purely for ease of use in interpolating algorithms and any other constants could be chosen 3 2 cross stream boundary conditions the boundary conditions for s are straightforward to implement but the boundary conditions for t are more complicated to impose as the mask contains no directional information for which side of the channel is which the cross stream boundary conditions have been implemented using two methods in this study the first is using a cross stream distance map which is relatively straightforward to implement and the second uses a more complex image analysis method which can account for branching and islands detailed in the next section a distance map is a rasterised map containing the scalar distance to the nearest interface here the nearest channel bank distance maps are very widely used in geographic information system gis analysis and can be rapidly constructed using a number of algorithms including the level set or fast marching method sethian 1999 in this study a level set method was used as this was found to be more stable and robust than the fast marching method cells in the map contain a scalar value representing the distance from the banks of the channel d or a null indicator indicating that the cell lies outside the river channel the initial distance value is set to d 1 if points lie outside the river area d 1 if the points are within the river area or d 0 for the edge case of the point exactly on the river bank the distance function can be constructed by solving the eikonal equation d 1 sethian 2001 with the given initial condition the level set method solves this for d by converting the eikonal equation to a time dependent equation and solving 20 d t d to steady state the second order method given by sethian 1999 was used to evaluate the gradient term on the right hand side of eq 20 to ensure stability at the boundaries of the domain or for a neighbouring null cell a neumann condition was applied by setting the gradient to zero the distance map then allowed the normal vector of the river channel banks n ˆ to be evaluated as 21 n ˆ d d the required t boundary conditions are calculated using the dot product of the skew gradient of s and the normal vector to the interface as 22 t s i g n s n ˆ where the normal vector is determined using eq 21 and the skew gradient is given by 23 s s y s x 3 3 conformal map generation once the boundary conditions have been set the values of s x y and t x y are found by separately solving the two laplace equations in eq 18 numerically solving the laplace equations for s and t with the boundary conditions provides the required mapping from x y s t the importance of the laplace equation has given rise to a wide range of numerical solution methods these range from stationary iterative methods such as gauss seidel and successive over relaxation sor to modern non stationary iterative methods such as the conjugate gradient and related methods in this study a direct ldlt method was implemented from the eigen computational library guennebaud et al 2010 the ldlt method decomposes a symmetric positive definite matrix a into a l d l t where l is a lower triangular matrix and d is a diagonal matrix this decomposition can be used to directly solve matrix equations of the form a x b through substitution of the decomposed components for the laplace equation the matrix a is derived from a suitable discretisation of the domain and the forcing vector b 0 this direct solution method was used as indirect conjugate gradient methods were found to have difficult converging for very long domains a two dimensional cartesian gridded domain was used for the reconstruction with equal cell spacing δ in the x and y directions the gridded domain contained cells classified as water which required reconstruction or land which were ignored the laplace equation was modified to only calculate cells containing water using the following finite difference discretisation of the laplace equation 2 s 2 s x 2 2 s y 2 s x s y s x 1 δ 2 0 s i 1 j s i 1 j s i j s i 1 j s i 1 j s i 1 j s i 1 j s i j s i 1 j s i 1 j s i 1 j 2 s i j s i 1 j s i 1 j s i 1 j 24 s y 1 δ 2 0 s i j 1 s i j 1 s i j s i j 1 s i j 1 s i j 1 s i j 1 s i j s i j 1 s i j 1 s i j 1 2 s i j s i j 1 s i j 1 s i j 1 where s i j is the streamwise co ordinate on the two dimensional grid with cell indexes i and j in the x and y directions respectively and is a null value for cells classified as land eq 24 applies a neumann boundary condition to any points with null values on either side enforcing eq 19 the final matrix expression is given by a s 0 where a is constructed from eq 24 an identical laplace equation is solved for the t co ordinate a reconstruction is shown for an example river mask in fig 3 the mask is shown on the left hand side with solutions for s and t shown in the centre at the top and bottom respectively the central images are shaded in grayscale from 1 white to 1 black with isolines of constant value superimposed the final conformal map for the mask is shown on the right hand side a more complex example applied to a real river channel is shown in fig 4 the area used was part of a reach from the edward wakool region in the murray basin australia the input data for the conformal map generation was only a rasterised map of the water and land areas at 1 m resolution the tortuosity of this reach and the length around 20 km makes any manual processing to define centre lines or transects a time consuming process however the conformal mapping algorithm efficiently generated a map in around 12 s using a single computational thread on an intel xeon e5 processor inspection of the results inset figures in fig 4 shows that the map fits the boundaries of the river and accurately handles the winding river path the solution of the laplace equation for the downstream values s with neumann boundary conditions on the banks of the channel is in fact identical the potential flow solution within the channel with impermeable boundary conditions batchelor et al 1967 the isolines of t are normal to the isolines of s are the potential flow streamlines the conformal mapping method can therefore be considered to have a physical basis for s and t with t equivalent to flow streamlines and s equivalent to a scaled downstream distance a related method for river reconstruction based on this physical solution from potential flow was presented by lai et al 2018 in which the solution to the laplace equation was used to generate streamlines that preserved the shape of the physical domain the water edge and the thalweg were used as boundary conditions in the method and the elevation of the vertices along the streamlines were linearly interpolated from the nearest cross sections although this method produced an identical s co ordinate to the conformal mapping method the algorithm presented in this paper is also capable of constructing the orthogonal t co ordinate and can be seen as a generalization of this method the resulting conformal map is also related to the curvilinear coordinate system used in a number of previous studies fukuoka et al 1973 goff et al 2004 merwade et al 2005 glenn et al 2016 lai et al 2018 the definition of a curvilinear coordinate system has traditionally relied on the identification of the channel centre line or of the channel thalweg the channel centre line can be identified from the analysis of optical images glenn et al 2016 while the channel thalweg could be estimated using either an iterative protocol merwade et al 2005 or measured bathymetric data lai et al 2018 these lines were then used as reference and the s coordinate was the distance along this line while the n coordinate was the distance across the channel from the reference line in the conformal mapping approach proposed here the t coordinate similarly follows the flow centre line in most cases however this result is achieved automatically by the conformal mapping algorithm and does not have to be imposed in the method as a separate step in the reconstruction process as such the n and t co ordinates may differ between the methods and we have denoted the cross stream co ordinate t to emphasise that this is a metric based on a solution to the laplace equation rather than a measure from a pre defined path 3 4 reconstruction from functional forms once constructed the conformal maps can be used with any basic swept profile depending on the s t stream wise and cross stream distances using a functional form for the bathymetry depth such as z x y k f s t where k is the maximum depth examples of various functions of s and t are shown in fig 5 for the example river mask shown in fig 3 the functions f s t are shown below each case in all cases k 50 δ 50 m and the domain size was 1 km by 1 km the example functions in fig 5 were chosen to illustrate the versatility of the method fig 5a uses a function resulting in a sharp v shaped bathymetry due to the 1 t term which deepens downstream due to the 2 s term fig 5b uses a function with a quadratic cross section resulting in a flattened base fig 5c uses a gaussian profile for the bathymetry it is also possible to combine two functional forms and blend them together along the length of the channel fig 5d uses the functions from fig 5a and b blended down the channel using a weighting parameter w 3 5 extension for branching and islands the conformal mapping can easily be extended to account for topological changes along the channel including branching and reconnection forming islands the change to the input conditions is very straightforward and only applies to the cross stream initial condition for t any branches or islands mid channel simply have a fixed boundary condition of 0 rather than the 1 or 1 of the banks however the actual implementation of this simple change of boundary conditions is not straightforward as the branches and islands must be identified from the input a contour detection method based on image analysis was used here for this purpose the input map was processed using a contrast border following algorithm implemented using the opencv image analysis library bradski 2000 returning a hierarchical set of closed contours within the image the contours were sorted in terms of size and for the examples within this study the largest two contours were assumed to be the river banks and any other contours were assumed to be mid channel islands any cells within two largest contours were set to 1 and 1 in order and cells within the remaining contours were set to zero an example of this process is shown in fig 6 the input as in the previous example is a rasterised mask of the land and water areas fig 6a this can be used directly for the s boundary conditions as in the previous section fig 6b for the t boundary conditions the input mask was first processed into closed contours fig 6c ordered by size contour 1 and 2 were here assumed to be the banks and contour 3 was assumed to be an island or mid channel branch this strategy determined the boundary conditions for t fig 6d from which the conformal map was generated fig 6e the algorithm must be further extended if multiple islands or branches occur across the channel in this case the boundary conditions can be set to a value ordered by their cross channel position in order to give a smooth conformal map the process for doing this is not currently automated and is the subject of future study however such an algorithm could be based on a combination of a cross channel distance calculation and the contour analysis method an example is shown in fig 7 for a braided river channel with multiple islands and branches the land and water mask is shown in fig 7a and the corresponding boundary conditions for the conformal mapping method are shown in fig 7b the boundary values are ordered depending on their proximity to the nearest river bank the resulting reconstruction is shown in fig 7c it can be seen that the s and t co ordinates smooth follow the channel and split around each of the islands re connecting on the other side 3 6 interpolation surfaces the second part of the reconstruction processing using the conformal map involves interpolation over the map to reconstruct the bathymetry if depth measurements are known at certain points the conformal map can be used to convert these points to s t space construct a continuous interpolation surface and then create the river bathymetry in x y space any interpolation method can be used on the conformal map for example inverse distance weighting kriging or polynomial interpolation here a general least squares surface fit was used to demonstrate an overall approach to a combined conformal mapping and interpolation method the least squares fit was implemented with both a polynomial and a radial basis approximation of the surface to show the flexibility of the approach it should be emphasised that any suitable interpolation method could be used and these particular methods were chosen to demonstrate the utility of the method if the river depth z s t at point s t can be expressed in a general form as 25 z s t i c i f a i s g b i t where f and g are smooth continuous functions c i are coefficients to be determined and a i and b i are integer permutation indexes the least squares minimisation scalar e is given by 26 e j i c i f a i s j g b i t j z j 2 where z j s j t j are a set of known depths the scalar is minimised with respect to each coefficient 27 e c i j 2 f a i s j g b i t j i c i f a i s j g b i t j z j 0 giving the matrix equation a c d where a i j k f a i s k g b i t k f a j s k g b j t k 28 d i j f a i s j g b i t j z j for polynomial interpolation functions such as 29 z s t i c i s a i t b i the a i and b i coefficients can be generated up to a given order using a straightforward permutation of exponents however the method is not restricted to polynomial interpolation and thus any combination of suitably smooth functions could be used in this study two interpolation methods were compared the first was polynomials constrained by a chosen functional form of 1 2 1 cos π v to ensure the reconstructed values on the river bank smoothly approached zero in the cross stream direction 30 f a i s s a i g b i t 1 2 1 cos π t t b i the second was radial basis functions constrained by a cross stream form of 1 t 6 31 f a i s exp 2 s δ a i 1 2 1 2 g b i t 1 2 1 cos π t exp 2 t δ b i 1 2 1 2 where δ is the radial basis spacing it was found that the radial basis reconstruction was prone to overshoots and the choice of 1 t 6 which served to impose a flat cross section reaching zero at the banks was less prone to overshoots for the radial basis functions than the cosine constraint used in the polynomial interpolation 4 reconstruction from bathymetric measurements 4 1 study area and data the reconstruction techniques were applied to a 3 km long reach of the balonne river in the condamine culgoa balonne catchment queensland australia fig 8 a the catchment drainage area is 136 014 km2 with a total main channel length of 1195 km the climate is semi arid with frequent droughts and floods the rivers in the region provide most of the water for agricultural and industrial demands the waters of the lower condamine river and of the balonne river have a high turbidity 100 500 ntu and 0 5 1 5 g l tss waters 2012 state of queensland department of natural resources mines and energy 2018 and field surveys rather than lidar scans are the only viable solution for bathymetric data collection bathymetric data of the balonne river reach close to the township of st george upstream of the jack taylor weir were collected in may 2016 using a sontek m9 hydrosurveyor acoustic doppler profiler mounted on a kayak the m9 has built in compensation for pitch and roll and an integrated differential global positioning system dgps positioning solution giving a horizontal accuracy of 1 m or better vertical profiles of salinity temperature and pressure were sampled using the sontek castaway and interpolated in space and time using the hydrosurveyor software in order to achieve a full sound speed compensation of depth data and thus a 0 02 m depth accuracy the sampling path aimed for the collection of cross sections the spacing and location of which were planned according to guidelines defined in previous studies cunge et al 1980 samuels 1990 castellarin et al 2009 conner et al 2014 efforts in the field were made to sample along the designated cross sections however weeds and protruding trunks often impeded the measurement of areas close to the riverbanks with the resulting cross sections being limited to the central area of the river whenever possible navigation between adjacent cross sections followed diagonal survey routes with the purpose to collect as much detail of the three dimensional channel morphologic variability as possible altenau et al 2017 the final sampling path is shown in fig 8b the database used for this study consisted of a total of 10 026 sonar samplings sonar samples provide the depth at the time of measurement and thus information of water surface elevation is required to convert these depth values into river bed elevations relative to a vertical datum the samples were adjusted relative to the australian height datum ahd using a planar surface fitted to water elevation records registered at st george 422201f qld department of natural resources mines and energy the field campaign was completed during a low flow period and so a horizontal water surface upstream of the jack taylor weir was assumed for the purpose of this study this surface had an elevation of 193 1 mahd this adjustment relative to a common elevation datum allowed a seamless digital elevation map of the river bed and surrounding land to be produced for applications such as hydrodynamic modelling of floodplain inundation fig 8b shows the measured depth values the derived elevation values of the soundings and the three dimensional representation of the floodplain provided by a 1 m lidar digital elevation model dem vertical accuracy of 0 15 m and horizontal accuracy of 0 45 m state of queensland department of natural resources mines and energy 2016 irregular patterns of river bed elevation values both along and across the flow direction revealed a complex three dimensional morphology which is the outcome of the interaction of low flow velocity upstream of the weir with physical and biological processes such as sedimentation and tree roots growth a total of 32 approximate cross sections were derived from the database of sampling data following an approach presented in previous studies marvanek et al 2017 the points along the measured path were reported along a straight line perpendicular to the main river stream using the nearest neighbour technique these linearized cross sections blue lines in fig 8c were connected to the floodplain to achieve an approximate representation of the river geometry and flow capacity the geometric properties of the approximate cross sections are summarised in fig 8c the river width w ranged between 110 8 and 229 2 m the maximum depth dmax varied between 2 6 m and 7 2 m and for each cross section it was found to be slightly higher than the mean channel depth computed as the ratio between flow area a and river width w thus revealing the non rectangular shape of the river the lidar dem used to map the land was also used to generate the water surface mask required as input to the conformal mapping reconstruction method in lidar and satellite altimeter derived terrain elevation datasets water bodies usually appear as nearly horizontal surfaces across the river and gently sloping along the river flow consequently a threshold was applied to the lidar dem to remove this planar surface and retrieve the water surface mask when a lidar dem is not available such water surface masks could be derived from optical or radar remote datasets mueller et al 2016 4 2 comparison of methods the capability of the spline based kriging and conformal mapping algorithms to reconstruct river bathymetry was evaluated using quantitative and qualitative approaches methods for strict quantitative evaluation include point scale bootstrapping error estimation osting 2004 merwade et al 2006 merwade 2009 altenau et al 2017 and transect cross validation approaches legleiter et al 2008 cheng et al 2015 lai et al 2018 however the use of these metrics has only been recommended for areas of high sampling density altenau et al 2017 with plan view qualitative comparisons of the reconstructed surfaces used to more effectively illustrate the spatial variability of the reconstructed morphological features in areas of low sampling density andes et al 2017 the difference between interpolated surfaces and measured points was computed to provide a quantitative evaluation of the accuracy of reconstructed surfaces at the local scale altenau et al 2017 analysis of the storage volume of the reconstructed river surfaces was also computed to investigate the potential impact of result inaccuracies on practical applications curtarelli et al 2015 andes et al 2017 ferreira et al 2017 following a common evaluation approach merwade 2009 kriging and the conformal mapping approach were tested for their capability to reconstruct measurement derived cross section shape and flow area in fact the assessment of cross section flow area is pivotal for the accuracy of hydraulic flood forecasting models at the basin and continental scale where knowledge of the exact shape of each cross section is not required but information on flow capacity is essential trigg et al 2009 caviedes voullième et al 2014 finally evaluation of the reconstructed depth values along the river centre line and along a hypothetical thalweg fig 8d were used to evaluate the potential effect of a reconstructed surface as input to a numerical hydrodynamic model as non realistic abrupt stream wise slopes or oscillations in the river bed can lead to numerical instabilities in such models andes et al 2017 the river centre line was defined using the river banks extracted from the lidar data a thalweg line was defined by connecting the deepest point of each linearized cross section a consequence of the non completeness of the measured cross sections due to sampled data not always reaching the river banks as discussed in the previous section is that this thalweg line should be considered only as a hypothetical thalweg line based on the available data the sensitivity of reconstructed surfaces to the input datasets has relevant practical implications santillan et al 2016 krüger et al 2018 for this reason data scarce scenarios were artificially created in order to investigate the effect of limited input data on the reconstruction algorithms in total four different datasets were used as input to the reconstruction algorithms these four datasets were the full measurement dataset which will be referred to as data rich dr scenario and three smaller samples of measured points which will be referred to as data scarce scenarios 1 2 and 3 ds1 ds2 and ds3 fig 8d the sampling points of the ds scenarios were extracted from the dr dataset following hypothetical quicker sampling paths a reduced field data collection workload was the only criterion used for the creation of the data scarce scenarios more specifically ds1 included 9 of the 32 cross sections of the dr scenario a straight navigation path close to the river centre was used for nearly continuous sampling between cross sections ds2 was then derived from ds1 more specifically it retained 5 cross sections and sampling along the river centre was highly discontinuous finally ds3 had the same navigation path as ds2 but lower sampling frequency it is here clarified that when sampling the navigation speed has to be limited to guarantee measurement accuracy the maximum navigation speed while sampling depends on the instrument specifications for instance for the sontek m9 hydrosurveyor navigation speed has to be lower than twice the river flow velocity based on the authors experience the collection of the data scarce scenarios could require 50 ds1 25 ds2 and 20 ds3 of the measurement time required by the dr scenario quantitative evaluation of the accuracy of the reconstruction algorithms for all the input dataset was achieved by computing two performance metrics specifically the rmse and the correlation coefficient r 32 r m s e 1 n i 1 n r i m i 2 33 r 1 n 1 i 1 n r i μ r σ r m i μ m σ m where r i represents the result of the reconstruction algorithm m i represents the measurements n is the total number of measurements used for the analysis μ r μ m are the mean of r i and m i and σ r σ m are the standard deviation of r i and m i a relative comparison between the performance of the reconstruction algorithms in the dr and ds scenarios was achieved by computing the percent variance pv of the selected performance metric m as 34 p v 100 m d s m d r m d r where m d r is the value of the selected performance metric in the dr scenario and m d s is the value of the same metric in ds scenarios 1 2 or 3 it should be noted that the results of the spline based algorithm and kriging and conformal mapping algorithms are not entirely comparable as points were used as input to the kriging algorithms and conformal mapping algorithms whereas the spline based algorithm used only linearized cross sections which could not include small channels around islands in fact the impact of measurement density on the accuracy of spline based reconstruction methods has been previously investigated schäppi et al 2010 caviedes voullième et al 2014 and the performance of methods that allow more flexibility during data collection without strictly requiring cross sections is of greater interest in this study for these reasons the analysis presented here focused on kriging and conformal mapping algorithms similarly to the approach in sanders et al 2004 the results of the well established kriging algorithm were used as the benchmark for the evaluation of the performance of the novel conformal mapping algorithm simple and ordinary kriging methods were also trialled in addition to the universal kriging method and their performances briefly discussed below 4 2 1 surface and volume comparison the spline derived reconstruction algorithm shown in fig 9 naturally produced smooth surfaces in the streamflow direction however abrupt changes of river depth values can be seen close to the river banks this effect appears to be due to the lack of completeness in the measured data used for the assessment of river cross sections fig 10 shows the reconstructed surfaces for the data rich and the three data scarce scenarios the dr scenario fig 10a c includes all the points of the irregular three dimensional input dataset universal kriging resulted in reconstructed surfaces with several localised bumps simple and ordinary kriging resulted in very similar patterns however as suggested in section 2 2 these methods attribute more weight to the average value of the input dataset leading to non realistic results for morphologically complex areas for this reason only the surfaces derived using universal kriging are discussed in detail here conformal mapping algorithms resulted in smooth surfaces with features stretched in the stream wise direction however when using radial basis functions the conformal mapping algorithm returned some values lower than zero close to the river banks due to overshoots in the fitted surface the conformal mapping approach using polynomial functions was found to produce the smoothest surface across the methodologies implemented here in the data scarce scenarios ds1 ds2 and ds3 fig 10d l the algorithms were constrained using a decreasing number of measurement points the universal kriging method resulted in reconstructed surfaces of increasing smoothness nevertheless localised bumps persisted in each ds scenario the conformal mapping methods generally produced smooth features in the stream wise direction nevertheless the conformal mapping using radial basis functions still resulted in outliers in ds1 and in unrealistic deepening of the upstream river reach in ds2 and ds3 in fact the inaccuracies observed in the dr scenario were exacerbated when using a smaller input dataset the results of the conformal mapping method using polynomial functions were qualitatively consistent for all the scenarios the storage volume of the reconstructed river reach is shown in fig 11 top in the spline based algorithms cubic interpolations between cross sections could not reconstruct some deep pools thus leading to smaller values of reservoir volume moreover this method could not include the small river on the other side of the island the large storage volume predicted by the kriging algorithms is the outcome of the abrupt change in depth values close to the river banks in contrast as detailed in section 3 6 the conformal mapping approach used here contained a term that automatically forced the depth values to zero at the river banks it should be noted that only a relative comparison between the algorithms could be carried out as the actual storage volume for the reach is unknown fig 11 bottom shows the rmse between the reconstructed surfaces and the measured points the spline based algorithm resulted in the largest discrepancy from the measured data several meters the use of only the linearized cross sections and the lack of information provided by points sampled between cross sections significantly contributed to these large rmse values this result demonstrated that interpolation of data between adjacent cross sections can lead to large inaccuracies in reconstructed surfaces in the data rich dr scenario the input dataset and the evaluation dataset were identical for both the kriging and conformal mapping methods this choice does not allow a rigorous comparison between methods as reconstructed surfaces based on kriging are constrained to pass through all the input points whereas the conformal mapping algorithm produces parameterized surfaces which may not pass through these points due to this surfaces reconstructed using the conformal mapping method may have larger deviations from the input points however the comparison can serve to highlight different features of the kriging and conformal mapping algorithms consequently the universal kriging algorithm returned the lowest rmse value 0 08 m while conformal mapping algorithms resulted in larger rmse values with radial functions having higher point accuracy than polynomial functions rmse values were 0 27 m and 0 56 m respectively when evaluating the data scarce scenarios ds1 ds2 and ds3 the rmse was computed using the points excluded from the input dataset the conformal mapping method was less affected by the size of the input dataset than the kriging method more specifically for each data scarce scenario the conformal mapping with polynomial functions had the lowest loss of accuracy with a maximum pv of 38 in ds3 the conformal mapping with radial functions had intermediate performances with pv values of 340 380 and 430 for ds1 ds2 and ds3 respectively the universal kriging method resulted in the largest loss of accuracy with pv values of 640 750 and 830 for ds1 ds2 and ds3 respectively as a consequence the conformal mapping with polynomial functions resulted in rmse values seven times larger than rmse values of universal kriging for dr but similar 0 78 m for ds3 these results confirmed the high sensitivity of the kriging algorithm to the density of the input dataset detailed in previous studies legleiter et al 2008 and suggested that conformal mapping approaches allow for a certain degree of flexibility for sampling strategy despite rmse values allow only point scale assessment these results showed that exploring the geometry of the river from bank to bank was essential for bathymetric accuracy while navigating along the centre line or high along path sampling frequency added little information this latter conclusion was somewhat expected however the low sensitivity of the results of the conformal mapping algorithm to the sample dataset suggested that a zig zag navigation trajectory could be a cost effective strategy for field data collection if this method were to be used 4 2 2 cross sections comparison fig 12 shows measured and estimated depth profiles for three representative cross sections the locations of these cross sections named 4 12 and 25 are shown in fig 8c measurement points along the selected transects were available only for the dr scenario reconstruction was attempted using kriging with the addition of zero depth points at the river banks however the addition of these points strongly affected the results of the kriging algorithm especially for the data scarce scenarios this appeared to be caused by the kriging algorithm attempting to reproduce the zero depth points along the boundary and consequently returning flat non realistic surfaces which underestimated cross section flow capacity subsequently kriging algorithms were not forced to zero depth at the river banks and in order to allow a strict comparison between reconstruction algorithms these points were not considered in the following analysis in the data rich scenario universal kriging provided the most accurate reconstruction of the cross section shape with an average r value over the 32 cross sections above 0 96 the conformal mapping method with radial functions resulted in an adequate reconstruction of cross sections shape with average r value of 0 68 the conformal mapping method with polynomial functions could not precisely reconstruct many morphological singularities and resulted in an averaged r value of only 0 21 in the data scarce scenarios the kriging algorithm resulted in nearly flat cross sections as opposed to the conformal mapping methods which resulted in a large morphological variability especially when using the radial function flow area values were computed for cross sections included in the dr scenario but not used in the ds scenarios after excluding the points on the river banks fig 13 shows the rmse values between the values of flow area of reconstructed and measurement derived cross sections for each method and input data scenario universal kriging had the highest accuracy in the dr scenario yet computation of pv highlighted that this method had the highest sensitivity to the density of the input data more specifically in ds3 pv values were 1350 950 and 60 for universal kriging conformal mapping with radial functions and conformal mapping with polynomial functions respectively in fact the conformal mapping with polynomial functions was the least sensitive to the input dataset and resulted in the lowest rmse value for both ds2 and ds3 finally it is noted that the results of this analysis suggested that a reduced along path sampling frequency can be a viable solution when reducing along river sampling frequency 4 2 3 centre and thalweg line comparison fig 14 a shows the comparison between the reconstructed centre lines for dr ds1 ds2 ds3 measured points included and excluded by each scenario are also shown the spline based algorithm resulted in a large depth variation at the downstream boundary of the river reach since the analysis of the measured data did not support the existence of such a morphological feature this large oscillation was considered an artefact of the spline algorithm for the dr scenario the universal kriging algorithm attempted to match each measurement point rmse 0 26 m with the centre line showing small but frequent oscillations the conformal mapping approach with radial functions achieved a good point accuracy rmse 0 32 m with some irregularities while the conformal mapping approach with polynomial functions method returned a gently sloping centre line which reproduced the general trend of the measured data at the cost of a lower point accuracy rmse 0 45 m the oscillations and irregularities observed in the dr scenario for kriging and conformal mapping with radial functions were exacerbated in the ds scenarios in contrast the conformal mapping method with polynomial functions returned a smooth line in ds3 the pv of rmse metric for the universal kriging algorithm the conformal mapping method with radial functions and the conformal mapping method with polynomial functions were 148 159 and 43 respectively fig 14b shows the reconstructed thalweg lines and measured points for dr ds1 ds2 ds3 the point scale evaluation confirmed the results of the centre line analysis with the spline based algorithm resulting in large several meters oscillations which were not produced by other algorithms as mentioned above the spline based algorithms connect corresponding points in adjacent cross sections in this case study the measurement of complete cross sections was often impeded and points belonging to morphological features close to and far from the banks were sometimes artificially connected by the interpolation algorithm this led to non realistic artefacts in the reconstructed surface in the ds scenarios the conformal mapping method with radial functions resulted in large oscillations thus showing the highest sensitivity to input data 4 2 4 computational efficiency it is difficult to strictly compare the computational cost of the interpolation algorithms as they were implemented using different computation and processing methods the spline method was implemented in c using a single threaded approach the conformal methods were implemented in c with a graphical processing unit gpu accelerated method for the computation of the matrix coefficients in eq 28 and a c library eigen for the inversion of the dense matrix the kriging method was fully implemented in python and used the numpy library which uses the dgemm and dgesv subroutines from lapack dongarra et al 1991 to perform the matrix multiplication and dense matrix inversion operations in eq 20 overall the spline interpolation method and conformal mapping methods were found to take approximately the same time to complete approximately depending on the case however it should be noted that the spline interpolation method required cross sections as input data which were pre processed from the sampled points whereas the conformal and kriging methods could directly use these points without any pre processing the kriging method took much longer in some cases taking several hours to complete it is likely that this was mostly due to the overhead of paging large quantities of data between python and the matrix libraries however examination of the raw operational complexity shows that kriging requires an o n 3 inversion of a dense matrix where n is the number of measured data points plus the application of this inverted matrix to a vector for the n required data points o n n 3 plus a further n n dot product operations the creation of a conformal map requires the solution of two poisson equations this was implemented here using a direct o n 3 matrix inversion for accuracy plus a single least squares matrix inversion of o m 3 where m is the number of polynomial or radial basis function coefficients plus a further m n dot products hence although the initial solutions for conformal map generation and kriging require the same o n 3 operations to leading order the subsequent reconstruction takes o n n 3 operations for kriging and only o n m operations for the conformal mapping approach as a result the conformal mapping approach using interpolation will generally take a far shorter computational time than a kriging method 5 discussion the reconstruction of bathymetry in river systems is essential to the implementation of numerical models for ecosystem management in many catchments worldwide different levels of accuracy and detail of the reconstructed surface are needed for different applications including catchment scale flood forecasting biological studies and hydrodynamic flow models trigg et al 2009 legleiter et al 2011a b costabile et al 2015 glenn et al 2016 grimaldi et al 2018 this study was motivated in part by the need for a river reconstruction algorithm which could overcome or by pass the limitations of existing methodologies and support the investigation of a large number of real world scenarios more specifically the algorithm had to be automatic and computationally efficient yet capable of effectively handling islands complex morphologies and incomplete input datasets the resulting reconstructed surface had to meet the requirements for direct implementation of numerical models thus avoiding the need for any further time consuming tailored editing although many commercial geospatial software tools allow bathymetric reconstructions to be created these typically still require some manual input and can involve a complex processing workflow for example a workflow for bathymetric reconstruction of was developed in arcgis for the river section of the murray basin australia shown in fig 4 this consisted of reading transect data and resampling these measured depth profiles at a fixed number of equidistant points for cross channel transects this used basic arcgis feature conversion tools such as line splitting and subdivision then these profiles were linearly interpolated between corresponding points in the flow wise direction the interpolation maintained the proportional distance across the channel to ensure the interpolated depth was relative to the position across the channel regardless of bends in the river or variation in channel width this process populated the channel with a dense array of datapoints that was amenable to conventional spatial interpolation the method was found to work well but required a significant amount of manual effort at each stage and large numbers of additional data points to be generated the conformal mapping approach presented here may be able to overcome some of these constraints as river fitted co ordinates systems can be generated from only raster maps demarking water and land as demonstrated for the river sections in figs 4 and 15 applying an interpolation method over the conformal map then allows bathymetry to be reconstructed from either a sparse set of measured data points without needing to resample or generate further points along transects in particular it is difficult to handle features such as branching and islands in spline based sweeping methods especially in an automated manner such topology can be automatically processing using the conformal mapping approach for example fig 15 shows the resulting conformal map of the balonne river reach used in the study highlighting the ability of the method to deal with islands in the flow path the quality of a reconstructed surface depends on the location and spacing of depth measurements a sampling density capable of adequately capturing channel features is crucial for the reconstruction of channel morphology te stroet et al 2005 legleiter et al 2008 merwade et al 2008 glenn et al 2016 it is known that overshooting and abrupt vertical changes in reconstructed surfaces are often triggered or exacerbated by input data inaccuracy and sparsity merwade et al 2008 caviedes voullième et al 2014 and this can lead to significant inaccuracy in hydrodynamic flow modelling horritt et al 2006 nevertheless bathymetric measurements are affected by errors and data sparsity is an extremely frequent condition in real world contexts consequently reconstruction algorithms for hydrodynamic modelling must produce surfaces without overshoots and the quality of which are only loosely affected by the characteristics of the input dataset the proposed novel methodology was therefore tested on a real case scenario a 3 km long reach of the balonne river qld australia for which bathymetric survey data was available yet affected by a number of limitations and uncertainties which are common to many real world scenarios detailed in section 4 1 the subsequent analysis presented in this study demonstrated the negative impact on reconstructed surfaces from inadequate input data even when considerable effort was dedicated to the field data collection more specifically in the case study presented here data collection of straight bank to bank cross sections was impeded by navigation obstacles and weeds with a strong impact on the results of spline and kriging algorithms conversely the results of the conformal mapping algorithm were less affected by the sampling strategy in fact a sampling strategy based on cross sections perpendicular to the main stream has been strictly recommended to enhance the accuracy of kriging algorithms heritage et al 2007 santillan et al 2016 krüger et al 2018 however depending on the local flow conditions the pursuit of sampling trajectories perpendicular to the main stream might require time consuming manoeuvring and repetitions while obtaining data following zig zag sampling trajectories is usually easier and quicker the analysis of data scarce scenarios based on quicker cruise sampling trajectories requiring a fraction of the workload of the data rich scenario showed that the conformal mapping approach had lower sensitivity to the size and characteristics of the input dataset than kriging although further testing on other study areas are required the results presented in this paper suggest that the conformal mapping algorithm could be a cost effective solution for the reconstruction of river bathymetry in large basins a limited sensitivity of the results to the sampling path can enable optimal allocation of monetary resources for the sampling of longer river traits or for sampling repetition over time to monitor alterations of river path and morphology after flood events or changes in sediment transport conditions buffington 2012 soar et al 2017 the analysis suggests that the conformal mapping approach could be used to both lower the workload in the field and reduce computational time for the reconstruction of river bathymetry in large basins although the approach has been demonstrated here for the interpolation of ground based measurements the flexibility in the characteristics of the input dataset has the potential to allow reliable results for the interpolation of highly sparse measurements of bathymetry from remote sensors this type of data could be provided by the new generation of satellite borne lidar sensors that enable rapid detection of river depth in large river systems with relatively clear waters but with a sampling spacing as large as 10 m for example the advanced topographic laser altimeter system atlas a lidar system carried by the national aeronautics and space administration nasa satellite mission ice cloud and land elevation satellite 2 icesat 2 launched in september 2018 atlas has the potential for bathymetry retrieval at 19 m resolution forfinski sarkozi et al 2016 markus et al 2017 in middle depth water up to 8 m data from such sensors in conjunction with a mask of water areas could be potentially converted to bathymetric maps automatically using the conformal mapping approach 6 conclusions this paper presented a new and computationally efficient approach for the reconstruction of large river systems based on the construction of a river fitted co ordinate system using numerical conformal mapping and interpolation over this conformal map the accuracy and the optimal characteristics of the input dataset were assessed using a 3 km long river reach of the balonne river sampling datasets of decreasing size were sequentially used as input to the conformal mapping approach as well as benchmark spline and kriging algorithms the results showed that despite the morphological complexity of the surveyed area the conformal mapping approach with polynomial and radial basis interpolation produced a smooth surface that could be readily used as input to hydrodynamic models and which was loosely affected by sampling characteristics the results of this study suggest that the conformal mapping approach could enable efficient reconstruction of large river systems with islands and confluences in data scarce scenarios although further testing of the algorithm in other case studies and scenarios is required the analysis suggests that the accuracy of the reconstructed surfaces is only slightly affected by the decreasing size of the measurement sample this may relax the strict requirement for cross sectional depths sections to be measured perpendicularly from the river bank allowing for quicker and easier field campaigns and reducing the monetary and time costs for data collection finally although conceived for the interpolation of ground based measurements the proposed approach has adequate flexibility to be potentially applied for the interpolation of remote sensing lidar data future work includes extension of the current algorithm to the modelling of rivers with several islands the application of the conformal mapping algorithm to other study areas and the assessment of the impacts of the reconstructed surfaces on the outputs of hydrodynamic models acknowledgements the field data collection was funded through the monash faculty of engineering seed grants optimization of a hydraulic model using a doppler profiler and strategic high resolution monitoring of streams to improve operational flood forecasts stefania grimaldi is funded through the bushfire and natural hazards collaborative research centre grant improving flood forecast skill using remote sensing data 
26178,efforts to improve agroecosystem models require methods for unbiased comparisons among simulation algorithms with focus on evapotranspiration et in cotton2k the objectives were to develop a novel methodology for evaluating model parameterization options and to compare model performance using three et algorithms the cotton2k model was updated to include a standardized et method and two penman approaches were also tested sobol global sensitivity analysis and multiobjective optimization were used to identify influential parameters and select feasible parameterization options the three et methods led to differences in simulation accuracy for et soil water contents and several plant growth metrics p 0 05 however no et method could consistently outperform the other two methods and et simulation errors were up to 60 the simulation methodology permitted unbiased comparison of three et methods in cotton2k and highlighted areas for model improvement including the surface evaporation simulation and the linkage between simulated et and crop growth keywords cotton cotton2k crop model global sensitivity analysis multiobjective optimization water use 1 introduction evapotranspiration et is commonly the greatest pathway of water loss from crop production systems as a result the accuracy of water balance simulations in agroecosystem models is highly dependent on how well the model simulates et moreover the calculations of other model components including soil nutrient and crop growth and development algorithms depend on the accuracy of et and water balance simulations these concerns have driven recent efforts to evaluate and improve et calculations in many agroecosystem models including the decision support system for agrotechnology transfer dssat cropping system model csm attia et al 2016 dejonge et al 2012b marek et al 2017 sau et al 2004 thorp et al 2014b the root zone water quality model rzwqm anapalli et al 2016 and the soil and water assessment tool swat marek et al 2016 several of these model evaluation efforts have incorporated high quality daily et data sets such as those provided by the large weighing lysimeters at bushland texas anapalli et al 2016 marek et al 2016 2017 among various methods for et measurement weighing lysimeters that are properly designed installed and managed can provide accurate et data for agroecosystem model evaluation farahani et al 2007 a variety of methods exist for et simulation in agroecosystem models each having unique limiting assumptions algorithm complexity and input data requirements naturally comparisons of different et approaches both within and among models have aimed to identify the better performers farahani and bausch 1995 found better et estimates from the shuttleworth and wallace 1985 method as compared to a penman monteith approach ma et al 1999 demonstrated improved et simulations with energy combination methods as compared to pan evaporation approaches particularly when transpiration occurred lascano and van bavel 2007 compared explicit and recursive combination methods for computing penman based et finding the former to calculate as much as 25 less et than the latter on hot summer days in lubbock texas anothai et al 2013 compared priestley taylor and penman monteith et approaches in the dssat csm finding the latter method to better agree with measured et data from bowen ratio instrumentation kang et al 2009 performed a comprehensive comparison of et simulations from three wheat triticum aestivum l models cropwat modwht and dssat csm ceres wheat using measured et from the bushland weighing lysimeters and from gravimetric water content measurements at a site in china they noted overall poor simulation performance among the models and suggested considerable revisions were necessary to improve et calculations they also discussed the effects of interacting model components on et simulation performance highlighting for example the contribution of leaf area index lai simulation error to et error and vice versa further demonstrating the divergent nature of et methods in agroecosystem models kimball et al 2019 reported large variability in et simulation results from an intercomparison of 29 maize zea mays l models parameterized for iowa conditions the barriers to unbiased comparison of et methods in agroecosystem models are substantial most agroecosystem models are manually calibrated meaning input parameters are adjusted until performance is deemed acceptable through simple statistical calculations jacovides and kontoyiannis 1995 and human assessment of measured versus simulated data plots computational approaches can eliminate the potential for modeler bias to influence the calibration effort for example soldevilla martinez et al 2014 developed a simulated annealing global optimization method to calibrate and compare the dssat csm and the water and agrochemicals in soil crop and vadose environment wave model using measured drainage and et data from a weighing lysimeter and soil water measurements from capacitance sensors while the approach highlighted differences in the drainage simulations among the two models a primary disadvantage of simulated annealing and similar iterative optimization approaches is that only one solution is recommended by the algorithm despite substantial computational expense one alternative seeks to first develop a comprehensive database of model input and output relationships and any assessment of simulation output in comparison with measurements occurs subsequently irmak et al 2000 welch et al 2001 in addition to the benefit of fully representing model responses to input parameterization the database approach can also improve computational efficiency because simulations can be easily parallelized on high performance computers lamsal et al 2018 as recognized by kang et al 2009 another barrier to objective evaluation of et methods in agroecosystem models involves the shared feedback between the et algorithm and other model components for example parameter adjustments that improve et simulations might also worsen crop yield simulations and vice versa dejonge et al 2012a better et algorithms should demonstrate better performance in multiple aspects of the model simulation not just in the et simulation itself this means that the model parameterization effort requires the optimization of multiple often conflicting objectives as embodied in the comparison of measurements and simulation output for multiple types of agronomic data e g lai plant dry matter crop height yield et soil water content etc many approaches for multiobjective optimization have been developed for solving engineering design problems chiandussi et al 2012 zio and bazzo 2012 however application of these methods to agroecosystem model evaluation and comparison is relatively uncommon as one example charoenhirunyingyos et al 2011 combined measured and simulated data for lai et and soil water content into a single objective function and used a genetic algorithm to optimize the soil water atmosphere plant swap model similarly thorp et al 2015 used an objective function that combined measured and simulated data for lai et crop height and seed cotton yield to parameterize csm cropgro cotton using simulated annealing optimization neither of these studies incorporated the database approach of welch et al 2001 and therefore suffered the disadvantage of computationally intensive iterative optimization techniques resulting in only one solution furthermore welch et al 2001 described an additional drawback of efforts to combine multiple variables into a single objective function it permits the optimizer to differentially gain accuracy in one variable by sacrificing accuracy in another an improved strategy for multiobjective optimization problems involves the computation and assessment of solutions among the pareto optimal set cheikh et al 2010 mishra and harit 2010 taboada et al 2007 welch et al 2001 which is the subset of possible solutions that are not dominated by any other solution mathematical definition to follow unfortunately a single solution rarely optimizes all objective functions in multiobjective problems however the set of plausible solutions can be objectively narrowed by computing the pareto optimal solutions a final barrier to unbiased evaluation of et methods in agroecosystem models involves the statistical approaches used to make comparisons evaluations of models rarely entail more than calculations of simple statistical metrics such as root mean squared error rmse or mean bias error mbe jacovides and kontoyiannis 1995 demonstrated how rmse and mbe can be combined to calculate the t statistic which when compared to a critical t value from standard statistical tables can assess the statistical significance of the model s calculations at a given confidence level in their comparison of four et simulation methods the best performer was sometimes improperly selected if statistical hypothesis tests were not incorporated in the analysis based on hierarchical linear regression modeling thorp et al 2014b showed that csm cropgro cotton simulations of yield and et could explain variability in the measured data independent of the growing season their methodology provided statistical support that the model was responding appropriately to the agronomic treatments imposed in a given year by using assessments of statistical inference to compare et methods in agroecosystem models the conclusions regarding the relative performance of each method can be strengthened the overall goal of this study was to develop a novel methodology for unbiased evaluation and comparison of three et algorithms in the cotton2k agroecosystem model http departments agri huji ac il fieldcrops cotton field data for the analysis included et measurements from weighing lysimeters at bushland texas and other agronomic measurements from surrounding field experiments that compared fully irrigated deficit irrigated and dryland cotton gossypium hirsutum l production specific objectives were to 1 incorporate global sensitivity analysis multiobjective optimization and high performance computing to fully evaluate cotton2k parameterization options 2 compare the effects of three et algorithms on the accuracy of simulated et cotton fiber and seed yield soil water content and plant growth metrics using statistical inference and 3 evaluate et simulation behavior in cotton2k using crop coefficient methods 2 materials and methods 2 1 simulation workflow a novel simulation approach was developed for unbiased analysis and comparison of simulation results for three et algorithms in cotton2k fig 1 aspects of the workflow included 1 a sobol sampling scheme to choose large numbers of input parameterization options from a high dimensional parameter space 2 high performance computing to efficiently conduct large numbers of cotton2k simulations 3 a database approach to link input parameter sets with error statistics from comparisons of measured and simulated data 4 a global sensitivity analysis gsa to identify influential cotton2k input parameters 5 a multiobjective optimization moo method to calculate pareto optimal solution sets by evaluating model error statistics among multiple agroecosystem metrics 6 a pruning algorithm to cull pareto optimal solutions based on a user specified order for objective function priority and 7 classical inferential statistics to assess differences in model performance among the pruned pareto optimal sets for each et algorithm further details on the field measurements the cotton2k model and the workflow implementation are described in the following sections 2 2 field measurements cotton field experiments to quantify et of fully irrigated deficit irrigated and dryland cotton production were conducted in four weighing lysimetry fields at the usda ars conservation and production research laboratory cprl near bushland texas 35 187 n 102 097 w 1170 m above mean sea level during the 2000 and 2001 growing seasons howell et al 2004 also the bushland evapotranspiration and agricultural remote sensing experiment bearex08 quantified et for fully irrigated and dryland cotton production at the same site during 2008 evett et al 2012 the soil texture at the site was predominantly clay loam and silty clay loam as determined from textural analysis of soil samples tolk et al 1998 growing season precipitation short crop reference et from april through september amounted to 155 1707 160 1579 and 230 1624 mm in 2000 2001 and 2008 respectively strong regional advection from the south and southwest typically led to high reference et values at the site and low precipitation levels led to water limitation and need for irrigation in all three seasons irrigation was applied using a 10 span lateral move overhead sprinkler irrigation system lindsay manufacturing omaha nebraska equipped with mid elevation spray application mesa nozzles at a height of approximately 1 5 m above the ground surface the machine was oriented from north to south traveled in an east or west direction and irrigated two lysimeter fields simultaneously four large weighing lysimeters were installed at the bushland field site in the 1980 s marek et al 1988 and have been used to monitor et for a variety of crops for nearly three decades evett et al 2012 2016 howell et al 1995 2004 during the 2000 and 2001 cotton studies the southeast and northeast lysimeter fields were managed using full and limited irrigation respectively full irrigation was defined as weekly irrigation to replenish root zone soil water content to field capacity and limited irrigation was half of the full rate in 2008 season both the northeast and southeast lysimeter fields were fully irrigated the northwest and southwest lysimeter fields were not irrigated dryland production in 2001 or 2002 and less than 130 mm was applied in the 2008 early season to encourage germination and emergence soil water content was periodically measured at two access tube locations in each lysimeter using a calibrated neutron scattering probe model 503dr hydroprobe cpn international inc martinez california which provided data from 0 1 to 1 9 m in 0 2 m incremental depths specific protocols for weighing lysimeter measurements during the three cotton growing seasons were given by howell et al 2004 and evett et al 2012 howell et al 1995 discussed the calibration technique for mass measurement within the lysimeter which can provide et estimates at time scales less than 1 h more recently marek et al 2014 presented techniques for quality assurance and quality control of data collected from the lysimeters based on this post processing protocol lysimeter et data for the present study was aggregated on a daily basis from 1 january through 31 december in 2000 2001 and 2008 cotton planting dates ranged from mid may to early june in the three growing seasons after establishment cotton plants were destructively sampled on a two week basis from small areas 1 0 2 0 m2 more than 10 m away from the lysimeter the samples were processed in the laboratory to estimate leaf area index lai leaf dry matter ldm stem dry matter sdm boll dry matter bdm canopy height cht mainstem node count nod green boll count gbl and mature boll count mbl cotton harvest dates ranged from late october to early december in the three growing seasons yield measurements were obtained by sampling mature bolls from five 10 0 m2 areas in each lysimeter field turnout percentages were measured using a small research gin which provided data for fiber yield fby cottonseed yield sdy and seed cotton yield scy 2 3 cotton2k the cotton2k agroecosystem model is described online at the website provided previously and was recently reviewed by thorp et al 2014a model development descended from several early and notable efforts in cotton growth modeling including gossym baker et al 1972 1983 simcoti baker et al 1972 simcotii jones et al 1974 and calgos marani et al 1992a b c primarily cotton2k made these models more relevant for cotton production in arid irrigated environments such as the western u s and israel whereas gossym calculates water balance processes on a daily time step cotton2k uses hourly weather information to calculate hourly water and energy balances which is thought to improve the accuracy of the model s et calculations cotton2k simulates a two dimensional soil profile with a depth of 2 m and width equal to the cotton row spacing lambert et al 1976 bar yosef et al 1982 the soil profile is divided into a fixed number of small compartments both horizontally and vertically water and nitrogen contents and root growth are calculated in each compartment additionally the compartments are grouped vertically into a set of nine soil horizons each with a thickness of 15 cm van genuchten 1980 parameters are used to define the soil water holding and hydraulic conductivity characteristics in each soil horizon these were determined by fitting the model to soil water measurements and other data as discussed later due to lacking experimental data initial conditions for nitrate and ammonium were assumed to be 5 0 kg ha 1 for all soil horizons and all simulations initial organic matter was specified uniquely for each experimental year based on limited pre season measurements of surface organic matter at the field site and estimates of vertical organic matter distribution from a usda soil survey of the study area initial soil water content was estimated as the average of neutron scattering probe readings during the subsequent growing season and were specified uniquely for each lysimeter field management information required by the model including irrigation and fertilization schedules and dates of planting and harvest was specified as recorded for each lysimeter field highly detailed and comprehensive simulations of cotton development and growth and are possible with cotton2k the model simulates development of mainstem nodes in response to air temperature using heat unit concepts and the development of vegetative or fruiting branches from each mainstem node is also simulated development of leaves and reproductive structures including squares green bolls and mature bolls is simulated at individual sites along the vegetative and fruiting branches this permits simulation of cotton plant maps where the position of each reproductive structure on the simulated plant is explicitly considered growth rates of unique plant organs including roots stems leaf blades petioles squares green bolls and mature bolls are based on carbon supply and demand relationships effects of water stress on simulated plant growth are calculated as a function of leaf water potential and nitrogen stress effects depend on supply and demand relationships for nitrogen in vegetative material fruits and roots jones et al 1974 stress alters simulated plant growth via reductions in organ growth potential and shedding of squares and bolls i e abscissions a set of 51 variety parameters are used to simulate the effects of genetics on cotton growth and development responses a number of source code edits were required to facilitate efficient model input output i o for high performance computing and to correct any encountered coding errors for example the model code was altered to directly output soil water contents corresponding to the positions of neutron scattering probe measurements also some parameter sets caused underflow or overflow errors at run time which required coding edits to restrict ranges for certain state variables a fortran version of cotton2k was obtained from the developers of the palmscot landscape scale cotton modeling tool booker et al 2014 2015 although a newer c version of cotton2k exists the code for cotton growth simulations is interwoven with code for its microsoft windows based graphical user interface gui which precluded its use on a linux high performance computing system after incorporating coding edits the model was compiled using gfortran within the open source gnu s not unix gnu compiler collection www gnu org 2 4 evapotranspiration methods the et methodology in cotton2k is based on the california irrigation management information system cimis algorithm using the modified penman equation snyder and pruitt 1985 two options are possible for obtaining the required hourly weather information either input hourly data directly as measured cimis hr or use the methodology of ephrath et al 1996 to estimate hourly data from daily measurements cimis dy both of these options were tested in this study table 1 required hourly meteorological information including solar irradiance mj m 2 air temperature c dew point temperature c and wind speed km d 1 was obtained from a texas high plains et network weather station which was positioned over a well watered clipped grass surface adjacent to the field site hourly precipitation data mm were obtained from a tipping bucket rain gauge managed by the experimentalists at the field site daily weather inputs included maximum and minimum daily air temperature along with the daily aggregations of the other weather measurements since the initial development of the cotton2k model efforts in the et community have aimed to standardize et computations allen et al 1998 walter et al 2005 therefore a third et option ascek hr table 1 was added to cotton2k based on the findings of dejonge and thorp 2017 who demonstrated the use of more modern and standardized et methodologies to identify problematic et responses in another agroecosystem model computations of short crop reference et etos or tall crop reference et etrs from meteorological data form the basis of standardized et etsz calculations walter et al 2005 the standard algorithm for hourly etos was added to cotton2k which required the same hourly weather data as discussed previously similar to dejonge and thorp 2017 a dual crop coefficient methodology allen et al 1998 was used in the ascek hr et method to calculate basal crop coefficients kcb from simulated lai 1 k cb k cbmin k cbmax k cbmin 1 exp s kc lai maximum kcb kcbmax was specified as 1 25 based on the fao 56 tabulated value for cotton with appropriate adjustment for the bushland environment the kcbmin and skc factors were fixed at 0 0 and 0 6 respectively as discussed in dejonge and thorp 2017 evaporation coefficients ke were calculated using the methods described in fao 56 allen et al 1998 daily etos was summed from hourly etos and used to calculate daily potential transpiration epo kcbetos and potential soil water evaporation eso keetos although a standardized etos algorithm for daily weather data also exists walter et al 2005 it was deemed nonsensical for the current study because the cotton2k water balance fundamentally operates with an hourly timestep even with the cimis dy et method table 1 daily weather data was immediately converted to hourly data upon input to the model ephrath et al 1996 2 5 simulations cotton2k was setup to run 12 simulation scenarios based on the three cotton growing seasons and four uniquely managed lysimeter fields table 2 simulations were initiated on 1 january of each year and concluded on the recorded harvest date for each lysimeter field within the southwest lysimeter field in 2001 twin rows spaced 25 cm apart were planted on 76 cm centers because cotton2k did not consider this planting configuration a row spacing of 38 cm i e half of 76 cm was simulated simulations were conducted using usda s new high performance computing resource called ceres which consists of 64 compute nodes each having 40 logical cores on intel xeon processors with hyper threading and a shared 2 pb storage system with lustre design the operating system on ceres was a linux centos distribution ver 6 7 located in ames iowa access to ceres occurred via the dedicated high speed networking resource called scinet about 60 cotton2k simulations s 1 were possible on ceres as compared to no more than 3 simulations s 1 on a modern desktop machine thus high performance computing increased simulation capability by a factor of 20 a python script www python org that incorporated python s multiprocessing package was developed to manage the simulation tasks on ceres the python script loaded a list of parameter sets into a processing queue created 40 working directories for conducting simulations in parallel and copied pertinent cotton2k files to each directory it then established 40 independent worker processes one for each requested processing core each worker process iteratively selected an item from the processing queue adjusted cotton2k input files to incorporate the current parameter set conducted the 12 simulation scenarios and extracted simulated data from cotton2k output files for pairing with associated measurements measured and simulated data for 22 agroecosystem metrics table 3 were aggregated among the 12 simulation scenarios table 2 for each tested parameter set by calculating the percent root mean squared error rmse uniquely for each metric 2 rmse i f i m i s i 100 m i 1 n i j 1 n i m i j s i j 2 where m i s i and n i are the measured and simulated data vectors and vector length respectively among the 12 scenarios for the ith metric table 3 the python script and simulation job concluded by outputting a model response database that included the cotton2k input parameter sets with associated rmse statistics for each of the 22 agroecosystem metrics 2 6 first sobol sampling the simulation workflow entailed two main phases one leading to a sobol global sensitivity analysis and another leading to a multiobjective optimization approach for model calibration fig 1 both phases began by using a sobol sampling procedure to choose high dimensional parameter sets for input to cotton2k a python script that incorporated the sensitivity analysis library salib was developed to conduct the sobol sampling and later to compute the sobol sensitivity indices saltelli 2002 saltelli et al 2010 sobol 2001 sobol sampling techniques were previously shown to be advantageous and more efficient to develop databases that describe high dimensional model input and output relationships lamsal et al 2018 because the sobol algorithm can select parameter sets that are more evenly dispersed across the multidimensional parameter space initially 72 model input parameters were sampled for inclusion in the sobol gsa table 4 1 18 parameters that defined the sand and clay contents for each of the nine soil horizons 2 two parameters that define the soil matric potential at field capacity smpfc and at which free drainage occurs smpfd 3 the 51 cotton variety parameters varpar01 through varpar51 and 4 the skc parameter for et simulations with the ascek hr method eq 1 the sand and clay contents chosen by the sobol algorithm were input to the rosetta pedotransfer function zhang and schaap 2017 to calculate the associated van genuchten 1980 parameters for each soil horizon appropriate parameter ranges table 4 for sand and clay were based on texture measurements of soil samples collected at the field site tolk et al 1998 based on recommendations in the model documentation smpfc ranged from 0 38 to 0 28 bars and smpfd ranged from 0 15 to 0 05 bars 1 bar 100 kpa appropriate ranges for variety parameters were chosen by using spreadsheet software excel 2013 microsoft corporation redmond washington to test outcomes of the model equations that incorporated each parameter table 4 the appropriate equations were taken from the model code and programmed into the spreadsheet to facilitate tests of equation outcomes with different input parameter values which permitted specification of reasonable parameter ranges the skc parameter was varied from 0 5 to 0 9 based on the recommendation of dejonge and thorp 2017 many of the cotton2k variety parameters are unitless variables used in empirical equations that drive specific plant growth or development characteristics the n parameter of the sobol sampling algorithm was set to 34 723 with specification to prepare for calculation of second order sensitivity effects thus the number of n dimensional parameter sets n 72 chosen by sobol for the sobol gsa was n 2 n 2 5 069 558 as defined within the sobol algorithm table 5 the value of n was selected based on preliminary estimates of the rate of simulations on ceres with plans to contain the simulation timeframe to within a couple weeks most importantly as revealed by later tests the number of simulations was more than satisfactory to ensure stability of sobol sensitivity indices with 12 simulations per parameter set the sobol gsa needed a total of 60 834 696 simulations which required 60 217 cpu hr on ceres and approximately 301 h of wall clock time to minimize computational expense simulations for the gsa were conducted only for the asce hr et method table 1 2 7 global sensitivity analysis to gain insights on cotton2k responses to adjustment of model input parameters a sobol gsa cariboni et al 2007 pianosi et al 2016 saltelli et al 2000 was conducted using algorithms from the salib package in python saltelli 2002 saltelli et al 2010 sobol 2001 an astute reader will note that less computationally intensive sensitivity analyses are normally conducted first followed by more intensive methods like sobol gsa here only sobol gsa was used because 1 the availability of high performance computing resources ensured efficiency of simulations and 2 subsequent portions of the workflow also incorporated the sobol sampling aspect of the sobol gsa fig 1 using algorithms from the salib package first order second order and total sensitivity indices were calculated for each combination of the 72 input parameters and 22 agroecosystem metrics the rmse statistic for each agroecosystem metric was used as the objective function for sensitivity index calculations based on the recommendation of zhang et al 2015 any parameter having a first order sensitivity index greater than 0 05 for any of the 22 agroecosystem metrics was considered an influential parameter and remained flexible in the subsequent analysis fig 1 table 4 other parameters were considered non influential and were fixed to default values for the remainder of the analysis the overall purpose of the sobol gsa was to eliminate non influential parameters prior to using multiple objective optimization for model calibration 2 8 second sobol sampling the gsa results suggested that only 35 of the 72 cotton2k parameters influenced the simulation outcomes table 4 subsequently the sobol sampling algorithm in salib was again used to choose cotton2k parameter sets but only among the influential parameters as identified by the sobol gsa a second gsa based on the reduced parameter set was not conducted instead simulation results based on the second sobol sampling was used for multiobjective optimization to identify the parameter sets that provided optimal agreement between measured and simulated results it was assumed that the best solutions to the multiobjective optimization problem were among the parameter sets chosen by the second iteration of sobol sampling fig 1 each sobol parameter set was used for simulations of the 12 cotton2k scenarios table 2 using three different et methods in the model table 1 resulting in three model response databases that linked parameter sets to rmse outcomes for each agroecosystem metric table 3 using the same sobol n parameter for these runs n 34 723 the number of n dimensional parameter sets n 35 chosen by sobol was n 2 n 2 2 500 056 with 12 simulations per parameter set for three et approaches this analysis needed 90 002 016 simulations which required 89 889 cpu hr on ceres and approximately 449 h of wall clock time table 5 2 9 multiobjective optimization because there were 22 agroecosystem metrics to consider table 3 optimizing the cotton2k parameterization required multiobjective optimization moo techniques taboada et al 2007 the objective function to be optimized incorporated k unique rmse calculations one for each agroecosystem metric k 22 expressed as 3 f moo m s f 1 m 1 s 1 f 2 m 2 s 2 f k m k s k where the terms are as described for equation 2 equation 3 represents the set of rmse calculations eq 2 for each of k agroecosystem metrics based on the results of simulations for a given parameter set among more than 2 5 million sets tested table 5 the first step toward reduction of plausible parameter sets was to calculate the subset of pareto optimal solutions cheikh et al 2010 which were the solutions that were not dominated or non dominated by any other solution in mathematical terms a solution x 1 dominates another solution x 2 if the following two conditions are met f i x 1 f i x 2 for all i 1 2 k f j x 1 f j x 2 for at least one j 1 2 k in words a solution dominates another if the rmse calculations for k agroecosystem metrics are all less than or equal to that for the other and at least one rmse calculation is less than that for the other the goal was to find the parameter sets with rmse calculations that were not dominated by the rmse calculations for any other parameter set a python script was developed to calculate the pareto optimal solution set for each cotton2k et method efficiencies in computation were gained by sorting the data set such that non dominated solutions were more likely evaluated first mishra and harit 2010 ceasing evaluation of a solution immediately after determining it was dominated and using python s multiprocessing package to divide computational load among processors on ceres a known problem with pareto optimal sets is that they often remain large and cumbersome and they do not adequately ease the burden of selecting one or several practical solutions taboada et al 2007 tested two approaches to aid selection of practical solutions by pruning the pareto optimal set one based on a clustering technique and another based on a user defined priority ranking among the set of objective functions eq 3 borrowing from the latter strategy a python script was created to reevaluate each pareto optimal solution and combine the k objective function outcomes to a single evaluation criterion by assigning random weightings approximately in order of objective function priority specifically k random weightings w i 1 k w i 1 0 were computed by using python s numpy package to 1 generate k random numbers from a uniform distribution r 0 r i 1 i 1 2 k 2 sort r from highest to lowest and 3 calculate w i r i j 1 k r j i 1 2 k given the objectives of this study and that crop yield and water use are commonly the most important of agroecosystem metrics the priority of objective functions were specified in the following order et fby lai scy cht sdy nod ldm sdm gbl mbl and bdm followed by the ten soil water content measurements from top to bottom in the profile table 3 weightings were iteratively assigned to objective function in order of priority but to relax strictness of the subjectively chosen order weightings were assigned randomly from the highest three remaining weightings this allowed the algorithm to vary the objective function priority at each iteration while generally preserving the priority overall the evaluation criterion c was then computed for each solution in the pareto optimal set 4 c i 1 k w i f i m i s i and the solution having the minimum c was identified as a pruned pareto optimal solution to ensure equal treatment among objective functions the values f i m i s i for each objective function i e see equation 2 were normalized from 0 0 to 1 0 prior to computing c the process of random weighting computation and c computation was iterated until 100 000 iterations passed without identification of a new pruned solution the set of pruned pareto optimal solutions determined from this process was used for all further analysis the pareto optimal sets for each of the three cotton2k et methods were pruned using identical random weightings multiple trials of the pruning algorithm produced very similar results which highlighted the consistency of the approach because the algorithm also counted the number of times a particular pruned solution was selected a single most popular solution could be identified however according to the definition of multiobjective optimization all solutions in the pareto optimal set have equal feasibility as a solution to the optimization problem 2 10 et method comparison the performance of the three cotton2k et methods table 1 was compared by conducting an analysis of variance anova on rmse results for each of the 22 agroecosystem metrics table 3 among the pruned pareto optimal solutions tukey s multiple comparisons tests were also conducted to identify which cotton2k et method resulted in statistically different rmse values for each agroecosystem metric p 0 05 and the lowest of these identified the better et method for a given metric statistical analysis was conducted using the r project for statistical computing software www r project org further efforts involving measured and simulated crop coefficient plots were used to evaluate and compare et time series for the most popular pareto optimal solution for each et method dejonge and thorp 2017 demonstrated how crop coefficient methods can be used to assess the et outputs of agroecosystem models relative to either measured or theoretically expected crop coefficient time series allen et al 1998 the approach required daily calculation of the evaporation coefficient ke e etos the basal crop coefficient adjusted for water stress kcbks t etos and the single crop coefficient kc et etos where e t and et are the daily cotton2k simulated outputs for soil water evaporation plant transpiration and evapotranspiration respectively furthermore measured kc values were computed from the weighing lysimeter data 3 results 3 1 global sensitivity analysis the gsa identified 35 influential parameters of the 72 included in the analysis table 4 these parameters contributed most substantially to sensitivity in the model outputs as expected sand and clay parameters at each soil depth had the greatest influence on the simulated soil water contents near that depth the smpfc parameter soil matric potential at field capacity was most influential on simulated soil water content in the deeper soil layers from 120 to 200 cm while the smpfd soil matric potential at which free drainage occurred parameter was influential on soil water content throughout the entire soil profile six variety parameters were influential for four or more of the agroecosystem metrics varpar01 varpar04 varpar31 varpar32 varpar47 and varpar49 four of the 22 agroecosystem metrics including lai ldm cht and nod were sensitive to the varpar01 parameter which adjusts plant growth simulations in response to plant density the varpar04 parameter which controls growth potential of leaves at prefruiting nodes mainly influenced et and soil water contents at 10 70 and 90 cm likely through its effects on lai the varpar31 parameter had greatest effects on et cht and soil water contents at all the soil depths the parameter defines the physiological days required for developing the fourth through the ninth prefruiting nodes and thus has great effect on late stage vegetative growth prior to fruiting similarly varpar32 defines the physiological days for development of the first and second prefruiting nodes and was influential on et and soil water contents at 3 of the 10 depths varpar47 through varpar50 are used to calculate the probability of fruit abscission as a function of fruit age and these were the most influential parameters on gbl fby sdy and scy other influential parameters had more specific effects on cotton2k outputs varpar12 which controls stem growth potential primarily influenced sdm varpar21 and varpar22 both influenced cht through their effects on calculations of mainstem growth as a function of prefruit node age after second square likewise varpar26 has a multiplicative effect on daily vertical stem growth and was therefore influential on cht simulations booker et al 2014 also identified varpar26 as a highly influential parameter varpar30 acts as a multiplier on the temperature function used to calculate squaring date and it was influential on gbl as expected both lai and ldm were sensitive to varpar34 which sets the initial leaf area of prefruiting leaves varpar35 which was influential on cht affects the calculation of physiological days between successive fruiting branches results of the gsa were often intuitive and helped to identify influential model parameters for subsequent model parameterization efforts table 4 fig 1 3 2 multiobjective optimization recalling that the second sobol sampling resulted in 2 500 056 cotton2k parameter sets that were evaluated using multiobjective optimization table 5 calculation of the pareto optimal solutions for the cimis dy cimis hr and ascek hr et methods reduced the number of plausible solutions to 461 448 369 347 and 419 784 respectively thus the pareto optimal solution set eliminated up to 85 of the total evaluated solutions although the remaining number was still very large pruning the pareto optimal solutions using the objective function weighting approach further reduced the number of solutions to 19 12 and 22 for the cimis dy cimis hr and ascek hr et methods respectively this means the multiobjective optimization technique could reduce the solution set to better than 0 0009 of the total evaluated solutions the result highlighted the difficulty of practical decision making from the pareto optimal set while pruning the set based on limited user input i e the order of objective function importance substantially reduced the number of solutions and improved the practicality of determining a feasible model parameterization 3 3 statistical analysis simulation results among the three et methods in cotton2k table 1 were different p 0 05 for 15 of the 22 agroecosystem metrics et lai scy cht sdy nod ldm gbl mbl and soil water content at 10 30 50 130 150 and 190 cm table 6 with the ascek hr method et was simulated significantly better than with the other two et methods although the mean rmse improvement was less than 1 because the rmse calculation for et involved more than 3700 data values table 3 rmse variation among the pruned pareto optimal sets was low which contributed to the significant difference among et methods the ascek hr method also performed significantly better than the other two et methods for cht gbl and soil water content at 130 cm however the cimis dy method performed significantly better than both the other two methods for sdy the cimis hr method did not perform significantly better than both the other methods for any of the agroecosystem metrics collectively cimis dy and cimis hr both performed better than ascek hr for scy ldm and soil water contents at 10 30 and 50 cm thus the statistical results demonstrated no clear winner among the three et methods generally each et method statistically outperformed the other methods for one or more metric but not without significantly underperforming on a different metric an exception was in the comparison of cimis dy and cimis hr where the results showed no advantage to using directly measured hourly weather data cimis hr as compared to estimating hourly weather from daily measurements based on ephrath et al 1996 cimis dy that is cimis dy performed as well as or better than cimis hr for all 22 agroecosystem metrics an unexpected result but nonetheless demonstrated statistically importantly based on comparisons to daily measurements from weighing lysimeters at bushland the ascek hr standardized et method statistically outperformed the cimis dy and cimis hr methods for simulating et in cotton2k 3 4 model evaluation the pruning algorithm iterated 465 213 times to identify 22 or less pruned pareto optimal solutions for each et simulation method among the results at each iteration the most popular solutions for the cimis dy cimis hr and ascek hr methods were selected 63 86 and 61 of the time the most popular solutions for cimis hr and asce hr were identical table 4 perhaps because these two et approaches used the same weather input data for cimis dy a different solution was most popular the solution identified as the most popular for each method i e most commonly selected by the pruning algorithm was used to parameterize cotton2k for all further evaluations of the model of the 2 500 056 evaluated parameter sets only one parameter set was commonly present in the pruned pareto optimal solutions for all three et methods but it was not a most popular solution for any method to more fully evaluate the et simulations in cotton2k the simulation timeframe was divided into sections dominated by soil water evaporation 1 january through 31 may and transpiration 1 june through 30 september because cotton2k simulations terminated on the specified harvest date table 2 availability of information for november and december was inconsistent so it was not included here cotton2k consistently underestimated cumulative et during the soil water evaporation dominated portion with rmse 44 for all three et methods fig 2 a because soil water evaporation is simulated based only on the water content of the top soil layer the underestimation may be due to inadequate simulation of upward water flux from deeper soil layers for example if the simulated soil water evaporation demand is larger than the upward water flux into the top soil layer soil water evaporation will decline even though the next deepest soil layer may have plenty of water to meet the demand cumulative et during the transpiration dominated period was also consistently underestimated by the model fig 2b for all et methods the rmse for the ascek hr method 17 2 was lowest among the et methods with better performance apparent at the extreme levels of et corresponding to fully irrigated treatments overall after thorough evaluation of cotton2k parameterization options with three et simulation strategies cotton2k consistently underestimated et as compared to et measurements from the bushland weighing lysimeters fig 2 the cotton2k simulations of lai were mostly overestimated for lai 2 5 m2 m 2 and underestimated for lai 2 5 m2 m 2 fig 2c only the fully irrigated treatments in 2001 and 2008 achieved a maximum lai of 3 0 m2 m 2 or more thus the model was generally unable to fully respond to plant growth conditions under full irrigation while also often drastically overestimating lai for water limited conditions simulation results for scy were moderately accurate fig 2d although the scy results for the ascek hr et method were worse than for the other two methods overall cotton2k simulations of lai and scy were poorer than that obtained with a different agroecosystem model at a nearby research station in west texas modala et al 2015 and in central arizona thorp et al 2017 note that the rmse statistics for lai and scy are different in fig 2 as compared to table 6 because the latter provides mean rmse among solutions in the pruned pareto optimal set while the former reports rmse for the most popular pruned pareto optimal solution 3 5 evapotranspiration behavior as suggested by dejonge and thorp 2017 analysis of the crop coefficients i e kc kcbks and ke as computed from simulated et variables and etos can reveal insights about model behavior and functionality as well as the reasonableness of simulated transpiration and soil water evaporation time series the approach is also useful for further comparisons among et simulation methods crop coefficient plots for simulations of the fully irrigated 2008 nelys field generally showed similar behavior between the three et methods fig 3 for example all three methods demonstrated similar pre season doy 100 160 and post season doy 275 300 spikes in kc indicating increased soil water evaporation and ke due to precipitation events however the pre season peaks in simulated kc were not as high as measured kc from the lysimeter indicating underestimated soil water evaporation no matter which et algorithm was used mid season kc from doy 190 to 240 was better simulated with the ascek hr method than with the cimis methods as compared to kc from lysimetry gross underestimation of kc at the end of the season from doy 260 to 275 contributed to the general trend of underestimated et during the transpiration dominated portion of the season fig 2b and indicates need for improvements to the late season transpiration calculations other aspects of the crop coefficient curves highlighted localized differences among the simulated et time series although the rmse between measured and simulated et were not different for cimis hr and cimis dy table 6 visual differences in mid season kc doy 220 240 indicated different simulated et time series for the two cimis methods fig 3 during this time the field received 106 mm of precipitation over a five day period doy 226 230 despite simulation of full canopy during these days lai 2 6 not shown the cimis dy method simulated higher evaporation with ke ranging from 0 37 to 0 50 this resulted in high kc values reaching 1 74 for cimis dy on doy 230 fig 3a the ke results for cimis hr and ascek hr were smaller during this time 0 29 0 34 figs 3b and 0 19 0 31 fig 3c respectively this reduced kc to 1 22 or less for cimis hr and ascek hr which is closer to a more typical mid season value the result was likely due to providing precipitation data in hourly versus daily amounts specifying rain events hourly allowed infiltration processes to be simulated over several timesteps while specifying daily rainfall amounts filled the top soil layer in a single timestep and increased opportunity for soil water evaporation thereafter because the ascek hr method generally had high lai fig 2c and thus kcb eq 1 the ke peaks generally decreased as the season progressed from doy 180 to 280 fig 3c this was a sensible response because increases in plant growth should further limit soil water evaporation and similar behavior was recently found with another agroecosystem model dejonge and thorp 2017 however neither ke time series for cimis dy fig 3a nor cimis hr fig 3b demonstrated this declining trend over the season the results indicated that the soil water evaporation routine for the cimis methods in cotton2k should be further investigated and potentially improved another interesting finding was the drastic changes in transpiration i e kcbks as simulated for the 2000 swlys field with dryland cotton production particularly for the cimis et methods fig 4 crop coefficients from the cimis et methods exhibited tremendous and seemingly unrealistic transpiration variability whereas the ascek hr method resulted in a more steady increase in transpiration for example over a sixteen day period from doy 178 193 kcbks for cimis hr varied from 0 15 to 1 11 and back to 0 28 fig 4b because lai was simulated as a steadily increasing curve over this time period not shown it seems unlikely that the crop s transpiration potential shifted from near zero to full potential and back to near zero in just a few days the cimis dy method also exhibited a similar large swing in kcbks values at this time fig 4a however the ascek hr method demonstrated a more typical kcbks curve that generally followed the pattern of biomass growth because the method intrinsically linked simulated lai with transpiration via equation 1 fig 4c because simulated kcbks often appeared incongruent with simulated lai time series not shown the interaction between vegetation production and transpiration needs further investigation to achieve more reasonable crop coefficient time series particularly under water limited conditions with the cimis methods in cotton2k 4 discussion verifying the accuracy of simulation results from agroecosystem models is a fundamental research question that has generated numerous studies on techniques for model evaluation sensitivity analysis and model intercomparison herein a novel approach was developed to compare and highlight deficiencies in cotton2k simulations based on three et algorithms the main advantage of the approach was its inherent objectivity there was very little effect of subjectivity in the model comparisons when subjectivity was required for example in specifying the parameter ranges table 4 the threshold for sensitivity indices or the priority order of agroecosystem metrics table 3 the decision could be applied uniformly among all et methods thus the comparisons were more scientifically robust and repeatable as compared to model intercomparison techniques that involve more subjective model parameterization decisions through the use of high performance computing the approach also permitted evaluation of many more parameter sets than could be practical using manual model parameterization techniques thus the comparisons could be conducted with a more thorough assessment of model responses to variability in parameterization the results also reinforced an important lesson about agroecosystem model evaluation when multiple types of measured data are available it can be difficult to find solutions that improve simulations of all metrics simultaneously given any two model parameterizations considered in this study there was a 15 18 chance that one parameterization was no better than the other when considering all 22 agroecosystem metrics the implications for manual calibration are potentially freeing for perfectionist modelers who are frustrated when they can no longer improve simulations of one metric without worsening simulations of another that is the calibration effort may have approached a solution within the pareto optimal set and further efforts to reduce simulation error among all metrics may be futile furthermore the number of solutions among the pareto optimal set is likely too large to be assessed adequately using manual calibration methods but more advanced computational techniques could be used for further assessment the gsa highlighted opportunities to condense and simplify the crop variety parameters in cotton2k only 15 of 51 variety parameters were identified by the gsa to substantially influence the simulation results many of the parameters work together in empirical equations to control a particular aspect of the simulation for example three parameters are involved in leaf growth on the mainstem nodes four parameters affect post squaring stem growth and four other parameters control the probability of boll abscission table 4 efforts to merge the effects of such parameters could possibly increase their sensitivity while also reducing the complexity of the model parameterization however realistic simulation detail should not be lost in the effort to reduce parameters conducting the gsa for each of the 12 scenarios separately may also reveal differences in model sensitivity that are caused by environment using three et algorithms cotton2k simulated et lai and scy with rmse between measured and simulated data ranging from 59 6 to 60 9 60 7 64 6 and 40 7 44 6 respectively table 6 however as reported by modala et al 2015 and thorp et al 2017 the rmse results for these important agroecosystem metrics were lower for a different cotton simulation model based on data from different sites in the western u s this result is surprising because cotton2k simulates cotton growth and development and soil water balance processes in more detail than other cotton simulation models for example cotton2k simulates a two dimensional soil profile and considers development and growth of individual cotton bolls whereas other models simulate these processes with greater simplicity future efforts will focus on evaluation of other cotton models using the data sets from bushland texas this will allow a more direct comparison of cotton2k performance to other models with reduced complexity no matter which et method was used cotton2k generally underestimated et as compared to data from the bushland weighing lysimeters fig 2 borrowing the terminology of lamsal et al 2018 cotton2k likely suffered from a lack of expressibility which means there were possibly no parameterization options that would prevent the model from underestimating et particularly for the cimis approaches adjustments of kcbmax eq 1 higher than the value recommended by fao 56 could possibly increase the transpiration component of et but only for the ascek hr et method alternatively transpiration might be increased by expanding the simulated root profile which is also two dimensional in cotton2k because soil water evaporation is supplied only by the water content of the top soil layer in cotton2k underestimation of evaporation is likely related to a water limitation in that layer increasing the thickness of the top layer in cotton2k is a potential solution to increase the water supply available for evaporation lascano and van bavel 1986 future work should further investigate and possibly improve the cotton2k soil water balance simulation particularly focusing on how the simulated soil and root profile supplies water for et and on the simulated relationships between plant growth and et also due to the importance of et partitioning in models of soil plant atmosphere systems kool et al 2014 data sets that better describe the soil water evaporation and plant transpiration components of et are needed for more thorough model evaluation and improvement 5 conclusions with the development of a novel methodology for making unbiased cotton2k parameterization decisions the study demonstrated how three et simulation methodologies could be intercompared while reducing modeler bias and subjectivity the methodology was useful for determining statistically whether one et method performed better than another while the present study provided a specific example related to comparison of et methods the approach is likely also useful for comparisons of other agroecosystem model components the study demonstrated differences p 0 05 in cotton2k simulations of et and other agroecosystem metrics among three et simulation methods however none of the tested et methods could perform as well as or better than both of the other methods when considering all metrics collectively considering et alone the ascek hr method led to minimal but significant improvements in simulated et and crop coefficient curves were often more realistic while the three et methods had many qualitative and programmatic differences that led to localized differences in simulated et time series the overall contribution of these differences toward comprehensive improvement of the simulation results was negligible to improve et simulations with cotton2k future research should investigate the simulation methodologies used for soil water flux near the soil surface and for linking water use with crop growth acknowledgments the authors acknowledge cotton incorporated for contributing partial funding for this research riley anderson and brooke conrardy are acknowledged for preliminary efforts with the cotton2k model which eventually led to the present research objectives this research used resources provided by the scinet project of the usda agricultural research service ars project number 0500 00093 001 00 d appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 007 
26178,efforts to improve agroecosystem models require methods for unbiased comparisons among simulation algorithms with focus on evapotranspiration et in cotton2k the objectives were to develop a novel methodology for evaluating model parameterization options and to compare model performance using three et algorithms the cotton2k model was updated to include a standardized et method and two penman approaches were also tested sobol global sensitivity analysis and multiobjective optimization were used to identify influential parameters and select feasible parameterization options the three et methods led to differences in simulation accuracy for et soil water contents and several plant growth metrics p 0 05 however no et method could consistently outperform the other two methods and et simulation errors were up to 60 the simulation methodology permitted unbiased comparison of three et methods in cotton2k and highlighted areas for model improvement including the surface evaporation simulation and the linkage between simulated et and crop growth keywords cotton cotton2k crop model global sensitivity analysis multiobjective optimization water use 1 introduction evapotranspiration et is commonly the greatest pathway of water loss from crop production systems as a result the accuracy of water balance simulations in agroecosystem models is highly dependent on how well the model simulates et moreover the calculations of other model components including soil nutrient and crop growth and development algorithms depend on the accuracy of et and water balance simulations these concerns have driven recent efforts to evaluate and improve et calculations in many agroecosystem models including the decision support system for agrotechnology transfer dssat cropping system model csm attia et al 2016 dejonge et al 2012b marek et al 2017 sau et al 2004 thorp et al 2014b the root zone water quality model rzwqm anapalli et al 2016 and the soil and water assessment tool swat marek et al 2016 several of these model evaluation efforts have incorporated high quality daily et data sets such as those provided by the large weighing lysimeters at bushland texas anapalli et al 2016 marek et al 2016 2017 among various methods for et measurement weighing lysimeters that are properly designed installed and managed can provide accurate et data for agroecosystem model evaluation farahani et al 2007 a variety of methods exist for et simulation in agroecosystem models each having unique limiting assumptions algorithm complexity and input data requirements naturally comparisons of different et approaches both within and among models have aimed to identify the better performers farahani and bausch 1995 found better et estimates from the shuttleworth and wallace 1985 method as compared to a penman monteith approach ma et al 1999 demonstrated improved et simulations with energy combination methods as compared to pan evaporation approaches particularly when transpiration occurred lascano and van bavel 2007 compared explicit and recursive combination methods for computing penman based et finding the former to calculate as much as 25 less et than the latter on hot summer days in lubbock texas anothai et al 2013 compared priestley taylor and penman monteith et approaches in the dssat csm finding the latter method to better agree with measured et data from bowen ratio instrumentation kang et al 2009 performed a comprehensive comparison of et simulations from three wheat triticum aestivum l models cropwat modwht and dssat csm ceres wheat using measured et from the bushland weighing lysimeters and from gravimetric water content measurements at a site in china they noted overall poor simulation performance among the models and suggested considerable revisions were necessary to improve et calculations they also discussed the effects of interacting model components on et simulation performance highlighting for example the contribution of leaf area index lai simulation error to et error and vice versa further demonstrating the divergent nature of et methods in agroecosystem models kimball et al 2019 reported large variability in et simulation results from an intercomparison of 29 maize zea mays l models parameterized for iowa conditions the barriers to unbiased comparison of et methods in agroecosystem models are substantial most agroecosystem models are manually calibrated meaning input parameters are adjusted until performance is deemed acceptable through simple statistical calculations jacovides and kontoyiannis 1995 and human assessment of measured versus simulated data plots computational approaches can eliminate the potential for modeler bias to influence the calibration effort for example soldevilla martinez et al 2014 developed a simulated annealing global optimization method to calibrate and compare the dssat csm and the water and agrochemicals in soil crop and vadose environment wave model using measured drainage and et data from a weighing lysimeter and soil water measurements from capacitance sensors while the approach highlighted differences in the drainage simulations among the two models a primary disadvantage of simulated annealing and similar iterative optimization approaches is that only one solution is recommended by the algorithm despite substantial computational expense one alternative seeks to first develop a comprehensive database of model input and output relationships and any assessment of simulation output in comparison with measurements occurs subsequently irmak et al 2000 welch et al 2001 in addition to the benefit of fully representing model responses to input parameterization the database approach can also improve computational efficiency because simulations can be easily parallelized on high performance computers lamsal et al 2018 as recognized by kang et al 2009 another barrier to objective evaluation of et methods in agroecosystem models involves the shared feedback between the et algorithm and other model components for example parameter adjustments that improve et simulations might also worsen crop yield simulations and vice versa dejonge et al 2012a better et algorithms should demonstrate better performance in multiple aspects of the model simulation not just in the et simulation itself this means that the model parameterization effort requires the optimization of multiple often conflicting objectives as embodied in the comparison of measurements and simulation output for multiple types of agronomic data e g lai plant dry matter crop height yield et soil water content etc many approaches for multiobjective optimization have been developed for solving engineering design problems chiandussi et al 2012 zio and bazzo 2012 however application of these methods to agroecosystem model evaluation and comparison is relatively uncommon as one example charoenhirunyingyos et al 2011 combined measured and simulated data for lai et and soil water content into a single objective function and used a genetic algorithm to optimize the soil water atmosphere plant swap model similarly thorp et al 2015 used an objective function that combined measured and simulated data for lai et crop height and seed cotton yield to parameterize csm cropgro cotton using simulated annealing optimization neither of these studies incorporated the database approach of welch et al 2001 and therefore suffered the disadvantage of computationally intensive iterative optimization techniques resulting in only one solution furthermore welch et al 2001 described an additional drawback of efforts to combine multiple variables into a single objective function it permits the optimizer to differentially gain accuracy in one variable by sacrificing accuracy in another an improved strategy for multiobjective optimization problems involves the computation and assessment of solutions among the pareto optimal set cheikh et al 2010 mishra and harit 2010 taboada et al 2007 welch et al 2001 which is the subset of possible solutions that are not dominated by any other solution mathematical definition to follow unfortunately a single solution rarely optimizes all objective functions in multiobjective problems however the set of plausible solutions can be objectively narrowed by computing the pareto optimal solutions a final barrier to unbiased evaluation of et methods in agroecosystem models involves the statistical approaches used to make comparisons evaluations of models rarely entail more than calculations of simple statistical metrics such as root mean squared error rmse or mean bias error mbe jacovides and kontoyiannis 1995 demonstrated how rmse and mbe can be combined to calculate the t statistic which when compared to a critical t value from standard statistical tables can assess the statistical significance of the model s calculations at a given confidence level in their comparison of four et simulation methods the best performer was sometimes improperly selected if statistical hypothesis tests were not incorporated in the analysis based on hierarchical linear regression modeling thorp et al 2014b showed that csm cropgro cotton simulations of yield and et could explain variability in the measured data independent of the growing season their methodology provided statistical support that the model was responding appropriately to the agronomic treatments imposed in a given year by using assessments of statistical inference to compare et methods in agroecosystem models the conclusions regarding the relative performance of each method can be strengthened the overall goal of this study was to develop a novel methodology for unbiased evaluation and comparison of three et algorithms in the cotton2k agroecosystem model http departments agri huji ac il fieldcrops cotton field data for the analysis included et measurements from weighing lysimeters at bushland texas and other agronomic measurements from surrounding field experiments that compared fully irrigated deficit irrigated and dryland cotton gossypium hirsutum l production specific objectives were to 1 incorporate global sensitivity analysis multiobjective optimization and high performance computing to fully evaluate cotton2k parameterization options 2 compare the effects of three et algorithms on the accuracy of simulated et cotton fiber and seed yield soil water content and plant growth metrics using statistical inference and 3 evaluate et simulation behavior in cotton2k using crop coefficient methods 2 materials and methods 2 1 simulation workflow a novel simulation approach was developed for unbiased analysis and comparison of simulation results for three et algorithms in cotton2k fig 1 aspects of the workflow included 1 a sobol sampling scheme to choose large numbers of input parameterization options from a high dimensional parameter space 2 high performance computing to efficiently conduct large numbers of cotton2k simulations 3 a database approach to link input parameter sets with error statistics from comparisons of measured and simulated data 4 a global sensitivity analysis gsa to identify influential cotton2k input parameters 5 a multiobjective optimization moo method to calculate pareto optimal solution sets by evaluating model error statistics among multiple agroecosystem metrics 6 a pruning algorithm to cull pareto optimal solutions based on a user specified order for objective function priority and 7 classical inferential statistics to assess differences in model performance among the pruned pareto optimal sets for each et algorithm further details on the field measurements the cotton2k model and the workflow implementation are described in the following sections 2 2 field measurements cotton field experiments to quantify et of fully irrigated deficit irrigated and dryland cotton production were conducted in four weighing lysimetry fields at the usda ars conservation and production research laboratory cprl near bushland texas 35 187 n 102 097 w 1170 m above mean sea level during the 2000 and 2001 growing seasons howell et al 2004 also the bushland evapotranspiration and agricultural remote sensing experiment bearex08 quantified et for fully irrigated and dryland cotton production at the same site during 2008 evett et al 2012 the soil texture at the site was predominantly clay loam and silty clay loam as determined from textural analysis of soil samples tolk et al 1998 growing season precipitation short crop reference et from april through september amounted to 155 1707 160 1579 and 230 1624 mm in 2000 2001 and 2008 respectively strong regional advection from the south and southwest typically led to high reference et values at the site and low precipitation levels led to water limitation and need for irrigation in all three seasons irrigation was applied using a 10 span lateral move overhead sprinkler irrigation system lindsay manufacturing omaha nebraska equipped with mid elevation spray application mesa nozzles at a height of approximately 1 5 m above the ground surface the machine was oriented from north to south traveled in an east or west direction and irrigated two lysimeter fields simultaneously four large weighing lysimeters were installed at the bushland field site in the 1980 s marek et al 1988 and have been used to monitor et for a variety of crops for nearly three decades evett et al 2012 2016 howell et al 1995 2004 during the 2000 and 2001 cotton studies the southeast and northeast lysimeter fields were managed using full and limited irrigation respectively full irrigation was defined as weekly irrigation to replenish root zone soil water content to field capacity and limited irrigation was half of the full rate in 2008 season both the northeast and southeast lysimeter fields were fully irrigated the northwest and southwest lysimeter fields were not irrigated dryland production in 2001 or 2002 and less than 130 mm was applied in the 2008 early season to encourage germination and emergence soil water content was periodically measured at two access tube locations in each lysimeter using a calibrated neutron scattering probe model 503dr hydroprobe cpn international inc martinez california which provided data from 0 1 to 1 9 m in 0 2 m incremental depths specific protocols for weighing lysimeter measurements during the three cotton growing seasons were given by howell et al 2004 and evett et al 2012 howell et al 1995 discussed the calibration technique for mass measurement within the lysimeter which can provide et estimates at time scales less than 1 h more recently marek et al 2014 presented techniques for quality assurance and quality control of data collected from the lysimeters based on this post processing protocol lysimeter et data for the present study was aggregated on a daily basis from 1 january through 31 december in 2000 2001 and 2008 cotton planting dates ranged from mid may to early june in the three growing seasons after establishment cotton plants were destructively sampled on a two week basis from small areas 1 0 2 0 m2 more than 10 m away from the lysimeter the samples were processed in the laboratory to estimate leaf area index lai leaf dry matter ldm stem dry matter sdm boll dry matter bdm canopy height cht mainstem node count nod green boll count gbl and mature boll count mbl cotton harvest dates ranged from late october to early december in the three growing seasons yield measurements were obtained by sampling mature bolls from five 10 0 m2 areas in each lysimeter field turnout percentages were measured using a small research gin which provided data for fiber yield fby cottonseed yield sdy and seed cotton yield scy 2 3 cotton2k the cotton2k agroecosystem model is described online at the website provided previously and was recently reviewed by thorp et al 2014a model development descended from several early and notable efforts in cotton growth modeling including gossym baker et al 1972 1983 simcoti baker et al 1972 simcotii jones et al 1974 and calgos marani et al 1992a b c primarily cotton2k made these models more relevant for cotton production in arid irrigated environments such as the western u s and israel whereas gossym calculates water balance processes on a daily time step cotton2k uses hourly weather information to calculate hourly water and energy balances which is thought to improve the accuracy of the model s et calculations cotton2k simulates a two dimensional soil profile with a depth of 2 m and width equal to the cotton row spacing lambert et al 1976 bar yosef et al 1982 the soil profile is divided into a fixed number of small compartments both horizontally and vertically water and nitrogen contents and root growth are calculated in each compartment additionally the compartments are grouped vertically into a set of nine soil horizons each with a thickness of 15 cm van genuchten 1980 parameters are used to define the soil water holding and hydraulic conductivity characteristics in each soil horizon these were determined by fitting the model to soil water measurements and other data as discussed later due to lacking experimental data initial conditions for nitrate and ammonium were assumed to be 5 0 kg ha 1 for all soil horizons and all simulations initial organic matter was specified uniquely for each experimental year based on limited pre season measurements of surface organic matter at the field site and estimates of vertical organic matter distribution from a usda soil survey of the study area initial soil water content was estimated as the average of neutron scattering probe readings during the subsequent growing season and were specified uniquely for each lysimeter field management information required by the model including irrigation and fertilization schedules and dates of planting and harvest was specified as recorded for each lysimeter field highly detailed and comprehensive simulations of cotton development and growth and are possible with cotton2k the model simulates development of mainstem nodes in response to air temperature using heat unit concepts and the development of vegetative or fruiting branches from each mainstem node is also simulated development of leaves and reproductive structures including squares green bolls and mature bolls is simulated at individual sites along the vegetative and fruiting branches this permits simulation of cotton plant maps where the position of each reproductive structure on the simulated plant is explicitly considered growth rates of unique plant organs including roots stems leaf blades petioles squares green bolls and mature bolls are based on carbon supply and demand relationships effects of water stress on simulated plant growth are calculated as a function of leaf water potential and nitrogen stress effects depend on supply and demand relationships for nitrogen in vegetative material fruits and roots jones et al 1974 stress alters simulated plant growth via reductions in organ growth potential and shedding of squares and bolls i e abscissions a set of 51 variety parameters are used to simulate the effects of genetics on cotton growth and development responses a number of source code edits were required to facilitate efficient model input output i o for high performance computing and to correct any encountered coding errors for example the model code was altered to directly output soil water contents corresponding to the positions of neutron scattering probe measurements also some parameter sets caused underflow or overflow errors at run time which required coding edits to restrict ranges for certain state variables a fortran version of cotton2k was obtained from the developers of the palmscot landscape scale cotton modeling tool booker et al 2014 2015 although a newer c version of cotton2k exists the code for cotton growth simulations is interwoven with code for its microsoft windows based graphical user interface gui which precluded its use on a linux high performance computing system after incorporating coding edits the model was compiled using gfortran within the open source gnu s not unix gnu compiler collection www gnu org 2 4 evapotranspiration methods the et methodology in cotton2k is based on the california irrigation management information system cimis algorithm using the modified penman equation snyder and pruitt 1985 two options are possible for obtaining the required hourly weather information either input hourly data directly as measured cimis hr or use the methodology of ephrath et al 1996 to estimate hourly data from daily measurements cimis dy both of these options were tested in this study table 1 required hourly meteorological information including solar irradiance mj m 2 air temperature c dew point temperature c and wind speed km d 1 was obtained from a texas high plains et network weather station which was positioned over a well watered clipped grass surface adjacent to the field site hourly precipitation data mm were obtained from a tipping bucket rain gauge managed by the experimentalists at the field site daily weather inputs included maximum and minimum daily air temperature along with the daily aggregations of the other weather measurements since the initial development of the cotton2k model efforts in the et community have aimed to standardize et computations allen et al 1998 walter et al 2005 therefore a third et option ascek hr table 1 was added to cotton2k based on the findings of dejonge and thorp 2017 who demonstrated the use of more modern and standardized et methodologies to identify problematic et responses in another agroecosystem model computations of short crop reference et etos or tall crop reference et etrs from meteorological data form the basis of standardized et etsz calculations walter et al 2005 the standard algorithm for hourly etos was added to cotton2k which required the same hourly weather data as discussed previously similar to dejonge and thorp 2017 a dual crop coefficient methodology allen et al 1998 was used in the ascek hr et method to calculate basal crop coefficients kcb from simulated lai 1 k cb k cbmin k cbmax k cbmin 1 exp s kc lai maximum kcb kcbmax was specified as 1 25 based on the fao 56 tabulated value for cotton with appropriate adjustment for the bushland environment the kcbmin and skc factors were fixed at 0 0 and 0 6 respectively as discussed in dejonge and thorp 2017 evaporation coefficients ke were calculated using the methods described in fao 56 allen et al 1998 daily etos was summed from hourly etos and used to calculate daily potential transpiration epo kcbetos and potential soil water evaporation eso keetos although a standardized etos algorithm for daily weather data also exists walter et al 2005 it was deemed nonsensical for the current study because the cotton2k water balance fundamentally operates with an hourly timestep even with the cimis dy et method table 1 daily weather data was immediately converted to hourly data upon input to the model ephrath et al 1996 2 5 simulations cotton2k was setup to run 12 simulation scenarios based on the three cotton growing seasons and four uniquely managed lysimeter fields table 2 simulations were initiated on 1 january of each year and concluded on the recorded harvest date for each lysimeter field within the southwest lysimeter field in 2001 twin rows spaced 25 cm apart were planted on 76 cm centers because cotton2k did not consider this planting configuration a row spacing of 38 cm i e half of 76 cm was simulated simulations were conducted using usda s new high performance computing resource called ceres which consists of 64 compute nodes each having 40 logical cores on intel xeon processors with hyper threading and a shared 2 pb storage system with lustre design the operating system on ceres was a linux centos distribution ver 6 7 located in ames iowa access to ceres occurred via the dedicated high speed networking resource called scinet about 60 cotton2k simulations s 1 were possible on ceres as compared to no more than 3 simulations s 1 on a modern desktop machine thus high performance computing increased simulation capability by a factor of 20 a python script www python org that incorporated python s multiprocessing package was developed to manage the simulation tasks on ceres the python script loaded a list of parameter sets into a processing queue created 40 working directories for conducting simulations in parallel and copied pertinent cotton2k files to each directory it then established 40 independent worker processes one for each requested processing core each worker process iteratively selected an item from the processing queue adjusted cotton2k input files to incorporate the current parameter set conducted the 12 simulation scenarios and extracted simulated data from cotton2k output files for pairing with associated measurements measured and simulated data for 22 agroecosystem metrics table 3 were aggregated among the 12 simulation scenarios table 2 for each tested parameter set by calculating the percent root mean squared error rmse uniquely for each metric 2 rmse i f i m i s i 100 m i 1 n i j 1 n i m i j s i j 2 where m i s i and n i are the measured and simulated data vectors and vector length respectively among the 12 scenarios for the ith metric table 3 the python script and simulation job concluded by outputting a model response database that included the cotton2k input parameter sets with associated rmse statistics for each of the 22 agroecosystem metrics 2 6 first sobol sampling the simulation workflow entailed two main phases one leading to a sobol global sensitivity analysis and another leading to a multiobjective optimization approach for model calibration fig 1 both phases began by using a sobol sampling procedure to choose high dimensional parameter sets for input to cotton2k a python script that incorporated the sensitivity analysis library salib was developed to conduct the sobol sampling and later to compute the sobol sensitivity indices saltelli 2002 saltelli et al 2010 sobol 2001 sobol sampling techniques were previously shown to be advantageous and more efficient to develop databases that describe high dimensional model input and output relationships lamsal et al 2018 because the sobol algorithm can select parameter sets that are more evenly dispersed across the multidimensional parameter space initially 72 model input parameters were sampled for inclusion in the sobol gsa table 4 1 18 parameters that defined the sand and clay contents for each of the nine soil horizons 2 two parameters that define the soil matric potential at field capacity smpfc and at which free drainage occurs smpfd 3 the 51 cotton variety parameters varpar01 through varpar51 and 4 the skc parameter for et simulations with the ascek hr method eq 1 the sand and clay contents chosen by the sobol algorithm were input to the rosetta pedotransfer function zhang and schaap 2017 to calculate the associated van genuchten 1980 parameters for each soil horizon appropriate parameter ranges table 4 for sand and clay were based on texture measurements of soil samples collected at the field site tolk et al 1998 based on recommendations in the model documentation smpfc ranged from 0 38 to 0 28 bars and smpfd ranged from 0 15 to 0 05 bars 1 bar 100 kpa appropriate ranges for variety parameters were chosen by using spreadsheet software excel 2013 microsoft corporation redmond washington to test outcomes of the model equations that incorporated each parameter table 4 the appropriate equations were taken from the model code and programmed into the spreadsheet to facilitate tests of equation outcomes with different input parameter values which permitted specification of reasonable parameter ranges the skc parameter was varied from 0 5 to 0 9 based on the recommendation of dejonge and thorp 2017 many of the cotton2k variety parameters are unitless variables used in empirical equations that drive specific plant growth or development characteristics the n parameter of the sobol sampling algorithm was set to 34 723 with specification to prepare for calculation of second order sensitivity effects thus the number of n dimensional parameter sets n 72 chosen by sobol for the sobol gsa was n 2 n 2 5 069 558 as defined within the sobol algorithm table 5 the value of n was selected based on preliminary estimates of the rate of simulations on ceres with plans to contain the simulation timeframe to within a couple weeks most importantly as revealed by later tests the number of simulations was more than satisfactory to ensure stability of sobol sensitivity indices with 12 simulations per parameter set the sobol gsa needed a total of 60 834 696 simulations which required 60 217 cpu hr on ceres and approximately 301 h of wall clock time to minimize computational expense simulations for the gsa were conducted only for the asce hr et method table 1 2 7 global sensitivity analysis to gain insights on cotton2k responses to adjustment of model input parameters a sobol gsa cariboni et al 2007 pianosi et al 2016 saltelli et al 2000 was conducted using algorithms from the salib package in python saltelli 2002 saltelli et al 2010 sobol 2001 an astute reader will note that less computationally intensive sensitivity analyses are normally conducted first followed by more intensive methods like sobol gsa here only sobol gsa was used because 1 the availability of high performance computing resources ensured efficiency of simulations and 2 subsequent portions of the workflow also incorporated the sobol sampling aspect of the sobol gsa fig 1 using algorithms from the salib package first order second order and total sensitivity indices were calculated for each combination of the 72 input parameters and 22 agroecosystem metrics the rmse statistic for each agroecosystem metric was used as the objective function for sensitivity index calculations based on the recommendation of zhang et al 2015 any parameter having a first order sensitivity index greater than 0 05 for any of the 22 agroecosystem metrics was considered an influential parameter and remained flexible in the subsequent analysis fig 1 table 4 other parameters were considered non influential and were fixed to default values for the remainder of the analysis the overall purpose of the sobol gsa was to eliminate non influential parameters prior to using multiple objective optimization for model calibration 2 8 second sobol sampling the gsa results suggested that only 35 of the 72 cotton2k parameters influenced the simulation outcomes table 4 subsequently the sobol sampling algorithm in salib was again used to choose cotton2k parameter sets but only among the influential parameters as identified by the sobol gsa a second gsa based on the reduced parameter set was not conducted instead simulation results based on the second sobol sampling was used for multiobjective optimization to identify the parameter sets that provided optimal agreement between measured and simulated results it was assumed that the best solutions to the multiobjective optimization problem were among the parameter sets chosen by the second iteration of sobol sampling fig 1 each sobol parameter set was used for simulations of the 12 cotton2k scenarios table 2 using three different et methods in the model table 1 resulting in three model response databases that linked parameter sets to rmse outcomes for each agroecosystem metric table 3 using the same sobol n parameter for these runs n 34 723 the number of n dimensional parameter sets n 35 chosen by sobol was n 2 n 2 2 500 056 with 12 simulations per parameter set for three et approaches this analysis needed 90 002 016 simulations which required 89 889 cpu hr on ceres and approximately 449 h of wall clock time table 5 2 9 multiobjective optimization because there were 22 agroecosystem metrics to consider table 3 optimizing the cotton2k parameterization required multiobjective optimization moo techniques taboada et al 2007 the objective function to be optimized incorporated k unique rmse calculations one for each agroecosystem metric k 22 expressed as 3 f moo m s f 1 m 1 s 1 f 2 m 2 s 2 f k m k s k where the terms are as described for equation 2 equation 3 represents the set of rmse calculations eq 2 for each of k agroecosystem metrics based on the results of simulations for a given parameter set among more than 2 5 million sets tested table 5 the first step toward reduction of plausible parameter sets was to calculate the subset of pareto optimal solutions cheikh et al 2010 which were the solutions that were not dominated or non dominated by any other solution in mathematical terms a solution x 1 dominates another solution x 2 if the following two conditions are met f i x 1 f i x 2 for all i 1 2 k f j x 1 f j x 2 for at least one j 1 2 k in words a solution dominates another if the rmse calculations for k agroecosystem metrics are all less than or equal to that for the other and at least one rmse calculation is less than that for the other the goal was to find the parameter sets with rmse calculations that were not dominated by the rmse calculations for any other parameter set a python script was developed to calculate the pareto optimal solution set for each cotton2k et method efficiencies in computation were gained by sorting the data set such that non dominated solutions were more likely evaluated first mishra and harit 2010 ceasing evaluation of a solution immediately after determining it was dominated and using python s multiprocessing package to divide computational load among processors on ceres a known problem with pareto optimal sets is that they often remain large and cumbersome and they do not adequately ease the burden of selecting one or several practical solutions taboada et al 2007 tested two approaches to aid selection of practical solutions by pruning the pareto optimal set one based on a clustering technique and another based on a user defined priority ranking among the set of objective functions eq 3 borrowing from the latter strategy a python script was created to reevaluate each pareto optimal solution and combine the k objective function outcomes to a single evaluation criterion by assigning random weightings approximately in order of objective function priority specifically k random weightings w i 1 k w i 1 0 were computed by using python s numpy package to 1 generate k random numbers from a uniform distribution r 0 r i 1 i 1 2 k 2 sort r from highest to lowest and 3 calculate w i r i j 1 k r j i 1 2 k given the objectives of this study and that crop yield and water use are commonly the most important of agroecosystem metrics the priority of objective functions were specified in the following order et fby lai scy cht sdy nod ldm sdm gbl mbl and bdm followed by the ten soil water content measurements from top to bottom in the profile table 3 weightings were iteratively assigned to objective function in order of priority but to relax strictness of the subjectively chosen order weightings were assigned randomly from the highest three remaining weightings this allowed the algorithm to vary the objective function priority at each iteration while generally preserving the priority overall the evaluation criterion c was then computed for each solution in the pareto optimal set 4 c i 1 k w i f i m i s i and the solution having the minimum c was identified as a pruned pareto optimal solution to ensure equal treatment among objective functions the values f i m i s i for each objective function i e see equation 2 were normalized from 0 0 to 1 0 prior to computing c the process of random weighting computation and c computation was iterated until 100 000 iterations passed without identification of a new pruned solution the set of pruned pareto optimal solutions determined from this process was used for all further analysis the pareto optimal sets for each of the three cotton2k et methods were pruned using identical random weightings multiple trials of the pruning algorithm produced very similar results which highlighted the consistency of the approach because the algorithm also counted the number of times a particular pruned solution was selected a single most popular solution could be identified however according to the definition of multiobjective optimization all solutions in the pareto optimal set have equal feasibility as a solution to the optimization problem 2 10 et method comparison the performance of the three cotton2k et methods table 1 was compared by conducting an analysis of variance anova on rmse results for each of the 22 agroecosystem metrics table 3 among the pruned pareto optimal solutions tukey s multiple comparisons tests were also conducted to identify which cotton2k et method resulted in statistically different rmse values for each agroecosystem metric p 0 05 and the lowest of these identified the better et method for a given metric statistical analysis was conducted using the r project for statistical computing software www r project org further efforts involving measured and simulated crop coefficient plots were used to evaluate and compare et time series for the most popular pareto optimal solution for each et method dejonge and thorp 2017 demonstrated how crop coefficient methods can be used to assess the et outputs of agroecosystem models relative to either measured or theoretically expected crop coefficient time series allen et al 1998 the approach required daily calculation of the evaporation coefficient ke e etos the basal crop coefficient adjusted for water stress kcbks t etos and the single crop coefficient kc et etos where e t and et are the daily cotton2k simulated outputs for soil water evaporation plant transpiration and evapotranspiration respectively furthermore measured kc values were computed from the weighing lysimeter data 3 results 3 1 global sensitivity analysis the gsa identified 35 influential parameters of the 72 included in the analysis table 4 these parameters contributed most substantially to sensitivity in the model outputs as expected sand and clay parameters at each soil depth had the greatest influence on the simulated soil water contents near that depth the smpfc parameter soil matric potential at field capacity was most influential on simulated soil water content in the deeper soil layers from 120 to 200 cm while the smpfd soil matric potential at which free drainage occurred parameter was influential on soil water content throughout the entire soil profile six variety parameters were influential for four or more of the agroecosystem metrics varpar01 varpar04 varpar31 varpar32 varpar47 and varpar49 four of the 22 agroecosystem metrics including lai ldm cht and nod were sensitive to the varpar01 parameter which adjusts plant growth simulations in response to plant density the varpar04 parameter which controls growth potential of leaves at prefruiting nodes mainly influenced et and soil water contents at 10 70 and 90 cm likely through its effects on lai the varpar31 parameter had greatest effects on et cht and soil water contents at all the soil depths the parameter defines the physiological days required for developing the fourth through the ninth prefruiting nodes and thus has great effect on late stage vegetative growth prior to fruiting similarly varpar32 defines the physiological days for development of the first and second prefruiting nodes and was influential on et and soil water contents at 3 of the 10 depths varpar47 through varpar50 are used to calculate the probability of fruit abscission as a function of fruit age and these were the most influential parameters on gbl fby sdy and scy other influential parameters had more specific effects on cotton2k outputs varpar12 which controls stem growth potential primarily influenced sdm varpar21 and varpar22 both influenced cht through their effects on calculations of mainstem growth as a function of prefruit node age after second square likewise varpar26 has a multiplicative effect on daily vertical stem growth and was therefore influential on cht simulations booker et al 2014 also identified varpar26 as a highly influential parameter varpar30 acts as a multiplier on the temperature function used to calculate squaring date and it was influential on gbl as expected both lai and ldm were sensitive to varpar34 which sets the initial leaf area of prefruiting leaves varpar35 which was influential on cht affects the calculation of physiological days between successive fruiting branches results of the gsa were often intuitive and helped to identify influential model parameters for subsequent model parameterization efforts table 4 fig 1 3 2 multiobjective optimization recalling that the second sobol sampling resulted in 2 500 056 cotton2k parameter sets that were evaluated using multiobjective optimization table 5 calculation of the pareto optimal solutions for the cimis dy cimis hr and ascek hr et methods reduced the number of plausible solutions to 461 448 369 347 and 419 784 respectively thus the pareto optimal solution set eliminated up to 85 of the total evaluated solutions although the remaining number was still very large pruning the pareto optimal solutions using the objective function weighting approach further reduced the number of solutions to 19 12 and 22 for the cimis dy cimis hr and ascek hr et methods respectively this means the multiobjective optimization technique could reduce the solution set to better than 0 0009 of the total evaluated solutions the result highlighted the difficulty of practical decision making from the pareto optimal set while pruning the set based on limited user input i e the order of objective function importance substantially reduced the number of solutions and improved the practicality of determining a feasible model parameterization 3 3 statistical analysis simulation results among the three et methods in cotton2k table 1 were different p 0 05 for 15 of the 22 agroecosystem metrics et lai scy cht sdy nod ldm gbl mbl and soil water content at 10 30 50 130 150 and 190 cm table 6 with the ascek hr method et was simulated significantly better than with the other two et methods although the mean rmse improvement was less than 1 because the rmse calculation for et involved more than 3700 data values table 3 rmse variation among the pruned pareto optimal sets was low which contributed to the significant difference among et methods the ascek hr method also performed significantly better than the other two et methods for cht gbl and soil water content at 130 cm however the cimis dy method performed significantly better than both the other two methods for sdy the cimis hr method did not perform significantly better than both the other methods for any of the agroecosystem metrics collectively cimis dy and cimis hr both performed better than ascek hr for scy ldm and soil water contents at 10 30 and 50 cm thus the statistical results demonstrated no clear winner among the three et methods generally each et method statistically outperformed the other methods for one or more metric but not without significantly underperforming on a different metric an exception was in the comparison of cimis dy and cimis hr where the results showed no advantage to using directly measured hourly weather data cimis hr as compared to estimating hourly weather from daily measurements based on ephrath et al 1996 cimis dy that is cimis dy performed as well as or better than cimis hr for all 22 agroecosystem metrics an unexpected result but nonetheless demonstrated statistically importantly based on comparisons to daily measurements from weighing lysimeters at bushland the ascek hr standardized et method statistically outperformed the cimis dy and cimis hr methods for simulating et in cotton2k 3 4 model evaluation the pruning algorithm iterated 465 213 times to identify 22 or less pruned pareto optimal solutions for each et simulation method among the results at each iteration the most popular solutions for the cimis dy cimis hr and ascek hr methods were selected 63 86 and 61 of the time the most popular solutions for cimis hr and asce hr were identical table 4 perhaps because these two et approaches used the same weather input data for cimis dy a different solution was most popular the solution identified as the most popular for each method i e most commonly selected by the pruning algorithm was used to parameterize cotton2k for all further evaluations of the model of the 2 500 056 evaluated parameter sets only one parameter set was commonly present in the pruned pareto optimal solutions for all three et methods but it was not a most popular solution for any method to more fully evaluate the et simulations in cotton2k the simulation timeframe was divided into sections dominated by soil water evaporation 1 january through 31 may and transpiration 1 june through 30 september because cotton2k simulations terminated on the specified harvest date table 2 availability of information for november and december was inconsistent so it was not included here cotton2k consistently underestimated cumulative et during the soil water evaporation dominated portion with rmse 44 for all three et methods fig 2 a because soil water evaporation is simulated based only on the water content of the top soil layer the underestimation may be due to inadequate simulation of upward water flux from deeper soil layers for example if the simulated soil water evaporation demand is larger than the upward water flux into the top soil layer soil water evaporation will decline even though the next deepest soil layer may have plenty of water to meet the demand cumulative et during the transpiration dominated period was also consistently underestimated by the model fig 2b for all et methods the rmse for the ascek hr method 17 2 was lowest among the et methods with better performance apparent at the extreme levels of et corresponding to fully irrigated treatments overall after thorough evaluation of cotton2k parameterization options with three et simulation strategies cotton2k consistently underestimated et as compared to et measurements from the bushland weighing lysimeters fig 2 the cotton2k simulations of lai were mostly overestimated for lai 2 5 m2 m 2 and underestimated for lai 2 5 m2 m 2 fig 2c only the fully irrigated treatments in 2001 and 2008 achieved a maximum lai of 3 0 m2 m 2 or more thus the model was generally unable to fully respond to plant growth conditions under full irrigation while also often drastically overestimating lai for water limited conditions simulation results for scy were moderately accurate fig 2d although the scy results for the ascek hr et method were worse than for the other two methods overall cotton2k simulations of lai and scy were poorer than that obtained with a different agroecosystem model at a nearby research station in west texas modala et al 2015 and in central arizona thorp et al 2017 note that the rmse statistics for lai and scy are different in fig 2 as compared to table 6 because the latter provides mean rmse among solutions in the pruned pareto optimal set while the former reports rmse for the most popular pruned pareto optimal solution 3 5 evapotranspiration behavior as suggested by dejonge and thorp 2017 analysis of the crop coefficients i e kc kcbks and ke as computed from simulated et variables and etos can reveal insights about model behavior and functionality as well as the reasonableness of simulated transpiration and soil water evaporation time series the approach is also useful for further comparisons among et simulation methods crop coefficient plots for simulations of the fully irrigated 2008 nelys field generally showed similar behavior between the three et methods fig 3 for example all three methods demonstrated similar pre season doy 100 160 and post season doy 275 300 spikes in kc indicating increased soil water evaporation and ke due to precipitation events however the pre season peaks in simulated kc were not as high as measured kc from the lysimeter indicating underestimated soil water evaporation no matter which et algorithm was used mid season kc from doy 190 to 240 was better simulated with the ascek hr method than with the cimis methods as compared to kc from lysimetry gross underestimation of kc at the end of the season from doy 260 to 275 contributed to the general trend of underestimated et during the transpiration dominated portion of the season fig 2b and indicates need for improvements to the late season transpiration calculations other aspects of the crop coefficient curves highlighted localized differences among the simulated et time series although the rmse between measured and simulated et were not different for cimis hr and cimis dy table 6 visual differences in mid season kc doy 220 240 indicated different simulated et time series for the two cimis methods fig 3 during this time the field received 106 mm of precipitation over a five day period doy 226 230 despite simulation of full canopy during these days lai 2 6 not shown the cimis dy method simulated higher evaporation with ke ranging from 0 37 to 0 50 this resulted in high kc values reaching 1 74 for cimis dy on doy 230 fig 3a the ke results for cimis hr and ascek hr were smaller during this time 0 29 0 34 figs 3b and 0 19 0 31 fig 3c respectively this reduced kc to 1 22 or less for cimis hr and ascek hr which is closer to a more typical mid season value the result was likely due to providing precipitation data in hourly versus daily amounts specifying rain events hourly allowed infiltration processes to be simulated over several timesteps while specifying daily rainfall amounts filled the top soil layer in a single timestep and increased opportunity for soil water evaporation thereafter because the ascek hr method generally had high lai fig 2c and thus kcb eq 1 the ke peaks generally decreased as the season progressed from doy 180 to 280 fig 3c this was a sensible response because increases in plant growth should further limit soil water evaporation and similar behavior was recently found with another agroecosystem model dejonge and thorp 2017 however neither ke time series for cimis dy fig 3a nor cimis hr fig 3b demonstrated this declining trend over the season the results indicated that the soil water evaporation routine for the cimis methods in cotton2k should be further investigated and potentially improved another interesting finding was the drastic changes in transpiration i e kcbks as simulated for the 2000 swlys field with dryland cotton production particularly for the cimis et methods fig 4 crop coefficients from the cimis et methods exhibited tremendous and seemingly unrealistic transpiration variability whereas the ascek hr method resulted in a more steady increase in transpiration for example over a sixteen day period from doy 178 193 kcbks for cimis hr varied from 0 15 to 1 11 and back to 0 28 fig 4b because lai was simulated as a steadily increasing curve over this time period not shown it seems unlikely that the crop s transpiration potential shifted from near zero to full potential and back to near zero in just a few days the cimis dy method also exhibited a similar large swing in kcbks values at this time fig 4a however the ascek hr method demonstrated a more typical kcbks curve that generally followed the pattern of biomass growth because the method intrinsically linked simulated lai with transpiration via equation 1 fig 4c because simulated kcbks often appeared incongruent with simulated lai time series not shown the interaction between vegetation production and transpiration needs further investigation to achieve more reasonable crop coefficient time series particularly under water limited conditions with the cimis methods in cotton2k 4 discussion verifying the accuracy of simulation results from agroecosystem models is a fundamental research question that has generated numerous studies on techniques for model evaluation sensitivity analysis and model intercomparison herein a novel approach was developed to compare and highlight deficiencies in cotton2k simulations based on three et algorithms the main advantage of the approach was its inherent objectivity there was very little effect of subjectivity in the model comparisons when subjectivity was required for example in specifying the parameter ranges table 4 the threshold for sensitivity indices or the priority order of agroecosystem metrics table 3 the decision could be applied uniformly among all et methods thus the comparisons were more scientifically robust and repeatable as compared to model intercomparison techniques that involve more subjective model parameterization decisions through the use of high performance computing the approach also permitted evaluation of many more parameter sets than could be practical using manual model parameterization techniques thus the comparisons could be conducted with a more thorough assessment of model responses to variability in parameterization the results also reinforced an important lesson about agroecosystem model evaluation when multiple types of measured data are available it can be difficult to find solutions that improve simulations of all metrics simultaneously given any two model parameterizations considered in this study there was a 15 18 chance that one parameterization was no better than the other when considering all 22 agroecosystem metrics the implications for manual calibration are potentially freeing for perfectionist modelers who are frustrated when they can no longer improve simulations of one metric without worsening simulations of another that is the calibration effort may have approached a solution within the pareto optimal set and further efforts to reduce simulation error among all metrics may be futile furthermore the number of solutions among the pareto optimal set is likely too large to be assessed adequately using manual calibration methods but more advanced computational techniques could be used for further assessment the gsa highlighted opportunities to condense and simplify the crop variety parameters in cotton2k only 15 of 51 variety parameters were identified by the gsa to substantially influence the simulation results many of the parameters work together in empirical equations to control a particular aspect of the simulation for example three parameters are involved in leaf growth on the mainstem nodes four parameters affect post squaring stem growth and four other parameters control the probability of boll abscission table 4 efforts to merge the effects of such parameters could possibly increase their sensitivity while also reducing the complexity of the model parameterization however realistic simulation detail should not be lost in the effort to reduce parameters conducting the gsa for each of the 12 scenarios separately may also reveal differences in model sensitivity that are caused by environment using three et algorithms cotton2k simulated et lai and scy with rmse between measured and simulated data ranging from 59 6 to 60 9 60 7 64 6 and 40 7 44 6 respectively table 6 however as reported by modala et al 2015 and thorp et al 2017 the rmse results for these important agroecosystem metrics were lower for a different cotton simulation model based on data from different sites in the western u s this result is surprising because cotton2k simulates cotton growth and development and soil water balance processes in more detail than other cotton simulation models for example cotton2k simulates a two dimensional soil profile and considers development and growth of individual cotton bolls whereas other models simulate these processes with greater simplicity future efforts will focus on evaluation of other cotton models using the data sets from bushland texas this will allow a more direct comparison of cotton2k performance to other models with reduced complexity no matter which et method was used cotton2k generally underestimated et as compared to data from the bushland weighing lysimeters fig 2 borrowing the terminology of lamsal et al 2018 cotton2k likely suffered from a lack of expressibility which means there were possibly no parameterization options that would prevent the model from underestimating et particularly for the cimis approaches adjustments of kcbmax eq 1 higher than the value recommended by fao 56 could possibly increase the transpiration component of et but only for the ascek hr et method alternatively transpiration might be increased by expanding the simulated root profile which is also two dimensional in cotton2k because soil water evaporation is supplied only by the water content of the top soil layer in cotton2k underestimation of evaporation is likely related to a water limitation in that layer increasing the thickness of the top layer in cotton2k is a potential solution to increase the water supply available for evaporation lascano and van bavel 1986 future work should further investigate and possibly improve the cotton2k soil water balance simulation particularly focusing on how the simulated soil and root profile supplies water for et and on the simulated relationships between plant growth and et also due to the importance of et partitioning in models of soil plant atmosphere systems kool et al 2014 data sets that better describe the soil water evaporation and plant transpiration components of et are needed for more thorough model evaluation and improvement 5 conclusions with the development of a novel methodology for making unbiased cotton2k parameterization decisions the study demonstrated how three et simulation methodologies could be intercompared while reducing modeler bias and subjectivity the methodology was useful for determining statistically whether one et method performed better than another while the present study provided a specific example related to comparison of et methods the approach is likely also useful for comparisons of other agroecosystem model components the study demonstrated differences p 0 05 in cotton2k simulations of et and other agroecosystem metrics among three et simulation methods however none of the tested et methods could perform as well as or better than both of the other methods when considering all metrics collectively considering et alone the ascek hr method led to minimal but significant improvements in simulated et and crop coefficient curves were often more realistic while the three et methods had many qualitative and programmatic differences that led to localized differences in simulated et time series the overall contribution of these differences toward comprehensive improvement of the simulation results was negligible to improve et simulations with cotton2k future research should investigate the simulation methodologies used for soil water flux near the soil surface and for linking water use with crop growth acknowledgments the authors acknowledge cotton incorporated for contributing partial funding for this research riley anderson and brooke conrardy are acknowledged for preliminary efforts with the cotton2k model which eventually led to the present research objectives this research used resources provided by the scinet project of the usda agricultural research service ars project number 0500 00093 001 00 d appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 06 007 
26179,today environmental scientists heavily rely on geospatial web services gws however many online facilities are under utilized by the environmental modelling community because accessing the disparate service interfaces requires highly specialized technical expertise this paper proposes a simple universal interface for services suis framework which is a client framework for accessing heterogeneous services via a single unified interface to simplify service access the supported services including open geospatial consortium ogc simple object access protocol soap and representational state transfer rest services suis relieves modellers from having to learn the details of service technologies such as protocols bindings and schemas suis4j a java implementation of the suis framework is developed and tested to combine multiple operational gws to demonstrate geoprocessing workflows in agricultural drought monitoring and coastal ocean modelling the results confirm the expected benefits suis is demonstrated to support simplified use of geospatial cyberinfrastructure for ad hoc environmental model integration keywords geospatial web service interoperability earth scientific model simplification geoprocessing workflow software availability name of software suis4j developer center for spatial information science and systems george mason university source language java contact information zsun gmu edu availability the source code and application jar can be accessed via github https github com csiss suis4j 1 introduction following the realization that the traditional personal computer oriented analysis workflows are hindering the use of large volume of geospatial data due to limited disk space and computing capacity wagemann et al 2018 geospatial web services gws appeared and brought great benefits to earth scientists by providing web access to massive geospatial datasets and functionalities in an elastic manner hey and trefethen 2005 richard et al 2014 vitolo et al 2015 wright and wang 2011 driven by the idea of e science hey and trefethen 2005 tens of thousands of web services were developed and deployed to continuously serve millions of spatial records and datasets on a daily basis more and more free and open source software foss for web applications and geographic information systems became available and used by data vendors to deploy their own thematic web services allowing vendors to directly connect with stakeholders swain et al 2015 these web services are the key tools that enable modellers to conduct data intensive science hey and trefethen 2005 today researchers in the whole spectrum of earth science domains including geography geology geophysics oceanography glaciology atmospheric sciences and so on frequently rely on gws to search download visualize analyze and disseminate data powerful tools could definitely improve the conduct of environmental research hey et al 2009 however more powerful tools are usually more complicated because simplicity is sacrificed in exchange for flexibility and generality unfortunately many scientists are prevented from using the full power of gws because the service client capacity is limited while gws service interfaces are too complex scientists wishing to utilize these powerful services are forced to understand intricate technical details and processes fig 1 that are not intuitive or easily comprehensible to users who lack computer and geospatial interoperability backgrounds each type of service takes a different approach to technical details such as operation names parameter names data types formats schemas value options special tokens protocol headers and exception codes this breadth of options supports flexibility at expense of service adoption the growth of web based resources in recent years has made this problem worse imagine a hypothetical seismic scientist who wants to combine an iris incorporated research institutions for seismology rest service shapiro et al 2005 that offers time series of waveform data with a ucar university corporation for atmospheric research web coverage service wcs offering radar data and a web processing service wps ogc 2007 to perform re gridding after their datasets are processed they want to use a soap service clements 2002 for data integration typically they will install desktop client software such as arcgis institute 2001 or qgis team 2013 for ogc services and find a jupyter notebook kluyver et al 2016 or write custom code for the soap and restful services this researcher might spend days studying obscure web apis application programming interfaces and navigating unnecessary software functionality once they master accessing these varied gws interfaces they still won t be able to programmatically chain the services together automatically to deliver the data in its final form instead they will spend more time pre processing the data manually or writing custom scripts although gws that offer required pre processing capabilities exist and even though the researcher is already using some gws to download the data and do the final data integration they will avoid utilizing existing web gws for pre processing because to them those interfaces appear obscure and cumbersome to access instead of taking advantage of these powerful facilities our hypothetical researcher will avoid learning confusing technical details and will continue to rely on inefficient and time consuming but familiar procedures this is a persistent problem and naturally many scientific communities have voiced their desire for simplified access to these powerful online facilities to reduce time spent performing manual data pre processing kelbert 2014 this paper proposes to solve the problem by applying a simple universal interface for services suis framework to gws clients suis client framework bridges the disparate service interfaces with a single generic interface that carefully abstracts service technical details such as protocols styles bindings schemas and addresses only the intuitive information for each service like operation names parameter names and data types are exposed to end users that information is generally intuitive and easier for scientists to comprehend because it is directly related to the scientific requirements of specific research fields operations in suis are mapped to the actions in the original service interfaces for each type of services suis provides a driver that accomplishes the simplified mapping automatically this means that once users learn how to use suis they can access all standard conforming online geospatial cyberinfrastructure the major benefits of suis are reduced barrier of entry and reduced risk of misunderstanding between endpoint consumers and service providers with suis endpoint users of gws are separated from heterogeneous interfaces and access with all services via the same set of uniform processes suis aims to lighten the burdens of learning about unnecessary software and to ease the pitfalls of coding clients to interact with complicated geospatial web service interfaces additionally by alleviating problems of interface complexity and heterogeneity suis supports easier composition of gws into workflows generic suis operations can be chained into geoprocessing models di 2004 yue et al 2010 that map to executable workflows composed of the multiple services chen et al 2009 yu et al 2012 a java library named suis4j was implemented to demonstrate and evaluate the suis concept suis4j currently supports the basics of soap wsdl web service description language restful wadl web application description language wps version 1 0 0 wcs version 2 0 0 wms version 1 3 0 and wfs version 2 0 0 suis4j was tested and its performance was evaluated with various existing web services the experiment shows that the suis approach shields users from overwhelming and unnecessary technical details and allows users to take advantage of gws in their applications in a simple way the remainder of the paper is organized as follows section 2 introduces the background of this research and lists the existing related work section 3 describes the objectives and design principles underlying suis while section 4 presents suis framework in full detail in section 5 the implementation of suis4j is presented section 6 describes how suis4j was tested with prominent online infrastructures in the earth science community section 7 discusses the pros and cons of suis framework section 8 concludes this research and maps out future work 2 related work this section talks about the circumstances and explains why simplification is an inevitable trend in geospatial cyberinfrastructure 2 1 gws interfaces the interfaces of gws can be generally divided into three groups soap simple object access protocol interfaces rest representational state transfer style interfaces and ogc open geospatial consortium standard compliant interfaces shown in fig 2 their regimes could be overlapped because a service may belong to more than one group for example an ogc wps could simultaneously provide both a soap endpoint and rest endpoint w3c identifies a set of common core technologies for web services the main ones are http hypertext transfer protocol xml extensible markup language soap simple object access protocol wsdl etc w3c 2015 soap messages are exchanged through xml payloads that are transmitted via http post wsdl is used to describe soap web services interfaces chinnici et al 2007 christensen et al 2001 historically the soap standard has maintained the monopoly position in service oriented architecture soa but as new standards have emerged soap has become one of several options in the market soap is well regarded due to its domain independence and security soap and wsdl services are employed in many b2b business to business and b2c business to consumer industries such as chemistry kim et al 2015 travel planning hotel booking dhara et al 2015 and decision support demirkan and delen 2013 rest is the newcomer and provides a lighter weight alternative to soap it is an architectural style of web services and is not a standard rest is used widely to develop world wide web applications in rest data and functionality are considered resources and are accessed using uniform resource identifiers uris rest requires that actions on the resources are limited to a small set of simple well defined operations usage of rest in web apis has skyrocketed in the past decade because restful services are lightweight highly scalable and maintainable wadl is a schema format developed to describe restful applications hadley 2006 both soap and rest are domain independent however in many scenarios target specific interfaces are required to facilitate geospatial applications dietz 2010 organizations like iso tc211 the international organization for standardization technical committee 211 and ogc open geospatial consortium have developed a series of standards targeted to the specific requirements from the geospatial community iso focuses on standardization of geographic information and geo informatics ostensen and smits 2002 while ogc majorly works on standardizing service interfaces and data models di 2003 some of their well well known products are iso19115 2003 metadata iso19119 2005 geographic information services percivall 2002 wms wms 2004 wcs wps wfs sos sensor observation service sps sensor planning service csw catalog service for the web and openls open location service botts et al 2008 ogc 2016 to further advance interoperability iso tc211 and ogc hold a cooperative agreement that allows them to cite each other s standards iso tc211 2009 these standards have greatly improved the interoperability among geospatial web services however the proliferation of standards has greatly increased the heterogeneity of service interfaces the more complicated the standards are the greater barriers of entry they present to scientists this complexity is one important reason for the low adoption rate of gws by endpoint users 2 2 geospatial cyberinfrastructure and spatial data infrastructure cyberinfrastructure ideas have been gradually embraced by earth science community and many teams have developed online digital systems to meet cyberinfrastructure needs that were previously unmet for years council 2007 richard et al 2014 sun et al 2014 that has spurred new research which utilizes web based instruments sensors high powered computers data storage capabilities visualization facilities and networks for communication and collaboration berman and brady 2005 hofer 2013 spatial data infrastructure sdi is one type of cyberinfrastructure in the early days of gis sdi was designed primarily for sharing geographic information in response to the high cost of information collection and maintenance later sdi efforts have evolved towards the creation of shared distributed and interoperable environments through gws davis and alves 2005 in sdi data providers register their services with a public server that scientists can use to search select data services of their interest and reach those services through the web table 1 sdi enables users to retrieve the latest version of the data products and simplifies the requirement for endpoint devices that can remain lightweight without the need for large local storage space despite advances in sdi many geospatial scientists who recognize the need for cyberinfrastructure continue to hold a wait and see attitude rising from the concern that the systems will not be helpful without broader input from the communities they are meant to serve cyberinfrastructure community needs to engage the geoscience population to reach a consensus on what kind of cyberinfrastructures are the most suitable for the community mookerjee et al 2015 2 3 client framework for gws most web service frameworks offer client frameworks for users to embed the code calling web services into their application e g apache axis soap rest apache cxf soap rest gsoap soap net framework rest yii rest jersey rest spring rest etc besides that service consumer groups develop some independent frameworks for gws like arcgis qgis gvsig and saga gis developed their own embedded client framework which is usually hidden and specifically invoked by a plugin dialog ogc has invested a lot of efforts in unifying ogc web service interfaces e g ows common whiteside and greenwood 2010 owslib kralidis 2015 and geoapi custer 2011 ows common defines unified getcapabilities operation and other minimum utilities and is the basis of most ogc service interface standards geoapi and owslib are java python client interfaces aiming to formalize the handling of the types defined in the ogc specifications however they are fully engaged with detailed technical terminologies in ogc standards and will cost a lot of time of the environmental scientists who don t want to invest too much time on web service in industry commercial service providers normally develop new client framework to interact with their own services such as the python java javascript go client libraries for google maps web services interactive sdk for bing maps web control mapkit js client for apple maps javascript api client for arcgis rest services simple api client for openweathermap javascript api of mapquest web mobile sdks for here wego maps etc all these client frameworks are independent very different and require long term engagement and interest which is not realistic in environmental modelling scientists need to focus on environment models rather than various service client sdks and a more universal and simple client framework will be of their interest 2 4 geoprocessing workflow earth science modelling geoprocessing denotes processing geographical data and is the core part of geographic information system allen 2011 chen et al 2009 frisbie 1979 goodchild 1982 kinzy 1978 mark 1979 roberts et al 2010 sun et al 2012 a geoprocessing workflow is a chain of several atomic functions to achieve more complex tasks di et al 2006 the available atomic processes differ among platforms in arcgis the processes are the tools in arcgis toolbox in cyberinfrastructure the processes are web services registered in centralized catalogs geoprocessing workflows are one of the major users of gws and they greatly extend the capability scope that cyberinfrastructure can cover but workflow approaches to modelling still struggle to advocate themselves within the community scientists have difficulties to leverage workflows in real science they are supposed to ease the burden on scientists for processing data but eventually they leave users with another big burden of dealing with workflow the workflows become even more complicated once they involve ontologies provenance inference based automation etc the hard to use impression of gws contributes to the unpopularity of geoprocessing workflows in recent studies integrated environmental modelling iem has been identified as a structural way to develop and organize environmental models gao et al 2019 jakeman and letcher 2003 kelly et al 2013 laniak et al 2013 geoprocessing workflow approach is listed as an option for constructing integrated models composed of heterogeneous atomic processes yue et al 2015 the approach is promising but remains too complicated for earth scientists without web service background 2 5 existing efforts for simplicity cyberinfrastructure community has recognized the complexity problem and has attempted to shield end users from some of the complexity initially they studied the causes of complexity shen et al 2007 concluded five types of heterogeneous issues among web services including semantic parameter data type parameter structure parameter number and parameter data unit many attempts have been made to extract common things among web services and create a generic interface to simplify the calling procedure on the client side for instance schindler et al present a generic and flexible framework for building geoscientific metadata portals independent of content standards for metadata and protocols schindler and diepenbroek 2008 kiehle et al built a generic service utilizing spatial standards of ogc iso and w3c world wide web consortium for providing common geoinformation capabilities in sdi kiehle et al 2006 de souza munoz et al propose a generic approach called openmodeller to handle different data formats and multiple algorithms that can be used in potential distribution modeling and make it easy in data preparation and comparison between algorithms using separate software applications de souza muñoz et al 2011 trabant et al used a simple subset of restful concepts common calling conventions a common tabular text dataset convention human readable documentation and tools to help scientists learn how to use the web services from iris data management center trabant et al 2015 burkon et al tried to develop and demonstrate the practical use of a generic model of service s interface that could be used as a basis for creation of a formal description of any service in any industry burkoň mackiewicz discussed the benefits of applying the generic interface definition gid of iec 61970 to power system operations and industrial applications mackiewicz 2006 tristan et al introduced a generic service wrapper enabling the optimization of legacy codes assembled in application workflows on grid infrastructure glatard et al 2006 most research work focuses on the server side and attempts to unify the service interfaces however the current landscape is not favorable for unifying service interfaces across domains and industries because service providers have different business goals and different prior knowledge because it s not reasonable to expect the existing service providers will simplify their interfaces this work should focus on the client and create a simple client framework that can handle the disparate service interfaces and provide a universal calling interface for end users 3 objective and design principles our general objective is to hide scientifically irrelevant technical details of geospatial web services and to expose only application related information to end users a generic client framework is created to act as a system building block that bridges scientific end users and disparate geospatial web services fig 3 the general objective is supported by three specific principles a keeping the processing interface simple b making gws more composable in the environmental workflow and c seeking common ground to become a universal solution 3 1 keeping it simple the geospatial process interfaces that take inputs and produce outputs are simple in traditional gis but have become complicated after being translated into gws paradigm the protocols for information communication have evolved concurrently with the long term organic development of the internet the complex historical development of web service technologies has embedded specialized technological knowledge into the gws interfaces as case in point every ordinary world wide web consortium www service request response exchange involves multiple layers of historical technical minutia that are irrelevant to web user s needs the world wide web is successful because web browsers engineers have artfully leveraged protocols without drowning web users in technical details protocol details are hidden far away from the surface that end users see meanwhile consumers of geospatial web service face complicated and frustrating client software that directly exposes gws protocols to end users service consumers must deal with a full stack of engineering information from data exchange to operation semantics for instance for a non gws expert environmental scientist the xml formated messages with many redundant and deeply nested labels are very likely to produce confusion and frustration our objective requires that technical details about communication protocols are simplified and hidden away from the application logic additionally current gws interface descriptions have too many layers in wsdl a service has bindings a binding has port types a port type has operations an operation has input elements an input element has messages a message has schemes and a scheme allows numerous compositions it is normal to initially become lost while figuring the relationships among these terminologies in most cases those concepts are only meaningful to expert users it is unreasonable to have every user encounter them although the layered architecture enhances engineering flexibility when building loosely coupled services it correspondingly raises interface complexity barriers for consumer comprehension to provide a geospatial service process interface that is as simple as gis suis removes those description layers that are not relevant to general users 3 2 making gws composable in environmental workflow at present the adoption of gws in the workflow is much less than was expected when gws were introduced lopez pellicer et al 2012 most scientists treat geospatial web services as simple tools for data access which is just one aspect of the design goals of cyberinfrastructure gws permit a major interoperability breakthrough of computer and network technologies that can directly support and transform the conduct of scientific and engineering research and yield revolutionary payoffs by empowering individual researchers and by increasing the scale scope and flexibility of collective research david 2004 gws are supposed to be chained into workflows for automation to practically help with most basic steps of the real scientific research and in the analysis of datasets of large scale areas and extended temporal periods there are already many successful experiments in using gws into environmental model workflows in lab however after these years of developments those vision goals of gws are never really been achieved in real world environmental model workflows to make gws more composable in the workflow simplification of the interfaces of gws to make them useable in workflows is a prerequisite step 3 3 seeking for common ground to become universal unifying all gws interfaces into a single universal interface is challenging because attempts to do that are obstructed by the extreme interface heterogeneity the reasons for that are highly varied and involve factors like service purpose operation granularity nested tree structures data formats message schemas context scenarios design concepts technical restrictions and subjective provider preferences there is enough idiosyncrasy to make it barely possible to precisely map to all gws interfaces to a single model of api interface our experiences in transforming ogc web services into soap services have confirmed that the famous precept to seek for common ground while reserving the differences bol 1987 in this case states the basic rule for designing a universal solution a universal client interface for all other gws interfaces should center on the common ground and relegate differences to the background we need to identify and classify identical or similar interface concepts and then organize them into a complete interface which is neat consistent and easily intelligible for scientists the outlying and disparate interface concepts that cannot be unified should be handled via hidden adapters or drivers 4 suis framework this section introduces the core model architectural design and usage of suis the core model has two major components profile and driver fig 4 4 1 profile suis profile is a model representing the smallest functional unit of gws it is composed of three public interface classes operation message and parameter and enumeration class datatype fig 5 1 operation denotes the action that the service provides it includes operation name input message output message and narrative description 2 message is the payload exchanged between the client and server it contains a content variable which could be any object such as json javascript object notation xml and kvp key value pairs 3 parameter represents the variables that are inputs and outputs in operations parameter attributes are identity string a data type a parameter name a parameter value a description and minimum and maximum occurrence limits to support suis profile implementation in multiple programming languages we define parameter value as an abstract object and give suis library developers the responsibility to determine the specific data type each parameter object must have an attribute referring to suis datatype enumeration 4 datatype has five named constants fig 5 which represent basic data types common to the general database gis database gws and general programming languages the mapping between the conventional data types and suis data types is listed in table 2 we combine similar data types to simplify the service profile three new types bool number and date are added to support logic description capabilities the datatype enumeration class describes the general types of data content communicated over the internet using structured data exchange formats such as xml json kvp base64 josefsson 2006 ascii binary etc although using different encodings and protocols these parameters belong to the same type file because non expert users have no interests to know how the files are encoded or transferred in communication the encoding and decoding downloading and uploading details should be erased from the surface and processed automatically on the backstage 4 2 driver the technical details of each gws type are isolated and processed in a low level container called suis driver the driver wraps the specific service interfaces with the suis profile and translates suis requests and responses to message formats compliant with service interfaces users only interact with the suis profile and are not required to understand technical details and complexity encapsulated in suis drivers the suis driver backgrounds technical details and acts as a gray box which non experts can treat as a black box while experts and power users can use it to leverage the backend service interface the driver mechanism makes it easy to transform the existing services into suis style without sacrificing their unique capabilities each suis driver wraps one type of service interface suis architecture allows new drivers to be created for other types of service interfaces that do not belong to the three groups discussed in section 2 all suis drivers must implement a mandatory set of methods for decoding the suis requests and encoding suis responses as illustrated in fig 6 a set of methods that translate suis requests to service requests yellow boxes and a set of methods for translating service replies to suis responses blue boxes 4 3 mapping the task of mapping the disparate gws interfaces to a suis profile demands some subtle and challenging design decisions it requires extracting incompatible operation semantics identifying their essential information roles and grouping them most effectively using the categories provided by suis profile specific services allow multiple possible mappings requiring careful consideration of overall semantics for example the common getcapabilities operation of ogc services can be mapped to a suis operation or it can be merged into the initial method digest phase which retrieves service descriptions and initializes the driver when multiple valid design choices are possible we evaluate each option against the general objective and goals of suis section 3 fig 7 shows the mapping we created between the three service categories soap rest ogc and the suis profile the mapping is not simple or direct because the ties lack fixed patterns such as one to one one to many or many to many for example a resource and one of its supported methods in rest interface are combined into a suis operation while the getcapabilities request is mapped to suis operations listing the provided assets taken together these complex mapping choices produce a simple and universal api model that represents capabilities of all gws interface types the specific level of simplifying on the suis interface depends on the acknowledged common requirements from environmental scientists 4 4 payload the data payloads transferred between the suis client and the gws interfaces are automatically generated by suis drivers in accordance to the gws interface schemas since the payloads encapsulate superfluous technical details the suis architecture makes them invisible to scientific end users suis users construct suis requests that are composed of parameter key value pairs that represent the core service request information suis drivers automatically decode and wrap suis requests into request payloads in the same fashion the drivers decode the response payloads and transform them into suis key value pairs to end users the transformation from simple suis data model to complex payload structure is invisible suis drivers provide two transmission methods send and receive for delivering and receiving service payloads if gws requires file inputs the suis drivers are required to support at least one of the three ways to transfer files into or out of gws url simplest http post multipart attachment file size limited or a third party file uploading service e g ftp to turn local files into urls 4 5 usage suis is designed to permit flexible usage that adapts to multiple context scenarios scientists are free to choose from a variety of existing gws facilities such as mobile or real time gws according to their application requirements customized suis drivers allow the inclusion of new message structures and formats the suis data types allow users to input or receive either gis datasets or literal values in program code input specification process activation and output retrieval tasks from diverse gws are presented by suis in a uniform fashion both synchronous and asynchronous modes of operation in the distributed processing environment are supported fig 8 the synchronous mode can be used for instantly responsive services while asynchronous mode allows interaction with extended duration gws processes the suis framework api can be expressed in all general purpose programming languages such as java python and c c thus allowing scientists to use suis with their preferred languages the main steps of using suis to invoke gws table 3 are 1 initialize suis drivers to parse the capabilities of the service such as the operations parameters data types capabilities information is used to configure the driver 2 examine the supported operations optional choose the required operation 3 examine input and output parameters of the chosen operation optional 4 construct the request message by setting values of input parameters 5 send the request and receive the response 6 examine the returned messages optional these steps could be altered to support complex application logic and to support program flow events such as exceptions to use services that are missing service description file or to perform asynchronous requests scientists can skip the service examination steps if they are familiar with the operations the async mode in suis is built because many web services don t support asynchronous requests e g most rest services for those services with async settings e g wps 2 0 ogc 2017 suis driver developers are recommended to directly reuse their native async settings 5 implementation the suis framework should be implemented by suis developers of different programming languages e g java suis4j python suispy etc each library will be maintained by the community of stakeholders who use the corresponding programming language the client providers like arcgis and qgis can contribute to the development and adopt the suis libraries in their software to avoid maintaining their own code to call gws compatibility issues should be fixed by suis developers driven by the science user communities suis has been implemented as a java library named suis4j it utilizes several open source java libraries to achieve suis functionality table 4 suis4j is available on github https github com csiss suis4j for downloading and sharing suis4j development and maintenance follow standard java ecosystem practices github issue tracking system is used for fixing bugs and planning enhancements apache maven miller et al 2010 is used to manage dependencies and to build releases maven allows developers to easily include suis4j as a dependency into their projects the code structure is split into two major packages the suis profile and drivers as described in the core framework model a client class provides the object oriented interface for end users to access suis capabilities the library has no dependencies to any complex gis system and works with all standards conformant gws 6 experiments to validate suis framework against its objectives we applied the suis4j library to two geospatial science use cases agricultural drought modelling deng et al 2013 sun et al 2017b and fvcom finite volume coastal ocean model data processing chen et al 2006 both of which involve a number of heterogeneous gws including gadmfs global agricultural drought monitoring and forecasting system wcs deng et al 2013 nws national weather service rest geoserver geobrain soap services di 2004 and wps all the service calls in both workflows are made in synchronous mode to ensure the service outputs are ready as the inputs of other services 6 1 agricultural drought suppose we are agricultural drought scientists and have created a new index to monitor agricultural drought the equation for the index is 1 droughtindex v c i m p 2 where vci vegetation condition index represents the relative status of vegetation comparing to the historical records in the same period mp monthly precipitation is derived from quantitative precipitation estimate qpe from nws the drought index supposes that vegetation status and precipitation are linearly correlated with drought remote sensing scientists are continuously searching for indices to accurately reflect observed conditions and this index represents a novel attempt in a realistic agricultural drought research scenario multiple datasets must be combined to calculate the drought index and to do our study we must retrieve vci products from gadmfs 4 4 http gis csiss gmu edu gadmfs and then download mp products from the nws ahps advanced hydrologic prediction service website once data is obtained we use geobrain web services han et al 2008 li et al 2010 to process the two products into the final drought index product we employ suis4j to automate these tasks into a geoprocessing workflow the workflow is shown in fig 9 where irregular shapes represent gws purple rectangles represent operations dashed lines represent data flow and solid lines represent suis calling web services we utilize geospatial web services to re project clip and calculate the final drought product based on our index equation we use suis4j to call the required services in the required order and then link their inputs and outputs to form a chain we apply the same workflow chain to different days in 2017 to generate a time series of drought products shown in fig 10 our results show that the long narrow central part of california the area between roads i 5 and ca 99 endures agricultural drought for almost the entire year and seasonally from may to july drought spreads to cover most places in california in august the drought starts to gradually dissipate to present our results we select the april 23 drought index product and render that as a drought map by overlaying drought index on google maps the finished experiment warrants discussion of technical results especially those related to performance issues the drought workflow uses web services from two categories data services and processing services both types of services introduce network load processing services involve computational load on the server and wait time for the client application architecture can be used to address some performance challenges for example suis application might cache the outputted data from gws to reduce both computational efforts and network load across multiple application runs the particular caching strategy depends on suis driver developers the recommended practice is to remember the paths of the files downloaded by users from gws next time when users input the same parameters suis will check the file paths and directly return files to users if they exist the lifetime of the cached files is equal to the time the downloaded files exist in their cached paths for time sensitive requests if the input parameters to gws are different from the input parameters which produced the cache files suis will resend the requests for new files if the input parameters to gws stay the same suis will provide an option for users to force refresh the cache files by downloading new ones to decrease the long delays caused by slow network connections between client and gws suis supports easy switching between multiple gws for example both gadmfs and noaa star provide vci products and gadmfs serves the data via wcs while noaa star uses ftp based shell scripts scientific users can quickly alter which service suis accesses by changing service endpoint and input parameters effective suis applications can preserve network resources by never downloading remote service data more than once for example in traditional usage the wcs getcoverage request will download data from the remote server to the local client then as the next step this data must be uploaded to another location from where it can be downloaded by the re projecting service suis can make this compound process more efficient by allowing service users to skip the download and upload steps and instead directly pass the wcs getcoverage url to the re projecting service interface keens 2007 ogc 2007 2017 as shown in fig 11 no network load is generated as data streams directly from wcs to the re projecting service without being repeatedly downloaded and uploaded the fake call mechanism can save the large part of the total time cost and has the added benefit of making the workflow more concise furthermore suis can prevent idle blocking while waiting for the result data to be received regardless whether a specific geospatial web service supports asynchronous operation semantics suis provides its own asynchronous communication mode to minimize the time scientists spend idly waiting for processing results to derive precise quantitative measures from the aforementioned performance issues we recorded and evaluated the inputs outputs and the duration of each suis call to calculate a representative workload scenario we made simple assumptions concerning potential users and their behavior we then derived average values such as inter arrival times between incoming requests or the requested amount of data from the scenario when suis and gws exchange messages each exchange causes extra delays that vary depending on the client and server machines computing power we compare the computational effort of subsetting and re gridding coverages via wcs to the extra delay caused by suis wrappers and slow network connections fig 12 gives the average allocation of time cost of the suis steps after 100 times repeated tests on gadmfs wcs the experiments request 23 3 mbytes of vci covering the california area of 647 972 square kilometers fig 12 shows that it costs 9 2 of the total time to receive and parse the wcs capabilities document to initialize suis sending getcoverage requests and downloading the vci image only takes 90 7 of the time which is 1 03 s on average meanwhile suis own operations cost barely any time or computational power overall less than 1 ms the service description retrieving takes some time cost due to the complex structure of the capabilities document which makes automatic parsing slow we can improve it by exporting the corresponding suis driver state to a local file and read it back when scientists want to use that web service next time thus avoiding repeating the work of parsing the capabilities of that services recreating a suis driver from a configuration file is much faster than creating a new one from ogc capabilities document the time cost of sending receiving data will rise as the requested data becomes larger transmitting large binary datasets via web messages requires complex actions on both the server and the client protocols like soap allow multiple transmission options such as mtom w3c message transmission optimization mechanism base64 url reference ftp etc 6 2 coastal ocean modelling to demonstrate that suis is a domain independent tool we also use suis4j in a coastal ocean modelling study based on fvcom an unstructured grid finite volume coastal ocean model chen et al 2012 our study area is the gulf of mexico and parts of the atlantic ocean fvcom requires input temperature and salinity data to be formatted into model specific schemas this data transformation task engages a substantial amount of oceanographers time and they have voiced their need for automation of this work for a number of years we excise suis4j to the preprocess water temperature and salinity data to use with fvcom a java program 5 5 https github com zihengsun suis4j blob master src test java suis4j fvcomtest java generating salinity condition grid to use as input for fvcom was created and uploaded to github to demonstrate another possible use of suis this program uses services provided by the earthcube cyberconnector project sun et al 2017a we access three services to download raw data to interpolate it onto the fvcom grid and then finally to reformat it into a special model ready format suis4j invokes the three processes in sequence to produces a map of seawater salinity fig 13 this experiment shows that suis enables instant automation to produce a time series of maps by making some minimal changes to the input parameters and rerunning the workflow sequence this greatly relieves oceanographers from the repetitive tedious and error prone task of manually downloading and processing each dataset because suis vastly reduces the labor involved in using existing services oceanographers are able to take advantage of earthcube cyberconnector facilities that solve their specific data pre processing problems without suis these powerful facilities will remain under utilized besides the two case studies we have actively engaged with our stakeholders in various communities including ogc esip federation of earth science information partners agu american geophysical union and ams american meteorological society and invited modellers and cyberinfrastructure developers to help test suis4j we received some feedbacks which include many positive comments and also some suggestions for further improvements most of them confirm its necessity and simplicity and supporting more languages such as python and r is the most priority thing for broad adoption 7 discussion this section discusses the advantages and disadvantages of suis from both engineering and scientific user s perspective 7 1 vendor perspective 1 scalability scalability is strongly correlated with compatibility suis has exceptional compatibility with existing gws interfaces it supports all generic gws standards suis framework is open and extensible it is easy to create drivers to access service resources through new interfaces one negative consequence of broad compatibility is that the greater variety of interfaces makes work to adapt all of them more complicated 2 interoperability the interoperability of a systems framework determines its level of flexibility and greatly impacts its future development thomas et al 2007 suis supports two levels of interoperability service and workflow service interoperability is provided by compatibility with the standard interfaces of geospatial web services workflow interoperability is supported through workflow language standard and workflow engine suis workflows can be translated to workflows in other workflow languages and systems like bpel business process execution language oasis 2007 or taverna oinn et al 2004 3 performance the resource overhead of suis own operation steps is small and negligible fig 12 most time cost within suis is spent on communicating with gws which is inevitable the internal logic of suis does not incur significant time cost the performance of suis applications is determined mainly by the network capacity the client and server computational power and the workload 7 2 scientist perspective 5 simplicity suis is a clear lifesaver for users tired of interacting with varied and confusing web service interfaces suis simplifies the calling procedures into a unified process which is easy to master for beginners the disparate unnecessary and complicated technical details are safely buried in the background 6 reliability suis will operate without interruption as long as the corresponding geospatial web service is up and running suis itself won t interrupt the user logic unless it encounters a service related exception and has to terminate the entire workflow suis can run indefinitely without interruptions and suis4j library presents an easy and reliable introduction to all gws 7 short learning curve suis exposes minimal little technical details and avoids obscure technical jargon in its api model and documentation the terminology and concepts involved in understanding and using suis are as simple and understandable as possible no technical knowledge of service details is required because suis separates its intuitive profile from the messy service binding details as shown in table 3 users are able to take advantage of the service without learning about service standards web protocol web service profiles workflows xml etc the gws barrier of entry is substantially lowered by suis 8 conclusion this paper proposes a novel framework called suis to simplify the usage of gws in geospatial cyberinfrastructure which has been under utilized because of difficult and disparate interfaces suis creates a universal profile for the major geospatial web service categories and builds a convenient bridge between the existing gws and scientists in geospatial application domains it severely decreases the complexity of using cyberinfrastructure service resources in and especially benefits scientists without gws backgrounds simultaneously the framework supports high scalability interoperability and lower barriers of entry in the future scientists from various communities will take advantage of suis to develop new scientific use cases the suis workflow translation to standard workflow languages will be implemented as snippets of knowledge suis workflows can interconnect and form more advanced models to perform large and complex tasks such as global climate change simulation or global drought forecasting we will continue to work on include suis in broader collaborative research that includes datasets and functionalities from a greater variety of sources and disciplines security and service documentation enhancement are another two important issues and will be studied in the next stage of work in addition suis drivers should enumerate and rank possible transmission protocols according to their network performances for a given volume of data and then select the most effective option dynamic selection of transmission channels can help suis adapt to different data volume scaling scenarios and choices of data formats these methods can be utilized to reduce the time costs of the sending and receiving steps and avoid exceeding timeout limits or overloading the network infrastructure disclosure no interest conflict is claimed acknowledgment we sincerely thank the anonymous reviewers the authors of the software libraries tools and datasets we have used in this work and esip lab this study was supported by grants from the national science foundation grant number ags 1740693 cns 1739705 icer 1440294 pi prof liping di 
26179,today environmental scientists heavily rely on geospatial web services gws however many online facilities are under utilized by the environmental modelling community because accessing the disparate service interfaces requires highly specialized technical expertise this paper proposes a simple universal interface for services suis framework which is a client framework for accessing heterogeneous services via a single unified interface to simplify service access the supported services including open geospatial consortium ogc simple object access protocol soap and representational state transfer rest services suis relieves modellers from having to learn the details of service technologies such as protocols bindings and schemas suis4j a java implementation of the suis framework is developed and tested to combine multiple operational gws to demonstrate geoprocessing workflows in agricultural drought monitoring and coastal ocean modelling the results confirm the expected benefits suis is demonstrated to support simplified use of geospatial cyberinfrastructure for ad hoc environmental model integration keywords geospatial web service interoperability earth scientific model simplification geoprocessing workflow software availability name of software suis4j developer center for spatial information science and systems george mason university source language java contact information zsun gmu edu availability the source code and application jar can be accessed via github https github com csiss suis4j 1 introduction following the realization that the traditional personal computer oriented analysis workflows are hindering the use of large volume of geospatial data due to limited disk space and computing capacity wagemann et al 2018 geospatial web services gws appeared and brought great benefits to earth scientists by providing web access to massive geospatial datasets and functionalities in an elastic manner hey and trefethen 2005 richard et al 2014 vitolo et al 2015 wright and wang 2011 driven by the idea of e science hey and trefethen 2005 tens of thousands of web services were developed and deployed to continuously serve millions of spatial records and datasets on a daily basis more and more free and open source software foss for web applications and geographic information systems became available and used by data vendors to deploy their own thematic web services allowing vendors to directly connect with stakeholders swain et al 2015 these web services are the key tools that enable modellers to conduct data intensive science hey and trefethen 2005 today researchers in the whole spectrum of earth science domains including geography geology geophysics oceanography glaciology atmospheric sciences and so on frequently rely on gws to search download visualize analyze and disseminate data powerful tools could definitely improve the conduct of environmental research hey et al 2009 however more powerful tools are usually more complicated because simplicity is sacrificed in exchange for flexibility and generality unfortunately many scientists are prevented from using the full power of gws because the service client capacity is limited while gws service interfaces are too complex scientists wishing to utilize these powerful services are forced to understand intricate technical details and processes fig 1 that are not intuitive or easily comprehensible to users who lack computer and geospatial interoperability backgrounds each type of service takes a different approach to technical details such as operation names parameter names data types formats schemas value options special tokens protocol headers and exception codes this breadth of options supports flexibility at expense of service adoption the growth of web based resources in recent years has made this problem worse imagine a hypothetical seismic scientist who wants to combine an iris incorporated research institutions for seismology rest service shapiro et al 2005 that offers time series of waveform data with a ucar university corporation for atmospheric research web coverage service wcs offering radar data and a web processing service wps ogc 2007 to perform re gridding after their datasets are processed they want to use a soap service clements 2002 for data integration typically they will install desktop client software such as arcgis institute 2001 or qgis team 2013 for ogc services and find a jupyter notebook kluyver et al 2016 or write custom code for the soap and restful services this researcher might spend days studying obscure web apis application programming interfaces and navigating unnecessary software functionality once they master accessing these varied gws interfaces they still won t be able to programmatically chain the services together automatically to deliver the data in its final form instead they will spend more time pre processing the data manually or writing custom scripts although gws that offer required pre processing capabilities exist and even though the researcher is already using some gws to download the data and do the final data integration they will avoid utilizing existing web gws for pre processing because to them those interfaces appear obscure and cumbersome to access instead of taking advantage of these powerful facilities our hypothetical researcher will avoid learning confusing technical details and will continue to rely on inefficient and time consuming but familiar procedures this is a persistent problem and naturally many scientific communities have voiced their desire for simplified access to these powerful online facilities to reduce time spent performing manual data pre processing kelbert 2014 this paper proposes to solve the problem by applying a simple universal interface for services suis framework to gws clients suis client framework bridges the disparate service interfaces with a single generic interface that carefully abstracts service technical details such as protocols styles bindings schemas and addresses only the intuitive information for each service like operation names parameter names and data types are exposed to end users that information is generally intuitive and easier for scientists to comprehend because it is directly related to the scientific requirements of specific research fields operations in suis are mapped to the actions in the original service interfaces for each type of services suis provides a driver that accomplishes the simplified mapping automatically this means that once users learn how to use suis they can access all standard conforming online geospatial cyberinfrastructure the major benefits of suis are reduced barrier of entry and reduced risk of misunderstanding between endpoint consumers and service providers with suis endpoint users of gws are separated from heterogeneous interfaces and access with all services via the same set of uniform processes suis aims to lighten the burdens of learning about unnecessary software and to ease the pitfalls of coding clients to interact with complicated geospatial web service interfaces additionally by alleviating problems of interface complexity and heterogeneity suis supports easier composition of gws into workflows generic suis operations can be chained into geoprocessing models di 2004 yue et al 2010 that map to executable workflows composed of the multiple services chen et al 2009 yu et al 2012 a java library named suis4j was implemented to demonstrate and evaluate the suis concept suis4j currently supports the basics of soap wsdl web service description language restful wadl web application description language wps version 1 0 0 wcs version 2 0 0 wms version 1 3 0 and wfs version 2 0 0 suis4j was tested and its performance was evaluated with various existing web services the experiment shows that the suis approach shields users from overwhelming and unnecessary technical details and allows users to take advantage of gws in their applications in a simple way the remainder of the paper is organized as follows section 2 introduces the background of this research and lists the existing related work section 3 describes the objectives and design principles underlying suis while section 4 presents suis framework in full detail in section 5 the implementation of suis4j is presented section 6 describes how suis4j was tested with prominent online infrastructures in the earth science community section 7 discusses the pros and cons of suis framework section 8 concludes this research and maps out future work 2 related work this section talks about the circumstances and explains why simplification is an inevitable trend in geospatial cyberinfrastructure 2 1 gws interfaces the interfaces of gws can be generally divided into three groups soap simple object access protocol interfaces rest representational state transfer style interfaces and ogc open geospatial consortium standard compliant interfaces shown in fig 2 their regimes could be overlapped because a service may belong to more than one group for example an ogc wps could simultaneously provide both a soap endpoint and rest endpoint w3c identifies a set of common core technologies for web services the main ones are http hypertext transfer protocol xml extensible markup language soap simple object access protocol wsdl etc w3c 2015 soap messages are exchanged through xml payloads that are transmitted via http post wsdl is used to describe soap web services interfaces chinnici et al 2007 christensen et al 2001 historically the soap standard has maintained the monopoly position in service oriented architecture soa but as new standards have emerged soap has become one of several options in the market soap is well regarded due to its domain independence and security soap and wsdl services are employed in many b2b business to business and b2c business to consumer industries such as chemistry kim et al 2015 travel planning hotel booking dhara et al 2015 and decision support demirkan and delen 2013 rest is the newcomer and provides a lighter weight alternative to soap it is an architectural style of web services and is not a standard rest is used widely to develop world wide web applications in rest data and functionality are considered resources and are accessed using uniform resource identifiers uris rest requires that actions on the resources are limited to a small set of simple well defined operations usage of rest in web apis has skyrocketed in the past decade because restful services are lightweight highly scalable and maintainable wadl is a schema format developed to describe restful applications hadley 2006 both soap and rest are domain independent however in many scenarios target specific interfaces are required to facilitate geospatial applications dietz 2010 organizations like iso tc211 the international organization for standardization technical committee 211 and ogc open geospatial consortium have developed a series of standards targeted to the specific requirements from the geospatial community iso focuses on standardization of geographic information and geo informatics ostensen and smits 2002 while ogc majorly works on standardizing service interfaces and data models di 2003 some of their well well known products are iso19115 2003 metadata iso19119 2005 geographic information services percivall 2002 wms wms 2004 wcs wps wfs sos sensor observation service sps sensor planning service csw catalog service for the web and openls open location service botts et al 2008 ogc 2016 to further advance interoperability iso tc211 and ogc hold a cooperative agreement that allows them to cite each other s standards iso tc211 2009 these standards have greatly improved the interoperability among geospatial web services however the proliferation of standards has greatly increased the heterogeneity of service interfaces the more complicated the standards are the greater barriers of entry they present to scientists this complexity is one important reason for the low adoption rate of gws by endpoint users 2 2 geospatial cyberinfrastructure and spatial data infrastructure cyberinfrastructure ideas have been gradually embraced by earth science community and many teams have developed online digital systems to meet cyberinfrastructure needs that were previously unmet for years council 2007 richard et al 2014 sun et al 2014 that has spurred new research which utilizes web based instruments sensors high powered computers data storage capabilities visualization facilities and networks for communication and collaboration berman and brady 2005 hofer 2013 spatial data infrastructure sdi is one type of cyberinfrastructure in the early days of gis sdi was designed primarily for sharing geographic information in response to the high cost of information collection and maintenance later sdi efforts have evolved towards the creation of shared distributed and interoperable environments through gws davis and alves 2005 in sdi data providers register their services with a public server that scientists can use to search select data services of their interest and reach those services through the web table 1 sdi enables users to retrieve the latest version of the data products and simplifies the requirement for endpoint devices that can remain lightweight without the need for large local storage space despite advances in sdi many geospatial scientists who recognize the need for cyberinfrastructure continue to hold a wait and see attitude rising from the concern that the systems will not be helpful without broader input from the communities they are meant to serve cyberinfrastructure community needs to engage the geoscience population to reach a consensus on what kind of cyberinfrastructures are the most suitable for the community mookerjee et al 2015 2 3 client framework for gws most web service frameworks offer client frameworks for users to embed the code calling web services into their application e g apache axis soap rest apache cxf soap rest gsoap soap net framework rest yii rest jersey rest spring rest etc besides that service consumer groups develop some independent frameworks for gws like arcgis qgis gvsig and saga gis developed their own embedded client framework which is usually hidden and specifically invoked by a plugin dialog ogc has invested a lot of efforts in unifying ogc web service interfaces e g ows common whiteside and greenwood 2010 owslib kralidis 2015 and geoapi custer 2011 ows common defines unified getcapabilities operation and other minimum utilities and is the basis of most ogc service interface standards geoapi and owslib are java python client interfaces aiming to formalize the handling of the types defined in the ogc specifications however they are fully engaged with detailed technical terminologies in ogc standards and will cost a lot of time of the environmental scientists who don t want to invest too much time on web service in industry commercial service providers normally develop new client framework to interact with their own services such as the python java javascript go client libraries for google maps web services interactive sdk for bing maps web control mapkit js client for apple maps javascript api client for arcgis rest services simple api client for openweathermap javascript api of mapquest web mobile sdks for here wego maps etc all these client frameworks are independent very different and require long term engagement and interest which is not realistic in environmental modelling scientists need to focus on environment models rather than various service client sdks and a more universal and simple client framework will be of their interest 2 4 geoprocessing workflow earth science modelling geoprocessing denotes processing geographical data and is the core part of geographic information system allen 2011 chen et al 2009 frisbie 1979 goodchild 1982 kinzy 1978 mark 1979 roberts et al 2010 sun et al 2012 a geoprocessing workflow is a chain of several atomic functions to achieve more complex tasks di et al 2006 the available atomic processes differ among platforms in arcgis the processes are the tools in arcgis toolbox in cyberinfrastructure the processes are web services registered in centralized catalogs geoprocessing workflows are one of the major users of gws and they greatly extend the capability scope that cyberinfrastructure can cover but workflow approaches to modelling still struggle to advocate themselves within the community scientists have difficulties to leverage workflows in real science they are supposed to ease the burden on scientists for processing data but eventually they leave users with another big burden of dealing with workflow the workflows become even more complicated once they involve ontologies provenance inference based automation etc the hard to use impression of gws contributes to the unpopularity of geoprocessing workflows in recent studies integrated environmental modelling iem has been identified as a structural way to develop and organize environmental models gao et al 2019 jakeman and letcher 2003 kelly et al 2013 laniak et al 2013 geoprocessing workflow approach is listed as an option for constructing integrated models composed of heterogeneous atomic processes yue et al 2015 the approach is promising but remains too complicated for earth scientists without web service background 2 5 existing efforts for simplicity cyberinfrastructure community has recognized the complexity problem and has attempted to shield end users from some of the complexity initially they studied the causes of complexity shen et al 2007 concluded five types of heterogeneous issues among web services including semantic parameter data type parameter structure parameter number and parameter data unit many attempts have been made to extract common things among web services and create a generic interface to simplify the calling procedure on the client side for instance schindler et al present a generic and flexible framework for building geoscientific metadata portals independent of content standards for metadata and protocols schindler and diepenbroek 2008 kiehle et al built a generic service utilizing spatial standards of ogc iso and w3c world wide web consortium for providing common geoinformation capabilities in sdi kiehle et al 2006 de souza munoz et al propose a generic approach called openmodeller to handle different data formats and multiple algorithms that can be used in potential distribution modeling and make it easy in data preparation and comparison between algorithms using separate software applications de souza muñoz et al 2011 trabant et al used a simple subset of restful concepts common calling conventions a common tabular text dataset convention human readable documentation and tools to help scientists learn how to use the web services from iris data management center trabant et al 2015 burkon et al tried to develop and demonstrate the practical use of a generic model of service s interface that could be used as a basis for creation of a formal description of any service in any industry burkoň mackiewicz discussed the benefits of applying the generic interface definition gid of iec 61970 to power system operations and industrial applications mackiewicz 2006 tristan et al introduced a generic service wrapper enabling the optimization of legacy codes assembled in application workflows on grid infrastructure glatard et al 2006 most research work focuses on the server side and attempts to unify the service interfaces however the current landscape is not favorable for unifying service interfaces across domains and industries because service providers have different business goals and different prior knowledge because it s not reasonable to expect the existing service providers will simplify their interfaces this work should focus on the client and create a simple client framework that can handle the disparate service interfaces and provide a universal calling interface for end users 3 objective and design principles our general objective is to hide scientifically irrelevant technical details of geospatial web services and to expose only application related information to end users a generic client framework is created to act as a system building block that bridges scientific end users and disparate geospatial web services fig 3 the general objective is supported by three specific principles a keeping the processing interface simple b making gws more composable in the environmental workflow and c seeking common ground to become a universal solution 3 1 keeping it simple the geospatial process interfaces that take inputs and produce outputs are simple in traditional gis but have become complicated after being translated into gws paradigm the protocols for information communication have evolved concurrently with the long term organic development of the internet the complex historical development of web service technologies has embedded specialized technological knowledge into the gws interfaces as case in point every ordinary world wide web consortium www service request response exchange involves multiple layers of historical technical minutia that are irrelevant to web user s needs the world wide web is successful because web browsers engineers have artfully leveraged protocols without drowning web users in technical details protocol details are hidden far away from the surface that end users see meanwhile consumers of geospatial web service face complicated and frustrating client software that directly exposes gws protocols to end users service consumers must deal with a full stack of engineering information from data exchange to operation semantics for instance for a non gws expert environmental scientist the xml formated messages with many redundant and deeply nested labels are very likely to produce confusion and frustration our objective requires that technical details about communication protocols are simplified and hidden away from the application logic additionally current gws interface descriptions have too many layers in wsdl a service has bindings a binding has port types a port type has operations an operation has input elements an input element has messages a message has schemes and a scheme allows numerous compositions it is normal to initially become lost while figuring the relationships among these terminologies in most cases those concepts are only meaningful to expert users it is unreasonable to have every user encounter them although the layered architecture enhances engineering flexibility when building loosely coupled services it correspondingly raises interface complexity barriers for consumer comprehension to provide a geospatial service process interface that is as simple as gis suis removes those description layers that are not relevant to general users 3 2 making gws composable in environmental workflow at present the adoption of gws in the workflow is much less than was expected when gws were introduced lopez pellicer et al 2012 most scientists treat geospatial web services as simple tools for data access which is just one aspect of the design goals of cyberinfrastructure gws permit a major interoperability breakthrough of computer and network technologies that can directly support and transform the conduct of scientific and engineering research and yield revolutionary payoffs by empowering individual researchers and by increasing the scale scope and flexibility of collective research david 2004 gws are supposed to be chained into workflows for automation to practically help with most basic steps of the real scientific research and in the analysis of datasets of large scale areas and extended temporal periods there are already many successful experiments in using gws into environmental model workflows in lab however after these years of developments those vision goals of gws are never really been achieved in real world environmental model workflows to make gws more composable in the workflow simplification of the interfaces of gws to make them useable in workflows is a prerequisite step 3 3 seeking for common ground to become universal unifying all gws interfaces into a single universal interface is challenging because attempts to do that are obstructed by the extreme interface heterogeneity the reasons for that are highly varied and involve factors like service purpose operation granularity nested tree structures data formats message schemas context scenarios design concepts technical restrictions and subjective provider preferences there is enough idiosyncrasy to make it barely possible to precisely map to all gws interfaces to a single model of api interface our experiences in transforming ogc web services into soap services have confirmed that the famous precept to seek for common ground while reserving the differences bol 1987 in this case states the basic rule for designing a universal solution a universal client interface for all other gws interfaces should center on the common ground and relegate differences to the background we need to identify and classify identical or similar interface concepts and then organize them into a complete interface which is neat consistent and easily intelligible for scientists the outlying and disparate interface concepts that cannot be unified should be handled via hidden adapters or drivers 4 suis framework this section introduces the core model architectural design and usage of suis the core model has two major components profile and driver fig 4 4 1 profile suis profile is a model representing the smallest functional unit of gws it is composed of three public interface classes operation message and parameter and enumeration class datatype fig 5 1 operation denotes the action that the service provides it includes operation name input message output message and narrative description 2 message is the payload exchanged between the client and server it contains a content variable which could be any object such as json javascript object notation xml and kvp key value pairs 3 parameter represents the variables that are inputs and outputs in operations parameter attributes are identity string a data type a parameter name a parameter value a description and minimum and maximum occurrence limits to support suis profile implementation in multiple programming languages we define parameter value as an abstract object and give suis library developers the responsibility to determine the specific data type each parameter object must have an attribute referring to suis datatype enumeration 4 datatype has five named constants fig 5 which represent basic data types common to the general database gis database gws and general programming languages the mapping between the conventional data types and suis data types is listed in table 2 we combine similar data types to simplify the service profile three new types bool number and date are added to support logic description capabilities the datatype enumeration class describes the general types of data content communicated over the internet using structured data exchange formats such as xml json kvp base64 josefsson 2006 ascii binary etc although using different encodings and protocols these parameters belong to the same type file because non expert users have no interests to know how the files are encoded or transferred in communication the encoding and decoding downloading and uploading details should be erased from the surface and processed automatically on the backstage 4 2 driver the technical details of each gws type are isolated and processed in a low level container called suis driver the driver wraps the specific service interfaces with the suis profile and translates suis requests and responses to message formats compliant with service interfaces users only interact with the suis profile and are not required to understand technical details and complexity encapsulated in suis drivers the suis driver backgrounds technical details and acts as a gray box which non experts can treat as a black box while experts and power users can use it to leverage the backend service interface the driver mechanism makes it easy to transform the existing services into suis style without sacrificing their unique capabilities each suis driver wraps one type of service interface suis architecture allows new drivers to be created for other types of service interfaces that do not belong to the three groups discussed in section 2 all suis drivers must implement a mandatory set of methods for decoding the suis requests and encoding suis responses as illustrated in fig 6 a set of methods that translate suis requests to service requests yellow boxes and a set of methods for translating service replies to suis responses blue boxes 4 3 mapping the task of mapping the disparate gws interfaces to a suis profile demands some subtle and challenging design decisions it requires extracting incompatible operation semantics identifying their essential information roles and grouping them most effectively using the categories provided by suis profile specific services allow multiple possible mappings requiring careful consideration of overall semantics for example the common getcapabilities operation of ogc services can be mapped to a suis operation or it can be merged into the initial method digest phase which retrieves service descriptions and initializes the driver when multiple valid design choices are possible we evaluate each option against the general objective and goals of suis section 3 fig 7 shows the mapping we created between the three service categories soap rest ogc and the suis profile the mapping is not simple or direct because the ties lack fixed patterns such as one to one one to many or many to many for example a resource and one of its supported methods in rest interface are combined into a suis operation while the getcapabilities request is mapped to suis operations listing the provided assets taken together these complex mapping choices produce a simple and universal api model that represents capabilities of all gws interface types the specific level of simplifying on the suis interface depends on the acknowledged common requirements from environmental scientists 4 4 payload the data payloads transferred between the suis client and the gws interfaces are automatically generated by suis drivers in accordance to the gws interface schemas since the payloads encapsulate superfluous technical details the suis architecture makes them invisible to scientific end users suis users construct suis requests that are composed of parameter key value pairs that represent the core service request information suis drivers automatically decode and wrap suis requests into request payloads in the same fashion the drivers decode the response payloads and transform them into suis key value pairs to end users the transformation from simple suis data model to complex payload structure is invisible suis drivers provide two transmission methods send and receive for delivering and receiving service payloads if gws requires file inputs the suis drivers are required to support at least one of the three ways to transfer files into or out of gws url simplest http post multipart attachment file size limited or a third party file uploading service e g ftp to turn local files into urls 4 5 usage suis is designed to permit flexible usage that adapts to multiple context scenarios scientists are free to choose from a variety of existing gws facilities such as mobile or real time gws according to their application requirements customized suis drivers allow the inclusion of new message structures and formats the suis data types allow users to input or receive either gis datasets or literal values in program code input specification process activation and output retrieval tasks from diverse gws are presented by suis in a uniform fashion both synchronous and asynchronous modes of operation in the distributed processing environment are supported fig 8 the synchronous mode can be used for instantly responsive services while asynchronous mode allows interaction with extended duration gws processes the suis framework api can be expressed in all general purpose programming languages such as java python and c c thus allowing scientists to use suis with their preferred languages the main steps of using suis to invoke gws table 3 are 1 initialize suis drivers to parse the capabilities of the service such as the operations parameters data types capabilities information is used to configure the driver 2 examine the supported operations optional choose the required operation 3 examine input and output parameters of the chosen operation optional 4 construct the request message by setting values of input parameters 5 send the request and receive the response 6 examine the returned messages optional these steps could be altered to support complex application logic and to support program flow events such as exceptions to use services that are missing service description file or to perform asynchronous requests scientists can skip the service examination steps if they are familiar with the operations the async mode in suis is built because many web services don t support asynchronous requests e g most rest services for those services with async settings e g wps 2 0 ogc 2017 suis driver developers are recommended to directly reuse their native async settings 5 implementation the suis framework should be implemented by suis developers of different programming languages e g java suis4j python suispy etc each library will be maintained by the community of stakeholders who use the corresponding programming language the client providers like arcgis and qgis can contribute to the development and adopt the suis libraries in their software to avoid maintaining their own code to call gws compatibility issues should be fixed by suis developers driven by the science user communities suis has been implemented as a java library named suis4j it utilizes several open source java libraries to achieve suis functionality table 4 suis4j is available on github https github com csiss suis4j for downloading and sharing suis4j development and maintenance follow standard java ecosystem practices github issue tracking system is used for fixing bugs and planning enhancements apache maven miller et al 2010 is used to manage dependencies and to build releases maven allows developers to easily include suis4j as a dependency into their projects the code structure is split into two major packages the suis profile and drivers as described in the core framework model a client class provides the object oriented interface for end users to access suis capabilities the library has no dependencies to any complex gis system and works with all standards conformant gws 6 experiments to validate suis framework against its objectives we applied the suis4j library to two geospatial science use cases agricultural drought modelling deng et al 2013 sun et al 2017b and fvcom finite volume coastal ocean model data processing chen et al 2006 both of which involve a number of heterogeneous gws including gadmfs global agricultural drought monitoring and forecasting system wcs deng et al 2013 nws national weather service rest geoserver geobrain soap services di 2004 and wps all the service calls in both workflows are made in synchronous mode to ensure the service outputs are ready as the inputs of other services 6 1 agricultural drought suppose we are agricultural drought scientists and have created a new index to monitor agricultural drought the equation for the index is 1 droughtindex v c i m p 2 where vci vegetation condition index represents the relative status of vegetation comparing to the historical records in the same period mp monthly precipitation is derived from quantitative precipitation estimate qpe from nws the drought index supposes that vegetation status and precipitation are linearly correlated with drought remote sensing scientists are continuously searching for indices to accurately reflect observed conditions and this index represents a novel attempt in a realistic agricultural drought research scenario multiple datasets must be combined to calculate the drought index and to do our study we must retrieve vci products from gadmfs 4 4 http gis csiss gmu edu gadmfs and then download mp products from the nws ahps advanced hydrologic prediction service website once data is obtained we use geobrain web services han et al 2008 li et al 2010 to process the two products into the final drought index product we employ suis4j to automate these tasks into a geoprocessing workflow the workflow is shown in fig 9 where irregular shapes represent gws purple rectangles represent operations dashed lines represent data flow and solid lines represent suis calling web services we utilize geospatial web services to re project clip and calculate the final drought product based on our index equation we use suis4j to call the required services in the required order and then link their inputs and outputs to form a chain we apply the same workflow chain to different days in 2017 to generate a time series of drought products shown in fig 10 our results show that the long narrow central part of california the area between roads i 5 and ca 99 endures agricultural drought for almost the entire year and seasonally from may to july drought spreads to cover most places in california in august the drought starts to gradually dissipate to present our results we select the april 23 drought index product and render that as a drought map by overlaying drought index on google maps the finished experiment warrants discussion of technical results especially those related to performance issues the drought workflow uses web services from two categories data services and processing services both types of services introduce network load processing services involve computational load on the server and wait time for the client application architecture can be used to address some performance challenges for example suis application might cache the outputted data from gws to reduce both computational efforts and network load across multiple application runs the particular caching strategy depends on suis driver developers the recommended practice is to remember the paths of the files downloaded by users from gws next time when users input the same parameters suis will check the file paths and directly return files to users if they exist the lifetime of the cached files is equal to the time the downloaded files exist in their cached paths for time sensitive requests if the input parameters to gws are different from the input parameters which produced the cache files suis will resend the requests for new files if the input parameters to gws stay the same suis will provide an option for users to force refresh the cache files by downloading new ones to decrease the long delays caused by slow network connections between client and gws suis supports easy switching between multiple gws for example both gadmfs and noaa star provide vci products and gadmfs serves the data via wcs while noaa star uses ftp based shell scripts scientific users can quickly alter which service suis accesses by changing service endpoint and input parameters effective suis applications can preserve network resources by never downloading remote service data more than once for example in traditional usage the wcs getcoverage request will download data from the remote server to the local client then as the next step this data must be uploaded to another location from where it can be downloaded by the re projecting service suis can make this compound process more efficient by allowing service users to skip the download and upload steps and instead directly pass the wcs getcoverage url to the re projecting service interface keens 2007 ogc 2007 2017 as shown in fig 11 no network load is generated as data streams directly from wcs to the re projecting service without being repeatedly downloaded and uploaded the fake call mechanism can save the large part of the total time cost and has the added benefit of making the workflow more concise furthermore suis can prevent idle blocking while waiting for the result data to be received regardless whether a specific geospatial web service supports asynchronous operation semantics suis provides its own asynchronous communication mode to minimize the time scientists spend idly waiting for processing results to derive precise quantitative measures from the aforementioned performance issues we recorded and evaluated the inputs outputs and the duration of each suis call to calculate a representative workload scenario we made simple assumptions concerning potential users and their behavior we then derived average values such as inter arrival times between incoming requests or the requested amount of data from the scenario when suis and gws exchange messages each exchange causes extra delays that vary depending on the client and server machines computing power we compare the computational effort of subsetting and re gridding coverages via wcs to the extra delay caused by suis wrappers and slow network connections fig 12 gives the average allocation of time cost of the suis steps after 100 times repeated tests on gadmfs wcs the experiments request 23 3 mbytes of vci covering the california area of 647 972 square kilometers fig 12 shows that it costs 9 2 of the total time to receive and parse the wcs capabilities document to initialize suis sending getcoverage requests and downloading the vci image only takes 90 7 of the time which is 1 03 s on average meanwhile suis own operations cost barely any time or computational power overall less than 1 ms the service description retrieving takes some time cost due to the complex structure of the capabilities document which makes automatic parsing slow we can improve it by exporting the corresponding suis driver state to a local file and read it back when scientists want to use that web service next time thus avoiding repeating the work of parsing the capabilities of that services recreating a suis driver from a configuration file is much faster than creating a new one from ogc capabilities document the time cost of sending receiving data will rise as the requested data becomes larger transmitting large binary datasets via web messages requires complex actions on both the server and the client protocols like soap allow multiple transmission options such as mtom w3c message transmission optimization mechanism base64 url reference ftp etc 6 2 coastal ocean modelling to demonstrate that suis is a domain independent tool we also use suis4j in a coastal ocean modelling study based on fvcom an unstructured grid finite volume coastal ocean model chen et al 2012 our study area is the gulf of mexico and parts of the atlantic ocean fvcom requires input temperature and salinity data to be formatted into model specific schemas this data transformation task engages a substantial amount of oceanographers time and they have voiced their need for automation of this work for a number of years we excise suis4j to the preprocess water temperature and salinity data to use with fvcom a java program 5 5 https github com zihengsun suis4j blob master src test java suis4j fvcomtest java generating salinity condition grid to use as input for fvcom was created and uploaded to github to demonstrate another possible use of suis this program uses services provided by the earthcube cyberconnector project sun et al 2017a we access three services to download raw data to interpolate it onto the fvcom grid and then finally to reformat it into a special model ready format suis4j invokes the three processes in sequence to produces a map of seawater salinity fig 13 this experiment shows that suis enables instant automation to produce a time series of maps by making some minimal changes to the input parameters and rerunning the workflow sequence this greatly relieves oceanographers from the repetitive tedious and error prone task of manually downloading and processing each dataset because suis vastly reduces the labor involved in using existing services oceanographers are able to take advantage of earthcube cyberconnector facilities that solve their specific data pre processing problems without suis these powerful facilities will remain under utilized besides the two case studies we have actively engaged with our stakeholders in various communities including ogc esip federation of earth science information partners agu american geophysical union and ams american meteorological society and invited modellers and cyberinfrastructure developers to help test suis4j we received some feedbacks which include many positive comments and also some suggestions for further improvements most of them confirm its necessity and simplicity and supporting more languages such as python and r is the most priority thing for broad adoption 7 discussion this section discusses the advantages and disadvantages of suis from both engineering and scientific user s perspective 7 1 vendor perspective 1 scalability scalability is strongly correlated with compatibility suis has exceptional compatibility with existing gws interfaces it supports all generic gws standards suis framework is open and extensible it is easy to create drivers to access service resources through new interfaces one negative consequence of broad compatibility is that the greater variety of interfaces makes work to adapt all of them more complicated 2 interoperability the interoperability of a systems framework determines its level of flexibility and greatly impacts its future development thomas et al 2007 suis supports two levels of interoperability service and workflow service interoperability is provided by compatibility with the standard interfaces of geospatial web services workflow interoperability is supported through workflow language standard and workflow engine suis workflows can be translated to workflows in other workflow languages and systems like bpel business process execution language oasis 2007 or taverna oinn et al 2004 3 performance the resource overhead of suis own operation steps is small and negligible fig 12 most time cost within suis is spent on communicating with gws which is inevitable the internal logic of suis does not incur significant time cost the performance of suis applications is determined mainly by the network capacity the client and server computational power and the workload 7 2 scientist perspective 5 simplicity suis is a clear lifesaver for users tired of interacting with varied and confusing web service interfaces suis simplifies the calling procedures into a unified process which is easy to master for beginners the disparate unnecessary and complicated technical details are safely buried in the background 6 reliability suis will operate without interruption as long as the corresponding geospatial web service is up and running suis itself won t interrupt the user logic unless it encounters a service related exception and has to terminate the entire workflow suis can run indefinitely without interruptions and suis4j library presents an easy and reliable introduction to all gws 7 short learning curve suis exposes minimal little technical details and avoids obscure technical jargon in its api model and documentation the terminology and concepts involved in understanding and using suis are as simple and understandable as possible no technical knowledge of service details is required because suis separates its intuitive profile from the messy service binding details as shown in table 3 users are able to take advantage of the service without learning about service standards web protocol web service profiles workflows xml etc the gws barrier of entry is substantially lowered by suis 8 conclusion this paper proposes a novel framework called suis to simplify the usage of gws in geospatial cyberinfrastructure which has been under utilized because of difficult and disparate interfaces suis creates a universal profile for the major geospatial web service categories and builds a convenient bridge between the existing gws and scientists in geospatial application domains it severely decreases the complexity of using cyberinfrastructure service resources in and especially benefits scientists without gws backgrounds simultaneously the framework supports high scalability interoperability and lower barriers of entry in the future scientists from various communities will take advantage of suis to develop new scientific use cases the suis workflow translation to standard workflow languages will be implemented as snippets of knowledge suis workflows can interconnect and form more advanced models to perform large and complex tasks such as global climate change simulation or global drought forecasting we will continue to work on include suis in broader collaborative research that includes datasets and functionalities from a greater variety of sources and disciplines security and service documentation enhancement are another two important issues and will be studied in the next stage of work in addition suis drivers should enumerate and rank possible transmission protocols according to their network performances for a given volume of data and then select the most effective option dynamic selection of transmission channels can help suis adapt to different data volume scaling scenarios and choices of data formats these methods can be utilized to reduce the time costs of the sending and receiving steps and avoid exceeding timeout limits or overloading the network infrastructure disclosure no interest conflict is claimed acknowledgment we sincerely thank the anonymous reviewers the authors of the software libraries tools and datasets we have used in this work and esip lab this study was supported by grants from the national science foundation grant number ags 1740693 cns 1739705 icer 1440294 pi prof liping di 
